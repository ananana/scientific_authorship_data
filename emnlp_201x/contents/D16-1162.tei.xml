<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:33+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Incorporating Discrete Translation Lexicons into Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Arthur</surname></persName>
							<email>philip.arthur.om0@is.naist.jp gneubig@cs.cmu.edu s-nakamura@is.naist.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Information Science</orgName>
								<orgName type="institution" key="instit1">Nara Institute of Science and Technology † Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Information Science</orgName>
								<orgName type="institution" key="instit1">Nara Institute of Science and Technology † Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Nakamura</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Information Science</orgName>
								<orgName type="institution" key="instit1">Nara Institute of Science and Technology † Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Incorporating Discrete Translation Lexicons into Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1557" to="1567"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Neural machine translation (NMT) often makes mistakes in translating low-frequency content words that are essential to understanding the meaning of the sentence. We propose a method to alleviate this problem by augmenting NMT systems with discrete translation lexicons that efficiently encode translations of these low-frequency words. We describe a method to calculate the lexicon probability of the next word in the translation candidate by using the attention vector of the NMT model to select which source word lexical probabilities the model should focus on. We test two methods to combine this probability with the standard NMT probability: (1) using it as a bias, and (2) linear interpolation. Experiments on two corpora show an improvement of 2.0-2.3 BLEU and 0.13-0.44 NIST score, and faster convergence time. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural machine translation <ref type="bibr">(NMT, §2;</ref><ref type="bibr" target="#b16">Kalchbrenner and Blunsom (2013)</ref>, ) is a variant of statistical machine translation (SMT; <ref type="bibr" target="#b4">Brown et al. (1993)</ref>), using neural networks. NMT has recently gained popularity due to its ability to model the translation process end-to-end using a sin- gle probabilistic model, and for its state-of-the-art performance on several language pairs <ref type="bibr" target="#b26">(Luong et al., 2015a;</ref><ref type="bibr" target="#b36">Sennrich et al., 2016)</ref>.</p><p>One feature of NMT systems is that they treat each word in the vocabulary as a vector of <ref type="bibr">1</ref> Tools to replicate our experiments can be found at http://isw3.naist.jp/~philip-a/emnlp2016/index.html</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input:</head><p>I come from Tunisia. Reference:</p><p>Chunisia no shusshindesu.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(I'm from Tunisia.) System:</head><p>Noruue-no shusshindesu.</p><p>(I'm from Norway.) continuous-valued numbers. This is in contrast to more traditional SMT methods such as phrase-based machine translation (PBMT; <ref type="bibr" target="#b20">Koehn et al. (2003)</ref>), which represent translations as discrete pairs of word strings in the source and target languages. The use of continuous representations is a major advan- tage, allowing NMT to share statistical power be- tween similar words (e.g. "dog" and "cat") or con- texts (e.g. "this is" and "that is"). However, this property also has a drawback in that NMT systems often mistranslate into words that seem natural in the context, but do not reflect the content of the source sentence. For example, <ref type="figure" target="#fig_0">Figure 1</ref> is a sentence from our data where the NMT system mistakenly trans- lated "Tunisia" into the word for "Norway." This variety of error is particularly serious because the content words that are often mistranslated by NMT are also the words that play a key role in determining the whole meaning of the sentence.</p><p>In contrast, PBMT and other traditional SMT methods tend to rarely make this kind of mistake. This is because they base their translations on dis- crete phrase mappings, which ensure that source words will be translated into a target word that has been observed as a translation at least once in the training data. In addition, because the discrete map- pings are memorized explicitly, they can be learned efficiently from as little as a single instance (barring errors in word alignments). Thus we hypothesize that if we can incorporate a similar variety of infor- mation into NMT, this has the potential to alleviate problems with the previously mentioned fatal errors on low-frequency words.</p><p>In this paper, we propose a simple, yet effective method to incorporate discrete, probabilistic lexi- cons as an additional information source in NMT ( §3). First we demonstrate how to transform lexi- cal translation probabilities ( §3.1) into a predictive probability for the next word by utilizing attention vectors from attentional NMT models ( <ref type="bibr" target="#b1">Bahdanau et al., 2015)</ref>. We then describe methods to incorporate this probability into NMT, either through linear in- terpolation with the NMT probabilities ( §3.2.2) or as the bias to the NMT predictive distribution ( §3.2.1). We construct these lexicon probabilities by using traditional word alignment methods on the training data ( §4.1), other external parallel data resources such as a handmade dictionary ( §4.2), or using a hy- brid between the two ( §4.3).</p><p>We perform experiments ( §5) on two English- Japanese translation corpora to evaluate the method's utility in improving translation accuracy and reducing the time required for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural Machine Translation</head><p>The goal of machine translation is to translate a se- quence of source words F = f |F | 1 into a sequence of target words E = e |E| 1 . These words belong to the source vocabulary V f , and the target vocabulary V e respectively. NMT performs this translation by cal- culating the conditional probability p m (e i |F, e i−1 1 ) of the ith target word e i based on the source F and the preceding target words e i−1</p><p>1 . This is done by en- coding the context ⟨F, e i−1 1 ⟩ a fixed-width vector η i , and calculating the probability as follows:</p><formula xml:id="formula_0">p m (e i |F, e i−1 1 ) = softmax(W s η i + b s ),<label>(1)</label></formula><p>where W s and b s are respectively weight matrix and bias vector parameters.</p><p>The exact variety of the NMT model depends on how we calculate η i used as input. While there are many methods to perform this modeling, we opt to use attentional models ( <ref type="bibr" target="#b1">Bahdanau et al., 2015)</ref>, which focus on particular words in the source sen- tence when calculating the probability of e i . These models represent the current state of the art in NMT, and are also convenient for use in our proposed method. Specifically, we use the method of <ref type="bibr" target="#b26">Luong et al. (2015a)</ref>, which we describe briefly here and refer readers to the original paper for details.</p><p>First, an encoder converts the source sentence F into a matrix R where each column represents a sin- gle word in the input sentence as a continuous vec- tor. This representation is generated using a bidirec- tional encoder</p><formula xml:id="formula_1">− → r j = enc(embed(f j ), − → r j−1 ) ← − r j = enc(embed(f j ), ← − r j+1 ) r j = [ ← − r j ; − → r j ].</formula><p>Here the embed(·) function maps the words into a representation ( <ref type="bibr" target="#b2">Bengio et al., 2003)</ref>, and enc(·) is a stacking long short term memory (LSTM) neural network <ref type="bibr" target="#b14">(Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr" target="#b11">Gers et al., 2000;</ref>). Finally we con- catenate the two vectors − → r j and ← − r j into a bidirec- tional representation r j . These vectors are further concatenated into the matrix R where the jth col- umn corresponds to r j .</p><p>Next, we generate the output one word at a time while referencing this encoded input sentence and tracking progress with a decoder LSTM. The de- coder's hidden state h i is a fixed-length continuous vector representing the previous target words e i−1 1 , initialized as h 0 = 0. Based on this h i , we calculate a similarity vector α i , with each element equal to</p><formula xml:id="formula_2">α i,j = sim(h i , r j ).<label>(2)</label></formula><p>sim(·) can be an arbitrary similarity function, which we set to the dot product, following <ref type="bibr" target="#b26">Luong et al. (2015a)</ref>. We then normalize this into an attention vector, which weights the amount of focus that we put on each word in the source sentence</p><formula xml:id="formula_3">a i = softmax(α i ).<label>(3)</label></formula><p>This attention vector is then used to weight the en- coded representation R to create a context vector c i for the current time step c = Ra.</p><p>Finally, we create η i by concatenating the previous hidden state h i−1 with the context vector, and per- forming an affine transform</p><formula xml:id="formula_4">η i = W η [h i−1 ; c i ] + b η ,</formula><p>Once we have this representation of the current state, we can calculate p m (e i |F, e i−1 1 ) according to Equation (1). The next word e i is chosen according to this probability, and we update the hidden state by inputting the chosen word into the decoder LSTM</p><formula xml:id="formula_5">h i = enc(embed(e i ), h i−1 ).<label>(4)</label></formula><p>If we define all the parameters in this model as θ, we can then train the model by minimizing the negative log-likelihood of the training datâ</p><formula xml:id="formula_6">datâ θ = argmin θ ∑ ⟨F, E⟩ ∑ i − log(p m (e i |F, e i−1 1 ; θ)).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Integrating Lexicons into NMT</head><p>In §2 we described how traditional NMT models calculate the probability of the next target word p m (e i |e i−1 1 , F ). Our goal in this paper is to improve the accuracy of this probability estimate by incorpo- rating information from discrete probabilistic lexi- cons. We assume that we have a lexicon that, given a source word f , assigns a probability p l (e|f ) to tar- get word e. For a source word f , this probability will generally be non-zero for a small number of transla- tion candidates, and zero for the majority of words in V E . In this section, we first describe how we in- corporate these probabilities into NMT, and explain how we actually obtain the p l (e|f ) probabilities in §4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Converting Lexicon Probabilities into</head><p>Conditioned Predictive Proabilities First, we need to convert lexical probabilities p l (e|f ) for the individual words in the source sentence F to a form that can be used together with p m (e i |e i−1 1 , F ). Given input sentence F , we can construct a matrix in which each column corre- sponds to a word in the input sentence, each row corresponds to a word in the V E , and the entry cor- responds to the appropriate lexical probability:</p><formula xml:id="formula_7">L F =    p l (e = 1|f 1 ) · · · p l (e = 1|f |F | ) . . . . . . . . . p l (e = |V e ||f 1 ) · · · p l (e = |V e ||f |F | )    .</formula><p>This matrix can be precomputed during the encoding stage because it only requires information about the source sentence F . Next we convert this matrix into a predictive prob- ability over the next word: p l (e i |F, e i−1 1 ). To do so we use the alignment probability a from Equation (3) to weight each column of the L F matrix:</p><formula xml:id="formula_8">p l (e i |F, e i−1 1 ) = L F a i =    p l (e = 1|f 1 ) · · · p lex (e = 1|f |F | ) . . . . . . . . . p l (e = V e |f 1 ) · · · p lex (e = V e |f |F | )       a i,1 . . . a i,|F |    .</formula><p>This calculation is similar to the way how attentional models calculate the context vector c i , but over a vector representing the probabilities of the target vo- cabulary, instead of the distributed representations of the source words. The process of involving a i is important because at every time step i, the lexi- cal probability p l (e i |e i−1 1 , F ) will be influenced by different source words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Combining Predictive Probabilities</head><p>After calculating the lexicon predictive proba- bility p l (e i |e i−1 1 , F ), next we need to integrate this probability with the NMT model probability p m (e i |e i−1 1 , F ). To do so, we examine two methods: (1) adding it as a bias, and (2) linear interpolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Model Bias</head><p>In our first bias method, we use p l (·) to bias the probability distribution calculated by the vanilla NMT model. Specifically, we add a small constant ϵ to p l (·), take the logarithm, and add this adjusted log probability to the input of the softmax as follows:</p><formula xml:id="formula_9">p b (e i |F, e i−1 1 ) = softmax(W s η i + b s + log(p l (e i |F, e i−1 1 ) + ϵ)).</formula><p>We take the logarithm of p l (·) so that the values will still be in the probability domain after the softmax is calculated, and add the hyper-parameter ϵ to prevent zero probabilities from becoming −∞ after taking the log. When ϵ is small, the model will be more heavily biased towards using the lexicon, and when ϵ is larger the lexicon probabilities will be given less weight. We use ϵ = 0.001 for this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Linear Interpolation</head><p>We also attempt to incorporate the two probabil- ities through linear interpolation between the stan- dard NMT probability model probability p m (·) and the lexicon probability p l (·). We will call this the linear method, and define it as follows:</p><formula xml:id="formula_10">p o (e i |F, e i−1 1 ) =    p l (e i = 1|F, e i−1 1 ) p m (e = 1|F, e i−1 1 ) . . . . . . p l (e i = |V e ||F, e i−1 1 ) p m (e = |V e ||F, e i−1 1 )    [ λ 1 − λ ] ,</formula><p>where λ is an interpolation coefficient that is the re- sult of the sigmoid function λ = sig(x) = 1 1+e −x . x is a learnable parameter, and the sigmoid func- tion ensures that the final interpolation level falls be- tween 0 and 1. We choose x = 0 (λ = 0.5) at the beginning of training.</p><p>This notation is partly inspired by <ref type="bibr" target="#b0">Allamanis et al. (2016)</ref> and <ref type="bibr" target="#b12">Gu et al. (2016)</ref> who use linear inter- polation to merge a standard attentional model with a "copy" operator that copies a source word as-is into the target sentence. The main difference is that they use this to copy words into the output while our method uses it to influence the probabilities of all target words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Constructing Lexicon Probabilities</head><p>In the previous section, we have defined some ways to use predictive probabilities p l (e i |F, e i−1 1 ) based on word-to-word lexical probabilities p l (e|f ). Next, we define three ways to construct these lexical prob- abilities using automatically learned lexicons, hand- made lexicons, or a combination of both.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Automatically Learned Lexicons</head><p>In traditional SMT systems, lexical translation prob- abilities are generally learned directly from parallel data in an unsupervised fashion using a model such as the IBM models ( <ref type="bibr" target="#b4">Brown et al., 1993;</ref><ref type="bibr" target="#b34">Och and Ney, 2003</ref>). These models can be used to estimate the alignments and lexical translation probabilities p l (e|f ) between the tokens of the two languages us- ing the expectation maximization (EM) algorithm.</p><p>First in the expectation step, the algorithm esti- mates the expected count c(e|f ). In the maximiza- tion step, lexical probabilities are calculated by di- viding the expected count by all possible counts:</p><formula xml:id="formula_11">p l,a (e|f ) = c(f, e) ∑ ˜ e c(f, ˜ e) ,</formula><p>The IBM models vary in level of refinement, with Model 1 relying solely on these lexical probabil- ities, and latter IBM models (Models 2, 3, 4, 5) introducing more sophisticated models of fertility and relative alignment. Even though IBM models also occasionally have problems when dealing with the rare words (e.g. "garbage collecting" effects ( <ref type="bibr" target="#b23">Liang et al., 2006</ref>)), traditional SMT systems gen- erally achieve better translation accuracies of low- frequency words than NMT systems ), indicating that these problems are less prominent than they are in NMT.</p><p>Note that in many cases, NMT limits the target vocabulary (Jean et al., 2015) for training speed or memory constraints, resulting in rare words not be- ing covered by the NMT vocabulary V E . Accord- ingly, we allocate the remaining probability assigned by the lexicon to the unknown word symbol ⟨unk⟩:</p><formula xml:id="formula_12">p l,a (e = ⟨unk⟩|f ) = 1 − ∑ i∈Ve p l,a (e = i|f ).<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Manual Lexicons</head><p>In addition, for many language pairs, broad- coverage handmade dictionaries exist, and it is desir- able that we be able to use the information included in them as well. Unlike automatically learned lexi- cons, however, handmade dictionaries generally do not contain translation probabilities. To construct the probability p l (e|f ), we define the set of trans- lations K f existing in the dictionary for particular source word f , and assume a uniform distribution over these words:</p><formula xml:id="formula_13">p l,m (e|f ) = { 1 |K f | if e ∈ K f 0 otherwise .</formula><p>Following Equation (5), unknown source words will assign their probability mass to the ⟨unk⟩ tag.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Hybrid Lexicons</head><p>Handmade lexicons have broad coverage of words but their probabilities might not be as accurate as the </p><formula xml:id="formula_14">p l,h (e|f ) = { p l,a (e|f ) if f is covered p l,m (e|f ) otherwise<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment &amp; Result</head><p>In this section, we describe experiments we use to evaluate our proposed methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Settings</head><p>Dataset: We perform experiments on two widely- used tasks for the English-to-Japanese language pair: KFTT (Neubig, 2011) and BTEC ( <ref type="bibr" target="#b17">Kikui et al., 2003)</ref>. KFTT is a collection of Wikipedia article about city of Kyoto and BTEC is a travel conversa- tion corpus. BTEC is an easier translation task than KFTT, because KFTT covers a broader domain, has a larger vocabulary of rare words, and has relatively long sentences. The details of each corpus are de- picted in <ref type="table">Table 1</ref>. We tokenize English according to the Penn Tree- bank standard <ref type="bibr" target="#b28">(Marcus et al., 1993</ref>) and lowercase, <ref type="bibr">2</ref> Alternatively, we could imagine a method where we com- bined the training data and dictionary before training the word alignments to create the lexicon. We attempted this, and results were comparable to or worse than the fill-up method, so we use the fill-up method for the remainder of the paper.</p><p>3 While most words in the V f will be covered by the learned lexicon, many words (13% in experiments) are still left uncov- ered due to alignment failures or other factors. and tokenize Japanese using <ref type="bibr">KyTea (Neubig et al., 2011</ref>). We limit training sentence length up to 50 in both experiments and keep the test data at the original length. We replace words of frequency less than a threshold u in both languages with the ⟨unk⟩ symbol and exclude them from our vocabulary. We choose u = 1 for BTEC and u = 3 for KFTT, re- sulting in |V f | = 17.8k, |V e | = 21.8k for BTEC and |V f | = 48.2k, |V e | = 49.1k for KFTT. NMT Systems: We build the described models us- ing the Chainer 4 toolkit. The depth of the stacking LSTM is d = 4 and hidden node size h = 800. We concatenate the forward and backward encod- ings (resulting in a 1600 dimension vector) and then perform a linear transformation to 800 dimensions. We train the system using the Adam ( <ref type="bibr" target="#b18">Kingma and Ba, 2014</ref>) optimization method with the default set- tings: α = 1e−3, β 1 = 0.9, β 2 = 0.999, ϵ = 1e−8. Additionally, we add dropout ( <ref type="bibr" target="#b37">Srivastava et al., 2014</ref>) with drop rate r = 0.2 at the last layer of each stacking LSTM unit to prevent overfitting. We use a batch size of B = 64 and we run a total of N = 14 iterations for all data sets. All of the ex- periments are conducted on a single GeForce GTX TITAN X GPU with a 12 GB memory cache.</p><p>At test time, we use beam search with beam size b = 5. We follow <ref type="bibr" target="#b27">Luong et al. (2015b)</ref> in replac- ing every unknown token at position i with the tar- get token that maximizes the probability p l,a (e i |f j ). We choose source word f j according to the high- est alignment score in Equation (3). This unknown word replacement is applied to both baseline and proposed systems. Finally, because NMT models tend to give higher probabilities to shorter sentences ( <ref type="bibr" target="#b6">Cho et al., 2014</ref>), we discount the probability of ⟨EOS⟩ token by 10% to correct for this bias. Traditional SMT Systems: We also prepare two traditional SMT systems for comparison: a PBMT system ( <ref type="bibr" target="#b20">Koehn et al., 2003</ref>) using Moses <ref type="bibr">5 (Koehn et al., 2007)</ref>, and a hierarchical phrase-based MT sys- tem (Chiang, 2007) using Travatar 6 <ref type="bibr" target="#b33">(Neubig, 2013)</ref>, Systems are built using the default settings, with models trained on the training data, and weights tuned on the development data.   <ref type="table">Table 2</ref>: Accuracies for the baseline attentional NMT (attn) and the proposed bias-based method using the automatic (auto-bias) or hybrid (hyb-bias) dictionaries. Bold indicates a gain over the attn baseline, † indicates a significant increase at p &lt; 0.05, and * indicates p &lt; 0.10. Traditional phrase-based (pbmt) and hierarchical phrase based (hiero) systems are shown for reference.</p><p>proposed method, and apply bias and linear method for all of them, totaling 6 experiments. The first lexicon (auto) is built on the training data using the automatically learned lexicon method of §4.1 separately for both the BTEC and KFTT ex- periments. Automatic alignment is performed using GIZA++ <ref type="bibr" target="#b34">(Och and Ney, 2003)</ref>. The second lexicon (man) is built using the popular English-Japanese dictionary Eijiro 7 with the manual lexicon method of §4.2. Eijiro contains 104K distinct word-to-word translation entries. The third lexicon (hyb) is built by combining the first and second lexicon with the hybrid method of §4.3. Evaluation: We use standard single reference BLEU-4 ( <ref type="bibr" target="#b35">Papineni et al., 2002</ref>) to evaluate the trans- lation performance. Additionally, we also use NIST <ref type="bibr" target="#b9">(Doddington, 2002)</ref>, which is a measure that puts a particular focus on low-frequency word strings, and thus is sensitive to the low-frequency words we are focusing on in this paper. We measure the statistical significant differences between systems using paired bootstrap resampling <ref type="bibr" target="#b22">(Koehn, 2004</ref>) with 10,000 it- erations and measure statistical significance at the p &lt; 0.05 and p &lt; 0.10 levels. Additionally, we also calculate the recall of rare words from the references. We define "rare words" as words that appear less than eight times in the tar- get training corpus or references, and measure the percentage of time they are recovered by each trans- lation system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Effect of Integrating Lexicons</head><p>In this section, we first a detailed examination of the utility of the proposed bias method when used  with the auto or hyb lexicons, which empirically gave the best results, and perform a comparison among the other lexicon integration methods in the following section. <ref type="table">Table 2</ref> shows the results of these methods, along with the corresponding baselines.</p><p>First, compared to the baseline attn, our bias method achieved consistently higher scores on both test sets. In particular, the gains on the more diffi- cult KFTT set are large, up to 2.3 BLEU, 0.44 NIST, and 30% Recall, demonstrating the utility of the pro- posed method in the face of more diverse content and fewer high-frequency words.</p><p>Compared to the traditional pbmt systems hiero, particularly on KFTT we can see that the proposed method allows the NMT system to exceed the traditional SMT methods in BLEU. This is de- spite the fact that we are not performing ensembling, which has proven to be essential to exceed tradi- tional systems in several previous works (Sutskever Input Do you have an opinion regarding extramarital affairs? Reference  <ref type="table">Table 3</ref>: Examples where the proposed auto-bias improved over the baseline system attn. Underlines indicate words were mistaken in the baseline output but correct in the proposed model's output.</p><p>et al., <ref type="bibr" target="#b26">Luong et al., 2015a;</ref><ref type="bibr" target="#b36">Sennrich et al., 2016)</ref>. Interestingly, despite gains in BLEU, the NMT methods still fall behind in NIST score on the KFTT data set, demonstrating that traditional SMT systems still tend to have a small advantage in translating lower-frequency words, despite the gains made by the proposed method.</p><p>In <ref type="table">Table 3</ref>, we show some illustrative examples where the proposed method (auto-bias) was able to obtain a correct translation while the normal at- tentional model was not. The first example is a mistake in translating "extramarital affairs" into the Japanese equivalent of "soccer," entirely changing the main topic of the sentence. This is typical of the errors that we have observed NMT systems make (the mistake from <ref type="figure" target="#fig_0">Figure 1</ref> is also from attn, and was fixed by our proposed method). The second ex- ample demonstrates how these mistakes can then af- fect the process of choosing the remaining words, propagating the error through the whole sentence.</p><p>Next, we examine the effect of the proposed method on the training time for each neural MT method, drawing training curves for the KFTT data in <ref type="figure" target="#fig_3">Figure 2</ref>. Here we can see that the proposed bias training methods achieve reasonable BLEU scores in the upper 10s even after the first iteration. In con- trast, the baseline attn method has a BLEU score of around 5 after the first iteration, and takes signifi- cantly longer to approach values close to its maximal accuracy. This shows that by incorporating lexical probabilities, we can effectively bootstrap the learn- ing of the NMT system, allowing it to approach an appropriate answer in a more timely fashion. <ref type="bibr">8</ref> It is also interesting to examine the alignment vec- tors produced by the baseline and proposed meth-   <ref type="table">Table 4</ref>: A comparison of the bias and linear lexicon integration methods on the automatic, man- ual, and hybrid lexicons. The first line without lexi- con is the traditional attentional NMT.</p><p>ods, a visualization of which we show in <ref type="figure" target="#fig_5">Figure  3</ref>. For this sentence, the outputs of both meth- ods were both identical and correct, but we can see that the proposed method (right) placed sharper attention on the actual source word correspond- ing to content words in the target sentence. This trend of peakier attention distributions in the pro- posed method held throughout the corpus, with the per-word entropy of the attention vectors being 3.23 bits for auto-bias, compared with 3.81 bits for attn, indicating that the auto-bias method places more certainty in its attention decisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison of Integration Methods</head><p>Finally, we perform a full comparison between the various methods for integrating lexicons into the translation process, with results shown in <ref type="table">Table 4</ref>.</p><p>In general the bias method improves accuracy for the auto and hyb lexicon, but is less effective for the man lexicon. This is likely due to the fact that the manual lexicon, despite having broad coverage, did not sufficiently cover target-domain words (cov- erage of unique words in the source vocabulary was 35.3% and 9.7% for BTEC and KFTT respectively). Interestingly, the trend is reversed for the linear method, with it improving man systems, but causing decreases when using the auto and hyb lexicons. This indicates that the linear method is more suited for cases where the lexi- con does not closely match the target domain, and plays a more complementary role. Compared to the log-linear modeling of bias, which strictly en- forces constraints imposed by the lexicon distribu- tion <ref type="bibr" target="#b19">(Klakow, 1998)</ref>, linear interpolation is intu- itively more appropriate for integrating this type of complimentary information.</p><p>On the other hand, the performance of linear in- terpolation was generally lower than that of the bias method. One potential reason for this is the fact that we use a constant interpolation coefficient that was set fixed in every context. <ref type="bibr" target="#b12">Gu et al. (2016)</ref> have re- cently developed methods to use the context infor- mation from the decoder to calculate the different in- terpolation coefficients for every decoding step, and it is possible that introducing these methods would improve our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Additional Experiments</head><p>To test whether the proposed method is useful on larger data sets, we also performed follow-up ex- periments on the larger Japanese-English ASPEC dataset ( <ref type="bibr" target="#b30">Nakazawa et al., 2016</ref>) that consist of 2 million training examples, 63 million tokens, and 81,000 vocabulary size. We gained an improvement in BLEU score from 20.82 using the attn baseline to 22.66 using the auto-bias proposed method. This experiment shows that our method scales to larger datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>From the beginning of work on NMT, unknown words that do not exist in the system vocabulary have been focused on as a weakness of these sys- tems. Early methods to handle these unknown words replaced them with appropriate words in the target vocabulary <ref type="bibr" target="#b15">(Jean et al., 2015;</ref><ref type="bibr" target="#b27">Luong et al., 2015b)</ref> according to a lexicon similar to the one used in this work. In contrast to our work, these only handle unknown words and do not incorporate information from the lexicon in the learning procedure.</p><p>There have also been other approaches that incor- porate models that learn when to copy words as-is into the target language ( <ref type="bibr" target="#b0">Allamanis et al., 2016;</ref><ref type="bibr" target="#b12">Gu et al., 2016;</ref><ref type="bibr" target="#b13">Gülçehre et al., 2016</ref>). These models are similar to the linear approach of §3.2.2, but are only applicable to words that can be copied as- is into the target language. In fact, these models can be thought of as a subclass of the proposed approach that use a lexicon that assigns a all its probability to target words that are the same as the source. On the other hand, while we are simply using a static in- terpolation coefficient λ, these works generally have a more sophisticated method for choosing the inter- polation between the standard and "copy" models. Incorporating these into our linear method is a promising avenue for future work.</p><p>In addition <ref type="bibr" target="#b29">Mi et al. (2016)</ref> have also recently pro- posed a similar approach by limiting the number of vocabulary being predicted by each batch or sen- tence. This vocabulary is made by considering the original HMM alignments gathered from the train- ing corpus. Basically, this method is a specific ver- sion of our bias method that gives some of the vocab- ulary a bias of negative infinity and all other vocab- ulary a uniform distribution. Our method improves over this by considering actual translation probabil- ities, and also considering the attention vector when deciding how to combine these probabilities.</p><p>Finally, there have been a number of recent works that improve accuracy of low-frequency words us- ing character-based translation models ( <ref type="bibr" target="#b24">Ling et al., 2015;</ref><ref type="bibr" target="#b8">Costa-Jussà and Fonollosa, 2016;</ref><ref type="bibr" target="#b7">Chung et al., 2016)</ref>. However, <ref type="bibr" target="#b25">Luong and Manning (2016)</ref> have found that even when using character-based models, incorporating information about words al- lows for gains in translation accuracy, and it is likely that our lexicon-based method could result in im- provements in these hybrid systems as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion &amp; Future Work</head><p>In this paper, we have proposed a method to in- corporate discrete probabilistic lexicons into NMT systems to solve the difficulties that NMT systems have demonstrated with low-frequency words. As a result, we achieved substantial increases in BLEU (2.0-2.3) and NIST (0.13-0.44) scores, and observed qualitative improvements in the translations of con- tent words.</p><p>For future work, we are interested in conducting the experiments on larger-scale translation tasks. We also plan to do subjective evaluation, as we expect that improvements in content word translation are critical to subjective impressions of translation re- sults. Finally, we are also interested in improve- ments to the linear method where λ is calculated based on the context, instead of using a fixed value.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of a mistake made by NMT on low-frequency content words.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Lexicons</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Training curves for the baseline attn and the proposed bias method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Furin</head><label></label><figDesc>ni kanshite iken ga arimasu ka. attn Sakk¯ a ni kansuru iken wa arimasu ka. (Do you have an opinion about soccer?) auto-bias Furin ni kanshite iken ga arimasu ka. (Do you have an opinion about affairs?) Input Could you put these fragile things in a safe place? Reference Kono kowaremono o anzen'na basho ni oite moraemasen ka. attn Kich¯ o-hin o anzen ni dashitai nodesuga. (I'd like to safely put out these valuables.) auto-bias Kono kowaremono o anzen'na basho ni oite moraemasen ka. (Could you put these fragile things in a safe place?)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Attention matrices for baseline attn and proposed bias methods. Lighter colors indicate stronger attention between the words, and boxes surrounding words indicate the correct alignments.</figDesc><graphic url="image-2.png" coords="7,313.80,314.66,122.80,132.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>(</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>: We use a total of 3 lexicons for the</figDesc><table>System 

BTEC 
KFTT 
BLEU NIST RECALL BLEU NIST RECALL 
pbmt 
48.18 
6.05 
27.03 
22.62 
5.79 
13.88 
hiero 
52.27 
6.34 
24.32 
22.54 
5.82 
12.83 
attn 
48.31 
5.98 
17.39 
20.86 
5.15 
17.68 
auto-bias 49.74  *  6.11  *  
50.00 
23.20  † 5.59  † 
19.32 
hyb-bias 
50.34  † 6.10  *  
41.67 
22.80  † 5.55  † 
16.67 

</table></figure>

			<note place="foot" n="4"> http://chainer.org/index.html 5 http://www.statmt.org/moses/ 6 http://www.phontron.com/travatar/</note>

			<note place="foot" n="8"> Note that these gains are despite the fact that one iteration of the proposed method takes a longer (167 minutes for attn vs. 275 minutes for auto-bias) due to the necessity to calculate and use the lexical probability matrix for each sentence. It also takes an additional 297 minutes to train the lexicon with GIZA++, but this can be greatly reduced with more efficient training methods (Dyer et al., 2013).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>We thank Makoto Morishita and Yusuke Oda for their help in this project. We also thank the faculty members of AHC lab for their supports and sugges-tions.</p><p>This work was supported by grants from the Min-istry of Education, Culture, Sport, Science, and Technology of Japan and in part by JSPS KAKENHI Grant Number 16H05873.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A convolutional attention network for extreme summarization of source code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33th International Conference on Machine Learning (ICML)</title>
		<meeting>the 33th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Conference on Learning Representations</title>
		<meeting>the 4th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fill-up versus interpolation methods for phrasebased SMT adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 International Workshop on Spoken Language Translation (IWSLT)</title>
		<meeting>the 2011 International Workshop on Spoken Language Translation (IWSLT)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="136" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The mathematics of statistical machine translation: Parameter estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J Della</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">A</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="page" from="263" to="311" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hierarchical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="page" from="201" to="228" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Syntax and Structure in Statistical Translation (SSST)</title>
		<meeting>the Workshop on Syntax and Structure in Statistical Translation (SSST)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="103" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A character-level decoder without explicit segmentation for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1693" to="1703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Character-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><forename type="middle">R</forename><surname>Costa-Jussà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fonollosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="357" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automatic evaluation of machine translation quality using n-gram co-occurrence statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Doddington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Conference on Human Language Technology Research</title>
		<meeting>the Second International Conference on Human Language Technology Research</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="138" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A simple, fast, and effective reparameterization of IBM model 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Chahuneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="644" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to forget: Continual prediction with LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><forename type="middle">A</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="page" from="2451" to="2471" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
	<note>Cummins</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Incorporating copying mechanism in sequenceto-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">O K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1631" to="1640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pointing the unknown words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="140" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On using very large target vocabulary for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53th Annual Meeting of the Association for Computational Linguistics (ACL) and the 7th Internationali Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</title>
		<meeting>the 53th Annual Meeting of the Association for Computational Linguistics (ACL) and the 7th Internationali Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing<address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015-07-26" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1700" to="1709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Creating corpora for speech-to-speech translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gen-Ichiro</forename><surname>Kikui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiyuki</forename><surname>Takezawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seiichi</forename><surname>Yamamoto</surname></persName>
		</author>
		<idno>EU- ROSPEECH 2003 -INTERSPEECH 2003</idno>
	</analytic>
	<monogr>
		<title level="m">8th European Conference on Speech Communication and Technology</title>
		<meeting><address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-09-01" />
			<biblScope unit="page" from="381" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Log-linear interpolation of language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietrich</forename><surname>Klakow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Speech and Language Processing</title>
		<meeting>the 5th International Conference on Speech and Language Processing</meeting>
		<imprint>
			<publisher>ICSLP</publisher>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL)</title>
		<meeting>the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="48" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 45th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
	<note>Ondřej Bojar, Alexandra Constantin, and Evan Herbst</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Statistical significance tests for machine translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Alignment by agreement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL)</title>
		<meeting>the 2006 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="104" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Character-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Achieving open vocabulary neural machine translation with hybrid word-character models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1054" to="1063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Addressing the rare word problem in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53th Annual Meeting of the Association for Computational Linguistics (ACL) and the 7th Internationali Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</title>
		<meeting>the 53th Annual Meeting of the Association for Computational Linguistics (ACL) and the 7th Internationali Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015-07-26" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="11" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of English: The Penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Mitchell P Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Vocabulary manipulation for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Ittycheriah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="124" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Aspec: Asian scientific paper excerpt corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiaki</forename><surname>Nakazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Yaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyotaka</forename><surname>Uchimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hitoshi</forename><surname>Isahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC 2016)</title>
		<meeting>the Ninth International Conference on Language Resources and Evaluation (LREC 2016)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2204" to="2208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pointwise prediction for robust, adaptable Japanese morphological analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yosuke</forename><surname>Nakata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinsuke</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="529" to="533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">The Kyoto free translation task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<ptr target="http://www.phontron.com/kftt" />
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Travatar: A forest-to-string machine translation engine based on tree transducers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 51th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="91" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A systematic comparison of various statistical alignment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Linguistics</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="19" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Bleu: A method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="86" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>the 28th Annual Conference on Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
