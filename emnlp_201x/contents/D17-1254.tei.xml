<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:15+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Generic Sentence Representations Using Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchen</forename><surname>Pu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<postCode>98052</postCode>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Generic Sentence Representations Using Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2390" to="2400"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a new encoder-decoder approach to learn distributed sentence representations that are applicable to multiple purposes. The model is learned by using a convolutional neural network as an encoder to map an input sentence into a continuous vector, and using a long short-term memory recurrent neural network as a decoder. Several tasks are considered, including sentence reconstruction and future sentence prediction. Further, a hierarchical encoder-decoder model is proposed to encode a sentence to predict multiple future sentences. By training our models on a large collection of novels, we obtain a highly generic con-volutional sentence encoder that performs well in practice. Experimental results on several benchmark datasets, and across a broad range of applications, demonstrate the superiority of the proposed model over competing methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning sentence representations is central to many natural language modeling applications. The aim of a model for this task is to learn fixed- length feature vectors that encode the seman- tic and syntactic properties of sentences. Deep learning techniques have shown promising per- formance on sentence modeling, via feedfor- ward neural networks ( <ref type="bibr">Huang et al., 2013</ref>), re- current neural networks (RNNs) <ref type="bibr">(Hochreiter and Schmidhuber, 1997)</ref>, convolutional neural net- works (CNNs) ( <ref type="bibr">Kalchbrenner et al., 2014;</ref><ref type="bibr">Kim, 2014;</ref><ref type="bibr">Shen et al., 2014)</ref>, and recursive neural net- works ( <ref type="bibr">Socher et al., 2013</ref>). Most of these models are task-dependent: they are trained specifically for a certain task. However, these methods may be- come inefficient when we need to repeatedly learn sentence representations for a large number of dif- ferent tasks, because they may require retraining a new model for each individual task. In this paper, in contrast, we are primarily interested in learning generic sentence representations that can be used across domains.</p><p>Several approaches have been proposed for learn- ing generic sentence embeddings. The paragraph- vector model of <ref type="bibr">Le and Mikolov (2014)</ref> incorpo- rates a global context vector into the log-linear neu- ral language model ( <ref type="bibr">Mikolov et al., 2013</ref>) to learn the sentence representation; however, at predic- tion time, one needs to perform gradient descent to compute a new vector. The sequence autoencoder of <ref type="bibr">Dai and Le (2015)</ref> describes an encoder-decoder model to reconstruct the input sentence, while the skip-thought model of <ref type="bibr" target="#b6">Kiros et al. (2015)</ref> extends the encoder-decoder model to reconstruct the sur- rounding sentences of an input sentence. Both the encoder and decoder of the methods above are mod- eled as RNNs.</p><p>CNNs have recently achieved excellent results in various task-dependent natural language applica- tions as the sentence encoder ( <ref type="bibr">Kalchbrenner et al., 2014;</ref><ref type="bibr">Kim, 2014;</ref><ref type="bibr">Hu et al., 2014</ref>). This motivates us to propose a CNN encoder for learning generic sentence representations within the framework of encoder-decoder models proposed by <ref type="bibr">Sutskever et al. (2014)</ref>; <ref type="bibr">Cho et al. (2014)</ref>. Specifically, a CNN encoder performs convolution and pooling operations on an input sentence, then uses a fully- connected layer to produce a fixed-length encoding of the sentence. This encoding vector is then fed into a long short-term memory (LSTM) recurrent network to produce a target sentence. Depending on the task, we propose three models: (i) CNN- LSTM autoencoder: this model seeks to reconstruct the original input sentence, by capturing the in- tra-sentence information; (ii) CNN-LSTM future</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>you$$$$$$will$$$$$$love$$$$$$it$$$$$$$$$$! you$$will$$$love$$$it$$$$$$$$! i promise$$.</head><p>sentence&amp;encoder sentence&amp;decoder paragraph&amp; generator predictor: this model aims to predict a future sen- tence, by leveraging inter-sentence information; (iii) CNN-LSTM composite model: in this case, there are two LSTMs, decoding the representation to the input sentence itself and a future sentence. This composite model aims to learn a sentence en- coder that captures both intra-and inter-sentence information.</p><formula xml:id="formula_0">this$$$$$$$is$$$$$$$great$$$$$$$. you$$$$$$will$$$$$$love$$$$$$it$$$$$$$$$$! !$$$$$$$it$$$$$love$$will$$$you i promise$$.</formula><p>The proposed CNN-LSTM future predictor model only considers the immediately subsequent sentence as context. In order to capture longer- term dependencies between sentences, we further introduce a hierarchical encoder-decoder model. This model abstracts the RNN language model of <ref type="bibr">Mikolov et al. (2010)</ref> to the sentence level. That is, instead of using the current word in a sentence to predict future words (sentence continuation), we encode a sentence to predict multiple future sen- tences (paragraph continuation). This model is termed hierarchical CNN-LSTM model.</p><p>As in <ref type="bibr" target="#b6">Kiros et al. (2015)</ref>, we first train our proposed models on a large collection of novels. We then evaluate the CNN sentence encoder as a generic feature extractor for 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking and 5 standard classification benchmarks. In these experiments, we train a linear classifier on top of the extracted sentence features, without additional fine-tuning of the CNN. We show that our trained sentence encoder yields generic repre- sentations that perform as well as, or better, than those of <ref type="bibr" target="#b6">Kiros et al. (2015)</ref>; <ref type="bibr">Hill et al. (2016)</ref>, in all the tasks considered.</p><p>Summarizing, the main contribution of this pa- per is a new class of CNN-LSTM encoder-decoder models that is able to leverage the vast quan- tity of unlabeled text for learning generic sen- tence representations. Inspired by the skip-thought model ( <ref type="bibr" target="#b6">Kiros et al., 2015</ref>), we have further explored different variants: (i) CNN is used as the sentence encoder rather than RNN; (ii) larger context win- dows are considered: we propose the hierarchical CNN-LSTM model to encode a sentence for pre- dicting multiple future sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model description 2.1 CNN-LSTM model</head><p>Consider the sentence pair (s x , s y ). The encoder, a CNN, encodes the first sentence s x into a fea- ture vector z, which is then fed into an LSTM decoder that predicts the second sentence s y . Let w t x ∈ {1, . . . , V } represent the t-th word in sen- tences s x , where w t x indexes one element in a V - dimensional set (vocabulary); w t y is defined simi- larly w.r.t. s y . Each word w t x is embedded into a k-dimensional vector </p><formula xml:id="formula_1">x t = W e [w t x ],</formula><formula xml:id="formula_2">= W e [w t y ].</formula><p>CNN encoder <ref type="bibr">The CNN architecture in Kim (2014);</ref><ref type="bibr">Collobert et al. (2011)</ref> is used for sentence encoding, which consists of a convolution layer and a max-pooling operation over the entire sen- tence for each feature map. A sentence of length T (padded where necessary) is represented as a matrix X ∈ R k×T , by concatenating its word embeddings as columns, i.e., the t-th column of X is x t .</p><p>A convolution operation involves a filter W c ∈ R k×h , applied to a window of h words to produce a new feature. According to <ref type="bibr">Collobert et al. (2011)</ref>, we can induce one feature map c = f (X * W c + b) ∈ R T −h+1 , where f (·) is a nonlinear activation function such as the hyperbolic tangent used in our experiments, b ∈ R T −h+1 is a bias vector, and * denotes the convolutional operator. Convolving the same filter with the h-gram at every position in the sentence allows the features to be extracted independently of their position in the sentence. We then apply a max-over-time pooling operation <ref type="bibr">(Collobert et al., 2011</ref>) to the feature map and take its maximum value, i.e., ˆ c = max{c}, as the feature corresponding to this particular filter. This pooling scheme tries to capture the most important feature, i.e., the one with the highest value, for each fea- ture map, effectively filtering out less informative compositions of words. Further, pooling also guar- antees that the extracted features are independent of the length of the input sentence.</p><p>The above process describes how one feature is extracted from one filter. In practice, the model uses multiple filters with varying window sizes <ref type="bibr">(Kim, 2014)</ref>. Each filter can be considered as a linguistic feature detector that learns to rec- ognize a specific class of n-grams (or h-grams, in the above notation). However, since the h-grams are computed in the embedding space, the model naturally handles similar h-grams composed of syn- onyms. Assume we have m window sizes, and for each window size, we use d filters; then we obtain a md-dimensional vector to represent a sentence.</p><p>Compared with the LSTM encoders used in <ref type="bibr" target="#b6">Kiros et al. (2015)</ref>; <ref type="bibr">Dai and Le (2015)</ref>; <ref type="bibr">Hill et al. (2016)</ref>, a CNN encoder may have the following ad- vantages. First, the sparse connectivity of a CNN, which indicates fewer parameters are required, typ- ically improves its statistical efficiency as well as reduces memory requirements ( <ref type="bibr">Goodfellow et al., 2016)</ref>. For example, excluding the number of pa- rameters used in the word embeddings, our trained CNN sentence encoder has 3 million parameters, while the skip-thought vector of <ref type="bibr" target="#b6">Kiros et al. (2015)</ref> contains 40 million parameters. Second, a CNN is easy to implement in parallel over the whole sentence, while an LSTM needs sequential compu- tation.</p><p>LSTM decoder The CNN encoder maps sen- tence s x into a vector z. The probability of a length-T sentence s y given the encoded feature vector z is defined as</p><formula xml:id="formula_3">p(s y |z) = T t=1 p(w t y |w 0 y , . . . , w t−1 y , z) (1)</formula><p>where w 0 y is defined as a special start-of-the- sentence token. All the words in the sentence are sequentially generated using the RNN, until the end-of-the-sentence symbol is generated. Specif- ically, each conditional p(w t y |w &lt;t y , z), where &lt; t = {0, . . . , t − 1}, is specified as softmax(Vh t ), where h t , the hidden units, are recursively updated through h t = H(y t−1 , h t−1 , z), and h 0 is defined as a zero vector (h 0 is not updated during training). V is a weight matrix used for computing a distribu- tion over words. Bias terms are omitted for simplic- ity throughout the paper. The transition function H(·) is implemented with an LSTM <ref type="bibr">(Hochreiter and Schmidhuber, 1997)</ref>.</p><p>Given the sentence pair (s x , s y ), the objective function is the sum of the log-probabilities of the target sentence conditioned on the encoder repre- sentation in (1):</p><p>T t=1 log p(w t y |w &lt;t y , z). The total objective is the above objective summed over all the sentence pairs. Applications Inspired by <ref type="bibr">Srivastava et al. (2015)</ref>, we propose three models: (i) an autoencoder, (ii) a future predictor, and (iii) the composite model. These models share the same CNN-LSTM model architecture, but are different in terms of the choices of the target sentence. An illustration of the proposed encoder-decoder models is shown in <ref type="figure" target="#fig_0">Figure 1</ref>(left).</p><p>The autoencoder (i) aims to reconstruct the same sentence as the input. The intuition behind this is that an autoencoder learns to represent the data us- ing features that explain its own important factors of variation, and hence model the internal struc- ture of sentences, effectively capturing the intra- sentence information. Another natural task is en- coding an input sentence to predict the subsequent sentence. The future predictor (ii) achieves this, ef- fectively capturing the inter-sentence information, which has been shown to be useful to learn the se- mantics of a sentence ( <ref type="bibr" target="#b6">Kiros et al., 2015</ref>). These two tasks can be combined to create a composite model (iii), where the CNN encoder is asked to learn a feature vector that is useful to simultane- ously reconstruct the input sentence and predict a future sentence. This composite model encour- ages the sentence encoder to incorporate contextual information both within and beyond the sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Hierarchical CNN-LSTM model</head><p>The future predictor described in Section 2.1 only considers the immediately subsequent sentence as context. By utilizing a larger surrounding context, it is likely that we can learn even higher-quality sentence representations. Inspired by the standard RNN-based language model ( <ref type="bibr">Mikolov et al., 2010</ref>) that uses the current word to predict future words, we propose a hierarchical encoder-decoder model that encodes the current sentence to predict mul- tiple future sentences. An illustration of the hier- archical model is shown in <ref type="figure" target="#fig_0">Figure 1</ref>(right), with details provided in <ref type="figure" target="#fig_2">Figure 2</ref>.</p><p>Our proposed hierarchical model characterizes the hierarchy word-sentence-paragraph. A para- graph is modeled as a sequence of sentences, and each sentence is modeled as a sequence of words. Specifically, assume we are given a paragraph D = (s 1 , . . . , s L ), that consists of L sentences. The probability for paragraph D is then defined as</p><formula xml:id="formula_4">p(D) = L =1 p(s |s &lt;&lt; )<label>(2)</label></formula><p>where s 0 is defined as a special start-of-the- paragraph token. As shown in <ref type="figure" target="#fig_2">Figure 2</ref>(left), each p(s |s &lt;&lt; ) in <ref type="formula" target="#formula_4">(2)</ref> is calculated as</p><formula xml:id="formula_5">p(s |s &lt;&lt; ) = p(s |h (p) )<label>(3)</label></formula><formula xml:id="formula_6">h (p) = LSTM p (h (p) −1 , z −1 )<label>(4)</label></formula><formula xml:id="formula_7">z −1 = CNN(s −1 )<label>(5)</label></formula><p>where h . This hidden state h is used to guide the generation of the -th sentence through the decoder (3), which is defined as</p><formula xml:id="formula_8">p(s |h (p) ) = T t=1 p(w ,t |w ,&lt;t , h (p) )<label>(6)</label></formula><p>where w ,0 is defined as a special start-of-the- sentence token. T is the length of sentence , and w ,t denotes the t-th word in sentence . As shown in <ref type="figure" target="#fig_2">Figure 2</ref>(right), each p(w ,t |w ,&lt;t , h</p><p>) in (6) is calculated as</p><formula xml:id="formula_10">p(w ,t |w ,&lt;t , h (p) ) = softmax(Vh (s) ,t )<label>(7)</label></formula><formula xml:id="formula_11">h (s) ,t = LSTM s (h (s) ,t−1 , x ,t−1 , h (p) )<label>(8)</label></formula><p>where h (s)</p><p>,t denotes the t-th hidden state of the LSTM decoder for sentence , x ,t−1 denotes the word embedding for w ,t−1 , and h (s) ,0 is fixed as a zero vector for all = 1, . . . , L. V is a weight matrix used for computing distribution over words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related work</head><p>Various methods have been proposed for sentence modeling, which generally fall into two categories. The first consists of models trained specifically for a certain task, typically combined with downstream applications. Several models have been proposed along this line, ranging from simple additional com- position of the word vectors ( <ref type="bibr">Mitchell and Lapata, 2010;</ref><ref type="bibr" target="#b3">Yu and Dredze, 2015;</ref><ref type="bibr">Iyyer et al., 2015)</ref> to those based on complex nonlinear functions like re- cursive neural networks <ref type="bibr">(Socher et al., 2011</ref><ref type="bibr">(Socher et al., , 2013</ref>, convolutional neural networks ( <ref type="bibr">Kalchbrenner et al., 2014;</ref><ref type="bibr">Hu et al., 2014;</ref><ref type="bibr">Johnson and Zhang, 2015;</ref><ref type="bibr" target="#b4">Zhang et al., 2015;</ref><ref type="bibr">Gan et al., 2017)</ref>, and recurrent neural networks <ref type="bibr">(Tai et al., 2015;</ref><ref type="bibr">Lin et al., 2017</ref>).</p><p>The other category consists of methods aiming to learn generic sentence representations that can be used across domains. This includes the para- graph vector ( <ref type="bibr">Le and Mikolov, 2014</ref>), skip-thought vector ( <ref type="bibr" target="#b6">Kiros et al., 2015)</ref>, and the sequential de- noising autoencoders ( <ref type="bibr">Hill et al., 2016)</ref>. <ref type="bibr">Hill et al. (2016)</ref> </p><note type="other">also proposed a sentence-level log-linear bag-of-words (BoW) model, where a BoW repre- sentation of an input sentence is used to predict ad- jacent sentences that are also represented as BoW. Most recently, Wieting et al. (2016); Arora et al. (2017); Pagliardini et al. (2017) proposed methods in which sentences are represented as a weighted average of fixed (pre-trained) word vectors. Our model falls into this category, and is most related to Kiros et al. (2015).</note><p>However, there are two key aspects that make our model different from <ref type="bibr" target="#b6">Kiros et al. (2015)</ref>. First, we use CNN as the sentence encoder. The combination of CNN and LSTM has been considered in image captioning ( <ref type="bibr">Karpathy and Fei-Fei, 2015)</ref>, and in some recent work on machine translation <ref type="bibr">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr">Meng et al., 2015;</ref><ref type="bibr">Gehring et al., 2016)</ref>. Our utilization of a CNN is different, and more importantly, the ultimate goal of our model is different. Our work aims to use a CNN to learn generic sentence embeddings.</p><p>Second, we use the hierarchical CNN-LSTM model to predict multiple future sentences, rather than the surrounding two sentences as in <ref type="bibr" target="#b6">Kiros et al. (2015)</ref>. Utilizing a larger context window aids our model to learn better sentence representa- tions, capturing longer-term dependencies between sentences. Similar work to this hierarchical lan- guage modeling can be found in <ref type="bibr">Li et al. (2015)</ref>; <ref type="bibr">Sordoni et al. (2015)</ref>; <ref type="bibr">Lin et al. (2015)</ref>; <ref type="bibr">Wang and Cho (2016)</ref></p><note type="other">. Specifically, Li et al. (2015); Sordoni et al. (2015) uses an LSTM for the sentence en- coder, while Lin et al. (2015) uses a bag-of-words to represent sentences.</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We first provide qualitative analysis of our CNN encoder, and then present experimental results on 8 tasks: 5 classification benchmarks, paraphrase de- tection, semantic relatedness and image-sentence ranking. As in <ref type="bibr" target="#b6">Kiros et al. (2015)</ref>, we evaluate the capabilities of our encoder as a generic feature ex- tractor. To further demonstrate the advantage of our learned generic sentence representations, we also fine-tune our trained sentence encoder on the 5 clas- sification benchmarks. All the CNN-LSTM mod- els are trained using the BookCorpus dataset ( <ref type="bibr" target="#b6">Zhu et al., 2015)</ref>, which consists of 70 million sentences from over 7000 books.</p><p>We train four models in total: (i) an autoen- coder, (ii) a future predictor, (iii) the composite model, and (iv) the hierarchical model. For the CNN encoder, we employ filter windows (h) of sizes {3,4,5} with 800 feature maps each, hence each sentence is represented as a 2400-dimensional vector. For both, the LSTM sentence decoder and paragraph generator, we use one hidden layer of 600 units.</p><p>The CNN-LSTM models are trained with a vo- cabulary size of 22,154 words. In order to learn a generic sentence encoder that can encode a large number of possible words, we use two methods of considering words not in the training set. Sup- pose we have a large pretrained word embedding matrix, such as the publicly available word2vec vectors ( <ref type="bibr">Mikolov et al., 2013)</ref>, in which all test words are assumed to reside.</p><p>The first method learns a linear mapping be- tween the word2vec embedding space V <ref type="bibr">w2v</ref> and the learned word embedding space V cnn by solv- ing a linear regression problem ( <ref type="bibr" target="#b6">Kiros et al., 2015</ref>). Thus, any word from V <ref type="bibr">w2v</ref> can be mapped into V cnn for encoding sentences. The second method fixes the word vectors in V cnn as the corresponding word vectors in V <ref type="bibr">w2v</ref> , and we do not update the word embedding parameters during training. Thus, any word vector from V <ref type="bibr">w2v</ref> can be naturally used to encode sentences. By doing this, our trained sentence encoder can successfully encode 931,331 words.</p><p>For training, all weights in the CNN and non- recurrent weights in the LSTM are initialized from a uniform distribution in [-0.01,0.01]. Orthogonal initialization is employed on the recurrent matrices in the LSTM. All bias terms are initialized to zero. The initial forget gate bias for LSTM is set to 3. Gradients are clipped if the norm of the parame- ter vector exceeds 5 ( <ref type="bibr">Sutskever et al., 2014</ref>). The Adam algorithm ( <ref type="bibr">Kingma and Ba, 2015</ref>) with learn- ing rate 2 × 10 −4 is utilized for optimization. For all the CNN-LSTM models, we use mini-batches of size 64. For the hierarchical CNN-LSTM model, we use mini-batches of size 8, and each paragraph is composed of 8 sentences. We do not perform any regularization other than dropout ( <ref type="bibr">Srivastava et al., 2014</ref>). All experiments are implemented A you needed me? this is great. its lovely to see you.</p><p>he had thought he was going crazy. B you got me? this is awesome. its great to meet you. i felt like i was going crazy. C i got you. you are awesome. its great to meet him. i felt like to say the right thing.</p><p>D i needed you. you are great. its lovely to see him. he had thought to say the right thing. <ref type="table">Table 1</ref>: Vector "compositionality" using element-wise addition and subtraction. Let z(s) denote the vector representation z of a given sentence s. We first calculate z =z(A)-z(B)+z(C). The resulting vector is then sent to the LSTM to generate sentence D.</p><p>Query and nearest sentence johnny nodded his curly head , and then his breath eased into an even rhythm . aiden looked at my face for a second , and then his eyes trailed to my extended hand .</p><p>i yelled in frustration , throwing my hands in the air . i stand up , holding my hands in the air .</p><p>i loved sydney , but i was feeling all sorts of homesickness . i loved timmy , but i thought i was a self-sufficient person .</p><p>" i brought sad news to mistress betty , " he said quickly , taking back his hand . " i really appreciate you taking care of lilly for me , " he said sincerely , handing me the money .</p><p>" i am going to tell you a secret , " she said quietly , and he leaned closer . " you are very beautiful , " he said , and he leaned in .</p><p>she kept glancing out the window at every sound , hoping it was jackson coming back . i kept checking the time every few minutes , hoping it would be five oclock . i take tris 's hand and lead her to the other side of the car , so we can watch the city disappear behind us . i take emma 's hand and lead her to the first taxi , everyone else taking the two remaining cars . <ref type="table">Table 2</ref>: Query-retrieval examples. In each case (block of rows), the first sentence is a query, while the second sentence is the retrieved result from a random subset of 1 million sentences from the BookCorpus dataset.</p><p>in Theano ( <ref type="bibr">Bastien et al., 2012</ref>), using a NVIDIA GeForce GTX TITAN X GPU with 12GB memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Qualitative analysis</head><p>We first demonstrate that the sentence representa- tion learned by our model exhibits a structure that makes it possible to perform analogical reasoning using simple vector arithmetics, as illustrated in Ta- ble 1. It demonstrates that the arithmetic operations on the sentence representations correspond to word- level addition and subtractions. For instance, in the 3rd example, our encoder captures that the differ- ence between sentence B and C is "you" and "him", so that the former word in sentence A is replaced by the latter (i.e., "you"-"you"+"him"="him"), resulting in sentence D. <ref type="table">Table 2</ref> shows nearest neighbors of sentences from a CNN-LSTM autoencoder trained on the BookCorpus dataset. Nearest neighbors are scored by cosine similarity from a random sample of 1 million sentences from the BookCorpus dataset. As can be seen, our encoder learns to accurately capture semantic and syntax of the sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Quantitative evaluations</head><p>Classification benchmarks We first study the task of sentence classification on 5 datasets: MR (Pang and <ref type="bibr">Lee, 2005</ref>), CR (Hu and Liu, 2004), SUBJ ( <ref type="bibr">Pang and Lee, 2004</ref>), MPQA ( <ref type="bibr" target="#b0">Wiebe et al., 2005</ref>) and TREC ( <ref type="bibr">Li and Roth, 2002</ref>). On all the datasets, we separately train a logistic regression model on top of the extracted sentence features. We restrict our comparison to methods that also aims to learn generic sentence embeddings for fair compar- ison. We also provide the state-of-the-art results us- ing task-dependent learning methods for reference. Results are summarized in <ref type="table">Table 3</ref>. Our CNN en- coder provides better results than the combine-skip model of <ref type="bibr" target="#b6">Kiros et al. (2015)</ref> on all the 5 datasets.</p><p>We highlight some observations. First, the au- toencoder performs better than the future predic- tor, indicating that the intra-sentence information may be more important for classification than the inter-sentence information. Second, the hierarchi-  <ref type="table">Table 3</ref>: Classification accuracies on several standard benchmarks. The last column shows results on the task of paraphrase detection, where the evaluation metrics are classification accuracy and F1 score. † The first and second block in our results were obtained using the first and second method of considering words not in the training set, respectively. ‡ "combine" means concatenating the feature vectors learned from both the hierarchical model and the composite model. cal model performs better than the future predictor, demonstrating the importance of capturing long- term dependencies across multiple sentences. Our combined model, which concatenates the feature vectors learned from both the hierarchical model and the composite model, performs the best. This may be due to that: (i) both intra-and long-term inter-sentence information are leveraged; (ii) it is easier to linearly separate the feature vectors in higher dimensional spaces. Further, using (fixed) pre-trained word embeddings consistently provides better performance than using the learned word embeddings. This may be due to that word2vec provides more generic word representations, since it is trained on the large Google News dataset (con- taining 100 billion words) ( <ref type="bibr">Mikolov et al., 2013</ref>).</p><p>To further demonstrate the advantage of the learned generic representations, we train a CNN classifier (i.e., a CNN encoder with a logistic regres- sion model on top) with two different initialization strategies: random initialization and initialization with trained parameters from the CNN-LSTM com- posite model. Results are shown in <ref type="figure">Figure 3(</ref>  learning, with the autoencoder on all the data (la- beled and unlabled), and the classifier only on the labeled data.</p><p>Paraphrase detection Now we consider para- phrase detection on the MSRP dataset ( <ref type="bibr">Dolan et al., 2004</ref>). On this task, one needs to predict whether or not two sentences are paraphrases. The training set consists of 4076 sentence pairs, and the test set has 1725 pairs. As in <ref type="bibr">Tai et al. (2015)</ref>, given two sentence representations z x and z y , we first com- pute their element-wise product z x z y and their absolute difference |z x − z y |, and then concate- nate them together. A logistic regression model is trained on top of the concatenated features to predict whether two sentences are paraphrases. We present our results on the last column of <ref type="table">Table 3</ref>. Our best result is better than the other results that use task-independent methods.</p><p>Image-sentence ranking We consider the task of image-sentence ranking, which aims to retrieve items in one modality given a query from the other. We use the COCO dataset ( <ref type="bibr">Lin et al., 2014</ref>), which contains 123,287 images each with 5 captions. For development and testing we use the same splits as <ref type="bibr">Karpathy and Fei-Fei (2015)</ref>. The development and test sets each contain 5000 images. We further split them into 5 random sets of 1000 images, and report the average performance over the 5 splits. Performance is evaluated using Recall@K, which measures the average times a correct item is found within the top-K retrieved results. We also report the median rank of the closest ground truth result   in the ranked list. We represent images using 4096-dimensional feature vectors from VggNet ( <ref type="bibr">Simonyan and Zisserman, 2015)</ref>. Each caption is encoded using our trained CNN encoder. The training objec- tive is the same pairwise ranking loss as used in <ref type="bibr" target="#b6">Kiros et al. (2015)</ref>, which takes the form of max(0, α − f (x n , y n ) + f (x n , y m )), where f (·, ·) is the image-sentence score. (x n , y n ) denotes the related image-sentence pair, and (x n , y m ) is the randomly sampled unrelated image-sentence pair with n = m. For image retrieval from sentences, x denotes the caption, y denotes the image, and vice versa. The objective is to force the matching score of the related pair (x n , y n ) to be greater than the unrelated pair (x n , y m ) by a margin α, which is set to 0.1 in our experiments. <ref type="table" target="#tab_3">Table 4</ref> shows our results. Consistent with pre- vious experiments, we empirically found that the encoder trained using the fixed word embedding performed better on this task, hence only results using this method are reported. As can be seen, we obtain the same median rank as in <ref type="bibr" target="#b6">Kiros et al. (2015)</ref>, indicating that our encoder is as competi- tive as the skip-thought vectors ( <ref type="bibr" target="#b6">Kiros et al., 2015)</ref>. The performance gain between our encoder and the combine-skip model of <ref type="bibr" target="#b6">Kiros et al. (2015)</ref> on the R@1 score is significant, which shows that the CNN encoder has more discriminative power on re-trieving the most correct item than the skip-thought vector.</p><p>Semantic relatedness For our final experiment, we consider the task of semantic relatedness on the SICK dataset ( <ref type="bibr">Marelli et al., 2014</ref>), consisting of 9927 sentence pairs. Given two sentences, our goal is to produce a real-valued score between <ref type="bibr">[1,</ref><ref type="bibr">5]</ref> to indicate how semantically related two sentences are, based on human generated scores. We compute a feature vector representing the pair of sentences in the same way as on the MSRP dataset. We follow the method in <ref type="bibr">Tai et al. (2015)</ref>, and use the cross- entropy loss for training. Results are summarized in <ref type="table">Table 5</ref>. Our result is better than the combine- skip model of <ref type="bibr" target="#b6">Kiros et al. (2015)</ref>. This suggests that CNN also provides competitive performance at matching human relatedness judgements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We presented a new class of CNN-LSTM encoder- decoder models to learn sentence representations from unlabeled text. Our trained convolutional encoder is highly generic, and can be an alternative to the skip-thought vectors of <ref type="bibr" target="#b6">Kiros et al. (2015)</ref>. Compelling experimental results on several tasks demonstrated the advantages of our approach. In future work, we aim to use more advanced CNN architectures ( <ref type="bibr">Conneau et al., 2016</ref>) for learning generic sentence embeddings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of the CNN-LSTM encoder-decoder models. The sentence encoder is a CNN, the sentence decoder is an LSTM, and the paragraph generator is another LSTM. (Left) (a)+(c) represents the autoencoder; (b)+(c) represents the future predictor; (a)+(b)+(c) represents the composite model. (Right) hierarchical model. In this example, the input contiguous sentences are: this is great. you will love it! i promise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>denotes the -th hidden state of the LSTM paragraph generator, and h (p) 0 is fixed as a zero vector. The CNN in (5) is as described in Section 2.1, encoding the sentence s −1 into a vector representation z −1 . Equation (4) serves as the paragraph-level lan- guage model (Mikolov et al., 2010), which encodes all the previous sentence representations z &lt;&lt; into a vector representation h (p)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Detailed illustration of the hierarchical CNN-LSTM model. (Left) LSTM paragraph generator. (Right) LSTM sentence decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>leaning forward , he rested his elbows on his knees and let his hands dangle between his legs . stepping forward , i slid my arms around his neck and then pressed my body flush against his .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>5 :</head><label>5</label><figDesc>Results on the SICK semantic relatedness task. The evaluation metrics are Pearson's r, Spear- man's ρ and mean squared error (MSE). ( †) taken from Kiros et al. (2015). ( ‡) taken from Tai et al. (2015).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Results for image-sentence ranking ex-
periments on the COCO dataset. R@K denotes 
Recall@K (higher is better) and Med r is the me-
dian rank (lower is better). ( †) taken from Kiros 
et al. (2015). ( * ) taken from Karpathy and Fei-Fei 
(2015). ( ‡) taken from Mao et al. (2015). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported by ARO, DARPA, DOE, NGA, ONR and NSF. Natural language processing (almost) from scratch. In JMLR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head></div>
			</div>

			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Annotating expressions of opinions and emotions in language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Language resources and evaluation</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards universal paraphrastic sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Convolutional neural network for paraphrase identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning composition models for phrase embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>TACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Poupart</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.05070</idno>
		<title level="m">Self-adaptive hierarchical sentence model</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
