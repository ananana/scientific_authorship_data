<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Combining Generative and Discriminative Approaches to Unsupervised Dependency Parsing via Dual Decomposition *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>September 7-11, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjuan</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Combining Generative and Discriminative Approaches to Unsupervised Dependency Parsing via Dual Decomposition *</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1689" to="1694"/>
							<date type="published">September 7-11, 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Unsupervised dependency parsing aims to learn a dependency parser from unanno-tated sentences. Existing work focus-es on either learning generative models using the expectation-maximization algorithm and its variants, or learning dis-criminative models using the discrimina-tive clustering algorithm. In this paper, we propose a new learning strategy that learns a generative model and a discriminative model jointly based on the dual decomposition method. Our method is simple and general, yet effective to capture the advantages of both models and improve their learning results. We tested our method on the UD treebank and achieved a state-of-the-art performance on thirty languages.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Dependency parsing is an important task in nat- ural language processing. It identifies dependen- cies between words in a sentence, which have been shown to benefit other tasks such as semantic role labeling ( <ref type="bibr">Lei et al., 2015)</ref> and sentence classifica- tion ( <ref type="bibr" target="#b2">Ma et al., 2015)</ref>. Supervised learning of a dependency parser requires annotation of a train- ing corpus by linguistic experts, which can be time and resource consuming. Unsupervised dependen- cy parsing eliminates the need for dependency an- notation by directly learning from unparsed text.</p><p>Previous work on unsupervised dependency parsing mainly focuses on learning generative models, such as the dependency model with va- lence (DMV) ( <ref type="bibr">Klein and Manning, 2004</ref>) and combinatory categorial grammars (CCG) <ref type="bibr">(Bisk and Hockenmaier, 2012</ref>). Generative models have * This work was supported by the National Natural Sci- ence Foundation of China (61503248). many advantages. For example, the learning ob- jective function can be defined as the marginal likelihood of the training data, which is typical- ly easy to compute in a generative model. In addition, many types of inductive bias, such as those favoring short dependency arcs <ref type="bibr" target="#b7">(Smith and Eisner, 2006</ref>), encouraging correlations between POS tags <ref type="bibr">(Cohen et al., 2008;</ref><ref type="bibr">Cohen and Smith, 2009;</ref><ref type="bibr">Berg-Kirkpatrick et al., 2010;</ref><ref type="bibr">Jiang et al., 2016)</ref>, and limiting center embedding ( <ref type="bibr" target="#b4">Noji et al., 2016)</ref>, can be incorporated into generative models to achieve better parsing accuracy. However, due to the strong independence assumption in most generative models, it is difficult for these models to utilize context information that has been shown to benefit supervised parsing.</p><p>Recently, a feature-rich discriminative model for unsupervised parsing is proposed that captures the global context information of sentences ( <ref type="bibr">Grave and Elhadad, 2015)</ref>. Inspired by discriminative clustering, learning of the model is formulated as convex optimization of both the model parameters and the parses of training sentences. By utilizing language-independent rules between pairs of POS tags to guide learning, the model achieves state-of- the-art performance on the UD treebank dataset.</p><p>In this paper we propose to jointly train two state-of-the-art models of unsupervised dependen- cy parsing: a generative model called LC-DMV ( <ref type="bibr" target="#b4">Noji et al., 2016</ref>) and a discriminative model called Convex-MST ( <ref type="bibr">Grave and Elhadad, 2015)</ref>. We employ a learning algorithm based on the dual decomposition ( <ref type="bibr">Dantzig and Wolfe, 1960</ref>) infer- ence algorithm, which encourages the two models to influence each other during training.</p><p>We evaluated our method on thirty languages and found that the jointly trained models surpass their separately trained counterparts in parsing ac- curacy. Further analysis shows that the two models positively influence each other during joint train-ing by implicitly sharing the inductive bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">DMV</head><p>The dependency model with valence (DMV) <ref type="bibr">(Klein and Manning, 2004</ref>) is the first generative model that outperforms the left-branching base- line in unsupervised dependency parsing. In D- MV, a sentence is generated by recursively apply- ing three types of grammar rules to construct a parse tree from the top down. The probability of the generated sentence and parse tree is the prob- ability product of all the rules used in the genera- tion process. To learn the parameters (rule prob- abilities) of DMV, the expectation maximization algorithm is often used. <ref type="bibr" target="#b4">Noji et al. (2016)</ref> ex- ploited two universal syntactic biases in learning DMV: restricting the center-embedding depth and encouraging short dependencies. They achieved a comparable performance with state-of-the-art ap- proaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Convex-MST</head><p>Convex-MST ( <ref type="bibr">Grave and Elhadad, 2015</ref>) is a dis- criminative model for unsupervised dependency parsing based on the first-order maximum span- ning tree dependency parser <ref type="bibr" target="#b3">(McDonald et al., 2005</ref>). Given a sentence, whether each possible dependency exists or not is predicted based on a set of handcrafted features and a valid parse tree closest to the prediction is identified by the mini- mum spanning tree algorithm.</p><p>For each sentence x, a first-order dependency graph is built over the words of the sentence. The weight of each edge is calculated by w T f (x, i, j), where w is the parameters and f (x, i, j) is the handcrafted feature vector of the dependency from the i-th word to the j-th word in sentence x. For sentence x of length n, we can represent it as ma- trix X where each raw is a feature vector. The parse tree y is a spanning tree of the graph and can be represented as a binary vector with length n×n where each element is 1 if the corresponding arc is in the tree and 0 otherwise.</p><p>Learning is based on discriminative clustering with the following objective function:</p><formula xml:id="formula_0">1 N N α=1 1 2n α ||y α − X α w|| 2 2 − µv T y α + λ 2 ||w|| 2 2</formula><p>where X α is a matrix where each row is a feature representation f (x α , i, j) of an edge in the depen- dency graph of sentence x α , v represents whether each dependency arc in y α satisfies a set of pre- specified linguistic rules, and λ and µ are hyper- parameters. The Frank-Wolfe algorithm is em- ployed to optimize the objective function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Dual Decomposition</head><p>Dual decomposition <ref type="bibr">(Dantzig and Wolfe, 1960)</ref>, a special case of Lagrangian relaxation, is an opti- mization method that decomposes a hard problem into several small sub-problems. It has been wide- ly used in machine learning ( <ref type="bibr">Komodakis et al., 2007)</ref> and natural language processing ( <ref type="bibr">Koo et al., 2010;</ref><ref type="bibr" target="#b5">Rush and Collins, 2012)</ref>. <ref type="bibr">Komodakis et al. (2007)</ref> proposed using dual decomposition to do MAP inference for Markov random fields. <ref type="bibr">Koo et al. (2010)</ref> proposed a new dependency parser based on dual decomposition by combining a graph based dependency model and a non-projective head automata. In the work of <ref type="bibr" target="#b6">Rush et al. (2010)</ref>, they showed that dual de- composition can effectively integrate two lexical- ized parsing models or two correlated tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Agreement based Learning</head><p>Liang et al. <ref type="formula">(2008)</ref> proposed agreement based learning that trains several tractable generative models jointly and encourages them to agree on certain latent variables. To effectively train the system, a product EM algorithm was used. They showed that the joint model can perform better than each independent model on the accuracy or convergence speed. They also showed that the ob- jective function of the work of <ref type="bibr">Klein and Manning (2004)</ref> is a special case of the product EM algo- rithm for grammar induction. Our approach has a similar motivation to agreement based learning but has two important advantages. First, while their approach only combines generative models, our approach can make use of both generative and dis- criminative models. Second, while their approach requires the sub-models to share the same dynam- ic programming structure when performing decod- ing, our approach does not have such restriction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Joint Training</head><p>We minimize the following objective function that combines two different models of unsupervised dependency parsing:</p><formula xml:id="formula_1">J(M F , M G ) = N α=1 min yα∈Yα (F (x α , y α ; M F ) + G(x α , y α ; M G ))</formula><p>where N is the size of training data, M F and M G are the parameters of the first and second model respectively, F and G are their respective learn- ing objectives, and Y α is the set of valid depen- dency parses of sentence x α . While in princi- ple this objective can be used to combine many different types of models, here we consider two state-of-the-art models of unsupervised dependen- cy parsing, a generative model LC-DMV ( <ref type="bibr" target="#b4">Noji et al., 2016</ref>) and a discriminative model Convex- MST ( <ref type="bibr">Grave and Elhadad, 2015)</ref>. We denote the parameters of LC-DMV by Θ and the parameters of Convex-MST by w. Their respective objective functions are,</p><formula xml:id="formula_2">F (x α , y α ; Θ) = − log (P Θ (x α , y α )f (x α , y α )) G(x α , y α ; w) = 1 2n α ||y α − X α w|| 2 2 + λ 2N ||w|| 2 2 − µv T y</formula><p>where P Θ (x α , y α ) is the joint probability of sen- tence x α and parse y α , f is a constraint factor, and the notations in the second objective function are explained in section 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Learning</head><p>We use coordinate descent to optimize the param- eters of the two models. In each iteration, we first fix the parameters and find the best dependency parses of the training sentences (see section 3.2); we then fix the parses and optimize the parameter- s. The detailed algorithm is shown in Algorithm 1.</p><p>Pretraining of the two models is done by run- ning their original learning algorithms separate- ly. When the parses of the training sentences are fixed, it is easy to show that the parameters of the two models can be optimized separately. Updat- ing the parameters Θ of LC-DMV can be done by simply counting the number of times each rule is used in the parse trees and then normalizing the counts to get the maximum-likelihood probabili- ties. The parameters w of Convex-MST can be updated by stochastic gradient descent. After up- dating Θ and w at each iteration, we additional- ly train each model separately for three iterations, which we find further improves learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Parameter Learning</head><p>Input: Training sentence x 1 , x 2 , ..., x N Pre-train Θ and w repeat</p><p>Fix Θ and w and solve the decoding problem to get y α , α = 1, 2, . . . ,</p><note type="other">N Fix the parses and update Θ and w until Convergence Algorithm 2 Decoding via Dual Decomposition Input: Sentence x, fixed parameters w and Θ Initialize vector u of size n × n to 0 repeatˆy repeatˆ repeatˆy</note><formula xml:id="formula_3">= arg min y∈Y F (x, y; Θ) + u T y ˆ z = arg min z∈Y G(x, z; w) − u T z ifˆyifˆ ifˆy = ˆ z then returnˆyreturnˆ returnˆy else u = u − τ (ˆ y − ˆ z) end if until Convergence</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Joint Decoding</head><p>Given a training sample x and parameters w, Θ, the goal of decoding is to find the best parse tree:</p><formula xml:id="formula_4">ˆ y = arg min y∈Y 1 2n ||y−Xw|| 2 2 −µv T y−log P Θ (x, y)</formula><p>We employ the dual decomposition algorithm to solve this problem (shown in Algorithm 2), where τ represents the step size. The most important part of the algorithm is solving the two separate decoding problems:</p><formula xml:id="formula_5">ˆ y = arg min y∈Y − log(P Θ (x, y)f (x, y)) + u T y ˆ z = arg min z∈Y 1 2n ||z − Xw|| 2 2 − µv T z − u T z</formula><p>The first decoding problem can be solved by a modified CYK parsing algorithm that takes into account the information in vector u. The second decoding problem can be solved using the same al- gorithm of Grave and Elhadad (2015) (we use the projective version in our approach).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>We use UD Treebank 1.4 as our datasets. We sort- ed the datasets in the treebank by the number of training sentences of length ≤ 15 and selected the top thirty datasets, which is similar to the setup of <ref type="bibr" target="#b4">Noji et al. (2016)</ref>. For each dataset, we trained our method on the training data with length ≤ 15 and tested our method on the testing data with length ≤ 40. We tuned the hyper-parameters of our method on the dataset of the English language and re- ported the results on the thirty datasets without any further parameter tuning. We compared our method with four baselines. The first two base- lines are Convex-MST and LC-DMV that are in- dependently trained. To construct the third base- line, we used the independently trained Convex- MST baseline to parse all the training sentences and then used the parses to initialize the training of LC-DMV. This can be seen as a simple method to combine two different approaches. On the other hand, we did not use the LC-DMV baseline to ini- tialize Convex-MST training because the objective function of Convex-MST is convex and therefore the initialization does not matter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>In <ref type="table">Table 1</ref>, we compare our jointly trained models with the four baselines. We can see that with joint training and independent decoding, LC-DMV and Convex-MST can achieve superior overall perfor- mance than when they are separately trained with or without mutual initialization. Joint decoding with our jointly trained models performs worse than independent decoding. We made the same observation when applying joint decoding to the separately trained models (not shown in the table). We believe this is because unsupervised parsers have relatively low accuracy and forcing them to reconcile would not lead to better parses. On the other hand, joint decoding during training helps propagate useful inductive biases between models and thus leads to better trained models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis of Parsing Results</head><p>We analyze the parsing results from the two mod- els to see how they benefit each other with join- t training. Note that LC-DMV limits the depth of center embedding and encourages shorter de- pendency length, while Convex-MST encourages dependencies satisfying pre-specified linguistic rules. Therefore, we would like to see whether the jointly-trained LC-DMV produces more de- pendencies satisfying the linguistic priors than it- s separately-trained counterpart, and whether the jointly-trained Convex-MST produces parse trees </p><formula xml:id="formula_6">Language M D D-I M-J D-J DD A</formula><note type="other">Dutch-LS 42.4 27.0 16.4 43.2 41.2 36.3 English 54.0 56.0 49.8 57.3 60.1 53.4 Estonian 49.4 31.8 47.5 48.7 44.0 44.4 Finnish 44.7 26.9 39.0 44.2 43.5 31.2 Finnish-FTB 49.9 31.0 47.9 47.7 48.0 36.5 French 62.0 48.6 57.0 54.5 57.0 55.5 German 51.4 50.5 54.1 49.3 55.7 48.6 Gothic 52.7 49.9 47.3 59.6 56.4 58.0 Hindi 56.8 54.2 48.4 52.1 60.0 49.1 Italian 69.1 71.1 67.4 62.8 70.3 64.5 Japanese 44.8 43.8 43.8 42.8 45.8 41.0 Latin-ITTB 38.8 38.6 42.3 47.0 42.2 40.3 Latin-PROIEL 44.3 34.8 38.7 46.8 41.8 42.9 Norwegian 55.3 45.5 51.4 57.4 60.8 46.6 Old Church S</note><p>56.4 26.6 51.3 58.3 58.6 42.0 Polish 63.4 63.7 61.5 70.7 74.2 68.9 Portuguese 57.9 67.2 60.1 56.1 62.9 57.4 Portuguese-BR 59.3 63.1 62.0 65.5 68.8 58.3 Russian-STR 47.6 51.7 56.5 52.1 64.4 52.6 Slovak 57.4 59.3 51.9 61.7 65.9 58.7 Slovenian 54.0 49.5 56.3 65.5 69.6 56.1 Spanish 61.9 61.9 60.3 57.4 68.0 60.2 Spanish-AC 59.4 59.5 56.4 56.8 65.2 57.6 Average 52.7 47.2 50.3 54.2 56.5 49.6 Average ≤ 15 55.4 48.9 54.9 57.3 60.2 53.8 <ref type="table">Table 1</ref>: Directed dependency accuracy on thirty datasets with test sentences of length ≤ 40. The last row indicates the average directed accuracy on sentences of length ≤ 15. M (Convex-MST) and D (LC-DMV) are the independently trained baselines. D-I is the third baseline in which the LC- DMV training is initialized by the parses produced from the trained Convex-MST model. With our jointly trained models, M-J and D-J denote separate decoding and DD denotes joint decoding.</p><p>with less center embedding and shorter dependen- cies than its separately-trained counterpart. <ref type="figure">Figure 1</ref> shows the percentages of dependencies satisfying linguistic rules when using the separate- ly and jointly trained LC-DMV to parse the test sentences in the English dataset. As we can see, with joint training, LC-DMV is indeed influenced by Convex-MST and produces more dependencies satisfying linguistic rules. <ref type="table" target="#tab_2">Table 2</ref> shows the average dependency length when using the separately and jointly trained Convex-MST to parse the English test dataset. The dependency length can be seen to decrease with joint training, showing the influence from LC- DMV. As to center embedding depth, we find that separately trained Convext-MST already produces very few center embeddings of depth 2 or more,  so the influence from the center embedding con- straint of LC-DMV during joint training is not ob- vious. We note that the influence on Convex-MST from LC-DMV during joint training is relatively small, which may contribute to the much small- er accuracy improvement (1.5%) of Convex-MST with joint training in comparison with the 9.3% improvement of LC-DMV. We conducted an ad- ditional experiment that scaled down the Convex- MST objective in joint training in order to in- crease the influence of LC-DMV. The results show that LC-DMV indeed influences Convex-MST to a greater degree, but the parsing accuracies of the two models decrease.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proposed a new learning strategy for unsupervised dependency parsing that learns a generative model and a discriminative model joint- ly based on dual decomposition. We show that with joint training, two state-of-the-art models can positively influence each other and achieve better performance than their separately trained counter- parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head><p>Taylor </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Average dependency length in the Convex-MST 
parses of the English test dataset. 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">American</forename><surname>Chapter</surname></persName>
		</author>
		<title level="m">the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="1150" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Agreement-based learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Percy S Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>J. C. Platt, D. Koller, Y. Singer, and S. T. Roweis</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="913" to="920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingbo</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.01839</idno>
		<title level="m">Dependency-based convolutional neural networks for sentence embedding</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Online large-margin training of dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd annual meeting on association for computational linguistics</title>
		<meeting>the 43rd annual meeting on association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="91" to="98" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Using left-corner parsing to encode universal structural constraints in grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Noji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="33" to="43" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A tutorial on dual decomposition and lagrangian relaxation for inference in natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Int. Res</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="305" to="362" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On dual decomposition and linear programming relaxations for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Alexander M Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Annealing structural bias in multilingual weighted grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="569" to="576" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
