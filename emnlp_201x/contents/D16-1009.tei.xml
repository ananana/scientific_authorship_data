<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:34+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Modeling Human Reading with Neural Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Hahn</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Language, Cognition and Computation School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton Street</addrLine>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Keller</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Language, Cognition and Computation School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton Street</addrLine>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Modeling Human Reading with Neural Attention</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="85" to="95"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>When humans read text, they fixate some words and skip others. However, there have been few attempts to explain skipping behavior with computational models, as most existing work has focused on predicting reading times (e.g., using surprisal). In this paper , we propose a novel approach that models both skipping and reading, using an unsuper-vised architecture that combines a neural attention with autoencoding, trained on raw text using reinforcement learning. Our model explains human reading behavior as a tradeoff between precision of language understanding (encoding the input accurately) and economy of attention (fixating as few words as possible). We evaluate the model on the Dundee eye-tracking corpus, showing that it accurately predicts skipping behavior and reading times, is competitive with surprisal, and captures known qualitative features of human reading.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Humans read text by making a sequence of fixations and saccades. During a fixation, the eyes land on a word and remain fairly static for 200-250 ms. Sac- cades are the rapid jumps that occur between fixa- tions, typically lasting 20-40 ms and spanning 7- 9 characters <ref type="bibr" target="#b25">(Rayner, 1998)</ref>. Readers, however, do not simply fixate one word after another; some sac- cades go in reverse direction, and some words are fixated more than once or skipped altogether.</p><p>A range of computational models have been de- veloped to account for human eye-movements in reading <ref type="bibr" target="#b27">(Rayner and Reichle, 2010)</ref>, including mod- els of saccade generation in cognitive psychology, such as EZ-Reader ( <ref type="bibr" target="#b28">Reichle et al., 1998</ref><ref type="bibr" target="#b30">Reichle et al., , 2003</ref><ref type="bibr" target="#b29">Reichle et al., , 2009</ref>), SWIFT ( <ref type="bibr" target="#b7">Engbert et al., 2002</ref><ref type="bibr" target="#b8">Engbert et al., , 2005</ref>), or the Bayesian Model of <ref type="bibr" target="#b4">Bicknell and Levy (2010)</ref>. More recent approaches use machine learning mod- els trained on eye-tracking data to predict human reading patterns <ref type="bibr">Nivre, 2009, 2010;</ref><ref type="bibr" target="#b11">Hara et al., 2012;</ref><ref type="bibr" target="#b17">Matthies and Søgaard, 2013)</ref>. Both types of models involve theoretical assump- tions about human eye-movements, or at least re- quire the selection of relevant eye-movement fea- tures. Model parameters have to be estimated in a supervised way from eye-tracking corpora.</p><p>Unsupervised approaches, that do not involve training the model on eye-tracking data, have also been proposed. A key example is surprisal, which measures the predictability of a word in context, de- fined as the negative logarithm of the conditional probability of the current word given the preced- ing words <ref type="bibr" target="#b10">(Hale, 2001;</ref><ref type="bibr" target="#b16">Levy, 2008)</ref>. Surprisal is computed by a language model, which can take the form of a probabilistic grammar, an n-gram model, or a recurrent neural network. While surprisal has been shown to correlate with word-by-word reading times <ref type="bibr">(McDonald and Shillcock, 2003a,b;</ref><ref type="bibr" target="#b6">Demberg and Keller, 2008;</ref><ref type="bibr" target="#b9">Frank and Bod, 2011;</ref><ref type="bibr" target="#b31">Smith and Levy, 2013)</ref>, it cannot explain other aspects of hu- man reading, such as reverse saccades, re-fixations, or skipping. Skipping is a particularly intriguing phenomenon: about 40% of all words are skipped (in the Dundee corpus, see below), without apparent detriment to text understanding.</p><p>In this paper, we propose a novel model architec- ture that is able to explain which words are skipped and which ones are fixated, while also predicting reading times for fixated words. Our approach is completely unsupervised and requires only unla- beled text for training.</p><p>Compared to language as a whole, reading is a recent innovation in evolutionary terms, and peo- ple learning to read do not have access to compe- tent readers' eye-movement patterns as training data. This suggests that human eye-movement patterns emerge from general principles of language pro- cessing that are independent of reading. Our start- ing point is the Tradeoff Hypothesis: Human read- ing optimizes a tradeoff between precision of lan- guage understanding (encoding the input accurately) and economy of attention (fixating as few words as possible). Based on the Tradeoff Hypothesis, we ex- pect that humans only fixate words to the extent nec- essary for language understanding, while skipping words whose contribution to the overall meaning can be inferred from context.</p><p>In order to test these assumptions, this paper in- vestigates the following questions:</p><p>1. Can the Tradeoff Hypothesis be implemented in an unsupervised model that predicts skipping and reading times in quantitative terms? In par- ticular, can we compute surprisal based only on the words that are actually fixated?</p><p>2. Can the Tradeoff Hypothesis explain known qualitative features of human fixation patterns? These include dependence on word frequency, word length, predictability in context, a con- trast between content and function words, and the statistical dependence of the current fixa- tion on previous fixations.</p><p>To investigate these questions, we develop a generic architecture that combines neural language model- ing with recent ideas on integrating recurrent neural networks with mechanisms of attention, which have shown promise both in NLP and in computer vision. We train our model end-to-end on a large text cor- pus to optimize a tradeoff between minimizing input reconstruction error and minimizing the number of words fixated. We evaluate the model's reading be- havior against a corpus of human eye-tracking data. Apart from the unlabeled training corpus and the generic architecture, no further assumptions about language structure are made -in particular, no lex- icon or grammar or otherwise labeled data is re- quired. Our unsupervised model is able to predict human skips and fixations with an accuracy of 63.7%. This compares to a baseline of 52.6% and a supervised accuracy of 69.9%. For fixated words, the model significantly predicts human reading times in a lin- ear mixed effects analysis. The performance of our model is comparable to surprisal, even though it only fixates 60.4% of all input words. Furthermore, we show that known qualitative features of human fix- ation sequences emerge in our model without addi- tional assumptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>A range of attention-based neural network architec- tures have recently been proposed in the literature, showing promise in both NLP and computer vision (e.g., <ref type="bibr" target="#b21">Mnih et al., 2014;</ref><ref type="bibr" target="#b1">Bahdanau et al., 2015)</ref>. Such architectures incorporate a mechanism that allows the network to dynamically focus on a restricted part of the input. Attention is also a central concept in cognitive science, where it denotes the focus of cog- nitive processing. In both language processing and visual processing, attention is known to be limited to a restricted area of the visual field, and shifts rapidly through eye-movements <ref type="bibr" target="#b12">(Henderson, 2003)</ref>.</p><p>Attention-based neural architectures either em- ploy soft attention or hard attention. Soft attention distributes real-valued attention values over the in- put, making end-to-end training with gradient de- scent possible. Hard attention mechanisms make discrete choices about which parts of the input to focus on, and can be trained with reinforcement learning ( <ref type="bibr" target="#b21">Mnih et al., 2014</ref>). In NLP, soft atten- tion can mitigate the difficulty of compressing long sequences into fixed-dimensional vectors, with ap- plications in machine translation ( <ref type="bibr" target="#b1">Bahdanau et al., 2015</ref>) and question answering ( <ref type="bibr" target="#b13">Hermann et al., 2015)</ref>. In computer vision, both types of attention can be used for selecting regions in an image ( <ref type="bibr" target="#b0">Ba et al., 2015;</ref><ref type="bibr" target="#b35">Xu et al., 2015</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The NEAT Reading Model</head><p>The point of departure for our model is the Trade- off Hypothesis (see Section 1): Reading optimizes a tradeoff between precision of language understand-ing and economy of attention. We make this idea ex- plicit by proposing NEAT (NEural Attention Trade- off), a model that reads text and attempts to re- construct it afterwards. While reading, the network chooses which words to process and which words to skip. The Tradeoff Hypothesis is formalized us- ing a training objective that combines accuracy of reconstruction with economy of attention, encourag- ing the network to only look at words to the extent that is necessary for reconstructing the sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Architecture</head><p>We use a neural sequence-to-sequence architecture <ref type="bibr" target="#b32">(Sutskever et al., 2014</ref>) with a hard attention mech- anism. We illustrate the model in <ref type="figure" target="#fig_1">Figure 1</ref>, oper- ating on a three-word sequence w w w. The most basic components are the reader, labeled R, and the de- coder. Both of them are recurrent neural networks with Long Short-Term Memory (LSTM, Hochreiter and Schmidhuber, 1997) units. The recurrent reader network is expanded into time steps R 0 , . . . , R 3 in the figure. It goes over the input sequence, reading one word w i at a time, and converts the word se- quence into a sequence of vectors h 0 , . . . , h 3 . Each vector h i acts as a fixed-dimensionality encoding of the word sequence w 1 , . . . , w i that has been read so far. The last vector h 3 (more generally h N for sequence length N), which encodes the entire in- put sequence, is then fed into the input layer of the decoder network, which attempts to reconstruct the input sequence w w w. It is also realized as a recur- rent neural network, collapsed into a single box in the figure. It models a probability distribution over word sequences, outputting a probability distribu- tion P Decoder (w i |w w w 1,...,i−1 , h N ) over the vocabulary in the i-th step, as is common in neural language mod- eling ( <ref type="bibr" target="#b20">Mikolov et al., 2010)</ref>. As the decoder has access to the vector representation created by the reader network, it ideally is able to assign the high- est probability to the word sequence w w w that was ac- tually read. Up to this point, the model is a stan- dard sequence-to-sequence architecture reconstruct- ing the input sequence, that is, performing autoen- coding.</p><p>As a basic model of human processing, NEAT contains two further components. First, experimen- tal evidence shows that during reading, humans con- stantly make predictions about the upcoming input (e.g., <ref type="bibr" target="#b33">Van Gompel and Pickering, 2007)</ref>. As a model of this behavior, the reader network at each time step outputs a probability distribution P R over the lex- icon. This distribution describes which words are likely to come next (i.e., the reader network per- forms language modeling). Unlike the modeling per- formed by the decoder, P R , via its recurrent connec- tions, has access to the previous context only.</p><p>Second, we model skipping by stipulating that only some of the input words w i are fed into the reader network R, while R receives a special vec- tor representation, containing no information about the input word, in other cases. These are the words that are skipped. In NEAT, at each time step dur- ing reading, the attention module A decides whether the next word is shown to the reader network or not. When humans skip a word, they are able to identify it using parafoveal preview <ref type="bibr" target="#b26">(Rayner, 2009)</ref>. Thus, we can assume that the choice of which words to skip takes into account not only the prior context but also a preview of the word itself. We therefore allow the attention module to take the input word into account when making its decision. In addition, the attention module has access to the previous state h i−1 of the reader network, which summarizes what has been read so far. To allow for interaction be- tween skipping and prediction, we also give the at- tention module access to the probability of the in- put word according to the prediction P R made at the last time step. If we write the decision made by A as ω i ∈ {0, 1}, where ω i = 1 means that word w i is shown to the reader and 0 means that it is not, we can write the probability of showing word w i as:</p><formula xml:id="formula_0">P(ω i = 1|ω ω ω 1...i−1 , w w w) = P A (w i , h i−1 , P R (w i |w w w 1...i−1 , ω ω ω 1...i−1 ))<label>(1)</label></formula><p>We implement A as a feed-forward network, fol- lowed by taking a binary sample ω i . We obtain the surprisal of an input word by taking the negative logarithm of the conditional probability of this word given the context words that precede it:</p><formula xml:id="formula_1">Surp(w i |w w w 1...i−1 ) = − log P R (w i |w w w 1...i−1 , ω ω ω 1...i−1 )<label>(2)</label></formula><p>As a consequence of skipping, not all input words are accessible to the reader network. Therefore, the probability and surprisal estimates it computes cru- cially only take into account the words that have ac- tually been fixated. We will refer to this quantity as the restricted surprisal, as opposed to full surprisal, which is computed based on all prior context words.</p><p>The key quantities for predicting human reading are the fixation probabilities in equation <ref type="formula" target="#formula_0">(1)</ref>, which model fixations and skips, and restricted surprisal in equation <ref type="formula" target="#formula_1">(2)</ref>, which models the reading times of the words that are fixated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Objective</head><p>Given network parameters θ and a sequence w w w of words, the network stochastically chooses a sequence ω ω ω according to (1) and incurs a loss L(ω ω ω|w w w, θ) for language modeling and reconstruc- tion:</p><formula xml:id="formula_2">L(ω ω ω|w w w, θ) = − ∑ i log P R (w i |w w w 1,...,i−1 , ω ω ω 1,...,i−1 ; θ) − ∑ i log P Decoder (w i |w w w 1,...,i−1 ; h N ; θ)<label>(3)</label></formula><p>where P R (w i , . . . ) denotes the output of the reader af- ter reading w i−1 , and P Decoder (w i | . . . ; h N ) is the out- put of the decoder at time i − 1, with h N being the vector representation created by the reader network for the entire input sequence.</p><p>To implement the Tradeoff Hypothesis, we train NEAT to solve language modeling and reconstruc- tion with minimal attention, i.e., the network mini- mizes the expected loss:</p><formula xml:id="formula_3">Q(θ) := E w w w,ω ω ω [L(ω ω ω|w w w, θ) + α · ω ω ω 1 ]<label>(4)</label></formula><p>where word sequences w w w are drawn from a corpus, and ω ω ω is distributed according to P(ω ω ω|w w w, θ) as de- fined in (1). In (4), ω ω ω 1 is the number of words shown to the reader, and α &gt; 0 is a hyperparameter. The term α · ω ω ω 1 encourages NEAT to attend to as few words as possible.</p><p>Note that we make no assumption about linguis- tic structure -the only ingredients of NEAT are the neural architecture, the objective (4), and the corpus from which the sequences w w w are drawn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training</head><p>We follow previous approaches to hard attention in using a combination of gradient descent and rein- forcement learning, and separate the training of the recurrent networks from the training of A. To train the reader R and the decoder, we temporarily re- move the attention network A, set ω ω ω ∼ Binom(n, p) (n sequence length, p a hyperparameter), and mini- mize E[L(w w w|θ, ω ω ω)] using stochastic gradient descent, sampling a sequence ω ω ω for each input sequence. In effect, NEAT is trained to perform reconstruction and language modeling when there is noise in the input. After R and the decoder have been trained, we fix their parameters and train A using the RE- INFORCE rule <ref type="bibr" target="#b34">(Williams, 1992)</ref>, which performs stochastic gradient descent using the estimate</p><formula xml:id="formula_4">1 |B| ∑ w w w∈B;ω ω ω (L(ω ω ω|w w w, θ) + α · ω ω ω 1 ) ∂ θ A (log P(ω ω ω|w w w, θ)) (5) for the gradient ∂ θ A Q.</formula><p>Here, B is a minibatch, ω ω ω is sampled from P(ω ω ω|w w w, θ), and θ A ⊂ θ is the set of parameters of A. For reducing the variance of this estimator, we subtract in the i-th step an estimate of the expected loss:</p><formula xml:id="formula_5">U(w w w, ω ω ω 1...i−1 ) := E ω ω ω i...N [L(ω ω ω 1...i−1 ω ω ω i...N |w w w, θ) + α · ω ω ω 1 ]<label>(6)</label></formula><p>We compute the expected loss using an LSTM that we train simultaneously with A to predict L + αω ω ω 1 based on w w w and ω ω ω 1...i−1 . To make learning more stable, we add an entropy term encouraging the distribution to be smooth, following <ref type="bibr" target="#b35">Xu et al. (2015)</ref>. The parameter updates to A are thus:</p><formula xml:id="formula_6">∑ w w w,ω ω ω ∑ i (L(ω ω ω|w w w, θ) + αω ω ω 1 −U(w w w, ω ω ω 1...i−1 )) · ∂ θ A (log P(ω i |ω ω ω 1...i−1 , w w w, θ)) −γ ∂ θ A ∑ w w w,ω ω ω ∑ i H[P(ω i |ω ω ω 1,...,i−1 , w w w, θ)]<label>(7)</label></formula><p>where γ is a hyperparameter, and H the entropy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methods</head><p>Our aim is to evaluate how well NEAT predicts hu- man fixation behavior and reading times. Further- more, we want show that known qualitative prop- erties emerge from the Tradeoff Hypothesis, even though no prior knowledge about useful features is hard-wired in NEAT.</p><formula xml:id="formula_7">w 1 w 2 w 3 A A A R 0 R 1 R 2 R 3 Decoder h 0 h 1 h 2 P R 1 P R 2 P R 3</formula><p>Figure 1: The architecture of the proposed model, reading a three-word input sequence w 1 , w 2 , w 3 . R is the reader network and P R the probability distribution it computes in each time step. A is the attention network. At each time step, the input, its probability according to P R , and the previous state h i−1 of R are fed into A, which then decides whether the word is read or skipped.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training Setup</head><p>For both the reader and the decoder networks, we choose a one-layer LSTM network with 1,000 mem- ory cells. The attention network is a one-layer feed- forward network. For the loss estimator U, we use a bidirectional LSTM with 20 memory cells. Input data is split into sequences of 50 tokens, which are used as the input sequences for NEAT, disregarding sentence boundaries. Word embeddings have 100 di- mensions, are shared between the reader and the attention network, and are only trained during the training of the reader. The vocabulary consists of the 10,000 most frequent words from the training corpus. We trained NEAT on the training set of the Daily Mail section of the corpus described by <ref type="bibr" target="#b13">Hermann et al. (2015)</ref>, which consists of 195,462 arti- cles from the Daily Mail newspaper, containing ap- proximately 200 million tokens. The recurrent net- works and the attention network were each trained for one epoch. For initialization, weights are drawn from the uniform distribution. We set α = 5.0, γ = 5.0, and used a constant learning rate of 0.01 for A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Corpus</head><p>For evaluation, we use the English section of the Dundee corpus ( <ref type="bibr" target="#b15">Kennedy and Pynte, 2005)</ref>, which consists of 20 texts from The Independent, anno- tated with eye-movement data from ten English na- tive speakers. Each native speakers read all 20 texts and answered a comprehension question after each text. We split the Dundee corpus into a development and a test set, with texts 1-3 constituting the devel- opment set. The development set consists of 78,300 tokens, and the test set of 281,911 tokens. For evalu- ation, we removed the datapoints removed by Dem- berg and <ref type="bibr" target="#b6">Keller (2008)</ref>, mainly consisting of words at the beginning or end of lines, outliers, and cases of track loss. Furthermore, we removed datapoints where the word was outside of the vocabulary of the model, and those datapoints mapped to positions 1- 3 or 48-50 of a sequence when splitting the data. After preprocessing, 62.9% of the development to- kens and 64.7% of the test tokens remained. To ob- tain the number of fixations on a token and reading times, we used the eye-tracking measures computed by <ref type="bibr" target="#b6">Demberg and Keller (2008)</ref>. The overall fixation rate was 62.1% on the development set, and 61.3% on the test set. The development set was used to run preliminary versions of the human evaluation studies, and to de- termine the human skipping rate (see Section 5). All the results reported in this paper were computed on the test set, which remained unseen until the model was final.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head><p>Throughout this section, we consider the following baselines for the attention network: random atten- tion is defined by ω ω ω ∼ Binom(n, p), with p = 0.62, the human fixation rate in the development set. For full attention, we take ω ω ω = 1, i.e., all words are fixated. We also derive fixation predictions from full surprisal, word frequency, and word length by choosing a threshold such that the resulting fixation rate matches the human fixation rate on the develop- ment set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Quantitative Properties</head><p>By averaging over all possible fixation sequences, NEAT defines for each word in a sequence a prob- ability that it will be fixated. This probability is not efficiently computable, so we approximate it by sampling a sequence ω ω ω and taking the probabilities P(ω i = 1|ω 1...i−1 , w w w) for i = 1, . . . , 50. These sim-ulated fixation probabilities can be interpreted as defining a distribution of attention over the input sequence. <ref type="figure" target="#fig_0">Figure 2</ref> shows heatmaps of the simu- lated and human fixation probabilities, respectively, for the beginning of a text from the Dundee cor- pus. While some differences between simulated and human fixation probabilities can be noticed, there are similarities in the general qualitative features of the two heatmaps. In particular, function words and short words are less likely to be fixated than content words and longer words in both the simulated and the human data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reconstruction and Language Modeling</head><p>We first evaluate NEAT intrinsically by measuring how successful the network is at predicting the next word and reconstructing the input while minimiz- ing the number of fixations. We compare perplex- ity on reconstruction and language modeling for ω ω ω ∼ P(ω ω ω|w w w, θ). In addition to the baselines, we run NEAT on the fixations generated by the human read- ers of the Dundee corpus, i.e., we use the human fix- ation sequence as ω ω ω instead of the fixation sequence generated by A to compute perplexity. This will tell us to what extent the human behavior minimizes the NEAT objective (4).</p><p>The results are given in <ref type="table">Table 1</ref>. In all settings, the fixation rates are similar (60.4% to 62.1%) which makes the perplexity figures directly comparable. While NEAT has a higher perplexity on both tasks compared to full attention, it considerably outper- forms random attention. It also outperforms the word length, word frequency, and full surprisal base- lines. The perplexity on human fixation sequences is similar to that achieved using word frequency. Based on these results, we conclude that REINFORCE suc- cessfully optimizes the objective (4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Likelihood of Fixation Data</head><p>Human reading be- havior is stochastic in the sense that different runs of eye-tracking experiments such as the ones recorded in the Dundee corpus yield different eye-movement sequences. NEAT is also stochastic, in the sense that, given a word sequence w, it defines a probability dis- tribution over fixation sequences ω ω ω. Ideally, this dis- tribution should be close to the actual distribution of fixation sequences produced by humans reading the sequence, as measured by perplexity.</p><p>We find that the perplexity of the fixation se- Acc F1 fix F1 skip NEAT 63.7 70.4 53.0 Supervised Models <ref type="bibr" target="#b22">Nilsson and Nivre (2009)</ref> 69.5 75.2 62.6 <ref type="bibr" target="#b17">Matthies and Søgaard (2013)</ref> 69.9 72.3 66.1</p><p>Human Performance and Baselines Random Baseline 52.6 62.1 37.9 Full Surprisal 64.1 70.7 53.6 Word Frequency 67.9 74.0 58.3 Word Length 68.4 77.1 49.0 Human 69.5 76.6 53.6 quences produced by the ten readers in the Dundee corpus under NEAT is 1.84. A perplexity of 2.0 corresponds to the random baseline Binom(n, 0.5), and a perplexity of 1.96 to random attention Binom(n, 0.62). As a lower bound on what can achieved with models disregarding the context, us- ing the human fixation rates for each word as proba- bilities, we obtain a perplexity of 1.68.</p><p>Accuracy of Fixation Sequences Previous work on supervised models for modeling fixations <ref type="bibr" target="#b22">(Nilsson and Nivre, 2009;</ref><ref type="bibr" target="#b17">Matthies and Søgaard, 2013)</ref> has been evaluated by measuring the overlap of the fixation sequences produced by the models with those in the Dundee corpus. For NEAT, this method of evaluation is problematic as differences between model predictions and human data may be due to differences in the rate of skipping, and due to the in- herently stochastic nature of fixations. We therefore derive model predictions by rescaling the simulated fixation probabilities so that their average equals the fixation rate in the development set, and then greed- ily take the maximum-likelihood sequence. That is, we predict a fixation if the rescaled probability is greater than 0.5, and a skip otherwise. As in previ- ous work, we report the accuracy of fixations and skips, and also separate F1 scores for fixations and skips. As lower and upper bounds, we use the ran- dom baseline ω ω ω ∼ Binom(n, 0.62) and the agree- ment of the ten human readers, respectively. The re-</p><p>The decision of the Human Fertility and Embryology Authority (HFEA) to allow a couple to select genetically their next baby was bound to raise concerns that advances in biotechnology are racing ahead of our ability to control the consequences. The couple at the centre of this case have a son who suffers from a potentially fatal disorder and whose best hope is a marrow transplant from a sibling, so the stakes of this decision are particularly high. The HFEA's critics believe that it sanctions 'designer babies' and does not show respect for the sanctity of individual life. Certainly, the authority's backing for Shahana and Raj Hashmi's plea for genetic screening raises fundamental questions</p><p>The decision of the Human Fertility and Embryology Authority (HFEA) to allow a couple to select genetically their next baby was bound to raise concerns that advances in biotechnology are racing ahead of our ability to control the consequences. The couple at the centre of this case have a son who suffers from a potentially fatal disorder and whose best hope is a marrow transplant from a sibling, so the stakes of this decision are particularly high. The HFEA's critics believe that it sanctions 'designer babies' and does not show respect for the sanctity of individual life. Certainly, the authority's backing for Shahana and Raj Hashmi's plea for genetic screening raises fundamental questions    sults are shown in <ref type="table" target="#tab_0">Table 2</ref>. NEAT clearly outper- forms the random baseline and shows results close to full surprisal (where we apply the same rescal- ing and thresholding as for NEAT). This is remark- able given that NEAT has access to only 60.4% of the words in the corpus in order to predict skipping, while full surprisal has access to all the words. Word frequency and word length perform well, al- most reaching the performance of supervised mod- els. This shows that the bulk of skipping behavior is already explained by word frequency and word length effects. Note, however, that NEAT is com- pletely unsupervised, and does not know that it has to pay attention to word frequency; this is something the model is able to infer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Restricted Surprisal and Reading Times</head><p>To evaluate the predictions NEAT makes for reading times, we use linear mixed-effects models contain- ing restricted surprisal derived from NEAT for the Dundee test set. The mixed models also include a set of standard baseline predictors, viz., word length, log word frequency, log frequency of the previous word, launch distance, landing position, and the po- sition of the word in the sentence. We treat partici- pants and items as random factors. As the dependent variable, we take first pass duration, which is the sum of the durations of all fixations from first enter- ing the word to first leaving it. We compare against full surprisal as an upper bound and against ran- dom surprisal as a lower bound. Random surprisal is surprisal computed by a model with random at- tention; this allows us to assess how much surprisal degrades when only 60.4% of all words are fixated, but no information is available as to which words should be fixated. The results in <ref type="table" target="#tab_4">Table 3</ref> show that restricted surprisal as computed by NEAT, full sur- prisal, and random surprisal are all significant pre- dictors of reading time.</p><p>In order to compare the three surprisal estimates, we therefore need a measure of effect size. For this, we compare the model fit of the three mixed ef- fects models using deviance, which is defined as the difference between the log likelihood of the model under consideration minus the log likelihood of the baseline model, multiplied by −2. Higher de-  viance indicates greater improvement in model fit over the baseline model. We find that the mixed model that includes restricted surprisal achieves a deviance of 867, compared to the model contain- ing only the baseline features. With full surprisal, we obtain a deviance of 980. On the other hand, the model including random surprisal achieves a lower deviance of 832. This shows that restricted surprisal as computed by NEAT not only significantly predicts reading times, it also provides an improvement in model fit compared to the baseline predictors. Such an im- provement is also observed with random surprisal, but restricted surprisal achieves a greater improve- ment in model fit. Full surprisal achieves an even greater improvement, but this is not unexpected, as full surprisal has access to all words, unlike NEAT or random surprisal, which only have access to 60.4% of the words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Qualitative Properties</head><p>We now examine the second key question we de- fined in Section 1, investigating the qualitative fea- tures of the simulated fixation sequences. We will focus on comparing the predictions of NEAT with that of word frequency, which performs comparably at the task of predicting fixation sequences (see Sec-  tion 5.1). We show NEAT nevertheless makes rele- vant predictions that go beyond frequency.</p><p>Fixations of Successive Words While predictors derived from word frequency treat the decision whether to fixate or skip words as independent, hu- mans are more likely to fixate a word when the pre- vious word was skipped <ref type="bibr" target="#b25">(Rayner, 1998)</ref>. This effect is also seen in NEAT. More precisely, both in the human data and in the simulated fixation data, the conditional fixation probability P(ω i = 1|ω i−1 = 1) is lower than the marginal probability P(ω i = 1). The ratio of these probabilities is 0.85 in the human data, and 0.81 in NEAT. The threshold predictor de- rived from word frequency also shows this effect (as the frequencies of successive words are not indepen- dent), but it is weaker (ratio 0.91).</p><p>To further test the context dependence of NEAT's fixation behavior, we ran a mixed model predict- ing the fixation probabilities simulated by NEAT, with items as random factor and the log frequency of word i as predictor. Adding ω i−1 as a predic- tor results in a significant improvement in model fit (deviance = 4,798, t = 71.3). This shows that NEAT captures the context dependence of fixation sequences to an extend that goes beyond word fre- quency alone.</p><p>Parts of Speech Part of speech categories are known to be a predictor of fixation probabilities, with content words being more likely to be fixated than function words <ref type="bibr" target="#b5">(Carpenter and Just, 1983)</ref>. In <ref type="table" target="#tab_6">Table 4</ref>, we give the simulated fixation probabilities and the human fixation probabilities estimated from the Dundee corpus for the tags of the Universal PoS tagset ( <ref type="bibr">Petrov et al., 2012)</ref>, using the PoS annotation of <ref type="bibr">Barrett et al. (2015)</ref>. We again compare with the probabilities of a threshold predictor derived from word frequency. 1 NEAT captures the differences be- tween PoS categories well, as evidenced by the high correlation coefficients. The content word categories ADJ, ADV, NOUN, VERB and X consistently show higher probabilities than the function word cate- gories. While the correlation coefficients for word frequency are very similar, the numerical values of the simulated probabilities are closer to the human ones than those derived from word frequency, which tend towards more extreme values. This difference can be seen clearly if we compare the mean squared error, rather than the correlation, with the human fix- ation probabilities (last row of <ref type="table" target="#tab_6">Table 4</ref>).</p><p>Correlations with Known Predictors In the lit- erature, it has been observed that skipping correlates with predictability (surprisal), word frequency, and word length <ref type="bibr">(Rayner, 1998, p. 387</ref>). These correla- tions are also observed in the human skipping data derived from Dundee, as shown in <ref type="table">Table 5</ref>. (Hu- man fixation probabilities were obtained by averag- ing over the ten readers in Dundee.)</p><p>Comparing the known predictors of skipping with NEAT's simulated fixation probabilities, similar cor- relations as in the human data are observed. We ob- serve that the correlations with surprisal are stronger in NEAT, considering both restricted surprisal and full surprisal as measures of predictability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We investigated the hypothesis that human read- ing strategies optimize a tradeoff between precision of language understanding and economy of atten- tion. We made this idea explicit in NEAT, a neural reading architecture with hard attention that can be <ref type="bibr">1</ref> We omit the tag "." for punctuation, as punctuation charac- ters are not treated as separate tokens in Dundee.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human</head><p>NEAT Restricted Surprisal 0.465 0.762 Full Surprisal 0.512 0.720 Log Word Freq. −0.608 −0.760 Word Length 0.663 0.521 <ref type="table">Table 5</ref>: Correlations between human and NEAT fixation prob- abilities and known predictors trained end-to-end to optimize this tradeoff. Exper- iments on the Dundee corpus show that NEAT pro- vides accurate predictions for human skipping be- havior. It also predicts reading times, even though it only has access to 60.4% of the words in the cor- pus in order to estimate surprisal. Finally, we found that known qualitative properties of skipping emerge in our model, even though they were not explicitly included in the architecture, such as context depen- dence of fixations, differential skipping rates across parts of speech, and correlations with other known predictors of human reading behavior.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Top: Heatmap showing human fixation probabilities, as estimated from the ten readers in the Dundee corpus. In cases of track loss, we replaced the missing value with the corresponding reader's overall fixation rate. Bottom: Heatmap showing fixation probabilities simulated by NEAT. Color gradient ranges from blue (low probability) to red (high probability); words without color are at the beginning or end of a sequence, or out of vocabulary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 :</head><label>1</label><figDesc>Performance on language modeling and reconstruction as measured by perplexity. Random attention is an upper bound on perplexity, while full attention is a lower bound. For the human baseline, we give two figures, which differ in the treatment of missing data. The first figure is obtained when replacing missing values with a random variable ω ∼ Binom(n, 0.61); the second results from replacing missing values with 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Evaluation of fixation sequence predictions against hu-

man data. For the human baseline, we predicted the n-th reader's 

fixations by taking the fixations of the n + 1-th reader (with 

missing values replaced by reader average), averaging the re-

sulting scores over the ten readers. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 3 : Linear mixed effects models for first pass duration.</head><label>3</label><figDesc></figDesc><table>The first part of the table shows the coefficients, standard er-

rors, and t values for the predictors in the baseline model. The 

second part of the table gives the corresponding values for ran-

dom surprisal, restricted surprisal computed by NEAT, and full 

surprisal, residualized against the baseline predictors, in three 

models obtained by adding these predictors. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Actual and simulated fixation probabilities (in %) by 

PoS tag, with the ranks given in brackets, and correlations and 

mean squared error relative to human data. 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning wake-sleep recurrent attention models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2575" to="2583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Zeljko Agi´cagi´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Søgaard</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The Dundee treebank</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Workshop on Treebanks and Linguistic Theories</title>
		<meeting>the 14th International Workshop on Treebanks and Linguistic Theories</meeting>
		<imprint>
			<biblScope unit="page" from="242" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A rational model of eye movement control in reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klinton</forename><surname>Bicknell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1168" to="1178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">What your eyes do while your mind is reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Just</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eye Movements in Reading</title>
		<editor>K. Rayner</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1983" />
			<biblScope unit="page" from="275" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Data from eye-tracking corpora as evidence for theories of syntactic processing complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vera</forename><surname>Demberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="193" to="210" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A dynamical model of saccade generation in reading based on spatially distributed lexical processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Engbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">André</forename><surname>Longtin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhold</forename><surname>Kliegl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="621" to="636" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">SWIFT: A dynamical model of saccade generation during reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Engbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antje</forename><surname>Nuthmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eike</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhold</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kliegl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="777" to="813" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Insensitivity of the human sentence-processing system to hierarchical structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="829" to="834" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A probabilistic Earley parser as a psycholinguistic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="159" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Predicting word fixations in text with a CRF model for capturing general reading strategies among readers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadayoshi</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daichi</forename><surname>Mochihashi Yoshinobu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akiko</forename><surname>Kano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Eye-tracking and Natural Language Processing</title>
		<meeting>the 1st Workshop on Eye-tracking and Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="55" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Human gaze control in realworld scene perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="498" to="504" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kočisk</surname></persName>
		</author>
		<idno>ArXiv:1506.03340</idno>
		<title level="m">Teaching machines to read and comprehend</title>
		<editor>Kočisk`y, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Parafoveal-onfoveal effects in normal reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joël</forename><surname>Pynte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="153" to="168" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Expectation-based syntactic comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1126" to="1177" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">With blinkers on: Robust prediction of eye movements across readers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><surname>Matthies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="803" to="807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Eye movements reveal the on-line computation of lexical probabilities during reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">A</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">C</forename><surname>Shillcock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="648" to="652" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Low-level predictive inference in reading: the influence of transitional probabilities on eye movements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">A</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">C</forename><surname>Shillcock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="1735" to="1751" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukáš</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaňjaň</forename><surname>Cernock´ycernock´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2204" to="2212" />
		</imprint>
	</monogr>
	<note>Alex Graves, and others</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning where to look: Modeling eye movements in reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mattias</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference on Computational Natural Language Learning</title>
		<meeting>the 13th Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="93" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Towards a data-driven model of eye movement control in reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mattias</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics</title>
		<meeting>the Workshop on Cognitive Modeling and Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="63" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">McDonald. 2012. A universal part-of-speech tagset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Language Resources and Evaluation</title>
		<meeting>the 8th International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<biblScope unit="page" from="2089" to="2096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Eye movements in reading and information processing: 20 years of research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rayner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="372" to="422" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Eye movements in reading: Models and data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Rayner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Eye Movement Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Models of the reading process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Rayner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">D</forename><surname>Reichle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="787" to="799" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
<note type="report_type">Wiley Interdisciplinary Reviews</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Toward a model of eye movement control in reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Reichle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pollatsek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rayner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="125" to="157" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Using EZ Reader to model the effects of higher level language processing on eye movements during reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Reichle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Warren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcconnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The EZ Reader model of eyemovement control in reading: Comparisons to other models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">D</forename><surname>Reichle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Rayner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Pollatsek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page" from="445" to="476" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The effect of word predictability on reading time is logarithmic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathaniel</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="302" to="319" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Syntactic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Van Gompel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">J</forename><surname>Pg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pickering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Oxford Handbook of Psycholinguistics</title>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="289" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">34</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>ArXiv:1502.03044</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
