<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:29+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Joint Decoding of Tree Transduction Models for Sentence Compression</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 25-29, 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Ge</forename><surname>Yao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">China Key Laboratory of Computational Linguistic (Peking University)</orgName>
								<orgName type="institution" key="instit2">MOE</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Joint Decoding of Tree Transduction Models for Sentence Compression</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1828" to="1833"/>
							<date type="published">October 25-29, 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we provide a new method for decoding tree transduction based sentence compression models augmented with language model scores, by jointly decoding two components. In our proposed solution , rich local discriminative features can be easily integrated without increasing computational complexity. Utilizing an unobvious fact that the resulted two components can be independently decoded, we conduct efficient joint decoding based on dual decomposition. Experimental results show that our method outperforms traditional beam search decoding and achieves the state-of-the-art performance.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sentence compression is the task of generating a grammatical and shorter summary for a long sen- tence while preserving its most important informa- tion. One specific instantiation is deletion-based compression, namely generating a compression by dropping words. Various approaches have been proposed to challenge the task of deletion-based compression. Earlier pioneering works <ref type="bibr" target="#b11">(Knight and Marcu, 2000</ref>) considered several insightful approaches, including noisy-channel based gen- erative models and discriminative decision tree models. Structured discriminative compression models <ref type="bibr" target="#b13">(McDonald, 2006</ref>) are capable of inte- grating rich features and have been proved effec- tive for this task. Another powerful paradigm for sentence compression should be mentioned here is constraints-based compression,including inte- ger linear programming solutions <ref type="bibr" target="#b3">(Clarke and Lapata, 2008</ref>) and first-order Markov logic networks ( <ref type="bibr" target="#b10">Huang et al., 2012;</ref><ref type="bibr" target="#b19">Yoshikawa et al., 2012)</ref>.</p><p>A notable class of methods that explicitly deal with syntactic structures are tree transduction models ( <ref type="bibr" target="#b4">Cohn and Lapata, 2007;</ref><ref type="bibr" target="#b7">Cohn and Lapata, 2009)</ref>. In such models a synchronous gram- mar is extracted from a corpus of parallel syn- tax trees with leaves aligned. Compressions are generated from the grammar with learned weights. Previous works have noticed that local coherence is usually needed by introducing ngram language model scores, which will make accurate decoding intractable. Traditional approaches conduct beam search to find approximate solutions <ref type="bibr" target="#b7">(Cohn and Lapata, 2009)</ref>.</p><p>In this paper we propose a joint decoding strat- egy to challenge this decoding task. We ad- dress the problem as jointly decoding a simple tree transduction model that only considers rule weights and an ngram compression model. Al- though either part can be independently solved by dynamic programming, the naive way to integrate two groups of partial scores into a huge dynamic programming chart table is computationally im- practical. We provide an effective dual decompo- sition solution that utilizes the efficient decoding of both parts. By integrating rich structured fea- tures that cannot be efficiently involved in normal formulation, results get significantly improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Motivation</head><p>Under the tree transduction models, the sentence compression task is formulated as learning a map- ping from an input source syntax tree to a target tree with reduced number of leaves. This map- ping is known as a synchronous grammar. The synchronous grammar discussed through out this paper will be synchronous tree substitution gram- mar (STSG), as in previous studies.</p><p>In such formulations, sentence compression is finding the best derivation from a syntax tree that produces a simpler target tree, under the current definition of grammar and learned parameters. Each derivation is attached with a score. For the sake of efficient decoding, the score often decom-poses with rules involved in the derivation. A typ- ical score definition for a derivation y of source tree x is in such form <ref type="bibr" target="#b6">(Cohn and Lapata, 2008;</ref><ref type="bibr" target="#b7">Cohn and Lapata, 2009)</ref>:</p><formula xml:id="formula_0">S(x, y) = r∈y w T φ r (x)+log P (ngram(y)) (1)</formula><p>The first term is a weighted sum of features φ r (x) defined on each rule r. It is plausible to introduce local scores from ngram models. The second term in the above score definition is added with such purpose. <ref type="bibr" target="#b7">Cohn and Lapata (2009)</ref> explained that ex- act decoding of Equation 1 is intractable. They proposed a beam search decoding strategy cou- pled with cube-pruning heuristic <ref type="bibr" target="#b1">(Chiang, 2007)</ref>, which can further improve decoding efficiency at the cost of largely losing exactness in log probabil- ity calculations. For efficiency reasons, rich local ngram features have not been introduced as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Components of Joint Decoding</head><p>The score in Equation 1 consists of two parts: sum of weighted rule features and local ngram scores retrieved from a language model. There is an im- plicit fact that either part can be used alone with slight modifications to generate a coarse candidate compression. Therefore, we can build a joint de- coding system that consists of these two indepen- dently decodable components.</p><p>In this section we will refer to these two in- dependent models as the pure tree transduction model and the pure ngram compression model, described in Section 3.1 and Section 3.2 respec- tively. There is a direct generalization of the ngram model by introducing rich local features, which results in the structured discriminative mod- els (Section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pure Tree Transduction model</head><p>By merely considering scores from tree transduc- tion rules, i.e. the first part of Equation 1, we can have our scores factorized with rules. Then finding the best derivation from a STSG grammar can be easily solved by a dynamic programming process described by <ref type="bibr" target="#b4">Cohn and Lapata (2007)</ref>.</p><p>This simplified pure tree transduction model can still produce decent compressions if the rule weights are properly learned during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pure Ngram based Compression</head><p>The pure ngram based model will try to find the most locally smooth compression, reflected by having the maximum log probability score of ngrams.</p><p>To avoid the trivial solution of deleting all words, we find the target compression with speci- fied length by dynamic programming.</p><p>Furthermore, we can integrate features other than log probabilities. This is equivalent to using a structured discriminative model with rich features on ngrams of candidate compressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Structured Discriminative Model</head><p>The structured discriminative model proposed by <ref type="bibr" target="#b13">McDonald (2006)</ref> defines rich features on bigrams of possible compressions. The score is defined as weighted linear combination of those features:</p><formula xml:id="formula_1">f (x, z) = |z| j=2 w · f (x, L(z j−1 ), L(z j )) (2)</formula><p>where the function L(z k ) maps a token z k in com- pression z back to the index of the original sen- tence x. Decoding can still be efficiently done by dynamic programming.</p><p>With rich local structural information, the struc- tured discriminative model can play a complemen- tary role to the tree transduction model that focus more on global syntactic structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Joint Decoding</head><p>From now on the remaining issue is jointly de- coding the components. Either part factorizes over local structures: rules for the tree transduc- tion model and ngrams for the language model or structured discriminative model. We may build a large dynamic programming table to utilize this kind of locality. Unfortunately this is computa- tionally impractical. It is mathematically equiva- lent to perform exact dynamic programming de- coding of Equation 1, which would consume asymptotically O(SRL 2(n−1)V ) 1 time for build- ing the chart ( <ref type="bibr" target="#b7">Cohn and Lapata, 2009)</ref>. <ref type="bibr" target="#b7">Cohn and Lapata (2009)</ref> proposed a beam search approxima- tion along with cube-pruning heuristics to reduce the time complexity down to O(SRBV ) 2 .</p><p>In this work we utilize the efficiency of indepen- dent decoding from the two components respec- tively and then combine their solutions according to certain standards. This naturally results in a dual decomposition ) solution.</p><p>Dual decomposition has been applied in sev- eral natural language processing tasks, including dependency parsing ( <ref type="bibr" target="#b12">Koo et al., 2010)</ref>, machine translation ( <ref type="bibr" target="#b0">Chang and Collins, 2011;</ref><ref type="bibr" target="#b15">Rush and Collins, 2011</ref>) and information extraction <ref type="bibr" target="#b14">(Reichart and Barzilay, 2012)</ref>. However, the strength of this inference strategy has seldom been noticed in researches on language generation tasks.</p><p>We briefly describe the formulation here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Description</head><p>We denote the pure tree transduction part and the pure ngram part as g(y) and f (z) respectively. Then joint decoding is equivalent to solving: max</p><formula xml:id="formula_2">y∈Y,z∈Z g(y) + f (z) (3) s.t. z kt = y kt , ∀k ∈ {1, ..., n}, ∀t ∈ {0, 1}</formula><p>, where y denotes a derivation which yields a final compression {y 1 , ..., y m }. This derivation comes from a pure tree transduction model. z denotes the compression composed of {z 1 , ..., z m } from an ngram compression model. Without loss of gener- ality, we consider y k and z k as indicators that take value 1 if the k's token of original sentence has been preserved in the compression and 0 if it has been deleted. In the constraints of problem 3, y kt or z kt denote indicator variables that take value 1 if y k or z k = t and 0 otherwise. Let L(u, y, z) be the Lagrangian of (3). Then the dual objective naturally factorizes into two parts that can be evaluated independently:</p><formula xml:id="formula_3">L(u) = max y∈Y,z∈Z L(u, y, z) = max y∈Y,z∈Z g(y) + f (z) + k,t u kt (z kt − y kt ) = max y∈Y (g(y) − k,t u kt y kt ) + max z∈Z (f (z) + k,t u kt z kt )</formula><p>With this factorization, Algorithm 1 tries to solve the dual problem min u L(u) by alternatively decoding each component.</p><p>This framework is feasible and plausible in that the two subproblems (line 3 and line 4 in Algo- rithm 1) can be easily solved with slight modifica- Algorithm 1 Dual Decomposition Joint Decoding 1: Initialization: u (0) k = 0, ∀k ∈ {1, ..., n} 2: for i = 1 to M AX IT ER do 3:</p><formula xml:id="formula_4">y (i) ← argmax y∈Y (g(y) − k,t u (i−1) kt y kt ) 4: z (i) ← argmax z∈Z (f (z) + k,t u (i−1) kt z kt ) 5: if y (i) kt = z (i) kt ∀k ∀t then 6: return (y (i) , z (i) ) 7: else 8: u (i) kt ← u (i−1) kt − δi(z (i) kt − y (i) kt ) 9:</formula><p>end if 10: end for tions on the values of the original dynamic pro- gramming chart. Joint decoding of a pure tree transduction model and a structured discriminative model is almost the same.</p><p>The asymptotic time complexity of Algorithm 1 is O(k(SRV + L 2(n−1) )), where k denotes the number of iterations. This is a significant re- duction of O(SRL 2(n−1)V ) by directly solving the original problem and is also comparable to O(SRBV ) of conducting beam search decoding.</p><p>We apply a similar heuristic with <ref type="bibr" target="#b16">Rush and Collins (2012)</ref> to set the step size δ i = 1 t+1 , where t &lt; i is the number of past iterations that increase the dual value. This setting decreases the step size only when the dual value moves towards the wrong direction. We limit the maximum iteration number to 50 and return the best primal solution y (i) among all previous iterations for cases that do not converge in reasonable time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Baselines</head><p>The pure tree transduction model and the discrim- inative model naturally become part of our base- lines for comparison <ref type="bibr">3</ref> . Besides comparing our methods against the tree-transduction model with ngram scores by beam search decoding, we also compare them against the available previous work from <ref type="bibr" target="#b9">Galanis and Androutsopoulos (2010)</ref>. This state-of-the-art work adopts a two-stage method to rerank results generated by a discriminative maxi- mum entropy model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Data Preparation</head><p>We evaluated our methods on two standard cor- pora 4 , refer to as Written and Spoken respectively.</p><p>We split the datasets according to <ref type="table">Table 1</ref> All tree transduction models require parallel parse trees with aligned leaves. We parsed all sen- tences with the Stanford Parser 5 and aligned sen- tence pairs with minimum edit distance heuristic <ref type="bibr">6</ref> . Syntactic features of the discriminative model were also taken from these parse trees.</p><p>For systems involving ngram scores, we trained a trigram language model on the Reuters Corpus (Volume 1) <ref type="bibr">7</ref> with modified Kneser-Ney smooth- ing, using the widely used tool SRILM 8 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Model Training</head><p>The training process of a tree transduction model followed similarly to <ref type="bibr" target="#b4">Cohn and Lapata (2007)</ref> us- ing structured SVMs ( <ref type="bibr" target="#b18">Tsochantaridis et al., 2005</ref>). The structured discriminative models were trained according to <ref type="bibr" target="#b13">McDonald (2006)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Evaluation Metrics</head><p>We assessed the compression results by the F1- score of grammatical relations (provided by a dependency parser) of generated compressions against the gold-standard compression <ref type="bibr" target="#b2">(Clarke and Lapata, 2006</ref>). All systems were controlled to pro- duce similar compression ratios (CR) for fair com- parison. We also reported manual evaluation on a sampled subset of 30 sentences from each dataset. Three unpaid volunteers with self-reported fluency in English were asked to rate every candidate. Rat- ings are in the form of 1-5 scores for each com- pression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>We report test set performance of the struc- tured discriminative model, the pure tree transduc- tion (T3), <ref type="bibr" target="#b9">Galanis and Androutsopoulos (2010)</ref>'s method (G&amp;A2010), tree transduction with lan- guage model scores by beam search and the pro- posed joint decoding solutions. <ref type="table" target="#tab_1">Table 2</ref> shows the compression ratios and F- measure of grammatical relations in average for each dataset. <ref type="table" target="#tab_2">Table 3</ref> presents averaged human rat- ing results for each dataset. We carried out pair- wise t-test to examine the statistical significance of the differences <ref type="bibr">9</ref> . In both datasets joint decod- ing with dual decomposition solution outperforms other systems, especially when structured models involved. We can also find certain improvements of joint modeling with dual decomposition on the original beam search decoding of Equation 1, un- der very close compression ratios.</p><p>Joint decoding of pure tree transduction and dis- criminative model gives better performance than the joint model of tree transduction and language model. From <ref type="table" target="#tab_2">Table 3</ref> we can see that integrat- ing discriminative model will mostly improve the preservation of important information rather than grammaticality. This is reasonable under the fact that the language model is trained on large scale data and will often preserve local grammatical co- herence, while the discriminative model is trained on small but more compression specific corpora.  <ref type="table">Table 4</ref> shows some examples of compressed sentences produced by all the systems in compar- ison. The two groups of outputs are compressions of one sentence from the Written corpora and the Spoken corpora respectively. Ungrammatical compressions can be found very often by several baselines for different reasons, such as the outputs from pure tree transduction and the discriminative model in the first group. The reason behind the under generation of pure tree transduction is that it mainly deals with global syntactic integrity merely in terms of the application of synchronous rules. Introducing language model scores will smooth the candidate compressions and avoid many ag- gressive decisions of tree transduction. Discrim- inative models are good at local decisions with poor consideration of grammaticality. We can see that the joint models have collected their predic- tive power together. Unfortunately we can still observe some redundancy from our outputs in the examples. The size of training corpus is not large enough to provide enough lexicalized information. On the other hand, the time consumption of the joint model with dual decomposition decoding in our experiments matched the aforementioned asymptotic analysis. The training process based on new decoding method consumes similar time as beam search with cube-pruning heuristic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>In this paper we propose a joint decoding scheme for tree transduction based sentence compression. Experimental results suggest that the proposed framework works well. The overall performance gets further improved under our framework by in- troducing the structured discriminative model.</p><p>As several recent efforts have focused on ex- tracting large-scale parallel corpus for sentence compression <ref type="bibr" target="#b8">(Filippova and Altun, 2013)</ref>, we would like to study how larger corpora can af- fect tree transduction and our joint decoding so- <ref type="table">Table 4: Example outputs</ref> Original: It was very high for people who took their full-time education beyond the age of 18 , and higher among women than men for all art forms except jazz and art galleries . Discr.: It was high for people took education higher among women .</p><p>( <ref type="bibr" target="#b9">Galanis and Androutsopoulos, 2010)</ref>: It was high for people who took their education beyond the age of 18 , and higher among women . Pure T3: It was very high for people who took . T3+LM-BeamSearch: It was very high for people who took their education beyond the age of 18 , and higher among women than men . T3+LM-DualDecomp: It was very high for people who took their education beyond the age of 18 , and higher among women than men . T3+Discr.: It was high for people who took education beyond the age of 18 , and higher among women than men . Gold-Standard: It was very high for people who took full-time education beyond 18 , and higher among women for all except jazz and galleries .</p><p>Original: But they are still continuing to search the area to try and see if there were , in fact , any further shooting incidents . Discr.: they are continuing to search the area to try and see if there were , further shooting incidents . (Galanis and Androutsopoulos, 2010): But they are still continuing to search the area to try and see if there were , in fact , any further shooting incidents . Pure T3: they are continuing to search the area to try and see if there were any further shooting incidents . T3+LM-BeamSearch: But they are continuing to search the area to try and see if there were , in fact , any further shooting incidents . T3+LM-DualDecomp: But they are continuing to search the area to try and see if there were any further shooting incidents . T3+Discr.: they are continuing to search the area to try and see if there were further shooting incidents . Gold-Standard: they are continuing to search the area to see if there were any further incidents .</p><p>lution. Meanwhile, We would like to explore on how other text-rewriting problems can be formu- lated as a joint model and be applicable to similar strategies described in this work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>. Table 1 : Dataset partition (number of sentences) Corpus Training Development Testing</head><label>.1</label><figDesc></figDesc><table>Written 
1,014 
324 
294 
Spoken 
931 
83 
254 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Results of automatic evaluation. ( †: 
sig. diff. from T3+LM(DD); *: sig. diff. from 
T3+Discr.(DD) for p &lt; 0.01) 

Written 
CR(%) GR-F1(%) 
Discriminative 
70.3 
52.4  † *  
G&amp;A2010 
71.6 
60.2  *  
Pure Tree-Transduction 
72.6 
52.3  † *  
T3+LM (Beam Search) 
70.4 
58.8  *  
T3+LM (Dual Decomp.) 
70.7 
60.5 
T3+Discr. (Dual Decomp.) 
71.0 
62.3 
Gold-Standard 
71.4 
100.0 

Spoken 
CR(%) GR-F1(%) 
Discriminative 
69.5 
50.6  † *  
G&amp;A2010 
71.7 
59.2  *  
Pure Tree-Transduction 
73.6 
53.8  † *  
T3+LM (Beam Search) 
75.5 
59.5  *  
T3+LM (Dual Decomp.) 
75.3 
61.5 
T3+Discr. (Dual Decomp.) 
74.9 
63.3 
Gold-Standard 
72.4 
100.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 3 : Results of human rating. ( †: sig. diff. from T3+LM(DD); *: sig. diff. from T3+Discr.(DD), for p &lt; 0.01)</head><label>3</label><figDesc></figDesc><table>Written 
GR. 
Imp. 
CR(%) 
Discriminative 
3.92  † *  3.46  † *  
70.6 
G&amp;A2010 
4.11  † *  3.50  † *  
72.4 
Pure Tree-Transduction 
3.85  † *  3.42  † *  
70.1 
T3+LM (Beam Search) 
4.22  † *  3.69  *  
73.0 
T3+LM (Dual Decomp.) 
4.63 
3.98 
73.2 
T3+Discr. (Dual Decomp.) 4.62 
4.25 
73.5 
Gold-Standard 
4.89 
4.76 
72.9 

Spoken 
GR. 
Imp. 
CR(%) 
Discriminative 
3.95  † *  3.62  † *  
71.2 
G&amp;A2010 
4.09  † *  3.96  *  
72.5 
Pure Tree-Transduction 
3.92  † *  3.55  † *  
71.4 
T3+LM (Beam Search) 
4.20  *  
3.78  *  
75.0 
T3+LM (Dual Decomp.) 
4.35 
4.18 
74.5 
T3+Discr. (Dual Decomp.) 4.47 
4.26 
74.7 
Gold-Standard 
4.83 
4.80 
73.1 

</table></figure>

			<note place="foot" n="1"> S, R, L and V denote respectively for the number of source tree nodes, the number of rules, size of target lexicon and number of variables involved in each rule. 2 B denotes the beam width.</note>

			<note place="foot" n="3"> The pure ngram language model should not be considered here as it requires additional length constraints and in general does not produce competitive results at all merely by itself. 4 Available at http://jamesclarke.net/research/resources</note>

			<note place="foot" n="5"> http://nlp.stanford.edu/software/lex-parser.shtml 6 Ties were broken by always aligning a token in compression to its last appearance in the original sentence. This may better preserve the alignments of full constituents. 7 http://trec.nist.gov/data/reuters/reuters.html 8 http://www-speech.sri.com/projects/srilm/</note>

			<note place="foot" n="9"> For all multiple comparisons in this paper, significance level was adjusted by the Holm-Bonferroni method.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by National Hi-Tech Re-search and Development Program (863 Program) of China (2014AA015102, 2012AA011101) and National Natural Science Foundation of China (61170166, 61331011). We also thank the anony-mous reviewers for very helpful comments.</p><p>The contact author of this paper, according to the meaning given to this role by Peking Univer-sity, is Xiaojun Wan.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Exact decoding of phrase-based translation models through lagrangian relaxation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin-Wen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Edinburgh, Scotland, UK.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-07" />
			<biblScope unit="page" from="26" to="37" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Hierarchical phrase-based translation. computational linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="201" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Models for sentence compression: A comparison across domains, training requirements and evaluation measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="377" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Global inference for sentence compression: An integer linear programming approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="273" to="381" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large margin synchronous generation and its application to sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<title level="m">Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</title>
		<meeting><address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="73" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sentence compression beyond word deletion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Computational Linguistics</title>
		<meeting>the 22nd International Conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="137" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sentence compression as tree transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="637" to="674" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Overcoming the lack of parallel data in sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Filippova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasemin</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1481" to="1491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An extractive supervised two-stage method for sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Androutsopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Los Angeles, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010-06" />
			<biblScope unit="page" from="885" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Using first-order logic to compress sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Statisticsbased summarization-step one: Sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI/IAAI</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="703" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dual decomposition for parsing with non-projective head automata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010-10" />
			<biblScope unit="page" from="1288" to="1298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Discriminative sentence compression with soft syntactic evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ryan T Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-event extraction guided by global constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="70" to="79" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Exact decoding of syntactic translation models through lagrangian relaxation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="72" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A tutorial on dual decomposition and lagrangian relaxation for inference in natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="305" to="362" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On dual decomposition and linear programming relaxations for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Alexander M Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010-10" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Large margin methods for structured and interdependent output variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasemin</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page" from="1453" to="1484" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sentence compression with semantic role constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsumasa</forename><surname>Yoshikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsutomu</forename><surname>Hirao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryu</forename><surname>Iida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="349" to="353" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
