<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:19+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Relation Extraction with Coarse-to-Fine Grained Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018. 2236</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">State Key Lab on Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Yu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">State Key Lab on Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">State Key Lab on Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">State Key Lab on Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Pattern Recognition Center</orgName>
								<address>
									<addrLine>WeChat</addrLine>
									<region>Tencent</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchical Relation Extraction with Coarse-to-Fine Grained Attention</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2236" to="2245"/>
							<date type="published">October 31-November 4, 2018. 2018. 2236</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Distantly supervised relation extraction employs existing knowledge graphs to automatically collect training data. While distant supervision is effective to scale relation extraction up to large-scale corpora, it inevitably suffers from the wrong labeling problem. Many efforts have been devoted to identifying valid instances from noisy data. However, most existing methods handle each relation in isolation, regardless of rich semantic correlations located in relation hierarchies. In this paper, we aim to incorporate the hierarchical information of relations for distantly supervised relation extraction and propose a novel hierarchical attention scheme. The multiple layers of our hierarchical attention scheme provide coarse-to-fine granularity to better identify valid instances , which is especially effective for extracting those long-tail relations. The experimental results on a large-scale benchmark dataset demonstrate that our models are capable of modeling the hierarchical information of relations and significantly outperform other baselines. The source code of this paper can be obtained from https://github.com/ thunlp/HNRE.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Relation extraction (RE) aims to predict relational facts from plain text. Conventional supervised RE models ( <ref type="bibr" target="#b31">Zelenko et al., 2003;</ref><ref type="bibr" target="#b16">Mooney and Bunescu, 2006</ref>) usually suffer from the lack of high-quality training data, because manual label- ing of training data is time-consuming and human- intensive. <ref type="bibr" target="#b15">Mintz et al. (2009)</ref> propose distant su- pervision to automatically label training instances by aligning existing knowledge graphs (KGs) and text: For an entity pair in KGs, those sentences containing both the entities will be labeled with the corresponding relation of the entity pair in KGs. RE relies on distant supervision to scale up to large-scale training corpora. However, this au- tomatic mechanism is inevitably accompanied by the wrong labeling problem, because not all sen- tences containing two entities can exactly express their relations in KGs, e.g., we may mistakenly la- bel "Bill Gates retired from Microsoft" with the relation business/company/founders.</p><p>To alleviate the wrong labeling problem, many efforts ( <ref type="bibr" target="#b19">Riedel et al., 2010;</ref><ref type="bibr" target="#b5">Hoffmann et al., 2011;</ref><ref type="bibr" target="#b23">Surdeanu et al., 2012;</ref><ref type="bibr" target="#b32">Zeng et al., 2015)</ref> have been devoted to identifying valid instances from noisy data, especially the recent state-of-the-art attention-based methods ( <ref type="bibr" target="#b11">Lin et al., 2016;</ref><ref type="bibr" target="#b8">Ji et al., 2017;</ref><ref type="bibr" target="#b28">Wu et al., 2017)</ref>. Neverthe- less, each relation is handled in isolation in most existing methods. For each relation, there is often a separate model (e.g. neural attention scheme) to select relation-related informative instances from noisy data, regardless of rich semantic correlations among relations, typically located in the form of relation hierarchies.</p><p>We take the KG Freebase ( <ref type="bibr" target="#b1">Bollacker et al., 2008)</ref> as an example, in which relations are la- beled as hierarchical structures. For example, the relation /location/province/capital in Freebase indicates the relation between a province and its capital. It is labeled under the location branch. Under this branch, there are some other relations /location/location/contains and /location/country/capital, which are closely correlated to each other. The rich correlations among relations are well revealed by these relation hierarchies. In fact, <ref type="bibr" target="#b13">McCallum et al. (1998)</ref> take advantage of hierarchies of classes to improve classification models and inspire many later models ( <ref type="bibr" target="#b21">Rousu et al., 2005;</ref><ref type="bibr" target="#b27">Weinberger and Chapelle, 2009)</ref>. Furthermore, the hierarchical in- formation of entities in KGs has also been uti- lized and demonstrated to be effective for model enhancement ( <ref type="bibr" target="#b6">Hu et al., 2015;</ref><ref type="bibr" target="#b29">Xie et al., 2016</ref>).</p><p>To take advantage of the rich correlated infor- mation among relations, we propose a novel hier- archical attention scheme via utilizing the relation hierarchies, rather than directly utilizing hierarchi- cal information as features for models. Similar to the conventional attention-based method, our method also computes an attention score for each instance according to its significance of express- ing the corresponding relation. The key difference is that, as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, our hierarchical attention scheme follows the relation hierarchies to compute scores for those instances containing the same entity pair on the each layer of the hier- archies.</p><p>The hierarchical attention scheme provides coarse-to-fine granularity for identifying valid in- stances. The attention on the bottom layer can cap- ture more specific features of the relation, which has a comparable ability of fine-grained instance selection like conventional attention-based meth- ods. The attention on the top-layer can capture the common features shared by several related sub- relations, which provides coarse-grained instance selection. Since there are more sufficient data for training the top-layer attention, the whole hierar- chical attention scheme can enhance RE models for solving those long-tail relations.</p><p>We conduct experiments on a large-scale bench- mark dataset for RE in this paper. The experi- mental results show that the proposed coarse-to- fine grained attention scheme based on relation hi- erarchies significantly outperforms other baseline methods, even as compared to the recent state- of-the-art attention-based models, especially for those long-tail relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Supervised models ( <ref type="bibr" target="#b31">Zelenko et al., 2003;</ref><ref type="bibr" target="#b40">Zhou et al., 2005;</ref><ref type="bibr" target="#b16">Mooney and Bunescu, 2006</ref>) for RE require adequate amounts of annotated data for their training. It is time-consuming to manu- ally label large-scale training data. <ref type="bibr">Hence, Mintz et al. (2009)</ref> propose distant supervision to au- tomatically label data. Distant supervision in- evitably accompanies with the wrong labeling problem. To alleviate the noise issue caused by distant supervision, <ref type="bibr" target="#b19">Riedel et al. (2010)</ref> and <ref type="bibr" target="#b5">Hoffmann et al. (2011)</ref> propose multi-instance learning (MIL) mechanisms. <ref type="bibr" target="#b20">Riedel et al. (2013)</ref> propose universal schema to transmit information between relations of KGs and textual patterns to enhance extraction performance.</p><p>These early RE methods mainly extract seman- tic features using NLP tools to build relation clas- sifiers. Recently, neural models have been widely used for RE. These neural models can accurately capture textual relations without explicit linguis- tic analysis ( <ref type="bibr" target="#b33">Zeng et al., 2014;</ref><ref type="bibr" target="#b30">Xu et al., 2015;</ref><ref type="bibr" target="#b22">Santos et al., 2015;</ref><ref type="bibr" target="#b36">Zhang and Wang, 2015;</ref>. <ref type="bibr" target="#b32">Zeng et al. (2015)</ref> employ the MIL scheme by selecting one most valid instance for distantly supervised neural relation extraction (NRE), whose denois- ing capability is far from satisfactory because most informative instances are neglected. <ref type="bibr" target="#b11">Lin et al. (2016)</ref> and <ref type="bibr" target="#b37">Zhang et al. (2017)</ref> propose neural attention schemes to select those informative in- stances. To further improve the attention perfor- mance, some works incorporate knowledge infor- mation ( <ref type="bibr" target="#b34">Zeng et al., 2017;</ref><ref type="bibr" target="#b8">Ji et al., 2017;</ref><ref type="bibr" target="#b3">Han et al., 2018)</ref> and advanced training strategies ( <ref type="bibr" target="#b7">Huang and Wang, 2017)</ref>. More sophisti- cated mechanisms, such as reinforcement learning <ref type="bibr" target="#b2">(Feng et al., 2018;</ref><ref type="bibr" target="#b35">Zeng et al., 2018</ref>) and adver- sarial training ( <ref type="bibr" target="#b28">Wu et al., 2017)</ref>, have also been adapted for RE recently.</p><p>However, most existing works model each rela- tion in isolation to identify informative instances, neglecting rich correlations among relations, es- pecially the hierarchical information of those re- lations. Hierarchical information is widely ap- plied for model enhancement, especially for clas- sification models <ref type="bibr" target="#b13">(McCallum et al., 1998;</ref><ref type="bibr" target="#b21">Rousu et al., 2005;</ref><ref type="bibr" target="#b27">Weinberger and Chapelle, 2009;</ref><ref type="bibr" target="#b38">Zhao et al., 2011;</ref><ref type="bibr" target="#b0">Bi and Kwok, 2011;</ref><ref type="bibr" target="#b39">Zhou et al., 2011;</ref><ref type="bibr" target="#b26">Verma et al., 2012</ref>). Many efforts are also de- voted to utilizing hierarchical information in KGs. <ref type="bibr" target="#b10">Leacock and Chodorow (1998)</ref> and <ref type="bibr" target="#b18">Ponzetto and Strube (2007)</ref> adopt hierarchical information de- rived from KGs to construct concept relatedness. <ref type="bibr" target="#b17">Morin and Bengio (2005)</ref> propose a neural lan- guage model by utilizing hierarchical information in WordNet. Further, <ref type="bibr" target="#b6">Hu et al. (2015)</ref> learn entity representations by considering the whole entity hi- erarchies of Wikipedia and inspire many works ( <ref type="bibr" target="#b9">Krompaß et al., 2015;</ref><ref type="bibr" target="#b29">Xie et al., 2016</ref>) to utilize hierarchical type structures to help the representa- tion learning of KGs.</p><p>Different from the recent hierarchical models that mainly focus on entity hierarchies and directly utilize hierarchical information as simple features, we incorporate relation hierarchies to build a hier- archical attention scheme with coarse-to-fine gran- ularity to enhance RE performance. As compared with the existing models for RE, our models could take advantage of relation correlations to better identify informative instances, especially for those long-tail relations, by transferring the knowledge from their related relations of high-frequency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we introduce the overall framework of our hierarchical attention for RE, starting with notations and definitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notations</head><p>We denote a KG as G = {E, R, F}, where E, R and F indicate the sets of entities, relations and facts respectively. (h, r, t) ∈ F indicates that there is a relation r ∈ R between h ∈ E and t ∈ E. We follow the MIL setting and split the entire instances into multiple entity-pair bags {S h 1 ,t 1 , S h 2 ,t 2 , . . .}. Each bag S h i ,t i contains mul- tiple instances {s 1 , s 2 , . . .} mentioning both the entities h i and t i . The distant supervision mech- anism will label the bag with the corresponding relation of the mentioned entity pair. Each in- stance s in these bags is denoted as a word se- quence s = {w 1 , w 2 , . . .}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Framework</head><p>Given an entity pair (h, t) and its entity-pair bag S h,t , we adopt our models to measure the prob- ability of each relation r ∈ R holding between the pair. As shown in <ref type="figure" target="#fig_3">Figure 2</ref>, the overall frame- work of our models includes a sentence encoder and a coarse-to-fine grained hierarchical attention. The sentence encoder adopts several convolutional neural networks to represent sentence semantics with embeddings, and the hierarchical attention is used to select the most informative instances to ex- actly express their relations.</p><p>For each instance s i ∈ S h,t , we use the sentence encoder to represent its semantic information as an embedding s i . The details of the sentence encoder will be introduced in Section 3.3. Since not all in- stances in the bag S h,t are positive to express the relation between h and t, we apply the hierarchi- cal attention to compute an instance weight α i for each instance s i . The details of the hierarchical at- tention will be introduced in Section 3.4. We build the global textual relation representation r h,t with the weighted sum of instance output embeddings,</p><formula xml:id="formula_0">r h,t = m i=1 α i s i , s 1 , . . . , s m ∈ S h,t . (1)</formula><p>Here α i is the instance weight for the ith instance output embedding s i . By taking r h,t as the textual relation representation of the entity pair (h, t), we estimate its probability over each relation r ∈ R, i.e., whether there is a specific relation r between h and t. We define the conditional probability P (r|h, t, S h,t ),</p><formula xml:id="formula_1">P (r|h, t, S h,t ) = exp(o r ) ˜ r∈R exp(o ˜ r ) ,<label>(2)</label></formula><p>where o is the scores of all relations, which is de- fined as follows,</p><formula xml:id="formula_2">o = Mr h,t ,<label>(3)</label></formula><p>where M is the representation matrix to calculate the relation scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Sentence Encoder</head><p>Given an instance s containing two entities, we ap- ply several neural architectures to encode the in- stance into its corresponding embeddings s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Layer</head><p>The input layer of the sentence encoder aims to embed both semantic information and positional information of words into their input embeddings. Word Embedding is proposed by <ref type="bibr" target="#b4">Hinton (1986)</ref>, which aims to transform words into dis- tributed representations to capture syntactic and semantic meanings of words. Given a sentence s consisting of multiple words s = {w 1 , . . . , w n }, we adopt Skip-Gram ( <ref type="bibr" target="#b14">Mikolov et al., 2013</ref>) to   </p><formula xml:id="formula_3">compute all k w -dimensional word embeddings {w 1 , . . . , w n }.</formula><p>Position Embedding is proposed by <ref type="bibr" target="#b33">Zeng et al. (2014)</ref>. Position embedding is used to embed the relative distances of each word to the two entities into two k p -dimensional vectors. By concatenat- ing the distance embeddings for the current word w i to the both head and tail entities, we get a uni- fied position embedding p i ∈ R kp×2 .</p><p>For each word w i , we concatenate its word em- bedding w i and position embedding p i to build its input embedding</p><formula xml:id="formula_4">x i ∈ R k i (k i = k w + k p × 2).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoding Layer</head><p>The encoding layer aims to compose the input embeddings of the given instance into its cor- responding instance embedding. In this paper, we choose two convolutional neural architectures, CNN ( <ref type="bibr" target="#b33">Zeng et al., 2014</ref>) and PCNN ( <ref type="bibr" target="#b32">Zeng et al., 2015)</ref>, to encode input embeddings into instance embeddings.</p><p>Other neural architectures such as recurrent neural architectures (Zhang and <ref type="bibr" target="#b36">Wang, 2015)</ref> can also be used as sentence encoders. Since previ- ous works show that both convolutional and recur- rent architectures can achieve comparable state- of-the-art performance, we simply select convo- lutional architectures in this paper. Note that, our hierarchical attention scheme is designed indepen- dently to the encoder choices, hence it can be eas- ily adapted to fit other encoder architectures.</p><p>CNN slides a convolution kernel with the win- dow size m over the input sequence {x 1 , . . . , x n } to get the k h -dimensional hidden embeddings.</p><formula xml:id="formula_5">h i = CNN x i− m−1 2 , . . . , x i+ m−1 2 .<label>(4)</label></formula><p>A max-pooling is then applied over these hidden embeddings to output the final instance embed- ding s as follows,</p><formula xml:id="formula_6">[s] j = max 1≤i≤n {[h i ] j },<label>(5)</label></formula><p>where <ref type="bibr">[·]</ref> j is the j-th value of a vector. PCNN is an extension to CNN, which also adopts a convolution kernel to obtain hidden em- beddings. Then, a piecewise max-pooling is ap- plied over the hidden embeddings,</p><formula xml:id="formula_7">[s (1) ] j = max 1≤i≤i 1 {[h i ] j }, [s (2) ] j = max i 1 +1≤i≤i 2 {[h i ] j }, [s (3) ] j = max i 2 +1≤i≤n {[h i ] j },<label>(6)</label></formula><p>where <ref type="bibr">[·]</ref> j is the j-th value of a vector, i 1 and i 2 are entity positions. The final instance embedding s is achieved by concatenating these three pooling results as follows,</p><formula xml:id="formula_8">s = [s (1) ; s (2) ; s (3) ].<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Hierarchical Selective Attention</head><p>Given the entity pair (h, t) and its bag of instances S h,t = {s 1 , s 2 , . . . , s m }, we achieve the instance embeddings {s 1 , s 2 , . . . , s m } using the sentence encoder. Afterwards, we apply a hierarchical se- lective attention over them to get the textual rela- tion representation r h,t for extracting relations. In this part, we will first introduce a plain selective attention, and then introduce our hierarchical at- tention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Plain Selective Attention</head><p>The plain selective attention scheme computes the attention score α i for each instance s i to indicate how well the instance can express the relation be- tween the two entities. We assign a query vector q r to each relation r ∈ R and the attention for each sentence in S h,t = {s 1 , s 2 , . . . , s m } is de- fined as follows,</p><formula xml:id="formula_9">e i = q r W s s i , α i = exp(e i ) m j=1 exp(e j ) ,<label>(8)</label></formula><p>where W s is the weight matrix. The attention scores can be used in Eq. 1 to compute textual re- lation representations. For simplicity, we denote such a plain selective attention operation as the following equation,</p><formula xml:id="formula_10">r h,t = ATT(q r , {s 1 , s 2 , . . . , s m }).<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hierarchical Selective Attention</head><p>The inherent hierarchical structure of relations lead us to modeling hierarchical attention. Gen- erally, given a relation set R of a KG G (e.g. Freebase), which consists of base-level relations (e.g. /location/province/capital), we can generate the corresponding higher-level rela- tion set R H . The relations in the high-level set (e.g. location) are more general and common, which usually contain several sub-relations in the base-level set. We assume that the sub-relations of different relations are disjoint, in other words, the relation hierarchies are tree-structured. The gener- ation process can be done recursively. In practice, we start from R 0 = R which is the set of all re- lations we focus for RE, and generate k − 1 times to get a total of k-level hierarchical relation sets</p><formula xml:id="formula_11">{R 0 , R 1 , . . . , R k−1 }.</formula><p>As shown in <ref type="figure" target="#fig_3">Figure 2</ref>, for a relation r = r 0 ∈ R = R 0 , which is the focus for RE, we construct its hierarchical chain of parent relations by back- tracking the relation hierarchy as follows,</p><formula xml:id="formula_12">(r 0 , . . . , r k−1 ) ∈ R 0 × . . . × R k−1 ,<label>(10)</label></formula><p>where r i−1 is the sub-relation of r i . As with the plain attention, we assign a query vector q r to each relation r ∈ k−1 i=0 R i . With the hierarchical chain, we compute attention oper- ations on the each layer of the relation hierarchies to obtain corresponding textual relation represen- tations, r i h,t = ATT(q r i , {s 1 , s 2 , . . . , s m }).</p><p>During the training process, those relation query vectors of high-level relations (i.e., q r i with larger i) have more instances for training than those query vectors of base-level relations. Hence, the high-level query vectors are more robust for in- stance selection but with coarse-grained capabil- ity. In contrast, the base-level query vectors (i.e., q r i with smaller i) always suffer from data sparsity, especially for those long-tail base rela- tions. Hence, the base-level query vectors can per- form fine-grained instance selection but the per- formance is not stable. Based on the hierarchical selective attention, we can simply concatenate the textual relation repre- sentations on different layers as the final represen- tation,</p><formula xml:id="formula_14">r h,t = [r 0 h,t ; . . . ; r k−1 h,t ].<label>(12)</label></formula><p>The representation r h,t will be finally fed to com- pute the conditional probability P (r|h, t, S h,t ) in Eq. 2. Note that, those high-level representa- tions (i.e., r i h,t with larger i) are coarse-grained, and those base-level representations (i.e., r i h,t with smaller i) are fine-grained. These hierarchical rep- resentations can provide more informative infor- mation than single-layered attention for relation prediction, especially for those long-tail relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Initialization and Implementation Details</head><p>Here we introduce the learning and optimization details for our hierarchical attention model. Dur- ing the training process, we minimize the cross entropy loss function. Given the collection of entity-pair bags π = {S h 1 ,t 1 , S h 2 ,t 2 , . . .} and cor- responding labeled relations {r 1 , r 2 , . . .}, we de- fine the loss function as follows,</p><formula xml:id="formula_15">J(θ) = − 1 |π| |π| i=1 log P (r i |h i , t i , S h i ,t i ) + λθ 2 2 ,<label>(13)</label></formula><p>where λ is a harmonic factor, and θ 2 2 is the reg- ularizer defined as L 2 normalization. All mod- els are optimized using stochastic gradient descent (SGD).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Evaluation</head><p>We evaluate our models on the New York Times (NYT) dataset developed by <ref type="bibr" target="#b19">Riedel et al. (2010)</ref>, which is widely used in recent works ( <ref type="bibr" target="#b11">Lin et al., 2016;</ref><ref type="bibr" target="#b34">Zeng et al., 2017;</ref><ref type="bibr" target="#b8">Ji et al., 2017;</ref><ref type="bibr" target="#b3">Han et al., 2018;</ref><ref type="bibr" target="#b28">Wu et al., 2017;</ref><ref type="bibr" target="#b7">Huang and Wang, 2017;</ref><ref type="bibr" target="#b2">Feng et al., 2018;</ref><ref type="bibr" target="#b35">Zeng et al., 2018)</ref>. The dataset has 53 relations including the NA re- lation which indicates relations of instances are not available. The training set has 522, 611 sen- tences, 281, 270 entity pairs and 18, 252 relational facts. In the test set, there are 172, 448 sentences, 96, 678 entity pairs and 1, 950 relational facts. In both the training and test set, we truncate the sen- tences which have more than 120 words into 120 words.</p><p>We evaluate all models in the held-out evalua- tion. It evaluates models by comparing the rela- tional facts discovered from the test articles with those in Freebase and provides an approximate measure of precision without human evaluation. For evaluation, we draw precision-recall curves for all models. Besides precision-recall curves, we also show the precision values at the specific re- call rate to conduct a more direct comparison, and calculate the micro and macro average precision scores to show the overall effect of different mod- els. To further verify the effect of our hierarchical attention for few-shot entity pairs, we follow the previous works to report the Precision@N results. The dataset and baseline code can be found from Github 1 ( <ref type="bibr" target="#b11">Lin et al., 2016;</ref><ref type="bibr" target="#b28">Wu et al., 2017;</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Parameter Settings</head><p>To fairly compare the results of our hierarchical attention models with those baselines, we also set most of the experimental parameters following <ref type="bibr" target="#b11">Lin et al. (2016)</ref>. <ref type="table">Table 1</ref> shows all experimen- tal parameters used in the experiments. We ap- ply dropout on the output layers of our models to prevent overfitting. For CNN, we set the dropout rate to 0.5. For PCNN, we observe that this model tends to overfit on the training set very quickly, and hence we set the dropout rate to 0.9 to allevi- ate the overfitting problem. We also pre-train the sentence encoder of PCNN before training our hi- erarchical attention. <ref type="bibr">1</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Overall Evaluation Results</head><p>To evaluate the performance of our proposed hier- archical models, we compare the precision-recall curves of our models with various previous re- lation extraction models. The evaluation results are shown in <ref type="figure">Figure 3</ref> and <ref type="figure">Figure 4</ref>. We re- port the results of the neural architectures in- cluding CNN and PCNN with various attention- based methods: +HATT is our hierarchical at- tention method; +ATT is the plain selective at- tention method over instances ( <ref type="bibr" target="#b11">Lin et al., 2016)</ref>; +ATT+ADV is the denoising attention method by adding a small adversarial perturbation to instance embeddings ( <ref type="bibr" target="#b28">Wu et al., 2017)</ref>; +ATT+SL is the attention-based model using soft-labeling method to mitigate the side effect of the wrong label-ing problem at entity-pair level ( ; +ONE is a vanilla MIL neural model without at- tention schemes ( <ref type="bibr" target="#b32">Zeng et al., 2015</ref>). We also com- pare our method with feature-based models, in- cluding Mintz ( <ref type="bibr" target="#b15">Mintz et al., 2009</ref>), MultiR (Hoff- mann et al., 2011) and MIML ( <ref type="bibr" target="#b23">Surdeanu et al., 2012)</ref>.</p><p>From the results, we observe that:</p><p>(1) All methods have reasonable precision when recall is smaller than 0.05. When the recall grad- ually grows, the performance of the feature-based methods drops much more faster than those neu- ral models. It shows that human-designed features are very limited as compared to neural models, es- pecially in a noisy data environment. Hence, for simplicity, we mainly show the results of our mod- els and other attention-based neural models in the following experiments.</p><p>(2) Both for CNNs and PCNNs, the models with attention schemes outperform the vanilla models without attention schemes. Though vanilla neural models are powerful for relation classification, it is still difficult to address data noise. The attention- based methods apply attentions over multiple in- stances and dynamically reduce the influence of noisy instances, which can effectively improve the performance of RE and achieve the state-of-the-art results.</p><p>(3) As shown in both of the figures, the models using hierarchical attention (HATT) achieve the best results among all the attention-based models. Even when compared with PCNN+ATT+ADV and PCNN+ATT+SL which adopt sophisticated denoising schemes and extra information, our models still keep significant advantages. This in- dicates that, as compared to the conventional plain attention schemes which handle each relation in isolation, our method can better take advantage of the rich correlations among relations. We be- lieve the performance of our hierarchical atten- tion scheme can be further improved by adopting extra mechanisms like adversarial training, rein- forcement learning and soft-labeling at entity-pair level, which will be left as our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Effect of Hierarchical Attention for Different Relations</head><p>To further verify the effectiveness of our hierar- chical attention method for different relations, we evaluate the RE performance of our method and conventional attention methods. Since we focus   more on the performance of those top-ranked re- sults, we report the precision scores when the re- call is 0.1, 0.2, 0.3 and their mean. We also report micro average scores and macro average scores in this experiment. As an approximation of the area under the precision-recall curve, the micro aver- age score gives a more complete view of the model performance. Since the micro average score gen- erally overlooks the influences of those long-tail relations, we use the macro average score to give more emphasis on long-tail relations in test sets, which is often neglected by the previous works.</p><p>The evaluation results are shown in <ref type="table" target="#tab_2">Table 2</ref>, and from the results we observe that: Our HATT method achieves consistent and significant im- provements as compared to the plain ATT method. From the micro and macro average precision scores, we find that our HATT method effectively improves RE performance especially for those long-tail relations. As compared to the plain ATT method, our method can take advantage of correla- tions among relations to achieve the improvement, especially on the long-tail relations.</p><p>To further demonstrate the improvements in performance on long-tail relations after introduc-    <ref type="table" target="#tab_3">Table 3</ref>, we observe that:</p><p>(1) For both CNN and PCNN models, our hier- archical attention outperforms the plain attention model. By taking advantage of the relation hier- archy, our models can learn better about long-tail relations via correlation information among rela- tions. We also observe that even our hierarchical CNN model presents a better performance than the plain PCNN model. This shows the power of rela- tion hierarchies, which makes our simpler CNN model outperforms the PCNN model on those long-tail relations.</p><p>(2) Although our HATT method has achieved obvious progress on the long-tail relations as com- pared with the plain ATT method, the results of all these methods are still far from satisfactory. This indicates that distantly supervised RE models suf- fer from not only the wrong labeling problem, but also the long-tail relation problem. We will in- corporate more schemes and extra information to solve this problem in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Effect of Hierarchical Attention with Different Instances</head><p>Since our method mainly focuses on modifications over selective attention, we also conduct Preci- sion@N tests on those entity pairs with few in- stances following <ref type="bibr" target="#b11">(Lin et al., 2016)</ref>. We use the three test settings for this experiment: the ONE test set where we randomly select one instance for each entity pair for evaluation; the TWO test set where we randomly select two instances for each entity pair; the ALL test set where we use all in- stances for each remaining entity pair for evalua- tion. For the ONE and TWO test set, we intend to show that taking correlation information among relations into consideration can lead to a better re- lation classifier. The ALL test set is designed to show the effect of our attention over multiple in- stances. We report the precision values of top N triples extracted, where N ∈ {100, 200, 300}.</p><p>The evaluation results are shown in <ref type="table" target="#tab_5">Table 4</ref>, and from the results we observe that:</p><p>(1) The performance of all methods is generally improved as the instance number increases. This shows that the selective attention model can effec- tively take advantage of information from multi- ple noisy instances by combining useful instances while discarding useless ones.</p><p>(2) Our HATT method has higher precision val- ues in the ONE test set. This indicates that even in an insufficient information scenario, correlations among relations can be caught by our hierarchical attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Case Study</head><p>We give some examples of how our hierarchical selective attention takes effect in selecting the sen- tences. In <ref type="table" target="#tab_6">Table 5</ref>, we display the sentences that are scored highest ("Good") or lowest ("Bad") by the attention of different hierarchical levels ("High" and "Base").</p><p>The relation /people/person/children has fewer than 1000 training instances and it is a long-tail relation. For this relation, the in- stance recommended by the higher-level attention straightforwardly expresses the relational fact that Nathan is the child of David by telling that Nathan is David's son, while the sentence with the low at- tention score actually gives the relationship of be- ing at the same generation. On the contrary, the lower-level attention mistakenly assigns high at- tention to the incorrect sentence. This example shows that our hierarchical attention is beneficial for these long-tail relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this paper, we take advantage of relation hierar- chies and propose a novel hierarchical instance- level attention for relation extraction. As com- pared with previous attention-based methods, our hierarchical attention provides coarse-to-fine granularity in instance selection and performs bet- ter extraction for long-tail relations. We con- duct various experiments and the evaluation re- sults show that incorporating the inherent hierar- chical structure of relations into attention schemes can take advantage of correlations among relations and improve the performance significantly.</p><p>In the future, we plan to explore the following directions: (1) It will be promising to adopt ex- tra information to help train more efficient models for solving the long-tail relation problem. (2) We may also combine our attention method with re- cent denoising methods to further improve model performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of hierarchical relation extraction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The architecture of hierarchical attention model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Precision-recall curves for the proposed model and various baseline models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>NRE, AtNRE and soft-label-RE</head><label></label><figDesc></figDesc><table>Batch Size B 
160 
Learning Rate α 
0.2 
Hidden Layer Dimension kc for CNNs 230 
Word Dimension kw 
50 
Position Dimension kp 
5 
Convolution Kernel Size m 
3 

Table 1: Parameter settings. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Precision (%) of attention-based models 
for different recalls. 

Training Instances 
&lt;100 
&lt;200 
Hits@K (Micro) 
10 
15 
20 
10 
15 
20 

CNN 
+ATT 
&lt;5.0 &lt;5.0 21.1 &lt;5.0 30.0 50.0 
+HATT 
5.3 
36.8 52.6 
40.0 60.0 70.0 

PCNN 
+ATT 
&lt;5.0 
10.5 47.4 
33.3 43.3 66.7 
+HATT 
31.6 
52.6 63.2 
53.3 70.0 76.7 

Training Instances 
&lt;100 
&lt;200 
Hits@K (Macro) 
10 
15 
20 
10 
15 
20 

CNN 
+ATT 
&lt;5.0 &lt;5.0 18.5 &lt;5.0 16.2 33.3 
+HATT 
5.6 
31.5 57.4 
22.7 43.9 65.1 

PCNN 
+ATT 
&lt;5.0 
7.4 40.7 
17.2 24.2 51.5 
+HATT 
29.6 
51.9 61.1 
41.4 60.6 68.2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Accuracy (%) of Hits@K on relations 
with training instances fewer than 100/200. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Top-N precision (P@N) for RE on the entity pairs with different number of instances (%). 

Relation: /people/person/children 

High 

Good 
David and Jody Smith and their son Nathan of 
Ankeny , Iowa , stayed at the hotel, . . . 

Bad 
. . . doting grandfather of Amanda, Lindsay, 
David, Alexa, Reese, Paige and Nathan. 

Base 

Good 
. . . cherished grandfather of David, Michael, Ja-
son, Vicky, Andrew, Sam and Nathan 

Bad 
David and Jody Smith and their son Nathan of 
Ankeny, Iowa, stayed at the hotel . . . 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Example sentences for case study. 

ing relation hierarchies, we extract a subset of the 
test dataset in which all the relations has fewer 
than 100/200 training instances. We employ 
the Hits@K metric for evaluation. For each en-
tity pair, the evaluation requires its corresponding 
golden relation in the first K candidate relations 
recommanded by the models. Because it is diffi-
cult for the existing models to extract long-tail re-
lations, we select K from {10, 15, 20}. We report 
the both micro and macro average Hits@K accura-
cies for these subsets. From the evaluation results 
in </table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by the National Nat-ural Science Foundation of China <ref type="bibr">(NSFC No. 61572273, 61532010)</ref>. This work is also funded by the Natural Science Foundation of China (NSFC) and the German Research Foundation (DFG) in Project Crossmodal Learning, NSFC 61621136008 / DFC TRR-169.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-label classification on tree-and dag-structured hierarchies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James T</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD</title>
		<meeting>KDD</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reinforcement learning for relation classification from noisy data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural knowledge acquisition via mutual attention between knowledge graph and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning distributed representations of concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geoffrey E Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COGSCI</title>
		<meeting>COGSCI</meeting>
		<imprint>
			<date type="published" when="1986" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Knowledgebased weak supervision for information extraction of overlapping relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congle</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Entity hierarchy embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Poyao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingkai</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1292" to="1300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for weakly-supervised relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1803" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction with sentence-level attention and entity descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3060" to="3066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Type-constrained representation learning in knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Krompaß</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Baier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Tresp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ISWC</title>
		<meeting>ISWC</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Combining local context and wordnet similarity for word sense identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Leacock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WordNet: An electronic lexical database</title>
		<meeting>WordNet: An electronic lexical database</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural relation extraction with selective attention over instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2124" to="2133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A soft-label method for noisetolerant distantly supervised relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1790" to="1795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving text classification by shrinkage in a hierarchy of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="359" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACLIJCNLP</title>
		<meeting>ACLIJCNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
	<note>Rion Snow, and Dan Jurafsky</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Subsequence kernels for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mooney And Razvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bunescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="171" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hierarchical probabilistic neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AISTATS</title>
		<meeting>AISTATS</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="246" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Knowledge derived from wikipedia for computing semantic relatedness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Simone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ponzetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of JAIR</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="181" to="212" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECML-PKDD</title>
		<meeting>ECML-PKDD</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Relation extraction with matrix factorization and universal schemas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin M</forename><surname>Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="74" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning hierarchical multi-category text classification models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Rousu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandor</forename><surname>Szedmak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="744" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Classifying relations by ranking with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cicero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-IJCNLP</title>
		<meeting>ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="626" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-instance multi-label learning for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multilingual relation extraction using compositional universal schema</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="886" to="896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Row-less universal schema</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="63" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning hierarchical similarity metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nakul</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sundararajan</forename><surname>Sellamanickam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2280" to="2287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Large margin taxonomy embedding for document categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chapelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1737" to="1744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adversarial training for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bamman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1778" to="1783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Representation learning of knowledge graphs with hierarchical types</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2965" to="2971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Classifying relations via long short term memory networks along shortest dependency paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1785" to="1794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Kernel methods for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Zelenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chinatsu</forename><surname>Aone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Richardella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of JMLR</title>
		<meeting>JMLR</meeting>
		<imprint>
			<date type="published" when="2003-02" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1083" to="1106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction via piecewise convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1753" to="1762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Incorporating relation paths in neural relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1768" to="1777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Large scaled relation extraction with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01006</idno>
		<title level="m">Relation classification via recurrent neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Positionaware attention and supervised data improve slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="35" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Large-scale category structure aware image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="87" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Hierarchical classification via orthogonal transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingrui</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Exploring various knowledge in relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="427" to="434" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
