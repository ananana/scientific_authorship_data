<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:57+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TSDPMM: Incorporating Prior Topic Knowledge into Dirichlet Process Mixture Models for Text Clustering</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linmei</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoli</forename><surname>Li</surname></persName>
							<email>xlli@i2r.a-star.edu.sg, {birdlinux, koodoneko}@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Institute for Infocomm Research(I2R)</orgName>
								<orgName type="department" key="dep2">A*STAR</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Shao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuzhong</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">State Key Laboratory of Math. Eng. and Advanced Computing</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Sci. and Tech</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TSDPMM: Incorporating Prior Topic Knowledge into Dirichlet Process Mixture Models for Text Clustering</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Dirichlet process mixture model (DPM-M) has great potential for detecting the underlying structure of data. Extensive studies have applied it for text clustering in terms of topics. However, due to the unsupervised nature, the topic clusters are always less satisfactory. Considering that people often have some prior knowledge about which potential topics should exist in given data, we aim to incorporate such knowledge into the DPMM to improve text clustering. We propose a novel model TSDPMM based on a new seeded Pólya urn scheme. Experimental results on document clustering across three datasets demonstrate our proposed TSDPMM significantly outperforms state-of-the-art DPMM model and can be applied in a lifelong learning framework.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Dirichlet process mixture model (DPMM) <ref type="bibr" target="#b11">(Neal, 2000</ref>) has been used in detecting the underlying structure in data. For example, ( <ref type="bibr" target="#b15">Vlachos et al., 2008;</ref><ref type="bibr" target="#b16">Vlachos et al., 2009</ref>) applied it to lexical- semantic verb clustering. ( <ref type="bibr" target="#b17">Wang et al., 2011;</ref><ref type="bibr" target="#b9">Huang et al., 2013;</ref><ref type="bibr" target="#b18">Yin and Wang, 2014</ref>) applied it for text clustering in terms of their topics. While DPMM achieved some promising results, it can still sometimes produce unsatisfactory topic clus- ters due to its unsupervised nature.</p><p>On the other hand, people often have prior knowledge about what potential topics should ex- ist in a given text corpus. Take an earthquake event corpus as an example. The topics, such as "ca- sualties and damages", "rescue" and "government reaction", called prior topics, are expected to oc- cur in the corpus according to our common knowl- edge (e.g., the topics automatically learned from previous events using topic modeling <ref type="bibr" target="#b0">(Ahmed and Xing, 2008)</ref>) or external resources (e.g., table of contents at Wikipedia event pages 1 ). Similarly, in academic fields, "call for papers (CFP)" of confer- ences 2 lists main topics that conference organizers would like to focus on. Clearly, these prior topic- s can be represented as sets of words, which are available in many real-world applications. They can serve as weakly supervised information to en- hance the unsupervised DPMM for text clustering.</p><p>Standard DPMM <ref type="bibr" target="#b11">(Neal, 2000;</ref><ref type="bibr" target="#b13">Ranganathan, 2006</ref>) lacks a mechanism for incorporating pri- or knowledge. Some existing work <ref type="bibr" target="#b15">(Vlachos et al., 2008;</ref><ref type="bibr" target="#b16">Vlachos et al., 2009</ref>) added knowledge of observed instance-level constraints (must-links and cannot-links between documents) to DPMM. ( <ref type="bibr" target="#b0">Ahmed and Xing, 2008)</ref> proposed recurrent Chi- nese Restaurant Process to incorporate previous documents with known topic clusters. We focus on incorporating topic-level knowledge, which is more challenging, as seed/prior topics could be la- tent rather than observable.</p><p>Particularly, we construct our novel TSDPM- M (Topic Seeded DPMM) based on a principled seeded Pólya urn (sPU) scheme. Our model inher- its the nonparametric property of DPMM and has additional technical merits. Importantly, our mod- el is encouraged but not forced to find evidences of seed topics. Therefore, it has freedom to discover new topics beyond prior topics, as well as to detect which prior topics are not covered by current da- ta. It is thus convenient to observe topic variations between prior topics and newly mined topics. Ex- perimental results on document clustering across three corpora demonstrate that our model effec- tively incorporates prior topics, and significantly outperforms state-of-the-art DPMM model. Par- ticularly, our TSDPMM can be applied in a life- long learning framework which enables the prior topic knowledge to evolve as more and more data are observed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Topic Seeded DPMM</head><p>In this section, we first introduce the standard DPMM model for document clustering in terms of topics. Then we describe how to incorporate seed/prior topics into the model using a seeded Pólya urn (sPU) scheme, which gives us our novel TSDPMM model (Topic Seeded DPMM). Finally, we present the model inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">DPMM</head><p>The DPMM <ref type="bibr" target="#b3">(Antoniak, 1974)</ref> as a non-parametric model assumes the given data is governed by an infinite number of components where only a frac- tion of these components are activated by the da- ta. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the DPMM graphical model and its generative process of a document x i . First, we sample a topic θ i = {θ ij } j=|V | j=1 (a multinomial distribution over words belonging to the vocabu- lary V ) for the document x i according to a Dirich- let Process (DP) G ∼ DP (α, G 0 ), where α &gt; 0 is a concentration parameter and the base measure G 0 = Dir( ⃗ β) can be considered as a prior distri- bution for θ. Consider the document x i as a bag of words, given the topic θ i , the generative distribu- tion F is a given likelihood function parameterized by θ. We define F as p(</p><formula xml:id="formula_0">x i |θ i ) = ∏ |x i | j=1 p(x ij |θ i )</formula><p>, where x ij is the j th word in x i . Note that the DP- MM assumes each document can be assigned to one topic cluster only. The DP process of DPMM, according to which topic θ i for a document x i is drawn, can be ex- plained by the popular metaphor of Pólya urn (PU) scheme <ref type="bibr" target="#b4">(Blackwell and MacQueen, 1973)</ref>, equiv- alent to the Chinese Restaurant Process ( <ref type="bibr" target="#b0">Ahmed and Xing, 2008)</ref>. The PU scheme works on balls (documents) and colors (topics). It starts with an empty urn. With probability proportional to α, we draw θ i ∼ G 0 , and add a ball of this color to the urn. With probability proportional to i − 1 (i.e., the current number of balls in the urn), we draw a ball at random from the urn, observe its color θ i and replace the ball with two balls of the same col- or. In this way, we draw topic θ i for document x i . As shown in the process, the prior probability of assigning a document to a topic is proportional to the number of documents already assigned to the topic. As a result, the DPMM exhibits the "rich get richer" property.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">TSDPMM: Incorporating Seed Topics</head><p>In this section, we describe our proposed algorith- m to incorporate prior seed topics into the DPM- M. A prior/seed topic k is represented by a vec- tor ⃗ N (0) k (word frequencies under the topic). We can obtain the prior topics represented by ⃗ N (0) k from past learning of topic models or external re- sources such as Wikipedia and "CFP". Assuming we have K (0) prior topics, we use the parame- ter</p><formula xml:id="formula_1">⃗ α (0) = {α (0) k } K (0)</formula><p>k=1 to control our confidence about how likely each prior topic exists. Let us go back to Pólya urn (PU) scheme, where a prior topic can be taken as a known color. We extend the PU scheme to incorporate prior topics, which gives the sPU (seeded Pólya Urn) scheme. The sPU scheme can be described as follows:</p><p>• We start with an urn with α (0) k balls of each known color k ∈ {1, ..., K (0) }.</p><p>• With a probability proportional to α, we draw θ i ∼ G 0 and add a ball of this color to the urn.</p><p>• With probability proportional to</p><formula xml:id="formula_2">i − 1 + ∑ K (0) k=1 α (0)</formula><p>k , we draw a random ball from the urn, and replace the ball with two balls of the same color.</p><p>As shown in the above process, instead of start- ing with an empty urn in DPMM, we assume that the urn already has certain balls of known colors. In this way, we incorporate the prior seed topic- s. The number of initial balls (documents) α k for prior topics with dif- ferent confidence levels. This sPU scheme gives our novel model TSDPMM (Topic Seeded DPM- M) incorporating prior topics. The TSDPMM has similar graphical representation as DPMM <ref type="figure" target="#fig_0">(Fig- ure 1)</ref>, except the introduction of hyper-parameter ⃗ α <ref type="bibr">(0)</ref> . We then present a collapsed gibbs sampling algorithm for model inference as follows.</p><p>TSDPMM Inference. The model inference is described in detail in Algorithm 1. It first ini- tializes all documents with random topic clusters. Then it iteratively updates the topic cluster assign- ments of documents according to the conditional probabilities (Eq.1) until convergence. Eq.1 can be derived as:</p><formula xml:id="formula_3">p(z i | ⃗ Z −i , ⃗ X) ∝ p(z i | ⃗ Z −i , α, ⃗ α (0) )p(x i | ⃗ X −i , ⃗ Z, ⃗ β) (1)</formula><p>where z i is the topic assignment of observation x i , ⃗ X is the given document corpus, and ⃗ Z −i are ⃗ X −i are the set of topic assignments and the corpus ex- cluding the i th observation x i , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Collapsed Gibbs Sampling</head><p>Input:</p><formula xml:id="formula_4">Document dataset ⃗ X = {x i } m i=1 , prior topics { ⃗ N (0) k } K (0) k=1 , parameter ⃗ α (0)</formula><note type="other">Output: Topic assignments ⃗ Z of all documents Initialize the topic assignments ⃗ Z based on prior topics randomly; repeat Select a document x i ∈ ⃗ X randomly Fix the other topic assignments ⃗ Z −i Assign a new value to z</note><formula xml:id="formula_5">i :z i ∼ p(z i | ⃗ Z −i , ⃗ X)(Eq. 1) until Convergence;</formula><p>In Eq.1, the first item p(z i =k| ⃗ Z −i , α, ⃗ α (0) ) de- notes a prior probability of z i =k, which is pro- portional to the number of documents already as- signed to it. If k is a prior topic, it is propor- tional to n k,−i + α <ref type="formula" target="#formula_10">(0)</ref> k , where n k,−i is the num- ber of documents of topic k excluding the cur- rent document x i . If k is an existing (not pri- or) topic, it is proportional to n k,−i . If k is a new topic, the probability is proportional to α.</p><formula xml:id="formula_6">The second item p(x i | ⃗ X −i , ⃗ Z −i , z i = k, ⃗ β) is the likelihood of x i given ⃗ X −i , ⃗ Z −i and z i =k. They can be derived as p(x i | ⃗ X −i , ⃗ Z −i , z i = k, ⃗ β) ∝ p( ⃗ X| ⃗ Z, ⃗ β) p( ⃗ X −i | ⃗ Z −i , ⃗ β)</formula><p>where</p><formula xml:id="formula_7">p( ⃗ X| ⃗ Z, ⃗ β) = ∫ p( ⃗ X| ⃗ Z, Θ)p(Θ| ⃗ β)dΘ. As p(Θ| ⃗ β) is a Dirich- let distribution and p( ⃗ X| ⃗ Z, Θ) is a multinomial distribution, we can get p( ⃗ X| ⃗ Z)= ∏ K k=1 ∆( ⃗ N k + ⃗ β) ∆( ⃗ β)</formula><p>, where ⃗ N k = {N k,w } V w=1 and N k,w is the number of occurrences of word w in the k th topic. Here, we adopt the function ∆ in <ref type="figure">(Heinrich, 2009)</ref>, and</p><formula xml:id="formula_8">we have ∆( ⃗ β) = ∏ V w=1 Γ(β) Γ( ∑ V w=1 )β and ∆( ⃗ N k + ⃗ β) = ∏ V w=1 Γ(N k,w +β) Γ( ∑ V w=1 (N k,w +β))</formula><p>. Finally, we can derive:</p><formula xml:id="formula_9">p(z i = k| ⃗ Z −i , ⃗ X) ∝            (n k,−i + α (0) ) · ∆( ⃗ N .,i + ⃗ N k,−i + ⃗ N (0) k + ⃗ β) ∆( ⃗ N k,−i + ⃗ N (0) k + ⃗ β) prior n k,−i · ∆( ⃗ N .,i + ⃗ N k,−i + ⃗ β) ∆( ⃗ N k,−i + ⃗ β) existing α · ∆( ⃗ N .,i + ⃗ β) ∆( ⃗ β) new ,</formula><p>where ⃗ N k,−i is a vector with the word counts for all the documents assigned to topic k excluding x i , ⃗ N .,i and ⃗ N</p><p>k are vectors with word counts in doc- ument x i and in all the documents assigned to k in prior knowledge respectively. According to this equation, documents are likely to go into clusters which are bigger and give higher likelihood of the documents. When the Gibbs sampler converges, we obtain topic cluster assignments of all the doc- uments. Different from DPMM inference process in which topics are removed when no documents is assigned to them, TSDPMM inference can retain prior topics all the time due to the initial number of documents ⃗ α (0) , making it able to track prior topics, as well as to detect new topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We evaluate our proposed TSDPMM model for document clustering on 3 datasets where each cluster corresponds to a topic. We implement both DPMM and TSDPMM models -their source codes are available at https://github.com/ newsminer/DPMM_and_TSDPMM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>We collect machine learning conference NIPS datasets composed of paper titles and abstracts from 2012 to 2014 -each year includes 342, 360 and 411 documents respectively. They are named as NIPS-12, NIPS-13 and NIPS-14.</p><p>We also employ the standard benchmark news datasets, including 20 Newsgroups 3 and Reuters- 21578. As news is often timely reported, we choose three continuous days with the largest number of documents in 20 Newsgroups (i.e. 11, 12 and 13 May) and Reuters-21578 (i.e. 3, 4 and 5 March) for our experiments. These datasets are denoted <ref type="figure" target="#fig_0">as 20N-1, 20N-2, 20N-3 (including 103,  96, 106 documents)</ref> and Reu-1, Reu-2, <ref type="bibr">Reu-3 ( including 282, 249, 207 documents)</ref>, respectively.</p><p>For all the datasets, we conduct the following preprocessing: (1) Convert letters into lowercase; (2) Remove non-Latin characters and stop words; (3) Remove words with document frequency &lt; 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Setup</head><p>We take the standard DPMM as our baseline method and compare it with our proposed TSDP- MM model using different prior knowledge ob- tained with different manners.</p><p>For NIPS datasets, we use two kinds of prior knowledge: one is the topics learned by DPMM from previous year's dataset; the other one is from an external resource "CFP" 4 (10 topics, same for each year). We name them as TSDPMM-P and TSDPMM-E respectively. As the topic descrip- tions in "CFP" are sparse, we repeat each topic description by ten times and then represent a topic with the words with word frequencies in its de- scription text.</p><p>For both 20 Newsgroups and Reuters datasets, we use prior knowledge learned by DPMM from the previous day's dataset. Furthermore, to test if we can improve the results continuously by ap- plying TSDPMM, every time when we model a new dataset, we incorporate prior topics learned by TSDPMM from previous day's dataset, similar to lifelong learning <ref type="bibr" target="#b6">(Chen and Liu, 2014;</ref><ref type="bibr" target="#b14">Thrun, 1998)</ref>. We call this model as TSDPMM-L.</p><p>Parameter Setting. Following a previous work ( <ref type="bibr" target="#b16">Vlachos et al., 2009)</ref>, we set the hyper-parameters α=1, ⃗ α (0) ={1.0}, ⃗ β ={1.0}. We run Gibbs sam- pler for 100 iterations and stop the iteration once the log-likelihood of the training data converges.</p><p>Evaluation. The widely used NMI (normal- ized mutual information) measure <ref type="bibr" target="#b7">(Dom, 2002)</ref>, has been employed to evaluate document cluster- ing results. The higher a value of NMI, the better a clustering result is. However, NMI needs true class labels for documents, and can only be ap- plied to our benchmark news datasets. For NIPS datasets without true labels, we use the measure of perplexity, as defined in ( <ref type="bibr" target="#b5">Blei et al., 2003)</ref>, to test per-word likelihood of the datasets. The lower the perplexity, the better a model fits the data.  <ref type="table">Table 1</ref> shows the average perplexity values of five runs of 3 models on NIPS datasets. It shows that both TSDPMM-P and TSDPMM-E, lever- aging prior topics from previous learning and "CFP" significantly outperform DPMM. In ad- dition, TSDPMM-E achieves lower performance than TSDPMM-P due to its lower quality of prior topics directly obtained from "CFP", compared to higher quality topics from past learning. We may improve "CFP" knowledge by extending it with related texts from search engines or Wikipedia us- ing keywords in "CFP" in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>An insight of our clustering results on NIPS-14 dataset suggests that most prior topics in 2013 are covered again in 2014 (consistent topics), except a few missing topics such as "lasso for Bayesian networks". Additionally, some newly evolved top- ics in 2014, e.g. "monte carlo particle filtering" and "nash games", are successfully discovered by our proposed model.  <ref type="table" target="#tab_1">Table 2</ref> illustrates the average NMI values of five runs of DPMM, TSDPMM and TSDPMM-L on news datasets. The results show that TSDP- MM using prior topics learnt by DPMM outper- forms DPMM (on average +5.8%; p &lt;0.025 with t-test). Additionally, TSDPMM-L, which continu- ously uses prior topics learnt by TSDPMM from previous dataset, further outperforms TSDPMM (on average +3.2%; p &lt;0.025 with t-test). Note TSDPMM-L uses TSDPMM results of 20N-1 and Reu-1 as prior knowledge for the first time, so there are no TSDPMM-L results for the first days in <ref type="table" target="#tab_1">Table 2</ref> for 20N-1 and Reu-1 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Discussion</head><p>The experimental results across 3 datasets have demonstrated that our proposed models can im- prove DPMM model by incorporating prior top- ic knowledge, and the higher-quality knowledge will lead to better results. By applying our TS- DPMM in a lifelong continuous learning frame- work, namely TSDPMM-L, can further improve Models 20N-1 20N-2 20N-3 Reu-1 Reu-2 Reus-3 DPMM 0.610 0.537 0.590 0.509 0.647 0.653 TSDPMM 0.645 0.610 0.681 0.648 0.654 0.655 TSDPMM-L - 0.681 0.697 - 0.689 0.656 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Our work is related to papers <ref type="bibr" target="#b15">(Vlachos et al., 2008;</ref><ref type="bibr" target="#b16">Vlachos et al., 2009)</ref>, which added supervi- sion (instance-level must-links or cannot-links be- tween documents) to the DPMM. (Ahmed and X- ing, 2008) proposed recurrent Chinese Restauran- t Process to incorporate previous documents with known topic clusters. However, our work is very different as we focus on how to incorporate latent topic-level prior knowledge. We model prior top- ics as known colors that have a certain probability proportional to α</p><p>k to be assigned to a document. In addition, our inference mechanism subsequent- ly takes the prior knowledge into consideration for automatically assigning topics to documents.</p><p>Some existing studies such as ( <ref type="bibr" target="#b12">Ramage et al., 2009;</ref><ref type="bibr" target="#b1">Andrzejewski et al., 2009;</ref><ref type="bibr" target="#b10">Jagarlamudi et al., 2012;</ref><ref type="bibr" target="#b2">Andrzejewski et al., 2011</ref>) worked on incorporating prior lexical or domain knowledge into LDA. Different from all these work, we focus on the nonparametric model DPMM and propose to incorporate the prior topic knowledge obtained in multiple ways.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a novel problem of in- corporating prior topics into DPMM model and address it through a simple yet principled seeded Pólya urn scheme. We show that the topic knowl- edge can be obtained in multiple ways. Exper- iments on document clustering across 3 dataset- s demonstrate our proposed model can effectively incorporate the prior topic knowledge and signifi- cantly enhance the standard DPMM for text clus- tering. In future work, we will study how to dis- cover overlapping clusters, i.e., allowing one doc- ument to be grouped into multiple topic cluster- s. We will also explore how to incorporate prior knowledge about topic relations (such as causation and correlation) into topic modeling.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Graphical Representation of DPMM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>controls how likely the topic k exists. We can use different values of α (0)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>4</head><label></label><figDesc>https://nips.cc/Conferences/2014/CallForPapers</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 : Average NMI of different models on news datasets.</head><label>2</label><figDesc></figDesc><table>text clustering due to the better prior topic knowl-
edge obtained in the evolving environment. 

</table></figure>

			<note place="foot" n="1"> e.g., http://en.wikipedia.org/wiki/2010 Chile earthquake 2 e.g., https://nips.cc/Conferences/2014/CallForPapers</note>

			<note place="foot" n="3"> http://people.csail.mit.edu/jrennie/20Newsgroups/</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dynamic nonparametric mixture models and the recurrent chinese restaurant process: with applications to evolutionary clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SDM</title>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="219" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Incorporating domain knowledge into topic modeling via dirichlet forest priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Andrzejewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Craven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual ICML</title>
		<meeting>the 26th Annual ICML</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A framework for incorporating general domain knowledge into latent dirichlet allocation using first-order logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Andrzejewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Craven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ProceedingsIJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">1171</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Mixtures of dirichlet processes with applications to bayesian nonparametric problems. The annals of statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charles E Antoniak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1974" />
			<biblScope unit="page" from="1152" to="1174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Ferguson distributions via pólya urn schemes. The annals of statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Blackwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>James B Macqueen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973" />
			<biblScope unit="page" from="353" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mining topics in documents: standing on the shoulders of big data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference</title>
		<meeting>the 20th ACM SIGKDD international conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1116" to="1125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An information-theoretic external cluster-validity measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dom</forename><surname>Byron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth conference on Uncertainty in artificial intelligence</title>
		<meeting>the Eighteenth conference on Uncertainty in artificial intelligence</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="137" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Parameter estimation for text analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregor</forename><surname>Heinrich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>vsonix GmbH and University of Leipzig</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dirichlet process mixture model for document clustering with feature partition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruizhang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaojun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangxing</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1748" to="1759" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Incorporating lexical priors into topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jagadeesh</forename><surname>Jagarlamudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghavendra</forename><surname>Udupa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the European Chapter of the ACL</title>
		<meeting>the 13th Conference of the European Chapter of the ACL</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="204" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Markov chain sampling methods for dirichlet process mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computational and graphical statistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="249" to="265" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Labeled LDA: A supervised topic model for credit attribution in multi-labeled corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David Leo Wright</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on EMNLP</title>
		<meeting>the 2009 Conference on EMNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The dirichlet process mixture (dpm) model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ananth Ranganathan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Lifelong learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sebastian Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning to learn</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="181" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dirichlet process mixture models for verb clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICML workshop on Prior Knowledge for Text and Language</title>
		<meeting>the ICML workshop on Prior Knowledge for Text and Language</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised and constrained dirichlet process mixture models for verb clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on geometrical models of natural language semantics</title>
		<meeting>the workshop on geometrical models of natural language semantics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="74" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dirichlet process mixture models based topic identification for short text streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caixia</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwei</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NLPKE</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="80" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A dirichlet multinomial mixture model-based approach for short text clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhua</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD</title>
		<meeting>the 20th ACM SIGKDD</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="233" to="242" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
