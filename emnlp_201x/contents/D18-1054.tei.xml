<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cut to the Chase: A Context Zoom-in Network for Reading Comprehension</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sathish</forename><surname>Indurthi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung Research</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghak</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung Research</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seohyun</forename><surname>Back</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung Research</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heriberto</forename><surname>Cuayáhuitl</surname></persName>
							<email>HCuayahuitl@lincoln.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung Research</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Lincoln</orgName>
								<address>
									<settlement>Lincoln</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cut to the Chase: A Context Zoom-in Network for Reading Comprehension</title>
					</analytic>
					<monogr>
						<title level="j" type="main">Association for Computational Linguistics</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="volume">570</biblScope>
							<biblScope unit="page" from="570" to="575"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In recent years many deep neural networks have been proposed to solve Reading Comprehension (RC) tasks. Most of these models suffer from reasoning over long documents and do not trivially generalize to cases where the answer is not present as a span in a given document. We present a novel neural-based architecture that is capable of extracting relevant regions based on a given question-document pair and generating a well-formed answer. To show the effectiveness of our architecture, we conducted several experiments on the recently proposed and challenging RC dataset &apos;Nar-rativeQA&apos;. The proposed architecture outper-forms state-of-the-art results (Tay et al., 2018) by 12.62% (ROUGE-L) relative improvement.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Building Artificial Intelligence (AI) algorithms to teach machines to read and to comprehend text is a long-standing challenge in Natural Language Processing (NLP). A common strategy for assess- ing these AI algorithms is by treating them as RC tasks. This can be formulated as finding an an- swer to a question given the document(s) as evi- dence. Recently, many deep-learning based mod- els ( <ref type="bibr" target="#b7">Seo et al., 2017;</ref><ref type="bibr" target="#b16">Xiong et al., 2017;</ref><ref type="bibr" target="#b14">Wang et al., 2017;</ref><ref type="bibr" target="#b8">Shen et al., 2017;</ref><ref type="bibr">Clark and Gardner, 2017)</ref> have been proposed to solve RC tasks based on the SQuAD ( <ref type="bibr" target="#b5">Rajpurkar et al., 2016)</ref> and Trivi- aQA ( <ref type="bibr">Joshi et al., 2017</ref>) datasets, reaching human level performance. A common approach in these models is to score and/or extract candidate spans conditioned on a given question-document pair.</p><p>Most of these models have limited applicability to real problems for the following reasons. They do not generalize well to scenarios where the an- swer is not present as a span, or where several dis- continuous parts of the document are required to * To whom correspondence should be addressed. form the answer. In addition, unlike humans, they can not easily skip through irrelevant parts to com- prehend long documents <ref type="bibr" target="#b3">(Masson, 1983)</ref>.</p><p>To address the issues above we develop a novel context zoom-in network (ConZNet) for RC tasks, which can skip through irrelevant parts of a doc- ument and generate an answer using only the rel- evant regions of text. The ConZNet architecture consists of two phases. In the first phase we iden- tify the relevant regions of text by employing a reinforcement learning algorithm. These relevant regions are not only useful to generate the answer, but can also be presented to the user as support- ing information along with the answer. The sec- ond phase is based on an encoder-decoder archi- tecture, which comprehends the identified regions of text and generates the answer by using a resid- ual self-attention network as encoder and a RNN- based sequence generator along with a pointer net- work ( <ref type="bibr" target="#b12">Vinyals et al., 2015)</ref> as the decoder. It has the ability to generate better well-formed answers not verbatim present in the document than span prediction models.</p><p>Recently, there have been several attempts to adopt condensing documents in RC tasks. <ref type="bibr" target="#b13">Wang et al. (2018)</ref> retrieve a relevant paragraph based on the question and predict the answer span. <ref type="bibr">Choi et al. (2017)</ref> select sentence(s) to make a summary of the entire document with a feed-forward net- work and generate an answer based on the sum- mary. Unlike existing approaches, our method has the ability to select relevant regions of text not just based on the question but also on how well regions are related to each other. Moreover, our decoder combines span prediction and sequence genera- tion. This allows the decoder to copy words from the relevant regions of text as well as to generate words from a fixed vocabulary.</p><p>We evaluate our model using one of the chal- lenging RC datasets, called 'NarrativeQA', which  Experimental results show the usefulness of our framework for RC tasks and we outperform state- of-the-art results on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed Architecture</head><p>An overview of our architecture is shown in <ref type="figure" target="#fig_0">Figure  1</ref>, which consists of two phases. First, the identi- fication of relevant regions of text is computed by the Co-attention and Context Zoom layers as ex- plained in Sections 2.1 and 2.2. Second, the com- prehension of identified regions of text and out- put generation is computed by Answer Genera- tion block as explained in Section 2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Co-attention layer</head><p>The words in the document, question and an- swer are represented using pre-trained word em- beddings ( <ref type="bibr" target="#b4">Pennington et al., 2014</ref>). These word- based embeddings are concatenated with their cor- responding char embeddings. The char embed- dings are learned by feeding all the characters of a word into a Convolutional Neural Network (CNN) <ref type="bibr" target="#b0">(Kim, 2014)</ref>. We further encode the document and question embeddings using a shared bi-directional GRU ( <ref type="bibr">Cho et al., 2014</ref>) to get context-aware rep- resentations.</p><p>We compute the co-attention between document and question to get question-aware representations for the document by using tri-linear attention as proposed by <ref type="bibr" target="#b7">Seo et al. (2017)</ref>. Let d i be the vector representation for the document word i, q j be the vector for the question word j, and l d and l q be the lengths of the document and question respectively. The tri-linear attention is calculated as</p><formula xml:id="formula_0">a ij = w d d i + w q q j + w dq (d i q j ),<label>(1)</label></formula><p>where w d , w q , and w dq are learnable parameters and denotes the element-wise multiplication.</p><p>We compute the attended document word˜dword˜ word˜d i by first computing λ i = sof tmax(a i: ) and followed by˜dby˜ by˜d i = lq j=1 λ ij q j . Similarly, we compute a question to document attention vector˜qvector˜ vector˜q by first computing b = sof tmax(max(a i: )) and followed by˜qby˜ by˜q</p><formula xml:id="formula_1">= l d i=i d i b i . Finally, d i , ˜ d i , d i ˜ d i , ˜ d i ˜ q are concatenated</formula><p>to yield a query-aware contextual representation for each word in the document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Context Zoom Layer</head><p>This layer finds relevant regions of text. We use reinforcement learning to do that, with the goal of improving answer generation accuracy -see Sec- tion 2.4.</p><p>The Split Context operation splits the attended document vectors into sentences or fixed size chunks (useful when sentence tokenization is not available for a particular language). This results in n text regions with each having length l k , where</p><formula xml:id="formula_2">l d = n k=1 l k .</formula><p>We then get the representations, denoted as z k , for each text region by running a BiGRU and concatenating the last states of the for- ward and backward GRUs.</p><p>The text region representations, z k , encode how well they are related to the question, and their sur- rounding context. Generating an answer may de- pend on multiple regions, and it is important for each text region to collect cues from other regions which are outside of their surroundings. We can compute this by using a Self-Attention layer. It is a special case of co-attention where both operands (d i and q j ) are the text fragment itself, computed by setting a ij = −∞ when i = j in Eq. 1.</p><p>These further self-attended text region represen- tations, ˜ z k , are passed through a linear layer with tanh activation and softmax layer as follows:</p><formula xml:id="formula_3">u = tanh(W c [˜ z 1 , · · · , ˜ z n ] + b c ), (2) ψ = sof tmax(u),<label>(3)</label></formula><p>where ψ is the probability distribution of text re- gions, which is the evidence used to generate the answer. The policy of the reinforcement learner is defined as π(r|u; θ z ) = ψ r , where ψ r is the probability of a text region r (agent's action) be- ing selected, u is the environment state as defined in Eq. 2, and θ z are the learnable parameters. Dur- ing the training time we sample text regions using ψ, in inference time we follow greedy evaluation by selecting most probable region(s).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Answer Generation</head><p>This component is implemented based on the encoder-decoder architecture of ( ). The selected text regions from the Con- text Zoom layer are given as input to the encoder, where its output is given to the decoder in order to generate the answer. The encoder block uses residual connected self- attention layer followed by a BiGRU. The se- lected relevant text regions (∈ ψ r ) are first passed through a separate BiGRU, then we apply a self- attention mechanism similar to the Context Zoom layer followed by a linear layer with ReLU activa- tions. The encoder's output consists of representa- tions of the relevant text regions, denoted by e i .</p><p>The decoder block is based on an attention mechanism ( <ref type="bibr">Bahdanau et al., 2015</ref>) and a copy mechanism by using a pointer network similar to ( <ref type="bibr" target="#b6">See et al., 2017)</ref>. This allows the decoder to pre- dict words from the relevant regions as well as from the fixed vocabulary. At time step t, the de- coder predicts the next word in the answer using the attention distribution, context vector and cur- rent word embedding. The attention distribution and context vector are obtained as follows: where h t is hidden state of the decoder, v, W e , W h , b o are learnable parameters. The γ t repre- sents a probability distribution over words of rel- evant regions e i . The context vector is given by c t = i γ t i e i . The probability distribution to predict word w t from the fixed vocabulary (P f v ) is computed by passing state h t and context vector c t to a linear layer followed by a softmax function denoted as</p><formula xml:id="formula_4">o t i = v T tanh(W e e i + W h h t + b o ),<label>(4)</label></formula><formula xml:id="formula_5">γ t = sof tmax(o t i ),<label>(5)</label></formula><formula xml:id="formula_6">P f v = sof tmax(W v (X v [h t , c t ] + b p ) + b q ). (6)</formula><p>To allow decoder to copy words from the en- coder sequence, we compute a soft gate (P copy ), which helps the decoder to generate a word by sampling from the fixed vocabulary or by copying from a selected text regions (ψ r ). The soft gate is calculated as</p><formula xml:id="formula_7">P copy = σ(w T p c t + v T h h t + w T x x t + b c ),<label>(7)</label></formula><p>where x t is current word embedding, h t is hidden state of the decoder, c t is the context vector, and w p , v h , w x , and b c are learnable parameters. We maintain a list of out-of-vocabulary (OOV) words for each document. The fixed vocabulary along with this OOV list acts as an extended vocabulary for each document. The final probability distribu- tion (unnormalized) over this extended vocabulary (P ev ) is given by</p><formula xml:id="formula_8">P ev (w t ) = (1 − P copy )P f v (w t ) + P copy i:w i =wt γ t i .<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Training</head><p>We jointly estimate the parameters of our model coming from the Co-attention, Context Zoom, and Answer Generation layers, which are denoted as θ a , θ z , and θ g respectively. Estimating θ a and θ g is straight-forward by using the cross-entropy ob- jective J 1 ({θ a , θ g }) and the backpropagation algo- rithm. However, selecting text regions in the Con- text Zoom layer makes it difficult to estimate θ z given their discrete nature. We therefore formu- late the estimation of θ z as a reinforcement learn- ing problem via a policy gradient method. Specif- ically, we design a reward function over θ z .</p><p>We use mean F-score of ROUGE-1, ROUGE-2, and ROUGE-L ( <ref type="bibr" target="#b2">Lin and Hovy, 2003)</ref> as our re- ward function R. The objective function to max- imize is the expected reward under the probabil- ity distribution of current text regions ψ r , i.e., J 2 (θ z ) = E p(r|θz) <ref type="bibr">[R]</ref>. We approximate the gra- dient θz J 2 (θ z ) by following the REINFORCE <ref type="bibr" target="#b15">(Williams, 1992)</ref> algorithm. To reduce the high variance in estimating θz J 2 (θ z ) one widely used mechanism is to subtract a baseline value from the reward. It is shown that any number will re- duce the variance <ref type="bibr" target="#b15">(Williams, 1992;</ref><ref type="bibr" target="#b17">Zaremba and Sutskever, 2015)</ref>, here we used the mean of the mini-batch reward b as our baseline. The final ob- jective is to minimize the following equation:</p><formula xml:id="formula_9">J(θ) = J 1 ({θ a , θ g })−J 2 (θ z )+ B i=1 (R i −b), (9)</formula><p>where, B is the size of mini-batch, and R i is the reward of example i ∈ B. J(θ) is now fully differ- entiable and we use backpropagation to estimate θ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>The NarrativeQA dataset <ref type="bibr" target="#b1">(Kočisk`Kočisk`y et al., 2017)</ref> consists of fictional stories gathered from books and movie scripts, where corresponding sum- maries and question-answer pairs are generated with the help of human experts and Wikipedia arti- cles. The summaries in NarrativeQA are 4-5 times longer than documents in the SQuAD dataset. Moreover, answers are well-formed by human ex- perts and are not verbatim in the story, thus mak- ing this dataset ideal for testing our model. The statistics of NarrativeQA are available in <ref type="table">Table 1</ref> 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Baselines</head><p>We compare our model against reported models in Kočisk` <ref type="bibr" target="#b1">Kočisk`y et al. (2017)</ref>  <ref type="figure">(Seq2Seq, ASR, BiDAF)</ref> and the Multi-range Reasoning Unit (MRU) in <ref type="bibr" target="#b11">Tay et al. (2018)</ref>. We implemented two baseline mod- els (Baseline 1, Baseline 2) with Context Zoom layer similar to <ref type="bibr" target="#b13">Wang et al. (2018)</ref>. In both base- lines we replace the span prediction layer with an answer generation layer. In Baseline 1 we use an 1 please refer Kočisk` <ref type="bibr" target="#b1">Kočisk`y et al. (2017)</ref> for more details attention based seq2seq layer without using copy mechanism in the answer generation unit similar to <ref type="bibr">Choi et al. (2017)</ref>. In Baseline 2 the answer generation unit is similar to our ConZNet archi- tecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Implementation Details</head><p>We split each document into sentences using the sentence tokenizer of the NLTK toolkit ( <ref type="bibr">Bird and Loper, 2004</ref>). Similarly, we further tokenize each sentence, corresponding question and answer us- ing the word tokenizer of NLTK. The model is implemented using <ref type="bibr">Python and Tensorflow (Abadi et al., 2015</ref>). All the weights of the model are initialized by Glorot Initialization (Glorot et al., 2011) and biases are initialized with zeros. We use a 300 dimensional word vectors from GloVe <ref type="bibr" target="#b4">(Pennington et al., 2014</ref>) (with 840 billion pre-trained vectors) to initialize the word embeddings, which we kept constant during training. All the words that do not appear in Glove are initialized by sam- pling from a uniform random distribution between [-0.05, 0.05]. We apply dropout ( <ref type="bibr" target="#b9">Srivastava et al., 2014</ref>) between the layers with keep probability of 0.8 (i.e dropout=0.2). The number of hidden units are set to 100. We trained our model with the AdaDelta (Zeiler, 2012) optimizer for 50 epochs, an initial learning rate of 0.1, and a minibatch size of 32. The hyperparameter 'sample size' (num- ber of relevant sentences) is chosen based on the model performance on the devset. <ref type="table" target="#tab_3">Table 2</ref> shows the performance of various models on NarrativeQA. It can be noted that our model with sample size 5 (choosing 5 relevant sentences) outperforms the best ROUGE-L score available so far by 12.62% compared to <ref type="bibr" target="#b11">Tay et al. (2018)</ref>. The low performance of Baseline 1 shows that the hybrid approach (ConZNet) for generating words from a fixed vocabulary as well as copying words from the document is better suited than span pre- diction models (Seq2Seq, ASR, BiDAF, MRU).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Results</head><p>To validate the importance of finding relevant sentences in contrast to using an entire document for answer generation, we experimented with sam- ple sizes beyond 5. The performance of our model gradually dropped from sample size 7 onwards. This result shows evidence that only a few rele- vant sentences are sufficient to answer a question.</p><p>We also experimented with various sample sizes to see the effect of intra sentence relations for an-  swer generation. The performance of the model improved dramatically with sample sizes 3 and 5 compared to the sample size of 1. These results show that the importance of selecting multiple rel- evant sentences for generating an answer. In ad- dition, the low performance of Baseline 2 indi- cates that just selecting multiple sentences is not enough, they should also be related to each other. This result points out that the self-attention mech- anism in the Context zoom layer is an important component to identify related relevant sentences.</p><formula xml:id="formula_10">Model BLEU-1 BLEU-4 ROUGE-L METEOR</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We have proposed a new neural-based architecture which condenses an original document to facili- tate fast comprehension in order to generate better well-formed answers than span based prediction models. Our model achieved the best performance on the challenging NarrativeQA dataset. Future work can focus for example on designing an inex- pensive preprocess layer, and other strategies for improved performance on answer generation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The proposed ConZNet architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 : Performance of various models on NarrativeQA dataset (SS=sample size ≡ number of relevant sentences)</head><label>2</label><figDesc></figDesc><table></table></figure>

			<note place="foot">Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 1601-1611.</note>
		</body>
		<back>
			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-25" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">`</forename><surname>Kočisk`y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gábor</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.07040</idno>
		<title level="m">The narrativeqa reading comprehension challenge</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic evaluation of summaries using n-gram cooccurrence statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="71" to="78" />
		</imprint>
	</monogr>
	<note>NAACL &apos;03</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Conceptual processing of text during skimming and rapid sequential reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Masson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Memory &amp; Cognition</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="262" to="274" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointergenerator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1073" to="1083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Reasonet: Learning to stop reading in machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1047" to="1055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Multi-range reasoning for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu Cheung</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hui</surname></persName>
		</author>
		<idno>abs/1803.09074</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Pointer networks</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<title level="m">R$ˆ3$: Reinforced reader-ranker for open-domain question answering</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gated self-matching networks for reading comprehension and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="189" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dynamic coattention networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Reinforcement learning neural turing machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>abs/1505.00521</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
