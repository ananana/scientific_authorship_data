<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:32+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Experiments with Generative Models for Dependency Tree Linearization</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Futrell</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Brain and Cognitive Sciences</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Gibson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Brain and Cognitive Sciences</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Experiments with Generative Models for Dependency Tree Linearization</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present experiments with generative models for linearization of unordered labeled syntactic dependency trees (Belz et al., 2011; Rajkumar and White, 2014). Our linearization models are derived from generative models for dependency structure (Eisner, 1996). We present a series of generative dependency models designed to capture successively more information about ordering constraints among sister dependents. We give a dynamic programming algorithm for computing the conditional probability of word orders given tree structures under these models. The models are tested on corpora of 11 languages using test-set likelihood, and human ratings for generated forms are collected for En-glish. Our models benefit from representing local order constraints among sisters and from backing off to less sparse distributions , including distributions not conditioned on the head.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We explore generative models for producing lin- earizations of unordered labeled syntactic depen- dency trees. This specific task has attracted at- tention in recent years ( <ref type="bibr" target="#b8">Filippova and Strube, 2009;</ref><ref type="bibr" target="#b11">He et al., 2009;</ref><ref type="bibr" target="#b1">Belz et al., 2011;</ref><ref type="bibr" target="#b3">Bohnet et al., 2012;</ref><ref type="bibr" target="#b18">Zhang, 2013)</ref> because it forms a useful part of a natural language generation pipeline, especially in machine translation <ref type="bibr" target="#b5">(Chang and Toutanova, 2007)</ref> and summarization <ref type="bibr" target="#b0">(Barzilay and McKeown, 2005</ref>). Closely related tasks are generation of sentences given CCG parses ( <ref type="bibr" target="#b17">White and Rajkumar, 2012</ref>), bags of words ( <ref type="bibr" target="#b14">Liu et al., 2015)</ref>, and semantic graphs ( <ref type="bibr" target="#b4">Braune et al., 2014</ref>).</p><p>Here we focus narrowly on testing probabilistic generative models for dependency tree lineariza- tion. In contrast, the approach in most previ- ous work is to apply a variety of scoring func- tions to trees and linearizations and search for an optimally-scoring tree among some set. The prob- abilistic linearization models we investigate are derived from generative models for dependency trees <ref type="bibr" target="#b6">(Eisner, 1996)</ref>, as most commonly used in unsupervised grammar induction <ref type="bibr" target="#b13">(Klein and Manning, 2004;</ref><ref type="bibr" target="#b10">Gelling et al., 2012)</ref>. Generative de- pendency models have typically been evaluated in a parsing task <ref type="bibr" target="#b7">(Eisner, 1997)</ref>. Here, we are interested in the inverse task: inferring a distri- bution over linear orders given unordered depen- dency trees. This is the first work to consider generative de- pendency models from the perspective of word ordering. The results can potentially shed light on how ordering constraints are best represented in such models. In addition, the use of proba- bilistic models means that we can easily define well-motivated normalized probability distribu- tions over orders of dependency trees. These dis- tributions are useful for answering scientific ques- tions about crosslinguistic word order in quan- titative linguistics, where obtaining robust esti- mates has proven challenging due to data sparsity ( <ref type="bibr" target="#b9">Futrell et al., 2015)</ref>.</p><p>The remainder of the work is organized as fol- lows. In Section 2 we present a set of generative linearization models. In Section 3 we compare the performance of the different models as mea- sured by test-set probability and human accept- ability ratings. We also compare our performance with other systems from the literature. Section 4 concludes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Generative Models for Projective Dependency Tree Linearization</head><p>We investigate head-outward projective generative dependency models. In these models, an ordered dependency tree is generated by the following kind  <ref type="formula" target="#formula_1">(2)</ref> From the AP comes this story. Order 2 is the original order in the corpus, but order 1 is much more likely under our models.</p><p>of procedure. Given a head node, we use some generative process G to generate a depth-1 sub- tree rooted in that head node. Then we apply the procedure recursively to each of the depen- dent nodes. By applying the procedure starting at a ROOT node, we generate a dependency tree. For example, to generate the dependency tree in <ref type="figure">Fig</ref> , and so on. In this work, we experiment with differ- ent specific generative processes G which generate a local subtree conditioned on a head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model Types</head><p>Here we describe some possible generative pro- cesses G which generate subtrees conditioned on a head. These models contain progressively more information about ordering relations among sister dependents.</p><p>A common starting point for G is Eisner Model C <ref type="bibr" target="#b6">(Eisner, 1996)</ref>. In this model, dependents on one side of the head are generated by repeatedly sampling from a categorical distri- bution until a special stop-symbol is generated. The model only captures the propensity of depen- dents to appear on the left or right of the head, and does not capture any order constraints between sister dependents on one side of the head.</p><p>We consider a generalization of Eisner Model C which we call Dependent N-gram models. In a Dependent N-gram model, we generate depen- dents on each side the head by sampling a se- quence of dependents from an N-gram model. Each dependent is generated conditional on the N − 1 previously generated dependents from the head outwards. We have two separate N- gram sequence distributions for left and right dependents. Eisner Model C can be seen as a Dependent N-gram model with N = 1.</p><p>We also consider a model which can capture many more ordering relations among sister depen- dents: given a head h, sample a subtree whose head is h from a Categorical distribution over sub- trees. We call this the Observed Orders model because in practice we are simply sampling one of the observed orders from the training data. This generative process has the capacity to capture the most ordering relations between sister dependents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Distributions over Permutations of Dependents</head><p>We have discussed generative models for ordered dependency trees. Here we discuss how to use them to make generative models for word orders conditional on unordered dependency trees.</p><p>Suppose we have a generative process G for de- pendency trees which takes a head h and gener- ates a sequence of dependents w l to the left of h and a sequence of dependents w r to the right of h. Let w denote the pair (w l , w r ), which we call the configuration of dependents. To get the probabil- ity of some w given an unordered subtree u, we want to calculate the probability of w given that G has generated the particular multiset W of depen- dents corresponding to u. To do this, we calculate:</p><formula xml:id="formula_0">p(w|W) = p(w, W) p(W) = p(w) Z ,<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">Z = w ∈W p(w )<label>(2)</label></formula><p>and W is the set of all possible configurations (w l , w r ) compatible with multiset W. That is, W is the set of pairs of permutations of multisets W l and W r for all possible partitions of W into W l and W r . The generative dependency model gives us the probability p(w). It remains to calculate the normalizing constant Z, the sum of probabilities of possible configura- tions. For the Observed Orders model, Z is the sum of probabilities of subtrees with the same de- pendents as subtree u. For the Dependent N-gram models of order N , we calculate Z using a dy- namic programming algorithm, presented in Al-gorithm 1 as memoized recursive functions. When N = 1 (Eisner Model C), Z is more simply:</p><formula xml:id="formula_2">Zemc = pL(stop) × pR(stop) × (W l ,Wr )∈PARTS(W) |W l |! × |Wr|! × w∈W l pL(w) w∈Wr pR(w),<label>(3)</label></formula><p>where PARTS(W) is the set of all partitions of multiset W into two multisets W l and W r , p L is the probability mass function for a dependent to the left of the head, p R is the function for a depen- dent to the right, and stop is a special symbol in the support of p L and p R which indicates that gen- eration of dependents should halt. The probability mass functions may be conditional on the head h. These methods for calculating Z make it possible to transform a generative dependency model into a model of dependency tree ordering conditional on local subtree structure.  </p><formula xml:id="formula_3">1 Z ← Z + pL(ri|c) × LEFT NORM(r , c ) end for return Z end memoized function Result is LEFT NORM(W, [start])</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Labelling</head><p>The previous section discussed the question of the structure of the generative process for depen- dency trees. Here we discuss an orthogonal mod- eling question, which we call labelling: what in- formation about the labels on dependency tree nodes and edges should be included in our mod- els. Dependency tree nodes are labeled with word- forms, lemmas, and parts-of-speech (POS) tags; and dependency tree edges are labeled with rela- tion types. A model might generate orders of de- pendents conditioned on all of these labels, or a subset of them. For example, a generative depend- necy model might generate (relation type, depen- dent POS tag) tuples conditioned on the POS tag of the head of the phrase. When we use such a model for dependency linearization, we would say the model's labelling is relation type, dependent POS, and head POS. In this study, we avoid in- cluding wordforms or lemmas in the labelling, to avoid data sparsity issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Model Estimation and Smoothing</head><p>In order to alleviate data sparsity in fitting our models, we adopt two smoothing methods from the language modelling literature.</p><p>All categorical distributions are estimated us- ing add-k smoothing where k = 0.01. For the Dependent N-gram models, this means adding k pseudocounts for each possible dependent in each context. For the Observed Orders model, this means adding k pseudocounts for each possible permutation of the head and its dependents.</p><p>We also experiment with combining our mod- els into mixture distributions. This can be viewed as a kind of back-off smoothing <ref type="bibr" target="#b12">(Katz, 1987)</ref>, where the Observed Orders model is the model with the most context, and Dependent N-grams and Eisner Model C are backoff distributions with successively less context. Similarly, models with less information in the labelling can serve as back- off distributions for models with more information in the labelling. For example, a model which is conditioned on the POS of the head can be backed off to a model which does not condition on the head at all. We find optimal mixture weights us- ing the Baum-Welch algorithm tuned on a held-out development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head><p>Here we empirically evaluate some options for model type and model labelling as described above. We are interested in how many of the pos- sible orders of a sentence our model can generate (recall), and in how many of our generated orders really are acceptable (precision). As a recall-like measure, we quantify the probability of the word orders of held-out test sentences. Low probabil-  ities assigned to held-out sentences indicate that there are possible orders which our model is miss- ing. As a precision-like measure, we get human acceptability ratings for sentence reorderings gen- erated by our model. We carry out our evaluations using the de- pendency corpora of the Universal Dependen- cies project (v1.1) <ref type="bibr">(Agi´cAgi´c et al., 2015)</ref>, with the train/dev/test splits provided in that dataset. We remove nodes and edges dealing with punctuation. Due to space constraints, we only present results from 11 languages here.</p><note type="other">Labelling Model Basque Czech English Finnish French German Hebrew Indonesian Persian Spanish</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Test-Set Probability</head><p>Here we calculate average probabilities of word orders per sentence in the test set. This number can be interpreted as the (negative) average amount of information contained in the word order of a sen- tence beyond information about dependency rela- tions.</p><p>The results for selected languages are shown in <ref type="table" target="#tab_1">Table 1</ref>. The biggest gains come from us- ing Dependent N-gram models with N &gt; 1, and from backing off the model labelling. The Observed Orders model does poorly on its own, likely due to data sparsity; its performance is much improved when backing off from condition- ing on the head. Eisner Model C (n1) also per- forms poorly, likely because it cannot represent any ordering constraints among sister dependents. The fact it helps to back off to distributions not conditioned on the head suggests that there are commonalities among distributions of dependents of different heads, which could be exploited in fur- ther generative dependency models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Human Evaluation</head><p>We collected human ratings for sentence reorder- ings sampled from the English models from 54 na- tive American English speakers on Amazon Me- chanical Turk. We randomly selected a set of 90 sentences from the test set of the English Universal Dependencies corpus. We generated a reordering of each sentence according to each of 12 model configurations in <ref type="table" target="#tab_1">Table 1</ref>. Each participant saw an original sentence and a reordering of it, and was asked to rate how natural each version of the sentence sounded, on a scale of 1 to 5. The or- der of presentation of the original and reordered forms was randomized, so that participants were not aware of which form was the original and which was a reordering. Each participant rated 56 sentence pairs. Participants were also asked whether the two sentences in a pair meant the same thing, with "can't tell" as a possible answer. <ref type="table">Table 2</ref> shows average human acceptability rat- ings for reorderings, and the proportion of sen- tence pairs judged to mean the same thing. The original sentences have an average acceptability rating of 4.48/5. The very best performing models are those which do not back off to a distribution not conditioned on the head. However, in the case of the Observed Orders and other sparse models, we see consistent improvement from this backoff. <ref type="figure">Figure 2</ref> shows the acceptability ratings (out of 5) plotted against test set probability. We see that  <ref type="table">Table 2</ref>: Mean acceptability rating out of 5, and proportion of reordered sentences with the same meaning as the original, for English models. La- bels as in <ref type="table" target="#tab_1">Table 1</ref>. Figure 2: Comparison of test set probability (Ta- ble 1) and acceptability ratings <ref type="table">(Table 2)</ref> for En- glish across models. A least-squares linear regres- sion line is shown. Labels as in <ref type="table" target="#tab_1">Table 1.</ref> the models which yield poor test set probability also have poor acceptability ratings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison with other systems</head><p>Previous work has focused on the ability to cor- rectly reconstruct the word order of an observed dependency tree. Our goal is to explicitly model a distribution over possible orders, rather than to re- cover a single correct order, because many orders are often possible, and the particulator order that a dependency tree originally appeared in might not be the most natural. For example, our models typ- ically reorder the sentence "From the AP comes this story" (in <ref type="figure" target="#fig_0">Figure 1)</ref> as "This story comes from the AP"; the second order is arguably more natu- ral, though the first is idiomatic for this particular phrase. So we do not believe that BLEU scores and other metrics of similarity to a "correct" or- dering are particularly relevant for our task. Previous work uses BLEU scores ( <ref type="bibr" target="#b15">Papineni et al., 2002</ref>) and human ratings to evaluate genera- tion of word orders. To provide some comparabil- ity with previous work, we report BLEU scores on the 2011 Shared Task data here. The systems re- ported in <ref type="bibr" target="#b1">Belz et al. (2011)</ref> achieve BLEU scores ranging from 23 to 89 for English; subsequent work achieves BLEU scores of 91.6 on the same data ( <ref type="bibr" target="#b3">Bohnet et al., 2012)</ref>. Drawing the highest- probability orderings from our models, we achieve a top BLEU score of 57.7 using the model config- uration hdr/oo. Curiously, hdr/oo is typically the worst model configuration in the test set probabil- ity evaluation (Section 3.1). The BLEU perfor- mance is in the middle range of the Shared Task systems. The human evaluation of our models is more optimistic: the best score for Meaning Sim- ilarity in the Shared Task was 84/100 <ref type="bibr" target="#b2">(Bohnet et al., 2011</ref>), while sentences ordered according to our models were judged to have the same meaning as the original in 85% of cases <ref type="table">(Table 2)</ref>, though these figures are based on different data. These comparisons suggest that these generative models do not provide state-of-the-art performance, but do capture some of the same information as previous models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Discussion</head><p>Overall, the most effective models are the Dependent N-gram models. The naive approach to modeling order relations among sister depen- dents, as embodied in the Observed Orders model, does not generalize well. The result suggests that models like the Dependent N-gram model might be effective as generative dependency models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We have discussed generative models for depen- dency tree linearization, exploring a path less trav- eled by in the dependency linearization literature. We believe this approach has value for answering scientific questions in quantitative linguistics and for better understanding the linguistic adequacy of generative dependency models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example unordered dependency tree. Possible linearizations include (1) This story comes from the AP and (2) From the AP comes this story. Order 2 is the original order in the corpus, but order 1 is much more likely under our models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>- ure 1 from the node comes down, we take the head comes and generate the subtree comes story AP nsubj nmod , then we take the head story and generate story this det</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1</head><label>1</label><figDesc>Compute the sum of probabilities of all configurations of dependents W under a Dependent N-gram model with two component N- gram models of order N : p R for sequences to the right of the head and p L for sequences to the left.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>memoized function RIGHT NORM(r, c) if |r| = 0 then return pR(stop | c) end if Z ← 0 for i = 1 : |r| do r ← elements of r except the ith c ← append ri to c then truncate to length N − 1 Z ← Z + pR(ri|c) × RIGHT NORM(r , c ) end for return Z end memoized function memoized function LEFT NORM(r, c) Z ← pL(stop | c) × RIGHT NORM([start], r) for i = 1 : |r| do r ← elements of r except the ith c ← append ri to c then truncate to length N −</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Average log likelihood of word order per sentence in test set under various models. Under 
"Labelling", HDR means conditioning on Head POS, Dependent POS, and Relation Type, and R means 
conditioning on Relation Type alone (see Section 2.2). Under "Model", oo is the Observed Orders model, 
n1 is the Dependent 1-gram model (Eisner Model C), n2 is the Dependent 2-gram model, and n3 is the 
Dependent 3-gram model (see Section 2.1). In both columns, x+y means a mixture of model x and 
model y; n123 means n1+n2+n3. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank William P. Li, Kyle Mahowald, and Tim O'Donnell for helpful discussions, and Michael White for help accessing data.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sentence fusion for multidocument news summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kathleen R Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="328" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The first surface realisation shared task: Overview and evaluation results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anja</forename><surname>Belz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominic</forename><surname>Espinosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Kow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deirdre</forename><surname>Hogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Stent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th European Workshop on Natural Language Generation</title>
		<meeting>the 13th European Workshop on Natural Language Generation</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="217" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">StuMaBa : from deep representation to surface</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Mille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoˆıtbenoˆıt</forename><surname>Favre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Wanner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th European workshop on natural language generation</title>
		<meeting>the 13th European workshop on natural language generation</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="232" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generating non-projective word order in statistical linearization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Björkelund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="928" to="939" />
		</imprint>
	</monogr>
	<note>Wolfgang Seeker, and Sina Zarrieß</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mapping between english strings and reentrant semantic graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabienne</forename><surname>Braune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Language Resources and Evaluation (LREC)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A discriminative syntactic word order model for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pi-Chuan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>page 9</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Three new probabilistic models for dependency parsing: An exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference on Computational Linguistics</title>
		<meeting>the 16th Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="340" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">An empirical comparison of probability models for dependency grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eisner</surname></persName>
		</author>
		<idno>96-11</idno>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
<note type="report_type">IRCS Report</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Tree linearization in English: Improving language model based approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Filippova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Quantifying word order freedom in dependency corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Futrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Mahowald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Gibson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Conference on Dependency Linguistics</title>
		<meeting>the Third International Conference on Dependency Linguistics<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The pascal challenge on grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Gelling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Graça</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL-HLT Workshop on the Induction of Linguistic Structure</title>
		<meeting>the NAACL-HLT Workshop on the Induction of Linguistic Structure</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="64" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dependency based Chinese sentence realization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Estimation of probabilities from sparse data for the language model component of a speech recognizer. Acoustics, Speech and Signal Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Slava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="400" to="401" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Corpusbased induction of syntactic structure: Models of dependency and constituency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 42nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">478</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Transition-based syntactic linearization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Better surface realization through psycholinguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajakrishnan</forename><surname>Rajkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language and Linguistics Compass</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="428" to="448" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Minimal dependency length in realization ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajakrishnan</forename><surname>Rajkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="244" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Partial-tree linearization: generalized word ordering for text synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Third international joint conference on Artificial Intelligence</title>
		<meeting>the Twenty-Third international joint conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2232" to="2238" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
