<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Better Embeddings for Rare Words Using Distributional Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Sergienya</surname></persName>
							<email>sergienya@cis.lmu.de</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing</orgName>
								<orgName type="institution">University of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing</orgName>
								<orgName type="institution">University of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sch¨</forename><surname>Schütze</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing</orgName>
								<orgName type="institution">University of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Better Embeddings for Rare Words Using Distributional Representations</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>There are two main types of word representations: low-dimensional embeddings and high-dimensional distributional vectors , in which each dimension corresponds to a context word. In this paper, we initialize an embedding-learning model with distributional vectors. Evaluation on word similarity shows that this initialization significantly increases the quality of embed-dings for rare words.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Standard neural network (NN) architectures for in- ducing embeddings have an input layer that repre- sents each word as a one-hot vector (e.g., <ref type="bibr" target="#b21">Turian et al. (2010)</ref>, <ref type="bibr" target="#b6">Collobert et al. (2011)</ref>, <ref type="bibr" target="#b13">Mikolov et al. (2013)</ref>). There is no usable information avail- able in this input-layer representation except for the identity of the word. We call this standard ini- tialization method one-hot initialization.</p><p>Distributional representations (e.g., <ref type="bibr" target="#b20">Schütze (1992)</ref>, <ref type="bibr" target="#b11">Lund and Burgess (1996)</ref>, <ref type="bibr" target="#b18">Sahlgren (2008)</ref>, <ref type="bibr" target="#b23">Turney and Pantel (2010)</ref>, <ref type="bibr" target="#b0">Baroni and Lenci (2010)</ref>) represent a word as a high- dimensional vector in which each dimension cor- responds to a context word. They have been suc- cessfully used for a wide variety of tasks in natu- ral language processing such as phrase similarity <ref type="bibr" target="#b15">(Mitchell and Lapata, 2010)</ref> and sentiment analy- sis ( <ref type="bibr" target="#b22">Turney and Littman, 2003)</ref>.</p><p>In this paper, we investigate distributional ini- tialization: the use of distributional vectors as rep- resentations of words at the input layer of NN ar- chitectures for embedding learning to improve the embeddings of rare words. It is difficult for one- hot initialization to learn good embeddings from only a few examples. In contrast, distributional initialization provides an additional source of in- formation -the global distribution of the word in the corpus -that improves embeddings learned for rare words. We will demonstrate this type of im- provement in the experiments reported below.</p><p>In summary, we introduce the idea of dis- tributional initialization for embedding learn- ing, an alternative to one-hot initialization that combines distributed representations (or embed- dings) with distributional representations (or high- dimensional vectors). We show that distributional initialization significantly improves the quality of embeddings learned for rare words.</p><p>We will first describe our methods in Section 2 and the experimental setup in Section 3. Section 4 presents and discusses experimental results. We summarize related work in Section 5 and finish with conclusion in Section 6 and discussion of fu- ture work in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Weighting. We use two different weighting schemes for distributional vectors. Let v 1 , . . . , v n be the vocabulary of context words. In BINARY weighting, entry 1 ≤ i ≤ n in the distributional vector of target word w is set to 1 iff v i and w cooccur at a distance of at most ten words in the corpus and to 0 otherwise.</p><p>In PPMI weighting, entry 1 ≤ i ≤ n in the distributional vector of target word w is set to the PPMI (positive pointwise mutual information, in- troduced by <ref type="bibr" target="#b16">Niwa and Nitta (1994)</ref>) of w and v i . We divide PPMI values by their maximum to en- sure they are in <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> because we will combine one-hot vectors (whose values are 0/1) with PPMI weights and it is important that they are on the same scale.</p><p>We use two different distributional initializa- tions, shown in <ref type="figure" target="#fig_0">Figure 1</ref>: separate (left) and mixed (right). Combinations of these two initializations with both BINARY and PPMI weighting will be investigated in the experiments.</p><p>Recall that n is the dimensionality of the distri-k n freq. words butional vectors. Let k be the number of words with frequency &gt; θ, where the frequency thresh- old θ is a parameter.</p><formula xml:id="formula_0">                 w 1 1 0 w 2 1 w 3 1 . . . . . . w k 1 rare words      w k+1 0 1 1 0 · · · 0 1 . . . . . . w n 0 1 0 · · · 1 1 k + n k n − k freq. words                  w 1 1 0 w 2 1 w 3 1 . . . . . . w k 1 rare words      w k+1 0 1 1 · · · 0 1 1 · · · 0 . . . . . . . . . w n 0 0 1 · · · 1 1 0 · · · 1 n</formula><p>In separate initialization, the input represen- tation for a word is the concatenation of a k- dimensional vector and an n-dimensional vec- tor. For a word with frequency &gt; θ, the k- dimensional vector is a one-hot vector and the n- dimensional vector is zero. For a word with fre- quency ≤ θ, the k-dimensional vector is zero and the n-dimensional vector is its distributional vec- tor.</p><p>In mixed initialization, the input representation for a word is an n-dimensional vector: a one-hot vector for a word with frequency &gt; θ and a distri- butional vector for a word with frequency ≤ θ.</p><p>In summary, separate initialization uses sepa- rate representation spaces for frequent words (one- hot space) and rare words (distributional space). Mixed initialization uses the same representation space for all words; and rare words share weights with the frequent words that they cooccur with.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental setup</head><p>We use ukWaC+WaCkypedia ( <ref type="bibr" target="#b1">Baroni et al., 2009</ref>), a corpus of 2.4 billion tokens and 6 million word types. Based on ( <ref type="bibr" target="#b21">Turian et al., 2010)</ref>, we preprocess the corpus by removing sentences that are less than 90% lowercase; lowercasing; replac- ing URLs, email addresses and digits by special tokens; tokenization <ref type="bibr" target="#b19">(Schmid, 2000</ref>); replacing words with frequency 1 with &lt;unk&gt;; and adding end-of-sentence tokens. After preprocessing, the size n of the context word vocabulary is 2.7 mil- lion.</p><p>We evaluate on six word similarity judgment data sets (number of pairs in parentheses): RG Our goal in this paper is to investigate the ef- fect of using distributional initialization vs. one- hot initialization on the quality of embeddings of rare words.</p><p>However, except for RW, the six data sets con- tain only a single word with frequency ≤100, all other words are more frequent.</p><p>To address this issue, we artificially make all words in the six data sets rare. We do this by keeping only θ randomly chosen occurrences in the corpus (for words with frequency &gt;θ) and re- placing all other occurrences with a different to- ken (e.g., "fire" is replaced with "*fire*"). This procedure -corpus downsampling -ensures that all words in the six data sets are rare in the corpus and that our setup directly evaluates the impact of distributional initialization on rare words.</p><p>Note that we use θ for two different purposes: (i) θ is the frequency threshold that determines which words are classified as rare and which as frequent in <ref type="figure" target="#fig_0">Figure 1</ref> -changing θ corresponds to moving the horizontal dashed line in separate and mixed initialization up and down; (ii) θ is the pa- rameter that determines how many occurrences of a word are left in the corpus when we remove oc-  <ref type="table">Table 1</ref>: Spearman correlation coefficients ×100 between human and embedding-based similarity judgments, averaged over 5 runs. Distributional initialization correlations that are higher (resp. significantly higher) than corresponding one-hot correlations are set in bold (resp. marked *).</p><formula xml:id="formula_1">A B C D E F G H I J K L RG MC MEN WS RW SL θ mixed</formula><p>currences to ensure that words from the evaluation data sets are rare in the corpus. We covary these two parameters in the experi- ments below; e.g., we apply distributional initial- ization with θ = 20 to a corpus constructed to have θ = 20 occurrences of words from similarity data sets. We do this to ensure that all evaluation words are rare words for the purpose of distributional ini- tialization and so we can exploit all pairs in the evaluation data sets for evaluating the efficacy of our method for rare words.</p><p>We modified word2vec <ref type="bibr">5 (Mikolov et al., 2013</ref>) to accommodate distributional initialization; to support distributional vectors at the input layer, we changed the implementation of activation func- tions and backpropagation. We use the skipgram model, hierarchical softmax, set the size of the context window to 10 (10 words to the left and 10 to the right), min-count to 1 (train on all tokens), embedding size to 100, sampling rate to 10 −3 and train models for one epoch.</p><p>For four values of the frequency threshold, θ ∈ {10, 20, 50, 100}, <ref type="bibr">6</ref> we train word2vec models <ref type="bibr">5</ref> code.google.com/p/word2vec 6 A reviewer asks whether the value of θ should depend on the size of the training corpus. Our intuition is that it is in- dependent of corpus size. If a certain amount of information -corresponding to a certain number of contexts -is required to learn a meaningful representation of a word, then it should not matter whether that given number of contexts occurs in a small corpus or in a large corpus. However, if the contexts themselves contain many rare words (which is more likely in a small corpus), then corpus size could be an important vari- with one-hot initialization and with the four com- binations of weighting (BINARY, PPMI) and dis- tributional initialization (mixed, separate), a total of 4 × (1 + 2 × 2) = 20 models. For each train- ing run, we perform corpus downsampling and ini- tialize the parameters of the models randomly. To get a reliable assessment of performance, we train 5 instances of each model and report averages of the 5 runs. One model takes ∼3 hours to train on 23 CPU cores, 2.30GHz. <ref type="table">Table 1</ref> shows experimental results, averaged over 5 runs. The evaluation measure is Spearman correlation ×100 between human and machine- generated pair similarity judgments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental results and discussion</head><p>Frequency threshold θ. The main result is that for θ ∈ {10, 20} distributional initialization is better than one-hot initialization (see bold num- bers): compare lines 1&amp;5 with line 9; and lines 2&amp;6 with line 10. This is true for both mixed and separate initialization, with the exception of WS, for which mixed (column G) is better in only 1 (line 5) of 4 cases.</p><p>Looking only at results for θ ∈ {10, 20}, 18 of 24 improvements are significant 7 for mixed initial- ization and 16 of 24 improvements are significant for separate initialization (lines 1&amp;5 vs 9 and lines able to take into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2&amp;6 vs 10).</head><p>For θ ∈ {50, 100}, mixed initialization does well for RG, MC and SL, but the gap between mixed and one-hot initializations is generally smaller for these larger values of θ; e.g., the dif- ference is larger than 9 for θ = 10 (A1&amp;A5 vs A/B9, C1&amp;C5 vs C/D9, K1&amp;K5 vs K/L9) and less than 9 for θ = 100 (A4&amp;A8 vs A/B12, C4&amp;C8 vs C/D12, K4&amp;K8 vs K/L12) for these three data sets.</p><p>Recall that each value of θ effectively results in a different training corpus -a training corpus in which the number of occurrences of the words in the evaluation data sets has been reduced to ≤ θ (cf. Section 3).</p><p>Our results indicate that distributional initializa- tion is beneficial for very rare words -those that occur no more than 20 times in the corpus. Our results for medium rare words -those that occur between 50 and 100 times -are less clear: either there are no improvements or improvements are small.</p><p>Thus, our recommendation is to use θ = 20. Scalability. The time complexity of the ba- sic version of word2vec is O(ECW D log V ) ( <ref type="bibr" target="#b13">Mikolov et al., 2013</ref>) where E is the number of epochs, C is the corpus size, W is the con- text window size, D is the number of dimensions of the embedding space, and V is the vocabu- lary size. Distributional initialization adds a term I, the average number of entries in the distribu- tional vectors, so that time complexity increases to O(IECW D log V ). For rare words, I is small, so that there is no big difference in efficiency between one-hot initialization and distributional initializa- tion of word2vec. However, for frequent words I would be large, so that distributional initializa- tion may not be scalable in that case. So even if our experiments had shown that distributional ini- tialization helps for both rare and frequent words, scalability would be an argument for only using it for rare words.</p><p>Binary vs. PPMI. PPMI weighting is almost al- ways better than BINARY, with three exceptions (I8, L7, L8) where the difference between the two is small and not significant. The probable explana- tion is that the PPMI weights in <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> convey de- tailed, graded information about the strength of as- sociation between two words, taking into account their base frequencies. In contrast, the BINARY weights in {0, 1} only indicate if there was any in- stance of cooccurrence at all -without considering frequency of cooccurrence and without normaliz- ing for base frequencies.</p><p>Mixed vs. Separate. Mixed initialization is less variable and more predictable than separate initial- ization: performance for mixed initialization al- ways goes up as θ increases, e.g., 56.54 → 59.08 → 63.20 → 68.33 (column A, lines 1-4). In con- trast, separate initialization performance often de- creases, e.g., from 47.06 to 45.31 (column B, lines 1-2) when θ is increased. Since more informa- tion (more occurrences of the words that simi- larity judgments are computed for) should gener- ally not have a negative effect on performance, the only explanation is that separate is more variable than mixed and that this variability sometimes re- sults in decreased performance. <ref type="figure" target="#fig_0">Figure 1</ref> explains this difference between the two initializations: in mixed initialization (right panel), rare words are tied to frequent words, so their representations are smoothed by representations learned for frequent words. In separate initialization (left panel), no such links to frequent words exist, resulting in higher variability.</p><p>Because of its lower variability, our experiments suggest that mixed initialiation is a better choice than separate initialization.</p><p>One-hot vs. Distributional initialization. Our experiments show that distributional representa- tion is helpful for rare words. It is difficult for one-hot initialization to learn good embeddings for such words, based on only a small number of contexts in the corpus. In such cases, distribu- tional initialization makes the learning task easier since in addition to the contexts of the rare word, the learner now also has access to the global dis- tribution of the rare word and can take advantage of weight sharing with other words that have sim- ilar distributional representations to smooth em- beddings systematically.</p><p>Thus, distributional initialization is a form of smoothing: the embedding of a rare word is tied to the embeddings of other words via the links shown in <ref type="figure" target="#fig_0">Figure 1</ref>: the 1s in the lower "rare words" part of the illustrations for separate and mixed initial- ization. As is true for smoothing in general, pa- rameter estimates for frequent events benefit less from smoothing or can even deteriorate. In con- trast, smoothing is essential for rare events. Where the boundary lies between rare and frequent events depends on the specifics of the problem and the smoothing method used and is usually an empiri- cal question. Our results indicate that that bound- ary lies somewhere between 20 and 50 in our set- ting. <ref type="bibr">8</ref> Variance of results. <ref type="table">Table 1</ref> shows averages of five runs. The variance of results was quite high for low-performing models. For higher perform- ing models -those with values ≥ 40 -the ra- tio of standard deviation divided by mean ranged from .005 to .29. The median was .044. While the variance from run to run is quite high for low- performing models and for a few high-performing models, the significance test takes this into ac- count, so that the relatively high variability does not undermine our results.</p><p>In summary, we have shown that distributional initialization improves the quality of word embed- dings for rare words. Our recommendation is to use mixed initialization with PPMI weighting and the value θ = 20 of the frequency threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related work</head><p>An alternative to using distributional information for initialization is to use syntactic and semantic information for initialization. Approaches along these lines include <ref type="bibr" target="#b4">Botha and Blunsom (2014)</ref> who represent a word as a sum of embedding vec- tors of its morphemes. <ref type="bibr" target="#b7">Cui et al. (2014)</ref> use a weighted average of vectors of morphologically similar words.  extend a word's vector with vectors of entity categories and POS tags. This line of work also is partially motivated by improving the embeddings of rare words. Dis- tributional information on the one hand and syn- tactic/semantic information on the other hand are likely to be complementary, so that a combination of our approach with this prior work is promising. <ref type="bibr" target="#b10">Le et al. (2010)</ref> propose three schemes to ad- dress word embedding initialization. Reinitializa- tion and iterative reinitialization use vectors from prediction space to initialize the context space dur- ing training. This approach is both more complex and less efficient than ours. One-vector initializa- tion initializes all word embeddings with the same 8 A reviewer asks: "If a word is rare, its distributional vec- tor should also be sparse and less informative, which does not guarantee to be a good starting point." This is true and it sug- gests that it may not be possible to learn a very high-quality representation for a rare word. But this it not our goal. Our goal is simply to learn a better representation than the one that is learned by standard word2vec. Our explanation for our positive experimental results is that distributional initial- ization implements a form of smoothing. random vector to keep rare words close to each other. This approach is also less efficient than ours since the initial embedding is much denser than in our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have introduced distributional initialization of neural network architectures for learning better embeddings for rare words. Experimental results on a word similarity judgment task demonstrate that embeddings of rare words learned with dis- tributional initialization perform better than em- beddings learned with traditional one-hot initial- ization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Future work</head><p>Our work is the first exploration of the utility of distributional representations as initialization for embedding learning algorithms like word2vec. There are a number of research questions we would like to investigate in the future.</p><p>First, we showed that distributional represen- tation is beneficial for words with very low fre- quency. It was not beneficial in our experiments for more frequent words. A more extensive analy- sis of the factors that are responsible for the posi- tive effect of distributional representation is in or- der.</p><p>Second, to simplify our experimental setup and make the number of runs mangeable, we used the parameter θ both for corpus processing (only θ oc- currences of a particular word were left in the cor- pus) and as the separator between rare words that are distributionally initialized and frequent words that are not. It remains to be investigated whether there are interactions between these two properties of our model, e.g., a high rare-frequent separator may work well for words whose corpus frequency is much smaller than the separator.</p><p>Third, while we have shown that distributional initialization improves the quality of representa- tions of rare words, we did not investigate whether distributional initialization for rare words has any adverse effect on the quality of representations of frequent words for which one-hot initialization is applied. Since rare and frequent words are linked in the mixed model, this possibility cannot be dis- missed and we plan to investigate it in future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: One-hot vectors of frequent words and distributional vectors of rare words are separate in separate initialization (left) and overlap in mixed initialization (right). This example is for BINARY weighting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(</head><label></label><figDesc>Rubenstein and Goodenough (1965), 65) MC (Miller and Charles (1991), 30), MEN 1 (Bruni et al. (2012), 3000), WordSim353 2 (Finkelstein et al. (2001), 353), Stanford Rare Word 3 (Luong et al. (2013), 2034) and SimLex-999 4 (Hill et al. (2014), 999). We exclude from the evaluation the 16 pairs in RW that contain a word that does not occur in our corpus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>sep mixed sep mixed sep mixed sep mixed sep</figDesc><table>mixed sep 
1 
BINARY 
10 *56.54 47.06 35.96 32.10 *43.76*45.56 34.21*40.93 *24.81 20.85 *18.30*13.76 
2 
20 *59.08 45.31 *46.66 35.22 52.05*52.38 41.44 47.53 *29.48 26.93 *20.85*16.86 
3 
50 *63.20 51.07 *52.35 37.45 58.21 53.80 43.14 44.88 31.32 29.16 *24.19*22.45 
4 100 68.33 52.50 61.70 35.94 61.69 55.23 48.25 44.89 33.29 30.22 *26.74 24.66 
5 
PPMI 
10 *56.87*51.94 *37.31*46.52 *48.05*50.49 38.41*47.54 *25.53 23.12 *19.70*15.59 
6 
20 *59.08*50.32 *47.51*45.17 *54.88*56.42 43.31*53.19 *29.78*28.51 *21.84*19.23 
7 
50 *64.90*64.36 *55.27*56.75 60.51 61.04 45.76 55.55 32.05 30.25 *25.11*21.60 
8 100 71.08 58.37 68.14 52.33 63.05 60.74 48.66 55.49 33.25 30.49 *27.13 22.60 
9 
one-hot 
10 
38.93 
16.67 
40.70 
35.17 
20.69 
8.97 
10 
20 
42.17 
25.21 
50.21 
43.74 
26.58 
13.62 
11 
50 
56.01 
42.35 
60.22 
54.10 
32.16 
20.01 
12 100 
67.47 
61.33 
65.14 
59.87 
35.19 
24.06 

</table></figure>

			<note place="foot" n="1"> clic.cimec.unitn.it/ ˜ elia.bruni/MEN 2 alfonseca.org/eng/research/wordsim353.html 3 www-nlp.stanford.edu/ ˜ lmthang/morphoNLM/ 4 cl.cam.ac.uk/ ˜ fh295/simlex.html</note>

			<note place="foot" n="7"> Two-sample t-test, two-tailed, assuming equal variance, p &lt; .05</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Distributional memory: A general framework for corpus-based semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="673" to="721" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The WaCky Wide Web: A Collection of Very Large Linguistically Processed Web-Crawled Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Bernardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriano</forename><surname>Ferraresi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eros</forename><surname>Zanchetta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="226" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Knowledge-powered deep learning for word embedding</title>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases-European Conference, ECML PKDD</title>
		<imprint>
			<biblScope unit="page" from="132" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Compositional morphology for word representations and language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">A</forename><surname>Botha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning (ICML)</title>
		<meeting>the 31st International Conference on Machine Learning (ICML)<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Distributional semantics in technicolor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Boleda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam Khanh</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="136" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Knet: A general framework for learning word embedding using morphological knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1407.1687</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">Preprint pubslished on arXiv</note>
	<note>cs.CL</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Placing search in context: The concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="406" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Simlex-999: Evaluating semantic models with (genuine) similarity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<idno>arXiv:1408:3456 [cs.CL</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">Preprint pubslished on arXiv</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Training continuous space language models: Some practical issues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Hai Son Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Wisniewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yvon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="778" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Producing high-dimensional semantic spaces from lexical cooccurrence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Curt</forename><surname>Burgess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Behavior Research Methods, Instruments, &amp; Computers</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="203" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Better word representations with recursive neural networks for morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop at ICLR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><forename type="middle">G</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charles</surname></persName>
		</author>
		<title level="m">Contextual correlates of semantic similarity. Language &amp; Cognitive Processes</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Composition in distributional models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1388" to="1439" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cooccurrence vectors from corpora vs. distance vectors from dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshiki</forename><surname>Niwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshihiko</forename><surname>Nitta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="304" to="309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Contextual correlates of synonymy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Rubenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">B</forename><surname>Goodenough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="627" to="633" />
			<date type="published" when="1965-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The distributional hypothesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><surname>Sahlgren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Rivista di Linguistica (Italian Journal of Linguistics)</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="53" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Unsupervised Learning of Period Disambiguation for Tokenisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
		<respStmt>
			<orgName>IMS, University of Stuttgart</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dimensions of Meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM/IEEE Conference on Supercomputing</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="787" to="796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Word representations: A simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev-Arie</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Measuring praise and criticism: Inference of semantic orientation from association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">L</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOIS</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="315" to="346" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">From frequency to meaning: Vector space models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research (JAIR)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
