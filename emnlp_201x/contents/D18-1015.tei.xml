<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:09+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Temporally Grounding Natural Sentence in Video</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyuan</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinpeng</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Temporally Grounding Natural Sentence in Video</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="162" to="171"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>162</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We introduce an effective and efficient method that grounds (i.e., localizes) natural sentences in long, untrimmed video sequences. Specifically , a novel Temporal GroundNet (TGN) 1 is proposed to temporally capture the evolving fine-grained frame-byword interactions between video and sentence. TGN sequentially scores a set of temporal candidates ended at each frame based on the exploited frame-byword interactions, and finally grounds the segment corresponding to the sentence. Unlike traditional methods treating the overlapping segments separately in a sliding window fashion, TGN aggregates the historical information and generates the final grounding result in one single pass. We extensively evaluate our proposed TGN on three public datasets with significant improvements over the state-of-the-arts. We further show the consistent effectiveness and efficiency of TGN through an ablation study and a runtime test.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We examine the task of Natural Sentence Ground- ing in <ref type="bibr">Video (NSGV)</ref>. Given an untrimmed video and a natural sentence, the goal is to determine the start and end timestamps of the segment in the video which corresponds to the given sen- tence, as shown in <ref type="figure" target="#fig_2">Figure 1</ref> (a). Comparing with the other video researches, such as bidirectional video-sentence retrieval ( <ref type="bibr" target="#b39">Xu et al., 2015b</ref>), video attractiveness prediction ( <ref type="bibr" target="#b4">Chen et al., 2018</ref><ref type="bibr" target="#b2">Chen et al., , 2016</ref>, and video captioning ( <ref type="bibr" target="#b22">Pasunuru and Bansal, 2017;</ref><ref type="bibr">Wang et al., 2018a,b)</ref>, NSGV needs to model not only the characteristics of sentence and video but also the fine-grained interactions between the two modalities, which is even more challenging. * Work done while Jingyuan Chen and Xinpeng Chen were Research Interns with Tencent AI Lab.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8.1s 26.3s</head><p>Sentence:</p><p>A woman reels a kite back in toward herself.</p><p>... Recently, several related works ( <ref type="bibr" target="#b8">Gao et al., 2017;</ref><ref type="bibr" target="#b9">Hendricks et al., 2017</ref>) leverage one tempo- ral sliding window approach over video sequences to generate video segment candidates, which are then independently combined ( <ref type="bibr" target="#b8">Gao et al., 2017</ref>) or compared ( <ref type="bibr" target="#b9">Hendricks et al., 2017</ref>) with the given sentence to make the grounding prediction. Al- though the existing works have achieved promis- ing performances, they are still suffering from in- ferior effectiveness and efficiency. First, existing methods project the video segment and sentence into one common space, as shown in <ref type="figure" target="#fig_2">Figure 1 (b)</ref>, where the two generated embedding vectors are used to perform the matching between video seg- ment and sentence. Such a matching is only per- formed in the global segment and sentence level and thus not expressive enough, which ignores the fine-grained matching relations between video frames and the words in sentences. Second, in or- der to handle the diverse temporal scales and loca- tions of the candidate segments, exhaustive match- ing between the large amount of overlapping seg- ments and the sentence is required. As such, the sliding window methods are very computationally expensive.</p><p>In order to tackle the above two limitations, we introduce a novel Temporal GroundNet (TGN) model, the first dynamic single-stream deep archi- tecture for the NSGV task that takes full advantage of fine-grained interactions between video frames and words in a sentence, as shown in <ref type="figure" target="#fig_2">Figure 1</ref> (c). TGN sequentially processes video frames, where at each time step we rely on a novel multimodal in- teractor to exploit the evolving fine-grained frame- by-word interactions. Then, TGN works on the yielded interaction status to simultaneously score a set of temporal candidates of multiple scales and finally localize the video segment that corresponds to the sentence. More importantly, our proposed TGN is able to analyze an untrimmed video frame by frame without resorting to handling overlap- ping temporal video segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Grounding Natural Language in Image</head><p>Grounding natural language in image is also known as natural language object retrieval. The task is to localize an image region described by natural language, which involves comprehend- ing and modeling different spatial contexts, such as spatial configurations ( , at- tributes ( <ref type="bibr" target="#b41">Yu et al., 2018)</ref>, and relationships be- tween objects ( <ref type="bibr" target="#b11">Hu et al., 2017</ref>). Specifically, the task is usually formulated as a ranking problem over a set of candidate regions in a given image, where candidate spatial locations come from re- gion proposal methods <ref type="bibr" target="#b31">(Uijlings et al., 2013;</ref><ref type="bibr">Jie et al., 2016b,a;</ref><ref type="bibr" target="#b25">Ren et al., 2017</ref>) such as Edge- <ref type="bibr">Box (Zitnick and Doll√°r, 2014</ref>). Earlier stud- ies ( <ref type="bibr" target="#b20">Mao et al., 2016;</ref>) score the generated candidate regions according to their appearances and spatial features along with fea- tures of the entire image. However, these meth- ods fail to incorporate the interactions between ob- jects, because the scoring process of each region proposal is isolated. More recent studies ( <ref type="bibr" target="#b11">Hu et al., 2017;</ref><ref type="bibr" target="#b21">Nagaraja et al., 2016</ref>) improve the perfor- mance with the aid of modeling relationships be- tween objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Grounding Natural Language in Video</head><p>Analogous to spatial grounding in image, this work studies a similar problem-temporal natural language grounding in video. Earlier works ( <ref type="bibr" target="#b40">Yu and Siskind, 2013;</ref><ref type="bibr" target="#b18">Lin et al., 2014</ref>) learn the se- mantics of sentences, which are then matched to visual concepts via exploiting object appearance, motion and spatial relationships. However, they are limited to a small set of objects. Recently, larger datasets ( <ref type="bibr" target="#b8">Gao et al., 2017;</ref><ref type="bibr" target="#b9">Hendricks et al., 2017</ref>) are constructed to support more flexible groundings. The methods proposed in ( <ref type="bibr" target="#b8">Gao et al., 2017;</ref><ref type="bibr" target="#b9">Hendricks et al., 2017</ref>) learn a common embedding space shared by video segment fea- tures and sentence representations, in which their similarities are measured. Specifically, moment context network (MCN) ( <ref type="bibr" target="#b9">Hendricks et al., 2017</ref>) learns a shared embedding for video clip-level fea- tures and language features. The video features integrate local video features, global features, and temporal endpoint features. Cross-modal tempo- ral regression localizer (CTRL) ( <ref type="bibr" target="#b8">Gao et al., 2017)</ref> contains four modules, specifically a visual en- coder extracting clip-level features with context, a sentence encoder yielding its embedding through LSTM, a multimodal processing network generat- ing the fused representations via element-wise op- erations, and a temporal regression network pro- ducing the alignment scores and location offsets. One limitation of those common space matching methods is that the video segment generation pro- cess is computationally expensive, as they carry out overlapping sliding window matching ( <ref type="bibr" target="#b8">Gao et al., 2017</ref>) or exhaustive search ( <ref type="bibr" target="#b9">Hendricks et al., 2017)</ref>. Another weakness is that they exploit the relationships between textual and visual modali- ties by conducting a simple concatenation ( <ref type="bibr" target="#b8">Gao et al., 2017)</ref> or measuring a squared distance loss ( <ref type="bibr" target="#b9">Hendricks et al., 2017)</ref>, which ignores the evolving fine-grained video-sentence interactions. In this paper, a novel model TGN is proposed to deal with the aforementioned limitations for the task of natural sentence grounding in video.</p><p>Given a long and untrimmed video sequence V and a natural sentence S, the NSGV task is to lo- calize a video segment V s = {f t } te t=t b from V , be- ginning at t b and ending at t e , which corresponds to and expresses the same semantic meaning as the given sentence S. In order to perform the ground- ing, each video is represented as V = {f t } T t=1 , where T is the total number of frames and f t de- notes the feature representation of the t-th video frame. Similarly, each sentence is represented as S = {w n } N n=1 , where w n is the embedding vector of the n-th word in the sentence and N denotes the total number of words.</p><p>We propose a novel model, namely Temporal GroundNet (TGN), to tackle the NSGV problem. As illustrated in <ref type="figure" target="#fig_3">Figure 2</ref>, TGN consists of three modules. 1) Encoder: visual and textual encoders are used to compose the video frame representa- tions and word embeddings, respectively. 2) Inter- actor: a multimodal interactor learns the frame-by- word interactions between the video and sentence. 3) Grounder: a grounder generates the temporal localization in one single pass. Please note that these three modules are fully coupled together, which can thus be trained in an end-to-end fash- ion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Encoder</head><p>With the obtained video frame features V = {f t } T t=1 and word embeddings of the sentence S = {w n } N n=1 , we employ two long short- term memory networks (LSTMs) <ref type="bibr" target="#b10">(Hochreiter and Schmidhuber, 1997</ref>) to sequentially process the two different modalities, i.e., video and sentence, independently. Specifically, one LSTM sequen- tially models the video V , yielding the hidden states {h v t } T t=1 , while the other LSTM processes the sequential words in the sentence S, resulting in its corresponding hidden states {h s n } N n=1 . Owing to natural behaviors and characteristics of LSTMs, both {h v t } T t=1 and {h s n } N n=1 can encode and ag- gregate the contextual evidences ( <ref type="bibr" target="#b36">Wang and Jiang, 2016b</ref>) from the sequential video frame represen- tations and word embeddings of the sentence, re- spectively, meanwhile casting aside the irrelevant information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Interactor</head><p>Based on the hidden states of the video and sen- tence yielded from the leveraged encoders, we de-  sign a multimodal interactor to perform the frame- by-word interactions between the video and sen- tence. First, the frame-specific sentence feature is generated through summarizing the sentence hid- den states by considering their relationships with the specific video frame at each time step. Af- terwards, an interaction LSTM, dubbed i-LSTM, is performed to aggregate frame-by-word interac- tions.</p><formula xml:id="formula_0">i-¬≠LSTM i-¬≠LSTM i-¬≠LSTM i-¬≠LSTM K grounding candidates Œ¥ Œ¥ ‚ãÖ 2 Œ¥ ‚ãÖ 3 Œ¥ ‚ãÖ K ..</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Frame-Specific Sentence Feature</head><p>Directly operating on the clip-level and sentence- level features generated by the encoders cannot well exploit the frame-by-word relationships be- tween video and sentence that evolve over time.</p><p>Inspired by ( <ref type="bibr" target="#b35">Wang and Jiang, 2016a;</ref><ref type="bibr" target="#b6">Feng et al., 2018</ref>), we introduce one novel frame-specific sen- tence feature, which adaptively summarizes the hidden states of the sentence {h s n } N n=1 with re- spect to the t-th video frame:</p><formula xml:id="formula_1">H s t = N n=1 Œ± n t h s n ,<label>(1)</label></formula><p>where H s t denotes the summarized sentence rep- resentation specified by the t-th video frame. At each time step t, we utilize the hidden state h v t to selectively attend the words and summarize them accordingly. The attention weight Œ± n t encodes the degree to which the n-th word in the sentence is aligned with the t-th video frame. As the pro- cessing of video frames proceeds, the attention weights dynamically change regarding to the cur- rent video frame. As such, the generated frame- specific sentence features {H s t } T t=1 consider the frame-by-word relationships between all the video frames and all the words in the sentence.</p><p>As the generation of frame-specific sentence feature is deeply coupled with the following inter- action LSTM, we will explain the calculation of the attention weight Œ± n t later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Interaction LSTM (i-LSTM)</head><p>In order to accurately ground the sentence in a video, the multimodal interation behaviors be- tween the video and sentence need to be com- prehensively modeled. Previous approaches on multimodal interactions were limited to concate- nation ( <ref type="bibr" target="#b42">Zhu et al., 2016)</ref>, element-wise product or sum ( <ref type="bibr" target="#b8">Gao et al., 2017)</ref>, and bilinear pooling ( <ref type="bibr" target="#b7">Fukui et al., 2016</ref>). These methods are not expressive enough since they ignore the evolving fine-grained interactions across video and sentence, particu- larly the frame-by-word interactions. In this paper, we propose a novel multimodal interaction model, which is realized by LSTM. We term it interaction LSTM (i-LSTM), which sequentially processes the video sequence frame by frame, holding deep interactions with the words in the sentence.</p><p>In order to well capture the complicated tempo- ral interactions between the video and sentence, at each time step t, the input of the i-LSTM is formed by concatenating the t-th video hidden state h v t and the t-th frame-specific sentence feature H s t as:</p><formula xml:id="formula_2">r t = h v t H s t .</formula><p>r t is then fed into the i-LSTM unit to yield the t-th intermediate interaction status be- tween the video and sentence:</p><formula xml:id="formula_3">h r t = i-LSTM(r t , h r t‚àí1 ),<label>(2)</label></formula><p>where h r t is the yielded hidden state, encoding the fine-grained interactions between the word and video frame. h r t will be further used to perform the grounding process. Due to the inherent properties and characteristics of LSTMs, important cues re- garding to grounding up to the current stage will be "remembered", while non-essential ones will be "forgotten". Now we go back to the generation of attention weight Œ± n t in Eq. <ref type="formula" target="#formula_1">(1)</ref>, based on the obtained vi- sual hidden states h v t and textual hidden state h s n as well as the yielded interaction status h r t‚àí1 in the previous step. The widely used soft-attention mechanism ( <ref type="bibr">Xu et al., 2015a;</ref>) is used to generate the attention weights in a frame- by-word manner. As aforementioned, the i-LSTM models the evolving frame-by-word interactions between the sentence and video. Therefore, the at- tention weight between the n-th word h s n and the t-th video frame h v t is determined by not only the content of the video and sentence but also their in- teraction status. Thus, we design one network to compute the relevance score of one video frame with respect to each word:</p><formula xml:id="formula_4">Œ≤ n t = w tanh(W S h s n +W V h v t +W R h r t‚àí1 +b)+c,<label>(3)</label></formula><p>where vector w, matrices W * , bias vector b, and bias c are the network parameters to be learned. h r t‚àí1 is the hidden state of the i-LSTM at t ‚àí 1 time step. The final word-level attention weights are obtained by:</p><formula xml:id="formula_5">Œ± n t = exp(Œ≤ n t ) N j=1 exp(Œ≤ j t )</formula><p>.</p><p>The obtained attention weight Œ± n t is thereafter to generate the frame-specific sentence feature as in Eq. (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Grounder</head><p>In this section, we introduce the grounder, which works on the yielded interaction status h r t from i-LSTM, to localize the video segment that cor- responds to the sentence. Our proposed grounder works in one single pass without introducing over- lapping sliding windows, which thus results in a fast runtime. As shown in <ref type="figure" target="#fig_3">Figure 2</ref>, at each time step t, the grounder efficiently scores a set of K grounding candidates by considering multiple time scales <ref type="bibr" target="#b0">(Buch et al., 2017</ref>) that end at time step t. Specifically, we use different K for different datasets, which is determined by the distribution of the lengths of all ground-truth groundings in a certain dataset. To simplify the following discus- sions, the lengths of K time scales are assumed to be an arithmetic sequence with the common differ- ence Œ¥ and all the temporal candidates are sorted by increasing lengths. In other words, the length of the k-th candidate is kŒ¥. Note that all grounding candidates considered at time t have a fixed ending boundary.</p><p>Specifically, at each time step t, the grounder will classify each temporal candidate in consid- eration as a positive grounding or a negative one with respect to the given sentence. Considering multiple time scales, the grounder will generate the confidence scores C t = (c 1 t , c 2 t , ..., c K t ) that correspond to the set of K visual grounding can- didates, all ending at time step t. The hidden state h r t generated by i-LSTM at time t, repre- senting the interaction status between the sentence and video sequence up to the current position, is naturally suited to yield the confidence scores for the different time scales ending at time step t. In this paper, the confidence scores, indicating the sentence grounding, are generated by a fully- connected layer with sigmoid nonlinearity:</p><formula xml:id="formula_7">C t = œÉ(W K h r t + b r t ),<label>(5)</label></formula><p>where W K and b r t are the corresponding parame- ters, and œÉ denotes the nonlinear sigmoid function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training</head><p>The training samples collected in X for NSGV are video-sentence pairs. Specifically, each video V is temporally associated with a set of sentence an- notations:</p><formula xml:id="formula_8">A = {(S i , t b i , t e i )} M i=1</formula><p>, where M is the number of annotated sentences of the video, and S i is a sentence description of a video clip, with t b i and t e i indicating the beginning and ending time in the video. Each training sample corresponds to a ground-truth matrix y ‚àà R T √óK with binary en- tries. We use y k t to denote the (t, k)-th entry of the ground-truth matrix. y k t is interpreted as whether the k-th grounding candidate at time step t corre- sponds to the given natural sentence. Concretely, the entry y k t is set as 1, indicating that the corre- sponding video segment (ends at time step t with length kŒ¥) has a temporal Intersection-over-Union (IoU) with (t b , t e ) larger than a threshold Œ∏. Oth- erwise y k t is set as 0. For a training pair (V, S) ‚àà X , the objective at time step t is given by a weighted binary cross entropy loss L(t, V, S):</p><formula xml:id="formula_9">‚àí K k=1 w k 0 y k t log c k t + w k 1 (1 ‚àí y k t ) log(1 ‚àí c k t ),<label>(6)</label></formula><p>where the weights w k 0 and w k 1 are calculated ac- cording to the frequencies of positive and negative samples in the training set with length kŒ¥. y k t is the ground-truth value and c k t denotes the predic- tion results by our proposed model.</p><p>Our TGN backpropagates at every time step t to learn all the parameters of the fully-coupled three modules: encoder, interactor, and grounder. The objective of all training video-sentence pairs X is defined as:</p><formula xml:id="formula_10">L X = (V,S)‚ààX T t=1 L(t, V, S).<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Inference</head><p>During the inference stage, given a testing video V and a sentence S, the textual and visual en- coders first generate hidden states for each word and video frame, respectively. Then, the interac- tor sequentially goes through the video frame by frame to yield the frame-by-word interaction sta- tus. At each position t, a K-dimensional score vector C t is generated by the grounder. There- fore, after processing the last frame in the video, a T √ó K score matrix is obtained for the whole video, with the (t, k)-th entry in the matrix indicat- ing the probability that the video segment ended at position t with length kŒ¥ in video V corresponds to sentence S. Eventually, the evaluation is re- duced to a ranking problem over all the grounding candidates based on the generated scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we evaluate the effectiveness of our proposed TGN on the NSGV task. We be- gin by describing the datasets used for evaluation, followed by the introduction of the experimental settings including the baselines, configurations, as well as the evaluation metrics. Afterwards, we demonstrate the effectiveness of TGN by compar- ing with the state-of-the-art approaches and effi- ciency through a runtime test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We experiment on three publicly accessible datasets: TACoS 3 consists of 127 videos selected from the MPII Cooking Composite Activities video cor- pus ( <ref type="bibr">Rohrbach et al., 2012</ref>). The same split as in ( <ref type="bibr" target="#b8">Gao et al., 2017</ref>) is used, consisting of 10146, 4589, and 4083 video-sentence pairs for training, validation, and testing, respectively. ActivityNet Captions 4 consists of 19, 209 videos amounting to 849 hours. The public split is used for our experiments, which has 37421, 17505, and 17031 video-sentence pairs for training, valida- tion, and testing, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Baselines</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Evaluation Metrics</head><p>A grounding of one natural sentence in a video is considered as "correct" if its temporal IoU with the ground-truth boundary is above a threshold Œ∏. To be consistent with the baselines, we adopt R@N , IoU=Œ∏, and mean IoU (mIoU) as our eval- uation metrics. R@N , IoU=Œ∏ represents the per- centage of testing samples which have at least one of the top-N results with IoU larger than Œ∏. mIoU means the average IoU over all testing samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Configurations</head><p>Generally, the video frame features are usually ex- tracted with a time resolution. For the videos in DiDeMo and TACoS, we sample every 5 second as done by <ref type="bibr" target="#b9">(Hendricks et al., 2017)</ref>. As the videos in DiDeMo are 25-30 second long, the video feature length is reduced to 6. For videos in ActivityNet Captions, we sample every second. To extract vi- sual features, we consider both appearance and optical flow features. Specifically, we study four widely-used visual features: VGG16 (Simonyan and Zisserman, 2014), C3D ( <ref type="bibr" target="#b30">Tran et al., 2015)</ref>, Inception-V4 ( <ref type="bibr" target="#b29">Szegedy et al., 2017)</ref>, and optical flow ( . Please note that when comparing with specific baseline methods, we use the same features as baseline methods, specifi- cally, VGG16 and optical flow for MCN and C3D for CTRL, VSA-RNN, and VSA-STV. For sentences, we tokenize each sentence by Stanford CoreNLP ( ) and use the 300-D word embeddings from GloVe ( <ref type="bibr" target="#b23">Pennington et al., 2014</ref>) to initialize the models. The words not found in GloVe are initialized as zero vectors. The hidden state dimensions of all LSTMs (including the video, sentence, and in- teraction LSTMs) are set as 512. We use the Adam ( <ref type="bibr" target="#b16">Kingma and Ba, 2014</ref>) optimizer with Œ≤ 1 = 0.5 and Œ≤ 2 = 0.999. The initial learning rate is set to 0.001. We train the network for 200 iter- ations, and the learning rate is gradually decayed over time. The mini-batch size is set to 64.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Results and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Comparisons with State-of-the-Arts</head><p>Experiments on DiDeMo. <ref type="table" target="#tab_2">Table 1</ref> illustrates the performance comparisons on the DiDeMo dataset. In addition to MCN, we also compare with the baseline Moment Frequency Prior (MFP) in <ref type="bibr" target="#b9">(Hendricks et al., 2017)</ref>, which selects segments corresponding to the positions of videos in the training dataset with most annotations. First, TGN with different features can significantly outper- forms the "prior baseline" MFP, which retrieves segments corresponding to the most common start and end points in the dataset. Second, it can be ob- served that with the same visual features, specifi- cally VGG16 and optical flow, TGN significantly outperforms MCN. And the performance of TGN with optical flow is better than that with VGG16. One possible reason is that the videos in DiDeMo are relatively short, which only contain a single event. In such a case, the action information plays a more critical role. This finding is also consis- tent with ( <ref type="bibr" target="#b9">Hendricks et al., 2017)</ref>. By fusing the results obtained by VGG16 and optical flow to- gether, the performance can be further boosted, as demonstrated by TGN-Fusion and MCN-Fusion. Third, MCN introduces the temporal endpoint fea- ture (TEF) as prior knowledge, which indicates when a segment occurs in a video. With TEF, the performance of MCN can be significantly im- proved. However, it is still inferior to our proposed TGN. MCN is designed as an enumeration-based ap- proach. Each video in the DiDeMo dataset is split into six five-second chunks which are considered as the time unit for localization. Therefore, in total there are only C 2 7 = 7 √ó 6/2 = 21 different ways of localization for DiDeMo videos. Therefore, al- though MCN can be effectively applied to videos with several chunks due to the small search space, it is not practical for untrimmed long videos. In the Section 4.3.3, we will evaluate and compare the efficiencies of MCN, CTRL, and our proposed TGN.</p><p>Experiments on TACoS. <ref type="table" target="#tab_3">Table 2</ref> illustrates the experimental results on TACoS. First, it can be observed that CTRL performs much better than VSA-RNN and VSA-STV. The reasons lie in twofold ( <ref type="bibr" target="#b8">Gao et al., 2017</ref>). On one hand, CTRL utilizes a multilayer alignment network to learn better alignment. On the other hand, VSA-RNN and VSA-STV do not encode temporal context in- formation of video. Second, with the same visual feature, specifically C3D, TGN-C3D significantly outperforms CTRL-C3D. This is due to the fact that TGN exploits not only the contextual infor- mation but also the fine-grained interaction behav- iors. More concretely, TGN considers the frame- by-word correlations by introducing an attentive combinations of the words in the sentence, where each weight encodes the degree to which the word is aligned with each specific frame. This mecha- nism is beneficial to capturing the informative se-  mantics in the sentences for alignment.</p><p>Experiments on ActivityNet Captions. Be- sides the two benchmarks, we also evaluate our model on the ActivityNet Captions dataset. Dif- ferent CNNs are used to encode video visual in- formation. Specifically, we consider VGG16, C3D, and Inception-V4. The results are included in <ref type="table" target="#tab_4">Table 3</ref>. First, our proposed TGN can per- form effectively on long untrimmed videos. Sec- ond, Inception-V4 performs generally better than VGG16 and C3D, which is consistent with the finding in ( <ref type="bibr" target="#b1">Canziani et al., 2016)</ref>. Therefore, more powerful visual representations of video features will undoubtedly improve the the performance of our proposed TGN on the NSGV task. Some qualitative results of our proposed TGN on ActiveityNet Captions dataset is illustrated in <ref type="figure" target="#fig_5">Figure 3</ref>. It can be observed that with different visual features, different grounding results are ob- tained. For the first and second examples, TGN with VGG16 and Inception-V4 generates more ac- curate groundings than that with C3D, while TGN with C3D yields more accurate grounding results for the third example. More specifically, our pro- posed TGN with VGG16 and Inception-V4 can well identify the visual information related with the sentence, i.e. "A man in a red shirt claps his hands".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Effect of Frame-by-Word Attention</head><p>We examine the effect of the frame-by-word atten- tion in interactor. We ablate TGN into two other methods. 1) NA: There is no attention layer in this model. After obtaining the sequential hid- den states of the sentence, mean pooling is used to generate the representation for the whole sentence. Then the generated representation is concatenated with video representation, based on which the scores for multiple grounding candidates are pre- dicted. 2) NM: The idea of generating frame- specific sentence feature is still reserved in the NM model. The difference between NM and TGN is that there is no interaction LSTM in NM. Specif- ically, when calculating the attention weight for each word as in Eq. (3), the hidden state h r t‚àí1 in- dicating the interaction status is not incorporated.</p><p>The quantitative results are displayed in <ref type="table" target="#tab_5">Table 4</ref>. First, when the attention mechanism is applied (NM), the performance is improved as compared with utilizing mean pooling (NA) for sentence fea- tures. The better performance demonstrates that our assumption about the evolving frame-by-word correlations between two modalities is reasonable. This also indicates that it is necessary to discrim- inate the contribution of each word in a sentence to perform the NSGV task. Second, utilizing the interaction LSTM module (TGN) achieves better performance than simply concatenating the video representation and the attentive sentence represen- tation (NM). This result indicates that the interac- tion LSTM yields better interaction status between these two modalities, which can thereby benefit the final grounding.</p><p>We provide some qualitative examples in <ref type="figure" target="#fig_6">Fig- ure 4</ref> for a better understanding of the frame-by- word attention. Meanwhile, the grounding results yielded by TGN-Fusion (considering both VGG16 and optical flow) are also illustrated. This ex- periment is designed to verify whether the frame- by-word attention mechanism in interactor is use- ful to highlight the representative concepts in the sentence. The attention weights Œ± for two test- ing samples in DiDeMo are illustrated in <ref type="figure" target="#fig_6">Fig- ure 4</ref>, where the darker the color is, the larger  the attention weight is. It can be observed that some words well match the frames. For exam- ple, in <ref type="figure" target="#fig_6">Figure 4</ref> (a), the concept "forest" ap- pears across all the video frames presenting an evenly distributed attention weights, while the other concept "waterfall" only presents in the first two frames. In addition to nouns, the ad- jective "blue" in <ref type="figure" target="#fig_6">Figure 4</ref> (b) also receives rela- tively higher attention weights in relevant frames. Lastly, for stop words like "a", "the" and "in", their attention weights, which are very small, also present an even distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Efficiency</head><p>We evaluate the efficiency of our proposed TGN, by comparing its runtime with MCN and CTRL on a Tesla M40 GPU. The efficiency is mea- sured by frames per second (FPS) as shown in Ta- ble 5. Please not that the feature extraction time is excluded. It can be observed that our TGN model achieves much faster processing speeds, with 1,363 fps vs. 562 and 286 for CTRL and MCN, respectively. The reason mainly attributes to that the proposed TGN only process each video in one single pass without processing overlapped sliding windows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we focused on the task of natu- ral sentence grounding in video that is believed to offer a comprehensive understanding of bridg- ing computer vision and natural language process- ing. Towards this task, we proposed an end-to-end Temporal GroundNet (TGN) by incorporating the evolving fine-grained frame-by-word interactions across video-sentence modalities to generate a vi- sual grounding tailored to each given natural sen- tence. Moreover, TGN performs efficiently, which only needs to process the video sequence in one single pass. Extensive experiments on three real- world datasets clearly demonstrate the effective- ness and efficiency of the proposed TGN.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>github.io/archive/tgn.html.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a) The Natural Sentence Grounding in Video (NSGV) task. (b) A common space based matching method performs in a sliding window fashion. (c) Our proposed Temporal GroundNet (TGN) localizes the candidate video segments at multiple scales in a single processing pass. The frames in the video and the words in the sentence interact attentively to perform fine-grained frame-by-word matchings for grounding sentence in video.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The architecture of our proposed TGN model. TGN consists of three modules. The visual and textural encoders aggregate the contextual evidences from the sequential video frame representations and word embeddings of the sentence, respectively. The multimodal interactor learns the fine-grained frame-byword interactions between the video and sentence. The grounder yields the temporal grounding of the sentence in the video sequence via one single pass.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>We compare our proposed TGN against the fol- lowing two state-of-the-art models, specifically, the MCN (Hendricks et al., 2017), CTRL (Gao et al., 2017), visual-semantic alignment with LSTM (VSA-RNN) (Karpathy and Li, 2015), and visual-semantic alignment with skip thought vec- tor (VSA-STV) (Kiros et al., 2015). For fair comparisons, we compare the results of MCN on DiDeMo and the results of CTRL, VSA-RNN, VSA-STV on TACoS reported in their papers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The qualitative grounding results of our TGN model on the ActiveityNet Captions dataset with different visual features.</figDesc><graphic url="image-56.png" coords="8,84.81,62.81,425.19,389.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Visualization results on frame-by-word attention. The darker the color is, the larger its represented attention value is.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 1 : Performance comparisons of different meth- ods on DiDeMo. The best performance for each metric entry is highlighted in boldface.</head><label>1</label><figDesc></figDesc><table>Method 
R@1 
IoU=1 

R@5 
IoU=1 
mIoU 

MFP 
19.40 
66.38 
26.65 
MCN-VGG16 
13.10 
44.82 
25.13 
MCN-Flow 
18.35 
56.25 
31.46 
MCN-Fusion 
19.88 
62.39 
33.51 
MCN-Fusion+TEF 
28.10 
78.21 
41.08 
TGN-VGG16 
24.28 
71.43 
38.62 
TGN-Flow 
27.52 
76.94 
42.84 
TGN-Fusion 
28.23 
79.26 
42.97 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 2 : Performance comparisons of different meth- ods on TACoS. The best performance for each metric entry is highlighted in boldface.</head><label>2</label><figDesc></figDesc><table>Method 
R@1 
IoU=0.5 

R@1 
IoU=0.3 

R@1 
IoU=0.1 

R@5 
IoU=0.5 

R@5 
IoU=0.3 

R@5 
IoU=0.1 

VSA-RNN 
4.78 
6.91 
8.84 
9.10 
13.90 
19.05 
VSA-STV 
7.56 
10.77 
15.01 
15.5 
23.92 
32.82 
CTRL-C3D 
13.30 
18.32 
24.32 
25.42 
36.39 
48.73 
TGN-C3D 
18.90 
21.77 
41.87 
31.02 
39.06 
53.40 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 3 : Performance comparisons of different visual features on ActivityNet Captions. The best perfor- mance for each metric entry is highlighted in boldface.</head><label>3</label><figDesc></figDesc><table>Feature 
R@1 
IoU=0.5 

R@1 
IoU=0.3 

R@1 
IoU=0.1 

R@5 
IoU=0.5 

R@5 
IoU=0.3 

R@5 
IoU=0.1 

C3D 
27.93 
43.81 
69.59 
44.20 
54.56 
78.66 
VGG16 
23.90 
42.24 
65.76 
40.17 
51.82 
76.21 
Inception-V4 
28.47 
45.51 
70.06 
43.33 
57.32 
79.10 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Ablation studies on TACoS. The best perfor-
mance for each metric entry is highlighted in boldface. 

Feature 
R@1 
IoU=0.5 

R@1 
IoU=0.3 

R@1 
IoU=0.1 

R@5 
IoU=0.5 

R@5 
IoU=0.3 

R@5 
IoU=0.1 

NA 
5.53 
7.67 
24.23 
15.20 
18.94 
41.25 

NM 
13.89 
18.60 
41.41 
26.60 
31.74 
47.70 
TGN 
18.90 
21.77 
41.87 
31.02 
39.06 
53.40 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Efficiency comparison in terms of frame per 
second. 

CTRL MCN TGN 

FPS 
562 
286 
1,363 

</table></figure>

			<note place="foot" n="2"> https://goo.gl/JpbAhg.</note>

			<note place="foot" n="3"> https://goo.gl/ajmsva. 4 https://goo.gl/N355bG.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sst: Singlestream temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyamal</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5534" to="5542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">An analysis of deep neural network models for practical applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfredo</forename><surname>Canziani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugenio</forename><surname>Culurciello</surname></persName>
		</author>
		<idno>abs/1605.07678</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Micro tells macro: Predicting the popularity of micro-videos via a transductive model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuemeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="898" to="907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Attentive collaborative filtering: Multimedia recommendation with item-and component-level attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="335" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fine-grained video attractiveness prediction using multimodal deep learning on a large real-world dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Video re-localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>In ECCV</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daylen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="457" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">TALL: temporal activity localization via language query</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5277" to="5285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Localizing moments in video with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5804" to="5813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J√ºrgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Modeling relationships in referential expressions with compositional modular networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4418" to="4427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Natural language object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhe</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4555" to="4564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Tree-structured reinforcement learning for sequential object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="127" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scale-aware pixelwise object proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><forename type="middle">Feng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Eng Hock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4525" to="4539" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Antonio Torralba, and Sanja Fidler. 2015. Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visual semantic search: Retrieving videos via complex textual queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2657" to="2664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The stanford corenlp natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Association for Computer Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generation and comprehension of unambiguous object descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oana</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Modeling context between objects for referring expression understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Varun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><forename type="middle">I</forename><surname>Nagaraja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="792" to="807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-task video captioning with video and entailment generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakanth</forename><surname>Pasunuru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1273" to="1283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Grounding action descriptions in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaela</forename><surname>Regneri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominikus</forename><surname>Wetzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Thater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="25" to="36" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Bernt Schiele, and Manfred Pinkal</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Grounding of textual phrases in images by reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="817" to="834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sikandar Amin, Manfred Pinkal, and Bernt Schiele. 2012. Script data for attribute-based recognition of composite activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaela</forename><surname>Regneri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="page" from="144" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4278" to="4284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lubomir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Jasper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Koen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold</forename><forename type="middle">W M</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Reconstruction network for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bairui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bidirectional attentive fusion with context gating for dense video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning natural language inference with LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1442" to="1451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Machine comprehension using match-lstm and answer pointer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<idno>abs/1608.07905</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Jointly modeling deep video and compositional text to bridge vision and language in a unified framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2346" to="2352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Grounded language learning from video described with sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haonan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">Mark</forename><surname>Siskind</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="53" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Mattnet: Modular attention network for referring expression comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<idno>abs/1801.08186</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Visual7w: Grounded question answering in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Feifei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4995" to="5004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll√°r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="391" to="405" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
