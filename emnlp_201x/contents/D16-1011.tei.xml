<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:30+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rationalizing Neural Predictions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Rationalizing Neural Predictions</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="107" to="117"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Prediction without justification has limited applicability. As a remedy, we learn to extract pieces of input text as justifications-rationales that are tailored to be short and coherent , yet sufficient for making the same prediction. Our approach combines two modular components, generator and encoder, which are trained to operate well together. The generator specifies a distribution over text fragments as candidate rationales and these are passed through the encoder for prediction. Rationales are never given during training. Instead , the model is regularized by desiderata for rationales. We evaluate the approach on multi-aspect sentiment analysis against manually annotated test cases. Our approach out-performs attention-based baseline by a significant margin. We also successfully illustrate the method on the question retrieval task. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many recent advances in NLP problems have come from formulating and training expressive and elabo- rate neural models. This includes models for senti- ment classification, parsing, and machine translation among many others. The gains in accuracy have, however, come at the cost of interpretability since complex neural models offer little transparency con- cerning their inner workings. In many applications, such as medicine, predictions are used to drive criti- cal decisions, including treatment options. It is nec- essary in such cases to be able to verify and under-the beer was n't what i expected, and i'm not sure it's "true to style", but i thought it was delicious. a very pleasant ruby red-amber color with a rela9vely brilliant finish, but a limited amount of carbona9on, from the look of it. aroma is what i think an amber ale should be -a nice blend of caramel and happiness bound together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Review</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ratings</head><p>Look: 5 stars Smell: 4 stars stand the underlying basis for the decisions. Ide- ally, complex neural models would not only yield improved performance but would also offer inter- pretable justifications -rationales -for their predic- tions.</p><p>In this paper, we propose a novel approach to in- corporating rationale generation as an integral part of the overall learning problem. We limit ourselves to extractive (as opposed to abstractive) rationales. From this perspective, our rationales are simply sub- sets of the words from the input text that satisfy two key properties. First, the selected words represent short and coherent pieces of text (e.g., phrases) and, second, the selected words must alone suffice for prediction as a substitute of the original text. More concretely, consider the task of multi-aspect senti- ment analysis. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates a product review along with user rating in terms of two categories or aspects. If the model in this case predicts five star rating for color, it should also identify the phrase "a very pleasant ruby red-amber color" as the rationale underlying this decision.</p><p>In most practical applications, rationale genera-tion must be learned entirely in an unsupervised manner. We therefore assume that our model with rationales is trained on the same data as the origi- nal neural models, without access to additional ra- tionale annotations. In other words, target rationales are never provided during training; the intermedi- ate step of rationale generation is guided only by the two desiderata discussed above. Our model is com- posed of two modular components that we call the generator and the encoder. Our generator specifies a distribution over possible rationales (extracted text) and the encoder maps any such text to task specific target values. They are trained jointly to minimize a cost function that favors short, concise rationales while enforcing that the rationales alone suffice for accurate prediction.</p><p>The notion of what counts as a rationale may be ambiguous in some contexts and the task of select- ing rationales may therefore be challenging to eval- uate. We focus on two domains where ambiguity is minimal (or can be minimized). The first sce- nario concerns with multi-aspect sentiment analysis exemplified by the beer review corpus <ref type="bibr" target="#b23">(McAuley et al., 2012)</ref>. A smaller test set in this corpus iden- tifies, for each aspect, the sentence(s) that relate to this aspect. We can therefore directly evaluate our predictions on the sentence level with the caveat that our model makes selections on a finer level, in terms of words, not complete sentences. The second sce- nario concerns with the problem of retrieving similar questions. The extracted rationales should capture the main purpose of the questions. We can therefore evaluate the quality of rationales as a compressed proxy for the full text in terms of retrieval perfor- mance. Our model achieves high performance on both tasks. For instance, on the sentiment predic- tion task, our model achieves extraction accuracy of 96%, as compared to 38% and 81% obtained by the bigram SVM and a neural attention baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Developing sparse interpretable models is of con- siderable interest to the broader research commu- nity( <ref type="bibr" target="#b19">Letham et al., 2015;</ref><ref type="bibr" target="#b15">Kim et al., 2015</ref>). The need for interpretability is even more pronounced with recent neural models. Efforts in this area include analyzing and visualizing state activation <ref type="bibr" target="#b10">(Hermans and Schrauwen, 2013;</ref><ref type="bibr" target="#b14">Karpathy et al., 2015;</ref>, learning sparse interpretable word vec- tors <ref type="bibr" target="#b7">(Faruqui et al., 2015b)</ref>, and linking word vectors to semantic lexicons or word properties <ref type="bibr" target="#b6">(Faruqui et al., 2015a;</ref><ref type="bibr" target="#b8">Herbelot and Vecchi, 2015)</ref>.</p><p>Beyond learning to understand or further con- strain the network to be directly interpretable, one can estimate interpretable proxies that approximate the network. Examples include extracting "if-then" rules <ref type="bibr" target="#b28">(Thrun, 1995)</ref> and decision trees <ref type="bibr" target="#b4">(Craven and Shavlik, 1996</ref>) from trained networks. More recently, <ref type="bibr" target="#b25">Ribeiro et al. (2016)</ref> propose a model- agnostic framework where the proxy model is learned only for the target sample (and its neighbor- hood) thus ensuring locally valid approximations. Our work differs from these both in terms of what is meant by an explanation and how they are derived. In our case, an explanation consists of a concise yet sufficient portion of the text where the mechanism of selection is learned jointly with the predictor.</p><p>Attention based models offer another means to ex- plicate the inner workings of neural models <ref type="bibr" target="#b1">(Bahdanau et al., 2015;</ref><ref type="bibr" target="#b3">Cheng et al., 2016;</ref><ref type="bibr" target="#b22">Martins and Astudillo, 2016;</ref><ref type="bibr" target="#b2">Chen et al., 2015;</ref><ref type="bibr" target="#b31">Xu and Saenko, 2015;</ref><ref type="bibr" target="#b33">Yang et al., 2015</ref>). Such models have been successfully applied to many NLP problems, improving both prediction accuracy as well as vi- sualization and interpretability <ref type="bibr" target="#b27">(Rush et al., 2015;</ref><ref type="bibr">Rocktäschel et al., 2016;</ref><ref type="bibr" target="#b9">Hermann et al., 2015)</ref>.  introduced a stochastic attention mechanism together with a more standard soft at- tention on image captioning task. Our rationale ex- traction can be understood as a type of stochastic attention although architectures and objectives dif- fer. Moreover, we compartmentalize rationale gen- eration from downstream encoding so as to expose knobs to directly control types of rationales that are acceptable, and to facilitate broader modular use in other applications.</p><p>Finally, we contrast our work with rationale-based classification ( <ref type="bibr" target="#b34">Zaidan et al., 2007;</ref><ref type="bibr" target="#b21">Marshall et al., 2015;</ref><ref type="bibr" target="#b35">Zhang et al., 2016</ref>) which seek to improve pre- diction by relying on richer annotations in the form of human-provided rationales. In our work, ratio- nales are never given during training. The goal is to learn to generate them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Extractive Rationale Generation</head><p>We formalize here the task of extractive rationale generation and illustrate it in the context of neural models. To this end, consider a typical NLP task where we are provided with a sequence of words as input, namely x = {x 1 , · · · , x l }, where each x t ∈ R d denotes the vector representation of the i- th word. The learning problem is to map the input sequence x to a target vector in R m . For example, in multi-aspect sentiment analysis each coordinate of the target vector represents the response or rat- ing pertaining to the associated aspect. In text re- trieval, on the other hand, the target vectors are used to induce similarity assessments between input se- quences. Broadly speaking, we can solve the associ- ated learning problem by estimating a complex pa- rameterized mapping enc(x) from input sequences to target vectors. We call this mapping an encoder. The training signal for these vectors is obtained ei- ther directly (e.g., multi-sentiment analysis) or via similarities (e.g., text retrieval). The challenge is that a complex neural encoder enc(x) reveals lit- tle about its internal workings and thus offers little in the way of justification for why a particular pre- diction was made.</p><p>In extractive rationale generation, our goal is to select a subset of the input sequence as a rationale. In order for the subset to qualify as a rationale it should satisfy two criteria: 1) the selected words should be interpretable and 2) they ought to suffice to reach nearly the same prediction (target vector) as the original input. In other words, a rationale must be short and sufficient. We will assume that a short selection is interpretable and focus on opti- mizing sufficiency under cardinality constraints.</p><p>We encapsulate the selection of words as a ratio- nale generator which is another parameterized map- ping gen(x) from input sequences to shorter se- quences of words. Thus gen(x) must include only a few words and enc(gen(x)) should result in nearly the same target vector as the original input passed through the encoder or enc(x). We can think of the generator as a tagging model where each word in the input receives a binary tag pertaining to whether it is selected to be included in the rationale. In our case, the generator is probabilistic and specifies a distri- bution over possible selections.</p><p>The rationale generation task is entirely unsuper- vised in the sense that we assume no explicit anno- tations about which words should be included in the rationale. Put another way, the rationale is intro- duced as a latent variable, a constraint that guides how to interpret the input sequence. The encoder and generator are trained jointly, in an end-to-end fashion so as to function well together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Encoder and Generator</head><p>We use multi-aspect sentiment prediction as a guid- ing example to instantiate the two key components - the encoder and the generator. The framework itself generalizes to other tasks.</p><p>Encoder enc(·): Given a training instance (x, y) where</p><formula xml:id="formula_0">x = {x t } l t=1</formula><p>is the input text sequence of length l and y ∈ [0, 1] m is the target m-dimensional sentiment vector, the neural encoder predicts˜ypredicts˜ predicts˜y = enc(x). If trained on its own, the encoder would aim to minimize the discrepancy between the pre- dicted sentiment vector˜yvector˜ vector˜y and the gold target vector y. We will use the squared error (i.e. L 2 distance) as the sentiment loss function,</p><formula xml:id="formula_1">L(x, y) = ˜ y − y 2 2 = enc(x) − y 2 2</formula><p>The encoder could be realized in many ways such as a recurrent neural network. For example, let h t = f e (x t , h t−1 ) denote a parameterized recurrent unit mapping input word x t and previous state h t−1 to next state h t . The target vector is then generated on the basis of the final state reached by the recur- rent unit after processing all the words in the input sequence. Specifically,</p><formula xml:id="formula_2">h t = f e (x t , h t−1 ), t = 1, . . . , l ˜ y = σ e (W e h l + b e )</formula><p>Generator gen(·): The rationale generator ex- tracts a subset of text from the original input x to function as an interpretable summary. Thus the ra- tionale for a given sequence x can be equivalently defined in terms of binary variables {z 1 , · · · , z l } where each z t ∈ 0, 1 indicates whether word x t is selected or not. From here on, we will use z to specify the binary selections and thus (z, x) is the actual rationale generated (selections, input). We will use generator gen(x) as synonymous with a probability distribution over binary selections, i.e., z ∼ gen(x) ≡ p(z|x) where the length of z varies with the input x.</p><p>In a simple generator, the probability that the t th word is selected can be assumed to be conditionally independent from other selections given the input x. That is, the joint probability p(z|x) factors accord- ing to</p><formula xml:id="formula_3">p(z|x) = l t=1 p(z t |x) (independent selection)</formula><p>The component distributions p(z t |x) can be mod- eled using a shared bi-directional recurrent neural network. Specifically, let − → f () and ← − f () be the for- ward and backward recurrent unit, respectively, then</p><formula xml:id="formula_4">− → h t = − → f (x t , −−→ h t−1 ) ← − h t = ← − f (x t , ←−− h t+1 ) p(z t |x) = σ z (W z [ − → h t ; ← − h t ] + b z )</formula><p>Independent but context dependent selection of words is often sufficient. However, the model is un- able to select phrases or refrain from selecting the same word again if already chosen. To this end, we also introduce a dependent selection of words,</p><formula xml:id="formula_5">p(z|x) = l t=1 p(z t |x, z 1 · · · z t−1 )</formula><p>which can be also expressed as a recurrent neural network. To this end, we introduce another hidden state s t whose role is to couple the selections. For example,</p><formula xml:id="formula_6">p(z t |x, z 1,t−1 ) = σ z (W z [ − → h t ; ← − h t ; s t−1 ] + b z ) s t = f z ([ − → h t ; ← − h t ; z t ], s t−1 )</formula><p>Joint objective: A rationale in our definition cor- responds to the selected words, i.e., {x k |z k = 1}. We will use (z, x) as the shorthand for this rationale and, thus, enc(z, x) refers to the target vector ob- tained by applying the encoder to the rationale as the input. Our goal here is to formalize how the ratio- nale can be made short and meaningful yet function well in conjunction with the encoder. Our generator and encoder are learned jointly to interact well but they are treated as independent units for modularity.</p><p>The generator is guided in two ways during learn- ing. First, the rationale that it produces must suffice as a replacement for the input text. In other words, the target vector (sentiment) arising from the ratio- nale should be close to the gold sentiment. The cor- responding loss function is given by</p><formula xml:id="formula_7">L(z, x, y) = enc(z, x) − y 2 2</formula><p>Note that the loss function depends directly (para- metrically) on the encoder but only indirectly on the generator via the sampled selection.</p><p>Second, we must guide the generator to realize short and coherent rationales. It should select only a few words and those selections should form phrases (consecutive words) rather than represent isolated, disconnected words. We therefore introduce an ad- ditional regularizer over the selections</p><formula xml:id="formula_8">Ω(z) = λ 1 z + λ 2 t |z t − z t−1 |</formula><p>where the first term penalizes the number of selec- tions while the second one discourages transitions (encourages continuity of selections). Note that this regularizer also depends on the generator only indi- rectly via the selected rationale. This is because it is easier to assess the rationale once produced rather than directly guide how it is obtained.</p><p>Our final cost function is the combination of the two, cost(z, x, y) = L(z, x, y) + Ω(z). Since the selections are not provided during training, we min- imize the expected cost:</p><formula xml:id="formula_9">min θe,θg (x,y)∈D E z∼gen(x) [cost(z, x, y)]</formula><p>where θ e and θ g denote the set of parameters of the encoder and generator, respectively, and D is the collection of training instances. Our joint objective encourages the generator to compress the input text into coherent summaries that work well with the as- sociated encoder it is trained with.</p><p>Minimizing the expected cost is challenging since it involves summing over all the possible choices of rationales z. This summation could potentially be made feasible with additional restrictive assump- tions about the generator and encoder. However, we assume only that it is possible to efficiently sample from the generator.</p><p>Doubly stochastic gradient We now derive a sampled approximation to the gradient of the ex- pected cost objective. This sampled approxima- tion is obtained separately for each input text x so as to work well with an overall stochastic gradient method. Consider therefore a training pair (x, y). For the parameters of the generator θ g ,</p><formula xml:id="formula_10">∂E z∼gen(x) [cost(z, x, y)] ∂θ g = z cost(z, x, y) · ∂p(z|x) ∂θ g = z cost(z, x, y) · ∂p(z|x) ∂θ g · p(z|x) p(z|x)</formula><p>Using the fact (log f (θ)) = f (θ)/f (θ), we get</p><formula xml:id="formula_11">z cost(z, x, y) · ∂p(z|x) ∂θ g · p(z|x) p(z|x) = z cost(z, x, y) · ∂ log p(z|x) ∂θ g · p(z|x) = E z∼gen(x) cost(z, x, y) ∂ log p(z|x) ∂θ g</formula><p>The last term is the expected gradient where the ex- pectation is taken with respect to the generator dis- tribution over rationales z. Therefore, we can simply sample a few rationales z from the generator gen(x) and use the resulting average gradient in an overall stochastic gradient method. A sampled approxima- tion to the gradient with respect to the encoder pa- rameters θ e can be derived similarly,</p><formula xml:id="formula_12">∂E z∼gen(x) [cost(z, x, y)] ∂θ e = z ∂cost(z, x, y) ∂θ e · p(z|x) = E z∼gen(x) ∂cost(z, x, y) ∂θ e</formula><p>Choice of recurrent unit We employ recurrent convolution (RCNN), a refinement of local-ngram based convolution. RCNN attempts to learn n-gram features that are not necessarily consecutive, and average features in a dynamic (recurrent) fashion. Specifically, for bigrams (filter width n = 2) RCNN computes h t = f (x t , h t−1 ) as follows </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of reviews 1580k Avg length of review 144.9 Avg correlation between aspects 63.5% Max correlation between two aspects 79.1% Number of annotated reviews 994</head><formula xml:id="formula_13">λ t = σ(W λ x t + U λ h t−1 + b λ ) c (1) t = λ t c<label>(1)</label></formula><formula xml:id="formula_14">t−1 + (1 − λ t ) (W 1 x t ) c (2) t = λ t c (2) t−1 + (1 − λ t ) (c (1) t−1 + W 2 x t ) h t = tanh(c (2) t + b)</formula><p>RCNN has been shown to work remarkably in clas- sification and retrieval applications ( <ref type="bibr" target="#b17">Lei et al., 2015;</ref><ref type="bibr" target="#b18">Lei et al., 2016</ref>) compared to other alternatives such CNNs and LSTMs. We use it for all the recurrent units introduced in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluate the proposed joint model on two NLP applications: (1) multi-aspect sentiment analysis on product reviews and (2) similar text retrieval on AskUbuntu question answering forum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Multi-aspect Sentiment Analysis</head><p>Dataset We use the BeerAdvocate 2 review dataset used in prior work ( <ref type="bibr" target="#b23">McAuley et al., 2012)</ref>. <ref type="bibr">3</ref> This dataset contains 1.5 million reviews written by the website users. The reviews are naturally multi- aspect -each of them contains multiple sentences describing the overall impression or one particu- lar aspect of a beer, including appearance, smell (aroma), palate and the taste. In addition to the writ- ten text, the reviewer provides the ratings (on a scale of 0 to 5 stars) for each aspect as well as an overall rating. The ratings can be fractional (e.g. 3.5 stars), so we normalize the scores to <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> and use them as the (only) supervision for regression.</p><p>McAuley   We use this set as our test set to evaluate the preci- sion of words in the extracted rationales. <ref type="table" target="#tab_0">Table 1</ref> shows several statistics of the beer review dataset. The sentiment correlation between any pair of aspects (and the overall score) is quite high, get- ting 63.5% on average and a maximum of 79.1% (between the taste and overall score). If directly training the model on this set, the model can be con- fused due to such strong correlation. We therefore perform a preprocessing step, picking "less corre- lated" examples from the dataset. <ref type="bibr">4</ref> This gives us a de-correlated subset for each aspect, each contain- ing about 80k to 90k reviews. We use 10k as the development set. We focus on three aspects since the fourth aspect taste still gets &gt; 50% correlation with the overall sentiment.</p><p>Sentiment Prediction Before training the joint model, it is worth assessing the neural encoder sepa- rately to check how accurately the neural network predicts the sentiment. To this end, we compare neural encoders with bigram SVM model, training medium and large SVM models using 260k and all <ref type="bibr">4</ref> Specifically, for each aspect we train a simple linear regres- sion model to predict the rating of this aspect given the ratings of the other four aspects. We then keep picking reviews with largest prediction error until the sentiment correlation in the se- lected subset increases dramatically. 1580k reviews respectively. As shown in <ref type="table" target="#tab_3">Table 3</ref>, the recurrent neural network models outperform the SVM model for sentiment prediction and also re- quire less training data to achieve the performance. The LSTM and RCNN units obtain similar test er- ror, getting 0.0094 and 0.0087 mean squared error respectively. The RCNN unit performs slightly bet- ter and uses less parameters. Based on the results, we choose the RCNN encoder network with 2 stack- ing layers and 200 hidden states.</p><p>To train the joint model, we also use RCNN unit with 200 states as the forward and backward recur- rent unit for the generator gen(). The dependent generator has one additional recurrent layer. For this layer we use 30 states so the dependent version still has a number of parameters comparable to the inde- pendent version. The two versions of the generator have 358k and 323k parameters respectively. <ref type="figure" target="#fig_1">Figure 2</ref> shows the performance of our joint de- pendent model when trained to predict the sentiment of all aspects. We vary the regularization λ 1 and λ 2 to show various runs that extract different amount of text as rationales. Our joint model gets performance close to the best encoder run (with full text) when few words are extracted. a beer that is not sold in my neck of the woods , but managed to get while on a roadtrip . poured into an imperial pint glass with a generous head that sustained life throughout . nothing out of the ordinary here , but a good brew s9ll . body was kind of heavy , but not thick . the hop smell was excellent and en9cing . very drinkable very dark beer . pours a nice finger and a half of creamy foam and stays throughout the beer . smells of coffee and roasted malt . has a major coffee-like taste with hints of chocolate . if you like black coffee , you will love this porter . creamy smooth mouthfeel and definitely gets smoother on the palate once it warms . it 's an ok porter but i feel there are much beAer one 's out there .</p><p>poured into a sniBer . produces a small coffee head that reduces quickly . black as night . preAy typical imp . roasted malts hit on the nose . a liAle sweet chocolate follows . big toasty character on the taste . in between i 'm geDng plenty of dark chocolate and some biAer espresso . it finishes with hop biAerness . nice smooth mouthfeel with perfect carbona9on for the style . overall a nice stout i would love to have again , maybe with some age on it .</p><p>i really did not like this . it just seemed extremely watery . i dont ' think this had any carbona9on whatsoever . maybe it was flat , who knows ? but even if i got a bad brew i do n't see how this would possibly be something i 'd get 9me and 9me again . i could taste the hops towards the middle , but the beer got preAy nasty towards the boAom . i would never drink this again , unless it was free . i 'm kind of upset i bought this . a : poured a nice dark brown with a tan colored head about half an inch thick , nice red/garnet accents when held to the light . liAle clumps of lacing all around the glass , not too shabby . not terribly impressive though s : smells like a more guinness-y guinness really , there are some roasted malts there , signature guinness smells , less burnt though , a liAle bit of chocolate … … m : rela9vely thick , it is n't an export stout or imperial stout , but s9ll is preAy heBy in the mouth , very smooth , not much carbona9on . not too shabby d : not quite as drinkable as the draught , but s9ll not too bad . i could easily see drinking a few of these .  Rationale Selection To evaluate the supporting rationales for each aspect, we train the joint encoder- generator model on each de-correlated subset. We set the cardinality regularization λ 1 between values {2e − 4, 3e − 4, 4e − 4} so the extracted rationale texts are neither too long nor too short. For simplic- ity, we set λ 2 = 2λ 1 to encourage local coherency of the extraction.</p><p>For comparison we use the bigram SVM model and implement an attention-based neural network model. The SVM model successively extracts un- igram or bigram (from the test reviews) with the highest feature. The attention-based model learns a normalized attention vector of the input tokens (us- ing similarly the forward and backward RNNs), then the model averages over the encoder states accord- ingly to the attention, and feed the averaged vector to the output layer. Similar to the SVM model, the attention-based model can selects words based on their attention weights. The smell (aroma) aspect is the target aspect. <ref type="table" target="#tab_2">Table 2</ref> presents the precision of the extracted ra- tionales calculated based on sentence-level aspect annotations. The λ 1 regularization hyper-parameter is tuned so the two versions of our model extract similar number of words as rationales. The SVM and attention-based model are constrained similarly for comparison. <ref type="figure" target="#fig_3">Figure 4</ref> further shows the preci- sion when different amounts of text are extracted. Again, for our model this corresponds to changing the λ 1 regularization. As shown in the table and the figure, our encoder-generator networks extract text pieces describing the target aspect with high preci- sion, ranging from 80% to 96% across the three as- pects appearance, smell and palate. The SVM base- line performs poorly, achieving around 30% accu- racy. The attention-based model achieves reasonable but worse performance than the rationale generator, suggesting the potential of directly modeling ratio- nales as explicit extraction. <ref type="figure" target="#fig_4">Figure 5</ref> shows the learning curves of our model for the smell aspect. In the early training epochs, both the independent and (recurrent) dependent se- lection models fail to produce good rationales, get- ting low precision as a result. After a few epochs of exploration however, the models start to achieve high accuracy. We observe that the dependent ver- sion learns more quickly in general, but both ver- sions obtain close results in the end.</p><p>Finally we conduct a qualitative case study on the extracted rationales. <ref type="figure" target="#fig_2">Figure 3</ref> presents several reviews, with highlighted rationales predicted by the model. Our rationale generator identifies key phrases or adjectives that indicate the sentiment of a particular aspect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Similar Text Retrieval on QA Forum</head><p>Dataset For our second application, we use the real-world AskUbuntu 5 dataset used in recent work <ref type="bibr" target="#b5">(dos Santos et al., 2015;</ref><ref type="bibr" target="#b18">Lei et al., 2016)</ref>. This set contains a set of 167k unique questions (each consisting a question title and a body) and 16k user- identified similar question pairs. Following previ- ous work, this data is used to train the neural en- coder that learns the vector representation of the input question, optimizing the cosine distance (i.e. cosine similarity) between similar questions against random non-similar ones. We use the "one-versus- all" hinge loss (i.e. positive versus other negatives) for the encoder, similar to ( <ref type="bibr" target="#b18">Lei et al., 2016)</ref>. Dur- ing development and testing, the model is used to score 20 candidate questions given each query ques- tion, and a total of 400×20 query-candidate question pairs are annotated for evaluation <ref type="bibr">6</ref> .</p><p>Task/Evaluation Setup The question descriptions are often long and fraught with irrelevant details. In this set-up, a fraction of the original question text should be sufficient to represent its content, and be used for retrieving similar questions. Therefore, we will evaluate rationales based on the accuracy of the question retrieval task, assuming that better ratio- nales achieve higher performance. To put this per- formance in context, we also report the accuracy when full body of a question is used, as well as ti- tles alone. The latter constitutes an upper bound on <ref type="bibr">5</ref>    the model performance as in this dataset titles pro- vide short, informative summaries of the question content. We evaluate the rationales using the mean average precision (MAP) of retrieval.</p><p>Results <ref type="table" target="#tab_5">Table 4</ref> presents the results of our ratio- nale model. We explore a range of hyper-parameter values <ref type="bibr">7</ref> . We include two runs for each version. The first one achieves the highest MAP on the develop- ment set, The second run is selected to compare the models when they use roughly 10% of question text (7 words on average). We also show the results of different runs in <ref type="figure" target="#fig_5">Figure 6</ref>. The rationales achieve the MAP up to 56.5%, getting close to using the titles. The models also outperform the baseline of using the noisy question bodies, indicating the the models' capacity of extracting short but important fragments. <ref type="figure" target="#fig_6">Figure 7</ref> shows the rationales for several questions in the AskUbuntu domain, using the recurrent ver- sion with around 10% extraction. Interestingly, the model does not always select words from the ques- tion title. The reasons are that the question body can contain the same or even complementary infor- mation useful for retrieval. Indeed, some rationale fragments shown in the figure are error messages, i accidentally removed the ubuntu soBware centre , when i was actually trying to remove my ubuntu one applica9ons . although i do n't remember directly uninstalling the centre , i think dele9ng one of those packages might have triggered it . i can not look at history of applica9on changes , as the soBware centre is missing . please advise on how to install , or rather reinstall , ubuntu soBware centre on my computer . how do i install ubuntu soBware centre applica9on ? i know this will be an odd ques9on , but i was wondering if anyone knew how to install the ubuntu installer package in an ubuntu installa9on . to clarify , when you boot up to an ubuntu livecd , it 's got the installer program available so that you can install ubuntu to a drive . naturally , this program is not present in the installed ubuntu . is there , though , a way to download and install it like other packages ? invariably , someone will ask what i 'm trying to do , and the answer … install installer package on an installed system ? what is the easiest way to install all the media codec available for ubuntu ? i am having issues with mul9ple applica9ons promp9ng me to install codecs before they can play my files . how do i install media codecs ? what should i do when i see &lt;unk&gt; report this &lt;unk&gt; ? an unresolvable problem occurred while ini9alizing the package informa9on . please report this bug against the 'update-manager ' package and include the following error message : e : encountered a sec9on with no package : header e : problem with mergelist &lt;unk&gt; e : the package lists or status file could not be parsed or opened .</p><p>please any one give the solu9on for this whenever i try to convert the rpm file to deb file i always get this problem error : &lt;unk&gt; : not an rpm package ( or package manifest ) error execu9ng `` lang=c rpm -qp -- queryformat % { name } &lt;unk&gt; ' '' : at &lt;unk&gt; line 489 thanks conver9ng rpm file to debian fle how do i mount a hibernated par99on with windows 8 in ubuntu ? i ca n't mount my other par99on with windows 8 , i have ubuntu 12.10 amd64 : error moun9ng /dev/sda1 at &lt;unk&gt; : command-linèmount -t `` n[s '' -o `` uhelper=udisks2 , nodev , nosuid , uid=1000 , gid=1000 , dmask=0077 , fmask=0177 '' `` /dev/sda1 '' `` &lt;unk&gt; '' ' exited with non-zero exit status 14 : windows is hibernated , refused to mount . failed to mount '/dev/sda1 ' : opera9on not permiAed the n[s par99on is hibernated . please resume and shutdown windows properly , or mount the volume read-only with the 'ro ' mount op9on which are typically not in the titles but very useful to identify similar questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>We proposed a novel modular neural framework to automatically generate concise yet sufficient text fragments to justify predictions made by neural net- works. We demonstrated that our encoder-generator framework, trained in an end-to-end manner, gives rise to quality rationales in the absence of any ex- plicit rationale annotations. The approach could be modified or extended in various ways to other appli- cations or types of data.</p><p>Choices of enc(·) and gen(·). The encoder and generator can be realized in numerous ways with- out changing the broader algorithm. For instance, we could use a convolutional network <ref type="bibr" target="#b16">(Kim, 2014;</ref><ref type="bibr" target="#b13">Kalchbrenner et al., 2014</ref>), deep averaging net- work ( <ref type="bibr" target="#b11">Iyyer et al., 2015;</ref><ref type="bibr" target="#b12">Joulin et al., 2016</ref>) or a boosting classifier as the encoder. When rationales can be expected to conform to repeated stereotypi- cal patterns in the text, a simpler encoder consistent with this bias can work better. We emphasize that, in this paper, rationales are flexible explanations that may vary substantially from instance to another. On the generator side, many additional constraints could be imposed to further guide acceptable rationales.</p><p>Dealing with Search Space. Our training method employs a REINFORCE-style algorithm <ref type="bibr" target="#b30">(Williams, 1992)</ref> where the gradient with respect to the param- eters is estimated by sampling possible rationales.</p><p>Additional constraints on the generator output can be helpful in alleviating problems of exploring po- tentially a large space of possible rationales in terms of their interaction with the encoder. We could also apply variance reduction techniques to increase sta- bility of stochastic training <ref type="bibr">(cf. (Weaver and Tao, 2001;</ref><ref type="bibr" target="#b24">Mnih et al., 2014;</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of a review with ranking in two categories. The rationale for Look prediction is shown in bold.</figDesc><graphic url="image-1.png" coords="1,318.96,238.76,214.83,102.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Mean squared error of all aspects on the test set (yaxis) when various percentages of text are extracted as rationales (x-axis). 220k training data is used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Examples of extracted rationales indicating the sentiments of various aspects. The extracted texts for appearance, smell and palate are shown in red, blue and green color respectively. The last example is shortened for space.</figDesc><graphic url="image-2.png" coords="7,76.77,54.76,455.84,188.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Precision (y-axis) when various percentages of text are extracted as rationales (x-axis) for the appearance aspect.</figDesc><graphic url="image-6.png" coords="7,111.49,292.53,144.80,77.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Learning curves of the optimized cost function on the development set and the precision of rationales on the test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Retrieval MAP on the test set when various percentages of the texts are chosen as rationales. Data points correspond to models trained with different hyper-parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Examples of extracted rationales of questions in the AskUbuntu domain.</figDesc><graphic url="image-7.png" coords="9,74.52,58.90,463.08,170.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : Statistics of the beer review dataset.</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>et al. (2012) also provided sentence- level annotations on around 1,000 reviews. Each sentence is annotated with one (or multiple) aspect label, indicating what aspect this sentence covers.</figDesc><table>Method 

Appearance 
Smell 
Palate 
% precision % selected % precision % selected % precision % selected 
SVM 
38.3 
13 
21.6 
7 
24.9 
7 
Attention model 
80.6 
13 
88.4 
7 
65.3 
7 
Generator (independent) 
94.8 
13 
93.8 
7 
79.3 
7 
Generator (recurrent) 
96.3 
14 
95.1 
7 
80.2 
7 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Precision of selected rationales for the first three aspects. The precision is evaluated based on whether the selected words 

are in the sentences describing the target aspect, based on the sentence-level annotations. Best training epochs are selected based 

on the objective value on the development set (no sentence annotation is used). 

D 
d 
l 
|θ| 
MSE 
SVM 
260k 
-
-2.5M 0.0154 
SVM 
1580k 
-
-7.3M 0.0100 
LSTM 
260k 200 2 644k 0.0094 
RCNN 260k 200 2 323k 0.0087 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Comparing neural encoders with bigram SVM model. MSE is the mean squared error on the test set. D is the amount of data used for training and development. d stands for the hid- den dimension, l denotes the depth of network and |θ| denotes the number of parameters (number of features for SVM).</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Comparison between rationale models (middle and 

bottom rows) and the baselines using full title or body (top row). 

</table></figure>

			<note place="foot" n="1"> Our code and data are available at https://github. com/taolei87/rcnn.</note>

			<note place="foot" n="2"> www.beeradvocate.com 3 http://snap.stanford.edu/data/ web-BeerAdvocate.html</note>

			<note place="foot" n="7"> λ1 ∈ {.008, .01, .012, .015}, λ2 = {0, λ1, 2λ1}, dropout ∈ {0.1, 0.2}</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgments</head><p>We thank Prof. Julian McAuley for sharing the re-view dataset and annotations. We also thank MIT NLP group and the reviewers for their helpful com-ments. The work is supported by the Arabic Lan-guage Technologies (ALT) group at Qatar Com-puting Research Institute (QCRI) within the IYAS project. Any opinions, findings, conclusions, or rec-ommendations expressed in this paper are those of the authors, and do not necessarily reflect the views of the funding organizations.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multiple object recognition with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Abccnn: An attention based convolutional neural network for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyuan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05960</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Long short-term memory-networks for machine reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.06733</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Extracting tree-structured representations of trained networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jude</forename><forename type="middle">W</forename><surname>Craven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shavlik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning hybrid representations to retrieve semantically equivalent questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luciano</forename><surname>Cicero Dos Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dasha</forename><surname>Barbosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianca</forename><surname>Bogdanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zadrozny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="694" to="699" />
		</imprint>
	</monogr>
	<note>Short Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Retrofitting word vectors to semantic lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sujay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sparse overcomplete word vector representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Building a shared world: mapping distributional to modeltheoretic semantic spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurélie</forename><surname>Herbelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><forename type="middle">Maria</forename><surname>Vecchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1684" to="1692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Training and analysing deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michiel</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Schrauwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="190" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep unordered composition rivals syntactic methods for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Manjunatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01759</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02078</idno>
		<title level="m">Visualizing and understanding recurrent networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mind the gap: A generative approach to interpretable feature selection and extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Empiricial Methods in Natural Language Processing</title>
		<meeting>the Empiricial Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Molding cnns for text: non-linear, non-consecutive convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semi-supervised question retrieval with gated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrishikesh</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Tymoshenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Alessandro Moschitti, and Lluís M` arquez</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Letham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><forename type="middle">H</forename><surname>Mccormick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Madigan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1350" to="1371" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Visualizing and understanding neural models in nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Robotreviewer: evaluation of a system for automatically assessing bias in clinical trials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joël</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron C</forename><surname>Kuiper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wallace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">From softmax to sparsemax: A sparse model of attention and multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramón</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fernandez Astudillo</surname></persName>
		</author>
		<idno>abs/1602.02068</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning attitudes and attributes from multi-aspect reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining (ICDM), 2012 IEEE 12th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1020" to="1025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">why should i trust you?&quot;: Explaining the predictions of any classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Marco Tulio Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Karl Moritz Hermann, Tomáš Kočisk`Kočisk`y, and Phil Blunsom. 2016. Reasoning about entailment with neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In International Conference on Learning Representations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Alexander M Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Extracting rules from artificial neural networks with distributed representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sebastian Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The optimal reward baseline for gradient-based reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lex</forename><surname>Weaver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth conference on Uncertainty in artificial intelligence</title>
		<meeting>the Seventeenth conference on Uncertainty in artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine learning</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Ask, attend and answer: Exploring question-guided spatial attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05234</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML)</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02274</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Using &quot;annotator rationales&quot; to improve machine learning for text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><surname>Zaidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><forename type="middle">D</forename><surname>Piatko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics</title>
		<meeting>Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="260" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Rationale-augmented convolutional neural networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><forename type="middle">James</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
		<idno>abs/1605.04469</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
