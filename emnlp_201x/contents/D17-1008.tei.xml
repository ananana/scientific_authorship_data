<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:31+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Train-O-Matic: Large-Scale Supervised Word Sense Disambiguation in Multiple Languages without Manual Training Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommaso</forename><surname>Pasini</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Sapienza University of Rome</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Sapienza University of Rome</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Train-O-Matic: Large-Scale Supervised Word Sense Disambiguation in Multiple Languages without Manual Training Data</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="78" to="88"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Annotating large numbers of sentences with senses is the heaviest requirement of current Word Sense Disambiguation. We present Train-O-Matic, a language-independent method for generating millions of sense-annotated training instances for virtually all meanings of words in a language&apos;s vocabulary. The approach is fully automatic: no human intervention is required and the only type of human knowledge used is a WordNet-like resource. Train-O-Matic achieves consistently state-of-the-art performance across gold standard datasets and languages, while at the same time removing the burden of manual annotation. All the training data is available for research purposes at http://trainomatic.org.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Word Sense Disambiguation (WSD) is a key task in computational lexical semantics, inasmuch as it addresses the lexical ambiguity of text by mak- ing explicit the meaning of words occurring in a given context <ref type="bibr" target="#b22">(Navigli, 2009)</ref>. Anyone who has struggled with frustratingly unintelligible transla- tions from an automatic system, or with the mean- ing bias of search engines, can understand the im- portance for an intelligent system to go beyond the surface appearance of text.</p><p>There are two mainstream lines of research in WSD: supervised and knowledge-based WSD. Su- pervised WSD frames the problem as a classi- cal machine learning task in which, first a train- ing phase occurs aimed at learning a classification model from sentences annotated with word senses and, second the model is applied to previously- unseen sentences focused on a target word. A key difference from many other problems, however, is that the classes to choose from (i.e., the senses of a target word) vary for each word, therefore requir- ing a separate training process to be performed on a word by word basis. As a result, hundreds of training instances are needed for each ambiguous word in the vocabulary. This would necessitate a million-item training set to be manually created for each language of interest, an endeavour that is currently beyond reach even in resource-rich lan- guages like English.</p><p>The second paradigm, i.e., knowledge-based WSD, takes a radically different approach: the idea is to exploit a general-purpose knowledge resource like WordNet <ref type="bibr">(Fellbaum, 1998</ref>) to de- velop an algorithm which can take advantage of the structural and lexical-semantic information in the resource to choose among the possible senses of a target word occurring in context. For ex- ample, a PageRank-based algorithm can be devel- oped to determine the probability of a given sense being reached starting from the senses of its con- text words. Recent approaches of this kind have been shown to obtain competitive results <ref type="bibr" target="#b3">(Agirre et al., 2014;</ref><ref type="bibr" target="#b21">Moro et al., 2014</ref>). However, due to its inherent nature, knowledge-based WSD tends to adopt bag-of-word approaches which do not ex- ploit the local lexical context of a target word, including function and collocation words, which limits this approach in some cases.</p><p>In this paper we get the best of both worlds and present Train-O-Matic, a novel method for gen- erating huge high-quality training sets for all the words in a language's vocabulary. The approach is language-independent, thanks to its use of a mul- tilingual knowledge resource, <ref type="bibr">BabelNet (Navigli and Ponzetto, 2012)</ref>, and it can be applied to any kind of corpus. The training sets produced with Train-O-Matic are shown to provide competitive performance with those of manually and semi-automatically tagged corpora. Moreover, state-of- the-art performance is also reported for low re- sourced languages (i.e., Italian and Spanish) and domains, where manual training data is not avail- able.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Building a Training Set from Scratch</head><p>In this Section we present Train-O-Matic, a language-independent approach to the automatic construction of a sense-tagged training set. Train- O-Matic takes as input a corpus C (e.g., Wikipedia) and a semantic network G = (V, E). We assume a WordNet-like structure of G, i.e., V is the set of concepts (i.e., synsets) such that, for each word w in the vocabulary, Senses(w) is the set of vertices in V that are expressed by w, e.g., the WordNet synsets that include w as one of their senses.</p><p>Train-O-Matic consists of three steps:</p><p>• Lexical profiling: for each vertex in the se- mantic network, we compute its Personalized PageRank vector, which provides its lexical- semantic profile (Section 2.1).</p><p>• Sentence scoring: For each sentence con- taining a word w, we compute a probability distribution over all the senses of w based on its context (Section 2.2).</p><p>• Sentence ranking and selection: for each sense s of a word w in the vocabulary, we select those sentences that are most likely to use w in the sense of s (Section 2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Lexical profiling</head><p>In terms of semantic networks the probability of reaching a node v starting from v can be inter- preted as a measure of relatedness between the synsets v and v . Thus we define the lexical profile of a vertex v in a graph G = (V, E) as the prob- ability distribution over all the vertices v in the graph. Such distribution is computed by applying the Personalized PagaRank algorithm, a variant of the traditional PageRank ( <ref type="bibr" target="#b6">Brin and Page, 1998)</ref>. While the latter is equivalent to performing ran- dom walks with uniform restart probability on ev- ery vertex at each step, PPR, on the other hand, makes the restart probability non-uniform, thereby concentrating more probability mass in the sur- roundings of those vertices having higher restart probability. Formally, (P)PR is computed as fol- lows:</p><formula xml:id="formula_0">v (t+1) = (1 − α)v (0) + αM v (t) (1)</formula><p>where M is the row-normalized adjacency ma- trix of the semantic network, the restart probabil- ity distribution is encoded by vector v (0) , and α is the well-known damping factor usually set to 0.85 <ref type="bibr" target="#b6">(Brin and Page, 1998</ref>). If we set v (0) to a unit probability vector (0, . . . , 0, 1, 0, . . . , 0), i.e., restart is always on a given vertex, PPR outputs the probability of reaching every vertex starting from the restart vertex after a certain number of steps. This approach has been used in the literature to create semantic signatures (i.e., profiles) of indi- vidual concepts, i.e., vertices of the semantic net- work ( <ref type="bibr" target="#b27">Pilehvar et al., 2013)</ref>, and then to determine the semantic similarity of concepts. As also done by <ref type="bibr" target="#b26">Pilehvar and Collier (2016)</ref>, we instead use the PPR vector as an estimate of the conditional prob- ability of a word w given the target sense 1 s ∈ V of word w:</p><formula xml:id="formula_1">P (w |s, w) = max s ∈Senses(w ) v s (s ) Z<label>(2)</label></formula><p>where Z = w" P (w"|s, w) is a normalization constant, v s is the vector resulting from an ade- quate number of random walks used to calculate PPR, and v s (s ) is the vector component corre- sponding to sense s . To fix the number of iter- ations needed to have a sufficiently accurate vec- tor, we follow <ref type="bibr" target="#b16">Lofgren et al. (2014)</ref> and set the error δ = 0.00001 and the number of iterations to 1 δ = 100, 000.</p><p>As a result of this lexical profiling step we have a probability distribution over vocabulary words for each given word sense of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Sentence scoring</head><p>The objective of the second step is to score the im- portance of word senses for each of the corpus sen- tences which contain the word of interest. Given a sentence σ = w 1 , w 2 , . . . , w n , for a given target word w in the sentence (w ∈ σ), and for each of its senses s ∈ Senses(w), we compute the probabil- ity P (s|σ, w). Thanks to Bayes' theorem we can determine the probability of sense s of w given the sentence as follows:</p><formula xml:id="formula_2">P (s|σ, w) = P (σ|s, w)P (s|w) P (σ|w)<label>(3)</label></formula><p>= P (w 1 , . . . , w n |s, w)P (s|w) P (w 1 , . . . , w n |w) ∝ P (w 1 , . . . , w n |s, w)P (s|w)</p><p>≈ P (w 1 |s, w) . . . P (w n |s, w)P (s|w)</p><p>where Formula 4 is proportional to the original probability (due to removing the constant in the denominator) and is approximated with Formula 5 due to the assumption of independence of the words in the sentence. P (w i |s, w) is calculated as in Formula 2 and P (s|w) is set to 1/|Senses(w)| (recall that s is a sense of w). For example, given the sentence σ = "A match is a tool for starting a fire", the target word w = match and its set of senses S match = {s 1 match , s 2 match }, where s 1 match is the sense of lighter and s 2 match is the sense of game match, we want to calculate the probability of each s i match ∈ S match of being the correct sense of match in the sentence σ. Following Formula 5 we have:</p><formula xml:id="formula_5">P (s 1 match |σ, match) ≈ P (tool|s 1 match , match) · P (start|s 1 match , match) · P (fire|s 1 match , match) · P (s 1 match |match) = 2.1 · 10 −4 · 2 · 10 −3 · 10 −2 · 5 · 10 −1 = 2.1 · 10 −9 P (s 2 match |σ, match) ≈ P (tool|s 2 match , match) · P (start|s 2 match , match) · P (fire|s 2 match , match) · P (s 2 match |match) = 10 −5 · 2.9 · 10 −4 · 10 −6 · 5 · 10 −1 = 1.45 · 10 −15</formula><p>As can be seen, the first sense of match has a much higher probability due to its stronger relatedness to the other words in the context (i.e. start, fire and tool). Note also that all the probabilities for the second sense are at least one magnitude less than the probability of the first sense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Sense-based sentence ranking and selection</head><p>Finally, for a given word w and a given sense s 1 ∈ Senses(w), we score each sentence σ in which w appears and s 1 is its most likely sense according to a formula that takes into account the difference between the first (i.e., s 1 ) and the sec- ond most likely sense of w in σ:</p><formula xml:id="formula_6">∆ s 1 (σ) = P (s 1 |σ, w) − P (s 2 |σ, w)<label>(6)</label></formula><p>where s 1 = arg max s∈Senses(w) P (s|σ, w), and s 2 = arg max s∈Senses(w)\{s 1 } P (s|σ, w). We then sort all sentences based on ∆ s 1 (·) and return a ranked list of sentences where word w is most likely to be sense-annotated with s 1 . Although we recognize that other scoring strategies could have been used, this was experimentally the most effec- tive one when compared to alternative strategies, i.e., the sense probability, the number of words re- lated to the target word w, the sentence length or a combination thereof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Creating a Denser and Multilingual Semantic Network</head><p>In the previous Section we assumed that WordNet was our semantic network, with synsets as vertices and edges represented by its semantic relations. However, while its lexical coverage is high, with a rich set of fine-grained synsets, at the relation level WordNet provides mainly paradigmatic in- formation, i.e., relations like hypernymy (is-a) and meronymy (part-of). It lacks, on the other hand, syntagmatic relations, such as those that connect verb synsets to their arguments (e.g., the appro- priate senses of eat v and food n ), or pairs of noun synsets (e.g., the appropriate senses of bus n and driver n ).</p><p>Intuitively, Train-O-Matic would suffer from such a lack of syntagmatic relations, as the rel- evance of a sense for a given word in a sen- tence depends directly on the possibility of vis- iting senses of the other words in the same sen- tence (cf. Formula 5) via random walks as calcu- lated with Formula 1. Such reachability depends on the connections available between synsets. Be- cause syntagmatic relations are sparse in Word- Net, if it was used on its own, we would end up with a poor ranking of sentences for any given word sense. Moreover, even though the methodology presented in Section 2 is language- independent, Train-O-Matic would lack informa-  <ref type="table">Table 1</ref>: Top-ranking synsets of the PPR vectors computed on WordNet (first and third columns) and WordNet BN (second and fourth columns) for mouse as animal (left) and as device (right).</p><formula xml:id="formula_7">mouse (animal) mouse (device) WordNet WordNet BN WordNet WordNet BN mouse 1 n mouse 1 n mouse 4</formula><p>tion (e.g. senses for a word in an arbitrary vocab- ulary) for languages other than English.</p><p>To cope with these issues, we exploit Babel- Net, 2 a huge multilingual semantic network ob- tained from the automatic integration of WordNet, Wikipedia, Wiktionary and other resources <ref type="bibr" target="#b24">(Navigli and Ponzetto, 2012)</ref>, and create the Babel- Net subgraph induced by the WordNet vertices. The result is a graph whose vertices are BabelNet synsets that contain at least one WordNet synset and whose edge set includes all those relations in BabelNet coming either from WordNet itself or from links in other resources mapped to Word- Net (such as hyperlinks in a Wikipedia article con- necting it to other articles). The greatest contribu- tion of syntagmatic relations comes, indeed, from Wikipedia, as its articles are linked to related ar- ticles (e.g., the English Wikipedia Bus article 3 is linked to Passenger, Tourism, Bus lane, Timetable, School, and many more).</p><p>Because not all Wikipedia (and other re- sources') pages are connected with the same degree of relatedness (e.g., countries are often linked, but they are not necessarily closely related to the source article in which the link occurs), we apply the following weighting strategy to each edge (s, s ) ∈ E of our WordNet-induced sub- graph of BabelNet G = (V, E): overlap measure which calculates the similarity between two synsets:</p><formula xml:id="formula_8">w(s, s ) = 1 (s, s ) ∈ E(WordNet) W O(</formula><formula xml:id="formula_9">W O(s, s ) = |S| i=1 (r 1 i + r 2 i ) −1 |S| i=1 (2i) −1</formula><p>where r 1 i and r 2 i are the rankings of the i-th synsets in the set S of the components in common between the vectors associated with s and s , respectively. Because at this stage we still have to calculate our synset vector representation, we use the pre- computed NASARI vectors <ref type="bibr" target="#b7">(Camacho-Collados et al., 2015</ref>) to calculate WO. This choice is due to WO's higher performance over cosine similar- ity for vectors with explicit dimensions ( <ref type="bibr" target="#b27">Pilehvar et al., 2013)</ref>.</p><p>As a result, each row of the original adjacency matrix M of G will be replaced with the weights calculated in Formula 7 and then normalized in order to be ready for PPR calculation (see For- mula 1). An idea of why a denser semantic net- work has more useful connections and thus leads to better results is provided by the example in <ref type="table">Table 1</ref>  <ref type="bibr">4</ref> , where we show the highest-probability synsets in the PPR vectors calculated with For- mula 1 for two different senses of mouse (its animal and device senses) when WordNet (left) and our WordNet-induced subgraph of BabelNet (WordNet BN , right) are used as the underlying semantic network for PPR computation. Note that WordNet's top synsets are related to the tar- get synset via paradigmatic (i.e., hypernymy and meronymy) relations, while WordNet BN includes many syntagmatically-related synsets (e.g., exper-iment for the animal, and operating system and Windows for the device sense, among others).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>Corpora for sense annotation We used two dif- ferent corpora to extract sentences: Wikipedia and the United Nations Parallel Corpus ( <ref type="bibr" target="#b39">Ziemski et al., 2016</ref>). The first is the largest and most up-to-date encyclopedic resource, containing definitional in- formation, the second, on the other hand, is a public collection of parliamentary documents of the United Nations. The application of Train- O-Matic to the two corpora produced two sense- annotated datasets, which we named T-O-M W iki and T-O-M U N , respectively.</p><p>Semantic Network We created sense-annotated corpora with Train-O-Matic both when using PPR vectors computed from vanilla WordNet and when using WordNet BN , our denser network obtained from the WordNet-induced subgraph of BabelNet (see Section 3).</p><p>Gold standard datasets We performed our evaluations using the framework made available by <ref type="bibr" target="#b30">Raganato et al. (2017a)</ref> on five different all- words datasets, namely: the Senseval-2 (Ed- monds and Cotton, 2001), Senseval-3 (Snyder and Palmer, 2004), <ref type="bibr">SemEval-2007</ref><ref type="bibr" target="#b29">(Pradhan et al., 2007</ref>, <ref type="bibr">SemEval-2013</ref> and <ref type="bibr">SemEval-2015 (Moro and</ref>) WSD datasets. We focused on nouns only, given the fact that Wikipedia provides connections between nominal synsets only, and therefore contributes mainly to syntagmatic relations between nouns.</p><p>Comparison sense-annotated corpora To show the impact of our T-O-M corpora in WSD, we compared its performance on the above gold standard datasets, against training with:</p><p>• SemCor ( <ref type="bibr" target="#b19">Miller et al., 1993</ref>), a corpus con- taining about 226,000 words annotated man- ually with WordNet senses.</p><p>• One Million Sense-Tagged Instances (Taghipour and Ng, 2015, OMSTI), a sense-annotated dataset obtained via a semi-automatic approach based on the disambiguation of a parallel corpus, i.e., the United Nations Parallel Corpus, performed by exploiting manually translated word senses. Because OMSTI integrates SemCor to increase coverage, to keep a level playing field we excluded the latter from the corpus.</p><p>We note that T-O-M, instead, is fully automatic and does not require any WSD-specific human in- tervention nor any aligned corpus.</p><p>Reference system In all our experiments, we used It Makes Sense ( <ref type="bibr">Zhong and Ng, 2010, IMS)</ref>, a state-of-the-art WSD system based on linear Support Vector Machines, as our reference system for comparing its performance when trained on T- O-M, against the same WSD system trained on other sense-annotated corpora (i.e., SemCor and OMSTI). Following the WSD literature, unless stated otherwise, we report performance in terms of F1, i.e., the harmonic mean of precision and re- call.</p><p>We note that it is not the purpose of this paper to show that T-O-M, when integrated into IMS, beats all other configurations or alternative systems, but rather to fully automatize the WSD pipeline with performances which are competitive with the state of the art.</p><p>Baseline As a traditional baseline in WSD, we used the Most Frequent Sense (MFS) baseline given by the first sense in WordNet. The MFS is a very competitive baseline, due to the sense skew- ness phenomenon in language <ref type="bibr" target="#b22">(Navigli, 2009</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of training sentences per sense</head><p>Given a target word w, we sorted its senses Senses(w) following the WordNet ordering and selected the top k i training sentences for the i-th sense accord- ing to Formula 6, where:</p><formula xml:id="formula_10">k i = 1 i z * K<label>(8)</label></formula><p>with K = 500 and z = 2 which were tuned on a separate small in-house development dataset 5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Impact of syntagmatic relations</head><p>The first result we report regards the impact of vanilla WordNet vs. our WordNet-induced sub- graph of BabelNet (WordNet BN ) when calculat- ing PPR vectors. As can be seen from <ref type="table" target="#tab_1">Table 2</ref> - which shows the performance of the T-O-M W iki corpora generated with the two semantic networks -using WordNet for PPR computation decreases</p><formula xml:id="formula_11">Dataset T-O-M W iki BN T-O-M W iki WN Senseval-2</formula><p>70.5 70.0 Senseval-3 67.4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="63.1">SemEval-07</head><p>59.8 57.9 SemEval-13 65.5 63.7 SemEval-15</p><p>68.6 69.5 ALL 67.3 65.7 the overall performance of IMS from 0.5 to around 4 points across the five datasets, with an overall loss of 1.6 F1 points. Similar performance losses were observed when using T-O-M U N (see <ref type="table" target="#tab_3">Table  3</ref>). This corroborates our hunch discussed in Sec- tion 3 that a resource like BabelNet can contribute important syntagmatic relations that are beneficial for identifying (and ranking high) sentences which are semantically relevant for the target word sense.</p><p>In the following experiments, we report only re- sults using WordNet BN .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison against sense-annotated corpora</head><p>We now move to comparing the performance of T-O-M, which is fully automatic, against cor- pora which are annotated manually (SemCor) and semi-automatically (OMSTI). In <ref type="table" target="#tab_3">Table 3</ref> we show the F1-score of IMS on each gold standard dataset in the evaluation framework and on all datasets merged together (last row), when it is trained with the various corpora described above.</p><p>As can be seen, T-O-M W iki and T-O-M U N ob- tain higher performance than OMSTI (up to 5.5 points above) on 3 out of 5 datasets, and, over- all, T-O-M W iki scores 1 point above OMSTI. The MFS is in the same ballpark as T-O-M W iki , per- forming better on some datasets and worse on oth- ers. We note that IMS trained on T-O-M W iki succeeds in surpassing or obtaining the same re- sults as IMS trained on SemCor on SemEval- 15 and SemEval-13. We view this as a signifi- cant achievement given the total absence of man- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Performance without backoff strategy</head><p>IMS uses the MFS as a backoff strategy when no sense can be output for a target word in context ( <ref type="bibr" target="#b38">Zhong and Ng, 2010)</ref>. Consequently, the perfor- mance of the MFS is mixed up with that of the SVM classifier. As shown in <ref type="table" target="#tab_4">Table 4</ref>, OMSTI is able to provide annotated sentences for roughly half of the tokens in the datasets. Train-O-Matic, on the other hand, is able to cover almost all words in each dataset with at least one training sentence. This means that in around 50% of cases OMSTI gives an answer based on the IMS backoff strat- egy.</p><p>To determine the real impact of the different training data, we therefore decided to perform an additional analysis of the IMS performance when the MFS backoff strategy is disabled. Because we suspected the system would not always return a sense for each target word, in this experiment we measured precision, recall and their harmonic mean, i.e., F1. The results in <ref type="table">Table 5</ref> confirm our hunch, showing that OMSTI's recall drops heav- ily, thereby affecting F1 considerably. T-O-M per- formances, instead, remain high in terms of pre- cision, recall and F1. This confirms that OMSTI relies heavily on data (those obtained for the MFS and from SemCor) that are produced manually, rather than semi-automatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Domain-oriented WSD</head><p>To further inspect the ability of T-O-M to enable disambiguation in different domains, we decided to evaluate on specific documents from the vari- ous gold standard datasets which could be clearly assigned a domain label. Specifically, we tested on 13 SemEval-13 documents from various domains <ref type="bibr">6</ref> and 2 SemEval-15 documents (namely, maths &amp; computers, and biomedicine) and carried out two separate tests and evaluations of T-O-M on each domain: once using the MFS backoff strategy, and once not using it. In <ref type="table" target="#tab_6">Tables 6 and 7</ref> we report the results of both T-O-M W iki and T-O-M U N to deter- mine the impact of the corpus type.</p><p>As can be seen in the tables, T-O-M W iki sys- tematically attains higher scores than OMSTI (ex- cept for the biology domain), and, in most cases, attains higher scores than MFS when the backoff is used, with a drastic, systematic increase over OMSTI with both Train-O-Matic configurations    <ref type="table">Table 5</ref>: Precision, Recall and F1 of IMS trained on OMSTI and Train-O-Matic corpus without MFS backoff strategy for Senseval-2, Senseval-3, SemEval-07, SemEval-13 and SemEval-15.</p><formula xml:id="formula_12">Dataset Train-O-Matic W iki Train-O-Matic U N OMSTI SemCor MFS Senseval</formula><p>in recall and F1 when the backoff strategy is dis- abled. This demonstrates the usefulness of the cor- pora annotated by Train-O-Matic not only on open text, but also on specific domains. We note that T-O-M U N obtains the best results in the politics domain, which is the closest domain to the UN corpus from which its training sentences are ob- tained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Scaling up to Multiple Languages</head><p>Experimental Setup In this section we investi- gate the ability of Train-O-Matic to scale to low- resourced languages, such as Italian and Spanish, for which training data for WSD is not available. Thanks to BabelNet, in fact, Train-O-Matic can be used to generate sense-annotated data for any language supported by the knowledge base. Thus, in order to build new training datasets for the two languages, we ran Train-O-Matic on their corre- sponding versions of Wikipedia, then we tuned the two parameters K and z on an in-house develop- ment dataset <ref type="bibr">7</ref> . In contrast to the English setting, in order to calculate Formula 8 we sorted the senses of each word by vertex degree. Finally we used the output data to train IMS.</p><p>Results To perform our evaluation we chose the most recent multilingual task (SemEval 2015 task 13) which includes gold data for Italian and Spanish. As can be seen from <ref type="table" target="#tab_8">Table 8</ref> Train- O-Matic enabled IMS to perform better than the best participating system (Manion and Sainudiin, 2014, SUDOKU) in all three settings (All do- mains, Maths &amp; Computer and Biomedicine). Its performance was in fact, 1 to 3 points higher, with a 6-point peak on Maths &amp; Computer in Span- ish and on Biomedicine in Italian. This demon- strates the ability of Train-O-Matic to enable su- pervised WSD systems to surpass state-of-the- art knowledge-based WSD approaches in low- resourced languages without relying on manually curated data for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>There are two mainstream approaches to Word Sense Disambiguation: supervised and knowledge-based approaches. Both suffer in different ways from the so-called knowledge acquisition bottleneck, that is, the difficulty in obtaining an adequate amount of lexical-semantic data: for training in the case of supervised sys- tems, and for enriching semantic networks in the case of knowledge-based ones (Pilehvar and     <ref type="bibr" target="#b22">Navigli, 2009)</ref>.</p><formula xml:id="formula_13">Domain Backoff T-O-M W iki T-O-MUN OMSTI SemCor MFS Size P R F1 P R F1 P R F1 F1</formula><p>State-of-the-art supervised systems include Support Vector Machines such as IMS ( <ref type="bibr" target="#b38">Zhong and Ng, 2010)</ref> and, more recently, LSTM neural net- works with attention and multitask learning <ref type="bibr" target="#b32">(Raganato et al., 2017b</ref>) as well as LSTMs paired with nearest neighbours classification ( <ref type="bibr" target="#b18">Melamud et al., 2016;</ref><ref type="bibr" target="#b37">Yuan et al., 2016)</ref>. The latter also in- tegrates a label propagation algorithm in order to enrich the sense annotated dataset. The main dif- ference from our approach is its need for a man- ually annotated dataset to start the label propaga- tion algorithm, whereas Train-O-Matic is fully au- tomatic. An evaluation against this system would have been interesting, but neither the proprietary training data nor the code are available at the time of writing.</p><p>In order to generalize effectively, these super- vised systems require large numbers of training in- stances annotated with senses for each target word occurrence. Overall, this amounts to millions of training instances for each language of interest, a number that is not within reach for any lan- guage. In fact, no supervised system has been sub- mitted in major multilingual WSD competitions for languages other than English ( <ref type="bibr" target="#b20">Moro and Navigli, 2015)</ref>. To overcome this problem, new methodologies have recently been developed which aim to create sense-tagged cor- pora automatically. <ref type="bibr" target="#b31">Raganato et al. (2016)</ref> devel- oped 7 heuristics to grow the number of hyperlinks in Wikipedia pages. <ref type="bibr" target="#b25">Otegi et al. (2016)</ref> applied a different disambiguation pipeline for each lan- guage to parallel text in Europarl ( <ref type="bibr" target="#b13">Koehn, 2005)</ref> and QTLeap ( <ref type="bibr" target="#b0">Agirre et al., 2015</ref>) in order to enrich them with semantic annotations. <ref type="bibr" target="#b34">Taghipour and Ng (2015)</ref>, the work closest to ours, exploits the alignment from English to Chinese sentences of the United Nation Parallel Corpus ( <ref type="bibr" target="#b39">Ziemski et al., 2016)</ref> to reduce the ambiguity of English words and sense-tag English sentences. The assump- tion is that the second language is less ambiguous than the first one and that hand-made translations of senses are available for each WordNet synset. This approach is, therefore, semi-automatic and relies on certain assumptions, in contrast to Train- O-Matic which is, instead, fully automatic and can be applied to any kind of corpus (and lan- guage) depending on the specific need. Earlier attempts at the automatic extraction of training samples were made by <ref type="bibr" target="#b2">Agirre and De Lacalle (2004)</ref> and <ref type="bibr" target="#b10">Fernández et al. (2004)</ref>. Both exploited the monosemous relatives method ( <ref type="bibr" target="#b14">Leacock et al., 1998</ref>) in order to retrieve sentences from the Web which contained a given monosemous noun or a relative monosemous word (e.g., a synonym, a hy- pernym, etc.). As can be seen in <ref type="bibr" target="#b10">(Fernández et al., 2004</ref>) this approach can lead to the retrieval of very accurate examples, but its main drawback lies in the number of senses covered. In fact, for all those synsets that do not have any monosemous relative, the system is unable to retrieve examples, thus heavily affecting the performance in terms of recall and F1. Knowledge-based WSD, instead, bypasses the heavy requirement of sense-annotated corpora by applying algorithms that exploit a general-purpose semantic network, such as WordNet, which en- codes the relational information that interconnects synsets via different kinds of relation. Approaches include variants of Personalized PageRank ( <ref type="bibr" target="#b3">Agirre et al., 2014</ref>) and densest subgraph approxima- tion algorithms <ref type="bibr" target="#b21">(Moro et al., 2014</ref>) which, thanks to the availability of multilingual resources such as BabelNet, can easily be extended to perform WSD in arbitrary languages. Other approaches to knowledge-based WSD exploit the definitional knowledge contained in a dictionary. The Lesk al- gorithm <ref type="bibr" target="#b15">(Lesk, 1986)</ref> and its variants ( <ref type="bibr" target="#b4">Banerjee and Pedersen, 2002;</ref><ref type="bibr" target="#b12">Kilgarriff and Rosenzweig, 2000;</ref><ref type="bibr" target="#b36">Vasilescu et al., 2004</ref>) aim to determine the correct sense of a word by comparing each word- sense definition with the context in which the tar- get word appears. The limit of knowledge-based WSD, however, lies in the absence of mechanisms that can take into account the very local context of a target word occurrence, including non-content words such as prepositions and articles. Further- more, recent studies seem to suggest that such approaches are barely able to surpass supervised WSD systems when they enrich their networks starting from a comparable amount of annotated data <ref type="bibr" target="#b28">(Pilehvar and Navigli, 2014</ref>). With T-O-M, rather than further enriching an existing semantic network, we exploit the information available in the network to annotate raw sentences with sense information and train a state-of-the-art supervised WSD system without task-specific human annota- tions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this paper we presented Train-O-Matic, a novel approach to the automatic construction of large training sets for supervised WSD in an arbitrary language. Train-O-Matic removes the burden of manual intervention by leveraging the structural semantic information available in the WordNet graph enriched with additional relational infor- mation from BabelNet, and achieves performance competitive to that of semi-automatic approaches and, in some cases, of manually-curated train- ing data. T-O-M was shown to provide training data for virtually all the target ambiguous nouns, in marked contrast to alternatives like OMSTI, which covers in many cases around half of the to- kens, resorting to the MFS otherwise. Moreover Train-O-Matic has proven to scale well to low- resourced languages, for which no manually an- notated dataset exists, surpassing the current state of the art of knowledge-based systems.</p><p>We believe that the ability of T-O-M to over- come the current paucity of annotated data for WSD, coupled with video games with a pur- pose for validation purposes <ref type="bibr" target="#b35">Vannella et al., 2014</ref>), paves the way for high-quality multilingual supervised WSD. All the training corpora, including approximately one million sentences which cover English, Italian and Spanish, are made available to the community at http://trainomatic.org.</p><p>As future work we plan to extend our approach to verbs, adjectives and adverbs. Following Ben- nett et al. (2016) we will also experiment on more realistic estimates of P (s|w) in Formula 5 as well as other assumptions made in our work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>ual effort involved in T-O-M. Because overall T-O-M W iki outperforms T-O-M U N , in what fol- lows we report all the results with T-O-M W iki , ex- cept for the domain-oriented evaluation (see Sec- tion 5.4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>F1 of IMS trained on T-O-M when PPR is 
obtained from the WordNet graph (WN) and from 
the WordNet-induced subgraph of BabelNet (BN). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>F1 of IMS trained on Train-O-Matic, OMSTI and SemCor, and MFS for the Senseval-2, 
Senseval-3, SemEval-07, SemEval-13 and SemEval-15 datasets. 

Dataset 
OMSTI Train-O-Matic Total 
Senseval-2 
469 
1005 1066 
Senseval-3 
494 
860 
900 
Semeval-07 
89 
159 
159 
Semeval-13 
757 
1428 1644 
Semeval-15 
249 
494 
531 
ALL 
2058 
3946 4300 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Number of nominal tokens for which at 
least one training example is provided by OMSTI 
or Train-O-Matic for each dataset. 

Dataset 
OMSTI 
Train-O-Matic 
P 
R 
F1 
P 
R 
F1 
Senseval-2 
64.8 28.5 39.6 69.5 65.5 67.4 
Senseval-3 
55.7 31.0 39.8 66.1 63.1 64.6 
SemEval-07 64.1 35.9 46.0 59.8 59.8 59.8 
SemEval-13 50.7 23.4 32.0 61.3 53.3 57.0 
SemEval-15 57.0 26.7 36.4 67.0 62.3 64.6 
ALL 
56.5 27.0 36.5 65.1 59.7 62.3 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Performance comparison over SemEval-2013 domain-specific datasets. 

T-O-M W iki 
T-O-MUN 
OMSTI 
SemCor 
MFS 
Size 
Domain 
Backoff 
P 
R 
F1 
P 
R 
F1 
P 
R 
F1 
F1 
F1 

Biomedicine 
MFS 
76.3 76.3 76.3 
66.0 66.0 66.0 
64.9 64.9 64.9 
70.3 
71.1 
100 
-
76.1 72.2 74.1 
64.4 59.8 62.0 
60.5 26.8 37.2 
-
Maths &amp; 
MFS 
50.0 50.0 50.0 
48.0 48.0 48.0 
36.0 36.0 36.0 
40.6 
40.9 
97 
Computer 
-
50.0 47.0 48.5 
47.8 44.0 45.8 
21.2 11.0 14.5 
-

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="true"><head>Table 7 : Performance comparison over the Biomedical and Maths &amp; Computer domains in SemEval-15.</head><label>7</label><figDesc></figDesc><table>Language Dataset 
Best System 
Train-O-Matic 
F1 
P 
R 
F1 

Italian 
ALL 
56.6 
65.1 55.6 59.9 
Computers &amp; Math 
46.6 
52.7 43.3 47.6 
Biomedicine 
65.9 
76.6 67.6 71.8 

Spanish 
ALL 
56.3 
61.3 54.8 57.9 
Computers &amp; Math 
42.4 
53.3 44.4 48.5 
Biomedicine 
65.5 
71.8 65.5 68.5 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>Performance comparison between T-O-M and SemEval-2015's best SUDOKU Run. 

</table></figure>

			<note place="foot" n="1"> Note that we use senses and concepts (synsets) interchangeably, because-given a word-a word sense unambiguously determines a concept (i.e., the synset it is contained in) and vice versa.</note>

			<note place="foot" n="4"> We use the notation w k p introduced in (Navigli, 2009) to denote the k-th sense of word w with part-of-speech tag p.</note>

			<note place="foot" n="5"> 50 word-sense pairs annotated manually.</note>

			<note place="foot" n="6"> Namely biology, climate, finance, health care, politics, social issues and sport.</note>

			<note place="foot" n="7"> We set K = 100 and z = 2.3 for Spanish and K = 100 and z = 2.5 for Italian.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors gratefully acknowledge the support of the ERC Consolidator Grant MOUSSE No. 726487.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">António</forename><surname>Branco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Popel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiril</forename><surname>Simov</surname></persName>
		</author>
		<title level="m">Europarl QTLeap WSD/NED corpus</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<title level="m">LINDAT/CLARIN digital library at the Institute of Formal and Applied Linguistics</title>
		<imprint/>
		<respStmt>
			<orgName>Charles University in Prague</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Publicly available topic signatures for all wordnet nominal senses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oier Lopez De</forename><surname>Lacalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Random walks for knowledge-based word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oier</forename><surname>Lopez De Lacalle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Soroa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="84" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An adapted Lesk algorithm for word sense disambiguation using wordnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satanjeev</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Text Processing and Computational Linguistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="136" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Lexsemtm: A semantic dataset based on all-words unsupervised sense distribution learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jey</forename><forename type="middle">Han</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54nd Annual Meeting of the Association for Computational Linguistics (ACL 2016)</title>
		<meeting>the 54nd Annual Meeting of the Association for Computational Linguistics (ACL 2016)<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1513" to="1524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The anatomy of a large-scale hypertextual web search engine. Computer Networks and ISDN Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Page</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="107" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Nasari: a novel approach to a semantically-aware representation of items</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Taher</forename><surname>Pilehvar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="567" to="577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Senseval-2: overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Edmonds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cotton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Proceedings of the Second International Workshop on Evaluating Word Sense Disambiguation Systems</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">WordNet: An Electronic Database</title>
		<editor>Christiane Fellbaum</editor>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic acquisition of sense examples using exretriever</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><forename type="middle">Castillo</forename><surname>Valdés</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><forename type="middle">Rigau</forename><surname>Claramunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><forename type="middle">Atserias</forename><surname>Batalla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Tormo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IBERAMIA Workshop on Lexical Resources and The Web for Word Sense Disambiguation</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">It&apos;s All Fun and Games until Someone Annotates: Video Games with a Purpose for Linguistic Annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="449" to="464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Framework and results for english SENSEVAL</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kilgarriff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Rosenzweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and the Humanities</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="15" to="48" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Europarl: A parallel corpus for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MT summit</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Using corpus statistics and wordnet relations for sense identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Leacock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chodorow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="147" to="165" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from an ice cream cone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lesk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Annual Conference on Systems Documentation</title>
		<meeting>the 5th Annual Conference on Systems Documentation<address><addrLine>Toronto, Ontario, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1986" />
			<biblScope unit="page" from="24" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fast-ppr: Scaling personalized pagerank estimation for large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Peter A Lofgren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seshadhri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1436" to="1445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An iterative &quot;sudoku style&quot; approach to subgraph-based word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Steve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raazesh</forename><surname>Manion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sainudiin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Joint Conference on Lexical and Computational Se-mantics (*SEM 2014)</title>
		<meeting>the Third Joint Conference on Lexical and Computational Se-mantics (*SEM 2014)<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="40" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning generic context embedding with bidirectional lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Melamud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CONLL</title>
		<meeting>CONLL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="51" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A semantic concordance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Leacock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randee</forename><surname>Tengi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Bunker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd DARPA Workshop on Human Language Technology</title>
		<meeting>the 3rd DARPA Workshop on Human Language Technology<address><addrLine>Plainsboro, N.J.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="303" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Moro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<title level="m">Semeval2015 task 13: Multilingual all-words sense disambiguation and entity linking. Proc. of SemEval2015</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Entity Linking meets Word Sense Disambiguation: a Unified Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Moro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Raganato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="231" to="244" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Word Sense Disambiguation: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="69" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semeval-2013 task 12: Multilingual word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Vannella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7 th International Workshop on Semantic Evaluation (SemEval 2013), in conjunction with the Second Joint Conference on Lexical and Computational Semantics (*SEM 2013)</title>
		<meeting>the 7 th International Workshop on Semantic Evaluation (SemEval 2013), in conjunction with the Second Joint Conference on Lexical and Computational Semantics (*SEM 2013)<address><addrLine>Atlanta, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="222" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><forename type="middle">Paolo</forename><surname>Ponzetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">193</biblScope>
			<biblScope unit="page" from="217" to="250" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Qtleap wsd/ned corpora: Semantic annotation of parallel corpora in six languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Otegi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nora</forename><surname>Aranberri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Branco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Neale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petya</forename><surname>Osenova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Popel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiril</forename><surname>Simov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Language Resources and Evaluation Conference, LREC</title>
		<meeting>the 10th Language Resources and Evaluation Conference, LREC</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3023" to="3030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">De-conflated semantic representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Taher Pilehvar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Collier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1680" to="1690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Align, disambiguate and walk: A unified approach for measuring semantic similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Taher Pilehvar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1341" to="1351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A large-scale pseudoword-based evaluation framework for state-of-the-art word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Taher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pilehvar</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="837" to="881" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semeval-2007 task 17: English lexical sample, srl and all words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Sameer S Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Dligach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Workshop on Semantic Evaluations</title>
		<meeting>the 4th International Workshop on Semantic Evaluations</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="87" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Word Sense Disambiguation: A Unified Evaluation Framework and Empirical Comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Raganato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="99" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Automatic Construction and Evaluation of a Large Semantically Enriched Wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Raganato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><forename type="middle">Delli</forename><surname>Bovi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI<address><addrLine>New York City, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2894" to="2900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Neural sequence learning models for word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Raganato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><forename type="middle">Delli</forename><surname>Bovi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The english all-words task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="41" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">One million sense-tagged instances for word sense disambiguation and induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaveh</forename><surname>Taghipour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">338</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Validating and extending semantic knowledge bases using video games with a purpose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Vannella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Scarfini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Domenico</forename><surname>Toscani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014)</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014)<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1294" to="1304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Evaluating variants of the lesk approach for disambiguating words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florentina</forename><surname>Vasilescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Langlais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Lapalme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Semi-supervised word sense disambiguation with neural models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dayu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Doherty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Altendorf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1374" to="1385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">It makes sense: A wide-coverage word sense disambiguation system for free text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2010 System Demonstrations</title>
		<meeting>the ACL 2010 System Demonstrations<address><addrLine>Uppsala</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="78" to="83" />
		</imprint>
	</monogr>
	<note>Sweden</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The United Nations parallel corpus v1.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha</forename><surname>Ziemski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Pouliquen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016)</title>
		<meeting>the Tenth International Conference on Language Resources and Evaluation (LREC 2016)<address><addrLine>Portoroz, Slovenia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
