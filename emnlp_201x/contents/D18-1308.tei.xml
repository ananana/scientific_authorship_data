<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:05+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Structured Multi-Label Biomedical Text Tagging via Attentive Neural Tree Decoding</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Singh</surname></persName>
							<email>gaurav.singh.15@ucl.ac.uk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Thomas</surname></persName>
							<email>james.thomas@ucl.ac.uk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><forename type="middle">J</forename><surname>Marshall</surname></persName>
							<email>iain.marshall@kcl.ac.uk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Shawe-Taylor</surname></persName>
							<email>j.shawe-taylor@ucl.ac.uk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
							<email>b.wallace@northeastern.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University College London</orgName>
								<orgName type="institution" key="instit2">University College London</orgName>
								<address>
									<settlement>College London</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">University College London</orgName>
								<orgName type="institution" key="instit2">Northeastern University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Structured Multi-Label Biomedical Text Tagging via Attentive Neural Tree Decoding</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2837" to="2842"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2837</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a model for tagging unstructured texts with an arbitrary number of terms drawn from a tree-structured vocabulary (i.e., an on-tology). We treat this as a special case of sequence-to-sequence learning in which the decoder begins at the root node of an ontological tree and recursively elects to expand child nodes as a function of the input text, the current node, and the latent decoder state. In our experiments the proposed method outper-forms state-of-the-art approaches on the important task of automatically assigning MeSH terms to biomedical abstracts.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We consider the task of multilabel text anno- tation, where labels are drawn from an ontol- ogy. We are motivated by problems in biomed- ical NLP ( <ref type="bibr" target="#b19">Zweigenbaum et al., 2007;</ref><ref type="bibr">DemnerFushman et al., 2016)</ref>. Specifically, scientific ab- stracts in this domain are typically associated with multiple Medical Subject Heading (MeSH) terms. MeSH is a controlled, hierarchically structured vo- cabulary that facilitates semantic labeling of texts at varying levels of granularity. This in turn sup- ports semantic indexing of biomedical literature, thus facilitating improved search and retrieval. <ref type="bibr">1</ref> At present, MeSH annotation is largely per- formed manually by highly skilled annotators employed by the National Library of Medicine (NLM). Automating this annotation task is thus highly desirable, and there have been considerable efforts to do so. The BIOASQ 2 challenge, in par- ticular, concerns MeSH annotation, and competi- tive systems have emerged from this in past years ( <ref type="bibr" target="#b11">Liu et al., 2014;</ref><ref type="bibr" target="#b17">Tsoumakas et al., 2013</ref>); these constitute baseline approaches in the present work. Decoding (NTD) model. Input text is encoded, and a decoder then conditionally traverses the la- bel tree to select all relevant nodes to apply, with node-wise attention induced over the input text.</p><p>More generally, MeSH annotation is a specific instance of multi-label classification, which has received substantial attention in general ( <ref type="bibr" target="#b8">Elisseeff and Weston, 2002;</ref><ref type="bibr" target="#b9">Fürnkranz et al., 2008;</ref><ref type="bibr" target="#b15">Read et al., 2011;</ref><ref type="bibr" target="#b2">Bhatia et al., 2015;</ref><ref type="bibr" target="#b6">Daumé III et al., 2017;</ref><ref type="bibr" target="#b3">Chen et al., 2017;</ref><ref type="bibr" target="#b10">Jernite et al., 2016)</ref>. Our work differs from these prior efforts in that MeSH tagging involves structured multi-label classifica- tion: the label space is a tree 3 in which nodes represent nested semantic concepts, and the speci- ficity of these increases with depth.</p><p>Past efforts in multi-label classification have considered hierarchical and tree-based approaches for tagging <ref type="bibr" target="#b10">(Jernite et al., 2016;</ref><ref type="bibr" target="#b1">Beygelzimer et al., 2009;</ref><ref type="bibr" target="#b6">Daumé III et al., 2017)</ref>, but these have not assumed a given structured label space; instead, these efforts have attempted to induce trees to im- prove inference efficiency. By contrast, we pro- pose to explicitly capitalize on a known output structure codified here by the target ontology from which tags are drawn. We realize this by recur- sively traversing the tree to make (conditional) bi-nary tag application predictions.</p><p>The contribution of this work is a neural sequence-to-sequence (seq2seq) model ( <ref type="bibr" target="#b0">Bahdanau et al., 2014</ref>) for structured multi-label classifica- tion. Our approach entails encoding the input text to be tagged using an RNN, and then decoding into the ontological output space. This involves a tree traversal beginning at the root of the tree. At each step, the decoder decides whether to 'expand' chil- dren as a function of a hidden state vector, node embeddings, and induced attention weights over the input text. This approach is schematized in <ref type="figure" target="#fig_0">Figure 1</ref>. Expanded nodes are added to the pre- dicted tag set. This process is repeated recursively until either leaf nodes are reached or no children are selected for expansion. This neural tree de- coding (NTD) model outperforms state-of-the-art models for MeSH tagging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>Overview. Our model is an instance of an encoder-decoder architecture. For the encoder, we adopt a standard Gated Recurrent Unit (GRU) network (Cho et al., 2014a), which yields hidden states for the tokens comprising an input docu- ment. The decoder network consumes these out- puts and begins at the root of the ontological tree. It induces an attention distribution over encoder states, which is used together with the current de- coder state vector to inform which (if any) of its immediate children are applicable to the input text ( <ref type="figure" target="#fig_0">Figure 1</ref>). This decoding process proceeds recur- sively for all children deemed relevant. Below we provide more in-depth technical detail regarding the constituent modules.</p><p>The encoder (ENC) consumes as input a raw sequence of words, here composing an abstract. These are passed through an embedding layer, pro- ducing a sequence of word embeddings x (for clar- ity we omit a document index here), which are then passed through a GRU ( <ref type="bibr" target="#b5">Cho et al., 2014b</ref>) to obtain a sequence of hidden vectors h = {h 0 , · · · , h |x|−1 }, where h t = GRU(x t , h t−1 ).</p><p>These are then passed to our neural tree decoder, which is responsible for tagging the encoded text with an arbitrary number of terms from the label tree, i.e., sequences in the structured output space. This module traverses the label space top-down, beginning at the root, thus exploiting the concept hierarchy codified by the tree structure.</p><p>At each step in the decoding process, the de- coder will be positioned at a particular node in the tree n. Children -immediate descendents - of this node are then considered for expansion in turn, based on a hidden state vector s n , and a con- text vector c n . Both of these are initialized to zero vectors and recursively updated during traversal, i.e., as nodes are selected for expansion (and hence added to the predicted tag set). More specifically, the context vector that informs the decision to ex- pand node v in the label hierarchy from its parent node n is a weighted sum of the encoder hidden states h, where weights reflect induced attention over inputs, conditioned on n. That is:</p><formula xml:id="formula_0">c n = j α nj h j<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">α nj = exp{a(s n , h j )|θ n } l exp{a(s n , h l )|θ n }<label>(2)</label></formula><p>and a is a simple multi-layer perceptron (MLP), with node-specific parameters θ n . Here both sums range over the length of the input text. Given c n , we then estimate the probability that child label v is applicable to the current input text as a function of the decoder state vector (s n ), the current context vector (c n ) and the decoder pa- rameters. In particular, this is realized via a stan- dard linear layer with sigmoid activations, param- eterized by a weight matrix W comprising inde- pendent weight vectors for each output node v. Thus the score for a particular output node v is σ(W v · [s n , c n ]), where W v denotes the weight vector for output node v.</p><p>Pseudocode for the training and decoding pro- cedures are presented in Algorithm 1. In the NODELOSS function, n denotes a particular node. The set of hidden vectors induced by the encoder (corresponding to the inputs) are denoted by h, s is the hidden state of the decoder, and y is the reference label (this encodes a path in the output tree). We assume the decoder, DEC, consumes input representations, a node index and a hidden state and yields a context vector for n, c n and an updated state vector s n ; in our case the latter is implemented via a GRU. The advantage of using an RNN during decoding is that this allows the ex- ploitation of learned, distributed hidden represen- tations of partial tree paths, which inform node- wise attention and subsequent predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 RECURSIVETREEDECODING</head><p>1: function NODELOSS(n, h, s, y) 2:</p><p>ln ← 0 3: cn, sn ← DEC(h, n, s) 4:</p><p>for each child v ∈ children(n) do 5:</p><formula xml:id="formula_2">ˆ yv ← σ(Wv · [sn, cn]) 6:</formula><p>pv ← ∝ depth in tree 7:</p><p>Bv</p><note type="other">∼ Ber(pv) 8: if Bv then 9: ln ← ln + L(ˆ yv, y) 10: ifˆyvifˆ ifˆyv &gt; τ then 11: ln ← ln + NODELOSS(v, h, sn, y) return ln 12: function TRAIN(x, y, α, epochs) 13: θ ← INIT(θ) 14: e ← 0 15: while e &lt; epochs do 16: for each instance xi ∈ x do 17: hi ← ENC(xi) 18: s0 ← 0 19: li ← NODELOSS(ROOT, hi, s0, yi) 20: ∆θ ← BACKPROP(li) 21: θ ← θ + α∆θ 22:</note><p>e ← e + 1 return θ</p><p>Incurring loss for all nodes along the path spec- ified by y would place a disproportionate amount of emphasis on correctly applying terms that are 'higher' in the ontology, as loss will be propagated for the initial predictions concerning the applica- tion of these and then also, due to recursive appli- cation, for all of their children (and so on). Thus we only incur (and hence backpropagate) loss for a node v stochastically, according to a Bernoulli distribution B with parameter p v . We set p v to be proportional to the depth of node v in the tree such that we are likely to incur larger loss for deeper (rarely occurring) nodes. We operational- ize this as: p v = min(1, 0.5 + m fv ), where m is the count corresponding to the least frequently ob- served node in the training corpus and f v is the count for node v. In Section 4 we demonstrate the benefit of this approach.</p><p>At train time we use teacher forcing <ref type="bibr" target="#b18">(Williams and Zipser, 1989</ref>) during decoding. That is, we revert the model back to the correct (training) tree subsequence when it goes off-course, and continue decoding from there. We have elided this detail from the pseudocode for clarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental setup</head><p>Below we describe experimental details concern- ing our implementation, datasets and baselines. Code and data to reproduce our results is available at https://github.com/gauravsc/NTD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Implementation Details</head><p>We limited the vocabulary to the 50, 000 most fre- quent words. Word embeddings were initialized to pre-trained vectors induced via word2vec, trained over a large set of abstracts indexed on PubMed. <ref type="bibr">4</ref> Ontology node embeddings were pre-trained using DeepWalk ( <ref type="bibr" target="#b14">Perozzi et al., 2014</ref>), fit over PubMed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dataset</head><p>Our dataset comprises abstracts of articles de- scribing randomized controlled trials (RCTs) from PubMed along with their MeSH terms. The MeSH annotations were manually applied by profession- als at the National Library of Medicine (NLM). The label space underlying MeSH terms is codi- fied by a publicly available ontology. <ref type="bibr">5</ref> We split this dataset into disjoint sets for train- ing/development and final evaluation <ref type="table">(Table 1)</ref>. We further separated the former into train, vali- dation and development test subsets, to refine our approach. For our final evaluation we used a held- out set of 10,000 abstracts that were not seen in any way during model development and/or hyper- parameter tuning. We performed extensive hyper- parameter tuning for the baseline models to ensure fair comparison; details regarding this tuning are provided in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Baselines</head><p>We compare our proposed approach to three base- lines, including two prior winners of the annual BioASQ challenge, which includes an automated MeSH annotation task. However, it is important to note that we used a different (and considerably smaller) dataset in the current work, as compared to the corpus used in the BioASQ challenge.</p><p>LSSI ( <ref type="bibr" target="#b17">Tsoumakas et al., 2013</ref>) use an approach that involves predicting both the number of terms and which to apply to a given abstract. They use linear models for both tasks, which operate over TF-IDF representations of abstracts. Specifically, they train a regressor to predict k, the number of MeSH terms to be applied to an abstract. Simul- taneously, a binary linear SVM is trained indepen- dently for each MeSH term appearing in the train set. At test time, these SVMs provide scores for each term and the topˆktopˆ topˆk terms are applied, wherê k is the estimate from the aforementioned regressor <ref type="table" target="#tab_1">. Train  20000  Validation  4000  Dev test  18884  Test (held-out)  10000  Mean MeSH terms per article  15.33  Total unique MeSH terms  27892  Unique MeSH terms in dataset 3781   Table 1</ref>: Dataset statistics.</p><p>UIUC ( <ref type="bibr" target="#b11">Liu et al., 2014</ref>) uses a learning-to-rank model to identify the top MeSH terms for an ab- stract from a candidate set of terms, which is ob- tained from the nearest neighbours of the abstract. Additionally, one SVM classifier is trained for each of the MeSH terms (similar to the above ap- proach), and scores for each are used to obtain ad- ditional terms to be added to the candidate set. In the end, a threshold (tuned on the validation set) is used to select the final set of terms to be assigned. Finally, we consider a deep multilabel classifica- tion model DML ( <ref type="bibr" target="#b16">Rios and Kavuluru, 2015</ref>) that takes as input unstructured abstracts and activates the output nodes corresponding to the relevant MeSH terms. In brief, embedded tokens are fed through a CNN to induce a vector representation, which is then passed on to the dense output layer. Finally, this is passed through a sigmoid activation function. Note that this model exploits the same pre-trained word embeddings as our model does.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Evaluation metrics</head><p>We first evaluate model performance via output node-wise precision, recall and F1 measure. How- ever, these metrics are overly strict in the sense that a model will be penalized equally for all mis- takes, regardless of whether they are nearby or far from the target in the label tree. This is problem- atic because whether to apply a specific MeSH term or its immediate parent may be somewhat subjective in practice. To quantify this, and to ex- plore the extent to which explicitly decoding into the target label space yields improved predictions, we also consider a measure that we refer to as se- mantic distance (SD):</p><formula xml:id="formula_3">SD = 1 |Y| u∈Y min v∈ˆYv∈ˆ v∈ˆY dist(u, v) (3)</formula><p>where Y andˆYandˆ andˆY are the sets of target and predicted terms respectively, and dist is a function that re- turns the shortest distance between two nodes in  the label ontology tree. The idea is that this penal- izes less for 'near misses'. Thus if a model fails to apply a particular tag t, but does apply one near to t in the label tree, then it is penalized less. <ref type="bibr">6</ref> We hypothesize that our model will improve results markedly with respect to this metric, given our ex- ploitation of the tree structure.</p><p>As in the case of recall, SD can be 'gamed': one can achieve a perfect score by predicting that all nodes apply to a given abstract. Thus this is only meaningful alongside complementary metrics like F1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>Results on the test set (which was completely held out during development) are reported in Ta- ble 2. The proposed Neural Tree Decoding model with stochastic backpropagation (NTD-s) bests the most competitive baseline (LSSI) in F1 score by over 2 points.</p><p>To explore the effect of backpropagating loss from nodes in proportion to their depth in the on- tology, we also include results for a deterministic variant that does not do this, NTD-d. This version does not perform as well, demonstrating the utility of the proposed training approach.</p><p>The metrics reported thus far do not account of the structure in the output space. We thus addi- tionally report results with respect to the the se- mantic distance (SD) metric (Eq. 3). We ob- serve a marked performance increase of ∼21% over the best performing baseline. This is intuitive given that we are explicitly decoding into the label tree structure, and demonstrates the ability of our model to learn the ontological structure, thereby predicting semantically appropriate terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions, Discussion &amp; Limitations</head><p>We developed a neural attentive sequence tree de- coding model for structured multilabel classifica- tion where labels are drawn from a known ontol- ogy. The proposed method can decode an input text into a tree of labels, effectively using the struc- ture in the output space. We demonstrated that this model outperformed SOTA approaches for the important task of tagging biomedical abstracts with Medical Subject Heading (MeSH) terms on a modestly sized training corpus. Code and data to reproduce these results are available at https: //github.com/gauravsc/NTD.</p><p>One limitation of our model is that it is com- paratively slow, due to having to traverse the tree structure during decoding. Prediction speed may not be a major issue in practice, as articles on PubMed could be batch tagged nightly as they ar- rive. However, slow decoding also means lengthy training (see Appendix, section A.2 for details). For this reason we have here used a modest train- ing set of ∼20k abstracts, which is smaller than corpora used in prior work on this task. Given the relative expressiveness of our model, we expect it to benefit substantially from additional training data, moreso than the simpler baseline architec- tures. But at present this is only a conjecture.</p><p>In future work we thus hope to apply this model to larger datasets, and to address the efficiency is- sue. Concerning the latter, sibling subtrees may be traversed in parallel, conditioned on the hidden state of their parent. Another promising direction would be to move to convolutional encoder and decoder architectures, designing the latter in a way similarly capitalizes on the label space tree struc- ture.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of the proposed Neural Tree Decoding (NTD) model. Input text is encoded, and a decoder then conditionally traverses the label tree to select all relevant nodes to apply, with node-wise attention induced over the input text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Results on the held-out test dataset. SD 
refers to semantic distance, defined in Eq. 3. 

</table></figure>

			<note place="foot" n="1"> This problem also resembles tagging clinical notes with ICD codes (Mullenbach et al., 2018). 2 http://bioasq.org/</note>

			<note place="foot" n="3"> Technically, MeSH comprises multiple trees, but we join these by insertion of an overarching root node.</note>

			<note place="foot" n="4"> A repository of biomedical literature. 5 https://meshb-prev.nlm.nih.gov/ treeView</note>

			<note place="foot" n="6"> This metric is equivalent to the sum of two metrics (&quot;divergent path to gold standard&quot; and &quot;divergent path to prediction&quot;) defined in (Perotte et al., 2013).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgements</head><p>JT and GS acknowledge support from Cochrane via the Transform project. BCW was supported by the National Library of Medicine (NLM) of the National Institutes of Health (NIH), grant R01LM012086. IJM acknowledges support from the MRC (UK), through its grant MR/N015185/1.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Conditional probability tree estimation analysis and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Lifshits</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Gregory Sorkin, and Alex Strehl</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sparse local embeddings for extreme multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kush</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Purushottam</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manik</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prateek</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Doctag2vec: An embedding based multi-label learning approach for document tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Soni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aasish</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Representation Learning for NLP</title>
		<meeting>the Workshop on Representation Learning for NLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">Encoder-decoder approaches. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Logarithmic time oneagainst-some</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Karampatziakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Mineiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Aspiring to unintended consequences of natural language processing: A review of recent developments in clinical and consumer-generated text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Elhadad</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>IMIA Yearbook</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A kernel method for multi-labelled classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">André</forename><surname>Elisseeff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Multilabel classification via calibrated label ranking. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Fürnkranz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eyke</forename><surname>Hüllermeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneldo</forename><surname>Loza Mencía</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Brinker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">73</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Simultaneous learning of trees and representations for extreme classification, with application to language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Choromanska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.04658</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The fudan-uiuc participation in the bioasq challenge task 2a: The antinomyra system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqiu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanfeng</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">129816</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Explainable prediction of medical codes from clinical text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Mullenbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Wiegreffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Diagnosis code assignment: models and evaluation metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rimma</forename><surname>Adler Perotte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Pivovarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicole</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Weiskopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noémie</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Classifier chains for multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Read</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eibe</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">85</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for biomedical text classification: application in indexing biomedical articles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Rios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakanth</forename><surname>Kavuluru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Bioinformatics, Computational Biology and Health Informatics</title>
		<meeting>the ACM Conference on Bioinformatics, Computational Biology and Health Informatics</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Large-scale semantic indexing of biomedical publications at BioASQ</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grigorios</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manos</forename><surname>Laliotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BioASQ Workshop</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Nikos Markantonatos, and Ioannis Vlahavas</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A learning algorithm for continually running fully recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zipser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Frontiers of biomedical text mining: current progress</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Zweigenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Briefings in bioinformatics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
