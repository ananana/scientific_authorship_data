<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:03+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Dynamic Computation Graphs via Sparse Latent Structure</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Niculae</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Instituto de Telecomunicações / ♭ Unbabel</orgName>
								<address>
									<settlement>Lisbon</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">André</forename><forename type="middle">F T</forename><surname>Martins</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Instituto de Telecomunicações / ♭ Unbabel</orgName>
								<address>
									<settlement>Lisbon</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Cornell University</orgName>
								<address>
									<settlement>Ithaca</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Dynamic Computation Graphs via Sparse Latent Structure</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="905" to="911"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>905</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Deep NLP models benefit from underlying structures in the data-e.g., parse trees-typically extracted using off-the-shelf parsers. Recent attempts to jointly learn the latent structure encounter a tradeoff: either make factor-ization assumptions that limit expressiveness, or sacrifice end-to-end differentiability. Using the recently proposed SparseMAP inference, which retrieves a sparse distribution over latent structures, we propose a novel approach for end-to-end learning of latent structure predic-tors jointly with a downstream predictor. To the best of our knowledge, our method is the first to enable unrestricted dynamic computation graph construction from the global latent structure, while maintaining differentiability.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Latent structure models are a powerful tool for modeling compositional data and building NLP pipelines <ref type="bibr">(Smith, 2011</ref>). An interesting emerging direction is to dynamically adapt a network's com- putation graph, based on structure inferred from the input; notable applications include learning to write programs ( <ref type="bibr" target="#b0">Bosnjak et al., 2017</ref>), answering visual questions by composing specialized modules ( <ref type="bibr" target="#b7">Hu et al., 2017;</ref><ref type="bibr" target="#b8">Johnson et al., 2017)</ref>, and compos- ing sentence representations using latent syntactic parse trees ( <ref type="bibr" target="#b12">Yogatama et al., 2017)</ref>.</p><p>But how to learn a model that is able to condi- tion on such combinatorial variables? The ques- tion then becomes: how to marginalize over all possible latent structures? For tractability, exist- ing approaches have to make a choice. Some of them eschew global latent structure, resorting to computation graphs built from smaller local deci- sions: e.g., structured attention networks use lo- cal posterior marginals as attention weights <ref type="bibr" target="#b9">(Kim et al., 2017;</ref><ref type="bibr" target="#b11">Liu and Lapata, 2018)</ref>, and <ref type="bibr" target="#b12">Maillard et al. (2017)</ref> construct sentence representa- tions from parser chart entries. Others allow more flexibility at the cost of losing end-to-end differ- entiability, ending up with reinforcement learning problems ( <ref type="bibr" target="#b12">Yogatama et al., 2017;</ref><ref type="bibr" target="#b7">Hu et al., 2017;</ref><ref type="bibr" target="#b8">Johnson et al., 2017;</ref><ref type="bibr">Williams et al., 2018</ref>). More traditional approaches employ an off-line structure predictor (e.g., a parser) to define the computation graph ( <ref type="bibr">Tai et al., 2015;</ref><ref type="bibr" target="#b4">Chen et al., 2017)</ref>, some- times with some parameter sharing <ref type="bibr" target="#b2">(Bowman et al., 2016)</ref>. However, these off-line methods are unable to jointly train the latent model and the downstream classifier via error gradient information.</p><p>We propose here a new strategy for building dy- namic computation graphs with latent structure, through sparse structure prediction. Sparsity al- lows selecting and conditioning on a tractable num- ber of global structures, eliminating the limitations stated above. Namely, our approach is the first that:</p><p>A) is fully differentiable; B) supports latent structured variables; C) can marginalize over full global structures.</p><p>This contrasts with off-line and with reinforcement learning-based approaches, which satisfy B and C but not A; and with local marginal-based meth- ods such as structured attention networks, which satisfy A and B, but not C. Key to our approach is the recently proposed SparseMAP inference ( <ref type="bibr">Niculae et al., 2018)</ref>, which induces, for each data example, a very sparse posterior distribution over the possible structures, allowing us to compute the expected network output efficiently and explicitly in terms of a small, interpretable set of latent struc- tures. Our model can be trained end-to-end with gradient-based methods, without the need for pol- icy exploration or sampling.</p><p>We demonstrate our strategy on inducing latent dependency TreeLSTMs, achieving competitive results on sentence classification, natural language inference, and reverse dictionary lookup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Sparse Latent Structure Prediction</head><p>We describe our proposed approach for learning with combinatorial structures (in particular, non- projective dependency trees) as latent variables.</p><formula xml:id="formula_0">the movie rocks Embed Bi-LSTM h1 ⋆ • • • 1.0 1.0 1.0 h2 ⋆ • • • 1.0 1.0 1.0 h3 ⋆ • • • 1.0 1.0 1.0 · · · SparseMAP parser p θ (h 1 | x) = .6 p θ (h 2 | x) = .4 p θ (h 3 | x) = 0 · · · TreeLSTM(h 1 ) TreeLSTM(h 2 ) p ξ (y | h 1 , x) p ξ (y | h 2 , x) Prediction p(y | x) = h∈H p ξ (y | h, x) p θ (h | x)</formula><p>Figure 1: Our method computes a sparse probability distribution over all possible latent structures: here, only two have nonzero probability. For each selected tree h, we evaluate p ξ (y | h, x) by dynamically building the corresponding computation graph (e.g., a TreeLSTM). The final, posterior prediction is a sparse weighted average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Latent Structure Models</head><p>Let x and y denote classifier inputs and outputs, and h ∈ H(x) a latent variable; for example, H(x) can be the set of possible dependency trees for x. We would like to train a neural network to model</p><formula xml:id="formula_1">p(y | x) := h∈H(x) p θ (h | x) p ξ (y | h, x),<label>(1)</label></formula><p>where p θ (h | x) is a structured-output parsing model that defines a distribution over trees, and p ξ (y | h, x) is a classifier whose computation graph may depend freely and globally on the struc- ture h (e.g., a TreeLSTM). The rest of this section focuses on the challenge of defining p θ (h | x) such that Eqn. 1 remains tractable and differentiable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Global Inference</head><p>Denote by f θ (h; x) a scoring function, assigning each tree a non-normalized score. For instance, we may have an arc-factored score f θ (h; x) := a∈h s θ (a; x), where we interpret a tree h as a set of directed arcs a, each receiving an atomic score s θ (a; x). Deriving p θ given f θ is known as structured inference. This can be written as a Ω-regularized optimization problem of the form</p><formula xml:id="formula_2">p θ (· | x) := argmax q∈△ |H(x)| h∈H(x) q(h)f θ (h; x) − Ω(q),</formula><p>where △ |H(x)| is the set of all possible probability distributions over H(x). Examples follow.</p><p>Marginal inference. With negative entropy reg- ularization, i.e., Ω(q) :</p><formula xml:id="formula_3">= h∈H(x) q(h) log q(h),</formula><p>we recover marginal inference, and the probability of a tree becomes (Wainwright and Jordan, 2008)</p><formula xml:id="formula_4">p θ (h | x) ∝ exp(f θ (h; x)).</formula><p>This closed-form derivation, detailed in Ap- pendix A, provides a differentiable expression for p θ . However, crucially, since exp(·) &gt; 0, every tree is assigned strictly nonzero probabil- ity. Therefore-unless the downstream p ξ is con- strained to also factor over arcs, as in <ref type="bibr" target="#b9">Kim et al. (2017)</ref>; <ref type="bibr" target="#b11">Liu and Lapata (2018)</ref>-the sum in Eqn. 1 requires enumerating the exponentially large H(x). This is generally intractable, and even hard to ap- proximate via sampling, even when p θ is tractable.</p><p>MAP inference. At the polar opposite, setting Ω(q) := 0 yields maximum a posteriori (MAP) inference (see Appendix A). MAP assigns a prob- ability of 1 to the highest-scoring tree, and 0 to all others, yielding a very sparse p θ . However, since the top-scoring tree (or top-k, for fixed k) does not vary with small changes in θ, error gra- dients cannot propagate through MAP. This pre- vents end-to-end gradient-based training for MAP- based latent variables, which makes them more difficult to use. Related reinforcement learning ap- proaches also yield only one structure, but sidestep non-differentiability by instead introducing more challenging search problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Sparse Inference</head><p>In this work, we propose using SparseMAP in- ference ( <ref type="bibr">Niculae et al., 2018</ref>) to sparsify the set H while preserving differentiability. SparseMAP uses a quadratic penalty on the posterior marginals</p><formula xml:id="formula_5">Ω(q) := u(q) 2 2 , where [u(q)] a := h:a∈h q(h).</formula><p>Situated between marginal inference and MAP in- ference, SparseMAP assigns nonzero probability to only a small set of plausible trees ¯ H ⊂ H, of size at most equal to the number of arcs ( <ref type="bibr">Martins et al., 2015, Proposition 11)</ref>. This guarantees that the summation in Eqn. 1 can be computed efficiently by iterating over ¯ H: this is depicted in <ref type="figure">Figure 1</ref> and described in the next paragraphs.</p><p>Forward pass. To compute p(y | x) (Eqn. 1), we observe that the SparseMAP posterior p θ is nonzero only on a small set of trees ¯ H, and thus we only need to compute p ξ (y | h, x) for h ∈ ¯ H. The support and values of p θ are obtained by solv- ing the SparseMAP inference problem, as we de- scribe in <ref type="bibr">Niculae et al. (2018)</ref>. The strategy, based on the active set algorithm <ref type="bibr">(Nocedal and Wright, 1999, chapter 16)</ref>, involves a sequence of MAP calls (here: maximum spanning tree problems.)</p><p>Backward pass. We next show how to compute end-to-end gradients efficiently. Recall from Eqn. 1</p><formula xml:id="formula_6">p(y | x) = h∈H p θ (h | x) p ξ (y | h, x)</formula><p>, where h is a discrete index of a tree. To train the classifier, we have ∂p(y|x) /∂ξ = h∈H p θ (h | x) ∂p ξ (y|h,x) /∂ξ, therefore only the terms with nonzero probabil- ity (i.e., h ∈ ¯ H) contribute to the gradient. ∂p ξ (y|h,x) /∂ξ is readily available by implementing p ξ in an automatic differentiation library. 1 To train the latent parser, the total gradient ∂p(y|x) /θ is the sum h∈ ¯ H p ξ (y | h, x) ∂p θ (h|x) /∂θ. We derive the expression of ∂p θ (h|x) /∂θ in Appendix B. Crucially, the gradient sum is also sparse, like p θ , and ef- ficient to compute, amounting to multiplying by a | ¯ H(x)|-by-| ¯ H(x)| matrix. The proof, given in Ap- pendix B, is a novel extension of the SparseMAP backward pass ( <ref type="bibr">Niculae et al., 2018</ref>).</p><p>Generality. Our description focuses on proba- bilistic classifiers, but our method can be readily applied to networks that output any representa- tion, not necessarily a probability. For this, we define a function r ξ (h, x), consisting of any auto- differentiable computation w.r.t. x, conditioned on <ref type="bibr">1</ref> Here we assume θ and ξ to be disjoint, but weight sharing is easily handled by automatic differentiation via the product rule. Differentiation w.r.t. the summation index h is not neces- sary: p ξ may use the discrete structure h freely and globally. subj.</p><p>SST  the discrete latent structure h in arbitrary, non- differentiable ways. We then compute</p><formula xml:id="formula_7">¯ r(x) := h∈H(x) p θ (h | x)r ξ (h, x) = E h∼p θ r ξ (h, x).</formula><p>This strategy is demonstrated in our reverse- dictionary experiments in §3.4. In addition, our approach is not limited to trees: any structured model with tractable MAP inference may be used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We evaluate our approach on three natural language processing tasks: sentence classification, natural language inference, and reverse dictionary lookup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Common aspects</head><p>Word vectors. Unless otherwise mentioned, we initialize with 300-dimensional GloVe word embed- dings ( <ref type="bibr">Pennington et al., 2014</ref>) We transform every sentence via a bidirectional LSTM encoder, to pro- duce a context-aware vector v i encoding word i.</p><p>Dependency TreeLSTM. We combine the word vectors v i in a sentence into a single vector using a tree-structured Child-Sum LSTM, which allows an arbitrary number of children at any node ( <ref type="bibr">Tai et al., 2015)</ref>. Our baselines consist in extreme cases of de- pendency trees: where the parent of word i is word i+1 (resulting in a left-to-right sequential LSTM), and where all words are direct children of the root node (resulting in a flat additive model). We also consider off-line dependency trees precomputed by Stanford CoreNLP ( <ref type="bibr" target="#b13">Manning et al., 2014</ref>).</p><p>Neural arc-factored dependency parsing. We compute arc scores s θ (a; x) with one-hidden-layer perceptrons <ref type="bibr" target="#b10">(Kiperwasser and Goldberg, 2016)</ref>.</p><p>Experimental setup. All networks are trained via stochastic gradient with 16 samples per batch.</p><p>We tune the learning rate on a log-grid, using a decay factor of 0.9 after every epoch at which the validation performance is not the best seen, and stop after five epochs without improvement.  <ref type="table">Table 2</ref>: Results on the reverse dictionary lookup task ( <ref type="bibr" target="#b6">Hill et al., 2016)</ref>. Following the authors, for an input definition, we rank a shortlist of approximately 50k candidate words according to the cosine similarity to the output vector, and report median rank of the expected word, accuracy at 10, and at 100.</p><p>chosen on the validation set, controlling the sparsity of the SparseMAP distribution. All hidden layers are 300-dimensional. <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sentence classification</head><p>We evaluate our models for sentence-level subjec- tivity classification ( <ref type="bibr">Pang and Lee, 2004</ref>) and for binary sentiment classification on the Stanford Sen- timent <ref type="bibr">Treebank (Socher et al., 2013</ref>). In both cases, we use a softmax output layer on top of the Dependency TreeLSTM output representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Natural language inference (NLI)</head><p>We apply our strategy to the SNLI corpus <ref type="bibr" target="#b1">(Bowman et al., 2015)</ref>, which consists of classify- ing premise-hypothesis sentence pairs into entail- ment, contradiction or neutral relations. In this case, for each pair (x P , x H ), the running sum is over two latent distributions over parse trees, i.e.,</p><formula xml:id="formula_8">h P ∈H(x P ) h H ∈H(x H ) p ξ (y | x {P,H} , h {P,H} ) p θ (h P | x P )p θ (h H | x H ).</formula><p>For each pair of trees, we independently encode the premise and hypothe- sis using a TreeLSTM. We then concatenate the two vectors, their difference, and their element-wise product ( <ref type="bibr" target="#b15">Mou et al., 2016</ref>). The result is passed through one tanh hidden layer, followed by the softmax output layer. <ref type="bibr">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Reverse dictionary lookup</head><p>The reverse dictionary task aims to compose a dic- tionary definition into an embedding that is close to the defined word. We therefore used fixed in- put and output embeddings, set to unit-norm 500- dimensional vectors provided, together with train- ing and evaluation data, by <ref type="bibr" target="#b6">Hill et al. (2016)</ref>. The 28% ⋆ a vivid cinematic portrait . ⋆ a vivid cinematic portrait . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>· · · 13%</head><p>⋆ a vivid cinematic portrait . network output is a projection of the TreeLSTM encoding back to the dimension of the word embed- dings, normalized to unit ℓ 2 norm. We maximize the cosine similarity of the predicted vector with the embedding of the defined word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>Experimental performance. Classification and NLI results are reported in <ref type="table" target="#tab_1">Table 1</ref>. Compared to the latent structure model of <ref type="bibr" target="#b12">Yogatama et al. (2017)</ref>, our model performs better on SNLI (80.5%) but worse on SST (86.5%). On SNLI, our model also outperforms <ref type="bibr" target="#b12">Maillard et al. (2017)</ref> (81.6%). To our knowledge, latent structure models have not been tested on subjectivity classification. Surprisingly, the simple flat and left-to-right baselines are very strong, outperforming the off-line dependency tree models on all three datasets. The latent TreeLSTM model reaches the best accuracy on two out of the three datasets. On reverse dictionary lookup (Ta-† 22.6% ⋆ lovely and poignant .</p><p>1.0 1.0</p><p>1.0 1.0</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>21.4%</head><p>⋆ lovely and poignant .</p><p>1.0 1.0 1.0 1.0</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>19.84%</head><p>⋆ lovely and poignant .</p><p>1.0 1.0</p><formula xml:id="formula_9">1.0 1.0 · · · 15.33%</formula><p>⋆ a deep and meaningful film .</p><formula xml:id="formula_10">1.0 1.0 1.0 1.0 1.0 1.0 † 15.27%</formula><p>⋆ a deep and meaningful film .</p><p>1.0 1.0</p><formula xml:id="formula_11">1.0 1.0 1.0 1.0 · · · 0%</formula><p>⋆ a deep and meaningful film . Selected latent structures. We analyze the la- tent structures selected by our model on SST, where the flat composition baseline is remarkably strong. We find that our model, to maximize accuracy, prefers flat or nearly-flat trees, but not exclusively: the average posterior probability of the flat tree is 28.9%. In <ref type="figure" target="#fig_2">Figure 2</ref>, the highest-ranked tree is flat, but deeper trees are also selected, including the projective CoreNLP parser output. Syntax is not necessarily an optimal composition order for a latent TreeLSTM, as illustrated by the poor per- formance of the off-line parser <ref type="table" target="#tab_1">(Table 1)</ref>. Conse- quently, our (fully unsupervised) latent structures tend to disagree with CoreNLP: the average prob- ability of CoreNLP arcs is 5.8%; Williams et al. (2018) make related observations. Indeed, some syntactic conventions may be questionable for re- cursive composition. <ref type="figure" target="#fig_3">Figure 3</ref> shows two examples where our model identifies a plausible symmetric composition order for coordinate structures: this analysis disagrees with CoreNLP, which uses the asymmetrical Stanford / UD convention of assign- ing the left-most conjunct as head <ref type="bibr">(Nivre et al., 2016</ref>). Assigning the conjunction as head instead seems preferable in a Child-Sum TreeLSTM.</p><p>Training efficiency. Our model must evaluate at least one TreeLSTM for each sentence, mak- ing it necessarily slower than the baselines, which evaluate exactly one. Thanks to sparsity and auto- batching, the actual slow-down is not problematic; moreover, as the model trains, the latent parser gets more confident, and for many unambiguous sentences there may be only one latent tree with nonzero probability. On SST, our average training epoch is only 4.7× slower than the off-line parser and 6× slower than the flat baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and future work</head><p>We presented a novel approach for training latent structure neural models, based on the key idea of sparsifying the set of possible structures, and demonstrated our method with competitive latent dependency TreeLSTM models. Our method's gen- erality opens up several avenues for future work: since it supports any structure for which MAP in- ference is available (e.g., matchings, alignments), and we have no restrictions on the downstream p ξ (y | h, x), we may design latent versions of more complicated state-of-the-art models, such as ESIM for NLI ( <ref type="bibr" target="#b4">Chen et al., 2017)</ref>. In concurrent work, <ref type="bibr">Peng et al. (2018)</ref> proposed an approximate MAP backward pass, relying on a relaxation and a gradient projection. Unlike our method, theirs does not support multiple latent structures; we intend to further study the relationship between the methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Three of the sixteen trees with nonzero probability for an SST test example. Flat representations, such as the first tree, perform well on this task, as reflected by the baselines. The second tree, marked with , agrees with the off-line parser.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Examples of coordinate structures where our model assigns high probability to a symmetric parse (marked †). While not consistent with the standard asymmetrical parse produced by CoreNLP (marked with ), the symmetric analysis may be more appropriate for TreeLSTM composition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 : Accuracy scores for classification and NLI.</head><label>1</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="2"> Our dynet (Neubig et al., 2017) implementation is available at https://github.com/vene/sparsemap. 3 For NLI, our architecture is motivated by our goal of evaluating the impact of latent structure for learning compositional sentence representations. State-of-the-art models conditionally transform the sentences to achieve better performance, e.g., 88.6% accuracy in Chen et al. (2017).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the European Research Council (ERC StG DeepSPIN 758969) and by the Fundação para a Ciência e Tecnologia through con-tract UID/EEA/50008/2013. We thank Annabelle Carrell, Chris Dyer, Jack Hessel, Tim Vieira, Jus-tine Zhang, Sydney Zink, and the anonymous re-viewers, for helpful and well-structured feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Programming with a differentiable Forth interpreter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matko</forename><surname>Bosnjak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Naradowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A fast unified model for parsing and sentence understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gauthier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lieven</forename><surname>Vandenberghe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Enhanced LSTM for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The generalized simplex method for minimizing a linear form under linear inequality restraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>George B Dantzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Orden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wolfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pacific Journal of Mathematics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="183" to="195" />
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning to understand phrases by embedding the dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">TACL</biblScope>
			<biblScope unit="page" from="17" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to reason: End-to-end module networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Inferring and executing programs for visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Structured attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loung</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliyahu</forename><surname>Kiperwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<title level="m">Simple and accurate dependency parsing using bidirectional LSTM feature representations. TACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="313" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning structured text representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="63" to="75" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Jointly learning sentence embeddings and syntax with unsupervised tree-LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Maillard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.09189</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL (demonstrations)</title>
		<meeting>ACL (demonstrations)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">AD3: Alternating directions dual decomposition for MAP inference in graphical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Mário</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Q</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Aguiar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="495" to="545" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">How transferable are neural networks in NLP applications?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
