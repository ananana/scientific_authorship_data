<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:52+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Analyzing the Behavior of Visual Question Answering Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech † Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech † Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech † Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Analyzing the Behavior of Visual Question Answering Models</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1955" to="1960"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recently, a number of deep-learning based models have been proposed for the task of Visual Question Answering (VQA). The performance of most models is clustered around 60-70%. In this paper we propose systematic methods to analyze the behavior of these models as a first step towards recognizing their strengths and weaknesses, and identifying the most fruitful directions for progress. We analyze two models, one each from two major classes of VQA models-with-attention and without-attention and show the similarities and differences in the behavior of these models. We also analyze the winning entry of the VQA Challenge 2016. Our behavior analysis reveals that despite recent progress, today&apos;s VQA models are &quot;my-opic&quot; (tend to fail on sufficiently novel instances), often &quot;jump to conclusions&quot; (con-verge on a predicted answer after &apos;listening&apos; to just half the question), and are &quot;stubborn&quot; (do not change their answers across images).</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Visual Question Answering (VQA) is a recently- introduced ( <ref type="bibr" target="#b2">Antol et al., 2015;</ref><ref type="bibr" target="#b4">Geman et al., 2014;</ref><ref type="bibr">Malinowski and Fritz, 2014</ref>) problem where given an image and a natural language question (e.g., "What kind of store is this?", "How many people are waiting in the queue?"), the task is to automati- cally produce an accurate natural language answer ("bakery", "5"). A flurry of recent deep-learning based models have been proposed for VQA <ref type="bibr" target="#b2">(Antol et al., 2015;</ref><ref type="bibr">Chen et al., 2015;</ref><ref type="bibr">Xu and Saenko, 2016;</ref><ref type="bibr" target="#b6">Jiang et al., 2015;</ref><ref type="bibr" target="#b0">Andreas et al., 2016a;</ref><ref type="bibr">Kafle and Kanan, 2016;</ref><ref type="bibr" target="#b10">Lu et al., 2016;</ref><ref type="bibr" target="#b1">Andreas et al., 2016b;</ref><ref type="bibr">Shih et al., 2016;</ref><ref type="bibr" target="#b8">Kim et al., 2016;</ref><ref type="bibr" target="#b3">Fukui et al., 2016;</ref><ref type="bibr">Noh and Han, 2016;</ref><ref type="bibr" target="#b5">Ilievski et al., 2016;</ref><ref type="bibr" target="#b13">Wu et al., 2016;</ref><ref type="bibr" target="#b14">Xiong et al., 2016;</ref><ref type="bibr" target="#b17">Zhou et al., 2015;</ref><ref type="bibr">Saito et al., 2016)</ref>. Curiously, the performance of most methods is clustered around 60-70% (com- pared to human performance of 83% on open-ended task and 91% on multiple-choice task) with a mere 5% gap between the top-9 entries on the VQA Chal- lenge 2016. <ref type="bibr">1</ref> It seems clear that as a first step to understand these models, to meaningfully compare strengths and weaknesses of different models, to de- velop insights into their failure modes, and to iden- tify the most fruitful directions for progress, it is cru- cial to develop techniques to understand the behav- ior of VQA models.</p><p>In this paper, we develop novel techniques to characterize the behavior of VQA models. As con- crete instantiations, we analyze two VQA models ( <ref type="bibr" target="#b10">Lu et al., 2016)</ref>, one each from two major classes of VQA models -with-attention and without-attention. We also analyze the winning en- try ( <ref type="bibr" target="#b3">Fukui et al., 2016)</ref> of the VQA Challenge 2016.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our work is inspired by previous works that diag- nose the failure modes of models for different tasks. ( <ref type="bibr" target="#b7">Karpathy et al., 2016</ref>) constructed a series of ora- cles to measure the performance of a character level language model. ( <ref type="bibr">Hoiem et al., 2012</ref>) provided anal- ysis tools to facilitate detailed and meaningful inves- tigation of object detector performance. This paper aims to perform behavior analyses as a first step to- wards diagnosing errors for VQA.</p><p>( ) categorize the errors made by their VQA model into four categories -model fo- cuses attention on incorrect regions, model focuses attention on appropriate regions but predicts incor- rect answers, predicted answers are different from labels but might be acceptable, labels are wrong. While these are coarse but useful failure modes, we are interested in understanding the behavior of VQA models along specific dimensions -whether they generalize to novel instances, whether they listen to the entire question, whether they look at the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Behavior Analyses</head><p>We analyze the behavior of VQA models along the following three dimensions - Generalization to novel instances: We investi- gate whether the test instances that are incorrectly answered are the ones that are "novel" i.e., not sim- ilar to training instances. The novelty of the test in- stances may be in two ways -1) the test question- image (QI) pair is "novel", i.e., too different from training QI pairs; and 2) the test QI pair is "famil- iar", but the answer required at test time is "novel", i.e., answers seen during training are different from what needs to be produced for the test QI pairs.</p><p>Complete question understanding: To investi- gate whether a VQA model is understanding the in- put question or not, we analyze if the model 'listens' to only first few words of the question or the entire question, if it 'listens' to only question (wh) words and nouns or all the words in the question.</p><p>Complete image understanding: The absence of a large gap between performance of language- alone and language + vision VQA models ( <ref type="bibr" target="#b2">Antol et al., 2015</ref>) provides evidence that current VQA mod- els seem to be heavily reliant on the language model, perhaps not really understanding the image. In order to analyze this behavior, we investigate whether the predictions of the model change across images for a given question.</p><p>We present our behavioral analyses on the VQA dataset ( <ref type="bibr" target="#b2">Antol et al., 2015)</ref>. VQA is a large- scale free-form natural-language dataset containing ∼0.25M images, ∼0.76M questions, and ∼10M an- swers, with open-ended and multiple-choice modal- ities for answering the visual questions. All the ex- perimental results are reported on the VQA valida- tion set using the following models trained on the VQA training set for the open-ended task - CNN + LSTM based model without-attention (CNN+LSTM): We use the best performing model of (Antol et al., 2015) (code provided by ( ), which achieves an accuracy of 54.13% on the VQA validation set. It is a two channel model -one channel processes the image (using Convolu- tional Neural Network (CNN) to extract image fea- tures) and the other channel processes the question (using Long Short-Term Memory (LSTM) recurrent neural network to obtain question embedding). The image and question features obtained from the two channels are combined and passed through a fully connected (FC) layer to obtain a softmax distribu- tion over the space of answers.</p><p>CNN + LSTM based model with-attention (ATT): We use the top-entry on the VQA challenge leaderboard (as of June 03, 2016) ( <ref type="bibr" target="#b10">Lu et al., 2016)</ref>, which achieves an accuracy of 57.02% on the VQA validation set. <ref type="bibr">2</ref> This model jointly reasons about im- age and question attention, in a hierarchical fashion. The attended image and question features obtained from different levels of the hierarchy are combined and passed through a FC layer to obtain a softmax distribution over the space of answers.</p><p>VQA Challenge 2016 winning entry (MCB): This is the multimodal compact bilinear (mcb) pool- ing model from ( <ref type="bibr" target="#b3">Fukui et al., 2016</ref>) which won the real image track of the VQA Challenge 2016. This model achieves an accuracy of 60.36% on the VQA validation set. 3 In this model, multimodal compact bilinear pooling is used to predict attention over im- age features and also to combine the attended image features with the question features. These combined features are passed through a FC layer to obtain a softmax distribution over the space of answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Generalization to novel instances</head><p>Do VQA models make mistakes because test in- stances are too different from training ones? To an- alyze the first type of novelty (the test QI pair is novel), we measure the correlation between test ac- curacy and distance of test QI pairs from its k near- est neighbor (k-NN) training QI pairs. For each test QI pair we find its k-NNs in the training set and compute the average distance between the test QI pair and its k-NNs. The k-NNs are computed in the space of combined image + question embed- ding (just before passing through FC layer) for all the three models (using euclidean distance metric for the CNN+LSTM model and cosine distance metric for the ATT and MCB models).</p><p>The correlation between accuracy and average distance is significant (-0.41 at k=50 4 for the CNN+LSTM model and -0.42 at k=15 5 for the ATT model). A high negative correlation value tells that the model is less likely to predict correct an- swers for test QI pairs which are not very similar to training QI pairs, suggesting that the model is not very good at generalizing to novel test QI pairs. The correlation between accuracy and average dis- tance is not significant for the MCB model (-0.14 at k=1 6 ) suggesting that MCB is better at generalizing to novel test QI pairs.</p><p>We also found that 67.5% of mistakes made by the CNN+LSTM model can be successfully predicted by checking distance of test QI pair from its k-NN training QI pairs (66.7% for the ATT model, 55.08% for the MCB model). Thus, this analysis not only exposes a reason for mistakes made by VQA mod- els, but also allows us to build human-like models that can predict their own oncoming failures, and potentially refuse to answer questions that are 'too different' from ones seen in past.</p><p>To analyze the second type of novelty (the answer required at test time is not familiar), we compute the correlation between test accuracy and the average distance of the test ground truth (GT) answer with GT answers of its k-NN training QI pairs. The dis- tance between answers is computed in the space of 4 k=50 leads to highest correlation 5 k=15 leads to highest correlation 6 k=1 leads to highest correlation These distance features are also good at pre- dicting failures -74.19% of failures can be pre- dicted by checking distance of test GT answer with GT answers of its k-NN training QI pairs for CNN+LSTM model (75.41% for the ATT model, 70.17% for the MCB model). Note that unlike the previous analysis, this analysis only explains fail- ures but cannot be used to predict failures (since it uses GT labels). See <ref type="figure" target="#fig_0">Fig. 1</ref> for qualitative examples.</p><p>From <ref type="figure" target="#fig_0">Fig. 1 (row1)</ref> we can see that the test QI pair is semantically quite different from its k-NN training QI pairs ({1st, 2nd, 3rd}-NN distances are {15.05, 15.13, 15.17}, which are higher than the corresponding distances averaged across all success cases: {8.74, 9.23, 9.50.}), explaining the mistake. Row2 shows an example where the model has seen the same question in the training set (test QI pair is semantically similar to training QI pairs) but, since it has not seen "green cone" for training instances (an- swers seen during training are different from what needs to be produced for the test QI pair), it is unable to answer the test QI pair correctly. This shows that current models lack compositionality: the ability to combine the concepts of "cone" and "green" (both of which have been seen in training set) to answer "green cone" for the test QI pair. This composition- ality is desirable and central to intelligence. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Complete question understanding</head><p>We feed partial questions of increasing lengths (from 0-100% of question from left to right). We then compute what percentage of responses do not change when more and more words are fed. <ref type="figure" target="#fig_1">Fig. 2</ref> shows the test accuracy and percentage of questions for which responses remain same (com- pared to entire question) as a function of partial question length. We can see that for 40% of the questions, the CNN+LSTM model seems to have converged on a predicted answer after 'listening' to just half the question. This shows that the model is listening to first few words of the question more than the words towards the end. Also, the model has 68% of the final accuracy (54%) when making pre- dictions based on half the original question. When making predictions just based on the image, the ac- curacy of the model is 24%. The ATT model seems to have converged on a predicted answer after listen- ing to just half the question more often (49% of the time), achieving 74% of the final accuracy (57%). The MCB model converges on a predicted answer after listening to just half the question 45% of the time, achieving 67% of the final accuracy (60%). See <ref type="figure" target="#fig_2">Fig. 3</ref> for qualitative examples.</p><p>We also analyze the change in responses of the model's predictions (see <ref type="figure" target="#fig_3">Fig. 4</ref>), when words of a particular part-of-the-speech (POS) tag are dropped from the question. The experimental results indi- cate that wh-words effect the model's decisions the most (most of the responses get changed on drop- ping these words from the question), and that pro- nouns effect the model's decisions the least.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Complete image understanding</head><p>Does a VQA model really 'look' at the image? To analyze this, we compute the percentage of the time (say X) the response does not change across im- ages (e.g.,, answer for all images is "2") for a given question (e.g., "How many zebras?") and plot his- togram of X across questions (see <ref type="figure">Fig. 5</ref>). We do this analysis for questions occurring for atleast 25 images in the VQA validation set, resulting in to- tal 263 questions. The cumulative plot indicates that for 56% questions, the CNN+LSTM model outputs the same answer for at least half the images. This is fairly high, suggesting that the model is picking the same answer no matter what the image is. Promis- ingly, the ATT and MCB models (that do not work with a holistic entire-image representation and pur- portedly pay attention to specific spatial regions in an image) produce the same response for at least half the images for fewer questions (42% for the ATT model, 40% for the MCB model).</p><p>Interestingly, the average accuracy (see the VQA accuracy plots in <ref type="figure">Fig. 5</ref>) for questions for which the models produce same response for &gt;50% and &lt;55% of the images is 56% for the CNN+LSTM <ref type="figure">Figure 5</ref>: Histogram of percentage of images for which model produces same answer for a given question and its comparison with test accuracy. The cumulative plot shows the % of questions for which model produces same answer for atleast x % of images. model (60% for the ATT model, 73% for the MCB model) which is more than the respective average accuracy on the entire VQA validation set (54.13% for the CNN+LSTM model, 57.02% for the ATT model, 60.36% for the MCB model). Thus, pro- ducing the same response across images seems to be statistically favorable. <ref type="figure" target="#fig_4">Fig. 6</ref> shows examples where the CNN+LSTM model predicts the same response across images for a given question. The first row shows examples where the model makes errors on several images by predicting the same answer for all images. The second row shows examples where the model is always correct even if it predicts the same answer across images. This is so because questions such as "What covers the ground?" are asked for an image in the VQA dataset only when ground is covered with snow (because subjects were looking at the image while asking questions about it). Thus, this analysis exposes label biases in the dataset. La- bel biases (in particular, for "yes/no" questions) have also been reported in ( ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We develop novel techniques to characterize the be- havior of VQA models, as a first step towards under- standing these models, meaningfully comparing the strengths and weaknesses of different models, devel- oping insights into their failure modes, and identify- ing the most fruitful directions for progress. Our be- havior analysis reveals that despite recent progress, today's VQA models are "myopic" (tend to fail on sufficiently novel instances), often "jump to conclu- sions" (converge on a predicted answer after 'listen- ing' to just half the question), and are "stubborn" (do not change their answers across images), with attention based models being less "stubborn" than non-attention based models.</p><p>As a final thought, we note that the somewhat pathological behaviors exposed in the paper are in some sense "correct" given the model architectures and the dataset being trained on. Ignoring optimiza- tion error, the maximum-likelihood training objec- tive is clearly intended to capture statistics of the dataset. Our motive is simply to better understand current generation models via their behaviors, and use these observations to guide future choices -do we need novel model classes? or dataset with dif- ferent biases? etc. Finally, it should be clear that our use of anthropomorphic adjectives such as "stub- born", "myopic" etc. is purely for pedagogical rea- sons -to easily communicate our observations to our readers. No claims are being made about today's VQA models being human-like.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Examples from test set where the CNN+LSTM model makes mistakes and their corresponding nearest neighbor training instances. See supplementary for more examples.</figDesc><graphic url="image-1.png" coords="3,313.20,57.82,226.80,124.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: X-axis shows length of partial question (in %) fed as input. Y-axis shows percentage of questions for which responses of these partial questions are the same as full questions and VQA accuracy of partial questions.</figDesc><graphic url="image-2.png" coords="4,72.00,57.83,226.79,119.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Examples where the CNN+LSTM model does not change its answer after first few question words. On doing so, it is correct for some cases (the extreme left example) and incorrect for other cases (the remaining three examples). See supplementary for more examples.</figDesc><graphic url="image-3.png" coords="4,313.20,57.83,226.79,80.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Percentage of questions for which responses remain same (compared to entire question) as a function of POS tags dropped from the question.</figDesc><graphic url="image-4.png" coords="4,313.20,207.09,226.80,110.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Examples where the predicted answers do not change across images for a given question. See supplementary for more examples.</figDesc><graphic url="image-6.png" coords="5,313.20,57.83,226.79,100.69" type="bitmap" /></figure>

			<note place="foot" n="1"> http://www.visualqa.org/challenge.html</note>

			<note place="foot" n="2"> Code available at https://github.com/ jiasenlu/HieCoAttenVQA 3 Code available at https://github.com/ akirafukui/vqa-mcb</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank the EMNLP reviewers for valuable feedback and Yash Goyal for sharing his code. This work was supported in part by: NSF CAREER awards, ARO YIP awards, ICTAS Junior Faculty awards, Google Faculty Research awards, awarded to both DB and DP, ONR grant N00014-14-1-0679, AWS in Education Research grant, NVIDIA GPU donation, awarded to DB, Paul G. Allen Family Foundation Allen Distinguished Investiga-tor award, ONR YIP and Alfred P. Sloan Fellow-ship, awarded to DP. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the of-ficial policies or endorsements, either expressed or implied, of the U.S. Government or any sponsor.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Deep compositional question answering with neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename></persName>
		</author>
		<idno>CVPR. 1</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning to compose neural networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename></persName>
		</author>
		<idno>NAACL. 1</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">ABC-CNN: an attention based convolutional neural network for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Antol</surname></persName>
		</author>
		<idno>abs/1511.05960. 1</idno>
		<editor>ICCV. 1, 2 [Chen et al.2015] Kan Chen, Jiang Wang, Liang-Chieh Chen, Haoyuan Gao, Wei Xu, and Ram Nevatia</editor>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
	<note>Vqa: Visual question answering</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Fukui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Visual Turing Test for Computer Vision Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Geman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<editor>PNAS. 1 [Hoiem et al.2012] Derek Hoiem, Yodsawalai Chodpathumwan, and Qieyun Dai</editor>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Diagnosing error in object detectors</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A focused dynamic attention model for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ilievski</surname></persName>
		</author>
		<idno>abs/1604.01485. 1</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Compositional memory for visual question answering. CoRR, abs/1511.05676. 1 [Kafle and Kanan2016] Kushal Kafle and Christopher Kanan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
		<idno>CVPR. 1</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Answer-type prediction for visual question answering</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Visualizing and understanding recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karpathy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR Workshop. 1</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multimodal residual learning for visual QA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS. 1</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deeper lstm and normalized cnn visual question answering model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Lu</surname></persName>
		</author>
		<ptr target="https://github.com/VT-vision-lab/VQA_LSTM_CNN.1,2" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hierarchical question-image coattention for visual question answering. In NIPS. 1, 2 [Malinowski and Fritz2014] Mateusz Malinowski and Mario Fritz. 2014. A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS. 1</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Training recurrent answering units with joint loss minimization for vqa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mikolov</surname></persName>
		</author>
		<idno>CVPR. 1</idno>
	</analytic>
	<monogr>
		<title level="m">ICLR. 3 [Noh and Han2016] Hyeonwoo Noh and Bohyung Han</title>
		<editor>Kevin J. Shih, Saurabh Singh, and Derek Hoiem</editor>
		<imprint>
			<publisher>CoRR</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Where to look: Focus regions for visual question answering</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Explicit knowledge-based reasoning for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno>abs/1511.02570. 1</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Ask me anything: Free-form visual question answering based on knowledge from external sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wu</surname></persName>
		</author>
		<idno>CVPR. 1</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Anton van den Hengel</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ask, attend and answer: Exploring questionguided spatial attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV. 1</title>
		<editor>ICML. 1 [Xu and Saenko2016] Huijuan Xu and Kate Saenko</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Dynamic memory networks for visual and textual question answering</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Yin and Yang: Balancing and answering binary visual questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<idno>CVPR. 5</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Simple baseline for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhou</surname></persName>
		</author>
		<idno>abs/1512.02167. 1</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
