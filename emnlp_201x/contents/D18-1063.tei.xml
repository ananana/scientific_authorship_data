<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Unsupervised Word Translations Without Adversaries</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmoy</forename><surname>Mukherjee</surname></persName>
							<email>{t.mukherjee@sms.,t.hospedales@}ed.ac.uk, myamada@i.kyoto-u.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Yamada</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Kyoto University</orgName>
								<orgName type="institution" key="instit2">RIKEN AIP</orgName>
								<address>
									<country>JST PRESTO</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Edinburgh</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Unsupervised Word Translations Without Adversaries</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="627" to="632"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>627</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Word translation, or bilingual dictionary induction , is an important capability that impacts many multilingual language processing tasks. Recent research has shown that word translation can be achieved in an unsuper-vised manner, without parallel seed dictionaries or aligned corpora. However, state of the art methods for unsupervised bilingual dictionary induction are based on generative adver-sarial models, and as such suffer from their well known problems of instability and hyper-parameter sensitivity. We present a statistical dependency-based approach to bilingual dictionary induction that is unsupervised-no seed dictionary or parallel corpora required; and introduces no adversary-therefore being much easier to train. Our method performs comparably to adversarial alternatives and out-performs prior non-adversarial methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Translating words between languages, or more generally inferring bilingual dictionaries, is a long-studied research direction with applications including machine translation ( <ref type="bibr" target="#b16">Lample et al., 2017)</ref>, multilingual word embeddings <ref type="bibr" target="#b13">(Klementiev et al., 2012)</ref>, and knowledge transfer to low resource languages ( <ref type="bibr" target="#b10">Guo et al., 2016)</ref>. Research here has a long history under the guise of deci- pherment ( <ref type="bibr" target="#b14">Knight et al., 2006</ref>). Current contempo- rary methods have achieve effective word transla- tion through theme-aligned corpora ( <ref type="bibr" target="#b9">Gouws et al., 2015)</ref>, or seed dictionaries ( <ref type="bibr">Mikolov et al., 2013)</ref>. <ref type="bibr">Mikolov et al. (2013)</ref> showed that monolingual word embeddings exhibit isomorphism across lan- guages, and can be aligned with a simple lin- ear transformation. Given two sets word vectors learned independently from monolingual corpora, and a dictionary of seed pairs to learn a linear transformation for alignment; they were able to estimate a complete bilingual lexicon. Many stud- ies have since followed this approach, proposing various improvements such as orthogonal map- pings ( <ref type="bibr" target="#b2">Artetxe et al., 2016</ref>) and improved objec- tives ( <ref type="bibr" target="#b17">Lazaridou et al., 2015)</ref>.</p><p>Obtaining aligned corpora or bilingual seed dic- tionaries is nevertheless not straightforward for all language pairs. This has motivated a wave of very recent research into unsupervised word trans- lation: inducing bilingual dictionaries given only monolingual word embeddings ( <ref type="bibr" target="#b5">Conneau et al., 2018;</ref><ref type="bibr">Zhang et al., 2017b,a;</ref><ref type="bibr" target="#b3">Artetxe et al., 2017)</ref>. The most successful have leveraged ideas from Generative Adversarial Networks (GANs) <ref type="bibr" target="#b8">(Goodfellow et al., 2014)</ref>. In this approach the genera- tor provides the cross-modal mapping, taking em- beddings of dictionary words in one language and 'generating' their translation in another. The dis- criminator tries to distinguish between this 'fake' set of translations and the true dictionary of em- beddings in the target language. The two play a competitive game, and if the generator learns to fool the discriminator, then its cross-modal map- ping should be capable of inducing a complete dic- tionary, as per <ref type="bibr">Mikolov et al. (2013)</ref>.</p><p>Despite these successes, such adversarial meth- ods have a number of well-known drawbacks <ref type="bibr" target="#b1">(Arjovsky et al., 2017)</ref>: Due to the nature of their min-max game, adversarial training is very un- stable, and they are prone to divergence. It is extremely hyper-parameter sensitive, requiring problem-specific tuning. Convergence is also hard to diagnose and does not correspond well to effi- cacy of the generator in downstream tasks <ref type="bibr" target="#b12">(Hoshen and Wolf, 2018)</ref>.</p><p>In this paper, we propose an alternative sta- tistical dependency-based approach to unsuper- vised word translation. Specifically, we propose to search for the cross-lingual word pairing that max- imizes statistical dependency in terms of squared loss mutual information (SMI) ( <ref type="bibr" target="#b23">Yamada et al., 2015;</ref><ref type="bibr" target="#b21">Suzuki and Sugiyama, 2010</ref>). Compared to prior statistical dependency-based approaches such as Kernelized Sorting (KS) ( <ref type="bibr" target="#b20">Quadrianto et al., 2009)</ref> we advance: (i) through use of SMI rather than their Hilbert Schmidt Independence Criterion (HSIC) and (ii) through jointly optimis- ing cross-modal pairing with representation learn- ing within each view. In contrast to prior work that uses a fixed representation, by non-linearly pro- jecting monolingual world vectors before match- ing, we learn a new embedding where statistical dependency is easier to establish. Our method: (i) achieves similar unsupervised translation perfor- mance to recent adversarial methods, while being significantly easier to train and (ii) clearly outper- forms prior non-adversarial methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed model 2.1 Deep Distribution Matching</head><p>Let dataset D contain two sets of unpaired mono- lingual word embeddings from two languages</p><formula xml:id="formula_0">D = ({x i } n i=1 , {y j } n j=1</formula><p>) where x, y ∈ R d . Let π be a permutation function over {1, 2, . . . , n}, and Π the corresponding permutation indicator ma- trix: Π ∈ {0, 1} n×n , Π1 n = 1 n , and Π 1 n = 1 n . Where 1 n is the n-dimensional vector with all ones. We aim to optimize for both the per- mutation Π (bilingual dictionary), and non-linear transformations g x (·) and g y (·) of the respective wordvectors, that maximize statistical dependency between the views. While regularising by requir- ing the original word embedding information is preserved through reconstruction using decoders f x (·) and f y (·). Our overall loss function is:</p><formula xml:id="formula_1">min Θx,Θy,Π Ω(D; Θ x , Θ y ) Regularizer − λD Π (D; Θ x , Θ y ) Dependency , D Π (D; Θ x , Θ y ) = D Π ({g x (x i ), g y (y π(i) )} n i=1 ), Ω(D; Θ x , Θ y ) = n i=1 x i − f x (g x (x i )) 2 2 + y i − f y (g y (y i )) 2 2 + R(Θ x ) + R(Θ y ).<label>(1)</label></formula><p>where Θs parameterize the encoding and recon- struction transformations, R(·) is a regularizer (e.g., <ref type="bibr">2</ref> -norm and 1 -norm), and D Π (·, ·) is a sta- tistical dependency measure. Crucially compared to prior methods such as matching CCA <ref type="bibr" target="#b11">(Haghighi et al., 2008)</ref>, dependency measures such as SMI do not need comparable representations to get started, making the bootstrapping problem less severe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dependence Estimation</head><p>Squared-Loss Mutual Information (SMI)</p><p>The squared loss mutual information between two random variables x and y is defined as <ref type="bibr" target="#b21">(Suzuki and Sugiyama, 2010)</ref>:</p><formula xml:id="formula_2">SMI = p(x, y) p(x)p(y) − 1 2 p(x)p(y)dxdy,</formula><p>which is the Pearson divergence (Pearson, 1900) from p(x, y) to p(x)p(y). The SMI is an f - divergence ( <ref type="bibr" target="#b0">Ali and Silvey, 1966)</ref>. That is, it is a non-negative measure and is zero only if the ran- dom variables are independent.</p><p>To measure SMI from a set of samples we take a direct density ratio estimation approach <ref type="bibr" target="#b21">(Suzuki and Sugiyama, 2010)</ref>, which leads ( <ref type="bibr" target="#b23">Yamada et al., 2015</ref>) to the estimator:</p><formula xml:id="formula_3">SMI({(x i , y i )} n i=1 ) = 1 2n tr (diag ( α) KL) − 1 2 ,</formula><p>where K ∈ R n×n and L ∈ R n×n are the gram matricies for x and y respectively, and</p><formula xml:id="formula_4">H = 1 n 2 (KK ) • (LL ), h = 1 n (K • L)1 n , α = H + λI n −1 h,</formula><p>λ &gt; 0 is a regularizer and I n ∈ R n×n is the iden- tity matrix.</p><p>SMI for Matching SMI computes the depen- dency between two sets of variables, under an as- sumption of known correspondence. In our ap- plication this corresponds to a measure of depen- dency between two aligned sets of monolingual wordvectors. To exploit SMI for matching, we introduce a permutation variable Π by replacing L → Π LΠ in the estimator:</p><formula xml:id="formula_5">SMI({(xi, y π(i) )} n 1 ) = 1 2n tr diag ( α Π ) KΠ LΠ − 1 2 ,</formula><p>that will enable optimizing Π to maximize SMI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Optimization of parameters</head><p>To initialize Θ x and Θ y , we first independently estimate them using autoencoders. Then we em- ploy an alternative optimization on Eq. (1) for (Θ x , Θ y ) and Π until convergence. We use 3 layer MLP neural networks for both f and g. Al- gorithm 1 summarises the steps. Optimization for Θ x and Θ y With fixed per- mutation matrix Π (or π), the objective function</p><formula xml:id="formula_6">min Θx,Θy Ω(D; Θ x , Θ y ) − λD Π (D; Θ x , Θ y ) (2)</formula><p>is an autoencoder optimization with regularizer D Π (·), and can be solved with backpropagation.</p><p>Optimization for Π To find the permuta- tion (word matching) Π that maximizes SMI given fixed encoding parameters Θ x , Θ y , we only need to optimize the dependency term D Π in Eq. (1). We employ the LSOM algorithm ( <ref type="bibr" target="#b23">Yamada et al., 2015</ref>). The estimator of SMI for samples</p><formula xml:id="formula_7">{g x (x i ), g y (y π(i) )} n i=1 encoded with g x , g y is: SMI = 1 2n tr diag ( α Θ,Π ) K Θx Π L Θy Π − 1 2 .</formula><p>Which leads to the optimization problem:</p><formula xml:id="formula_8">max Π∈{0,1} n×n tr diag ( α Θ,Π ) K Θx Π L Θy Π s.t. Π1 n = 1 n , Π 1 n = 1 n .<label>(3)</label></formula><p>Since the optimization problem is NP-hard, we iteratively solve the relaxed problem ( <ref type="bibr" target="#b23">Yamada et al., 2015)</ref>:</p><formula xml:id="formula_9">Π new = (1 − η)Π old + η argmax Π tr diag α Θ,Π old K Θx Π L Θy Π old ,</formula><p>where 0 &lt; η ≤ 1 is a step size. The optimization problem is a linear assignment problem (LAP). Thus, we can efficiently solve the algorithm by using the Hungarian method <ref type="bibr" target="#b15">(Kuhn, 1955)</ref>. To get discrete Π, we solve the last step by setting η = 1. Intuitively, this can be seen as searching for the permutation Π for which the data in the two (initially unsorted views) have a matching within- view affinity (gram) matrix, where matching is de- fined by maximum SMI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section, we evaluate the efficacy of our pro- posed method against various state of the art meth- ods for word translation. Implementation Details Our autoencoder con- sists of two layers with dropout and a tanh non- linearity. We use polynomial kernel to compute Algorithm 1 SMI-based unsupervised word trans- lation Input: Unpaired word embeddings D = ({x i } n i=1 , {y j } n j=1 ). 1: Init: weights Θ x , Θ y , permutation matrix Π. 2: while not converged do 3:</p><p>Update Θ x , Θ y given Π: Backprop (2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Update Π given Θ x , Θ y : LSOM (3). 5: end while Output: Permutation Matrix Π. Params Θ x , Θ y . the gram matrices K and L. For all pairs of lan- guages, we fix the number of training epochs to 20. All the word vectors are 2 unit normalized. For CSLS we set the number of neighbors to 10. For optimizing Π at each epoch, we set the step size η = 0.75 and use 20 iterations. For the regu- larization R(Θ), we use the sum of the Frobenius norms of weight matrices. We train Θ using full batch gradient-descent, with learning rate 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>We performed experiments on the publicly available English-Italian, English- Spanish and English-Chinese datasets released by ( <ref type="bibr" target="#b25">Zhang et al., 2017b;</ref><ref type="bibr" target="#b22">Vulic and Moens, 2013</ref>). We name this collective set of benchmarks BLI. We also conduct further experiments on a much larger recent public benchmark, MUSE (Conneau et al., 2018) <ref type="bibr">1</ref> . Setting and Metrics We evaluate all methods in terms of Precision@1, following standard prac- tice. We note that while various methods in the literature were initially presented as fully super- vised ( <ref type="bibr">Mikolov et al., 2013)</ref>, semi-supervised (us- ing a seed dictionary) ( <ref type="bibr" target="#b11">Haghighi et al., 2008)</ref>, or unsupervised ( <ref type="bibr" target="#b25">Zhang et al., 2017b</ref>), most of them can be straightforwardly adapted to run in any of these settings. Therefore we evaluate all methods both in the unsupervised setting in which we are primarily interested, and also the commonly eval- uated semi-supervised setting with 500 seed pairs. Competitors: Non-Adversarial In terms of competitors that, like us, do not make use of GANs, we evaluate: Translation Matrix ( <ref type="bibr">Mikolov et al., 2013)</ref>, which alternates between estimating a linear transformation by least squares and matching by nearest neighbour (NN). <ref type="bibr">Multilingual Correlation (Faruqui and Dyer, 2014)</ref>, and Matching CCA ( <ref type="bibr" target="#b11">Haghighi et al., 2008)</ref>, which alternates between matching and estimat-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MUSE Dataset BLI Datasets</head><p>Methods es-en en-es it-en en-it zh-en en-zh es-en en-es it-en en-it zh-en en-zh Methods es-en en-es it-en en-it zh-en en-zh es-en en-es it-en en-it zh-en en-zh  Competitors: Adversarial In terms of com- petitors that do make use of adversarial training, we compare: W-GAN and EMDOT (Zhang et al., 2017b) make use of adversarial learning using Wasserstein GAN and Earth Movers Distance re- spectively. GAN-NN ( <ref type="bibr" target="#b5">Conneau et al., 2018</ref>) uses adversarial learning to train an orthogonal trans- formation, along with some refinement steps and an improvement to the conventional NN match- ing procedure called 'cross-domain similarity lo- cal scaling' (CSLS). Since this is a distinct step, we also evaluate our method with CSLS. We use the provided code for GAN-NN and Self-Train, while re-implementing EDOT/W- GAN to avoid dependency on theano.  <ref type="figure" target="#fig_0">Figure 1</ref>: Training process of Deep-SMI larger dataset than BLI, benefitting methods that can exploit a large amount of training data. In the ground-truth annotation, BLI contains 1-1 transla- tions while MUSE contains more realistic 1-many translations (if any correct translation is picked, a success is counted), making it easier to reach a higher score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fully Unsupervised</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semi-supervised</head><p>Results using a 500-word bilingual seed dictionary are presented in <ref type="table" target="#tab_2">Table 2</ref>. From these we observe: (i) The conventional methods' performances (top) jump up, showing that they are more competitive if at least some sparse data is available. (ii) Deep-SMI perfor- mance also improves, and still outperforms the classic methods significantly overall. (iii) Again, we perform comparably to the GAN methods. <ref type="figure" target="#fig_0">Figure 1</ref> shows the convergence process of Deep- SMI. From this we see that: (i) Unlike the adver- sarial methods, our objective (Eq. (1)) improves smoothly over time, making convergence much easier to assess. (ii) Unlike the adversarial meth- ods, our accuracy generally mirrors the model's loss. In contrast, the various losses of the adver- sarial approaches do not well reflect translation ac- curacy, making model selection or early stopping a challenge in itself. Please compare our There are two steps in our optimization: match- ing permutation Π and representation weights Θ. Although this is an alternating optimization, it is analogous to an EM-type algorithm optimizing la- tent variables (Π) and parameters (Θ). While local minima are a risk, every optimisation step for either variable reduces our objective Eq. (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Discussion</head><p>There is no min-max game, so no risk of di- vergence as in the case of adversarial GAN-type methods.</p><p>Our method can also be understood as provid- ing an unsupervised Deep-CCA type model for re- lating heterogeneous data across two views. This is in contrast to the recently proposed unsuper- vised shallow CCA <ref type="bibr" target="#b12">(Hoshen and Wolf, 2018)</ref>, and conventional supervised Deep-CCA ( <ref type="bibr" target="#b4">Chang et al., 2018</ref>) that requires paired data for training; and us- ing SMI rather than correlation as the optimisation objective.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 with</head><label>1</label><figDesc>Figure 1 shows the convergence process of DeepSMI. From this we see that: (i) Unlike the adversarial methods, our objective (Eq. (1)) improves smoothly over time, making convergence much easier to assess. (ii) Unlike the adversarial methods, our accuracy generally mirrors the model's loss. In contrast, the various losses of the adversarial approaches do not well reflect translation accuracy, making model selection or early stopping a challenge in itself. Please compare our Figure 1 with Fig 3 in Zhang et al. (2017b), and Fig 2 in Conneau et al. (2018). There are two steps in our optimization: matching permutation Π and representation weights Θ. Although this is an alternating optimization, it is analogous to an EM-type algorithm optimizing latent variables (Π) and parameters (Θ). While local minima are a risk, every optimisation step for either variable reduces our objective Eq. (1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Semi-supervised word translation on MUSE and BLI using 500 seed pair initial dictionary. Precision @ 
1 metric. Top group: Conventional methods. Middle group: Adversarial methods. Bottom group: Our methods. 

ing a joint linear subspace. Kernelized Sort-
ing (Quadrianto et al., 2009), which directly uses 
HSIC-based statistical dependency to match het-
erogeneous data points. Self Training (Artetxe 
et al., 2017) A recent state of the art method 
that alternate between estimating an orthonormal 
transformation, and NN matching. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 1 presents</head><label>1</label><figDesc></figDesc><table>compar-
</table></figure>

			<note place="foot" n="1"> https://github.com/facebookresearch/MUSE/</note>

			<note place="foot" n="4"> Conclusion We have presented an effective approach to unsupervised word translation that performs comparably to adversarial approaches while being significantly easier to train and diagnose; as well as outperforming prior non-adversarial approaches.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A general class of coefficients of divergence of one distribution from another</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">D</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silvey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="131" to="142" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning principled bilingual mappings of word embeddings while preserving monolingual invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning bilingual word embeddings with (almost) no bilingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scalable and effective deep CCA via soft decorrelation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobin</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Word translation without parallel data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Improving zero-shot learning by mitigating the hubness problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICLR Workshops</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving vector space word representations using multilingual correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bilbowa: Fast bilingual distributed representations without word alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A distributed representation-based framework for cross-lingual transfer parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAIR</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="995" to="1023" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning bilingual lexicons from monolingual corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yedid</forename><surname>Hoshen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Inducing crosslingual distributed representations of words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Klementiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binod</forename><surname>Bhattarai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised analysis for decipherment problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL-COLING</title>
		<meeting>ACL-COLING</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harold W Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Naval research logistics quarterly</title>
		<imprint>
			<date type="published" when="1955" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="83" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Unsupervised machine translation using monolingual corpora only</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00043</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hubness and pollution: Delving into cross-space mapping for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Ilya Sutskever, and Google Inc. 2013. Exploiting similarities among languages for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Inc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mountain</forename><surname>View</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Inc</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Pearson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">302</biblScope>
			<biblScope unit="page" from="157" to="175" />
			<date type="published" when="1900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Kernelized sorting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Novi</forename><surname>Quadrianto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sufficient dimension reduction via squared-loss mutual information estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taiji</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Crosslingual semantic similarity of words as the similarity of their semantic word responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vulic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cross-domain matching with squared-loss mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michalis</forename><surname>Raptis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Machiko</forename><surname>Toyoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1764" to="1776" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adversarial training for unsupervised bilingual lexicon induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Earth mover&apos;s distance minimization for unsupervised bilingual lexicon induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
