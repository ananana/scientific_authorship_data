<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:34+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CharManteau: Character Embedding Models For Portmanteau Creation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Gangal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsh</forename><surname>Jhamtani</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CharManteau: Character Embedding Models For Portmanteau Creation</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2917" to="2922"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Portmanteaus are a word formation phenomenon where two words are combined to form a new word. We propose character-level neural sequence-to-sequence (S2S) methods for the task of portmanteau generation that are end-to-end-trainable, language independent, and do not explicitly use additional phonetic information. We propose a noisy-channel-style model, which allows for the incorporation of unsupervised word lists, improving performance over a standard source-to-target model. This model is made possible by an exhaustive candidate generation strategy specifically enabled by the features of the portmanteau task. Experiments find our approach superior to a state-of-the-art FST-based baseline with respect to ground truth accuracy and human evaluation.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Portmanteaus (or lexical blends <ref type="bibr" target="#b0">Algeo (1977)</ref>) are novel words formed from parts of multiple root words in order to refer to a new concept which can't otherwise be expressed concisely. Portman- teaus have become frequent in modern-day social media, news reports and advertising, one popu- lar example being Brexit (Britain + Exit). <ref type="bibr" target="#b15">Petri (2012)</ref>. These are found not only in English but many other languages such as Bahasa Indone- sia <ref type="bibr" target="#b6">Dardjowidjojo (1979)</ref>, Modern Hebrew Bat- El <ref type="bibr">(1996)</ref>; <ref type="bibr" target="#b3">Berman (1989)</ref> and <ref type="bibr">Spanish Piñeros (2004)</ref>. Their short length makes them ideal for headlines and brandnames <ref type="bibr" target="#b9">(Gabler, 2015)</ref>. Unlike better-defined morphological phenomenon such as inflection and derivation, portmanteau generation * * denotes equal contribution <ref type="figure">Figure 1</ref>: A sketch of our BACKWARD, noisy- channel model. The attentional S2S model with bidirectional encoder gives P (x|y) and next- character model gives P (y), where y (spime) is the portmanteau and x = concat(x (1) , ";", x <ref type="bibr">(2)</ref> ) are the concatenated root words (space and time).</p><p>is difficult to capture using a set of rules. For instance, <ref type="bibr" target="#b17">Shaw et al. (2014)</ref> state that the com- position of the portmanteau from its root words depends on several factors, two important ones being maintaining prosody and retaining charac- ter segments from the root words, especially the head. An existing work by <ref type="bibr" target="#b7">Deri and Knight (2015)</ref> aims to solve the problem of predicting portman- teau using a multi-tape FST model, which is data- driven, unlike prior approaches. Their methods rely on a grapheme to phoneme converter, which takes into account the phonetic features of the lan- guage, but may not be available or accurate for non-dictionary words, or low resource languages.</p><p>Prior works, such as <ref type="bibr" target="#b8">Faruqui et al. (2016)</ref>, have demonstrated the efficacy of neural approaches for morphological tasks such as inflection. We hy- pothesize that such neural methods can (1) pro- vide a simpler and more integrated end-to-end framework than multiple FSTs used in the previ- ous work, and (2) automatically capture features such as phonetic similarity through the use of char- acter embeddings, removing the need for explicit grapheme-to-phoneme prediction. To test these hypotheses, in this paper, we propose a neural S2S model to predict portmanteaus given the two root words, specifically making 3 major contributions:</p><p>• We propose an S2S model that attends to the two input words to generate portmanteaus, and an additional improvement that lever- ages noisy-channel-style modelling to incor- porate a language model over the vocabulary of words ( §2).</p><p>• Instead of using the model to directly pre- dict output character-by-character, we use the features of portmanteaus to exhaustively gen- erate candidates, making scoring using the noisy channel model possible ( §3).</p><p>• We curate and share a new and larger dataset of 1624 portmanteaus ( §4).</p><p>In experiments ( §5), our model performs better than the baseline Deri and Knight (2015) on both objective and subjective measures, demonstrating that such methods can be used effectively in a mor- phological task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed Models</head><p>This section describes our neural models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Forward Architecture</head><p>Under our first proposed architecture, the input se- quence x = concat(x (1) , ";", x (2) ), while the out- put sequence is the portmanteau y. The model learns the distribution P (y|x). The network architecture we use is an atten- tional S2S model ( <ref type="bibr" target="#b1">Bahdanau et al., 2014</ref>). We use a bidirectional encoder, which is known to work well for S2S problems with similar token order, which is true in our case. Let −−−−→ LST M and ←−−−− LST M represent the forward and reverse en- coder; e enc () and e dec () represent the character embedding functions used by encoder and decoder The following equations describe the model:</p><formula xml:id="formula_0">h −→ enc 0 = − → 0 , h ←− enc |x| = − → 0 h −→ enc t = −−−−→ LST M (h enc t−1 , e enc (x t )) h ←− enc t = ←−−−− LST M (h enc t+1 , e enc (x t )) h enc t = h −→ enc t + h ←− enc t h dec 0 = h enc |x| h dec t = LST M (h dec t−1 , [concat(e dec (y t−1 ), c t−1 )]) p t = softmax(W hs [concat(h dec t , c t )] + b s )</formula><p>The context vector c t is computed using dot- product attention over encoder states. We choose dot-product attention because it doesn't add extra parameters, which is important in a low-data sce- nario such as portmanteau generation.</p><formula xml:id="formula_1">a t i = dot(h dec t , h enc i ), α t = softmax(a t ) ct = i=|x| i=1 α t i h enc i</formula><p>In addition to capturing the fact that port- manteaus of two English words typically sound English-like, and to compensate for the fact that available portmanteau data will be small, we pre- train the character embeddings on English lan- guage words. We use character embeddings learnt using an LSTM language model over words in an English dictionary, 1 where each word is a se- quence of characters, and the model will predict next character in sequence conditioned on previ- ous characters in the sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Backward Architecture</head><p>The second proposed model uses Bayes's rule to reverse the probabilities P (y|x) = P (x|y)P (y)</p><formula xml:id="formula_2">P (x)</formula><p>to get argmax y P (y|x) = argmax y P (x|y)P (y). Thus, we have a reverse model of the probabil- ity P (x|y) that the given root words were gen- erated from the portmanteau and a character lan- guage model model P (y). This is a probability distribution over all character sequences y ∈ A * , where A is the alphabet of the language. This way of factorizing the probability is also known as a noisy channel model, which has recently also been shown to be effective for neural MT ( <ref type="bibr" target="#b11">Hoang et al. (2017)</ref>, <ref type="bibr" target="#b21">Yu et al. (2016)</ref>). Such a model offers two advantages 1. The reverse direction model (or alignment model) gives higher probability to those port- manteaus from which one can discern the root words easily, which is one feature of good portmanteaus.</p><p>2. The character language model P (y) can be trained on a large vocabulary of words in the language. The likelihood of a word y is fac- torized as P (y) = Π i=|y| i=1 P (y i |y i−1 1 ), where y i j = y i , y i+1 . . . y j , and we train a LSTM to maximize this likelihood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Making Predictions</head><p>Given these models, we must make predictions, which we do by two methods</p><p>Greedy Decoding: In most neural sequence- to-sequence models, we perform auto- regressive greedy decoding, selecting the next character greedily based on the prob- ability distribution for the next character at current time step. We refer to this decoding strategy as GREEDY.</p><p>Exhaustive Generation: Many portmanteaus were observed to be concatenation of a prefix of the first word and a suffix of the second. We therefore generate all candidate outputs which follow this rule. Thereafter we score these candidates with the decoder and output the one with the maximum score. We refer to this decoding strategy as SCORE.</p><p>Given that our training data is small in size, we expect ensembling <ref type="bibr" target="#b4">(Breiman, 1996</ref>) to help reduce model variance and improve performance. In this paper, we ensemble our models wherever men- tioned by training multiple models on 80% sub- samples of the training data, and averaging log probability scores across the ensemble at test-time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Dataset</head><p>The existing dataset by <ref type="bibr" target="#b7">Deri and Knight (2015)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we show results comparing var- ious configurations of our model to the base- line FST model of Deri and Knight (2015) (BASELINE). Models are evaluated using exact- matches (Matches) and average Levenshtein edit- distance (Distance) w.r.t ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Objective Evaluation Results</head><p>In Experiment 1, we follow the same setup as <ref type="bibr" target="#b7">Deri and Knight (2015)</ref>. D Wiki is split into 10 folds. Each fold model uses 8 folds for training, 1 for val- idation, and 1 for test. The average (10 fold cross- validation style approach) performance metrics on the test fold are then evaluated.   initializing the word embeddings. We believe this is because portmanteaus have high fidelity towards their root word characters and its critical that the model can observe all root sequence characters, which attention manages to do as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Performance on Uncovered Examples</head><p>The set of candidates generated before scoring in the approximate SCORE decoding approach sometimes do not cover the ground truth. This holds true for 229 out of 1223 examples in D Blind . We compare the FORWARD approach along with a GREEDY decoding strategy to the BASELINE ap- proach for these examples. Both FORWARD+GREEDY and the BASELINE get 0 Matches on these examples. The Distance for these examples is 4.52 for BASELINE and 4.09 for FORWARD+GREEDY. Hence, we see that one of our approaches (FORWARD+GREEDY) outper- forms BASELINE even for these examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Significance Tests</head><p>Since our dataset is still small relatively small (1223 examples), it is essential to verify whether BACKWARD is indeed statistically significantly better than BASELINE in terms of Matches.  <ref type="table">GROUND TRUTH  shopping;marathon  shopparathon  shoathon  shopathon  fashion;fascism  fashism  fashism  fashism  wiki;etiquette  wikiquette  wiquette  wikiquette  clown;president  clowident  clownsident  clownsident</ref>   <ref type="table">Table 4</ref>: AMT annotator judgements on whether our system's proposed portmanteau is better or worse compared to the baseline</p><p>In order to do this, we use a paired bootstrap 4 comparison <ref type="bibr" target="#b12">(Koehn, 2004</ref>) between BACKWARD and BASELINE in terms of Matches. BACKWARD is found to be better (gets more Matches) than BASELINE in 99.9% (p = 0.999) of the subsets.</p><p>Similarly, BACKWARD has a lower Distance than BASELINE by a margin of 0.2 in 99.5% (p = 0.995) of the subsets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Subjective Evaluation and Analysis</head><p>On inspecting outputs, we observed that often out- put from our system seemed good in spite of high edit distance from ground truth. Such aspect of an output seeming good is not captured satisfactorily by measures like edit distance. To compare the errors made by our model to the baseline, we de- signed and conducted a human evaluation task on AMT. <ref type="bibr">5</ref> In the survey, we show human annotators outputs from our system and that of the baseline. We ask them to judge which alternative is better overall based on following criteria: 1. It is a good shorthand for two original words 2. It sounds bet- ter. We requested annotation on a scale of 1-4. To avoid ordering bias, we shuffled the order of two portmanteau between our system and that of baseline. We restrict annotators to be from Anglo- phone countries, have HIT Approval Rate &gt; 80% and pay 0.40$ per HIT (5 Questions per HIT).</p><p>As seen in <ref type="table">Table 4</ref>, output from our system was labelled better by humans as compared to the base- line 58.12% of the time. <ref type="table" target="#tab_3">Table 3</ref> shows outputs from different models for a few examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work¨Ozbal</head><p>Work¨ Work¨Ozbal and Strapparava (2012) generate new words to describe a product given its category and prop- erties. However, their method is limited to hand- crafted rules as compared to our data driven ap- proach. Also, their focus is on brand names. <ref type="bibr" target="#b10">Hiranandani et al. (2017)</ref> have proposed an ap- proach to recommend brand names based on brand/product description. However, they con- sider only a limited number of features like mem- orability and readability. <ref type="bibr" target="#b18">Smith et al. (2014)</ref> de- vise an approach to generate portmanteaus, which requires user-defined weights for attributes like sounding good. Generating a portmanteau from two root words can be viewed as a S2S problem. Recently, neural approaches have been used for S2S problems <ref type="bibr" target="#b19">(Sutskever et al., 2014</ref>) such as MT. <ref type="bibr" target="#b13">Ling et al. (2015)</ref> and <ref type="bibr" target="#b5">Chung et al. (2016)</ref> have shown that character-level neural sequence mod- els work as well as word-level ones for language modelling and MT. <ref type="bibr" target="#b22">Zoph and Knight (2016)</ref> pro- pose S2S models for multi-source MT, which have multi-sequence inputs, similar to our case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have proposed an end-to-end neural system to model portmanteau generation. Our experiments show the efficacy of proposed system in predict- ing portmanteaus given the root words. We con- clude that pre-training character embeddings on the English vocabulary helps the model. Through human evaluation we show that our model's pre- dictions are superior to the baseline. We have also released our dataset and code 6 to encourage further research on the phenomenon of portman- teaus. We also release an online demo 7 where our trained model can be queried for portmanteau sug- gestions. An obvious extension to our work is to try similar models on multiple languages.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Attention matrices while generating slurve from slider;curve, and bennifer from ben;jennifer respectively, using Forward model. ; and. are separator and stop characters. Darker cells are higher-valued</figDesc><graphic url="image-2.png" coords="4,72.00,204.70,57.60,106.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Wiki as the dataset of 1223 examples not from Wikipedia. We observed that 84.7% of the words in D Large can be generated by concatenating prefix of first word with a suffix of the second. 2 Not all neologisms are portmanteaus, so we manually choose those which are for our dataset.</figDesc><table>contains 401 portmanteau examples from 
Wikipedia. We refer to this dataset as D Wiki . 
Besides being small for detailed evaluation, D Wiki 
is biased by being from just one source. We 
manually collect D Large , a dataset of 1624 distinct 
English portmanteaus from following sources: 

• Urban Dictionary 2 

• Wikipedia 

• Wiktionary 

• BCU's Neologism Lists from '94 to '12. 

Naturally, D Wiki ⊂ D Large . We define D Blind = 
D Large − D Model 
Attn 
Ens 
Init 
Search 
Matches 
Distance 

BASELINE 
-
-
-
-
45.39% 
1.59 

FORWARD 


× 
× 
GREEDY 
22.00% 
1.98 

× 

GREEDY 
28.00% 
1.90 

× 
× 
BEAM 
13.25% 
2.47 

× 

BEAM 
15.25% 
2.37 

× 
× 
SCORE 
30.25% 
1.64 

× 

SCORE 
32.88% 
1.53 



SCORE 
42.25% 
1.33 


× 
SCORE 
41.25% 
1.34 
× 
× 

SCORE 
6.75% 
3.78 
× 
× 
× 
SCORE 
6.50% 
3.76 

BACKWARD 


× 
× 
SCORE 
37.00% 
1.53 

× 

SCORE 
42.25% 
1.35 



SCORE 
48.75% 
1.12 


× 
SCORE 
46.50% 
1.24 
× 
× 

SCORE 
5.00% 
3.95 
× 
× 
× 
SCORE 
4.75% 
3.98 

Table 1: 10-Fold Cross-Validation results, D Wiki . 
Attn, Ens, Init denote attention, ensembling, and 
initializing character embeddings respectively. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 shows</head><label>1</label><figDesc></figDesc><table>the 
results of Experiment 1 for various model config-
urations. We get the BASELINE numbers from 
Deri and Knight (2015). Our best model obtains 
48.75% Matches and 1.12 Distance, compared to 
45.39% Matches and 1.59 Distance using BASE-
LINE. 
For Experiment 2, we seek to compare our best 
approaches from Experiment 1 to the BASELINE 
on a large, held-out dataset. Each model is trained 
on D Wiki and tested on D Blind . BASELINE was 
similarly trained only on D Wiki , making it a fair 
comparison. Table 2 shows the results 3 . Our best 
model gets Distance of 1.96 as compared to 2.32 
from BASELINE. 
We observe that the Backward architecture per-
forms better than Forward architecture, confirm-
ing our hypothesis in  §2.2. In addition, ablation 
results confirm the importance of attention, and </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Results on D Blind (1223 Examples). In 
general, BACKWARD architecture performs better 
than FORWARD architecture. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 : Example outputs from different models (Refer to appendix for more examples)</head><label>3</label><figDesc></figDesc><table>Judgement 
Percentage of total 
Much Better (1) 
29.06 
Better (2) 
29.06 
Worse (3) 
25.11 
Much Worse (4) 
16.74 

</table></figure>

			<note place="foot" n="1"> Specifically in our experiments, 134K words from the CMU dictionary (Weide, 1998).</note>

			<note place="foot" n="3"> For BASELINE (Deri and Knight, 2015), we use their trained model from http://leps.isi.edu/fst/ step-all.php</note>

			<note place="foot" n="4"> We average across M = 1000 randomly chosen subsets of D Blind , each of size N = 611 (≈ 1223/2) 5 We avoid ground truth comparison because annotators can be biased to ground truth due to its existing popularity.</note>

			<note place="foot" n="6"> https://github.com/vgtomahawk/ Charmanteau-CamReady 7 http://tinyurl.com/y9x6mvy</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Dongyeop Kang, David Mortensen, Qinlan Shen and anonymous reviewers for their valuable comments. This research was sup-ported in part by DARPA grant FA8750-12-2-0342 funded under the DEFT program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Blends, a structural and systemic view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Algeo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American speech</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="64" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Selecting the best of the worst: the grammar of Hebrew blends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Outi</forename><surname>Bat-El</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phonology</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">03</biblScope>
			<biblScope unit="page" from="283" to="328" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The role of blends in Modern Hebrew word-formation. Studia linguistica et orientalia memoriae Haim Blanc dedicata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruth</forename><surname>Berman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>Harrassowitz</publisher>
			<biblScope unit="page" from="45" to="61" />
			<pubPlace>Wiesbaden</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bagging predictors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="140" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A character-level decoder without explicit segmentation for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06147</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Acronymic Patterns in Indonesian</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soenjono</forename><surname>Dardjowidjojo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pacific Linguistics Series C</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="143" to="160" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">How to make a frenemy: Multitape FSTs for portmanteau generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliya</forename><surname>Deri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="206" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Morphological Inflection Generation using Character Sequence to Sequence Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="634" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The Weird Science of Naming New Products</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neal</forename><surname>Gabler</surname></persName>
		</author>
		<ptr target="http://tinyurl.com/lmlq7ex" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurush</forename><surname>Hiranandani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Maneriker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsh</forename><surname>Jhamtani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09335</idno>
		<title level="m">Generating appealing brand names</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong Duy Vu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.02854</idno>
		<title level="m">Decoding as Continuous Optimization in Neural Machine Translation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Statistical significance tests for machine translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="388" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04586</idno>
		<title level="m">Character-based neural machine translation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A computational approach to the automation of creative naming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gözdë</forename><surname>Ozbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Strapparava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="703" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Say No to Portmanteaus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Petri</surname></persName>
		</author>
		<ptr target="http://tinyurl.com/kvmep2t" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The creation of portmanteaus in the extragrammatical morphology of Spanish</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos-Eduardo</forename><surname>Piñeros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Probus</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="240" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Emergent faithfulness to morphological and semantic heads in lexical blends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><forename type="middle">E</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elliott</forename><surname>Moreton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Monrose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meetings on Phonology</title>
		<meeting>the Annual Meetings on Phonology</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Nehovah: A neologism creator nomen ipsum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Ryan S Hintze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ventura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Creativity</title>
		<meeting>the International Conference on Computational Creativity</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="173" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The CMU pronunciation dictionary, release 0.6</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weide</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02554</idno>
		<title level="m">The Neural Noisy Channel</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.00710</idno>
		<title level="m">Multi-source neural translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
