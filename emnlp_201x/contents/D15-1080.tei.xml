<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploring Markov Logic Networks for Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Khot</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niranjan</forename><surname>Balasubramanian</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Gribkoff $</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Allen Institute for AI</orgName>
								<orgName type="institution" key="instit2">+ Stony Brook University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Exploring Markov Logic Networks for Question Answering</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Elementary-level science exams pose significant knowledge acquisition and reasoning challenges for automatic question answering. We develop a system that reasons with knowledge derived from textbooks , represented in a subset of first-order logic. Automatic extraction, while scalable, often results in knowledge that is incomplete and noisy, motivating use of reasoning mechanisms that handle uncertainty. Markov Logic Networks (MLNs) seem a natural model for expressing such knowledge , but the exact way of leveraging MLNs is by no means obvious. We investigate three ways of applying MLNs to our task. First, we simply use the extracted science rules directly as MLN clauses and exploit the structure present in hard constraints to improve tractability. Second, we interpret science rules as describing prototypical entities, resulting in a drastically simplified but brittle network. Our third approach, called Praline, uses MLNs to align lexical elements as well as define and control how inference should be performed in this task. Praline demonstrates a 15% accuracy boost and a 10x reduction in runtime as compared to other MLN-based methods, and comparable accuracy to word-based baseline approaches.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We consider the problem of answering questions in standardized science exams <ref type="bibr" target="#b5">(Clark et al., 2013</ref>), which are used as a benchmark for developing knowledge acquisition and reasoning capabilities. The 4th grade science exam dataset from <ref type="bibr" target="#b5">Clark et al. (2013)</ref> tests for a wide variety of knowledge and its application to specific scenarios. In partic- ular we focus on a subset that test students' un- derstanding of various kinds of general rules and principles (e.g., gravity pulls objects towards the Earth) and their ability to apply these rules to reason about specific situations or scenarios (e.g., which force is responsible for a ball to drop?).</p><p>Answering these questions can be naturally for- mulated as a reasoning task given the appropri- ate form of knowledge. Prior work on reasoning based approaches has largely relied on manually input knowledge <ref type="bibr" target="#b12">(Lenat, 1995)</ref>. We present an in- vestigation of a reasoning approach that operates over knowledge automatically extracted from text.</p><p>In order to effectively reason over knowledge derived from text, a QA system must handle in- complete and potentially noisy knowledge, and allow for reasoning under uncertainty. We cast QA as a reasoning problem in weighted-first order logic. While many probabilistic formalisms ex- ist, we use Markov Logic Networks for the ease of specification of weighted rules. MLNs have been adopted for many NLP tasks <ref type="bibr" target="#b23">(Singla and Domingos, 2006a;</ref><ref type="bibr" target="#b11">Kok and Domingos, 2008;</ref><ref type="bibr" target="#b19">Poon and Domingos, 2009)</ref>. Recently, <ref type="bibr" target="#b2">Beltagy et al. (2013)</ref> and  have shown that MLNs can be used to reason with rules derived from natural language. While MLNs appear to be a natural fit, it is a priori unclear how to effectively formulate the QA task as an MLN problem. We find that unique characteristics of this domain pose new challenges in efficient inference. Moreover, it is unclear how MLNs might perform on automat- ically extracted noisy rules and how they would fare against simpler baselines that do not rely as much on structured logical representations.</p><p>Our goal is to build a high accuracy reasoning- based QA system that can answer a question in near real time. To this end, we investigate three MLN-based formulations: (1) A natural formula- tion that is intuitive but suffers from inefficient in-ference (e.g., over 10 minutes on 31% of the ques- tions); (2) an extension that improves efficiency by using prototypical constants, but is brittle to vari- ation in structure; and (3) a formulation with im- proved flexibility to handle variation in text and structure that is 15% more accurate and 10x faster than the other approaches.</p><p>Despite significant improvements over two nat- ural MLN formulations, the best reasoning-based configuration still does not outperform a simpler word-based baseline. We surmise that without ef- fective salience models on text-derived rules, rea- soning is unable to leverage the systematic ad- vantages of the MLN-based models. The im- proved flexibility in the MLN-based models es- sentially appears to approximate word-based ap- proaches due to the noisy and incomplete nature of the input knowledge. Nevertheless, the reasoning based method shows improved performance when adding multiple rules, which provides a principled way to inject additional knowledge and control in- ference for further improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background: QA Task</head><p>Following <ref type="bibr" target="#b6">Clark et al. (2014)</ref>, we formulate QA as a reasoning task over knowledge derived from textual sources. A multiple choice question with k answer options is turned into k true-false ques- tions, each of which asserts some known facts (the setup) and posits a query. The reasoning task is to determine whether the query is true given the setup and the input knowledge.</p><p>The input knowledge is derived from 4th-grade science texts and augmented with a web search for terms appearing in the texts. Much of this knowl- edge is in terms of generalities, expressed natu- rally as IF-THEN rules. We use the representation and extraction procedures of <ref type="bibr" target="#b6">Clark et al. (2014)</ref>, recapitulated briefly here for completeness.</p><p>Rule Representation: The generalities in text convey information about classes of entities and events. Following the neo-davidsonian reified rep- resentation ( <ref type="bibr" target="#b7">Curran et al., 2007)</ref>, we encode infor- mation about events (e.g, falling) and entities (e.g., ball or stone) using variables. Predicates such as agent, cause, function, towards, and in define semantic relationships between variables. Rather than committing to a type ontology, the variables are associated with their original string represen- tation using an isa predicate.</p><p>The "if" or antecedent part of the rule is se- mantically interpreted as being universally quanti- fied (omitted below for conciseness) whereas ev- ery entity or event mentioned only in the "then" or consequent part of the rule is treated as existentially quantified. Both antecedent and consequent are interpreted as conjunctions. For example, "Growing thicker fur in winter helps some animals to stay warm" translates into: isa(g, grow), isa(a, some animals), isa(f, thicker fur), isa(w, the winter),</p><formula xml:id="formula_0">agent(g, a), object(g, f ), in(g, w)</formula><p>⇒ ∃s, r : isa(s, stays), isa(r, warm),</p><formula xml:id="formula_1">enables(g, s), agent(s, a), object(s, r) (1)</formula><p>Question Representation: The question repre- sentation is computed similarly except that we use fixed constants (represented as block letters) rather than variables. For example, consider the ques- tion: "A fox grows thick fur as the season changes. This helps the fox to (A) hide from danger (B) at- tract a mate (C) find food (D) keep warm?" The T/F question corresponding to option (D) trans- lates into:</p><formula xml:id="formula_2">setup :isa(F, fox), isa(G, grows), isa(T, thick fur), agent(G, F ), object(G, T ) query :isa(K, keep warm), enables(G, K), agent(K, F )</formula><p>Lexical Reasoning: Since entity and event vari- ables hold textual values, reasoning must accom- modate lexical variability and textual entailment. For example, the surface forms "thick fur" and "thicker fur" are semantically equivalent. Also, the string "fox" entails "some animal". We use a lexical reasoning component based on textual en- tailment to establish lexical equivalence or entail- ment between variables.</p><p>Most Likely Answer as Inference: Given KB rules and the question as input, we formulate a probabilistic reasoning problem by adding lexical reasoning probabilities and incorporating uncer- tainties in derived rules. Given setup facts S and k answer options Q i , we seek the most likely answer option: arg max i∈{1,...,k} Pr[Q i | S, KB ]. This is a Partial MAP computation which is known to be #P-hard <ref type="bibr" target="#b17">(Park, 2002</ref>). Hence methods such as In- teger Linear Programming are not directly appli- cable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Challenges</head><p>Reasoning with text-derived knowledge presents challenges that expose the brittleness and rigidity inherent in pure logic-based frameworks. Text- derived rules are incomplete and include lexical items as logical elements, making rule application in a pure logical setting extremely brittle: Many relevant rules cannot be applied because their pre- conditions are not fully satisfied due to poor align- ment. For example, naive matching of rule <ref type="formula">(1)</ref> with the facts in the setup would not conclude the query since the rule requires "in the winter" to be true. A robust inference mechanism must allow for rule application with partial evidence. Further, a single text-derived rule may be insuf- ficient to answer a question. For example, "An- imals grow thick fur in winter" and "Thick fur helps keep warm" may need to be chained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Probabilistic Formulations</head><p>Statistical Relational Learning (SRL) mod- els <ref type="bibr">(Getoor and Taskar, 2007</ref>) are a natural fit for QA reasoning. They provide probabilistic seman- tics over knowledge in first-order logic, thereby handling uncertainty in lexical reasoning and incomplete matching. While there are many SRL formalisms including Stochastic Logic Programs (SLPs) <ref type="bibr" target="#b14">(Muggleton, 1996)</ref>, <ref type="bibr">ProbLog (Raedt et al., 2007)</ref>, and PRISM ( <ref type="bibr" target="#b21">Sato and Kameya, 2001</ref>), we use Markov Logic Networks (MLNs) for their ease of specification and ability to naturally handle potentially cyclic rules.</p><p>Markov Logic Networks (MLNs) are rela- tional models represented using weighted first- order logic rules. The rules provide a template for generating a Markov network by grounding the variables to all the constants in the rules. Each rule f i forms a clique in the ground network and its weight w i determines the potential for the clique. Since all cliques generated by grounding the same clause have the same weight, the probability of a given assignment is calculated as:</p><formula xml:id="formula_3">P (X = x) = 1 Z exp i w i n i (x)</formula><p>where n i (x) is the number of times the ith for- mula is satisfied by the world x and Z is a normal- ization constant. Intuitively, a rule with a positive weight is more likely to be true than false.Higher the weight of a rule, more likely it is to be true.</p><p>We explore three MLN formulations: a) First-order MLN: Given a question and rel- evant first-order KB rules, we convert them into an MLN program and let MLN inference automati- cally handle rule chaining. While a natural first- order formulation of the QA task, this struggles with long conjunctions and existentials in rules, as well as relatively few atoms and little to no sym- metries. This results in massive grounding sizes, not remedied easily by existing solutions such as lazy, lifted, or structured inference. We exploit the structure imposed by hard constraints to vastly simplify groundings and bring them to the realm of feasibility, but performance remains poor. b) Entity Resolution MLN: Instead of reason- ing with rules that express generalities over classes of individuals, we replace the variables in the previous formulation with prototypical constants. This reduces the number of groundings, while re- taining the crux of the reasoning problem defined over generalities. Combining this idea with exist- ing entity resolution approaches substantially im- proves scalability. However, this turns out to be too brittle in handling lexical mismatches, espe- cially in the presence of differences in parse struc- tures c) Praline MLN: Both of the above MLNs rely on exactly matching the relations in the KB and question representation, making them too sensitive to syntactic differences. In response, PRobabilis- tic ALignment and INferencE (Praline) performs inference using primarily the string constants but guided by the edge or relational structure. We relax the rigidity in rule application by explicitly modeling the desired QA inference behavior in- stead of relying on MLN's semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">First-Order MLN Formulation</head><p>For a set R of first-order KB rules, arguably the most natural way to represent the QA task of computing Pr[Q i | S, R] as an MLN pro- gram M is to simply add R essentially verba- tim as first-order rules in M . For all existen- tially quantified variables, we introduce a new do- main constant. Predicates of M are those in R, along with a binary entails predicate represent- ing the lexical entailment blackbox, which allows M to probabilistically connect lexically related constants such as "thick fur" and "thicker fur" or "fox" and "some animals". entails is defined to be closed-world and is not necessarily transitive.</p><p>Evidence: Soft evidence for M consists of entails relations between every or- dered pair of entity (or event) strings, e.g., entails(fox, some animals). Hard evidence for M comprises grounded atoms in S.</p><p>Query: The query atom in M is result(), a new zero-arity predicate result() that is made equiva- lent to the conjunction of the predicates in Q i that have not been included in the evidence. We are interested in computing Pr[result() = true].</p><p>Semantic Rules: In addition to KB science rules, we add semantic rules that capture the in- tended meaning of our predicates, such as ev- ery event has a unique agent, cause(x, y) → effect(y, x), etc. Semantic predicates also en- force natural restrictions such as non-reflexivity, !r(x, x), and anti-symmetry, r(x, y) →!r(y, x).</p><p>Finally, to help bridge lexical gaps more, we use a simple external lexical alignment algo- rithm to estimate how much does the setup en- tail antecedent r of each KB rule r, and how much does consequent r entail query. These are then added as two additional MLN rules per KB rule.</p><p>Our rules have a specific first-order logic form:</p><formula xml:id="formula_4">∀x 1 , .., x k i R i (x i 1 , x i 2 ) → ∃x k+1 , .., x k+m j R j (x j 1 , x j 2 )</formula><p>Existentials spanning conjunctions in the conse- quent of this rule form can neither be directly fed into existing MLN systems nor naively translated into a standard form without incurring an expo- nential blowup. We introduce a new "existential" predicate E α j (x 1 , . . . , x k , x k+j ) for each existen- tial variable x k+j in each such rule α. This predi- cate becomes the consequent of α, and hard MLN rules make it equivant to the original consequent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Boosting Inference Efficiency.</head><p>A bottleneck in using MLN solvers out-of-the-box for this QA formulation is the prohibitively large grounded network size. For example, 34 out of 108 runs timed out during MLN grounding phase after 6 minutes. On average, the ground networks in these runs were of the order of 1.4 × 10 6 ground clauses. Such behavior has also been observed, perhaps to a lesser degree, in related NLP tasks such as RTE ( ) and STS ( ).</p><p>Existing techniques address large grounding size by focusing on relevant atoms <ref type="bibr" target="#b24">(Singla and Domingos, 2006b;</ref><ref type="bibr" target="#b22">Shavlik and Natarajan, 2009)</ref> or grouping atoms into large classes of inter- changeable atoms (de Salvo <ref type="bibr" target="#b8">Braz et al., 2005;</ref><ref type="bibr" target="#b10">Gogate and Domingos, 2011;</ref><ref type="bibr" target="#b25">Venugopal and Gogate, 2012)</ref>. Our QA encoding has very few atoms (often under 500) but very long clauses and highly asymmetric structure. This makes existing methods ineffective. For example, lazy inference in Alchemy-1 1 reduced ∼70K ground clauses to ∼56K on a question, while our method, described next, brought it down to only 951 clauses. Fur- ther, Lifted Blocked Gibbs and Probabilistic Theo- rem Proving, as implemented in Alchemy-2, were slower than basic Alchemy-1.</p><p>We utilize the combinatorial structure imposed by the set H of hard constraints (e.g., semantic rules, definition style rules, some science rules) present in the MLN, and use it to simplify the grounding of both hard and soft constraints. Im- portantly, this does not alter the first-order MLN semantics. The approach thus embraces hard clauses rather than relaxing them, as is often done in probabilistic inference techniques, especially when avoiding infinite energy barriers in MCMC based methods. Assuming an arbitrary constraint ordering in H, let F i denote the first i constraints. Starting with i = 1, we generate the propositional grounding G i of F i , use a propositional satisfia- bility (SAT) solver to identify the set B i of back- bone variables of G i (i.e., variables that take a fixed value in all solutions to G i ), freeze values of the corresponding atoms in B i , increment i, and repeat until G |H| has been processed. Although the end result can be described simply as freez- ing atoms corresponding to the backbone variables in the grounding of H, the incremental process helps us control the intermediate grounding size as a propositional variable is no longer generated for a frozen atom. Once the freezing process is complete, the full grounding of H is further sim- plified by removing frozen variables. Finally, the soft constraints S are grounded much more effi- ciently by taking frozen atoms into account. Our approach may also be seen as an extension of a proposal by Papai et al. (2011).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Entity Resolution Based MLN</head><p>Representing generalities as quantified rules de- fined over classes of entities or events appears to be a natural formulation, but is also quite inef- ficient leading to large grounded networks.</p><note type="other">De- spite the drastically reduced number of groundings by our inference approach, the first-order MLN formulation still timed out on 31% of the ques- tions. Hence we consider an alternative formula- tion that treats generalities as relations expressed over prototypical entities and events. This formu- lation leverages the fact that elementary level sci- ence questions can often be answered using rela- tively simple logical reasoning over exemplar ob- jects and homogeneous classes of objects. The only uncertainty present in our system is what's introduced by lexical variations and extraction er- rors, which we handle with probabilistic equality.</note><p>KB Rules and Question: We define rules over prototypical entity/event constants, rather than first-order variables. These constants are tied to their respective string representations, with the un- derstanding that two entities behave similarly if they have lexically similar strings. For example,</p><formula xml:id="formula_5">agent(Grow , Animals), object(Grow , Fur ) ⇒ enables(Grow , StayWarm)</formula><p>What was a first-order rule in FO-MLN is now al- ready fully grounded! Entities/events in the ques- tion are also similarly represented by constants. Note that the efficiency boost using hard con- straints (Section 3.1.1) is orthogonal to using pro- totypical constants and can be applied here as well.</p><p>Equivalence or Resolution Rules: Using a sim- ple probabilistic variant of existing Entity/Event Resolution frameworks <ref type="bibr" target="#b23">(Singla and Domingos, 2006a;</ref><ref type="bibr" target="#b11">Kok and Domingos, 2008)</ref>, we ensure that (a) two entities/events are considered similar when they are tied to lexically similar strings and (b) similar entities/events participate in similar rela- tions w.r.t. other entities/events. This defines soft clusters or equivalence classes of entities/events. We use a probabilistic sameAs predicate which is reflexive, symmetric, and transitive, and interacts with the rest of the MLN as follows:</p><formula xml:id="formula_6">isa(x, s), entails(s, s ) → isa(x, s ).</formula><p>isa(x, s), isa(y, s) → sameAs(x, y).</p><p>w : isa(x, s), !isa(y, s) → !sameAs(x, y) r(x, y), sameAs(y, z) → r(x, z). r in the last rule refers to any of the MLN pred- icates other than entails and isa. The sameAs predicate, as before, is implemented in a typed fashion, separately for entities and events. We will refer to this formulation as ER-MLN.</p><p>Partial Match Rules: Due to lexical variability, often not all conjuncts in a rule's antecedent are present in the question's setup. To handle incom- plete matches, for each KB derived MLN rule of the form (∧ k i=1 L i ) → R, we also add k soft rules of the form L i → R. This adds flexibility, by help- ing "fire" the rule in a soft manner. This differs from FO-MLN which uses an external alignment system to find parts of the antecedent mentioned in the setup, L and creates one rule L ⇒ R.</p><p>Comparison with FO-MLN: Long KB rules and question representation now no longer have quantified variables, only the binary or ternary rules above do. These mention at most 3 variables each and thus have relatively manageable ground- ings. On the other hand, as discussed in the next section, ER-MLN can fail on questions that have distinct entities with similar string representations (e.g. two distinct plants in a question would map to the same entity). Further, it fails to apply valid rules in the presence of syntactic differences such as agent(Fall, Things) generated by "things fall due to gravity" and object(Dropped, Ball) for "a student dropped a ball". Although "drop" entails "fall" and "ball" entails "object", ER-MLN cannot reliably bridge the structural difference involving object and agent, as these two relationships typi- cally aren't equivalent. Despite these limitations, ER-MLN provides a substantial scalability advan- tage over FO-MLN on a vast majority of the ques- tions that remain within its scope.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">PRobabilistic ALignment and INferencE</head><p>ER-MLN handles some of the word-level lexi- cal variation via resolution and soft partial match rules that break long antecedents. However, it is still rigid in two respects:</p><p>(1) ER-MLN primarily relies on the predicates (also referred to as links or edges) for inference. As a result, even if the words in the antecedent and setup have high entailment scores, the rule will still not "fire" if the edges do not match.</p><p>(2) As entities bound to lexically equivalent strings are forced to "behave" identically, ER-MLN fails on questions that involve two different entities that are bound to equivalent string repre- sentations. Consider the question: "A student puts two identical plants in the same type and amount of soil. She puts one of these plants near a sunny window and the other in a dark room. This exper- iment tests how the plants respond to (A) light (B) air (C) water (D) soil." The entities correspond- ing to the two plants will be bound to equivalent string representations and hence will be treated as the same entity. To avoid this, we do not force the entailment-based clusters of constants to behave similarly. Instead, as discussed below, we use the clusters to guide inference in a softer manner.</p><p>To introduce such flexibility, we define an MLN to directly control how new facts are inferred given the KB rules. The flexibility to control inference helps address two additional QA challenges:</p><p>Acyclic inference: While knowledge is ex- tracted from text as a set of directed rules each with an antecedent and a consequent, there is no guarantee that the rules taken together are acyclic. For example, a rule stating "Living things → de- pend on the sun" and "Sun → source of energy for living things" may exist side-by-side. Successful inference for QA must avoid feedback loops.</p><p>False unless proven: While MLNs assume atoms not mentioned in any rule to be true with probability 0.5, elementary level science reason- ing is better reflected in a system that assumes all atoms to be false unless stated in the question or proven through the application of a rule. This is similar to the semantics of Problog ( <ref type="bibr" target="#b20">Raedt et al., 2007)</ref> and PRISM <ref type="bibr" target="#b21">(Sato and Kameya, 2001)</ref>.</p><p>While acyclic inference and false unless proven can be handled by setting high negative priors in MLNs, inference behavior is susceptible to varia- tions in these weights. By using hard rules to con- trol the direction of inference, we can explicitly enforce these constraints.</p><p>We introduce a unary predicate called holds over string constants to capture the probability of a string constant being true given the setup is true (∀x ∈ setup, holds(x) = true) and the KB rules hold. Instead of using edges for inference, we use them as factors influencing alignment: similar constants have similar local neighborhoods. With n string constants, this reduces the number of un- observed groundings from O(n 2 ) edges in the ER- MLN to O(n) existence predicates. For the exam- ple rule (1), Praline can be viewed as using the following rule for inference:</p><p>holds(Grow ), holds(Animals), holds(Fur ), holds(Winter ) ⇒ holds(Stays), holds(Warm)</p><p>If we view KB rules and question as a labeled graph G <ref type="figure" target="#fig_1">(Figure 1)</ref>, alignment between string con- stants corresponds to node alignment in G. The nodes and edges of G are the input to the MLN, and the holds predicate on each node captures the probability of it being true given the setup. We now use MLNs (as described below) to define the inference procedure for any such input graph G.</p><p>Evidence: We represent the graph struc- ture of G using predicates node(nodeid) and edge(nodeid , nodeid , label ).</p><p>We use setup(nodeid ) and query(nodeid ) to represent the question's setup and query, resp. Similarly, we use inLhs(nodeid ) and inRhs(nodeid ) to represent rules' antecedent and consequent, resp. Graph Alignment Rules: Similar to the previ- ous approaches, we use entailment scores between words and short phrases to compute the alignment. In addition, we also expect aligned nodes to have similar edge structures:</p><p>aligns(x, y), edge(x, u, r),edge(y, v, s)</p><formula xml:id="formula_7">⇒ aligns(u, v)</formula><p>That is, if node x aligns with y then their chil- dren/ancestors should also align. We create copies of these rules for edges with the same label, r = s, with a higher weight and for edges with different labels, r = s, with a lower weight.</p><p>Inference Rules: We use MLNs to define the inference procedure to prove the query using the alignments from aligns. We assume that any node y that aligns with a node x that holds, also holds:</p><formula xml:id="formula_8">holds(x), aligns(x, y) ⇒ holds(y)<label>(2)</label></formula><p>For example, if the setup mentions "fox", all nodes that entail "fox" also hold. As we also use the edge structure during alignment, we would have a lower probability of "fox" in "fox finds food" to align with "animal" in "animal grows fur" as compared to "animal" in "animal finds food".</p><p>We use KB rules to further infer new facts that should hold based on the rule structure. We compute lhsHolds, the probability of the rule's  antecedent holding, and use it to infer rhsHolds, the probability of the consequent. Similar to ER- MLN, we break the rule into multiple small rules. Acyclic inference: We use two pred- icates, proves(nodeid , nodeid ) and ruleProves(rule, rule), to capture the infer- ence chain between nodes and rules, resp. To ensure acyclicity in inference, we add transi- tive clauses over these predicates and disallow reflexivity, i.e., !proves(x, x), and update rule (2):</p><p>w p :proves(x, y), holds(x) ⇒ holds(y) w a :aligns(x, y) ⇒ proves(x, y)</p><p>We capture inference direction between rules by checking consequent and antecedent alignments:</p><p>proves(x, y), inrhs(x, r1), inlhs(y, r2) ⇒ ruleP roves(r1, r2).</p><p>False unless proven: To ensure that nodes hold only if they can be proven from setup, we add bidirectional implications to our rules. An alter- native is to introduce a strong negative prior on holds and have a higher positive weight on all other clauses that conclude holds. However, the performance of our MLNs was very sensitive to the choice of the weight. We instead model this constraint explicitly. <ref type="figure" target="#fig_1">Figure 1</ref> shows a sample in- ference chain using dotted lines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Empirical Evaluation</head><p>We used Tuffy 0.4 4 (Niu et al., 2011) as the base MLN solver <ref type="bibr">5</ref> and extended it to incorpo- rate the hard-constraint based grounding reduc- tion technique discussed earlier, implemented us- ing the SAT solver Glucose 3.0 6 (Audemard and Simon, 2009) exploiting its "solving under as- sumptions" capability for efficiency. We used a 10 minute timelimit, including a max of 6 minutes for grounding. Marginal inference was performed using MC-SAT ( <ref type="bibr" target="#b18">Poon and Domingos, 2006</ref>), with default parameters and 5000 flips per sample to generate 500 samples for marginal estimation.</p><p>We used a 2-core 2.5 GHz Amazon EC2 linux machine with 16 GB RAM. We selected 108 elementary-level science questions (non-diagram, multiple-choice) from 4th grade New York Re- gents exam as our benchmark (Dev-108) and used another 68 questions from the same source as a blind test set (Unseen-68) <ref type="bibr">7</ref>   The KB, representing roughly 47,000 sen- tences, was generated in advance by processing the New York Regents 4th grade science exam syllabus, the corresponding Barron's study guide, and documents obtained by querying the Inter- net for relevant terms. Given a question, we use a simple word-overlap based matching algo- rithm, referred to as the rule selector, to retrieve the top 30 matching sentences to be considered for the question. Textual entailment scores be- tween words and short phrases were computed us- ing WordNet <ref type="bibr" target="#b13">(Miller, 1995)</ref>, and converted to "de- sired" probabilities for soft entails evidence. The accuracy reported for each approach is computed as the number of multiple-choice questions it an- swers correctly, with a partial credit of 1/k in case of a k-way tie between the highest scoring options if they include the correct answer. <ref type="table" target="#tab_1">Table 1</ref> compares the effectiveness of our three MLN formulations: FO-MLN, ER-MLN, and Pra- line. For each question and approach, we generate an MLN program for each answer option using the most promising KB rule for that answer option.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">MLN Formulation Comparison</head><p>In the case of FO-MLN, Tuffy exceeded the 6 minute time limit when generating groundings for 34 of the 108 × 4 MLNs for the Dev-108 question set, quitting after working with 1.4 × 10 6 clauses on average, despite starting with only around 35 first-order MLN rules. In the remaining MLNs, where our clause reduction technique successfully finished, the ground network size reduced dramat- ically to 524 clauses and 384 atoms on average.</p><p>Tuffy finished inference for all 4 answer options for 82 of the 108 questions; for other questions, it chose the most promising answer option among the ones it finished processing. Overall, this re- sulted in a score of 33.6% with an average of 280 seconds per multiple-choice question on Dev-108, and similar performance on Unseen-68.</p><p>ER-MLN, as expected, did not result in any timeouts during grounding. The number of ground clauses here, 2,308 on average, is dominated not by KB rules but by the binary and ternary en- tity resolution clauses involving the sameAs pred- icate. ER-MLN was roughly 33% faster than FO- MLN, but overall achieved similar exam scores.</p><p>Praline resulted in a 10x speedup over ER- MLN, explained in part by much smaller ground networks with only 219 clauses on average. It boosted exam performance by roughly 15%, push- ing it up to 48.8% on Dev-108 and 46.3% on Unseen-68 (statistically significantly better than FO-MLN with p-value &lt; 0.05). This demonstrates the value that the added flexibility and control Pra- line brings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Praline: Improvements and Ablation</head><p>We evaluate Praline when using multiple KB rules as a chain or multiple inference paths. Simply using the top two rules for inference turns out to be ineffective as they are often very similar. In- stead, we use incremental inference where we add one rule, perform inference to determine which additional facts now hold and which setup facts haven't yet been used, and then use this infor- mation to select the next best rule. This, as the Chain=2 entries in the first row of <ref type="table" target="#tab_2">Table 2</ref> show, improves Praline's accuracy on both datasets. The improvement comes at the cost of a modest run- time increase from 17 seconds per question to 38.</p><p>Finally, we evaluate the impact of Praline's ad- ditional rules to handle acyclicity (Acyclic) and the false unless proven (FUP) constraint. As   either of these constraints, highlighting their im- portance. Specifically, when using only one KB rule, dropping FUP clauses has a bigger influence that dropping Acyclic constraint clauses. With a single rule, there is still a possibility of cyclic in- ference within a rule, leading to a small drop in score there as well. When chaining multiple rules, however, the possibility of incorrect cyclic infer- ence is higher and we see a correspondingly larger drop in score when dropping Acyclic constraints. <ref type="table" target="#tab_4">Table 3</ref> compares Praline to a baseline word-based method on two question sets. The new set here is from 4th and 5th grade, with 170 Dev and 176 un- seen questions. The word-based approach calcu- lates the entailment score, using the same methods as for the soft entails evidence earlier, between the words in the T/F question and words in a rule in the KB. It then uses the maximum entailment score from all selected rules as the confidence measure i.e. max r∈R entailment(q, r). While the scores of the two approaches are statistically sim- ilar (p-value &gt; 0.05), the simple word-based ap- proach does have a slight edge over Praline. Au- tomatic extraction of knowledge from text pro- vides additional information (e.g., rule structure) that MLNs are capable of exploiting. However, we found this additional flexibility to not pay off with the current knowledge-base and questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison to baseline approaches</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>Reasoning with automatically extracted knowl- edge presents significant challenges. Our investigation of MLN-based formulations for elementary-level science exams highlights two key issues: 1) Natural translations of text de- rived knowledge into first-order representations are highly inefficient, resulting in large ground networks. 2) When the logical elements in the rules largely mirror the constructs in the source text, reasoning is hampered because of structural variability. In response, we proposed, Praline, an alignment based solution that is both efficient and accurate. Praline reasons with prototypical con- stants, and provides greater flexibility in how in- ference is performed and is therefore more robust to structural mismatches.</p><p>MLNs provided a flexible, structured frame- work to define inference for the QA task, while also providing reasoning chains used to arrive at an answer. While models such as MLNs seem a perfect fit for textual reasoning tasks such as RTE and QA, their performance on these tasks is still not up to par with textual feature-based ap- proaches ). We con- jecture that the increased flexibility of complex relational models results in increased susceptibil- ity to noisy input, and the systematic advantages of MLN models are difficult to exploit with text- derived rules. Automatically learning weights of these models may allow leveraging their flexibil- ity to address these issues, but weight learning re- mains challenging with only distant supervision.</p><p>We hope our datasets, knowledge bases, and MLN models 8 will help push NLP and SRL com- munities towards designing improved structured reasoning QA systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: KB rule and question as a graph where blue:setup; green:query; orange:antecedent; purple:consequent; dotted lines: alignments. lhsHolds combines individual probabilities of antecedent nodes and rhsHolds captures the probability of the consequent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2 w</head><label>2</label><figDesc>:holds(x), inLhs(x, r) ⇒ lhsHolds(r) w :!holds(x), inLhs(x, r) ⇒!lhsHolds(r) lhsHolds(r) ⇒ rhsHolds(r). rhsHolds(r), inRhs(r, x) ⇒ holds(x).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>.</head><label></label><figDesc></figDesc><table>Question 

MLN 
#Answered 
Exam 
#MLN 
#Atoms 
#Ground 
Runtime 
Set 
Formulation 
(some / all) 
Score 
Rules 
Clauses 
(all) 

Dev-108 
FO-MLN 
106 / 82 
33.6% 
35 
384  *  
524  *  
280 s 
ER-MLN 
107 / 107 
34.5% 
41 
284 
2,308 
188 s 
PRALINE 
108 
48.8% 
51 
182 
219 
17 s 

Unseen-68 
FO-MLN 
66 
33.8% 
-
-
-
288 s 
ER-MLN 
68 
31.3% 
-
-
-
226 s 
PRALINE 
68 
46.3% 
-
-
-
17 s 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>QA performance of various MLN formulations. #MLN-Rules, #GroundClauses, and Runtime 
per multiple-choice question are averaged over the corresponding dataset. #Answered column indicates 
questions where at least one answer option didn't time out (left) and where no answer option timed out 
(right). Of the 432 Dev MLNs (108 × 4), #Atoms and #GroundClauses for FO-MLN are averaged over 
the 398 MLNs where grounding finished; 34 remaining MLNs timed out after processing 1.4M clauses. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 shows, Praline's accuracy drops upon removing</head><label>2</label><figDesc></figDesc><table>One rule 

Chain=2 
MLN 
Dev-108 Unseen Dev-108 Unseen 

Praline 
48.8% 
46.3% 
50.3% 
52.7% 
-Acyclic 
44.7% 
36.0% 
43.6% 
30.9% 
-FUP 
35.0% 
30.9% 
42.1% 
29.4% 
-FUP -Acyclic 
37.3% 
34.2% 
36.6% 
24.3% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 : QA performance of Praline MLN variations.</head><label>2</label><figDesc></figDesc><table>Dev-108 Unseen-68 Dev-170 Unseen-176 

Praline 
50.3% 
52.7% 
33.2% 
36.6% 
Word-based 57.4% 
51.5% 
40.3% 
43.3% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 : QA performance: Praline vs. word-based.</head><label>3</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> http://alchemy.cs.washington.edu</note>

			<note place="foot" n="2"> An intuitive alternative for the 2nd clause doesn&apos;t capture the intending meaning, −w :!holds(x), inLhs(x, r) ⇒ lhsHolds(r) Praline defines a meta-inference procedure that is easily modifiable to enforce desired QA inference behavior, e.g. w : aligns(x, y), setup(x) ⇒ !query(y) would prevent a term from the setup to align with the query. Further, by representing the input KB and question as evidence, we can define a single static first-order MLN for all the questions instead of a compiled MLN for every question. This opens up the possibility of learning weights of this static MLN, which would be challenging for the previous two approaches. 3</note>

			<note place="foot" n="3"> In this work, we have set the weights manually. 4 http://i.stanford.edu/hazy/tuffy 5 Alchemy 1.0 gave similar results. 6 http://www.labri.fr/perso/lsimon/glucose 7 http://allenai.org/content/data/Ariscienceexams.txt</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like thank Pedro Domingos and Dan Weld for invaluable discussions and the Aristo team at AI2, especially Jesse Kinkead, for help with prototype development and evaluation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Predicting learnt clauses quality in modern SAT solvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Audemard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21st IJCAI</title>
		<meeting><address><addrLine>Pasadena, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-07" />
			<biblScope unit="page" from="399" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Efficient Markov logic inference for natural language semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Islam</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-07" />
			<pubPlace>Quebec City, Canada</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Montague meets Markov: Deep semantics with probabilistic logical form</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuong</forename><surname>Islam Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Boleda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Erk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
		<ptr target="http://allenai.org/software.html693" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Lexical and Computational Semantics: Proceeding of the Main Conference and the Shared Task</title>
		<imprint>
			<biblScope unit="page" from="11" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Probabilistic soft logic for semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Islam Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Erk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">52nd ACL</title>
		<meeting><address><addrLine>Baltimore, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="1210" to="1219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A study of the AKBC/requirements for passing an elementary science test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niranjan</forename><surname>Balasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the AKBC-WEKEX workshop at CIKM</title>
		<meeting>of the AKBC-WEKEX workshop at CIKM</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic construction of inferencesupporting knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niranjan</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumithra</forename><surname>Bhakthavatsalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Humphreys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Kinkead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th Workshop on Automated Knowledge Base Construction (AKBC)</title>
		<meeting><address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Linguistically motivated large-scale NLP with C&amp;C and Boxer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Curran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">45th ACL</title>
		<meeting><address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="33" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lifted first-order probabilistic inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>De Salvo Braz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eyal</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">19th IJCAI</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1319" to="1325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Introduction to Statistical Relational Learning</title>
		<editor>Lise Getoor and Ben Taskar</editor>
		<imprint>
			<date type="published" when="2007" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Probabilistic theorem proving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Gogate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011-07" />
			<biblScope unit="page" from="256" to="265" />
			<pubPlace>Barcelona, Spain</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Extracting semantic networks from text via relational clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><surname>Kok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">19th ECML</title>
		<meeting><address><addrLine>Antwerp, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-09" />
			<biblScope unit="page" from="624" to="639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cyc: A large-scale investment in knowledge infrastructure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Lenat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="33" to="38" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Stochastic logic programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stephen Muggleton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Inductive Logic Programming</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Tuffy: Scaling up statistical inference in Markov Logic Networks using an RDBMS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anhai</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jude</forename><forename type="middle">W</forename><surname>Shavlik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">37th VLDB</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="373" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Constraint propagation for efficient inference in Markov logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tivadar</forename><surname>Papai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parag</forename><surname>Singla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th CP</title>
		<meeting><address><addrLine>Perugia, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="691" to="705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">MAP complexity results and approximation methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Park</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002-08" />
			<biblScope unit="page" from="388" to="396" />
			<pubPlace>Edmonton, Canada</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sound and efficient inference with probabilistic and deterministic dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21st AAAI</title>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="458" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Problog: A probabilistic Prolog and its application in link discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelika</forename><surname>Luc De Raedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannu</forename><surname>Kimmig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toivonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Parameter learning of logic programs for symbolic-statistical modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taisuke</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Kameya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="page" from="391" to="454" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Speeding up inference in Markov logic networks by preprocessing to reduce the size of the resulting grounded network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jude</forename><surname>Shavlik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sriraam</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21st IJCAI</title>
		<meeting><address><addrLine>Pasadena, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1951" to="1956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Entity resolution with Markov logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parag</forename><surname>Singla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th ICDM</title>
		<meeting><address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-12" />
			<biblScope unit="page" from="572" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Memoryefficient inference in relational domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parag</forename><surname>Singla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21st AAAI</title>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="488" to="493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">On lifting the gibbs sampling algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Venugopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Gogate</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-12" />
			<biblScope unit="page" from="1664" to="1672" />
			<pubPlace>Lake Tahoe, NV</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
