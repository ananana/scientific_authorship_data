<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Verb Phrase Ellipsis Resolution Using Discriminative and Margin-Infused Algorithms</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kian</forename><surname>Kenyon-Dean</surname></persName>
							<email>kian.kenyon-dean@mail.mcgill.ca,</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jackie</forename><surname>Chi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kit</forename><surname>Cheung</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Precup</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Verb Phrase Ellipsis Resolution Using Discriminative and Margin-Infused Algorithms</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1734" to="1743"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Verb Phrase Ellipsis (VPE) is an anaphoric construction in which a verb phrase has been elided. It occurs frequently in dialogue and informal conversational settings, but despite its evident impact on event coreference resolution and extraction, there has been relatively little work on computational methods for identifying and resolving VPE. Here, we present a novel approach to detecting and resolving VPE by using supervised discriminative machine learning techniques trained on features extracted from an automatically parsed, publicly available dataset. Our approach yields state-of-the-art results for VPE detection by improving F1 score by over 11%; additionally, we explore an approach to antecedent identification that uses the Margin-Infused-Relaxed-Algorithm, which shows promising results.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Verb Phrase Ellipsis (VPE) is an anaphoric construc- tion in which a verbal constituent has been omitted. In English, an instance of VPE consists of two parts: a trigger, typically an auxiliary or modal verb, that indicates the presence of a VPE; and an antecedent, which is the verb phrase to which the elided element resolves ( <ref type="bibr" target="#b2">Bos and Spenader, 2011;</ref><ref type="bibr" target="#b6">Dalrymple et al., 1991)</ref>. For example, in the sentence, "The govern- ment includes money spent on residential renova- tion; Dodge does not", the trigger "does" resolves to the antecedent "includes money spent on residen- tial renovation".</p><p>The ability to perform VPE resolution is impor- tant for tasks involving event extraction, especially in conversational genres such as informal dialogue where VPE occurs more frequently <ref type="bibr" target="#b17">(Nielsen, 2005</ref>). Most current event extraction systems ignore VPE and derive some structured semantic representation by reading information from a shallow dependency parse of a sentence. Such an approach would not only miss many valid links between an elided verb and its arguments, it could also produce nonsensi- cal extractions if applied directly on an auxiliary trigger. In the example above, a naive approach might produce an unhelpful semantic triple such as <ref type="bibr">(Dodge, agent, do)</ref>.</p><p>There have been several previous empirical stud- ies of VPE <ref type="bibr" target="#b9">(Hardt, 1997;</ref><ref type="bibr" target="#b17">Nielsen, 2005;</ref><ref type="bibr" target="#b2">Bos and Spenader, 2011;</ref><ref type="bibr" target="#b3">Bos, 2012;</ref><ref type="bibr" target="#b13">Liu et al., 2016</ref>). Many previous approaches were restricted to solving spe- cific subclasses of VPE (e.g., VPE triggered by do <ref type="bibr" target="#b3">(Bos, 2012)</ref>), or have relied on simple heuristics for some or all of the steps in VPE resolution, such as by picking the most recent previous clause as the an- tecedent.</p><p>In this paper, we develop a VPE resolution pipeline which encompasses a broad class of VPEs <ref type="figure" target="#fig_0">(Figure 1</ref>), decomposed into the following two steps. In the VPE detection step, the goal is to determine whether or not a word triggers VPE. The second step, antecedent identification, requires selecting the clause containing the verbal antecedent, as well as determining the exact boundaries of the antecedent, which are often difficult to define.</p><p>Our contribution is to combine the rich linguis- tic analysis of earlier work with modern statistical approaches adapted to the structure of the VPE res- olution problem. First, inspired by earlier work, our system exploits linguistically informed features specific to VPE in addition to standard features such as lexical features or POS tags. Second, we adapt the Margin-Infused-Relaxed-Algorithm (MIRA) <ref type="bibr" target="#b5">(Crammer et al., 2006</ref>), which has been popular in other tasks, such as machine translation ( <ref type="bibr" target="#b21">Watanabe et al., 2007</ref>) and parsing <ref type="bibr" target="#b15">(McDonald et al., 2005</ref>), to antecedent identification. This algo- rithm admits a partial loss function which allows candidate solutions to overlap to a large degree. This makes it well suited to antecedent identification, as candidate antecedents can overlap greatly as well.</p><p>On VPE detection, we show that our approach sig- nificantly improves upon a deterministic rule-based baseline and outperforms the state-of-the-art system of <ref type="bibr" target="#b13">Liu et al. (2016)</ref> by 11%, from 69.52% to 80.78%. For antecedent identification we present results that are competitive with the state-of-the-art ( <ref type="bibr" target="#b13">Liu et al., 2016)</ref>. We also present state-of-the-art results with our end-to-end VPE resolution pipeline. Finally, we perform feature ablation experiments to analyze the impact of various categories of features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>VPE has been the subject of much work in the- oretical linguistics <ref type="bibr" target="#b19">(Sag, 1976;</ref><ref type="bibr">Dalrymple et al., 1991, inter alia)</ref>. VPE resolution could have a sig- nificant impact on related problems such as event coreference resolution ( <ref type="bibr" target="#b11">Lee et al., 2012;</ref><ref type="bibr" target="#b1">Bejan and Harabagiu, 2010;</ref><ref type="bibr" target="#b12">Liu et al., 2014</ref>) and event ex- traction <ref type="bibr" target="#b0">(Ahn, 2006;</ref><ref type="bibr" target="#b10">Kim et al., 2009;</ref><ref type="bibr" target="#b18">Ritter et al., 2012)</ref>. It has, however, received relatively little at- tention in the computational literature.</p><p>Hardt (1992) engaged in the first study of compu- tational and algorithmic approaches for VPE detec- tion and antecedent identification by using heuris- tic, linguistically motivated rules. Hardt (1997) ex- tracted a dataset of 260 examples from the WSJ cor- pus by using an algorithm that exploited null ele- ments in the PTB parse trees. <ref type="bibr" target="#b17">Nielsen (2005)</ref> built a dataset that combined sections of the WSJ and BNC; he showed that the more informal settings captured in the BNC corpora show significantly more fre- quent occurrences of VPE, especially in dialogue ex- cerpts from interviews and plays. Using this dataset, he created a full VPE pipeline from raw input text to a full resolution by replacing the trigger with the intended antecedent <ref type="bibr">1</ref> .</p><p>Bos and Spenader (2011) annotated the WSJ for occurrences of VPE. They found over 480 instances of VPE, and 67 instances of the similar phenomenon of do-so anaphora. Bos (2012) studied do-VPE by testing algorithmic approaches to VPE detection and antecedent identification that utilize Discourse Rep- resentation Theory.</p><p>Concurrently with the present work, <ref type="bibr" target="#b13">Liu et al. (2016)</ref> explored various decompositions of VPE res- olution into detection and antecedent identification subtasks, and they corrected the BNC annotations created by <ref type="bibr" target="#b17">Nielsen (2005)</ref>, which were difficult to use because they depended on a particular set of preprocessing tools. Our work follows a similar pipelined statistical approach. However, we explore an expanded set of linguistically motivated features and machine learning algorithms adapted for each subtask. Additionally, we consider all forms of VPE, including to-VPE, whereas Liu et al. only con- sider modal or light verbs (be, do, have) as candi- dates for triggering VPE. This represented about 7% of the dataset that they examined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach and Data</head><p>We divide the problem into two separate tasks: VPE detection (Section 4), and antecedent identification (Section 5). Our experiments use the entire dataset presented in ( <ref type="bibr" target="#b2">Bos and Spenader, 2011</ref>). For prepro- cessing, we used CoreNLP ( <ref type="bibr" target="#b14">Manning et al., 2014</ref>) to automatically parse the raw text of WSJ for fea- ture extraction. We also ran experiments using gold- standard parses; however, we did not find significant differences in our results 2 . Thus, we only report re- sults on automatically generated parses. We divide auxiliaries into the six different cate- gories shown in <ref type="table">Table 1</ref>, which will be relevant for our feature extraction and model training process, as we will describe. This division is motivated by the fact that different auxiliaries exhibit different be- haviours ( <ref type="bibr" target="#b2">Bos and Spenader, 2011</ref>). The results we present on the different auxiliary categories (see Ta- bles 2 and 4) are obtained from training a single clas- sifier over the entire dataset and then testing on aux- iliaries from each category, with the ALL result be- ing the accuracy obtained over all of the test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">VPE Detection</head><p>The task of VPE detection is structured as a binary classification problem. Given an auxiliary, a, we extract a feature vector f , which is used to predict whether or not the auxiliary is a trigger for VPE. In <ref type="figure" target="#fig_0">Figure 1</ref>, for example, there is only one auxiliary present, "does", and it is a trigger for VPE. In our experiments, we used a logistic regression classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Feature Extraction</head><p>We created three different sets of features related to the auxiliary and its surrounding context.</p><p>Auxiliary. Auxiliary features describe the charac- teristics of the specific auxiliary, including the fol- lowing:</p><p>word identity of the auxiliary lemma of the auxiliary auxiliary type (as shown in <ref type="table">Table 1</ref>)</p><p>Lexical. These features represent: the three words before and after the trigger their part-of-speech (POS) tags their POS bigrams</p><p>Syntactic. We devise these features to encode the relationship between the candidate auxiliary and its local syntactic context. These features were deter- mined to be useful through heuristic analysis of VPE instances in a development set. The feature set in- cludes the following binary indicator features (a = the auxiliary): a c-commands 4 a verb a c-commands a verb that comes after it a verb c-commands a a verb locally 5 c-commands a a locally c-commands a verb a is c-commanded by "than", "as", or "so" a is preceded by "than", "as", or "so" a is next to punctuation the word "to" precedes a a verb immediately follows a a is followed by "too" or "the same"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline</head><p>As a baseline, we created a rule-based system in- spired by <ref type="bibr" target="#b17">Nielsen's (2005)</ref> approach to solving VPE detection. The baseline algorithm required signifi- cant experimental tuning on the development set be- cause different linguistically hand-crafted rules were needed for each of the six trigger forms. For exam- ple, the following rule for modals achieved 80% F1- accuracy (see <ref type="table" target="#tab_2">Table 2</ref>): "assume VPE is occurring if the modal does not c-command a verb that follows it". The other trigger forms, however, required sev- eral layers of linguistic rules. The rules for be and have triggers were the most difficult to formulate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiments</head><p>We evaluate our models as usual using precision, re- call and F1 metric for binary classification. The pri- mary results we present in this section are obtained through 5-fold cross validation over all 25 sections of the automatically-parsed dataset. We use cross validation because the train-test split suggested by Bos and Spenader (2011) could result in highly var- ied results due to the small size of the dataset (see <ref type="table">Table 1</ref>). Because the vast majority of auxiliaries do not trigger VPE, we over-sample the positive cases during training. <ref type="table" target="#tab_2">Table 2</ref> shows a comparison be- tween the machine learning technique and a rule- based baseline for the six auxiliary forms. <ref type="table" target="#tab_3">Table 3</ref> shows results obtained from using the same train- test split used by <ref type="bibr" target="#b13">Liu et al. (2016)</ref> in order to provide a direct comparison.</p><p>Results. Using a standard logistic regression clas- sifier, we achieve an 11% improvement in accuracy over the baseline approach, as can be seen in <ref type="table" target="#tab_2">Table 2</ref>.</p><p>The rule-based approach was insufficient for be and have VPE, where logistic regression provides the largest improvements. Although we improve upon the baseline by 29%, the accuracy achieved for be- VPE is still low; this occurs mainly because: (i) be is the most commonly used auxiliary, so the number of negative examples is high compared to the num- ber of positive examples; and, (ii) the analysis of the some of the false positives showed that there may have been genuine cases of VPE that were missed by the annotators of the dataset ( <ref type="bibr" target="#b2">Bos and Spenader, 2011</ref>   tecedent italicized) "Some people tend to ignore that a 50-point move is less in percentage terms than it was when the stock market was lower."; here it is clear that was is a trigger for VPE.</p><p>In <ref type="table" target="#tab_3">Table 3</ref>, we compare our results to those achieved by <ref type="bibr" target="#b13">Liu et al. (2016)</ref> when using WSJ sets 0-14 for training and sets 20-24 for testing. We im- prove on their overall accuracy by over 11%, due to the 25% improvement in recall achieved by our method. Our results show that oversampling the pos- itive examples in the dataset and incorporating lin- guistically motivated syntactic features provide sub- stantial gains for VPE detection. Additionally, we consider every instance of the word to as a potential trigger, while they do not -this lowers their recall be- cause they miss every gold-standard instance of to- VPE. Thus, not only do we improve upon the state- of-the-art accuracy, but we also expand the scope of VPE-detection to include to-VPE without causing a significant decrease in accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Antecedent Identification</head><p>In this section we assume that we are given a trig- ger, from which we have to determine the correct antecedent; i.e., in the example in <ref type="figure" target="#fig_0">Figure 1</ref>, our task would be to identify "includes money spent on res-idential renovation" as the correct antecedent. Our approach to this problem begins with generating a list of candidate antecedents. Next, we build a fea- ture vector for each candidate by extracting features from the context surrounding the trigger and an- tecedent. Lastly, we use these features to learn a weight vector by using the Margin-Infused-Relaxed- Algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Candidate Generation</head><p>We generate a list of candidate antecedents by first extracting all VPs and ADJPs (and all contiguous combinations of their constituents) from the current sentence and the prior one. We then filter these can- didates by predefining possible POS tags that an an- tecedent can start or end with according to the train- ing set's gold standard antecedents. This method generates an average of 55 candidate antecedents per trigger, where triggers in longer sentences cause the creation of a larger number of candidate antecedents due to the larger number of VPs. This strategy ac- counts for 92% of the gold antecedents on the val- idation set by head match. We experimented with a less restrictive generation filter, but performance was not improved due to the much larger number of candidate antecedents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Feature Extraction</head><p>We construct a feature vector representation for each candidate antecedent; in the example in <ref type="figure" target="#fig_0">Figure 1</ref>, for example, we would need feature vectors that differ- entiate between the two potential antecedents "in- cludes money" and "includes money spent on resi- dential renovation".</p><p>Alignment. This feature set results from an align- ment algorithm that creates a mapping between the S-clause nearest to the trigger, S t , and the S-clause nearest to the potential antecedent, S a . The purpose of these features is to represent the parallelism (or lack thereof) between an antecedent's local vicinity with that of the trigger. The creation of this align- ment algorithm was motivated by our intuition that the clause surrounding the trigger will have a par- allel structure to that of the antecedent, and that an alignment between the two would best capture this parallelism. In the example sentence in <ref type="figure" target="#fig_1">Figure 2</ref> (trigger in bold, antecedent italicized) "Investors can get slightly higher yields on deposits below $50,000 than they can on deposits of $90,000 and up" a sim- ple observation of parallelism is that both the trig- ger and the correct antecedent are followed by the phrase "on deposits".</p><p>Formally, for each S ∈ {S a , S t }, we extract the dependencies in S as chunks of tokens, where each dependency chunk d i contains all tokens between its governor and dependent (whichever comes first). Next, for each d i ∈ S a , if d i contains any tokens that belong to the antecedent, delete those tokens. Sim- ilarly, for each d i ∈ S t , delete any token in d i that belongs to T . We then perform a bipartite match- ing to align the d i ∈ S t to the d j ∈ S a , where each edge's weight is determined by a scoring func- tion s <ref type="figure">(d i , d j )</ref>. The scoring function we use consid- ers the F1-similarity between the lemmas, POS-tags, and words shared between the two chunks, as well as whether or not the chunks share the same depen- dency name.</p><p>In the example in <ref type="figure" target="#fig_1">Figure 2</ref> we can see that the correct antecedent, "get slightly higher yields", has a stronger alignment than the incorrect one, "get slightly higher yields on deposits". This occurs be- cause we remove the candidate antecedent from its S-clause before creating the chunks; this leaves three nodes for the correct antecedent which map to the three nodes of the trigger's S-clause. However, this process only leaves two nodes for the incorrect can- didate antecedent, thus causing one chunk to be un- mapped, thus creating a weaker alignment.</p><p>We then use this mapping to generate a feature vector for the antecedent, which contains: the mini- mum, maximum, average, and standard deviation of the scores between chunks in the mapping; the num- ber and percentage of unmapped chunks; the depen- dencies that have (and have not) been mapped to; the dependency pairs that were mapped together; and the minimum, maximum, average, and standard de- viation of the cosine-similarity between the average word embedding of the words in a chunk between each d i , d j pair in the mapping.</p><p>NP Relation. These features compare the Noun Phrase (NP) closest to the antecedent to the NP clos- est to the trigger. This is motivated by an obser- vation of many instances of VPE where it is often the case that the entity preceding the trigger is either repeated, similar, or corefers to the entity preced- ing the antecedent. The relationship between each NP is most significantly represented by features cre- ated with pre-trained word2vec word embeddings ( <ref type="bibr" target="#b16">Mikolov et al., 2013</ref>). For each NP, and for each word in the NP, we extract its pre-trained word em- bedding and then average them all together. We then use the cosine similarity between these two vectors as a feature.</p><p>Syntactic. Syntactic features are based on the re- lationship between the candidate antecedent's parse tree with that of the trigger. This feature set includes the following features, with the last three being in- fluenced by Hardt's (1997) "preference factors" (a = candidate antecedent, t = trigger): if a's first word is an auxiliary if a's head (i.e., first main verb) is an auxiliary the POS tag of a's first and last words the frequency of each POS tag in the antecedent the frequency of each phrase (i.e., NP, VP, ADJP, etc.) in a's sentence and t's sentence if "than", "as", or "so" is between a and t if the word before a has the same POS-tag or lemma as t if a word in a c-commands a word in t if a's first or last word c-commands the trigger Be-Do Form: if the lemma of the token preced- ing a is be and the t's lemma is do Recency: distance between a and t and the dis- tance between the t's nearest VP and a Quotation: if t is between quotation marks and similarly for a</p><p>Matching. This last feature set was influenced by the features described by <ref type="bibr" target="#b13">Liu et al. (2016)</ref>. We only use the "Match" features described by them; namely: whether the POS-tags, lemmas, or words in a two-token window before the start of the an- tecedent exactly match the two before the trigger; and whether the POS-tag, lemma, or word of the ith token before the antecedent equals that of the i-1th token before the trigger (for i ∈ {1, 2, 3}, where i = 1 considers the trigger itself).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Training Algorithm -MIRA</head><p>Since many potential antecedents share relatively similar characteristics, and since we have many fea- tures and few examples, we use the Margin-Infused- Relaxed-Algorithm (MIRA) in order to identify the most likely potential antecedent. MIRA maximizes the margin between the best candidate and the rest of the potential antecedents according to a loss function. It has been used for tasks with similar characteristics, such as statistical machine transla- tion ( <ref type="bibr" target="#b21">Watanabe et al., 2007)</ref>. The training algorithm begins with a random ini- tialization of the weight vector w. The training set contains triggers, each trigger's candidate an- tecedents, and their gold standard antecedents; it is reshuffled after each training epoch. We find the K highest-scoring potential antecedents, a 1 , . . . , a k , according to the current weight value. A learn- ing rate parameter determines how much we retain the new weight update with respect to the previous weight vector values.</p><p>MIRA defines the update step of the standard on- line training algorithm: it seeks to learn a weight vector that, when multiplied with a feature vector f i , gives the highest score to the antecedent that is most similar to the gold standard antecedent, a * . This is posed as an optimization problem:</p><formula xml:id="formula_0">minimize w i w i − w i−1 + C K k ξ k subject to w i · a * − w i · a k + ξ k ≥ L(a * , a k ), k = 1, . . . , K</formula><p>(1) Here, L is the loss function that controls the mar- gin between candidates and the gold standard; it is defined as the evaluation metric proposed by Bos and Spenader (2011) (described in Section 5.5).</p><p>The ξ are slack variables and C ≥ 0 is a hyper- parameter that controls the acceptable margin. This problem is solved by converting it to its Lagrange dual form 6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Baseline Algorithm</head><p>The baseline we created was motivated by Bos's (2012) baseline algorithm: given a trigger, return as the antecedent the nearest VP that does not include the trigger. This is a na¨ıvena¨ıve approach to antecedent identification because it does not consider the re- lationship between the context surrounding the an- tecedent and the context surrounding the trigger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Experiments</head><p>We evaluate our results following the proposed met- rics of Bos and Spenader (2011), as do <ref type="bibr" target="#b13">Liu et al. (2016)</ref>. Accuracy for antecedent identification is computed according to n = the number of correctly identified tokens between the candidate antecedent and the gold standard antecedent. Precision is n divided by the length of the candidate antecedent, recall is n divided by the length of the correct an- tecedent, and accuracy is the harmonic mean of pre- cision and recall. For MIRA, final results are deter- mined by choosing the weight vector that achieved the best performance on a validation set that is split off from part of the training set, as calculated after each update step.    MIRA has several hyper-parameters that were tuned through a grid search over the validation set. The most crucial parameters were the learning rate α, and C, while the value of K did not cause signif- icant changes in accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Auxiliary Baseline MIRA Change</head><p>Results. In <ref type="table" target="#tab_5">Table 4</ref>, we see that MIRA improves upon the baseline with a 29% increase in overall ac- curacy. MIRA provides significant gains for each form of VPE, although there is room for improve- ment, especially when identifying the antecedents of do-so triggers. <ref type="bibr" target="#b13">Liu et al. (2016)</ref> achieve an accuracy of 65.20% with their joint resolution model for antecedent iden- tification when using the train-test split proposed by <ref type="bibr" target="#b2">Bos and Spenader (2011)</ref>; our model achieves 62.20% accuracy. However, their experimental de- sign was slightly different than ours -they only considered antecedents of triggers detected by their oracle trigger detection method, while we use all gold-standard triggers, meaning our results are not directly comparable to theirs. Our cross validated results (65.18% accuracy) paint a better picture of the quality of our model because the small size of the dataset (554 samples) can cause highly varied results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Excluded</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P R F1</head><p>Auxiliary 0.7982 0.7611 0.7781 Lexical 0.6937 0.8408 0.7582 Syntactic 0.7404 0.7330 0.7343 NONE 0.8242 0.8120 0.8170 In <ref type="table" target="#tab_6">Table 5</ref> we present end-to-end results obtained from our system when using the triggers detected by our VPE detection model (see Section 4). We com- pare these results to the end-to-end results of the best model of <ref type="bibr" target="#b13">Liu et al. (2016)</ref>. Following Liu et al., we assign partial credit during end-to-end evaluation in the following way: for each correctly detected (true positive) trigger, the Bos and Spenader (2011) an- tecedent evaluation score between the trigger's pre- dicted antecedent and its gold antecedent is used (as opposed to a value of 1). As can be seen from <ref type="table" target="#tab_6">Table  5</ref>, we trade about 6 points of precision for 14 points of recall, thus improving state-of-the-art end-to-end accuracy from 47.51% to 51.96%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Feature Ablation Studies</head><p>We performed feature ablation experiments in order to determine the impact that the different feature sets had on performance.</p><p>Trigger Detection. In <ref type="table" target="#tab_7">Table 6</ref> we can see that the syntactic features were essential for obtaining the best results, as can be seen by the 8.3% improve- ment, from 73.4% to 81.7%, obtained from includ- ing these features. This shows that notions from the- oretical linguistics can prove to be invaluable when approaching the problem of VPE detection and that extracting these features in related problems may improve performance.</p><p>Antecedent Identification. <ref type="table" target="#tab_9">Table 7</ref> presents the results from a feature ablation study on antecedent identification. The most striking observation is that the alignment features do not add any significant im- provement in the results. This is either because there simply is not an inherent parallelism between the   trigger site and the antecedent site, or because the other features represent the parallelism adequately without necessitating the addition of the alignment features. The heuristic syntactic features provide a large (10%) accuracy improvement when included. These results show that a dependency-based align- ment approach to feature extraction does not rep- resent the parallelism between the trigger and an- tecedent as well as features based on the lexical and syntactic properties of the two.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Features Excluded Accuracy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>We presented an approach for the tasks of Verb Phrase Ellipsis detection and antecedent identifica- tion that leverages features informed both by the- oretical linguistics and NLP, and employs machine learning methods to build VPE detection and an- tecedent identification tools using these features.</p><p>Our results show the importance of distinguishing VPE triggers from each other, and highlight the im- portance of using the notion of c-command for both tasks.</p><p>For VPE detection, we improve upon the accu- racy of the state-of-the-art system by over 11%, from 69.52% to 80.78%. For antecedent identification, our results significantly improve upon a baseline al- gorithm and we present results that are competitive with the state-of-the-art, as well as state-of-the-art results for an end-to-end system. We also expand the scope of previous state-of-the-art by including the detection and resolution of to-VPE, thus building a system that encompasses the entirety of the Bos and Spenader (2011) VPE dataset.</p><p>In future work, we would like to further inves-tigate other margin-based optimizations similar to MIRA, but perhaps even more resilient to over- fitting. We also seek to improve the antecedent iden- tification approach by extracting stronger features.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of the VPE resolution pipeline on an example found in WSJ file wsj 0036.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Alignment algorithm example with simplified dependencies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Alignment</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>VPE detection results (baseline F1, Machine Learning 

F1, ML F1 improvement) obtained with 5-fold cross validation. 

Test Set Results 
P 
R 
F1 

Liu et al. (2016) 0.8022 0.6134 0.6952 
This work 
0.7574 0.8655 0.8078 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Results (precision, recall, F1) for VPE detection using 

the train-test split proposed by Bos and Spenader (2011). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Results (baseline accuracy, MIRA accuracy, accuracy 

improvement) for antecedent identification; obtained with 5-

fold cross validation. 

End-to-end Results 
P 
R 
F1 

Liu et al. (2016) 
0.5482 0.4192 0.4751 
This work 
0.4871 0.5567 0.5196 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>End-to-end results (precision, recall, F1) using the 

train-test split proposed by Bos and Spenader (2011). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Feature ablation results (feature set excluded, preci-

sion, recall, F1) on VPE detection; obtained with 5-fold cross 

validation. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Feature ablation results (feature set excluded, preci-

sion, recall, F1) on antecedent identification; obtained with 5-

fold cross validation. 

</table></figure>

			<note place="foot" n="1"> e.g., the resolution of the example in Figure 1 would be &quot;The government includes money spent on residential renovation; Dodge does not [include money spent on residential renovation]&quot;. We did not pursue this final step due to the lack of a complete dataset that explicitly depicts the correct grammatical resolution of the VPE.</note>

			<note place="foot" n="2"> An anonymous reviewer recommended that further experiments could be performed by using the more informative NPs created with NML nodes (Vadas and Curran, 2007) on the goldstandard parsed WSJ. 3 For example, &quot;John will go to the store and Mary will do the same/likewise/the opposite&quot;. Do X anaphora and modals are not technically auxiliary verbs, as noted by the annotators of our dataset (Bos and Spenader, 2011), but for the purposes of this study we generalize them all as auxiliaries while simultaneously dividing them into their correct lexical categories.</note>

			<note place="foot" n="4"> A word A c-commands another word B if A&apos;s nearest branching ancestor in the parse tree is an ancestor of B, following the definition of Carnie (2013). We use this term purely to define a syntactic relation between two points in a parse tree. 5 A word A and word B share a local structure if they have the same closest S-node ancestor in the parse tree.</note>

			<note place="foot" n="6"> In this study, the dual form was implemented by hand using Gurobi&apos;s python API (Gurobi Optimization Inc., 2015).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was funded by McGill University and the Natural Sciences and Engineering Research Council of Canada via a Summer Undergraduate Research Project award granted to the first author. We thank the anonymous reviewers for their helpful sugges-tions, and we thank Nielsen, Hector Liu, and Edgar Gonzàlez for their clarifying remarks over email.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The stages of event extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Annotating and Reasoning about Time and Events</title>
		<meeting>the Workshop on Annotating and Reasoning about Time and Events</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised event coreference resolution with rich linguistic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Cosmin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanda</forename><surname>Bejan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harabagiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1412" to="1422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">An annotated corpus for the analysis of VP ellipsis. Language Resources and Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Spenader</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="463" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Robust VP ellipsis resolution in DR theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">From Quantification to Conversation</title>
		<imprint>
			<publisher>College Publications</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="145" to="159" />
		</imprint>
	</monogr>
	<note>Staffan Larsson and Lars Borin</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Syntax: A generative introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Carnie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Shai ShalevShwartz, and Yoram Singer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofer</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Keshet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="551" to="585" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>Online passiveaggressive algorithms</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ellipsis and higher-order unification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><surname>Dalrymple</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando Cn</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linguistics and Philosophy</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="399" to="452" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Gurobi Optimization Inc. 2015. Gurobi optimizer reference manual</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An algorithm for VP ellipsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 30th Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="9" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An empirical approach to VP ellipsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="525" to="541" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Overview of BioNLP &apos;09 shared task on event extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Dong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoko</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshinobu</forename><surname>Kano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing: Shared Task</title>
		<meeting>the Workshop on Current Trends in Biomedical Natural Language Processing: Shared Task</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Joint entity and event coreference resolution across documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heeyoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="489" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Supervised within-document event coreference using information propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Araki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eduard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teruko</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4539" to="4544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Exploring the steps of verb phrase ellipsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<idno>co-located with NAACL 2016</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Coreference Resolution Beyond OntoNotes (CORBON 2016)</title>
		<meeting>the Workshop on Coreference Resolution Beyond OntoNotes (CORBON 2016)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="32" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL) System Demonstrations</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Online large-margin training of dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="91" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A Corpus-Based Study of Verb Phrase Ellipsis Identification and Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leif</forename><forename type="middle">Arda</forename><surname>Nielsen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<pubPlace>King&apos;s College London</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Open domain event extraction from twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 18th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1104" to="1112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Deletion and logical form</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sag</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1976" />
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adding noun phrase structure to the penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vadas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Curran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting-Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">240</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Online large-margin training for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taro</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajime</forename><surname>Tsukada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Isozaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
