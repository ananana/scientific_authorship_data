<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantic Parsing to Probabilistic Programs for Situated Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Krishnamurthy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Allen Institute for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Allen Institute for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Allen Institute for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Semantic Parsing to Probabilistic Programs for Situated Question Answering</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="160" to="170"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Situated question answering is the problem of answering questions about an environment such as an image or diagram. This problem requires jointly interpreting a question and an environment using background knowledge to select the correct answer. We present Parsing to Probabilistic Programs (P 3), a novel situated question answering model that can use background knowledge and global features of the question/environment interpretation while retaining efficient approximate inference. Our key insight is to treat semantic parses as prob-abilistic programs that execute nondetermin-istically and whose possible executions represent environmental uncertainty. We evaluate our approach on a new, publicly-released data set of 5000 science diagram questions, outper-forming several competitive classical and neu-ral baselines.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Situated question answering is a challenging prob- lem that requires reasoning about uncertain inter- pretations of both a question and an environment together with background knowledge to determine the answer. To illustrate these challenges, consider the 8th grade science diagram questions in <ref type="figure" target="#fig_0">Figure 1</ref>, which are motivated by the Aristo project <ref type="bibr" target="#b9">(Clark and Etzioni, 2016)</ref>. These questions require both com- puter vision to interpret the diagram and composi- tional question understanding. These components, being imperfect, introduce uncertainty that must be jointly reasoned about to avoid implausible interpre- tations. These uncertain interpretations must further  be combined with background knowledge, such as the definition of a "predator," to determine the cor- rect answer.</p><p>The challenges of situated question answering have not been completely addressed by prior work. Early "possible worlds" models ( <ref type="bibr" target="#b26">Matuszek et al., 2012;</ref><ref type="bibr" target="#b18">Krishnamurthy and Kollar, 2013;</ref><ref type="bibr" target="#b24">Malinowski and Fritz, 2014</ref>) were capable of composi- tional question understanding and using background knowledge, but did not jointly reason about environ-ment/question uncertainty. These models also used unscalable inference algorithms for reasoning about the environment, despite the lack of joint reasoning. More recent neural models ( <ref type="bibr" target="#b2">Antol et al., 2015;</ref><ref type="bibr" target="#b25">Malinowski et al., 2015;</ref><ref type="bibr" target="#b34">Yang et al., 2015</ref>) are incapable of using background knowledge and it remains un- clear to what extent these models can represent com- positionality in language.</p><p>We present Parsing to Probabilistic Programs (P 3 ), a novel approach to situated question answer- ing that addresses these challenges. It is motivated by two observations: (1) situated question answer- ing can be formulated as semantic parsing with an execution model that is a learned function of the environment, and (2) probabilistic programming is a natural and powerful method for specifying the space of permissible execution models and learning over it. In P 3 , we define a domain theory for the task as a probabilistic program, then train a joint loglin- ear model to semantically parse questions to logi- cal forms in this theory and execute them in an en- vironment. Importantly, the model includes global features over parsing and execution that enable it to avoid unlikely joint configurations. P 3 lever- ages semantic parsing to represent compositionality in language and probabilistic programming to spec- ify background knowledge and perform linear-time approximate inference over the environment.</p><p>We present an experimental evaluation of P 3 on a new data set of 5000 food web diagram questions ( <ref type="figure" target="#fig_0">Figure 1</ref>). We compare our approach to several baselines, including possible worlds and neural net- work approaches, finding that P 3 outperforms both. An ablation study demonstrates that global features help the model achieve high accuracy. We also demonstrate that P 3 improves accuracy on a previ- ously published data set. Finally, we have released our data and code to facilitate further research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Prior Work</head><p>Situated question answering is often formulated in terms of parsing both the question and environment into a common meaning representation where they can be combined to select the answer. This gen- eral approach has been implemented using different meaning representations:</p><p>Possible world models use a logical meaning representation defined by a knowledge base schema. These models train a semantic parser to map ques- tions to queries and an environment model to map environments to knowledge bases in this schema. Executing the queries against the knowledge bases produces answers. These models assume that the parser and environment model are independent and furthermore that the knowledge base consists of independent predicate instances ( <ref type="bibr" target="#b26">Matuszek et al., 2012;</ref><ref type="bibr" target="#b18">Krishnamurthy and Kollar, 2013;</ref><ref type="bibr" target="#b24">Malinowski and Fritz, 2014)</ref>. Despite these strong independence assumptions, these models have intractable infer- ence. An exception is <ref type="bibr" target="#b30">Seo et al., (2015)</ref> who in- corporate hard constraints on the joint question/envi- ronment interpretation; however, this approach does not generalize to soft constraints or arbitrary logical forms. In some work only the environment model is learned ( <ref type="bibr" target="#b17">Kollar et al., 2010;</ref><ref type="bibr" target="#b32">Tellex et al., 2011;</ref><ref type="bibr" target="#b15">Howard et al., 2014b;</ref><ref type="bibr" target="#b14">Howard et al., 2014a;</ref><ref type="bibr" target="#b5">Berant et al., 2014;</ref><ref type="bibr" target="#b20">Krishnamurthy and Mitchell, 2015)</ref>. Neural networks use a vector meaning repre- sentation that encodes both the question and envi- ronment as vectors. These networks have mostly been applied to visual question answering ( <ref type="bibr" target="#b2">Antol et al., 2015)</ref>, where many architectures have been pro- posed ( <ref type="bibr" target="#b25">Malinowski et al., 2015;</ref><ref type="bibr" target="#b34">Yang et al., 2015;</ref><ref type="bibr" target="#b10">Fukui et al., 2016)</ref>. It is unclear to what extent these networks can represent compositionality in language using their vector encodings. Dynamic Neural Mod- ule Networks ( <ref type="bibr" target="#b0">Andreas et al., 2016a;</ref><ref type="bibr" target="#b1">Andreas et al., 2016b</ref>) are the exception to the above generaliza- tion. This approach constructs a neural network to represent the meaning of the question via semantic parsing, then executes this network against the im- age to produce an answer. Our approach is similar except that we construct and execute a probabilistic program. Advantages of our approach are that it nat- urally represents the discrete structure of food webs and can use background knowledge.</p><p>Preliminaries for our work are semantic pars- ing and probabilistic programming. Semantic pars- ing translates natural language questions into exe- cutable logical forms and has been used in applica- tions such as question answering against a knowl- edge base <ref type="bibr" target="#b36">(Zelle and Mooney, 1993;</ref><ref type="bibr" target="#b37">Zettlemoyer and Collins, 2005;</ref><ref type="bibr" target="#b23">Liang et al., 2011;</ref><ref type="bibr" target="#b22">Kwiatkowski et al., 2013;</ref><ref type="bibr" target="#b4">Berant et al., 2013;</ref><ref type="bibr" target="#b29">Reddy et al., 2014;</ref><ref type="bibr" target="#b35">Yih et al., 2015;</ref><ref type="bibr" target="#b33">Xu et al., 2016)</ref>, direction following <ref type="bibr" target="#b6">(Chen and Mooney, 2011;</ref>, and information extraction ( <ref type="bibr" target="#b19">Krishnamurthy and Mitchell, 2012;</ref><ref type="bibr" target="#b7">Choi et al., 2015)</ref>. Semantic parsing alone is insufficient for situated question an- swering because it does not interpret the environ- ment; many of the above approaches use semantic parsing as a component in a larger model. Probabilistic programming languages extend pro- gramming languages with primitives for nondeter- ministic choice <ref type="bibr" target="#b27">(McCarthy, 1963;</ref><ref type="bibr" target="#b11">Goodman and Stuhlmüller, 2014</ref>). We express logical forms in a probabilistic variant of Scheme similar to <ref type="bibr">Church (Goodman et al., 2008)</ref>; however, this paper uses Python-like pseudocode for clarity. The lan- guage has a single choice primitive called choose that nondeterministically returns one of its argu- ments. For example choose <ref type="bibr">(1,</ref><ref type="bibr">2,</ref><ref type="bibr">3)</ref> can execute three ways, returning either 1, 2, or 3. Multiple calls to choose can be combined. For example, choose(1,2)+choose(1,2) adds two nondeter- ministically chosen values, and therefore has four executions that return 2, 3, 3 and 4. Each execution also has a probability; in our case, these probabili- ties are assigned by a trained model given the envi- ronment and not explicitly specified in the program.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Parsing to Probabilistic Programs (P )</head><p>The P 3 model is motivated by two observations. The first is that situated question answering can be formulated as semantic parsing with an execution model that is a learned function of the environment. Consider the first question in <ref type="figure" target="#fig_0">Figure 1</ref>. The meaning of this question could be represented by a logical form such as <ref type="bibr">COUNT(λx.EATS(x, DEER)</ref>), which we could train a semantic parser to predict given a suitable domain theory of functions such as COUNT and EATS. However, the information required to execute this logical form and answer the question must be extracted from the diagram. Specifically, EATS(x, y) depends on whether an arrow is present between x and y, which we must train a vision model to determine. Thus, EATS should be a learned function of the environment.</p><p>This first observation suggests a need for a for- malism for representing uncertainty and performing learning over the domain theory's functions. Our second observation is that probabilistic program- ming is a natural fit for this task. In this paradigm, the domain theory is a probabilistic program that de- fines the information to be extracted from the envi- ronment by using choose. To a first approximation, the diagram question theory includes: def eats(x, y) choose(true, false)</p><p>Logical forms are then probabilistic programs, each of whose possible executions represents a dif- ferent interpretation of the environment. For exam- ple, executing EATS(LION, DEER) hits the choose in the above definition, resulting in two executions where the lion either eats or does not eat the deer. In the COUNT example above, each execution rep- resents a different set of animals that eat the deer. To learn the correct environment interpretation, we train an execution model to assign a probability to each execution given features of the environment. Using probabilistic programming enables us to com- bine learned functions, such as EATS, with back- ground knowledge functions, such as COUNT, and also facilitates inference.</p><p>According to these observations, applying P 3 has two steps. The first step is to define an appropriate domain theory. This theory is the main design de- cision in instantiating P 3 and provides a powerful way to encode domain knowledge. The second step is to train a loglinear model consisting of a semantic parser and an execution model. This model learns to semantically parse questions into logical forms in the theory and execute them in the environment to answer questions correctly. We defer discussion of the diagram question domain theory to Section 4 and focus on the loglinear model in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Overview</head><p>The input to the P 3 model is a question and an en- vironment and its output is a denotation, which is a formal answer to the question. P 3 is a loglinear model with two factors: a semantic parser and an ex- ecution model. The semantic parser scores syntac- tic parses and logical forms for the question. These logical forms are probabilistic programs with mul- tiple possible executions (specified by the domain theory), each of which may return a different denota- tion. The execution model assigns a score to each of these executions given the environment. Formally, the model predicts a denotation γ for a question q in an environment v using three latent variables:</p><formula xml:id="formula_0">if S/N/S : λx.λy.λf. CAUSE(x, f (y)) mice N : MICE die S\N : λx.DECREASE(x) S : DECREASE(MICE) S/N : λy.λf.CAUSE(DECREASE(MICE), f (y)) snakes N :</formula><formula xml:id="formula_1">P (γ|v, q; θ) = e,,,t P (e, , t|v, q; θ)1(ret(e) = γ) P (e, , t|v, q; θ) = 1 Z q,v f ex (e, , v; θ ex )f p (, t, q; θ p )</formula><p>The model is composed of two factors. f p repre- sents the semantic parser that scores logical forms and syntactic parse trees t given question q and parameters θ p . f ex represents the execution model. Given parameters θ ex , this factor assigns a score to a logical form and its execution e in environment v. The denotation γ, i.e., the formal answer to the ques- tion, is simply the value returned by e. Z q,v repre- sents the model's partition function. The following sections describe these factors in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Semantic Parser</head><p>The factor f p represents a Combinatory Categorial Grammar (CCG) semantic parser <ref type="bibr" target="#b37">(Zettlemoyer and Collins, 2005</ref>) that scores logical forms for a ques- tion. Given a lexicon 1 mapping words to syntactic categories and logical forms, CCG defines a set of possible syntactic parses t and logical forms for a question q. <ref type="figure" target="#fig_4">Figure 3</ref>.2 shows an example CCG parse. f p is a loglinear model over parses (, t): f p (, t, q; θ p ) = exp{θ T p φ(, t, q)} The function φ maps parses to feature vectors. We use a rich set of features similar to those for syn- tactic CCG parsing <ref type="bibr" target="#b8">(Clark and Curran, 2007)</ref>; a full description is provided in an online appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Execution Model</head><p>The factor f ex is a loglinear model over the execu- tions of a logical form given an environment. Log- ical forms in P 3 are probabilistic programs with a set of possible executions, where each execution e is a sequence, e = [e 0 , e 1 , e 2 , ..., e n ]. e 0 is the pro- gram's starting state, e i represents the state immedi- ately after the ith call to choose, and e n is the state at termination. The score of an execution is:</p><formula xml:id="formula_2">f ex (e, , v; θ ex ) = n i=1 exp{θ T ex φ(e i−1 , e i , , v)}</formula><p>In the above equation, θ ex represents the model's parameters and φ represents a feature function that produces a feature vector for the difference between sequential program states e i−1 and e i given environ- ment v and logical form . φ can include arbitrary features of the execution, logical form and environ- ment, which is important, for example, to detect cy- cles in a food web (Section 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Inference</head><p>P 3 is designed to rely on approximate inference: our goal is to use rich features to accurately make local decisions, as in linear-time parsers <ref type="bibr" target="#b28">(Nivre et al., 2006</ref>). We perform approximate inference us- ing a two-stage beam search. Given a question q, the first stage performs a beam search over CCG parses to produce a list of logical forms scored by f p . This step is performed by using a CKY-style chart parsing algorithm then marginalizing out the syntactic parses. The second stage performs a beam search over executions of each logical form. The space of possible executions of a logical form is a tree <ref type="figure" target="#fig_6">(Figure 4</ref>.2) where each internal node rep- resents a partial execution up to a choose call. The search maintains a beam of partial executions at the same depth, and each iteration advances the beam to the next depth, discarding the lowest- scoring executions according to f ex to maintain a fixed size beam. This procedure runs in time linear to the number of choose calls. We implement the search by rewriting the probabilistic program into continuation-passing style, which allows choose to be implemented as a function that adds multiple con- tinuations to the search queue; we refer the reader to <ref type="bibr" target="#b11">Goodman and Stuhlmüller (2014)</ref> for details. Our experiments use a beam size of 100 in the seman- tic parser, executing each of the 10 highest-scoring logical forms with a beam of 100 executions. 3 is trained by maximizing loglikelihood with stochastic gradient ascent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Training</head><p>The training data</p><formula xml:id="formula_3">{(q i , v i , c i )} n i=1</formula><p>is a collection of questions q i and environments v i paired with supervision oracles c</p><note type="other">i . c i (e) = 1 for a correct execution e and c i (e) = 0 otherwise. The oracle c i can implement various kinds of supervision, including: (1) labeled denota- tions, by verifying the value returned by e and (2) la- beled environments, by verifying each choice made by e. The oracle for diagram question answering combines both forms of supervision (Section 4.5).</note><p>The objective function O is the loglikelihood of predicting a correct execution:</p><formula xml:id="formula_4">O(θ) = n i=1 log e,l,t c i (e)P (e, , t|q i , v i ; θ)</formula><p>We optimize this objective function using stochastic gradient ascent, using the approximate in- ference algorithm from Section 3.4 to estimate the necessary marginals. When computing the marginal distribution over correct executions, we filter each step of the beam search using the supervision oracle c i to improve the approximation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Diagram Question Answering with P 3</head><p>As a case study, we apply P 3 to the task of answer- ing food web diagram questions from an 8th grade science domain. A few steps are required to apply P 3 . First, we create a domain theory of food webs that represents extracted information from the dia- gram and background knowledge for the domain. Second, we define the features of the execution model that are used to learn how programs in the domain theory execute given a diagram. Third, we define a component to select a multiple-choice an- swer given a denotation. Finally, we define the su- pervision oracle used for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Food Web Diagram Questions</head><p>We consider the task of answering food web diagram questions. The input consists of a diagram depicting a food web, a natural language question and a list of natural language answer options <ref type="figure" target="#fig_0">(Figure 1</ref>). The goal is to select the correct answer option. This task has many regularities that require global features: for example, food webs are usually acyclic and cer- tain animals usually have certain roles (e.g., mice are herbivores). We have collected and released a data set for this task (Section 5.1).</p><p>We preprocess the diagrams in the data set us- ing a computer vision system that identifies can- didate diagram elements ( <ref type="bibr" target="#b16">Kembhavi et al., 2016)</ref>. This system extracts a collection of text labels (via OCR), arrows, arrowheads and objects, each with corresponding scores. It also extracts a collection of scored linkages between these elements. These ex- tractions are noisy and contain many discrepancies such as overlapping text labels and spurious link- ages. We use these extractions to define a set of can- didate organisms (using the text labels), and also to define features of the execution model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Domain Theory</head><p>The domain theory is a probabilistic program encod- ing the information to extract from the environment as well as background knowledge about food webs. It represents the structure of a food web using two functions. These functions are predicates that invoke choose to return either true or false. The execution model learns to predict which of these values is cor- rect for each set of arguments given the diagram. It furthermore has a collection of deterministic func- tions that encode domain knowledge, including def- initions of animal roles such as HERBIVORE and a model of population change causation. <ref type="figure" target="#fig_6">Figure 4</ref>.2 shows pseudocode for a portion of the domain theory. Food webs are represented using two functions over the extracted text la- bels: ORGANISM(x) indicates whether the label x is an organism (as opposed to, e.g., the dia- gram title); and EATS(x, y). The definitions of these functions invoke choose while remembering previously chosen values to avoid double counting probabilities when executing logical forms such as ORGANISM(DEER) ∧ ORGANISM(DEER). The re- membered values are stored in a global variable that is also used to implement the supervision oracle. Deterministic functions such as CAUSE are defined in terms of these learned functions.</p><p>The uses of choose in the domain theory create a tree of possible executions for every logical form. <ref type="figure" target="#fig_6">Figure 4</ref>.2 illustrates this tree for the logical form λf.CAUSE(DECREASE(MICE), f (SNAKES)), which # initialize predicate instance variables # from text labels in environment world = {"mice": undef, ("mice", "snakes"): undef, ...} def organism(name) if (world[name] == undef) world[name] = choose(true, false) return world <ref type="bibr">[name]</ref> def eats(x, y) # same as organism but with pairs.</p><p># entities referenced in the logical form # must be organisms. choose() represents # failure; it returns no values. corresponds to the question "what happens to the snakes when the mice decrease?" This logical form is shorthand for the following program:</p><p>filter(lambda f.cause( decrease(getOrganism("mice")), f(getOrganism("snakes"))), set(decrease, increase, unchanged)) Specifically, entities such as MICE are created by calling getOrganism and logical forms with func- tional types implicitly represent filters over the ap- propriate argument type. Executing this program first applies the filter predicate to decrease. Next, it evaluates getOrganism("mice"), which calls organism and encounters the first call to choose. This call is shown as the first branch of the tree in <ref type="figure" target="#fig_6">Figure 4</ref>.2. The successful branch proceeds to eval- uate getOrganism("snakes"), shown as the sec- ond branch. Finally, the successful branch evaluates cause, which calls eats twice, resulting in the final two branches. The value returned by each branch is determined by the causation model which performs  some deterministic logic on the truth values of the two eats relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Execution Features</head><p>The execution model uses three sets of features: in- stance features, predicate features, and denotation features. Instance features treat each predicate in- stance independently, while the remainder are global features of multiple predicate instances and the log- ical form. We provide a complete listing of features in an online appendix. Instance features fire whenever an execution chooses a truth value for a predicate instance. These features are similar to the per-predicate-instance fea- tures used in prior work to produce a distribution over possible worlds. For ORGANISM(x), our fea- tures are the vision model's extraction score for x and indicator features for the number of tokens in x. For EATS(x, y), our features are various combi- nations of the vision model's scores for arrows that may connect the text labels x and y.</p><p>Predicate features fire based on the global as- signment of truth values to all instances of a single predicate. The features for ORGANISM count oc- currences of overlapping text labels among true in- stances. The features for EATS include cycle count features for various cycle lengths and arrow reuse features. The cycle count features help the model learn that food webs are typically, but not always, acyclic and the arrow reuse features aim to prevent the model from predicting two different EATS in- stances on the basis of a single arrow.</p><p>Denotation features fire on the return value of an execution. There are two kinds of denotation fea- tures: size features that count the number of entities in denotations of various types and denotation ele- ment features for specific logical forms. The sec- ond kind of feature can be used to learn that the de- notation of λx.HERBIVORE(x) is likely to contain MOUSE, but unlikely to contain WOLF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Answer Selection</head><p>P 3 predicts a distribution over denotations for each question, which for our problem must be mapped to a distribution over multiple choice answers. An- swer selection performs this task using string match heuristics and an LSTM (Hochreiter and Schmidhu- ber, 1997). The string match heuristics score each answer option given a denotation then select the highest scoring answer, abstaining in the case of a tie. The score computation depends on the denota- tion's type. If the denotation is a set of entities, the score is an approximate count of the number of enti- ties in the denotation mentioned in the answer using a fuzzy string match. If the denotation is a set of change events, the score is a fuzzy match of both the change direction and the animal name. If the denota- tion is a number, string matching is straightforward. Applying these heuristics and marginalizing out de- notations yields a distribution over answer options.</p><p>A limitation of the above approach is that it does not directly incorporate linguistic prior knowledge about likely answers. For example, "snake" is usu- ally a good answer to "what eats mice?" regardless of the diagram. Such knowledge is known to be es- sential for visual question answering ( <ref type="bibr" target="#b2">Antol et al., 2015;</ref><ref type="bibr" target="#b1">Andreas et al., 2016b</ref>) and important in our task as well. We incorporate this knowledge in a standard way, by training a neural network on ques- tion/answer pairs (without the diagram) and combin- ing its predictions with the string match heuristics above. The network is a sequence LSTM that is ap- plied to the question concatenated with each answer option a to produce a 50-dimensional vector v a for each answer. The distribution over answers is the softmax of the inner product of these vectors with a learned parameter vector w. For simplicity, we combine these two components using a 50/50 mix of their answer distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Supervision Oracle</head><p>The supervision oracle for diagram question answer- ing combines supervision of both answers and envi- ronment interpretations. We assume that each dia- gram has been labeled with a food web. An exe- cution is correct if and only if (1) all of the chosen values in the global variable encoding the food web are consistent with the labeled food web, and (2) string match answer selection applied to its denota- tion chooses the correct answer. The first constraint guarantees that every logical form has at most one correct execution for any given diagram.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>Our evaluation compares P 3 to both possible worlds and neural network approaches on our data set of food web diagram questions. An ablation study demonstrates that both sets of global features im- prove accuracy. Finally, we demonstrate P 3 's gen- erality by applying it to a previously-published data set, obtaining state-of-the-art results.</p><p>Code, data and supplementary material for this paper are available at: http://www.allenai. org/paper-appendix/emnlp2016-p3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">FOODWEBS Data Set</head><p>FOODWEBS consists of ∼500 food web diagrams and ∼5000 questions designed to imitate actual questions encountered on 8th grade science exams. The train/validation/test sets contain ∼300/100/100 diagrams and their corresponding questions. The data set has three kinds of annotations in addition to the correct answer for each question. First, each di- agram is annotated with the food web that it depicts using ORGANISM and EATS. Second, each diagram has predictions from a vision system for various dia- gram elements such as arrows and text labels <ref type="bibr" target="#b16">(Kembhavi et al., 2016</ref>). These are noisy predictions, not ground truth. Finally, each question is annotated by the authors with a logical form (or null if its mean- ing is not representable in the domain theory). These logical forms are not used to train P 3 but are useful to measure per-component error.</p><p>We collected FOODWEBS by using a crowdsourc- ing process to expand a collection of real exam ques- tions. First, we collected 89 questions from 4th and 8th grade exams and 500 food web diagrams us-ing an image search engine. Second, we generated questions for these diagrams using Mechanical Turk. Workers were shown a diagram and a real question for inspiration and asked to write a new question and its answer options. We validated each gener- ated question by asking 3 workers to answer it, dis- carding questions where at least 2 did not choose the correct answer. We also manually corrected any am- biguous (e.g., two answer options are correct) and poorly-formatted (e.g., two answer options have the same letter) questions. The final data set has high quality: a human domain expert correctly answered 95 out of 100 randomly-sampled questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baseline Comparison</head><p>Our first experiment compares P 3 with several base- lines for situated question answering. The first base- line, WORLDS, is a possible worlds model based on <ref type="bibr" target="#b24">Malinowski and Fritz (2014)</ref>. This baseline learns a semantic parser P (, t|q) and a distribution over food webs P (w|v), then evaluates on w to pro- duce a distribution over denotations. This model is implemented by independently training P 3 's CCG parser (on question/answer pairs and labeled food webs) and a possible-worlds execution model (on la- beled food webs). The CCG lexicon for both P 3 and WORLDS was generated by applying PAL <ref type="bibr" target="#b21">(Krishnamurthy, 2016)</ref> to the same data. Both models select answers as described in Section 4.4.</p><p>We also compared P 3 to several neural network baselines. The first baseline, LSTM, is the text- only answer selection model described in Section 4.4. The second baseline, VQA, is a neural net- work for visual question answering. This model represents each image as a vector by using the fi- nal layer of a pre-trained VGG19 model <ref type="bibr" target="#b31">(Simonyan and Zisserman, 2014</ref>) and applying a single fully- connected layer. It scores answer options by using the answer selection LSTM to encode question/an- swer pairs, then computing a dot product between the text and image vectors. This model is somewhat limited because VGG features are unlikely to encode important diagram structure, such as the content of text labels. Our third baseline, DQA, is a neural net- work that rectifies this limitation ( <ref type="bibr" target="#b16">Kembhavi et al., 2016)</ref>. It encodes the diagram predictions from the vision system as vectors and attends to them using the LSTM-encoded question vector to select an an-  swer. This model is trained with question/answer pairs and diagram parses, which is roughly compa- rable to the supervision used to train P 3 . <ref type="table">Table 5</ref>.2 compares the accuracy of P 3 to these baselines. Accuracy is the fraction of questions an- swered correctly. LSTM performs well on this data set, suggesting that many questions can be answered without using the image. This result is consistent with results on visual question answering ( <ref type="bibr" target="#b2">Antol et al., 2015</ref>). The other neural network models have similar performance to LSTM, whereas both WORLDS and P 3 outperform it. We also find that P 3 outperforms WORLDS likely due to its global features, which we investigate in the next section.</p><p>Given these results, we hypothesized that the neu- ral models were largely memorizing common pat- terns in the text and were not able to interpret the diagram. We tested this hypothesis by running each model on a test set with unseen organisms created by reversing the organism names in every question and diagram <ref type="table">(Table 5</ref>.2, right column). As expected, the accuracy of LSTM is considerably reduced on this data set. VQA and DQA again perform similarly to LSTM, which is consistent with our hypothesis. In contrast, we find that the accuracies of WORLDS and P 3 are only slightly reduced, which is consistent with superior diagram interpretation abilities but in- effective LSTM answer selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Study</head><p>We performed an ablation study to further under- stand the impact of LSTM answer selection and global features. <ref type="table" target="#tab_1">Table 5.2</ref> shows the accuracy of P 3 trained without these components. We find that LSTM answer selection improves accuracy by 9 points, as expected due to the importance of linguis- tic prior knowledge. Global features improve accu- racy by 7 points, which is roughly comparable to the delta between P 3 and WORLDS in <ref type="table">Table 5</ref>.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Component Error Analysis</head><p>Our third experiment analyses sources of error by training and evaluating P 3 while providing the gold logical form, food web, or both as input. <ref type="table">Table  5</ref>.5 shows the accuracy of these three models. The final entry shows the maximum accuracy possible given our domain theory and answer selection. The larger accuracy improvement with gold food webs suggests that the execution model is responsible for more error than semantic parsing, though both com- ponents contribute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">SCENE Experiments</head><p>Our final experiment applies P 3 to the SCENE data set of <ref type="bibr" target="#b18">Krishnamurthy and Kollar (2013)</ref>. In this data set, the input is a natural language expression, such as "blue mug to the left of the monitor," and the output is the set of objects in an image that the ex- pression denotes. The images are annotated with a bounding box for each candidate object. The data set includes a domain theory that was automatically generated by creating a category and/or relation per word based on its part of speech. It also includes a CCG lexicon and image features. We use these re- sources, adding predicate and denotation features. <ref type="table">Table 5</ref>.5 compares P 3 to prior work on SCENE. The evaluation metric is exact match accuracy be- tween the predicted and labeled sets of objects. We consider three supervision conditions: QA trains with question/answer pairs, QA+E further includes labeled environments, and QA+E+LF further in- cludes labeled logical forms. We trained P 3 in the first two conditions, while prior work trained in the first and third conditions. KK2013 is a possible worlds model with a max-margin training objective. P 3 slightly outperforms in the QA condition and P 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Accuracy ∆ P 3 69.1 + gold logical form 75.1 +6.0 + gold food web 82.3 +13.2 + both 91.6 +22.5  trained with labeled environments outperforms prior work trained with additional logical form labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Parsing to Probabilistic Programs (P 3 ) is a novel model for situated question answering that jointly reasons about question and environment interpreta- tions using background knowledge to produce an- swers. P 3 uses a domain theory -a probabilistic program -to define the information to be extracted from the environment and background knowledge. A semantic parser maps questions to logical forms in this theory, which are probabilistic programs whose possible executions represent possible interpreta- tions of the environment. An execution model scores these executions given features of the environment. Both the semantic parser and execution model are jointly trained in a loglinear model, which thereby learns to both parse questions and interpret environ- ments. Importantly, the model includes global fea- tures of the logical form and executions, which help the model avoid implausible interpretations. We demonstrate P 3 on a challenging new data set of 5000 science diagram questions, where it outper- forms several competitive baselines.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 .</head><label>1</label><figDesc>According to the given food chain, what is the num- ber of organisms that eat deer? (A) 3 (B) 2 (C) 4 (D) 1 2. Which organism is both predator and prey? (A) Bark Beetles (B) Insect-eating birds (C) Deer (D) Hawks 3. Based on the given food web, what would happen if there were no insect-eating birds? (A) The grasshop- per population would increase. (B) The grasshop- per population would decrease. (C) There would be no change in grasshopper number.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example food web questions. A food web depicts a collection of organisms in an ecosystem with an arrow from organism x to y indicating that y eats x. Questions may require counting (1), knowing animal roles (2) and reasoning about population changes (3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example CCG parse of a question as predicted by the semantic parser fp. The logical form for the question is shown on the bottom line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>P</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Domain theory pseudocode for diagram question answering.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Tree of possible executions for the logical form λf.CAUSE(DECREASE(MICE), f (SNAKES)). Each path from root to leaf represents a single execution that returns the indicated denotation or fails, and each internal node represents a nondeterministic choice made with choose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Test set accuracy of P 3 removing LSTM answer se-
lection (Section 4.4), denotation features and predicate features 
(Section 4.3). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Accuracy of P 3 when trained and evaluated with la-
beled logical forms, food webs, or both. 

Supervision 
Model 
QA QA+E QA+E+LF 

P 3 
68 
75 
-
KK2013 67 
-
70 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Accuracy on the SCENE data set. KK2013 results are 
from Krishnamurthy and Kollar (2013). 

</table></figure>

			<note place="foot" n="1"> In our experiments, we automatically learn the lexicon in a preprocessing step. See Section 5.2 for details.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We gratefully acknowledge Minjoon Seo, Mike Sal-vato and Eric Kolve for their implementation help, Isaac Cowhey and Carissa Schoenick for their help with the data, and Oren Etzioni, Peter Clark, Matt Gardner, Hannaneh Hajishirzi, Mike Lewis, and Jonghyun Choi for their comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep compositional question answering with neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to compose neural networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">VQA: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of semantic parsers for mapping instructions to actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic parsing on Freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Modeling biological processes for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Srikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abby</forename><surname>Vander Linden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brittany</forename><surname>Harding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brad</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to interpret natural language navigation instructions from observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 25th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Scalable semantic parsing with partial ontologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Association for Computational Linguistics</title>
		<meeting>the 2015 Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Widecoverage efficient statistical parsing with CCG and log-linear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="493" to="552" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">My computer is an honor student-but how intelligent is it? standardized tests as a measure of ai</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="5" to="12" />
			<pubPlace>AI Magazine</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daylen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01847</idno>
		<title level="m">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The Design and Implementation of Probabilistic Programming Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stuhlmüller</surname></persName>
		</author>
		<ptr target="http://dippl.org.Accessed" />
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2016" to="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Church: A language for generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vikash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Mansinghka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Bonawitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient natural language interfaces for assistive robots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">M</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Istvan</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oron</forename><surname>Propp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) Workshop on Rehabilitation and Assistive Robotics</title>
		<imprint>
			<date type="published" when="2014-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A natural language planner interface for mobile manipulators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Thomas M Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Tellex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A diagram is worth a dozen images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Salvato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Kolve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><forename type="middle">Joon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Toward understanding natural language directions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Tellex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deb</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th ACM/IEEE International Conference on Human-Robot Interaction</title>
		<meeting>the 5th ACM/IEEE International Conference on Human-Robot Interaction</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Jointly learning to parse and perceive: Connecting natural language to the physical world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association of Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Weakly supervised training of semantic parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning a compositional semantics for freebase with an open predicate vocabulary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="257" to="270" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Probabilistic models for learning a semantic parser lexicon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Scaling semantic parsers with on-the-fly ontology matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning dependency-based compositional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A multiworld approach to question answering about realworld scenes based on uncertain input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ask your neurons: A neural-based approach to 169 answering questions about images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A joint model of language and perception for grounded attribute learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Matuszek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine Learning</title>
		<meeting>the 29th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A basis for a mathematical theory of computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Mccarthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Programming and Formal Systems</title>
		<imprint>
			<date type="published" when="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Maltparser: A data-driven parser-generator for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Nilsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Large-scale semantic parsing without question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Solving geometry problems: Combining text and diagram interpretation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clint</forename><surname>Malcolm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Understanding natural language commands for robotic navigation and mobile manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Tellex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Dickerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashis</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seth</forename><surname>Teller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Question Answering on Freebase via Relation Extraction and Textual Evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics (ACL</title>
		<meeting>the Association for Computational Linguistics (ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02274</idno>
		<title level="m">Stacked attention networks for image question answering</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Semantic parsing via staged query graph generation: Question answering with knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning semantic grammars with constructive inductive logic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Zelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th National Conference on Artificial Intelligence</title>
		<meeting>the 11th National Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning to map sentences to logical form: structured classification with probabilistic categorial grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI &apos;05, Proceedings of the 21st Conference in Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
