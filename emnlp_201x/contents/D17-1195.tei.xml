<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:52+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dynamic Entity Representations in Neural Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science &amp; Engineering</orgName>
								<orgName type="department" key="dep2">Department of Computational Linguistics</orgName>
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Heidelberg University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhao</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science &amp; Engineering</orgName>
								<orgName type="department" key="dep2">Department of Computational Linguistics</orgName>
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Heidelberg University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Martschat</surname></persName>
							<email>martschat@cl.uni-heidelberg.de</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science &amp; Engineering</orgName>
								<orgName type="department" key="dep2">Department of Computational Linguistics</orgName>
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Heidelberg University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science &amp; Engineering</orgName>
								<orgName type="department" key="dep2">Department of Computational Linguistics</orgName>
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Heidelberg University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science &amp; Engineering</orgName>
								<orgName type="department" key="dep2">Department of Computational Linguistics</orgName>
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Heidelberg University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">G</forename><surname>Allen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science &amp; Engineering</orgName>
								<orgName type="department" key="dep2">Department of Computational Linguistics</orgName>
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Heidelberg University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Dynamic Entity Representations in Neural Language Models</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1830" to="1839"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Understanding a long document requires tracking how entities are introduced and evolve over time. We present a new type of language model, ENTITYNLM, that can explicitly model entities, dynamically update their representations, and contextu-ally generate their mentions. Our model is generative and flexible; it can model an arbitrary number of entities in context while generating each entity mention at an arbitrary length. In addition, it can be used for several different tasks such as language modeling, coreference resolution, and entity prediction. Experimental results with all these tasks demonstrate that our model consistently outperforms strong baselines and prior work.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Understanding a narrative requires keeping track of its participants over a long-term context. As a story unfolds, the information a reader associates with each character in a story increases, and ex- pectations about what will happen next change ac- cordingly. At present, models of natural language do not explicitly track entities; indeed, in today's language models, entities are no more than the words used to mention them.</p><p>In this paper, we endow a generative language model with the ability to build up a dynamic rep- resentation of each entity mentioned in the text. Our language model defines a probability distribu- tion over the whole text, with a distinct generative story for entity mentions. It explicitly groups those mentions that corefer and associates with each en- tity a continuous representation that is updated by every contextualized mention of the entity, and that in turn affects the text that follows.</p><p>[John] 1 wanted to go to <ref type="bibr">[</ref>  <ref type="figure">Figure 1</ref>: ENTITYNLM explicitly tracks entities in a text, including coreferring relationships be- tween entities like <ref type="bibr">[John]</ref> 1 and [He] <ref type="bibr">1</ref> . As a lan- guage model, it is designed to predict that a coref- erent of [the coffee shop] 2 is likely to follow "told that," that the referring expression will be "it", and that "sold the best beans" is likely to come next, by using entity information encoded in the dynamic distributed representation.</p><p>Our method builds on recent advances in repre- sentation learning, creating local probability dis- tributions from neural networks. It can be un- derstood as a recurrent neural network language model, augmented with random variables for en- tity mentions that capture coreference, and with dynamic representations of entities. We estimate the model's parameters from data that is annotated with entity mentions and coreference.</p><p>Because our model is generative, it can be queried in different ways. Marginalizing every- thing except the words, it can play the role of a lan- guage model. In §5.1, we find that it outperforms both a strong n-gram language model and a strong recurrent neural network language model on the English test set of the CoNLL 2012 shared task on coreference evaluation ( <ref type="bibr" target="#b28">Pradhan et al., 2012</ref>). The model can also identify entity mentions and coreference relationships among them. In §5.2, we show that it can easily be used to add a per- formance boost to a strong coreference resolution system, by reranking a list of k-best candidate out- puts. On the CoNLL 2012 shared task test set, the reranked outputs are significantly better than the original top choices from the same system. Fi-nally, the model can perform entity cloze tasks. As presented in §5.3, it achieves state-of-the-art performance on the InScript corpus ( <ref type="bibr" target="#b21">Modi et al., 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>A language model defines a distribution over se- quences of word tokens; let X t denote the random variable for the tth word in the sequence, x t de- note the value of X t and x t the distributed repre- sentation (embedding) of this word. Our starting point for language modeling is a recurrent neural network ( <ref type="bibr" target="#b20">Mikolov et al., 2010)</ref>, which defines</p><formula xml:id="formula_0">p(X t | history) = softmax (W h h t−1 + b) (1) h t−1 = LSTM(h t−2 , x t−1 )<label>(2)</label></formula><p>where W h and b are parameters of the model (along with word embeddings x t ), LSTM is the widely used recurrent function known as "long short-term memory" (Hochreiter and Schmidhu- ber, 1997), and h t is a LSTM hidden state encoding the history of the sequence up to the tth word. Great success has been reported for this model ( <ref type="bibr">Zaremba et al., 2015)</ref>, which posits nothing ex- plicitly about the words appearing in the text se- quence. Its generative story is simple: the value of each X t is randomly chosen conditioned on the vector h t−1 encoding its history.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Additional random variables and representations for entities</head><p>To introduce our model, we associate with each word an additional set of random variables. At po- sition t,</p><p>• R t is a binary random variable that indi- cates whether x t belongs to an entity men- tion (R t = 1) or not (R t = 0). Though not explored here, this is easily generalized to a categorial variable for the type of the entity (e.g., person, organization, etc.).</p><p>• L t ∈ {1, . . . , max } is a categorical random variable if R t = 1, which indicates the num- ber of remaining words in this mention, in- cluding the current word (i.e., L t = 1 for the last word in any mention). max is a predefined maximum length fixed to be 25, which is an empirical value derived from the training corpora used in the experiments. If R t = 0, then L t = 1. We denote the value of L t by t .</p><p>• E t ∈ E t is the index of the entity referred to, if R t = 1. The set E t consists of {1, . . . , 1 + max t &lt;t e t }, i.e., the indices of all previously mentioned entities plus an additional value for a new entity. Thus E t starts as {1} and grows monotonically with t, allowing for an arbitrary number of entities to be mentioned. We denote the value of E t by e t . If R t = 0, then E t is fixed to a special value ø.</p><p>The values of these random variables for our run- ning example are shown in <ref type="figure">Figure 2</ref>. In addition to using symbolic variables to en- code mentions and coreference relationships, we maintain a vector representation of each entity that evolves over time. For the ith entity, let e i,t be its representation at time t. These vectors are different from word vectors (x t ), in that they are not parameters of the model. They are similar to history representations (h t ), in that they are de- rived through parameterized functions of the ran- dom variables' values, which we will describe in the next subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Generative story</head><p>The generative story for the word (and other variables) at timestep t is as follows; forward- referenced equations are in the detailed discussion that follows.</p><p>1. If t−1 = 1 (i.e., x t is not continuing an already-started entity mention):</p><p>• Choose r t (Equation 3).</p><p>• If r t = 0, set t = 1 and e t = ø; then go to step 3. Otherwise: -If there is no embedding for the new candidate entity with index 1 + max t &lt;t e t , create one follow- ing §2.4. -Select the entity e t from {1, . . . , 1 + max t &lt;t e t } (Equation 4). -Set e current = e et,t−1 , which is the entity embedding of e t before timestep t. -Select the length of the mention, t (Equation 5).</p><p>2. Otherwise,</p><formula xml:id="formula_1">• Set t = t−1 − 1, r t = r t−1 , e t = e t−1 .</formula><p>3. Sample x t from the word distribution given the LSTM hidden state h t−1 and the current X1:12: John wanted to go to the coffee shop in downtown Copenhagen . R1:12: <ref type="table" target="#tab_3">:  1  1  1  1  1  3  2  1  1  2  1</ref> 1 X13:22: He was told that it sold the best beans . R13:22:</p><formula xml:id="formula_2">1 0 0 0 0 1 1 1 0 1 1 0 E1:12: 1 ø ø ø ø 2 2 2 ø 3 3 ø L1:12</formula><formula xml:id="formula_3">1 0 0 0 1 0 1 1 1 . E13:22: 1 ø ø ø 2 ø 4 4 4 ø L13:22: 1 1 1 1 1 1 3 2 1 0</formula><p>Figure 2: The random variable values in ENTITYNLM for the running example in <ref type="figure">Figure 1</ref>.</p><p>(or most recent) entity embedding e current (Equation 6). (If r t = 0, then e current still represents the most recently mentioned en- tity.)</p><p>4. Advance the RNN, i.e., feed it the word vec- tor x t to compute h t (Equation 2).</p><p>5. If r t = 1, update e et,t using e et,t−1 and h t , then set e current = e et,t . Details of the entity updating are given in §2.4.</p><p>6. For every entity e ι ∈ E t \ {e t }, set e ι,t = e ι,t−1 (i.e., no changes to other entities' rep- resentations).</p><p>Note that at any given time step t, e current will al- ways contain the most recent vector representation of the most recently mentioned entity. A generative model with a similar hierarchi- cal structure was used by <ref type="bibr" target="#b10">Haghighi and Klein (2010)</ref> for coreference resolution. Our approach differs in two important ways. First, our model defines a joint distribution over all of the text, not just the entity mentions. Second, we use repre- sentation learning rather than Bayesian nonpara- metrics, allowing natural integration with the lan- guage model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Probability distributions</head><p>The generative story above referenced several parametric distributions defined based on vector representations of histories and entities. These are defined as follows.</p><p>For r ∈ {0, 1},</p><formula xml:id="formula_4">p(R t = r | h t−1 ) ∝ exp(h t−1 W r r), (3)</formula><p>where r is the parameterized embedding associ- ated with r, which paves the way for exploring en- tity type representations in future work; W r is a parameter matrix for the bilinear score for h t−1 and r.</p><p>To give the possibility of predicting a new en- tity, we need an entity embedding beforehand with index (1 + max t &lt;t e t ), which is randomly sam- pled from Equation 7. Then, for every e ∈ {1, . . . , 1 + max t &lt;t e t }:</p><formula xml:id="formula_5">p(E t = e | R t = 1, h t−1 ) ∝ exp(h t−1 W entity e e,t−1 + w dist f (e)),<label>(4)</label></formula><p>where e e,t−1 is the embedding of entity e at time step t−1 and W entity is the weight matrix for pre- dicting entities using their continuous representa- tions. The score above is normalized over values {1, . . . , 1 + max t &lt;t e t }. f (e) represents a vec- tor of distance features associated with e and the mentions of the existing entities. Hence two in- formation sources are used to predict the next en- tity: (i) contextual information h t−1 , and (ii) dis- tance features f (e) from the current mention to the closest mention from each previously mentioned entity. f (e) = 0 if e is a new entity. This term can also be extended to include other surface-form features for coreference resolution <ref type="bibr" target="#b19">(Martschat and Strube, 2015;</ref><ref type="bibr" target="#b5">Clark and Manning, 2016b</ref>). For the chosen entity e t from Equation 4, the distribution over its mention length is drawn ac- cording to</p><formula xml:id="formula_6">p(L t = | h t−1 , e et,t−1 ) ∝ exp(W length,, [h t−1 ; e et,t−1 ]),<label>(5)</label></formula><p>where e et,t−1 is the most recent embedding of the entity e t , not updated with h t . The intuition is that e et,t−1 will help contextual information h t−1 to select the residual length of entity e t . W length is the weight matrix for length prediction, with max = 25 rows. Finally, the probability of a word x as the next token is jointly modeled by h t−1 and the vector representation of the most recently mentioned en- tity e current :</p><formula xml:id="formula_7">p(X t = x | h t−1 , e current ) ∝ CFSM(h t−1 + W e e current ),<label>(6)</label></formula><p>where W e is a transformation matrix to adjust the dimensionality of e current . CFSM is a class factor- ized softmax function <ref type="bibr" target="#b9">(Goodman, 2001;</ref><ref type="bibr" target="#b1">Baltescu and Blunsom, 2015)</ref>. It uses a two-step prediction with predefined word classes instead of direct pre- diction on the whole vocabulary, and reduces the time complexity to the log of vocabulary size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Dynamic entity representations</head><p>Before predicting the entity at step t, we need an embedding for the new candidate entity with index e = 1 + max t &lt;t e t if it does not exist. The new embedding is generated randomly, according to a normal distribution, then projected onto the unit ball:</p><formula xml:id="formula_8">u ∼ N (r 1 , σ 2 I); e e ,t−1 = u u 2 ,<label>(7)</label></formula><p>where σ = 0.01. The time step t − 1 in e e ,t−1 means the current embedding contains no infor- mation from step t, although it will be updated once we have h t and if E t = e . r 1 is the pa- rameterized embedding for R t = 1, which will be jointly optimized with other parameters and is ex- pected to encode some generic information about entities. All the initial entity embeddings are cen- tered on the mean r 1 , which is used in Equation 3 to determine whether the next token belongs to an entity mention. Another choice would be to ini- tialize with a zero vector, although our preliminary experiments showed this did not work as well as random initialization in Equation 7. Assume R t = 1 and E t = e t , which means x t is part of a mention of entity e t . Then, we need to update e et,t−1 based on the new information we have from h t . The new embedding e et,t is a con- vex combination of the old embedding (e et,t−1 ) and current LSTM hidden state (h t ) with the in- terpolation (δ t ) determined dynamically based on a bilinear function:</p><formula xml:id="formula_9">δ t = σ(h t W δ e et,t−1 ); u = δ t e et,t−1 + (1 − δ t )h t ; e et,t = u u 2 ,<label>(8)</label></formula><p>This updating scheme will be used to update e t in each of all the following t steps. The projection in the last step keeps the magnitude of the entity embedding fixed, avoiding numeric overflow. A similar updating scheme has been used by <ref type="bibr">Henaff et al. (2016)</ref> for the "memory blocks" in their re- current entity network models. The difference is that their model updates all memory blocks in each time step. Instead, our updating scheme in Equa- tion 8 only applies to the selected entity e t at time step t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Training objective</head><p>The model is trained to maximize the log of the joint probability of R, E, L, and X:</p><formula xml:id="formula_10">(θ) = log P (R, E, L, X; θ) = t log P (R t , E t , L t , X t ; θ), (9)</formula><p>where θ is the collection of all the parameters in this model. Based on the formulation in §2.3, Equation 9 can be decomposed as the sum of con- ditional log-probabilities of each random variable at each time step. This objective requires the training data anno- tated as in <ref type="figure">Figure 2</ref>. We do not assume that these variables are observed at test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Implementation Details</head><p>Our model is implemented with DyNet ( <ref type="bibr" target="#b23">Neubig et al., 2017</ref>) and available at https:// github.com/jiyfeng/entitynlm. We use AdaGrad ( <ref type="bibr" target="#b6">Duchi et al., 2011</ref>) with learning rate λ = 0.1 and ADAM ( <ref type="bibr" target="#b16">Kingma and Ba, 2014</ref>) with default learning rate λ = 0.001 as the candi- date optimizers of our model. For all the parame- ters, we use the initialization tricks recommended by <ref type="bibr" target="#b8">Glorot and Bengio (2010)</ref>. To avoid overfitting, we also employ dropout ( <ref type="bibr" target="#b30">Srivastava et al., 2014</ref>) with the candidate rates as {0.2, 0.5}.</p><p>In addition, there are two tunable hyperpa- rameters of ENTITYNLM: the size of word em- beddings and the dimension of LSTM hidden states. For both of them, we consider the values {32, 48, 64, 128, 256}. We also experiment with the option to either use the pretrained GloVe word embeddings ( <ref type="bibr" target="#b25">Pennington et al., 2014</ref>) or randomly initialized word embeddings (then updated during training). For all experiments, the best configura- tion of hyperparameters and optimizers is selected based on the objective value on the development data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation Tasks and Datasets</head><p>We evaluate our model in diverse use scenarios: (i) language modeling, (ii) coreference resolution, and (iii) entity prediction. The evaluation on lan- guage modeling shows how the internal entity rep- resentation, when marginalized out, can improve the perplexity of language models. The evaluation on coreference resolution experiment shows how our new language model can improve a compet- itive coreference resolution system. Finally, we employ an entity cloze task to demonstrate the generative performance of our model in predicting the next entity given the previous context.</p><p>We</p><note type="other">use two datasets for the three evaluation tasks. For language modeling and coreference resolution, we use the English benchmark data from the CoNLL 2012 shared task on corefer- ence resolution (Pradhan et al., 2012). We employ the standard training/development/test split, which includes 2,802/343/348 documents with roughly 1M/150K/150K tokens, respectively. We follow the coreference annotation in the CoNLL dataset to extract entities and ignore the singleton men- tions in texts.</note><p>For entity prediction, we employ the InScript corpus created by <ref type="bibr" target="#b21">Modi et al. (2017)</ref>. It consists of 10 scenarios, including grocery shopping, taking a flight, etc. It includes 910 crowdsourced simple narrative texts in total and 18 stories were ignored due to labeling problems ( <ref type="bibr" target="#b21">Modi et al., 2017</ref>). On average, each story has 12.4 sentences, 24.9 en- tities and 217.2 tokens. Each entity mention is labeled with its entity index. We use the same training/development/test split as in ( <ref type="bibr" target="#b21">Modi et al., 2017)</ref>, which includes 619, 91, 182 texts, respec- tively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data preprocessing</head><p>For the CoNLL dataset, we lowercase all tokens and remove any token that only contains a punctu- ation symbol unless it is in an entity mention. We also replace numbers in the documents with the special token NUM and low-frequency word types with UNK. The vocabulary size of the CoNLL data after preprocessing is 10K. For entity mention ex- traction, in the CoNLL dataset, one entity men- tion could be embedded in another. For embed- ded mentions, only the enclosing entity mention is kept. We use the same preprocessed data for both language modeling and coreference resolu- tion evaluation.</p><p>For the InScript corpus, we apply similar data preprocessing to lowercase all tokens, and we re- place low-frequency word types with UNK. The vocabulary size after preprocessing is 1K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we present the experimental results on the three evaluation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Language modeling</head><p>Task description. The goal of language model- ing is to compute the marginal probability:</p><formula xml:id="formula_11">P (X) = R,E,L P (X, R, E, L).<label>(10)</label></formula><p>However, due to the long-range dependency in recurrent neural networks, the search space of R, E, L during inference grows exponentially. We thus use importance sampling to approxi- mate the marginal distribution of X. Specifi- cally, with the samples from a proposal distri- bution Q(R, E, L|X), the approximated marginal probability is defined as</p><formula xml:id="formula_12">P (X) = R,E,L P (X, R, E, L) = R,E,L Q(R, E, L | X) P (X, R, E, L) Q(R, E, L | X) ≈ 1 N {r (i) ,e (i) ,, (i) }∼Q P (r (i) , e (i) , (i) , x) Q(r (i) , e (i) , (i) | x)<label>(11)</label></formula><p>A similar idea of using importance sampling for language modeling evaluation has been used by . For language modeling evaluation, we train our model on the training set from the CoNLL 2012 dataset with coreference annotation. On the test data, we treat coreference structure as latent vari- ables and use importance sampling to approximate the marginal distribution of X. For each docu- ment, the model randomly draws N = 100 sam- ples from the proposal distribution, discussed next.</p><p>Proposal distribution. For implementation of Q, we use a discriminative variant of ENTI- TYNLM by taking the current word x t for predict- ing the entity-related variables in the same time step. Specifically, in the generative story described in §2.2, we delete step 3 (words are not gener- ated, but rather conditioned upon), move step 4 before step 1, and replace h t−1 with h t in the steps for predicting entity type R t , entity E t and mention length L t . This model variant provides a on the devel- opment set with coreference annotation to select the best model configuration and report the best number. On the test data, we are able to calcu- late perplexity by marginalizing all other random variables using Equation 11. To compute the per- plexity numbers on the test data, our model only takes account of log probabilities on word predic- tion. The difference is that coreference informa- tion is only used for training ENTITYNLM and not for test. All three models reported in <ref type="table">Table 1</ref> share the same vocabulary, therefore the numbers on the test data are directly comparable. As shown in <ref type="table">Table 1</ref>, ENTITYNLM outperforms both the 5- gram language model and the RNNLM on the test data. Better performance of ENTITYNLM on lan- guage modeling can be expected, if we also use the marginalization method defined in Equation 11 on the development data to select the best configura- tion. However, we plan to use the same experi- mental setup for all experiments, instead of cus- tomizing our model for each individual task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Coreference reranking</head><p>Task description. We show how ENTITYLM, which allows an efficient computation of the probability P (R, E, L, X), can be used as a coreference reranker to improve a competitive coreference resolution system due to <ref type="bibr" target="#b19">Martschat and Strube (2015)</ref>. This task is analogous to the reranking approach used in machine transla- tion ( <ref type="bibr" target="#b29">Shen et al., 2004</ref>). The specific formulation is as follows:</p><p>arg max</p><formula xml:id="formula_13">{r (i) ,e (i) ,l (i) }∈K P (r (i) , e (i) , l (i) , x)<label>(12)</label></formula><p>where K is the k-best list for a given document.</p><p>In our experiments, k = 100. To the best of our knowledge, the problem of obtaining k-best out- puts of a coreference resolution system has not been studied before.</p><p>Approximate k-best decoding. We rerank the output of a system that predicts an antecedent for each mention by relying on pairwise scores for mention pairs. This is the dominant approach for coreference resolution <ref type="bibr" target="#b19">(Martschat and Strube, 2015;</ref><ref type="bibr" target="#b4">Clark and Manning, 2016a</ref>). The predic- tions induce an antecedent tree, which represents antecedent decisions for all mentions in the doc- ument. Coreference chains are obtained by tran- sitive closure over the antecedent decisions en- coded in the tree. A mention also can have an empty mention as antecedent, which denotes that the mention is non-anaphoric. For extending Martschat and Strube's greedy decoding approach to k-best inference, we can- not simply take the k highest scoring trees ac- cording to the sum of edge scores, because dif- ferent trees may represent the same coreference chain. Instead, we use an heuristic that creates an approximate k-best list on candidate antecedent trees. The idea is to generate trees from the orig- inal system output by considering suboptimal an- tecedent choices that lead to different coreference chains. For each mention pair (m j , m i ), we com- pute the difference of its score to the score of the optimal antecedent choice for m j . We then sort pairs in ascending order according to this differ- ence and iterate through the list of pairs. For each pair (m j , m i ), we create a tree t j,i by replacing the antecedent of m j in the original system output with m i . If this yields a tree that encodes differ- ent coreference chains from all chains encoded by trees in the k-best list, we add t i,j to the k-best list. In the case that we cannot generate a given num- ber of trees (particularly for a short document with a large k), we pad the list with the last item added to the list. Evaluation measures. For coreference resolu- tion evaluation, we employ the CoNLL scorer ( <ref type="bibr" target="#b27">Pradhan et al., 2014</ref>). It computes three com- monly used evaluation measures MUC ( <ref type="bibr">Vilain et al., 1995)</ref>, B 3 (Bagga and <ref type="bibr" target="#b0">Baldwin, 1998)</ref>, and CEAF e ( <ref type="bibr" target="#b17">Luo, 2005)</ref>. We report the F 1 score of each evaluation measure and their average as the CoNLL score.</p><p>Competing systems. We employed CORT 1 (Martschat and Strube, 2015) as our baseline coreference resolution system. Here, we com- pare with the original (one best) outputs of CORT's latent ranking model, which is the best- performing model implemented in CORT. We consider two rerankers based on ENTITYNLM. The first reranking method only uses the log probability for ENTITYNLM to sort the candidate list (Equation 12). The second method uses a linear combination of both log probabilities from ENTITYNLM and the scores from CORT, where the coefficients were found via grid search with the CoNLL score on the development set.</p><p>Results. The reranked results on the CoNLL 2012 test set are reported in <ref type="table" target="#tab_3">Table 2</ref>. The numbers of the baseline are higher than the results reported in <ref type="bibr" target="#b19">Martschat and Strube (2015)</ref> since the feature set of CORT was subsequently extended. Lines 2 and 3 in <ref type="table" target="#tab_3">Table 2</ref> present the reranked best results. As shown in this table, both reranked results give more than 1% of CoNLL score improvement on the test set over CORT, which are significant based on an approximate randomization test <ref type="bibr">2</ref> .</p><p>Additional experiments also found that increas- ing k from 100 to 500 had a minor effect. That is because the diversity of each k-best list is limited by (i) the number of entity mentions in the docu- ment, (ii) the performance of the baseline corefer- ence resolution system, and possibly (iii) the ap- proximate nature of our k-best inference proce- dure. We suspect that a stronger baseline system (such as that of Clark and Manning, 2016a) could give greater improvements, if it can be adapted to provide k-best lists. Future work might incorpo- rate the techniques embedded in such systems into ENTITYNLM.</p><p>[I] 1 was about to ride <ref type="bibr">[my]</ref>  <ref type="bibr">1</ref> [bicycle] 2 to the <ref type="bibr">[park]</ref> 3 one day when <ref type="bibr">[I]</ref> 1 noticed that the front [tire] 4 was flat .</p><p>[I] 1 realized that <ref type="bibr">[I]</ref> 1 would have to repair <ref type="bibr">[it]</ref> </p><formula xml:id="formula_14">4 . [I] 1 went into [my] 1</formula><p>[garage] 5 to get some <ref type="bibr">[tools]</ref> 5 . The first thing <ref type="bibr">[I]</ref> 1 did was remove the xxxx <ref type="figure">Figure 3</ref>: A short story on bicycles from the InScript corpus ( <ref type="bibr" target="#b21">Modi et al., 2017</ref>). The entity prediction task requires predicting xxxx given the preceding text either by choosing a previously mentioned entity or deciding that this is a "new en- tity". In this example, the ground-truth prediction is <ref type="bibr">[tire]</ref> 4 . For training, ENTITYNLM attempts to predict every entity. While, for testing, it predicts a maximum of 30 entities after the first three sen- tences, which is consistent with the experimental setup suggested by <ref type="bibr" target="#b21">Modi et al. (2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Entity prediction</head><p>Task description. Based on <ref type="bibr" target="#b21">Modi et al. (2017)</ref>, we introduce a novel entity prediction task that tries to predict the next entity given the preced- ing text. For a given text as in <ref type="figure">Figure 3</ref>, this task makes a forward prediction based on only the left context. This is different from coreference reso- lution, where both left and right contexts from a given entity mention are used in decoding. It is also different from language modeling, since this task only requires predicting entities. Since EN- TITYNLM is generative, it can be directly applied to this task. To predict entities in test data, R t is always given and ENTITYNLM only needs to pre- dict E t when R t = 1.</p><p>Baselines and human prediction. We intro- duce two baselines in this task: (i) the always-new baseline that always predicts "new entity"; (ii) a linear classification model using shallow features from <ref type="bibr" target="#b21">Modi et al. (2017)</ref>, including the recency of an entity's last mention and the frequency. We also compare with the model proposed by <ref type="bibr" target="#b21">Modi et al. (2017)</ref>. Their work assumes that the model has prior knowledge of all the participant types, which are specific to each scenario and fine-grained, e.g., rider in the bicycle narrative, and predicts partic- ipant types for new entities. This assumption is unrealistic for pure generative models like ours.    Therefore, we remove this assumption and adapt their prediction results to our formulation by map- ping all the predicted entities that have not been mentioned to "new entity". We also compare to the adapted human prediction used in the In- Script corpus. For each entity slot, <ref type="bibr" target="#b21">Modi et al. (2017)</ref> acquired 20 human predictions, and the majority vote was selected. More details about human predictions are discussed in ( <ref type="bibr" target="#b21">Modi et al., 2017</ref>).</p><p>Results. <ref type="table" target="#tab_4">Table 3</ref> shows the prediction accura- cies. ENTITYNLM (line 4) significantly outper- forms both baselines (line 1 and 2) and prior work (line 3) (p 0.01, paired t-test). The compari- son between line 4 and 5 shows our model is even close to the human prediction performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Rich-context language models. The originally proposed recurrent neural network language mod- els only capture information within sentences.</p><p>To extend the capacity of RNNLMs, various re- searchers have incorporated information beyond sentence boundaries. Previous work focuses on contextual information from previous sen- tences (Ji et al., 2016a) or discourse relations be- tween adjacent sentences ( <ref type="bibr" target="#b15">Ji et al., 2016b</ref>), show- ing improvements to language modeling and re- lated tasks like coherence evaluation and discourse relation prediction. In this work, ENTITYNLM adds explicit entity information to the language model, which is another way of adding a memory network for language modeling. Unlike the work by <ref type="bibr" target="#b31">Tran et al. (2016)</ref>, where memory blocks are used to store general contextual information for language modeling, ENTITYNLM assigns each memory block specifically to only one entity.</p><p>Entity-related models. Two recent approaches to modeling entities in text are closely related to our model. The first is the "reference-aware" lan- guage models proposed by <ref type="bibr" target="#b34">Yang et al. (2016)</ref>, where the referred entities are from either a pre- defined item list, an external database, or the con- text from the same document. <ref type="bibr" target="#b34">Yang et al. (2016)</ref> present three models, one for each case. For mod- eling a document with entities, they use corefer- ence links to recover entity clusters, though they only model entity mentions as containing a single word (an inappropriate assumption, in our view). Their entity updating method takes the latest hid- den state (similar to h t when R t = 1 in our model) as the new representation of the current entity; no long-term history of the entity is maintained, just the current local context. In addition, their lan- guage model evaluation assumes that entity infor- mation is provided at test time <ref type="bibr">(Yang, personal communication)</ref>, which makes a direct compari- son with our model impossible. Our entity updat- ing scheme is similar to the "dynamic memory" method used by <ref type="bibr">Henaff et al. (2016)</ref>. Our entity representations are dynamically allocated and up- dated only when an entity appears up, while the EntNet from <ref type="bibr">Henaff et al. (2016)</ref> does not model entities and their relationships explicitly. In their model, entity memory blocks are pre-allocated and updated simultaneously in each timestep. So there is no dedicated memory block for every en- tity and no distinction between entity mentions and non-mention words. As a consequence, it is not clear how to use their model for coreference reranking and entity prediction.</p><p>Coreference resolution. The hierarchical struc- ture of our entity generation model is inspired by <ref type="bibr" target="#b10">Haghighi and Klein (2010)</ref>. They implemented this idea as a probabillistic graphical model with the distance-dependent Chinese Restaurant Pro- cess <ref type="bibr" target="#b26">(Pitman, 1995)</ref> for entity assignment, while our model is built on a recurrent neural network architecture. The reranking method considered in our coreference resolution evaluation could also be extended with samples from additional coref- erence resolution systems, to produce more va- riety ( <ref type="bibr" target="#b24">Ng, 2005</ref>). The benefit of such a system comes, we believe, from the explicit tracking of each entity throughout the text, providing entity- specific representations. In previous work, such information has been added as features ( <ref type="bibr" target="#b18">Luo et al., 2004;</ref><ref type="bibr" target="#b2">Björkelund and Kuhn, 2014</ref>) or by com- puting distributed entity representations <ref type="bibr" target="#b33">(Wiseman et al., 2016;</ref><ref type="bibr" target="#b5">Clark and Manning, 2016b</ref>). Our ap- proach complements these previous methods.</p><p>Entity prediction. The entity prediction task discussed in §5.3 is based on work by <ref type="bibr" target="#b21">Modi et al. (2017)</ref>. The main difference is that we do not as- sume that all entities belong to a previously known set of entity types specified for each narrative sce- nario. This task is also closely related to the "narrative cloze" task of <ref type="bibr" target="#b3">Chambers and Jurafsky (2008)</ref> and the "story cloze test" of <ref type="bibr" target="#b22">Mostafazadeh et al. (2016)</ref>. Those studies aim to understand re- lationships between events, while our task focuses on predicting upcoming entity mentions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have presented a neural language model, EN- TITYNLM, that defines a distribution over texts and the mentioned entities. It provides vector rep- resentations for the entities and updates them dy- namically in context. The dynamic representations are further used to help generate specific entity mentions and the following text. This model out- performs strong baselines and prior work on three tasks: language modeling, coreference resolution, and entity prediction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 : Coreference resolution scores on the CoNLL 2012 test set. CORT is the best-performing</head><label>2</label><figDesc></figDesc><table>model 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Entity prediction accuracy on the test set 
of the InScript corpus. 

</table></figure>

			<note place="foot" n="1"> https://github.com/smartschat/cort, we used version 0.2.4.5. 2 https://github.com/smartschat/art</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank anonymous reviewers for the helpful feedback on this work. We also thank the members of Noah's ARK and XLab at University of Wash-ington for their valuable comments, particularly Eunsol Choi for pointing out the InScript corpus. This research was supported in part by a Univer-sity of Washington Innovation Award, Samsung GRO, NSF grant IIS-1524371, the DARPA CwC program through ARO (W911NF-15-1-0543), and gifts by <ref type="bibr">Google and Facebook.</ref> </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Algorithms for scoring coreference chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Bagga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Breck</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC Workshop on Linguistic Coreference</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pragmatic neural language modelling in machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Baltescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning structured perceptrons for coreference resolution with latent antecedents and non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Björkelund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised Learning of Narrative Event Chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for mention-ranking coreference models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving coreference resolution by learning entitylevel distributed representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recurrent neural network grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Classes for fast maximum entropy training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Coreference resolution in a modular, entity-centered model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Scalable modified Kneser-Ney language model estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Pouzyrevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03969</idno>
		<title level="m">Antoine Bordes, and Yann LeCun. 2016. Tracking the world state with recurrent entity networks</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Document context language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (workshop track)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A latent variable recurrent neural network for discourse-driven language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<editor>NAACL-HLT</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On coreference resolution performance metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-EMNLP</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A mentionsynchronous coreference resolution algorithm based on the Bell tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Ittycheriah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyan</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanda</forename><surname>Kambhatla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Latent structures for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Martschat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="405" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
		<editor>INTERSPEECH</editor>
		<imprint>
			<date type="published" when="2010-01" />
		</imprint>
	</monogr>
	<note>Cernock`Cernock`y, and Sanjeev Khudanpur</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Modeling semantic expectation: Using script knowledge for referent prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Modi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vera</forename><surname>Demberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association of Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="31" to="44" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Asad Sayeed, and Manfred Pinkal</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A corpus and evaluation framework for deeper understanding of commonsense stories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonios</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Clothiaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.03980</idno>
	</analytic>
	<monogr>
		<title level="m">The dynamic neural network toolkit</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Machine learning for coreference resolution: From local classification to global ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Exchangeable and partially exchangeable random partitions. Probability Theory and Related Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Pitman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="145" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Scoring coreference partitions of predicted mentions: A reference implementation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">CoNLL2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLPCoNLL</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Discriminative reranking for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Recurrent memory networks for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<editor>NAACL-HLT</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dennis Connolly, and Lynette Hirschman. 1995. A modeltheoretic coreference scoring scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Vilain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Aberdeen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MUC</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning global features for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><forename type="middle">M</forename><surname>Shieber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01628</idno>
		<title level="m">Reference-aware language models</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<title level="m">Ilya Sutskever, and Oriol Vinyals. 2015. Recurrent neural network regularization. ICLR</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
