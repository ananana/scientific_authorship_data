<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">How Transferable are Neural Networks in NLP Applications?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Meng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Insitute of Computer Science and Technology of Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="laboratory">Key Laboratory of High Confidence Software Technologies (Peking University)</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<region>MoE</region>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">How Transferable are Neural Networks in NLP Applications?</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="479" to="489"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Transfer learning is aimed to make use of valuable knowledge in a source domain to help model performance in a target domain. It is particularly important to neural networks, which are very likely to be overfitting. In some fields like image processing, many studies have shown the effectiveness of neural network-based transfer learning. For neural NLP, however, existing studies have only casually applied transfer learning, and conclusions are inconsistent. In this paper, we conduct systematic case studies and provide an illuminating picture on the transferability of neural networks in NLP. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Transfer learning, or sometimes known as domain adaptation, <ref type="bibr">2</ref> plays an important role in various nat- ural language processing (NLP) applications, espe- cially when we do not have large enough datasets for the task of interest (called the target task T ). In such scenarios, we would like to transfer or adapt knowledge from other domains (called the source domains/tasks S) so as to mitigate the problem of overfitting and to improve model performance in T . For traditional feature-rich or kernel-based mod- els, researchers have developed a variety of ele- gant methods for domain adaptation; examples in- clude EasyAdapt <ref type="bibr" target="#b6">(Daumé III, 2007</ref>; Daumé III et * Yan Xu is currently a research scientist at Inveno Co., Ltd.</p><p>1 Code released on https://sites.google.com/site/transfernlp/ 2 In this paper, we do not distinguish the conceptual differ- ence between transfer learning and domain adaptation. Do- main-in the sense we use throughout this paper-is defined by datasets.</p><p>al., 2010), instance weighting ( <ref type="bibr" target="#b13">Jiang and Zhai, 2007;</ref><ref type="bibr" target="#b9">Foster et al., 2010)</ref>, and structural correspondence learning ( <ref type="bibr" target="#b1">Blitzer et al., 2006;</ref><ref type="bibr" target="#b19">Prettenhofer and Stein, 2010)</ref>.</p><p>Recently, deep neural networks are emerging as the prevailing technical solution to almost every field in NLP. Although capable of learning highly nonlinear features, deep neural networks are very prone to overfitting, compared with traditional meth- ods. Transfer learning therefore becomes even more important. Fortunately, neural networks can be trained in a transferable way by their incremental learning nature: we can directly use trained (tuned) parameters from a source task to initialize the net- work in the target task; alternatively, we may also train two tasks simultaneously with some parame- ters shared. But their performance should be verified by empirical experiments.</p><p>Existing studies have already shown some evi- dence of the transferability of neural features. For example, in image processing, low-level neural lay- ers closely resemble Gabor filters or color blobs <ref type="bibr" target="#b23">(Zeiler and Fergus, 2014;</ref><ref type="bibr" target="#b14">Krizhevsky et al., 2012)</ref>; they can be transferred well to different tasks. <ref type="bibr" target="#b7">Donahue et al. (2014)</ref> suggest that high-level layers are also transferable in general visual recognition; <ref type="bibr" target="#b22">Yosinski et al. (2014)</ref> further investigate the trans- ferability of neural layers in different levels of ab- straction.</p><p>Although transfer learning is promising in image processing, conclusions appear to be less clear in NLP applications. Image pixels are low-level sig- nals, which are generally continuous and less related to semantics. By contrast, natural language tokens are discrete: each word well reflects the thought of humans, but neighboring words do not share as much information as pixels in images do. Previ- ous neural NLP studies have casually applied trans- fer techniques, but their results are not consistent. <ref type="bibr" target="#b4">Collobert and Weston (2008)</ref> apply multi-task learn- ing to SRL, NER, POS, and CHK, 3 but obtain only 0.04-0.21% error reduction 4 (out of a base error rate of 16-18%). <ref type="bibr" target="#b2">Bowman et al. (2015)</ref>, on the contrary, improve a natural language inference task from an accuracy of 71.3% to 80.8% by initializing parame- ters with an additional dataset of 550,000 samples. Therefore, more systematic studies are needed to shed light on transferring neural networks in the field of NLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our Contributions</head><p>In this paper, we investigate the question "How transferable are neural networks in NLP applica- tions?"</p><p>We distinguish two scenarios of transfer: (1) transferring knowledge to a semantically simi- lar/equivalent task but with a different dataset; (2) transferring knowledge to a task that is semanti- cally different but shares the same neural topol- ogy/architecture so that neural parameters can in- deed be transferred. We further distinguish two transfer methods: (1) using the parameters trained on S to initialize T (INIT), and (2) multi-task learn- ing (MULT), i.e., training S and T simultaneously. (Please see Sections 2 and 4). Our study mainly fo- cuses on the following research questions: RQ1: How transferable are neural networks be- tween two tasks with similar or different se- mantics in NLP applications? RQ2: How transferable are different layers of NLP neural models?</p><p>RQ3: How transferable are INIT and MULT, re- spectively? What is the effect of combining these two methods?</p><p>We conducted extensive experiments over six datasets on classifying sentences and sentence pairs. We leveraged the widely-used convolutional neu- ral network (CNN) and long short term memory (LSTM)-based recurrent neural network (RNN) as our models.</p><p>Based on our experimental results, we have the following main observations, some of which are un- expected.</p><p>• Whether a neural network is transferable in NLP depends largely on how semantically similar the tasks are, which is different from the consensus in image processing.</p><p>• The output layer is mainly specific to the dataset and not transferable. Word embed- dings are likely to be transferable to seman- tically different tasks.</p><p>• MULT and INIT appear to be generally com- parable to each other; combining these two methods does not result in further gain in our study.</p><p>The rest of this paper is organized as follows. Sec- tion 2 introduces the datasets that neural models are transferred across; Section 3 details the neural archi- tectures and experimental settings. We describe two approaches (INIT and MULT) to transfer learning in Section 4. We present experimental results in Sec- tions 5-6 and have concluding remarks in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Datasets</head><p>In our study, we conducted two series of experi- ments using six open datasets as follows.</p><p>• Experiment I: Sentence classification − IMDB. A large dataset for binary sentiment classification (positive vs. negative). 5 − MR. A small dataset for binary sentiment clas- sification. 6 − QC. A (small) dataset for 6-way question classification (e.g., location, time, and number). Statistics (# of Samples) Experiment I Experiment II IMDB MR QC SNLI SICK MSRP #Train 550,000 8,500 4,800 550,152 4,439 3,575 #Val 50,000 1,100 600 10,000 495 501 #Test 2,000 1,100 500 10,000 4,906 1,725  • Experiment II: Sentence-pair classification − SNLI. A large dataset for sentence entail- ment recognition. The classification objec- tives are entailment, contradiction, and neutral. 8 − SICK. A small dataset with exactly the same classification objective as SNLI. 9 − MSRP. A (small) dataset for paraphrase de- tection. The objective is binary classification: judging whether two sentences have the same meaning. <ref type="bibr">10</ref> In each experiment, the large dataset serves as the source domain and small ones are the target do- mains. <ref type="table" target="#tab_1">Table 1</ref> presents statistics of the above datasets.</p><p>We distinguish two scenarios of transfer regard- ing semantic similarity: (1) semantically equivalent transfer (IMDB→MR, SNLI→SICK), that is, the tasks of S and T are defined by the same meaning, and (2) semantically different transfer <ref type="bibr">(IMDB→QC, SNLI→MSRP)</ref>. Examples are also illustrated in <ref type="table" target="#tab_1">Ta- ble 1</ref> to demonstrate semantic relatedness.</p><p>It should be noticed that in image or speech pro- cessing ( <ref type="bibr" target="#b22">Yosinski et al., 2014;</ref><ref type="bibr" target="#b21">Wang and Zheng, 2015)</ref>, the input of neural networks pretty much con- sists of raw signals; hence, low-level feature detec- tors are almost always transferable, even if <ref type="bibr" target="#b22">Yosinski et al. (2014)</ref> manually distinguish artificial objects and natural ones in an image classification task.</p><p>Distinguishing semantic relatedness-which emerges from very low layers of either word em- beddings or the successive hidden layer-is specific to NLP and also a new insight of our paper. As we shall see in Sections 5 and 6, the transferability of neural networks in NLP is more sensitive to semantics than in image processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Neural Models and Settings</head><p>In each group, we used a single neural model to solve three problems in a unified manner. That is to say, the neural architecture is the same among the three datasets, which makes it possible to investi- gate transfer learning regardless of whether the tasks are semantically equivalent. Concretely, the neural models are as follows.</p><p>• Experiment I: LSTM-RNN. To classify a sentence according to its sentiment or ques- tion type, we use a recurrent neural network (RNN, <ref type="figure" target="#fig_2">Figure 1a</ref>) with long short term mem- ory (LSTM) units (Hochreiter and Schmidhu- ber, 1997). A softmax layer is added to the last word's hidden state for classification.</p><p>• Experiment II: CNN-pair. In this group, we use a "Siamese" architecture ( <ref type="bibr" target="#b3">Bromley et al., 1993)</ref> to classify the relation of two sentences. We first apply a convolutional neural network (CNN, <ref type="figure" target="#fig_2">Figure 1b</ref>) with a window size of 5 to model local context, and a max pooling layer gathers information to a fixed-size vector. Then the sentence vectors are concatenated and fed to a hidden layer before the softmax output.</p><p>In our experiments, embeddings were pretrained by <ref type="bibr">word2vec (Mikolov et al., 2013)</ref>; all embed- dings and hidden layers were 100 dimensional. We  applied stochastic gradient descent with a mini- batch size of 50 for optimization. In each setting, we tuned the hyperparameters as follows: learning rate from {3, 1, 0.3, 0.1, 0.03}, power decay of learning rate from {fast, moderate, low} (defined by how much, after one epoch, the learning rate residual is: 0.1x, 0.3x, 0.9x, resp). We regularized our network by dropout with a rate from {0, 0.1, 0.2, 0.3}. Note that we might not run nonsensical settings, e.g., a larger dropout rate if the network has already been underfitting (i.e., accuracy has decreased when the dropout rate increases). We report the test perfor- mance associated with the highest validation accu- racy.</p><p>To setup a baseline, we trained our models with- out transfer 5 times by different random parameter initializations <ref type="table" target="#tab_3">(Table 2)</ref>. We have achieved reason- able performance that is comparable to similar mod- els reported in the literature with all six datasets. Therefore, our implementation is fair and suitable for further study of transfer learning.</p><p>It should be mentioned that the goal of this paper is not to outperform state-of-the-art results; instead,  we would like to conduct a fair comparison of dif- ferent methods and settings for transfer learning in NLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Transfer Methods</head><p>Transfer learning aims to use knowledge in a source domain to aid the target domain. As neural net- works are usually trained incrementally with gradi- ent descent (or variants), it is straightforward to use gradient information in both source and target do- mains for optimization so as to accomplish knowl- edge transfer. Depending on how samples in source and target domains are scheduled, there are two main approaches to neural network-based transfer learning:</p><p>• Parameter initialization (INIT). The INIT ap- proach first trains the network on S, and then di- rectly uses the tuned parameters to initialize the network for T . After transfer, we may fix ( ♂ ) the parameters in the target domain <ref type="bibr" target="#b10">(Glorot et al., 2011</ref>), i.e., no training is performed on T . But when labeled data are available in T , it would be better to fine-tune () the parameters.</p><p>INIT is also related to unsupervised pretraining such as word embedding learning ( <ref type="bibr" target="#b16">Mikolov et al., 2013</ref>) and autoencoders ( <ref type="bibr" target="#b0">Bengio et al., 2006</ref>). In these approaches, parameters that are (pre)trained in an unsupervised way are transferred to initial- ize the model for a supervised task <ref type="bibr" target="#b18">(Plank and Moschitti, 2013)</ref>. However, our paper focuses on "supervised pretraining," which means we trans- fer knowledge from a labeled source domain.</p><p>• Multi-task learning (MULT). MULT, on the other hand, simultaneously trains samples in both do- mains <ref type="bibr" target="#b4">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b15">Liu et al., 2016)</ref>. The overall cost function is given by</p><formula xml:id="formula_0">J = λJ T + (1 − λ)J S<label>(1)</label></formula><p>where J T and J S are the individual cost function of each domain. (Both J T and J S are normalized by the number of training samples.) λ ∈ (0, 1) is a hyperparameter balancing the two domains.</p><p>It is nontrivial to optimize Equation 1 in practice by gradient-based methods. One may take the par- tial derivative of J and thus λ goes to the learning rate ( <ref type="bibr" target="#b15">Liu et al., 2016</ref>), but the model is then vul- nerable because it is likely to blow up with large learning rates (multiplied by λ or 1 − λ) and be stuck in local optima with small ones.</p><p>Collobert and Weston <ref type="formula">(2008)</ref> alternatively choose a data sample from either domain with a certain probability (controlled by λ) and take the deriva- tive for the particular data sample. In this way, do- main transfer is independent of learning rates, but we may not be able to fully use the entire dataset of S if λ is large. We adopted the latter approach in our experiment for simplicity. (More in-depth analysis may be needed in future work.) Formally, our multi-task learning strategy is as follows.</p><p>1 Switch to T with prob. λ, or to S with prob. 1 − λ. 2 Compute the gradient of the next data sample in the particular domain.</p><p>Further, INIT and MULT can be combined straightforwardly, and we obtain the third setting:</p><p>• Combination (MULT+INIT). We first pretrain on the source domain S for parameter initialization, and then train S and T simultaneously.</p><p>From a theoretical perspective, INIT and MULT work in different ways. In the MULT approach, the source domain regularizes the model by "aliasing" the error surface of the target domain; hence the neural network is less prone to overfitting. In INIT, T 's error surface remains intact. Before training on the target dataset, the parameters are initialized in such a meaningful way that they contain additional knowledge in the source domain. However, in an ex- treme case where T 's error surface is convex, INIT is ineffective because the parameters can reach the global optimum regardless of their initialization. In practice, deep neural networks usually have highly complicated, non-convex error surfaces. By prop- erly initializing parameters with the knowledge of S, we can reasonably expect that the parameters are in a better "catchment basin," and that the INIT ap- proach can transfer knowledge from S to T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results of Transferring by INIT</head><p>We first analyze how INIT behaves in NLP-based transfer learning. In addition to two different trans- fer scenarios regarding semantic relatedness as de- scribed in Section 2, we further evaluated two set- tings: (1) fine-tuning parameters , and (2) freez- ing parameters after transfer ♂ . Existing evidence shows that frozen parameters would generally hurt the performance ( <ref type="bibr" target="#b17">Peng et al., 2015)</ref>, but this setting provides a more direct understanding on how trans- ferable the features are (because the factor of target domain optimization is ruled out). Therefore, we included it in our experiments. Moreover, we trans- ferred parameters layer by layer to answer our sec- ond research question.</p><p>Through Subsections 5.1-5.3, we initialized the parameters of T with the ones corresponding to the highest validation accuracy of S. In Subsec- tion 5.4, we further investigated when the parame- ters are ready to be transferred during the training on S. <ref type="table" target="#tab_5">Table 3</ref> shows the main results of INIT. A quick observation is that, in both groups, transfer learn- ing of semantically equivalent tasks (IMDB→MR, SNLI→SICK) appears to be successful with an im- provement of ∼6%. The results are not surprising and also reported in <ref type="bibr" target="#b2">Bowman et al. (2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Overall Performance</head><p>For IMDB→QC and SNLI→MSRP, however, there is no improvement of transferring hidden lay- ers (embeddings excluded), namely LSTM-RNN units and CNN feature maps. The EHO setting yields a slight degradation of 0.2-0.4%, ∼.5x std. The incapability of transferring is also proved by locking embeddings and hidden layers</p><formula xml:id="formula_1">(E ♂ H ♂ O)</formula><p>. We see in this setting, the test per- formance is very low in QC or even worse than majority-class guess in MSRP. By further examin- ing its training accuracy, which is 48.2% and 65.5%, respectively, we conclude that extracted features by LSTM-RNN and CNN models in S are almost irrel- evant to the ultimate tasks T (QC and MSRP).</p><p>Although in previous studies, researchers have mainly drawn positive conclusions about transfer learning, we find a negative result similar to ours upon careful examination of <ref type="bibr" target="#b4">Collobert and Weston (2008)</ref>, and unfortunately, their results may be somewhat misinterpreted. In that paper, the authors report transferring NER, POS, CHK, and pretrained word embeddings improves the SRL task by 1.91- 3.90% accuracy (out of 16.54-18.40% error rate), but their gain is mainly due to word embeddings. In the settings that use pretrained word embeddings (which is common in NLP), NER, POS, and CHK together improve the SRL accuracy by only 0.04- 0.21%.</p><p>The above results are rather frustrating, indicat- ing for RQ1 that neural networks may not be trans- ferable to NLP tasks of different semantics. Trans- fer learning for NLP is more prone to semantics than the image processing domain, where even high- level feature detectors are almost always transfer- able ( <ref type="bibr" target="#b7">Donahue et al., 2014;</ref><ref type="bibr" target="#b22">Yosinski et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Layer-by-Layer Analysis</head><p>To answer RQ2, we next analyze the transferabil- ity of each layer. First, we freeze both embeddings and hidden layers (E ♂ H ♂</p><p>). Even in semantically equivalent settings, if we further freeze the output layer (O ♂ ), the performance in both IMDB→MR and SNLI→SICK drops, but by randomly initializing the output layer's parameters (O), we can obtain a similar or higher result compared with the baseline (EHO). The finding suggests that the output layer is mainly specific to a dataset. Transferring the output layer's parameters yields little (if any) gain.</p><p>Regarding embeddings and hidden layers (in the settings EHO/EHO vs. EHO), the IMDB→MR experiment suggests both of em- beddings and the hidden layer play an important role, each improving the accuracy by 3%. In SNLI→SICK, however, the main improvement lies in the hidden layer. A plausible explanation is that   in sentiment classification tasks (IMDB and MR), in- formation emerges from raw input, i.e., sentiment lexicons and thus their embeddings, but natural lan- guage inference tasks (SNLI and SICK) address more on semantic compositionality and thus hidden layers are more important. Moreover, for semantically different tasks (IMDB→QC and SNLI→MSRP), the embeddings are the only parameters that have been observed to be transferable, slightly benefiting the target task by 2.7x and 1.8x std, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">How does learning rate affect transfer?</head><p>Bowman et al. (2015) suggest that after transferring, a large learning rate may damage the knowledge stored in the parameters; in their paper, they transfer the learning rate information (AdaDelta) from S to T in addition to the parameters. Although the rule of the thumb is to choose all hyperparameters-including the learning rate-by validation, we are curious whether the above con- jecture holds. Estimating a rough range of sensible hyperparameters can ease the burden of model selec- tion; it also provides evidence to better understand how transfer learning actually works.</p><p>We plot the learning curves of different learning rates α in <ref type="figure" target="#fig_4">Figure 2</ref> (IMDB→MR and SNLI→SICK, EHO). (In the figure, no learning rate decay is applied.) As we see, with a large learning rate like α = 0.3, the accuracy increases fast and peaks at earlier epochs. Training with a small learning rate (e.g., α = 0.01) is slow, but its peak performance is comparable to large learning rates when iterated by, say, 100 epochs. The learning curves in <ref type="figure" target="#fig_4">Figure 2</ref> are similar to classic speed/variance trade-off, and we have the following additional discovery:</p><p>In INIT, transferring learning rate information is not necessarily useful. A large learning rate does not damage the knowledge stored in the pretrained hyperparameters, but accelerates the training process to a large extent. In all, we may need to perform validation to choose the learning rate if computational resources are available. Learning curve of SNLI </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">When is it ready to transfer?</head><p>In the above experiments, we transfer the parame- ters when they achieve the highest validation perfor- mance on S. This is a straightforward and intuitive practice. However, we may imagine that the parameters well-tuned to the source dataset may be too specific to it, i.e., the model overfits S and thus may underfit T . Another advantage of early transfer lies in com-putational concerns. If we manage to transfer model parameters after one or a few epochs on S, we can save much time especially when S is large.</p><p>We therefore made efforts in studying when the neural model is ready to be transferred. Figures 3a and 3c plot the learning curves of the source tasks. The accuracy increases sharply from epochs 1-5; later, it reaches a plateau but is still growing slowly.</p><p>We then transferred the parameters at different stages (epochs) of training to target tasks (also with the setting EHO). Their accuracies are plotted in <ref type="figure" target="#fig_5">Figures 3b and 3d</ref>.</p><p>In IMDB→MR, the source performance and trans- ferring performance align well. The SNLI→SICK experiment, however, produces interesting yet unex- pected results. Using the second epoch of SNLI's training yields the highest transfer performance on SICK, i.e., 78.98%, when the SNLI performance itself is comparatively low (72.65% vs. 76.26% at epoch 23). Later, the transfer performance decreases gradually by ∼2.7%. The results in these two exper- iments are inconsistent and lack explanation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">MULT, and its Combination with INIT</head><p>To answer RQ3, we investigate how multi-task learning performs in transferring knowledge, as well as the effect of the combination of MULT and INIT. In this section, we applied the setting: sharing em- beddings and hidden layers (denoted as E♥H♥O), analogous to EHO in INIT. When combining MULT and INIT, we used the pretrained parameters of embeddings and hidden layers on S to initialize the multi-task training of S and T , visually repre- sented by E♥H♥O.</p><p>In both MULT and MULT+INIT, we had a hy- perparameter λ ∈ (0, 1) balancing the source and target tasks (defined in Section 4). λ was tuned with a granularity of 0.1. As a friendly reminder, λ = 1 refers to using T only; λ = 0 refers to using S only. After finding that a small λ yields high performance of MULT in the IMDB+MR and SNLI+SICK exper- iments (thick blue lines in <ref type="figure" target="#fig_6">Figures 4a and 4c)</ref>, we further tuned the λ from 0.01 to 0.09 with a fine- grained granularity of 0.02.</p><p>The results are shown in <ref type="figure" target="#fig_6">Figure 4</ref>. From the green curves in the 2nd and 4th subplots, we see MULT (with or without INIT) does not improve the accu- racy of target tasks (QC and MSRP); the inability to transfer is cross-checked by the INIT method in Section 5. For MR and SICK, on the other hand, transferability of the neural model is also consis- tently positive (blue curves in <ref type="figure" target="#fig_6">Figures 4a and 4c</ref>), supporting our conclusion to RQ1 that neural trans-fer learning in NLP depends largely on how similar in semantics the source and target datasets are. Moreover, we see that the peak performance of MULT is slightly lower than INIT in Experiment I <ref type="figure" target="#fig_6">(Figure 4a</ref>), but higher in Experiment II ( <ref type="figure" target="#fig_6">Figure 4c)</ref>; they are in the same ballpark.</p><p>In MULT+INIT (E♥H♥O), the transfer performance of MULT+INIT remains high for dif- ferent values of λ. Because the parameters given by INIT have already conveyed sufficient informa- tion about the source task, MULT+INIT consis- tently outperforms non-transferring by a large mar- gin. Its peak performance, however, is not higher than MULT or INIT. In summary, we answer our RQ3 as follows: in our experiments, MULT and INIT are generally comparable; we do not obtain further gain by combining MULT and INIT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Concluding Remarks</head><p>In this paper, we addressed the problem of trans- fer learning in neural network-based NLP applica- tions. We conducted two series of experiments on six datasets, showing that the transferability of neu- ral NLP models depends largely on the semantic re- latedness of the source and target tasks, which is different from other domains like image processing. We analyzed the behavior of different neural layers. We also experimented with two transfer methods: parameter initialization (INIT) and multi-task learn- ing (MULT). Besides, we reported two additional studies in Sections 5.3 and 5.4 (not repeated here). Our paper provides insight on the transferability of neural NLP models; the results also help to better understand neural features in general.</p><p>How transferable are the conclusions in this paper? We have to concede that empirical studies are subject to a variety of factors (e.g., models, tasks, datasets), and that conclusions may vary in different scenarios. In our paper, we have tested all results on two groups of experiments involving 6 datasets and 2 neural models (CNN and LSTM-RNN). Both models and tasks are widely studied in the literature, and not chosen deliberately. Results are mostly con- sistent (except Section 5.4). Along with analyzing our own experimental data, we have also collected related results in previous studies, serving as addi- tional evidence in answering our research questions. Therefore, we think the generality of this work is fair and that the conclusions can be generalized to similar scenarios.</p><p>Future work. Our work also points out some fu- ture directions of research. For example, we would like to analyze the effect of different MULT strate- gies. More efforts are also needed in developing an effective yet robust method for multi-task learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>7</head><label>7</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The models in our study. (a) Experiment I: RNNs with LSTM units for sentence classification. (b) Experiment II: CNN for sentence pair modeling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>H</head><label></label><figDesc>: hidden layers; O: output layer. : Word embeddings are pretrained by word2vec; : Parameters are randomly initial- ized); ♂ : Parameters are transferred but frozen; : Parame- ters are transferred and fine-tuned. Notice that the E ♂ H ♂ O ♂ and EHO settings are inapplicable to IMDB→QC and SNLI→MSRP, because the output targets do not share same meanings and numbers of target classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Learning curves of different learning rates (denoted as α). (a) Experiment I: IMDB→MR; (b) Experiment II: SNLI→SICK.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: (a) and (c): Learning curves of S. (b) and (d): Accuracies of T when parameters are transferred at a certain epoch during the training of S. Dotted lines refer to non-transfer, which can be equivalently viewed as transferring before training on S, i.e., epoch = 0. Note that the x-axis shares across different subplots.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Results of MULT and MULT+INIT, where we share word embeddings and hidden layers. Dotted lines are the nontransfer setting; dashed lines are the INIT setting EHO, transferred at the peak performance of IMDB and SNLI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Statistics and examples of the datasets.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc>showing that we have achieved comparable results, and thus are ready to investigate transfer learning. The models were run one only once in source domains, because we could only transfer a particular model instead of an average of several models.</figDesc><table>Accuracy (%) without transfer. We also include re-

lated models for comparison (Dong et al., 2015; Socher et al., 

2011; Zhao et al., 2015; Bowman et al., 2015; Hu et al., 2014), 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Main results of neural transfer learning by INIT. We 

report test accuracies (%) in this table. E: embedding layer; 

</table></figure>

			<note place="foot" n="3"> The acronyms refer to semantic role labeling, named entity recognition, part-of-speech tagging, and chunking, respectively. 4 Here, we quote the accuracies obtained by using unsupervised pretraining of word embeddings. This is the highest performance in that paper; using pretrained word embeddings is also a common practice in the literature.</note>

			<note place="foot" n="5"> https://drive.google.com/file/d/ 0B8yp1gOBCztyN0JaMDVoeXhHWm8/ 6 https://www.cs.cornell.edu/people/pabo/ movie-review-data/ 7 http://cogcomp.cs.illinois.edu/Data/QA/QC/</note>

			<note place="foot" n="8"> http://nlp.stanford.edu/projects/snli/ 9 http://http://alt.qcri.org/semeval2014/task1/ 10 http://research.microsoft.com/en-us/downloads/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank all reviewers for their constructive com-ments, Sam Bowman for helpful suggestion, and Vicky Li for discussion on the manuscript. This research is supported by the National Basic Re-search Program of China (the 973 Program) un-der Grant No. 2015CB352201 and the National Natural Science Foundation of China under Grant Nos. <ref type="bibr">61232015, 91318301, 61421091, 61225007, and 61502014.</ref> </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Greedy layer-wise training of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Domain adaptation with structural correspondence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="120" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="632" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Signature verification using a &quot;Siamese&quot; time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">W</forename><surname>Bentz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Säckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roopak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Pattern Recognition and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page" from="669" to="688" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning</title>
		<meeting>the 25th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Frustratingly easy semi-supervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avishek</forename><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Domain Adaptation for Natural Language Processing</title>
		<meeting>the Workshop on Domain Adaptation for Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="53" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="256" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">DeCAF: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="647" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A statistical parsing framework for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="293" to="336" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Discriminative instance weighting for domain adaptation in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyril</forename><surname>Goutte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="451" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Domain adaptation for large-scale sentiment classification: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning</title>
		<meeting>the 28th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutional neural network architectures for matching natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2042" to="2050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Instance weighting for domain adaptation in NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="264" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Implicit discourse relation classification via multi-task neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 30th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2750" to="2756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A comparative study on regularization strategies for embeddingbased neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2106" to="2111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Embedding semantic similarity in tree kernels for domain adaptation of relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1498" to="1507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Crosslanguage text classification using structural correspondence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1118" to="1127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Transfer learning for speech and language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">Fang</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asia-Pacific Signal and Information Processing Association Annual Summit and Conference</title>
		<meeting>the Asia-Pacific Signal and Information Processing Association Annual Summit and Conference</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1225" to="1237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 13th European Conference on Computer Vision</title>
		<meeting>13th European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Self-adaptive hierarchical sentence model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Poupart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4069" to="4076" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
