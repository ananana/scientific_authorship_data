<?xml version="1.0" ?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xlink="http://www.w3.org/1999/xlink" 
 xmlns:mml="http://www.w3.org/1998/Math/MathML">
	<teiHeader>
		<fileDesc xml:id="f_1546"/>
	</teiHeader>
	<text>
		<front/>
		<body/>
		<back>
			<listBibl>
<biblStruct >
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fr-En Corpus ; Callison-Burch</surname></persName>
		</author>
		<title level="m">Appendix A)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct >
	<monogr>
		<title level="m" type="main">this initialization resulted in raising micro-averaged F1 from 82.2 to 83.3 9 More recent discoveries on the usefulness of language model pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peters</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">300</biblScope>
		</imprint>
	</monogr>
	<note>Howard and Ruder, 2018) for RNN encoders suggest a promising direction for future SPRL experiments. uncased; glove.42B.300d from https://nlp.stanford.edu/projects/glove/</note>
</biblStruct>

<biblStruct >
	<analytic>
		<title level="a" type="main">533 out-of-vocabulary words across all datasets were assigned a random embedding</title>
	</analytic>
	<monogr>
		<title level="m">Embeddings remained fixed during training</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
	<note>uniformly from [âˆ’.01, .01</note>
</biblStruct>

			</listBibl>
		</back>
	</text>
</TEI>
