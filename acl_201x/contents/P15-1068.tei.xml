<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:23+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">How Far are We from Fully Automatic High Quality Grammatical Error Correction?</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Bryant</surname></persName>
							<email>bryant@comp.nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<addrLine>13 Computing Drive</addrLine>
									<postCode>117417</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<addrLine>13 Computing Drive</addrLine>
									<postCode>117417</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">How Far are We from Fully Automatic High Quality Grammatical Error Correction?</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="697" to="707"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we first explore the role of inter-annotator agreement statistics in grammatical error correction and conclude that they are less informative in fields where there may be more than one correct answer. We next created a dataset of 50 student essays, each corrected by 10 different annotators for all error types, and investigated how both human and GEC system scores vary when different combinations of these annotations are used as the gold standard. Upon learning that even humans are unable to score higher than 75% F 0.5 , we propose a new metric based on the ratio between human and system performance. We also use this method to investigate the extent to which annotators agree on certain error categories, and find that similar results can be obtained from a smaller subset of just 10 essays.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Interest in grammatical error correction (GEC) systems has grown considerably in the past few years, thanks mainly to the success of the recent Helping Our Own (HOO) ( <ref type="bibr" target="#b6">Dale and Kilgarriff, 2011;</ref><ref type="bibr" target="#b7">Dale et al., 2012</ref>) and Conference on Natu- ral Language Learning (CoNLL) ( <ref type="bibr" target="#b16">Ng et al., 2014</ref>) shared tasks. Despite this increas- ing attention, however, one of the most significant challenges facing GEC today is the lack of a robust evaluation practice. In fact <ref type="bibr" target="#b2">Chodorow et al. (2012)</ref> even go as far to say that it is sometimes "hard to draw meaningful comparisons between differ- ent approaches, even when they are evaluated on the same corpus."</p><p>One of the reasons for this is that, tradition- ally, system performance has only ever been eval- uated against the gold standard annotations of a single native speaker (rarely, two native speakers). As such, system output is not actually scored on the basis of grammatical acceptability alone, but rather is also constrained by the idiosyncrasies of the particular annotators.</p><p>The obvious solution to this problem would be to compare systems against the gold standard an- notations of multiple annotators, in an effort to di- lute the effect of individual annotator bias, how- ever creating manual annotations is often consid- ered too time consuming and expensive. In spite of this, while other studies have instead elected to use crowdsourcing to produce multiply-corrected an- notations, often concerning only a limited number of error types <ref type="bibr" target="#b14">(Madnani et al., 2011;</ref><ref type="bibr" target="#b17">Pavlick et al., 2014;</ref><ref type="bibr" target="#b22">Tetreault et al., 2014</ref>), one of the main con- tributions of this paper is the provision of a dataset of 10 human expert annotations, annotated in the tradition of <ref type="bibr">CoNLL-2014</ref>, that is moreover anno- tated for all error types. <ref type="bibr">1</ref> With this new dataset, we have, for the first time, been able to compare system output against the gold standard annotations of a larger group of human annotators, in a realistic grammar check- ing scenario, and consequently been able to quan- tify the extent to which additional annotators af- fect system performance. Additionally, we also noticed that some annotators tend to agree on cer- tain error categories more than others and so at- tempt to explain this.</p><p>In light of the results, we also explore how hu- man annotators themselves compare against the combined annotations of the remaining annotators and thus calculate an upper bound F 0.5 score for the given dataset and number of annotators; e.g., if one human versus nine other humans is only able to score a maximum of 70% F 0.5 , then it is unrea- sonable to expect a machine to do better. For this reason, we propose a more informative method of evaluating a system based on the ratio of that sys- tem's F 0.5 score against the equivalent human F 0.5 score.</p><p>Section 2 contains an overview of some of the latest research in both GEC and SMT that makes use of IAA statistics. Section 3 shows an example sentence from our dataset and qualitatively anal- yses how individual annotator bias affects their choice of corrections. Section 4 describes the data collection process and presents some preliminary results. Section 5 discusses the main quantitative results of the paper, formalizing the formulas used and introducing the more informative method of ratio scoring for GEC, while Section 6 summa- rizes the results from our additional experiments on category agreement and essay subsets. Section 7 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Inter-Annotator Agreement (IAA)</head><p>Whenever we discuss multiple annotators, re- searchers invariably raise the issue of inter- annotator agreement (IAA), or rather the extent to which annotators agree with each other. This is because data which shows a higher level of agree- ment is often believed to be in some way more reli- able than data which has a lower agreement score. Within GEC, agreement has often been reported in terms of Cohen's-κ <ref type="bibr" target="#b3">(Cohen, 1960)</ref>, although other agreement statistics could also be used. <ref type="bibr">2</ref> In the rest of this section, however, we wish to challenge the use of IAA statistics in GEC and question their value in this field. Specifically, while IAA statistics may be informative in areas where items can be classified into single, well- defined categories, such as in part-of-speech tag- ging, we argue that they are less well-suited to GEC and SMT, where there is often more than one correct answer. For example, two annotators may correct or translate a given sentence in two com- pletely different yet valid ways, but IAA statistics are only able to interpret the alternative answers as disagreements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Inter-Annotator Agreement in GEC</head><p>One important study that made use of κ as a mea- sure of agreement between raters is by <ref type="bibr" target="#b21">Tetrault and</ref><ref type="bibr">Chodorow (2008) (also in Tetreault et al. (2014)</ref>), who asked two native English speakers to insert a missing preposition into 200 randomly chosen, well-formed sentences from which a single prepo- sition had been removed.</p><p>Despite the simplicity of this correction task, the authors reported κ-agreement of just 0.7, not- ing that in cases where the raters disagreed, their disagreements were often "licensed by context" and thus actually "acceptable alternatives". This led them to conclude that they would "expect even more disagreement when the task is preposition er- ror detection in 'noisy' learner texts" and, by ex- tension, imply that detection of all error types in 'noisy' texts would show more disagreement still.</p><p>The most important question to ask then, as a result of this study, is whether low κ-scores in 'noisy' texts are truly indicative of real disagree- ment, or whether, as in this preposition test, the disagreement is actually the result of multiple cor- rect answers, and therefore not disagreement at all.</p><p>In a related study, and aware of the fact that there are often multiple ways to correct individual words in sentence, <ref type="bibr" target="#b18">Rozovskaya and Roth (2010)</ref> instead chose to compute agreement at the sen- tence level. Specifically, three raters were asked simply to decide whether they thought 200 sen- tences were correct or not.</p><p>This time, despite operating at the more gen- eral sentence level, the authors reported κ scores of just 0.16, 0.4 and 0.23, surmising that "the low numbers reflect the difficulty of the task and the variability of the native speakers' judgments about acceptable usage." If that is the case, then true dis- agreement may be indistinguishable from native variability, and we should be wary of using IAA statistics as a measure of agreement or evaluation in GEC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Inter-Annotator Agreement in SMT</head><p>In fact, the issues regarding the reliability of IAA metrics are not unique to GEC and we can also draw a parallel with the field of statistical machine translation (SMT). In the same way that there is often more than one way to correct a sentence in GEC, it is also well known that there is often more than one way to translate a sentence in SMT.</p><p>Nevertheless, while several papers have suc- cessfully discussed ways to minimize annotator bias effects in SMT ( <ref type="bibr" target="#b20">Snover et al., 2006;</ref><ref type="bibr" target="#b13">Madnani et al., 2008</ref>), IAA metrics such as κ still unhelp- fully play a role in the field and have, for exam- ple, been reported almost every year in the Work- shop on Machine Translation (WMT) conference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source:</head><p>To put it in the nutshell, I believe that people should have the obligation to tell their relatives about the genetic testing result for the good of their health.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1</head><p>To put it in a nutshell, I believe that people should be obliged to tell their relatives about their genetic test results for the good of their health.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2</head><p>In a nutshell, I believe that people should have an obligation to tell their relatives about the genetic testing result for the good of their health.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A3</head><p>In summary, I believe that people should have the obligation to tell their relatives about the genetic testing result for the good of their health.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A4</head><p>In a nutshell, I believe that people should be obligated to tell their relatives about the genetic testing result for the good of their health.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A5</head><p>To put it in a nutshell, I believe that people should be obligated to tell their relatives about the genetic testing results for the good of their health.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A6</head><p>To put it in the nutshell, I believe that people should have an obligation to tell their relatives about their genetic test results for the good of their health.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A7</head><p>To put it in a nutshell, I believe that people should have the obligation to tell their relatives about the genetic testing result for the good of their health.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A8</head><p>To put it in a nutshell, I believe that people should be obligated to tell their relatives about the genetic testing result for the good of their health.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A9</head><p>To put it in a nutshell, I believe that people should have the obligation to tell their relatives about the genetic test result for the good of their health.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A10</head><p>To put it in a nutshell, I believe that people should have the obligation to tell their relatives about the genetic test results for the good of their health. <ref type="table">Table 1</ref>: <ref type="table">Table showing</ref> how each of the 10 annotators edited the same source sentence in Essay 25. The words in the source sentence that were changed are highlighted in bold.</p><p>This is in spite of the fact that the average inter- annotator κ score across all language pairs over the past five years has never been higher than 0.4 ( <ref type="bibr" target="#b1">Bojar et al., 2014)</ref>. One important paper that attempts to explain why IAA metrics score so poorly in SMT is by <ref type="bibr" target="#b12">Lommel et al. (2014)</ref>, who asked annotators to highlight and categorize sections of automatically translated text they believed to be erroneous. Their results showed that while annotators were often able to agree on the rough locations of errors, they often disagreed as to the specific boundaries of those errors: for instance, given the phrase "had go", some annotators considered just the partici- ple "go" → "gone" to be the minimal error, while others considered the whole verbal unit, "had go" → "had gone", to be the minimal error. Simi- larly, the authors also noted that annotators some- times had problems categorizing ambiguous errors which could be classified into more than one error category.</p><p>In short, while annotators already vary as to what they consider an error, these observations show that even when they do apparently agree, there is no guarantee that every annotator will de- fine the error in exactly the same terms. This poses a problem for IAA statistics, which rely on an ex- act match to measure agreement.</p><p>Finally, it is also worth mentioning that a related study, by <ref type="bibr" target="#b8">Denkowski and Lavie (2010)</ref>, suggested that "annotators also have difficulty agreeing with themselves" (shown from intra-annotator agree- ment κ scores of about 0.6), and so we should be especially wary of using IAA metrics to validate datasets that may even be unreliable for a single annotator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Annotator Bias</head><p>In an effort to better understand how annotators' judgments might differ, we first carried out a small-scale qualitative analysis on a handful of random sentences corrected by the 10 human an- notators in our dataset. One such sentence, and all its various corrections, is shown in <ref type="table">Table 1</ref>.</p><p>It is interesting to note that, for even as short an idiom as "To put it in the nutshell', there are still multiple alternative edits. Although 8 out of the 10 annotators elected to replace the article "the" with "a", among them, A2 and A4 also deleted "To put it" from the expression. Of the remaining 2 an- notators, A3 chose to replace the idiom entirely with "In summary", while A6 made no correction at all. Although no correction appears to be un- acceptable to the majority of annotators, it is also not completely ungrammatical (just idiomatically awkward) so it may be that A6 has a higher tol- erance for this kind of error than the other anno- tators. Alternatively, there is also always the pos- sibility that, given such a large amount of text to correct, this error was simply overlooked.</p><p>Another noteworthy difference is that annota- tors A1, A4, A5, and A8 all elected to change the verb "have the obligation" from active to passive, although A1 still disagreed with the others on the form of the participle. Similarly, there is also a great difference of opinion on whether "testing re- sult" should be corrected or not, and if so, how. While half of the annotators left the phrase un- changed, A1, A6, and A10 all changed both words to "test results". Meanwhile, somewhere in be- tween, A5 decided to change "result" to "results", but not "testing" to "test", while, conversely, A9 decided to do the opposite. This would suggest that error correction of even minor phrases falls along a continuum governed by each annotator's natural bias.</p><p>Finally, one of the most important results of this qualitative evaluation is that even though all 10 annotators edited the same sentence to a level they deemed grammatical, not one single annota- tor agreed with another exactly. This fact alone suggests IAA statistics are not a good way to eval- uate GEC data and that a more robust agreement metric must take into account the possibility of al- ternative correct answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Data Collection</head><p>The raw text data in our dataset was originally pro- duced by 25 students at the National University of Singapore (NUS) who were non-native speakers of English. They were asked to write two essays on the topics of genetic testing and social media respectively. All essays were of similar length and quality. This was important because varying the skill level of the essays is likely to further affect the natural bias of the annotators, who may then consistently over-or under-correct essays. These raw essays also formed the basis of the CoNLL- 2014 test data ( <ref type="bibr" target="#b16">Ng et al., 2014)</ref>. See <ref type="table">Table 2</ref> for some basic statistics on the resulting 50 essays.</p><p>The 10 annotators who annotated all 50 essays include: the 2 official annotators of CoNLL-2014, the first author of this paper, and 7 freelancers who were recruited via online recruitment website, Elance. 3 All annotators are native British English speakers, many of whom also have backgrounds in English language teaching, proofreading, and/or Linguistics.</p><p>All annotations were made using an online an- notation platform, WAMP, especially designed for annotating ESL errors ( <ref type="bibr" target="#b5">Dahlmeier et al., 2013)</ref>. Using this platform, annotators were asked to 3 http://www.elance.com <ref type="table">Table 2</ref>: Statistics for the 50 unannotated essays.</p><note type="other">Total Average per essay # Paragraphs 252 5.0 # Sentences 1312 26.2 # Tokens 30144 602.9</note><p>highlight a minimal error string in the source text, provide an appropriate correction, and then cate- gorize their selection according to the same 28- category error framework used by CoNLL-2014. Before commencing annotation, however, each annotator was given detailed instructions on how to use the tool, along with an explanation of each of the error categories. In cases of uncertainty, an- notators were also encouraged to ask questions. As it was slightly harder to control the qual- ity of the 7 independently recruited annotators via Elance, they were each preliminarily asked to an- notate only the first two essays before being given detailed feedback on their work. The main pur- pose of this feedback was to make sure that they a) understood the error category framework, and b) knew how to deal with more complicated cases such as word insertions, punctuation, etc. Unless it was felt that they had overlooked an obvious er- ror in these first two essays, the feedback did not go so far as to tell annotators what they should and should not highlight in an effort to preserve indi- vidual annotator bias.</p><p>In all, while the specific time taken to complete annotation of all 50 essays was not calculated, all annotators completed the task over a period of about 3 weeks, at a rate of about 45 minutes per essay.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Early Observations</head><p>To investigate the extent to which different anno- tators have different biases, we first counted the total number of edits made by each annotator and sorted them by error category <ref type="table" target="#tab_1">(Table 3)</ref>.</p><p>As can be seen, there is quite a difference be- tween the annotator who made the most edits (A1) and the annotator who made the fewest edits (A7), with A1 making more than twice the number of edits as A7. This just goes to show how varied judgments on grammaticality can be. Incidentally, annotators A3 and A7, who are among those who made the fewest edits, were also the two official gold standard annotators in <ref type="bibr">CoNLL-2014</ref>.</p><p>There is also a large difference between edits in <ref type="table" target="#tab_1">Category   A1  A2  A3  A4  A5  A6  A7  A8  A9  A10  Total  ArtOrDet  879  639  443  503  665  620  331  358  390  624  5452  Cit  0  0  0  0  0  1  0  2  0  0  3  Mec  227  376  493  325  411  336  228  733  598  780  4507  Nn  404  290  228  264  360  300  215  254  277  365  2957  Npos  21  21  15  21  31  28  19  25  29  23  233  Others  42  186  49  116  95  43  44  34  125  105  839  Pform  431  52  18  57  30  83  47  53  19  18  808  Pref  4  79  153  18  223  53  96  92  250  180  1148  Prep  755  488  390  421  502  556  211  276  362  459  4420  Rloc- 488  308  199  331  187  244  94  174  296  240  2561  Sfrag  1  5  1  3  1  5  13  2  12  2  45  Smod  1  4  5  0  1  0  0  3  1  1  16  Spar  0  18  24  0  2  11  3  2  8  0  68  Srun  157  38  21  16  17  18  7  15  17  37</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Quantitative Analysis</head><p>In the main phase of experimentation, we first in- vestigated how different numbers of annotators af- fected the performance of various systems in the context of the CoNLL-2014 shared task. To do this, we downloaded the official system output of all the participating teams 4 and then the Max- Match (M2) Scorer 5 (Dahlmeier and Ng, 2012), which was the official scorer of the previous CoNLL-2013 and CoNLL-2014 shared tasks. This scorer evaluates a system at the sentence level in terms of correct edits, proposed edits, and gold edits, and uses these to calculate an F-score for each team. When more than one set of gold standard annotations is available, the scorer will calculate F-scores for each alternative gold-standard sentence and choose the one from whichever annotator scored the highest. As in CoNLL-2014, we calculate F 0.5 , which weights precision twice as much as recall, because it is more important for a system to be accurate than to correct every possible error. See ( <ref type="bibr" target="#b16">Ng et al., 2014</ref>) for more details on how F 0.5 is calculated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Pairwise Evaluation</head><p>In order to quantify how much the F-score can vary in a realistic grammar checking scenario when there is only one gold standard annotator, we first computed the scores for a participating sys- tem vs each annotator in a pairwise fashion. <ref type="table" target="#tab_3">Table  4</ref> hence shows how the top team in <ref type="bibr">CoNLL-2014</ref><ref type="bibr">, CAMB (Felice et al., 2014</ref>), performed against each of the 10 human annotators individually.</p><p>While Tetrault and Chodorow (2008) and <ref type="bibr" target="#b22">Tetreault et al. (2014)</ref> reported a difference of 10% precision and 5% recall between their two individ- ual annotators in their simplified preposition cor- rection task, <ref type="table" target="#tab_3">Table 4</ref> shows this difference can ac- tually be as much as almost 15% precision (A1 vs A7) and 6% recall (A1 vs A3) in a more realistic full scale correction task. This equates to a differ-  ence of over 7% F 0.5 (A3 vs A7) and once again shows how varied annotator's judgments can be.</p><formula xml:id="formula_0">CAMB P R F 0.5<label>A1</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">All Combinations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Human vs Human</head><p>Whereas previously we could only calculate F 0.5 scores on a system vs human basis, when there are two or more annotators, we can also calculate scores on a human vs human basis. In fact, as the number of annotators increases, we can also start to calculate scores against different combinations of gold standard annotations. <ref type="bibr">6</ref> To give an example, since we have 10 annota- tors, a subset of these annotators, say annotators a2-a8, could be chosen as the gold standard anno- tations. We could then evaluate how each of the re- maining annotators (i.e., annotator a1, a9, and a10) performs against this gold standard, by comput- ing the M2 score for annotator a1 against annota- tors a2-a8, annotator a9 against annotators a2-a8, and annotator a10 against annotators a2-a8. We then average these 3 M2 scores, to determine how, on average, an annotator performs when measured against gold standard annotators a2-a8.</p><p>It is worth reiterating, however, that when more than one annotator is used as the gold standard, the M2 scorer will choose whichever annotator for the given sentence produces the highest F-score; i.e., if a2-a8 are the gold standard and we want to compute the F-score for a9, the M2 scorer will compute a9 vs a2, a9 vs a3, . . . , a9 vs a8 separately for each sentence, and choose the highest.</p><p>The above calculations can be formalized as Equation 1:</p><formula xml:id="formula_1">g(X) = 1 |A| − |X| a∈A\X f (a, X) (1)</formula><p>where A is the set of all annotators (|A| = 10 in our case) and X is a non-empty and proper subset of A, denoting the set of annotators chosen to be in the gold standard. The function f (a, X) is the score computed by the M2 scorer to evaluate anno- tator a against each set of gold standard annotators X. g(X) is thus the average M2 scores for the re- maining annotators against the input gold standard combination X. So far, in our example, we have chosen anno- tators a2-a8 to be the gold standard. There are, however, many other different ways of choosing 7 annotators to serve as the gold standard. For exam- ple, we could have chosen { a1, a2, . . . , a7 }, { a1, a3, a4, . . . , a8 }, etc. In fact, there are 10</p><formula xml:id="formula_2">7 = 120</formula><p>different combinations of 7 annotators. As such, we can also compute how an individual human an- notator performs when measured against any com- bination of 7 gold standard annotators, by averag- ing these 120 M2 scores. The above calculation is formalized in the general case in Equation 2:</p><formula xml:id="formula_3">h i = 1 |A| |X| X:|X|=i g(X)<label>(2)</label></formula><p>where |A| |X| is the binomial coefficient for |A| choose |X| and 1 ≤ i &lt; |A|. The function g(X) is defined in Equation 1.</p><p>The resulting h i values are hence the average F 0.5 scores achieved by any human against any combination of i other humans, and so, in some ways, also represent the upper bound of human performance on the current dataset. The specific values for h i are shown in the second column of <ref type="table" target="#tab_5">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Caveat</head><p>One caveat regarding this method is that the num- ber of all possible combinations of annotators is of the order 2 |A| , which quickly becomes compu- tationally expensive for large values of |A|. Fortu- nately however, in a realistic GEC evaluation sce- nario, it is only the last row of <ref type="table" target="#tab_5">Table 5</ref> that we are most interested in, and so it is actually only neces- sary to calculate a much more manageable |A| |A|−1 gold standard combinations, which is conveniently  equal to the total number of annotators. We only compute all combinations here in order to quan- tify, for the first time, how much each additional annotator affects performance.</p><formula xml:id="formula_4">Gold Human (h i ) AMU CAMB CUUI Annotators (i) Avg F 0.5 Avg F 0.5 Ratio Avg F 0.5 Ratio Avg F 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">System vs Human</head><p>In addition to calculating scores on a human vs human basis, we also calculated the F-scores for the top three CoNLL-2014 teams, AMU (Junczys- Dowmunt and Grundkiewicz, 2014), CAMB <ref type="bibr" target="#b9">(Felice et al., 2014</ref>), and CUUI ( <ref type="bibr" target="#b19">Rozovskaya et al., 2014)</ref>, versus all the combinations of humans (Equation 3).</p><formula xml:id="formula_5">s i = 1 |A| |X| X:|X|=i f (s, X)<label>(3)</label></formula><p>Specifically, s ∈ S, where S is the set of all three shared task systems, i.e., {AMU, CAMB, CUUI}, and f (s, X) is the same function in Equa- tion 1 which is the score computed by the M2 scorer to evaluate system s against the set of an- notators X chosen to be in the gold standard. The average F 0.5 scores for each of the team's systems versus increasing numbers of i annotators are also shown in <ref type="table" target="#tab_5">Table 5</ref>.</p><p>We notice from these scores that, as expected, both system and human performance increases as more annotators are used in a gold standard. We do now, however, have data that quantifies exactly how much each additional annotator affects the score. This effect can be more clearly seen in <ref type="figure" target="#fig_0">Fig- ure 1</ref>.</p><p>It is important to note, however, that even with 9 annotators, human output itself does not reach close to 100% F 0.5 and instead, the difference be- tween the systems and the humans is about 20% F 0.5 . Furthermore, the curves for humans and sys- tems also remain roughly parallel, suggesting hu- man corrections gain as much benefit as system corrections from larger sets of gold standard an- notations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ratio Scoring</head><p>In light of the above observation that even humans vs humans are unable to score 100% F 0.5 , it thus seems unreasonable to expect machines to do the same. As such, we propose that it is much more informative to score system output against the av- erage performance of humans instead of against the theoretical maximum score. The ratio values for the three CoNLL-2014 teams against the hu- man gold standards of various sizes are hence also reported in <ref type="table" target="#tab_5">Table 5</ref>. The most important thing to note is that these figures are not only much higher than the low F 0.5 values currently reported in the literature, they are also more representative of the state of the art. For instance, it is highly significant that we can report that the top system in CoNLL- 2014, CAMB, is actually able to perform 73% as reliably as a human, which suggests GEC may ac- tually be a more viable technology than was pre- viously thought.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Additional Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Error Categories</head><p>As well as carrying out experiments at the system level, we also carried out similar experiments at the error category level. More specifically, we re- calculated the values of Equation 1 and 2 for cases where the set of annotations consisted of only a single specific error type. Since the participating teams in <ref type="bibr">CoNLL-2014</ref> were not asked to classify the type of errors their systems corrected, we were only able to calculate these new values using the 10 sets of human annotations.</p><p>Like <ref type="figure" target="#fig_0">Figure 1</ref>, we can see from <ref type="figure" target="#fig_1">Figure 2</ref> that the F 0.5 performance of individual error types in- creases diminishingly as the number of annotators in the gold standard also increases. More impor- tantly, however, we notice that some error types achieve much higher scores than others, which suggests some annotators agree on certain cate- gories more than others.</p><p>In particular, noun number (Nn) and subject- verb agreement (SVA) errors achieve the highest scores, at just under 90% F 0.5 , which is also not far from the 100% F 0.5 that would be achieved if we had gold standard answers for all possible al- ternative corrections of this type. The most likely reason for this is that, as the correction of these error types typically only involves the addition or removal of an -s suffix, i.e., a minor change in number morphology, there is very little room for annotators to disagree.</p><p>In contrast, the next highest category, article and determiner errors (ArtOrDet), has a slightly larger confusion set, {the, a/an, }, which may account for the slightly lower score. Similarly, the next group of error categories, spelling and punctuation (Mec), verb tense (Vt), and word form (Wform), which all often involve a similar type of edit op- eration to a word lemma, likewise have slightly larger confusion sets that include a larger variety of possible morphological inflections. It is likely that the next category, prepositions (Prep), also has a confusion set of a similar size.</p><p>The last three categories, conjunctions (all- types) (Trans), word order (WOinc) and word choice (Wci), are all notable because they per- form significantly worse than the hitherto men- tioned categories. The main reason for this is that these error types all typically have a scope much larger than most other categories in that they often involve changes at the structural or semantic level; e.g., changing an active to a passive or choosing a synonym. For this reason, there are often many more alternative ways to correct them, meaning they are also much more likely to be affected by annotator bias. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Essay Subsets</head><p>Now that we had empirical evidence showing how F 0.5 scores varied with the number of annotators, an additional question to ask was whether the same trends for 50 essays were also present in a smaller subset of essays. We therefore repeated the main experiment with all error types, but this time used just 10 essays (specifically, essays 1-10) in both the hypothesis and gold standard. The results are shown in <ref type="figure" target="#fig_2">Figure 3</ref>.</p><p>Compared to <ref type="figure" target="#fig_0">Figure 1</ref>, the most significant dif- ference between these two graphs is that the rank- ing for AMU and CUUI has changed, although not by much in terms of F 0.5 . The most likely reason for this is that the distribution of error types in the smaller subset of essays is better suited to AMU's more general SMT approach than to CUUI's more targeted classifier based approach. For instance, see <ref type="table">Table 9</ref> in <ref type="bibr" target="#b16">Ng et al. (2014)</ref> to compare each team's performance on different error types in the CoNLL-2014 shared task.</p><p>In other words, while the overall relationship between the system and human scores on 10 and 50 essays remains more or less the same, re- searchers must be aware that smaller datasets may have more skewed error distributions, which in turn may affect system performance, dependent upon correction strategy. With a balanced test set though, it would seem feasible to carry out future evaluation research on as few as 10 essays (about 6000 words).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>To summarize, we first showed that 10 individual annotators can all correct the same sentence in 10 different ways, yet also all produce valid alterna- tives. This implies that inter-annotator agreement statistics, which rely on exact matching, are not well-suited to grammatical error correction, be- cause it may not be the case that annotators truly disagree, but rather that they have a bias towards a particular type of alternative answer.</p><p>We next showed that, as has long been sus- pected, increasing the number of annotators in the gold standard also leads to an increase in F 0.5 , al- though at a diminishing rate. This data can be used to help researchers decide how many gold standard annotations should be used in GEC evaluation.</p><p>The main result of this paper however, is that by computing scores for human against human, we determined that it is not true that any human cor- rection is able to score 100% F 0.5 . Instead, we found that the human upper bound is roughly 73% F 0.5 and that the top 3 teams from CoNLL-2014 actually perform, on average, between 67-73% as reliably as this human upper bound. This result is highly significant, because it suggests GEC sys- tems may actually be more viable than their previ- ously low F 0.5 scores would suggest.</p><p>In addition to the above, we also found that hu- mans tend to agree on some error categories more than others, and suggest that one of the main rea- sons for this concerns the size of the confusion set of the particular error type.</p><p>Finally, not only are we making the corrections by 10 annotators of all 50 essays available with this paper, we also showed that the trends found in the data are also consistent with the annotations of just 10 essays, allowing future research to be conducted on much less text.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Graph showing how average F 0.5 scores for humans and systems increase as the number of gold standard annotators also increases (all error types, 50 Essays).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Graph showing how average F 0.5 scores for various error categories increase as the number of gold standard annotators also increases (50 essays). Calculations based on human annotations only.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Graph showing how average F 0.5 scores for humans and systems increase as the number of gold standard annotators also increases (all error types, 10 Essays).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Table showing how many annotations each annotator made in terms of error category. See Ng et al. (2014) Table 1 for a more detailed description of error categories.</figDesc><table>terms of category use, with almost half of all edits 
falling into the categories for article or determiner 
(ArtOrDet), spelling or punctuation (Mec), prepo-
sition (Prep), or word choice (Wci) errors. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Table showing the F 0.5 scores for the top 
team in CoNLL-2014, CAMB, against each of the 
10 annotators individually. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Table showing average human F 0.5 scores over all combinations of 1 ≤ i &lt; 10 gold annotators 
compared to the same averages for the top 3 systems in CoNLL-2014, and the ratio percentage of each 
team's average score versus the human average score. 

</table></figure>

			<note place="foot" n="1"> http://www.comp.nus.edu.sg/ ˜ nlp/sw/ 10gec_annotations.zip</note>

			<note place="foot" n="2"> See Hayes and Krippendorff (2007) or Artstein and Poesio (2008) for the pros and cons of different IAA metrics.</note>

			<note place="foot" n="4"> http://www.comp.nus.edu.sg/ ˜ nlp/ conll14st/official_submissions.tar.gz 5 http://www.comp.nus.edu.sg/ ˜ nlp/sw/ m2scorer.tar.gz</note>

			<note place="foot" n="6"> Note that by combinations of annotators, we mean simply that the M2 scorer has access to a larger number of alternative gold standard corrections; we do not attempt to merge annotations in any way.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research is supported by Singapore Ministry of Education Academic Research Fund Tier 2 grant MOE2013-T2-1-150. We would also like to thank the three anonymous reviewers for their comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Inter-coder agreement for computational linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Artstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="555" to="596" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Findings of the 2014 Workshop on Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Pecina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Saint-Amand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleš</forename><surname>Tamchyna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation<address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="12" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Problems in evaluating grammatical error detection systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Israel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">R</forename><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="611" to="628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A coefficient of agreement for nominal scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Educational and Psychological Measurement</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="46" />
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Better evaluation for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLTNAACL</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="568" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of learner English: The NUS Corpus of Learner English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siew Mei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the Eighth Workshop on Innovative Use of NLP for Building Educational Applications<address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="22" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Helping Our Own: The HOO 2011 pilot shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kilgarriff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Generation Challenges Session at the 13th European Workshop on Natural Language Generation</title>
		<meeting>the Generation Challenges Session at the 13th European Workshop on Natural Language Generation</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="242" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Helping Our Own: HOO 2012: A report on the preposition and determiner error correction shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Anisimoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Narroway</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the Seventh Workshop on Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="54" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Choosing the right evaluation for machine translation: an examination of annotator and automatic metric performance on human judgment tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of AMTA</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Grammatical error correction using hybrid systems and type filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariano</forename><surname>Felice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Øistein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kochmar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning: Shared Task</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="15" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Answering the call for a standard reliability measure for coding data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krippendorff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communication Methods and Measures</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="89" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The AMU system in the CoNLL-2014 shared task: Grammatical error correction by dataintensive and feature-rich statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Dowmunt</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning: Shared Task</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="25" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Assessing inter-annotator agreement for translation error annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Arle Richard Lommel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljoscha</forename><surname>Popovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Burchardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MTE: Workshop on Automatic and Manual Metrics for Operational Translation Evaluation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Are multiple reference translations necessary? Investigating the value of paraphrased reference translations in parameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Madnani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><forename type="middle">J</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Conference of the Association for Machine Translation in the Americas</title>
		<meeting>the Eighth Conference of the Association for Machine Translation in the Americas</meeting>
		<imprint>
			<date type="published" when="2008-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">They can help: Using crowdsourcing to improve the evaluation of grammatical error detection systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Madnani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">R</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alla</forename><surname>Rozovskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="508" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The CoNLL-2013 shared task on grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hwee Tou Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Siew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">R</forename><surname>Hadiwinoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning: Shared Task<address><addrLine>Sofia, Bulgaria. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The CoNLL-2014 shared task on grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hwee Tou Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Siew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">Hendy</forename><surname>Hadiwinoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Susanto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bryant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning: Shared Task<address><addrLine>Baltimore, Maryland, USA. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Crowdsourcing for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Companion Publication of the 17th ACM Conference on Computer Supported Cooperative Work and Social Computing, CSCW Companion &apos;14</title>
		<meeting>the Companion Publication of the 17th ACM Conference on Computer Supported Cooperative Work and Social Computing, CSCW Companion &apos;14<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="209" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Annotating ESL errors: Challenges and rewards</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alla</forename><surname>Rozovskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="28" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The IllinoisColumbia system in the CoNLL-2014 shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alla</forename><surname>Rozovskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sammons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning: Shared Task</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="34" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A study of translation error rate with targeted human annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Snover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linnea</forename><surname>Micciulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Machine Transaltion in the Americas</title>
		<meeting>the Association for Machine Transaltion in the Americas</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Native judgments of non-native usage: Experiments in preposition error detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">R</forename><surname>Tetrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING Workshop on Human Judgments in Computational Linguistics</title>
		<meeting><address><addrLine>Manchester, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="24" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bucking the trend: improved evaluation and annotation practices for ESL error detection systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">R</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Madnani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="31" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
