<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Some of Them Can be Guessed! Exploring the Effect of Linguistic Context in Predicting Quantifiers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandro</forename><surname>Pezzelle</surname></persName>
							<email>* sandro.pezzelle@unitn.it, † s.n.m.steinert-threlkeld@uva.nl, * ‡ raffaella.bernardi@unitn.it, † j.k.szymanik@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">CIMeC -Center for Mind/Brain Sciences, ‡ DISI</orgName>
								<orgName type="department" key="dep2">Language and Computation</orgName>
								<orgName type="institution" key="instit1">University of Trento † ILLC -Institute for Logic</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Steinert-Threlkeld</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">CIMeC -Center for Mind/Brain Sciences, ‡ DISI</orgName>
								<orgName type="department" key="dep2">Language and Computation</orgName>
								<orgName type="institution" key="instit1">University of Trento † ILLC -Institute for Logic</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">CIMeC -Center for Mind/Brain Sciences, ‡ DISI</orgName>
								<orgName type="department" key="dep2">Language and Computation</orgName>
								<orgName type="institution" key="instit1">University of Trento † ILLC -Institute for Logic</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Szymanik</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">CIMeC -Center for Mind/Brain Sciences, ‡ DISI</orgName>
								<orgName type="department" key="dep2">Language and Computation</orgName>
								<orgName type="institution" key="instit1">University of Trento † ILLC -Institute for Logic</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Some of Them Can be Guessed! Exploring the Effect of Linguistic Context in Predicting Quantifiers</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="114" to="119"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>114</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We study the role of linguistic context in predicting quantifiers (&apos;few&apos;, &apos;all&apos;). We collect crowdsourced data from human participants and test various models in a local (single-sentence) and a global context (multi-sentence) condition. Models significantly out-perform humans in the former setting and are only slightly better in the latter. While human performance improves with more linguistic context (especially on proportional quanti-fiers), model performance suffers. Models are very effective in exploiting lexical and morpho-syntactic patterns; humans are better at genuinely understanding the meaning of the (global) context.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A typical exercise used to evaluate a language learner is the cloze deletion test <ref type="bibr" target="#b14">(Oller, 1973)</ref>. In it, a word is removed and the learner must replace it. This requires the ability to understand the con- text and the vocabulary in order to identify the correct word. Therefore, the larger the linguistic context, the easier the test becomes. It has been recently shown that higher-ability test takers rely more on global information, with lower-ability test takers focusing more on the local context, i.e. in- formation contained in the words immediately sur- rounding the gap <ref type="bibr" target="#b8">(McCray and Brunfaut, 2018)</ref>.</p><p>In this study, we explore the role of linguis- tic context in predicting generalized quantifiers ('few', 'some', 'most') in a cloze-test task (see <ref type="figure">Figure 1</ref>). Both human and model performance is evaluated in a local (single-sentence) and a global context (multi-sentence) condition to study the role of context and assess the cognitive plau- sibility of the models. The reasons we are inter- <ref type="figure">Figure 1</ref>: Given a target sentence s t , or s t with the preceding and following sentence, the task is to predict the target quantifier replaced by &lt;qnt&gt;.</p><p>ested in quantifiers are myriad. First, quantifiers are of central importance in linguistic semantics and its interface with cognitive science <ref type="bibr" target="#b1">(Barwise and Cooper, 1981;</ref><ref type="bibr" target="#b17">Peters and Westerståhl, 2006;</ref><ref type="bibr" target="#b23">Szymanik, 2016)</ref>. Second, the choice of quanti- fier depends both on local context (e.g., positive and negative quantifiers license different patterns of anaphoric reference) and global context (the de- gree of positivity/negativity is modulated by dis- course specificity) ( <ref type="bibr" target="#b16">Paterson et al., 2009</ref>). Third and more generally, the ability of predicting func- tion words in the cloze test represents a bench- mark test for human linguistic competence <ref type="bibr" target="#b21">(Smith, 1971;</ref><ref type="bibr" target="#b3">Hill et al., 2016)</ref>.</p><p>We conjecture that human performance will be boosted by more context and that this effect will be stronger for proportional quantifiers (e.g. 'few', 'many', 'most') than for logical quantifiers (e.g. 'none', 'some', 'all') because the former are more dependent on discourse context <ref type="bibr" target="#b11">(Moxey and Sanford, 1993;</ref><ref type="bibr" target="#b22">Solt, 2016)</ref>. In contrast, we expect models to be very effective in exploiting the lo- cal context ( <ref type="bibr" target="#b3">Hill et al., 2016</ref>) but to suffer with a broader context, due to their reported inability to handle longer sequences ( <ref type="bibr" target="#b15">Paperno et al., 2016)</ref>. Both hypotheses are confirmed. The best mod-els are very effective in the local context condi- tion, where they significantly outperform humans. Moreover, model performance declines with more context, whereas human performance is boosted by the higher accuracy with proportional quanti- fiers like 'many' and 'most'. Finally, we show that best-performing models and humans make similar errors. In particuar, they tend to confound quanti- fiers that denote a similar 'magnitude' ( <ref type="bibr" target="#b2">Bass et al., 1974;</ref><ref type="bibr" target="#b13">Newstead and Collis, 1987)</ref>.</p><p>Our contribution is twofold. First, we present a new task and results for training models to learn semantically-rich function words. 1 Second, we analyze the role of linguistic context in both hu- mans and the models, with implications for cogni- tive plausibility and future modeling work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Datasets</head><p>To test our hypotheses, we need linguistic con- texts containing quantifiers. To ensure similarity in the syntactic environment of the quantifiers, we focus on partitive uses: where the quantifier is fol- lowed by the preposition 'of'. To avoid any effect of intensifiers like 'very' and 'so' and adverbs like 'only' and 'incredibly', we study only sentences in which the quantifier occurs at the beginning (see <ref type="figure">Figure 1</ref>). We experiment with a set of 9 quan- tifiers: 'a few', 'all', 'almost all', 'few', 'many', 'more than half', 'most', 'none', 'some'. This set strikes the best trade-off between number of quantifiers and their frequency in our source cor- pus, a large collection of written English including around 3B tokens. <ref type="bibr">2</ref> We build two datasets. One dataset -1-Sent - contains datapoints that only include the sentence with the quantifier (the target sentence, s t ). The second -3-Sent -contains datapoints that are 3-sentence long: the target sentence (s t ) together with both the preceding (s p ) and following one (s f ). To directly analyze the effect of the linguis- tic context in the task, the target sentences are ex- actly the same in both settings. Indeed, 1-Sent is obtained by simply extracting all target sentences &lt;s t &gt; from 3-Sent (&lt;s p , s t , s f &gt;).</p><p>The 3-Sent dataset is built as follows: (1) We split our source corpus into sentences and select those starting with a 'quantifier of' construction. Around 391K sentences of this type are found. <ref type="formula">(2)</ref> We tokenize the sentences and replace the quan- tifier at the beginning of the sentence (the target quantifier) with the string &lt;qnt&gt;, to treat all tar- get quantifiers as a single token. (3) We filter out sentences longer than 50 tokens (less than 6% of the total), yielding around 369K sentences. (4) We select all cases for which both the preceding and the following sentence are at most 50-tokens long. We also ensure that the target quantifier does not occur again in the target sentence. <ref type="formula">(5)</ref> We ensure that each datapoint &lt;s p , s t , s f &gt; is unique. The dis- tribution of target quantifiers across the resulting 309K datapoints ranges from 1152 cases ('more than half') to 93801 cases ('some'). To keep the dataset balanced, we randomly select 1150 points for each quantifier, resulting in a dataset of 10350 datapoints. This was split into train (80%), valida- tion (10%), and test (10%) sets while keeping the balancing. Then, 1-Sent is obtained by extract- ing the target sentences &lt;s t &gt; from &lt;s p , s t , s f &gt;.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Human Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Method</head><p>We ran two crowdsourced experiments, one per condition. In both, native English speakers were asked to pick the correct quantifier to replace &lt;qnt&gt; after having carefully read and under- stood the surrounding linguistic context. When more than one quantifier sounds correct, partici- pants were instructed to choose the one they think best for the context. To make the results of the two surveys directly comparable, the same randomly- sampled 506 datapoints from the validation sets are used. To avoid biasing responses, the 9 quan- tifiers were presented in alphabetical order. The surveys were carried out via CrowdFlower. <ref type="bibr">3</ref> Each participant was allowed to judge up to 25 points. To assess the judgments, 50 unambiguous cases per setting were manually selected by the native- English author and used as a benchmark. Over- all, we collected judgments from 205 annotators in 1-Sent (avg. 7.4 judgments/annotator) and from 116 in 3-Sent (avg. 13.1). Accuracy is then computed by counting cases where at least 2 out of 3 annotators agree on the correct answer (i.e., inter-annotator agreement ≥ 0.67).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Linguistic Analysis</head><p>Overall, the task turns out to be easier in 3-Sent (131/506 correctly-guessed cases; 0.258 accu-type text quantifier meaning &lt;qnt&gt; the original station buildings survive as they were used as a source of materials. . . none of PIs &lt;qnt&gt; these stories have ever been substantiated. none of contrast Q &lt;qnt&gt; the population died out, but a select few with the right kind of genetic instability. . . most of list &lt;qnt&gt; their major research areas are social inequality, group dynamics, social change. . . some of quantity &lt;qnt&gt; those polled (56%) said that they would be willing to pay for special events. . . more t. half of support Q &lt;qnt&gt; you have found this to be the case -click here for some of customer comments. many of lexicalized &lt;qnt&gt; the time, the interest rate is set on the lender's terms. . . most of syntax &lt;qnt&gt; these events was serious. none of <ref type="table">Table 1</ref>: Cues that might help human participants to predict the correct quantifier (1-Sent).</p><p>racy) compared to 1-Sent (112/506; 0.221 acc.). Broader linguistic context is thus generally bene- ficial to the task. To gain a better understanding of the results, we analyze the correctly-predicted cases and look for linguistic cues that might be helpful for carrying out the task. <ref type="table">Table 1</ref> reports examples from 1-Sent for each of these cues.</p><p>We identify 8 main types of cues and manually annotate the cases accordingly. (1) Meaning: the quantifier can only be guessed by understanding and reasoning about the context; (2) PIs: Polar- ity Items like 'ever', 'never', 'any' are licensed by specific quantifiers <ref type="bibr" target="#b7">(Krifka, 1995)</ref>; (3) Con- trast Q: a contasting-magnitude quantifier em- bedded in an adversative clause; (4) Support Q: a supporting-magnitude quantifier embedded in a coordinate or subordinate clause; (5) Quantity: explicit quantitative information (numbers, per- centages, fractions, etc.); (6) Lexicalized: lexi- calized patterns like 'most of the time'; (7) List: the text immediately following the quantifier is a list introduced by verbs like 'are' or 'include'; (8) Syntax: morpho-syntactic cues, e.g. agreement.</p><p>Figure 2 (left) depicts the distribution of anno- tated cues in correctly-guessed cases of 1-Sent. Around 44% of these cases include cues besides meaning, suggesting that almost half of the cases can be possibly guessed by means of lexical fac- tors such as PIs, quantity information, etc. As seen in <ref type="figure" target="#fig_0">Figure 2</ref> (right), the role played by the meaning becomes much higher in 3-Sent. Of the 74 cases that are correctly guessed in 3-Sent, but not in 1-Sent, more than 3 out of 4 do not display cues other than meaning. In the absence of lexical cues at the sentence level, the surrounding context thus plays a crucial role.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Models</head><p>We test several models, that we briefly describe below. All models except FastText are im- plemented in Keras and use ReLu as activation function; they are trained for 50 epochs with cat- egorical crossentropy, initialized with frozen 300- d word2vec embeddings ( <ref type="bibr" target="#b10">Mikolov et al., 2013</ref>) pretrained on GoogleNews. <ref type="bibr">4</ref> A thorough ablation study is carried out for each model to find the best configuration of parameters. <ref type="bibr">5</ref> The best configura- tion is chosen based on the lowest validation loss.</p><p>BoW-conc A bag-of-words (BoW) architecture which encodes a text as the concatenation of the embeddings for each token. This representation is reduced by a hidden layer before softmax.</p><p>BoW-sum Same as above, but the text is en- coded as the sum of the embeddings.</p><p>FastText Simple network for text classification that has been shown to obtain performance compa- rable to deep learning models ( <ref type="bibr" target="#b6">Joulin et al., 2016)</ref>. FastText represents text as a hidden variable obtained by means of a BoW representation.</p><p>CNN Simple Convolutional Neural Network (CNN) for text classification. <ref type="bibr">6</ref> It has two con- volutional layers (Conv1D) each followed by MaxPooling. A dense layer precedes softmax.</p><p>LSTM Standard Long-Short Term Memory net- work (LSTM) <ref type="bibr" target="#b4">(Hochreiter and Schmidhuber, 1997</ref>). Variable-length sequences are padded with zeros to be as long as the maximum sequence in the dataset. To avoid taking into account cells padded with zero, the 'mask zero' option is used.</p><p>bi-LSTM The Bidirectional LSTM <ref type="bibr" target="#b20">(Schuster and Paliwal, 1997</ref>) combines information from past and future states by duplicating the first re- current layer and then combining the two hidden states. As above, padding and mask zero are used. Att-LSTM LSTM augmented with an attention mechanism ( <ref type="bibr" target="#b18">Raffel and Ellis, 2016)</ref>. A feed- forward neural network computes an importance weight for each hidden state of the LSTM; the weighted sum of the hidden states according to those weights is then fed into the final classifier.</p><p>AttCon-LSTM LSTM augmented with an at- tention mechanism using a learned context vec- tor ( <ref type="bibr" target="#b24">Yang et al., 2016)</ref>. LSTM states are weighted by cosine similarity to the context vector. <ref type="table">Table 2</ref> reports the accuracy of all models and hu- mans in both conditions. We have three main re- sults. (1) Broader context helps humans to per- form the task, but hurts model performance. This can be seen by comparing the 4-point increase of human accuracy from 1-Sent (0.22) to 3-Sent (0.26) with the generally worse performance of all models (e.g. AttCon-LSTM, from 0.34 to 0.27  <ref type="table">Table 2</ref>: Accuracy of models and humans. Values in bold are the highest in the column. *Note that due to an imperfect balancing of data, chance level for humans (computed as majority class) is 0.124.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>in val). (2) All models are significantly better than humans in performing the task at the sen- tence level (1-Sent), whereas their performance is only slightly better than humans' in 3-Sent.</p><p>AttCon-LSTM, which is the best model in the former setting, achieves a significantly higher ac- curacy than humans' (0.34 vs 0.22). By contrast, in 3-Sent, the performance of the best model is closer to that of humans <ref type="formula">(</ref> , 'a few', 'more than half', 'some', and 'most': while the first three are generally hard for humans but predictable by the models, the last two show the opposite pattern. Moreover, quanti- fiers that are guessed by humans to a larger extent in 3-Sent compared to 1-Sent, thus profiting from the broader linguistic context, do not expe- rience the same boost with models. Human accu- racy improves notably for 'few', 'a few', 'many', and 'most', while model performance on the same quantifiers does not. To check whether humans and the models make similar errors, we look into the distribution of responses in 3-Sent (val), which is the most comparable setting with respect to accuracy. Ta- ble 3 reports responses by humans (top) and AttCon-LSTM (bottom). Human errors gener- ally involve quantifiers that display a similar mag- nitude as the correct one. To illustrate, 'some' is chosen in place of 'a few', and 'most' in place of either 'almost all' or 'more than half'. A simi- lar pattern is observed in the model's predictions, though we note a bias toward 'more than half'.</p><p>One last question concerns the types of linguis- tic cues exploited by the model (see section 3.2). We consider those cases which are correctly guessed by both humans and AttCon-LSTM in each setting and analyze the distribution of anno- tated cues. Non-semantic cues turn out to account for 41% of cases in 3-Sent and for 50% cases in 1-Sent. This analysis suggests that, compared to humans, the model capitalizes more on lexical, morpho-syntactic cues rather than exploiting the meaning of the context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>This study explored the role of linguistic context in predicting quantifiers. For humans, the task be- comes easier when a broader context is given. For the best-performing LSTMs, broader context hurts  performance. This pattern mirrors evidence that predictions by these models are mainly based on local contexts ( <ref type="bibr" target="#b3">Hill et al., 2016</ref>). Corroborating our hypotheses, proportional quantifiers ('few', 'many', 'most') are predicted by humans with a higher accuracy with a broader context, whereas logical quantifiers ('all', 'none') do not experience a similar boost. Interestingly, humans are almost always able to grasp the magnitude of the miss- ing quantifier, even when guessing the wrong one. This finding is in line with the overlapping mean- ing and use of these expressions <ref type="bibr" target="#b11">(Moxey and Sanford, 1993)</ref>. It also provides indirect evidence for an ordered mental scale of quantifiers <ref type="bibr" target="#b5">(Holyoak and Glass, 1978;</ref><ref type="bibr" target="#b19">Routh, 1994;</ref><ref type="bibr" target="#b12">Moxey and Sanford, 2000</ref>). The reason why the models fail with certain quantifiers and not others is yet not clear. It may be that part of the disadvantage in the broader context condition is due to engineering issues, as suggested by an anonymous reviewer. We leave investigating these issues to future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Left: Distribution of annotated cues across correcly-guessed cases in 1-Sent (112 cases). Right: Distribution of cues across correctly-guessed cases in 3-Sent, but not in 1-Sent (74 cases).</figDesc><graphic url="image-2.png" coords="4,124.05,62.81,124.95,143.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>0 .</head><label>0</label><figDesc>29 of Att-LSTM vs 0.26). It can be seen that LSTMs are over- all the best-performing architectures, with CNN showing some potential in the handling of longer sequences (3-Sent). (3) As depicted in Fig- ure 3, quantifiers that are easy/hard for humans are not necessarily easy/hard for the models. Com- pare 'few'</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Human vs AttCon-LSTM accuracy (val) across quantifiers, loosely ordered by magnitude.</figDesc><graphic url="image-4.png" coords="5,130.96,62.81,335.62,203.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Responses by humans (top) and 
AttCon-LSTM (bottom) in 3-Sent (val). Val-
ues in bold are the highest in the row. 

</table></figure>

			<note place="foot" n="1"> Data and code are at: https://github.com/ sandropezzelle/fill-in-the-quant 2 A concatenation of BNC, ukWaC, and a 2009-dump of Wikipedia (Baroni et al., 2014).</note>

			<note place="foot" n="3"> https://www.figure-eight.com/</note>

			<note place="foot" n="4"> Available here: http://bit.ly/1VxNC9t 5 We experiment with all possible combinations obtained by varying (a) optimizer: adagrad, adam, nadam; (b) hidden layers: 64 or 128 units; (c) dropout: 0.25, 0.5, 0.75. 6 Adapted from: http://bit.ly/2sFgOE1</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Marco Baroni, Raquel Fernández, Germán Kruszewski, and Nghia The Pham for their valuable feedback. We thank the NVIDIA Corporation for the donation of GPUs used for this research, and the iV&amp;L Net (ICT COST Ac-tion IC1307) for funding the first author's research visit. This project has received funding from the European Research Council (ERC) under the Eu-ropean Unions Horizon 2020 research and innova-tion programme (grant agreement No 716230).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Don&apos;t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="238" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generalized Quantifiers and Natural Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Barwise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linguistics and Philosophy</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="159" to="219" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Magnitude estimations of expressions of frequency and amount</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><forename type="middle">F</forename><surname>Bass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward J O&amp;apos;</forename><surname>Cascio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Connor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Psychology</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">313</biblScope>
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The Goldilocks Principle: Reading Children&apos;s books with explicit memory representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recognition confusions among quantifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Keith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold L</forename><surname>Holyoak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of verbal learning and verbal behavior</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="249" to="264" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Bag of Tricks for Efficient Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01759</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The semantics and pragmatics of polarity items</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Krifka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linguistic analysis</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="209" to="257" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Investigating the construct measured by banked gap-fill items: Evidence from eyetracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gareth</forename><surname>Mccray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tineke</forename><surname>Brunfaut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language Testing</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="51" to="73" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<idno type="doi">10.1177/0265532216677105</idno>
		<ptr target="https://doi.org/10.1177/0265532216677105" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Communicating Quantities. A psychological perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Linda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony J</forename><surname>Moxey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sanford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>Lawrence Erlbaum Associates Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Communicating quantities: A review of psycholinguistic evidence of how expressions determine perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Linda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony J</forename><surname>Moxey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sanford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="237" to="255" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Context and the interpretation of quantifiers of frequency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janet</forename><forename type="middle">M</forename><surname>Newstead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ergonomics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1447" to="1462" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cloze tests of second language proficiency and what they measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language learning</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="105" to="118" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The LAMBADA dataset: Word prediction requiring a broad discourse context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Paperno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandro</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pezzelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Boleda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fernández</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">B</forename><surname>Paterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruth</forename><surname>Filik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><forename type="middle">M</forename><surname>Moxey</surname></persName>
		</author>
		<title level="m">Quantifiers and Discourse Processing. Language and Linguistics Compass</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dag</forename><surname>Westerståhl</surname></persName>
		</author>
		<title level="m">Quantifiers in Language and Logic</title>
		<meeting><address><addrLine>Oxford</addrLine></address></meeting>
		<imprint>
			<publisher>Clarendon Press</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">FeedForward Networks with Attention Can Solve Some Long-Term Memory Problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ellis</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1512.08756" />
	</analytic>
	<monogr>
		<title level="m">International Conference of Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On representations of quantifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David A Routh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Semantics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="199" to="214" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuldip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Understanding reading: A psycholinguistic analysis of reading and learning to read</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971" />
			<pubPlace>Holt, Rinehart &amp; Winston</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On Measurement and Quantification: The Case of most and more than half</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Solt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page" from="65" to="100" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Quantifiers and Cognition. Logical and Computational Perspectives. Studies in Linguistics and Philosophy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Szymanik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hierarchical Attention Networks for Document Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/N16-1174</idno>
		<ptr target="https://doi.org/10.18653/v1/N16-1174" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT 2016</title>
		<meeting>NAACL-HLT 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
	<note>Xiaodong He, Alex Smola, and Eduard Hovy</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
