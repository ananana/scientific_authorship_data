<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:07+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Two Methods for Domain Adaptation of Bilingual Tasks: Delightfully Simple and Broadly Applicable</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Hangya</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing LMU Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabienne</forename><surname>Braune</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing LMU Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Volkswagen Data Lab Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Fraser</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing LMU Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sch¨</forename><surname>Schütze</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing LMU Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Two Methods for Domain Adaptation of Bilingual Tasks: Delightfully Simple and Broadly Applicable</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="810" to="820"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>810</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Bilingual tasks, such as bilingual lexicon induction and cross-lingual classification, are crucial for overcoming data sparsity in the target language. Resources required for such tasks are often out-of-domain, thus domain adaptation is an important problem here. We make two contributions. First, we test a delightfully simple method for domain adaptation of bilingual word embeddings. We evaluate these embed-dings on two bilingual tasks involving different domains: cross-lingual twitter sentiment classification and medical bilingual lexicon induction. Second, we tailor a broadly applicable semi-supervised classification method from computer vision to these tasks. We show that this method also helps in low-resource setups. Using both methods together we achieve large improvements over our baselines, by using only additional unlabeled data.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper we study two bilingual tasks that strongly depend on bilingual word embeddings (BWEs). Previously, specialized domain adap- tation approaches to such tasks were proposed. We instead show experimentally that a simple adaptation process involving only unlabeled text is highly effective. We then show that a semi- supervised classification method from computer vision can be applied successfully for further gains in cross-lingual classification.</p><p>Our BWE adaptation method is delightfully simple. We begin by adapting monolingual word embeddings to the target domain for source and target languages by simply building them using both general and target-domain unlabeled data. As a second step we use post-hoc mapping <ref type="bibr" target="#b24">(Mikolov et al., 2013b</ref>), i.e., we use a seed lexicon to trans- form the word embeddings of the two languages into the same vector space. We show experimen- tally for the first time that the domain-adapted bilingual word embeddings we produce using this extremely simple technique are highly effective. We study two quite different tasks and domains, where resources are lacking, showing that our sim- ple technique performs well for both of them: cross-lingual twitter sentiment classification and medical bilingual lexicon induction. In previous work, task-dependent approaches were used for this type of domain adaptation. Our approach is simple and task independent.</p><p>Second, we adapt the semi-supervised image classification system of <ref type="bibr">Häusser et al. (2017)</ref> for NLP problems for the first time. This approach is broadly applicable to many NLP classification tasks where unlabeled data is available. We tai- lor it to both of our cross-lingual tasks. The sys- tem exploits unlabeled data during the training of classifiers by learning similar features for similar labeled and unlabeled training examples, thereby extracting information from unlabeled examples as well. As we show experimentally, the system further improves cross-lingual knowledge transfer for both of our tasks.</p><p>After combining both techniques, the results of sentiment analysis are competitive with systems that use annotated data in the target language, an impressive result considering that we require no target-language annotated data. The method also yields impressive improvements for bilingual lex- icon induction compared with baselines trained on in-domain data. We show that this system re- quires the high-quality domain-adapted bilingual word embeddings we previously created to use un- labeled data well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Previous Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Bilingual Word Embeddings</head><p>Many approaches have been proposed for creating high quality BWEs using different bilingual sig- nals. Following <ref type="bibr" target="#b24">Mikolov et al. (2013b)</ref>, many au- thors <ref type="bibr" target="#b6">(Faruqui and Dyer, 2014;</ref><ref type="bibr" target="#b34">Xing et al., 2015;</ref><ref type="bibr" target="#b18">Lazaridou et al., 2015;</ref><ref type="bibr" target="#b29">Vuli´cVuli´c and Korhonen, 2016)</ref> map monolingual word embeddings (MWEs) into the same bilingual space. Others leverage paral- lel texts ( <ref type="bibr">Hermann and Blunsom, 2014;</ref> or create artificial cross-lingual cor- pora using seed lexicons or document alignments <ref type="bibr" target="#b30">(Vuli´cVuli´c and Moens, 2015;</ref><ref type="bibr" target="#b4">Duong et al., 2016)</ref> to train BWEs.</p><p>In contrast, our aim is not to improve the in- trinsic quality of BWEs, but to adapt BWEs to specific domains to enhance their performance on bilingual tasks in these domains. <ref type="bibr" target="#b5">Faruqui et al. (2015)</ref>, <ref type="bibr" target="#b9">Gouws and Søgaard (2015)</ref>, <ref type="bibr" target="#b27">Rothe et al. (2016)</ref> have previously studied domain adaptation of bilingual word embeddings, showing it to be highly effective for improving downstream tasks. However, importantly, their proposed methods are based on specialized domain lexicons (such as, e.g., sentiment lexicons) which contain task spe- cific word relations. Our delightfully simple ap- proach is, in contrast, effectively task independent (in that it only requires unlabeled in-domain text), which is an important strength.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Cross-Lingual Sentiment Analysis</head><p>Sentiment analysis is widely applied, and thus ide- ally we would have access to high quality super- vised models in all human languages. Unfortu- nately, good quality labeled datasets are missing for many languages. Training models on resource rich languages and applying them to resource poor languages is therefore highly desirable. Cross- lingual sentiment classification (CLSC) tackles this problem ( <ref type="bibr" target="#b22">Mihalcea et al., 2007;</ref><ref type="bibr" target="#b2">Banea et al., 2010;</ref><ref type="bibr" target="#b32">Wan, 2009;</ref><ref type="bibr" target="#b20">Lu et al., 2011;</ref><ref type="bibr" target="#b1">Balamurali and Joshi, 2012;</ref><ref type="bibr" target="#b10">Gui et al., 2013)</ref>. Recent CLSC ap- proaches use BWEs as features of deep learn- ing architectures which allows us to use a model for target-language sentiment classification, even when the model was trained only using source- language supervised training data. Following this approach we perform CLSC on Spanish tweets us- ing English training data. Even though Spanish is not resource-poor we simulate this by using only English annotated data. <ref type="bibr" target="#b33">Xiao and Guo (2013)</ref> proposed a cross-lingual log-bilinear document model to learn distributed representations of words, which can capture both the semantic similarities of words across lan- guages and the predictive information with respect to the classification task. Similarly, <ref type="bibr" target="#b28">Tang and Wan (2014)</ref> jointly embedded texts in different lan- guages into a joint semantic space representing sentiment. <ref type="bibr" target="#b36">Zhou et al. (2014)</ref> employed aligned sentences in the BWE learning process, but in the sentiment classification process only representa- tions in the source language are used for training, and representations in the target language are used for predicting labels. An important weakness of these three works was that aligned sentences were required.</p><p>Some work has trained sentiment-specific BWEs using annotated sentiment information in both languages ( <ref type="bibr" target="#b37">Zhou et al., 2015</ref><ref type="bibr" target="#b38">Zhou et al., , 2016</ref>, which is desirable, but this is not applicable to our sce- nario. Our goal is to adapt BWEs to a specific domain without requiring additional task-specific engineering or knowledge sources beyond having access to plentiful target-language in-domain un- labeled text. Both of the approaches we study in this work fit this criterion, the delightfully sim- ple method for adapting BWEs can improve the performance of any off-the-shelf classifier that is based on BWEs, while the broadly applicable semi-supervised approach of <ref type="bibr">Häusser et al. (2017)</ref> can improve the performance of any off-the-shelf classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Bilingual Lexicon Induction (BLI)</head><p>BLI is an important task that has been addressed by a large amount of previous work. The goal of BLI is to automatically extract word translation pairs using BWEs. While BLI is often used to pro- vide an intrinsic evaluation of BWEs ( <ref type="bibr" target="#b18">Lazaridou et al., 2015;</ref><ref type="bibr" target="#b30">Vuli´cVuli´c and Moens, 2015;</ref><ref type="bibr" target="#b29">Vuli´cVuli´c and Korhonen, 2016</ref>) it is also useful for tasks such as machine translation <ref type="bibr" target="#b21">(Madhyastha and España Bohnet, 2017)</ref>. Most work on BLI using BWEs fo- cuses on frequent words in high-resource domains such as parliament proceedings or news texts. Re- cently <ref type="bibr" target="#b13">Heyman et al. (2017)</ref> tackled BLI of words in the medical domain. This task is useful for many applications such as terminology extraction or OOV mining for machine translation of medi- cal texts. <ref type="bibr" target="#b13">Heyman et al. (2017)</ref> show that when only a small amount of medical data is available, BLI using BWEs tends to perform poorly. Es- pecially BWEs obtained using post-hoc mapping ( <ref type="bibr" target="#b24">Mikolov et al., 2013b;</ref><ref type="bibr" target="#b18">Lazaridou et al., 2015)</ref> fail on this task. Consequently, <ref type="bibr" target="#b13">Heyman et al. (2017)</ref> build BWEs using aligned documents and then en- gineer a specialized classification-based approach to BLI. In contrast, our delightfully simple ap- proach to create high-quality BWEs for the med- ical domain requires only monolingual data. We show that our adapted BWEs yield impressive im- provements over non-adapted BWEs in this task with both cosine similarity and with the classifier of <ref type="bibr" target="#b13">Heyman et al. (2017)</ref>. In addition, we show that the broadly applicable method can push per- formance further using easily accessible unlabeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Adaptation of BWEs</head><p>BWEs trained on general domain texts usually re- sult in lower performance when used in a system for a specific domain. There are two reasons for this. (i) Vocabularies of specific domains contain words that are not used in the general case, e.g., names of medicines or diseases. (ii) The mean- ing of a word varies across domains; e.g., "apple" mostly refers to a fruit in general domains, but is an electronic device in many product reviews.</p><p>The delightfully simple method adapts general domain BWEs in a way that preserves the seman- tic knowledge from general domain data and lever- ages monolingual domain specific data to create domain-specific BWEs. Our domain-adaptation approach is applicable to any language-pair in which monolingual data is available. Unlike other methods, our approach is task independent: it only requires unlabeled in-domain target language text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Approach</head><p>To create domain adapted BWEs, we first train MWEs (monolingual word embeddings) in both languages and then map those into the same space using post-hoc mapping ( <ref type="bibr" target="#b24">Mikolov et al., 2013b</ref>). We train MWEs for both languages by concate- nating monolingual out-of-domain and in-domain data. The out-of-domain data allows us to cre- ate accurate distributed representations of com- mon vocabulary while the in-domain data embeds domain specific words. We then map the two MWEs using a small seed lexicon to create the adapted BWEs. Because post-hoc mapping only requires a seed lexicon as bilingual signal it can easily be used with (cheap) monolingual data.</p><p>For post-hoc mapping, we use Mikolov et al. (2013b)'s approach. This model assumes a W ∈ R d 1 ×d 2 matrix which maps vectors from the source to the target MWEs where d 1 and d 2 are the embedding space dimensions. A seed lexicon of (x i , y i ) ∈ L ⊆ R d 1 ×R d 2 pairs is needed where x i and y i are source and target MWEs. W can be learned using ridge regression by minimizing the L 2 -regularized mapping error between the source x i and the target y i vectors:</p><formula xml:id="formula_0">min W i ||W x i − y i || 2 2 + λ||W || 2 2 (1)</formula><p>where λ is the regularization weight. Based on the source embedding x, we then compute a target embedding as W x.</p><p>We create MWEs with word2vec skipgram (Mikolov et al., 2013a) 1 and estimate W with scikit-learn ( <ref type="bibr" target="#b26">Pedregosa et al., 2011</ref>). We use de- fault parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Cross-Lingual Sentiment Classification</head><p>In CLSC, an important application of BWEs, we train a supervised sentiment model on training data available in the source (a resource rich lan- guage) and apply it to the target (a resource poor language, for which there is typically no train- ing data available). Because BWEs embed source and target words in the same space, annotations in the source (represented as BWEs) enable trans- fer learning. For CLSC of tweets, a drawback of BWEs trained on non-twitter data is that they do not produce embeddings for twitter-specific vo- cabulary, e.g., slang words like English coool and (Mexican) Spanish chido, resulting in lost infor- mation when a sentiment classifier uses them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training Data for Twitter Specific BWEs</head><p>As comparable non-twitter data we use OpenSub- titles ( <ref type="bibr" target="#b19">Lison and Tiedemann, 2016</ref>) which contains 49.2M English and Spanish subtitle sentences re- spectively (Subtitle). The reason behind choos- ing Subtitles is that although it is out-of-domain it contains slang words similar to tweets thus serving as a strong baseline in our setup. We experiment with two monolingual twitter data sets:</p><formula xml:id="formula_1">(i) 22M tweets: Downloaded 2 English (17.2M)</formula><p>and Spanish (4.8M) tweets using the public  <ref type="bibr" target="#b14">(Kilgarriff, 1997)</ref>, a list of 6,318 frequent English lemmas and their Span- ish translations, obtained from Google Translate. Note that we do not need a domain-specific lexi- con in order to get good quality adapted BWEs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training Data for Sentiment Classifiers</head><p>For sentiment classification, we use data from the RepLab 2013 shared task <ref type="bibr" target="#b0">(Amigó et al., 2013</ref>). The data is annotated with positive, neutral and negative labels and contains English and Spanish tweets. We used the official English training set (26.6K tweets) and the Spanish test set (14.9K) in the resource-poor setup. We only use the 7.2K Spanish labeled training data for comparison rea- sons in §6.2, which we will discuss later.</p><p>The shared task was on target-level sentiment analysis, i.e., given a pair (document, target en- tity), the gold annotation is based on whether the sentiment expressed by the document is about the target. For example: I cried on the back seat of my BMW! where BMW is the target would be neg- ative in the sentence-level scenario. However, it is neutral in the target-level case because the neg- ative sentiment is not related to BMW. The rea- son for using this dataset is that it contains com- parable English and Spanish tweets annotated for sentiment. There are other twitter datasets for En- glish ( <ref type="bibr" target="#b25">Nakov et al., 2016)</ref> and Spanish ( <ref type="bibr">GarcıaCumbreras et al., 2016</ref>), but they were down- loaded at different times and were annotated using different annotation methodologies, thus impeding a clean and consistent evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Sentiment Systems</head><p>For evaluating our adapted BWEs on the RepLab dataset we used a target-aware sentiment classi- fier introduced by <ref type="bibr" target="#b35">Zhang et al. (2016)</ref>. The net- work first embeds input words using pre-trained 3 dev.twitter.com/streaming/overview BWEs and feeds them to a bi-directional gated neural network. Pooling is applied on the hidden representations of the left and right context of the target mention respectively. Finally, gated neurons are used to model the interaction between the tar- get mention and its surrounding context. During training we hold our pre-trained BWEs fixed and keep the default parameters of the model.</p><p>We also implement Kim (2014)'s CNN-non- static system, which does not use the target in- formation in a given document (target-ignorant). The network first embeds input words using pre- trained BWEs and feeds them to a convolutional layer with multiple window sizes. Max pooling is applied on top of convolution followed by a fully connected network with one hidden layer. We used this system as well because it performed comparably to the target-aware system. The rea- son for this is that only 1% of the used data con- tains more than one target and out of these rare cases only 14% have differing sentiment labels in the same sentence, which are the difficult cases of target-level sentiment analysis. We used the de- fault parameters as described in <ref type="bibr" target="#b15">(Kim, 2014</ref>) with the exception of using 1000 feature maps and 30 epochs, based on our initial experiments. Word embeddings are fixed during the training just as for the target-aware classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results</head><p>As we previously explained we evaluate our adap- tation method on the task of target-level senti- ment classification using both target-aware and target-ignorant classifiers. For all experiments, our two baselines are off-the-shelf classifiers us- ing non-adapted BWEs, i.e., BWEs trained only using Subtitles. Our goal is to show that our BWE adaptation method can improve the performance of such classifiers. We train our adapted BWEs on the concatenation of Subtitle and 22M tweets or BACKGROUND respectively. In addition, we also report results with BWEs trained only on tweets.</p><p>To train the sentiment classifiers we use the En- glish Replab training set and we evaluate on the Spanish test set. To show the performance that can be reached in a monolingual setup, we report results obtained by using annotated Spanish sen- timent data instead of English (oracle). We train two oracle sentiment classifiers using (i) MWEs trained on only the Spanish part of Subtitle and (ii)  <ref type="table">Table 1</ref>: Accuracy of the BWE adaptation ap- proach on the target-level sentiment classification task. The oracle systems used Spanish sentiment training data instead of English.</p><p>BWEs trained on Subtitle using posthoc mapping.</p><p>The difference between the two is that the em- beddings of (ii) are enriched with English words which can be beneficial for the classification of Spanish tweets because they often contain a few English words. We do not compare with word embedding adap- tation methods relying on specialized resources. The point of our work is to study task-independent methods and to the best of our knowledge ours is the first such attempt. Similarly, we do not com- pare against machine translation based sentiment classifiers (e.g., (Zhou et al., 2016)) because for their adaptation in-domain parallel data would be needed. <ref type="table">Table 1</ref> gives results for both classifiers. It shows that the adaptation of Subtitle based BWEs with data from Twitter (22M tweets and BACK- GROUND) clearly outperforms the Baseline in all cases. The target-aware system performed poorly with the baseline BWEs and could bene- fit significantly from the adaptation approach. The target-ignorant performed better with the baseline BWEs but could also benefit from the adaptation.</p><p>Comparing results with the Twitter-dataset-only based BWEs, the 22M tweets performed better even though the BACKGROUND dataset is from the same topic as the RepLab train and test sets. Our conjecture is that the latter is too small to cre- ate good BWEs. In combination with Subtitles, 22M tweets also yields better results than when combined with BACKGROUND. Although the best accuracy was reached using the 22M tweets- only based BWEs, it is only slightly better then the adapted Subtitles+22M tweets based BWEs. In §6 we show that both the semantic knowledge from Subtitles and the domain-specific informa- tion from tweets are needed to further improve re- sults.</p><p>Comparing the two classifiers we can say that they performed similarly in terms of their best re- sults. On the other hand, the target-ignorant sys- tem had better results on average. This might seem surprising at first because the system does not use the target as information. But considering the characteristics of RepLab, i.e., that the number of tweets that contains multiple targets is negligi- ble, using the target offers no real advantage.</p><p>Although we did not focus on the impact of the seed lexicon size, we ran post-hoc mapping with different sizes during our preliminary experi- ments. With 1,000 and 100 word pairs in the lex- icon the target-ignorant system suffered 0.5% and 4.0% drop in average of our setups respectively.</p><p>To summarize the result: using adapted BWEs for the Twitter CLSC task improves the perfor- mance of off-the-shelf classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Medical Bilingual Lexicon Induction</head><p>Another interesting downstream task for BWEs is bilingual lexicon induction. Given a list of words in a source language, the goal of BLI is to mine translations for each word in a chosen target lan- guage. The medical bilingual lexicon induction task proposed in <ref type="bibr" target="#b13">(Heyman et al., 2017</ref>) aims to mine medical words using BWEs trained on a very small amount of English and Dutch monolingual medical data. Due to the lack of resources in this domain, good quality BWEs are hard to build us- ing in-domain data only. We show that by enrich- ing BWEs with general domain knowledge (in the form of general domain monolingual corpora) bet- ter results can be achieved on this medical domain task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>We evaluate our improved BWEs on the dataset provided by <ref type="bibr" target="#b13">Heyman et al. (2017)</ref>. The mono- lingual medical data consists of English and Dutch medical articles from Wikipedia. The En- glish (resp. Dutch) articles contain 52,336 (resp. 21,374) sentences. A total of 7,368 manually an- notated word translation pairs occurring in the En- glish (source) and Dutch (target) monolingual cor- pora are provided as gold data. This set is split 64%/16%/20% into trn/dev/test. 20% of the En- glish words have multiple translations. Given an English word, the task is to find the correct Dutch translation.</p><p>As monolingual general-domain data we use cosine similarity classifier  <ref type="table">Table 2</ref>: We report F 1 results for medical BLI with the cosine similarity and the classifier based sys- tems. We present baseline and our proposed domain adaptation method using both general and medical lexicons.</p><formula xml:id="formula_2">F 1 (top) F 1 (all) F 1 (top) F 1 (all)</formula><p>the English and Dutch data from Europarl (v7) ( <ref type="bibr" target="#b17">Koehn, 2005</ref>), a corpus of 2 million sentence pairs. Although Europarl is a parallel corpus, we use it in a monolingual way and shuffle each side of the corpus before training. By using massive cheap data we create high-quality MWEs in each language which are still domain-specific (due to inclusion of medical data). To obtain an out-of- domain seed lexicon, we translated the English words in BNC to Dutch using Google Translate (just as we did before for the Twitter CLSC task).</p><p>We then use the out-of-domain BNC and the in- domain medical seed lexicons in separate exper- iments to create BWEs with post-hoc mapping. Note, we did not concatenate the two lexicons because (i) they have a small common subset of source words which have different target words, thus having a negative effect on the mapping and (ii) we did not want to modify the medical seed lexicon because it was taken from previous work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">BLI Systems</head><p>To perform BLI we use two methods. Because BWEs represent words from different languages in a shared space, BLI can be performed via co- sine similarity in this space. In other words, given a BWE representing two languages V s and V t , the translation of each word s ∈ V s can be induced by taking the word t ∈ V t whose representation x t in the BWE is closest to the representation x s . As the second approach we use a classifier based system proposed by <ref type="bibr" target="#b13">Heyman et al. (2017)</ref>. This neural network based system is comprised of two main modules. The first is a character-level LSTM which aims to learn orthographic similar- ity of word pairs. The other is the concatenation of the embeddings of the two words using embed- ding layers with the aim of learning the similar- ity among semantic representations of the words. Dense layers are applied on top of the two mod- ules before the output soft-max layer. The clas- sifier is trained using positive and negative word pair examples and a pre-trained word embedding model. Negative examples are randomly gener- ated for each positive one in the training lexi- con. We used default parameters as reported by <ref type="bibr" target="#b13">Heyman et al. (2017)</ref> except for the t classifica- tion thresholds (used at prediction time). We fine- tuned these on dev. We note that the system works with pre-trained MWEs as well (and report these as official baseline results) but it requires BWEs for candidate generation at prediction time, thus we use BWEs for the system's input for all exper- iments. In preliminary work, we had found that MWE and BWE results are similar.  <ref type="table">Table 2</ref> compares its performance with our adapted BWEs, with both cosine similarity and classification based systems. "top" F 1 scores are based on the most probable word as prediction only; "all" F 1 scores use all words as prediction whose probability is above the threshold. It can be seen that the cosine similarity based system us- ing adapted BWEs clearly outperforms the non- adapted BWEs which were trained in a resource poor setup. <ref type="bibr">4</ref> Moreover, the best performance was reached using the general seed lexicon for the mapping which is due to the fact that general do- main words have better quality embeddings in the MWE models, which in turn gives a better quality mapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head><p>The classification based system performs sig- nificantly better comparing to cosine similarity by exploiting the seed lexicon better. Using adapted BWEs as input word embeddings for the system further improvements were achieved which shows the better quality of our BWEs. Simulating an even poorer setup by using a general lexicon, the performance gain of the classifier is lower. This shows the significance of the medical seed lexicon for this system. On the other hand, adapted BWEs have better performance compared to non-adapted ones using the best translation while they have just slightly lower F 1 using multiple translations. This result shows that while with adapted BWEs the system predicts better "top" translations, it has a harder time when predicting "all" due to the in- creased vocabulary size.</p><p>To summarize: we have shown that adapted BWEs increase performance for this task and do- main; and they do so independently of the task- specific system that is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Semi-Supervised Learning</head><p>In addition to the experiments that show our BWE- adaptation method's task and language indepen- dence, we investigate ways to further incorporate unlabeled data to overcome data sparsity.</p><p>Häusser et al. <ref type="formula" target="#formula_3">(2017)</ref> introduce a semi- supervised method for neural networks that makes associations from the vector representation of la- beled samples to those of unlabeled ones and back. This lets the learning exploit unlabeled samples as well. While <ref type="bibr">Häusser et al. (2017)</ref> use their model for image classification, we adapt it to CLSC of tweets and medical BLI. We show that our semi- supervised model requires adapted BWEs to be ef- fective and yields significant improvements. This innovative method is general and can be applied to any classification when unlabeled text is available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Model</head><p>Häusser et al. (2017)'s basic assumption is that the embeddings of labeled and unlabeled samples - i.e., the representations in the neural network on which the classification layer is applied -are sim- ilar within the same class. To achieve this, walking cycles are introduced: a cycle starts from a labeled sample, goes to an unlabeled one and ends at a la- beled one. A cycle is correct if the start and end samples are in the same class. The probability of going from sample A to B is proportional to the cosine similarity of their embeddings. To maxi- mize the number of correct cycles, two loss func- tions are employed: Walker loss and Visit loss.</p><p>Walker loss penalizes incorrect walks and en- courages a uniform probability distribution of walks to the correct class. It is defined as:</p><formula xml:id="formula_3">L walker := H(T, P aba )<label>(2)</label></formula><p>where H is the cross-entropy function, P aba ij is the probability that a cycle starts from sample i and ends at j and T is the uniform target distribu- tion:</p><formula xml:id="formula_4">T ij := 1/(#c(i)) if c(i) = c(j) 0 otherwise (3)</formula><p>where c(i) is the class of sample i and #c(i) is the number of occurrences of c(i) in the labeled set.</p><p>Visit loss encourages cycles to visit all unla- beled samples, rather than just those which are the most similar to labeled samples. It is defined as:</p><formula xml:id="formula_5">L visit := H(V, P visit ) P visit j := P ab ij i (4) V j := 1 U</formula><p>where H is cross-entropy, P ab ij is the probability that a cycle starts from sample i and goes to j and U is the number of unlabeled samples.</p><p>The total loss during training is the sum of the walker, visit and classification (cross-entropy be- tween predicted and gold labels) losses which is minimized using Adam ( <ref type="bibr" target="#b16">Kingma and Ba, 2015)</ref>.</p><p>We adapt this model (including the two losses) to sentiment classification, focusing on the target- ignorant classifier, and the classifier based ap- proach for BLI. We will call these systems semisup 5 . Due to the fact that we initialize the embedding layers for both classifiers with BWEs the models are able to make some correct cycles at the beginning of the training and improve them later on. We will describe the labeled and unla- beled datasets used in the subsequent sections be- low.</p><p>We use <ref type="bibr">Häusser et al. (2017)</ref>'s implementation of the losses, with 1.0, 0.5 and 1.0 weights for the walker, visit and classification losses, respectively, for CLSC based on preliminary experiments. We fine-tuned the weights for BLI on dev for each ex- periment. semisup domain adaptation Baseline 58.67% (-0.38%) BACKGROUND 57.41% (-1.09%) 22M tweets 60.19% (-0.95%) Subtitle+BACKGROUND 60.31% (0.97%) Subtitle+22M tweets 63.23% (2.17%) <ref type="table">Table 3</ref>: Accuracy on CLSC of the adapted BWE approach with the semisup (target-ignorant with additional loss functions) system comparing to the target-ignorant in brackets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Semi-Supervised CLSC</head><p>As in §4.4, we use pre-trained BWEs to initialize the classifier and use English sentiment training data as the labeled set. Furthermore, we use the Spanish sentiment training data as the unlabeled set, ignoring its annotation. This setup is very similar to real-word low-resource scenarios: unla- beled target-language tweets are easy to download while labeled English ones are available. <ref type="table">Table 3</ref> gives results for adapted BWEs and shows that semisup helps only when word embed- dings are adapted to the Twitter domain. As men- tioned earlier, semisup compares labeled and un- labeled samples based on their vector representa- tions. By using BWEs based on only Subtitles, we lose too many embeddings of similar English and Spanish tweets. On the other hand, if we use only tweet-based BWEs we lose good quality seman- tic knowledge which can be learned from more standard text domains. By combining the two do- mains we were able to capture both sides. For Sub- title+22M tweets, we even get very close to the best oracle (BWE Subtitle) in <ref type="table">Table 1</ref> getting only 0.27% less accuracy -an impressive result keep- ing in mind that we did not use labeled Spanish data.</p><p>The RepLab dataset contains tweets from 4 top- ics: automotive, banking, university, music. We manually analyzed similar tweets from the labeled and unlabeled sets. We found that when using semisup, English and Spanish tweets from the same topics are more similar in the embedding space than occurs without the additional losses. Topics differ in how they express sentiment -this may explain why semisup increases performance for RepLab.</p><p>Adding supervision. To show how well semisup can exploit the unlabeled data we used both English and Spanish sentiment training data together to train the sentiment classifiers. <ref type="table" target="#tab_4">Table 4</ref> shows that by using annotated data in both languages we get clearly better results than when using only one language. <ref type="table" target="#tab_4">Tables  3 and 4</ref> show that for Subtitle+22M tweets based BWEs, the semisup approach achieved high improvement (2.17%) comparing to target- ignorant with English training data only, while it achieved lower improvement (0.97%) with the Subtitle+BACKGROUND based BWEs. On the other hand, adding labeled Spanish data caused just a slight increase comparing to semisup with Subtitle+22M tweets based BWEs (0.59%), while in case of Subtitle+BACKGROUND we got significant additional improvement (2.61%). This means that with higher quality BWEs, unlabeled target-language data can be exploited better.</p><p>It can also be seen that the target-aware system outperformed the target-ignorant system using ad- ditional labeled target-language data. The reason could be that it is a more complex network and therefore needs more data to reach high perfor- mance.</p><p>The results in table 4 are impressive: our target- level system is strongly competitive with the of- ficial shared task results. We achieved high ac- curacy on the Spanish test set by using only En- glish training data. Comparing our best system which used all training data to the official results <ref type="bibr" target="#b0">(Amigó et al., 2013</ref>) we would rank 2 nd even though our system is not fine-tuned for the Re- pLab dataset. Furthermore, we also outperformed the oracles when using annotated data from both languages which shows the additional advantage of using BWEs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Semi-Supervised BLI</head><p>For BLI experiments with semisup we used word pairs from the medical seed lexicon as the la- beled set (with negative word pairs generated as described in §5.2). As opposed to CLSC and the work of <ref type="bibr">(Häusser et al., 2017)</ref>, for this task we do not have an unlabeled set, and therefore we need to generate it. We developed two scenarios. For the first, BNC, we generate a general unlabeled set us- ing English words from the BNC lexicon and gen- erate 10 pairs out of each word by using the 5 most similar Dutch words based on the corresponding BWEs and 5 random Dutch words. For the sec- ond scenario, medical, we generate an in-domain unlabeled set by generating for each English word in the medical lexicon the 3 most similar Dutch    words based on BWEs and for each of these we use the 5 most similar English words (ignoring the words which are in the original medical lexicon) and 5 negative words. The idea behind these meth- ods is to automatically generate an unlabeled set that hopefully has a similar positive and negative word pair distribution to the distribution in the la- beled set.</p><p>Results in <ref type="table" target="#tab_5">Table 5</ref> show that adding semisup to the classifier further increases performance for BLI as well. For the baseline system, when using only in-domain text for creating BWEs, only the medical unlabeled set was effective, general do- main word pairs could not be exploited due to the lack of general semantic knowledge in the BWE model. On the other hand, by using our domain adapted BWEs, which contain both general do- main and in-domain semantical knowledge, we can exploit word pairs from both domains. Results for adapted BWEs increased in 3 out of 4 cases, where the only exception is when using multiple translations for a given source word (which may have been caused by the bigger vocabulary size).</p><p>These results show that adapted BWEs are needed to exploit unlabeled data well which leads to an impressive overall 3.71 increase compared with the best result in previous work <ref type="bibr" target="#b13">(Heyman et al., 2017)</ref>, by using only unlabeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>Bilingual word embeddings trained on general domain data yield poor results in out-of-domain tasks. We presented experiments on two different low-resource task/domain combinations. Our de- lightfully simple task independent method to adapt BWEs to a specific domain uses unlabeled mono- lingual data only. We showed that with the sup- port of adapted BWEs the performance of off- the-shelf methods can be increased for both cross- lingual Twitter sentiment classification and medi- cal bilingual lexicon induction. Furthermore, by adapting the broadly applicable semi-supervised approach of <ref type="bibr">Häusser et al. (2017)</ref> (which until now has only been applied in computer vision) we were able to effectively exploit unlabeled data to fur- ther improve performance. We showed that, when also using high-quality adapted BWEs, the per- formance of the semi-supervised systems can be significantly increased by using unlabeled data at classifier training time. In addition, CLSC results are competitive with a system that uses target- language labeled data, even when we use no such target-language labeled data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Heyman et al. (2017)'s results are our base- line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>lang</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 4 : Accuracy on CLSC of both target-aware and target-ignorant systems using English or/and</head><label>4</label><figDesc></figDesc><table>Spanish sentiment training data. Column lang shows the language of the used training data. Differences 
comparing to semisup are indicated in brackets. 

F1 (top) 
F1 (all) 
Baseline+BNC 
35.04 (-0.66) 34.98 (-1.40) 
Baseline+medical 
36.20 (0.50) 
36.55 (0.16) 
Adapted+BNC 
41.01 (0.30) 
39.04 (0.95) 
Adapted+medical 
41.44 (0.73) 37.51 (-0.57) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Results with the semi-supervised system 
for BLI. Differences comparing to previous re-
sults are indicated in brackets. Baseline results are 
compared to rerun experiments of Heyman et al. 
(2017) using BWEs instead of MWEs. 

</table></figure>

			<note place="foot" n="1"> https://github.com/dav/word2vec 2 We downloaded for a month starting on 2016-10-15.</note>

			<note place="foot" n="4"> The results for cosine similarity in (Heyman et al., 2017) are based on BWESG-based BWEs (Vuli´cVuli´c and Moens, 2016) trained on a small document aligned parallel corpus without using a seed lexicon.</note>

			<note place="foot" n="5"> We publicly release our implementation: https:// github.com/hangyav/biadapt</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous review-ers for their valuable input. This project has re-ceived funding from the European Research Coun-cil (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement 640550).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Overview of replab 2013: Evaluating online reputation monitoring systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Amigó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Carrillo De Albornoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Chugur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adolfo</forename><surname>Corujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julio</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Martín</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Meij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damiano</forename><surname>Maarten De Rijke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Spina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Amigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Carrillo De Albornoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CLEF</title>
		<meeting>CLEF</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cross-lingual sentiment analysis for indian languages using linked wordnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Balamurali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adity</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multilingual subjectivity: Are more languages better?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Natural language processing with Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ewan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<pubPlace>O&apos;Reilly Media, Inc</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning crosslingual word embeddings without bilingual corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Kanayama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Retrofitting word vectors to semantic lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sujay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improving vector space word representations using multilingual correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Overview of tass</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Migueí</forename><surname>Angel Garcıa-Cumbreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julio</forename><surname>Villenaromán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugenio</forename><surname>Martınez-Cámara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><forename type="middle">Carlos</forename><surname>Díaz-Galiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>María-Teresa Martín-Valdivia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alfonso Ureña-López</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. TASS</title>
		<meeting>TASS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bilbowa: Fast bilingual distributed representations without word alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Simple task-specific bilingual word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A mixed model for cross lingual opinion analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruifeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Xiaolong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NLPCC</title>
		<meeting>NLPCC</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning by Association-A versatile semi-supervised training method for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Häusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Mordvintsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multilingual models for compositional distributed semantics</title>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<editor>Karl Moritz Hermann and Phil Blunsom</editor>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bilingual lexicon induction by learning to combine word-level and character-level representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geert</forename><surname>Heyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Putting frequencies in the dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kilgarriff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Lexicography</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Europarl: A parallel corpus for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MT Summit</title>
		<meeting>MT Summit</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hubness and pollution: Delving into cross-space mapping for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Opensubtitles2016: Extracting large parallel corpora from movie and tv subtitles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Lison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Joint bilingual sentiment classification with unlabeled parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">K</forename><surname>Tsou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning bilingual projections of embeddings for vocabulary expansion in machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swaroop</forename><surname>Pranava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><forename type="middle">España</forename><surname>Madhyastha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bohnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. RepL4NLP</title>
		<meeting>RepL4NLP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning multilingual subjective language via cross-lingual projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Exploiting similarities among languages for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<idno>abs/1309.4168</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">SemEval2016 task 4: Sentiment analysis in Twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Rosenthal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SemEval</title>
		<meeting>SemEval</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaël</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ultradense Word Embeddings by Orthogonal Transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sascha</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning bilingual embedding model for cross-language sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuewei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WI-IAT</title>
		<meeting>WI-IAT</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On the Role of Seed Lexicons in Learning Bilingual Word Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bilingual word embeddings from non-parallel documentaligned data applied to bilingual lexicon induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bilingual distributed word representations from documentaligned comparable data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Co-training for cross-lingual sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Semi-supervised representation learning for cross-lingual text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Normalized word embedding and orthogonal transform for bilingual word translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiye</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Gated Neural Networks for Targeted Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duy-Tin</forename><surname>Vo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Bridging the Language Gap: Learning Distributed Semantics for Cross-Lingual Sentiment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NLPCC</title>
		<meeting>NLPCC</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning bilingual sentiment word embeddings for cross-language sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fulin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Degen</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Cross-lingual sentiment classification with bilingual document representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
