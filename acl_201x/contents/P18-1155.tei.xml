<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:59+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">From Credit Assignment to Entropy Regularization: Two New Algorithms for Neural Sequence Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">From Credit Assignment to Entropy Regularization: Two New Algorithms for Neural Sequence Prediction</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1672" to="1682"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this work, we study the credit assignment problem in reward augmented maximum likelihood (RAML) learning, and establish a theoretical equivalence between the token-level counterpart of RAML and the entropy regularized reinforcement learning. Inspired by the connection , we propose two sequence prediction algorithms, one extending RAML with fine-grained credit assignment and the other improving Actor-Critic with a systematic entropy regularization. On two benchmark datasets, we show the proposed algorithms outperform RAML and Actor-Critic respectively, providing new alternatives to sequence prediction.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Modeling and predicting discrete sequences is the central problem to many natural language process- ing tasks. In the last few years, the adaption of re- current neural networks (RNNs) and the sequence- to-sequence model (seq2seq) <ref type="bibr" target="#b29">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b1">Bahdanau et al., 2014</ref>) has led to a wide range of successes in conditional sequence pre- diction, including machine translation ( <ref type="bibr" target="#b29">Sutskever et al., 2014;</ref><ref type="bibr" target="#b1">Bahdanau et al., 2014</ref>), automatic summarization ( <ref type="bibr">Rush et al., 2015)</ref>, image cap- tioning ( <ref type="bibr" target="#b11">Karpathy and Fei-Fei, 2015;</ref><ref type="bibr" target="#b33">Xu et al., 2015)</ref> and speech recogni- tion ( <ref type="bibr" target="#b4">Chan et al., 2016)</ref>.</p><p>Despite the distinct evaluation metrics for the aforementioned tasks, the standard training algo- rithm has been the same for all of them. Specif- ically, the algorithm is based on maximum likeli- hood estimation (MLE), which maximizes the log- * Equal contribution. likelihood of the "ground-truth" sequences empir- ically observed. <ref type="bibr">1</ref> While largely effective, the MLE algorithm has two obvious weaknesses. Firstly, the MLE train- ing ignores the information of the task specific metric. As a result, the potentially large discrep- ancy between the log-likelihood during training and the task evaluation metric at test time can lead to a suboptimal solution. Secondly, MLE can suf- fer from the exposure bias, which refers to the phenomenon that the model is never exposed to its own failures during training, and thus cannot recover from an error at test time. Fundamen- tally, this issue roots from the difficulty in statisti- cally modeling the exponentially large space of se- quences, where most combinations cannot be cov- ered by the observed data.</p><p>To tackle these two weaknesses, there have been various efforts recently, which we summarize into two broad categories:</p><p>• A widely explored idea is to directly opti- mize the task metric for sequences produced by the model, with the specific approaches rang- ing from minimum risk training (MRT) <ref type="bibr" target="#b28">(Shen et al., 2015</ref>) and learning as search optimization (LaSO) <ref type="bibr" target="#b5">(Daumé III and Marcu, 2005;</ref><ref type="bibr" target="#b32">Wiseman and Rush, 2016)</ref> to reinforcement learn- ing (RL) ( <ref type="bibr" target="#b24">Ranzato et al., 2015;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2016)</ref>. In spite of the technical differences, the key component to make these training al- gorithms practically efficient is often a delicate credit assignment scheme, which transforms the sequence-level signal into dedicated smaller units (e.g., token-level or chunk-level), and al- locates them to specific decisions, allowing for efficient optimization with a much lower vari- ance. For instance, the beam search optimiza-tion (BSO) <ref type="bibr" target="#b32">(Wiseman and Rush, 2016)</ref> utilizes the position of margin violations to produce sig- nals to the specific chunks, while the actor-critic (AC) algorithm ( <ref type="bibr" target="#b0">Bahdanau et al., 2016</ref>) trains a critic to enable token-level signals.</p><p>• Another alternative idea is to construct a task metric dependent target distribution, and train the model to match this task-specific target in- stead of the empirical data distribution. As a typical example, the reward augmented maxi- mum likelihood (RAML) ( <ref type="bibr" target="#b22">Norouzi et al., 2016</ref>) defines the target distribution as the exponen- tiated pay-off (sequence-level reward) distribu- tion. This way, RAML not only can incorporate the task metric information into training, but it can also alleviate the exposure bias by expos- ing imperfect outputs to the model. However, RAML only works on the sequence-level train- ing signal.</p><p>In this work, we are intrigued by the question whether it is possible to incorporate the idea of fine-grained credit assignment into RAML. More specifically, inspired by the token-level signal used in AC, we aim to find the token-level counter- part of the sequence-level RAML, i.e., defining a token-level target distribution for each auto- regressive conditional factor to match. Motived by the question, we first formally define the desider- ata the token-level counterpart needs to satisfy and derive the corresponding solution ( §2). Then, we establish a theoretical connection between the de- rived token-level RAML and entropy regularized RL ( §3). Motivated by this connection, we pro- pose two algorithms for neural sequence predic- tion, where one is the token-level extension to RAML, and the other a RAML-inspired improve- ment to the AC ( §4). We empirically evaluate the two proposed algorithms, and show different lev- els of improvement over the corresponding base- line. We further study the importance of vari- ous techniques used in our experiments, providing practical suggestions to readers ( §6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Token-level Equivalence of RAML</head><p>We first introduce the notations used throughout the paper. Firstly, capital letters will denote ran- dom variables and lower-case letters are the val- ues to take. As we mainly focus on conditional sequence prediction, we use x for the conditional input, and y for the target sequence. With y denot- ing a sequence, y j i then denotes the subsequence from position i to j inclusively, while y t denotes the single value at position t. Also, we use |y| to indicate the length of the sequence. To emphasize the ground-truth data used for training, we add su- perscript * to the input and target, i.e., x * and y * . In addition, we use Y to denote the set of all pos- sible sequences with one and only one eos symbol at the end, and W to denote the set of all possible symbols in a position. Finally, we assume length of sequences in Y is bounded by T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Background: RAML</head><p>As discussed in §1, given a ground-truth pair (x * , y * ), RAML defines the target distribution us- ing the exponentiated pay-off of sequences, i.e.,</p><formula xml:id="formula_0">P R (y | x * , y * ) = exp (R(y; y * )/τ ) y ∈Y exp (R(y ; y * )/τ ) ,<label>(1)</label></formula><p>where R(y; y * ) is the sequence-level reward, such as BLEU score, and τ is the temperature hyper- parameter controlling the sharpness. With the defi- nition, the RAML algorithm simply minimizes the cross entropy (CE) between the target distribution and the model distribution</p><formula xml:id="formula_1">P θ (Y | x * ), i.e., min θ CE P R (Y | x * , y * )P θ (Y | x * ) .<label>(2)</label></formula><p>Note that, this is quite similar to the MLE training, except that the target distribution is different. With the particular choice of target distribution, RAML not only makes sure the ground-truth reference re- mains the mode, but also allows the model to ex- plore sequences that are not exactly the same as the reference but have relatively high rewards. Compared to algorithms trying to directly opti- mize task metric, RAML avoids the difficulty of tracking and sampling from the model distribution that is consistently changing. Hence, RAML en- joys a much more stable optimization without the need of pretraining. However, in order to opti- mize the RAML objective (Eqn. (2)), one needs to sample from the exponentiated pay-off distribu- tion, which is quite challenging in practice. Thus, importance sampling is often used ( <ref type="bibr" target="#b22">Norouzi et al., 2016;</ref><ref type="bibr" target="#b17">Ma et al., 2017)</ref>. We leave the details of the practical implementation to Appendix B.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Token-level Target Distribution</head><p>Despite the appealing properties, RAML only op- erates on the sequence-level reward. As a result, the reward gap between any two sequences cannot be attributed to the responsible decisions precisely, which often leads to a low sample efficiency. Ide- ally, since we rely on the auto-regressive factor- ization P θ (y | x * ) = |y| t=1 P θ (y t | y t−1 1 , x * ), the optimization would be much more efficient if we have the target distribution for each token-level factor P θ (Y t | y t−1 1 , x * ) to match. Conceptually, this is exactly how the AC algorithm improves upon the vanilla sequence-level REINFORCE al- gorithm <ref type="bibr" target="#b24">(Ranzato et al., 2015)</ref>.</p><p>With this idea in mind, we set out to find such a token-level target. Firstly, we assume the token- level target shares the form of a Boltzmann distri- bution but parameterized by some unknown nega- tive energy function Q R , i.e., 2</p><formula xml:id="formula_2">P Q R (y t | y t−1 1 , y * ) = exp Q R (y t−1 1 , y t ; y * )/τ w∈W exp Q R (y t−1 1 , w; y * )/τ . (3)</formula><p>Intuitively, Q R (y t−1 1 , w; y * ) measures how much future pay-off one can expect if w is generated, given the current status y t−1 1 and the reference y * . This quantity highly resembles the action-value function (Q-function) in reinforcement learning. As we will show later, it is indeed the case.</p><p>Before we state the desiderata for Q R , we need to extend the definition of R in order to evaluate the goodness of an unfinished partial prediction, i.e., sequences without an eos suffix. Let Y − be the set of unfinished sequences, following <ref type="bibr" target="#b0">Bahdanau et al. (2016)</ref>, we define the pay-off function R for a partial sequencê y ∈ Y − , |ˆy||ˆy| &lt; T as</p><formula xml:id="formula_3">R(ˆ y; y * ) = R(ˆ y + eos; y * ),<label>(4)</label></formula><p>where the + indicates string concatenation. With the extension, we are ready to state two requirements for Q R :</p><p>1. Marginal match: For P Q R to be the token-level equivalence of P R , the sequence-level marginal distribution induced by P Q R must match P R , i.e., for any y ∈ Y,</p><formula xml:id="formula_4">|y| t=1 P Q R (y t | y t−1 1 ) = P R (y).<label>(5)</label></formula><p>Note that there are infinitely many Q R 's satisfy- ing Eqn. (5), because adding any constant value to Q R does not change the Boltzmann distribu- tion, known as shift-invariance w.r.t. the energy.</p><p>2. Terminal condition: Secondly, let's consider the value of Q R when emitting an eos symbol to immediately terminate the generation. As men- tioned earlier, Q R measures the expected future pay-off. Since the emission of eos ends the gen- eration, the future pay-off can only come from the immediate increase of the pay-off. Thus, we require Q R to be the incremental pay-off when producing eos, i.e.</p><formula xml:id="formula_5">Q R (ˆ y, eos; y * ) = R(ˆ y + eos; y * ) − R(ˆ y; y * ), (6)</formula><p>for anyˆyanyˆ anyˆy ∈ Y − . Since Eqn. (6) enforces the absolute of Q R at a point, it also solves the am- biguity caused by the shift-invariance property.</p><p>Based on the two requirements, we can derive the form Q R , which is summarized by Proposition 1. Proposition 1. P Q R and Q R satisfy requirements (5) and (6) if and only if for any ground-truth pair (x * , y * ) and any sequence prediction y ∈ Y,</p><formula xml:id="formula_6">Q R (y t−1 1 , y t ; y * ) = R(y t 1 ; y * ) − R(y t−1 1 ; y * ) + τ log w∈W exp Q R (y t 1 , w; y * )/τ ,<label>(7)</label></formula><p>when t &lt; |y|, and otherwise, i.e., when t = |y|</p><formula xml:id="formula_7">Q R (y t−1 1 , y t ; y * ) = R(y t 1 ; y * ) − R(y t−1</formula><p>Based on the notation, the goal of entropy- regularized RL augments is to learn a policy π(a t | s t ) which maximizes the discounted expected fu- ture return and causal entropy <ref type="bibr" target="#b34">(Ziebart, 2010)</ref>, i.e.,</p><formula xml:id="formula_8">max π t E st∼ρs,at∼π(·|st) γ t−1 [r(s t , a t ) + αH(π(· | s t ))],</formula><p>where H denotes the entropy and α is a hyper- parameter controlling the relative importance be- tween the reward and the entropy. Intuitively, compared to standard RL, the extra entropy term encourages exploration and promotes multi-modal behaviors. Such properties are highly favorable in a complex environment.</p><p>Given an entropy-regularized MDP, for any fixed policy π, the state-value function V π (s) and the action-value function Q π can be defined as</p><formula xml:id="formula_9">V π (s) = E a∼π(·|s) [Q π (s, a)] + αH(π(· | s)), Q π (s, a) = r(s, a) + E s ∼ρs [γV π (s )]. (9)</formula><p>With the definitions above, it can further be proved <ref type="bibr" target="#b34">(Ziebart, 2010;</ref><ref type="bibr" target="#b27">Schulman et al., 2017</ref>) that the optimal state-value function V * , the action- value function Q * and the corresponding optimal policy π * satisfy the following equations</p><formula xml:id="formula_10">V * (s) = α log a∈A exp Q * (s, a)/α , (10) Q * (s, a) = r(s, a) + γ E s ∼ρs [V * (s )], (11) π * (a | s) = exp (Q * (s, a)/α) a ∈A exp (Q * (s, a )/α) .<label>(12)</label></formula><p>Here, Eqn. <ref type="formula" target="#formula_0">(10)</ref> and <ref type="formula" target="#formula_0">(11)</ref> are essentially the entropy-regularized counterparts of the optimal Bellman equations in standard RL. Following pre- vious literature, we will refer to Eqn. <ref type="formula" target="#formula_0">(10)</ref> and <ref type="formula" target="#formula_0">(11)</ref> as the optimal soft Bellman equations, and the V * and Q * as optimal soft value functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">An RL Equivalence of the Token-level RAML</head><p>To reveal the connection, it is convenient to define the incremental pay-off</p><formula xml:id="formula_11">r(y t−1 1 , y t ; y * ) = R(y t 1 ; y * ) − R(y t−1 1 ; y * ),<label>(13)</label></formula><p>and the last term of Eqn. <ref type="formula" target="#formula_6">(7)</ref> as</p><formula xml:id="formula_12">V R (y t 1 ; y * ) = τ log w∈W exp Q R (y t 1 , w; y * )/τ<label>(14)</label></formula><p>Substituting the two definitions into Eqn. <ref type="formula" target="#formula_6">(7)</ref>, the recursion simplifies as</p><formula xml:id="formula_13">Q R (y t−1 1 , y t ; y * ) = r(y t−1 1 , y t ; y * ) + V R (y t 1 ; y * ). (15)</formula><p>Now, it is easy to see that the Eqn. <ref type="formula" target="#formula_0">(14)</ref> and <ref type="formula" target="#formula_0">(15)</ref>, which are derived from the token-level RAML, highly resemble the optimal soft Bellman equa- tions (10) and (11) in entropy-regularized RL. The following Corollary formalizes the connection. Corollary 1. For any ground-truth pair (x * , y * ), the recursion specified by Eqn. <ref type="formula" target="#formula_0">(13)</ref>, <ref type="formula" target="#formula_0">(14)</ref> and <ref type="formula" target="#formula_0">(15)</ref> is equivalent to the optimal soft Bellman equation of a "deterministic" MDP in entropy-regularized reinforcement learning, denoted as M R , where</p><p>• the state space S corresponds to Y − ,</p><p>• the action space A corresponds to W,</p><p>• the transition probability ρ s is a deterministic process defined by string concatenation • the reward function r corresponds to the in- cremental pay-off defined in Eqn. <ref type="formula" target="#formula_0">(13)</ref>, • the discounting factor γ = 1,</p><p>• the entropy hyper-parameter α = τ ,</p><p>• and a period terminates either when eos is emitted or when its length reaches T and we enforce the generation of eos.</p><p>Moreover, the optimal soft value functions V * and Q * of the MDP exactly match the V R and Q R de- fined by <ref type="bibr">Eqn. (14)</ref> and <ref type="formula" target="#formula_0">(15)</ref> respectively. The op- timal policy π * is hence equivalent to the token- level target distribution P Q R .</p><p>Proof. See Appendix A.1.</p><p>The connection established by Corollary 1 is quite inspiring:</p><p>• Firstly, it provides a rigorous and generalized view of the connection between RAML and entropy-regularized RL. In the original work, <ref type="bibr" target="#b22">Norouzi et al. (2016)</ref> point out RAML can be seen as reversing the direction of KL (P θ P R ), which is a sequence-level view of the connec- tion. Now, with the equivalence between the token-level target P Q R and the optimal Q * , it generalizes to matching the future action values consisting of both the reward and the entropy.</p><p>• Secondly, due to the equivalence, if we solve the optimal soft Q-function of the correspond- ing MDP, we directly obtain the token-level tar- get distribution. This hints at a practical algo- rithm with token-level credit assignment.</p><p>• Moreover, since RAML is able to improve upon MLE by injecting entropy, the entropy- regularized RL counterpart of the standard AC algorithm should also lead to an improvement in a similar manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Proposed Algorithms</head><p>In this section, we explore the insights gained from Corollary 1 and present two new algorithms for sequence prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Value Augmented Maximum Likelihood</head><p>The first algorithm we consider is the token-level extension of RAML, which we have been dis- cussing since §2. As mentioned at the end of §2.2, Proposition 1 only gives an implicit form of Q R , and so is the token-level target distribution P Q R (Eqn. <ref type="formula">(3)</ref>). However, thanks to Corollary 1, we now know that Q R is the same as the op- timal soft action-value function Q * of the entropy- regularized MDP M R . Hence, by finding the Q * , we will have access to P Q R . At the first sight, it seems recovering Q * is as difficult as solving the original sequence predic- tion problem, because solving Q * from the MDP is essentially the same as learning the optimal policy for sequence prediction. However, it is not true be- cause Q R (i.e., P Q R ) can condition on the correct reference y * . In contrast, the model distribution P θ can only depend on x * . Therefore, the func- tion approximator trained to recover Q * can take y * as input, making the estimation task much eas- ier. Intuitively, when recovering Q * , we are trying to train an ideal "oracle", which has access to the ground-truth reference output, to decide the best behavior (policy) given any arbitrary (good or not) state.</p><p>Thus, following the reasoning above, we first train a parametric function approximator Q φ to search the optimal soft action value. In this work, for simplicity, we employ the Soft Q- learning algorithm ( <ref type="bibr" target="#b27">Schulman et al., 2017</ref>) to per- form the policy optimization. In a nutshell, Soft Q-Learning is the entropy-regularized version of Q-Learning, an off-policy algorithm which mini- mizes the mean squared soft Bellman residual ac- cording to Eqn. (11). Specifically, given ground- truth pair (x * , y * ), for any trajectory y ∈ Y, the training objective is</p><formula xml:id="formula_14">min φ |y| t=1 Q φ (y t−1 1 , y t ; y * ) − ˆ Q φ (y t−1 1 , y t ; y * ) 2 ,<label>(16)</label></formula><note type="other">wherê Q φ (y t−1 1 , y t ; y * ) = r(y t−1 1 , y t ; y * ) + V φ (y t 1 ; y * )</note><p>is the one-step look-ahead target Q-value, and</p><formula xml:id="formula_15">V φ (y t 1 ; y * ) = τ log w∈W exp Q φ (y t 1 , w; y * )/τ</formula><p>as defined in Eqn. (10). In the recent instantia- tion of Q-Learning ( <ref type="bibr" target="#b20">Mnih et al., 2015)</ref>, to sta- bilize training, the target Q-value is often esti- mated by a separate slowly updated target net- work. In our case, as we have access to a signif- icant amount of reference sequences, we find the target network not necessary. Thus, we directly optimize Eqn. (16) using gradient descent, and let the gradient flow through both Q φ (y t−1 1 , y t ; y * ) and V φ (y t 1 ; y * ) <ref type="bibr" target="#b2">(Baird, 1995)</ref>. After the training of Q φ converges, we fix the parameters of Q φ , and optimize the cross en- tropy CE P Q φ P θ w.r.t. the model parameters θ, which is equivalent to 4</p><formula xml:id="formula_16">min θ E y∼P Q φ   |y| t=1 CE PQ φ (Yt | y t−1 1 )P θ (Yt | y t−1 1 )   .<label>(17)</label></formula><p>Compared to the of objective of RAML in Eqn.</p><p>(2), having access to P Q φ (Y t | y t−1 1 ) allows us to provide a distinct token-level target for each conditional factor P θ (Y t | y t−1 1 ) of the model. While directly sampling from P R is practically in- feasible ( §2.1), having a parametric target distri- bution P Q φ makes it theoretically possible to sam- ple from P Q φ and perform the optimization. How- ever, empirically, we find the samples from P Q φ are not diverse enough ( §6). Hence, we fall back to the same importance sampling approach (see Ap- pendix B.2) as used in RAML.</p><p>Finally, since the algorithm utilizes the optimal soft action-value function to construct the token- level target, we will refer to it as value augmented maximum likelihood (VAML) in the sequel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Entropy-regularized Actor Critic</head><p>The second algorithm follows the discussion at the end of §3.2, which is essentially an actor-critic al- gorithm based on the entropy-regularized MDP in Corollary 1. For this reason, we name the algo- rithm entropy-regularized actor critic (ERAC). As with standard AC algorithm, the training process interleaves the evaluation of current policy using the parametric critic Q φ and the optimization of the actor policy π θ given the current critic.</p><p>Critic Training. The critic is trained to perform policy evaluation using the temporal difference learning (TD), which minimizes the TD error</p><formula xml:id="formula_17">min φ E y∼π θ |y| t=1 Q φ (y t−1 1 , y t ; y * ) − ˆ Q ¯ φ (y t−1 1 , y t ; y * ) 2<label>(18)</label></formula><p>where the TD targetˆQtargetˆ targetˆQ ¯ φ is constructed based on fixed policy iteration in Eqn. (9), i.e.,</p><formula xml:id="formula_18">ˆ Q ¯ φ (y t−1 1 , y t ; y * ) = r(y t−1 1 , y t ) + τ H(π θ (· | y t 1 )) + w∈W π θ (w | y t 1 )Q ¯ φ (y t 1 , w; y * ).<label>(19)</label></formula><p>It is worthwhile to emphasize that the objective (18) trains the critic Q φ to evaluate the current pol- icy. Hence, it is entirely different from the objec- tive (16), which is performing policy optimization by Soft Q-Learning. Also, the trajectories y used in (18) are sequences drawn from the actor policy π θ , while objective (16) theoretically accepts any trajectory since Soft Q-Learning can be fully off- policy. <ref type="bibr">5</ref> Finally, following <ref type="bibr" target="#b0">Bahdanau et al. (2016)</ref>, the TD targetˆQtargetˆ targetˆQ ¯ φ in Eqn. <ref type="formula">(9)</ref> is evaluated us- ing a target network, which is indicated by the bar sign above the parameters, i.e., ¯ φ. The target network is slowly updated by linearly interpolat- ing with the up-to-date network, i.e., the update is</p><formula xml:id="formula_19">¯ φ ← βφ + (1 − β) ¯ φ for β in (0, 1) (Lillicrap et al., 2015).</formula><p>We also adapt another technique proposed by <ref type="bibr" target="#b0">Bahdanau et al. (2016)</ref>, which smooths the critic by minimizing the "variance" of Q-values, i.e.,</p><formula xml:id="formula_20">min φ λvar E y∼π θ |y| t=1 w∈W Q φ (y t 1 , w; y * ) − ¯ Q φ (y t 1 ; y * ) 2</formula><p>where ¯</p><formula xml:id="formula_21">Q φ (y t 1 ; y * ) = 1 |W| w ∈W Q φ (y t 1 , w ; y * )</formula><p>is the mean Q-value, and λ var is a hyper-parameter controlling the relative weight between the TD loss and the smooth loss.</p><p>Actor Training. Given the critic Q φ , the actor gradient (to maximize the expected return) is given by the policy gradient theorem of the entropy- regularized RL ( <ref type="bibr" target="#b27">Schulman et al., 2017)</ref>, which has the form</p><formula xml:id="formula_22">E y∼π θ |y| t=1 w∈W θ π θ (w | y t−1 1 )Q φ (y t−1 1 , w; y * ) + τ θ H(π θ (· | y t−1 1 )).<label>(20)</label></formula><p>Here, for each step t, we follow <ref type="bibr" target="#b0">Bahdanau et al. (2016)</ref> to sum over the entire symbol set W, in- stead of using the single sample estimation often seen in RL. Hence, no baseline is employed. It is worth mentioning that Eqn. <ref type="formula" target="#formula_1">(20)</ref> is not simply adding an entropy term to the standard policy gra- dient as in A3C ( <ref type="bibr" target="#b19">Mnih et al., 2016)</ref>. The difference lies in that the critic Q φ trained by Eqn. <ref type="formula" target="#formula_0">(18)</ref> ad- ditionally captures the entropy from future steps, while the θ H term only captures the entropy of the current step. Finally, similar to ( <ref type="bibr" target="#b0">Bahdanau et al., 2016)</ref>, we find it necessary to first pretrain the actor using MLE and then pretrain the critic before the actor- critic training. Also, to prevent divergence dur- ing actor-critic training, it is helpful to continue performing MLE training along with Eqn. <ref type="formula" target="#formula_1">(20)</ref>, though using a smaller weight λ mle .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Task Loss Optimization and Exposure Bias Apart from the previously introduced RAML, BSO, Actor-Critic ( §1), MIXER ( <ref type="bibr" target="#b24">Ranzato et al., 2015)</ref> also utilizes chunk-level signals where the length of chunk grows as training proceeds. In contrast, minimum risk training <ref type="bibr" target="#b28">(Shen et al., 2015)</ref> directly optimizes sentence-level BLEU. As a re- sult, it requires a large number (100) of samples per data to work well. To solve the exposure bias, scheduled sampling ( ) adopts a curriculum learning strategy to bridge the training and the inference. Professor forcing ( <ref type="bibr" target="#b12">Lamb et al., 2016)</ref> introduces an adversarial training mecha- nism to encourage the dynamics of the model to be the same at training time and inference time. For image caption, self-critic sequence training (SCST) ( <ref type="bibr" target="#b25">Rennie et al., 2016</ref>) extends the MIXER algorithm with an improved baseline based on the current model performance.</p><p>Entropy-regularized RL Entropy regulariza- tion been explored by early work in RL and in- verse RL <ref type="bibr" target="#b31">(Williams and Peng, 1991;</ref><ref type="bibr" target="#b35">Ziebart et al., 2008</ref>). Lately, <ref type="bibr" target="#b27">Schulman et al. (2017)</ref> establish the equivalence between policy gradients and Soft Q-Learning under entropy-regularized RL. Mo- tivated by the multi-modal behavior induced by entropy-regularized RL, <ref type="bibr" target="#b6">Haarnoja et al. (2017)</ref> ap- ply energy-based policy and Soft Q-Learning to continuous domain. Later, <ref type="bibr" target="#b21">Nachum et al. (2017)</ref> proposes path consistency learning, which can be seen as a multi-step extension to Soft Q-Learning. More recently, in the domain of simulated con- trol, <ref type="bibr" target="#b7">Haarnoja et al. (2018)</ref> also consider the ac- tor critic algorithm under the framework of en-tropy regularized reinforcement learning. Despite the conceptual similarity to ERAC presented here, <ref type="bibr" target="#b7">Haarnoja et al. (2018)</ref> focuses on continuous con- trol and employs the advantage actor critic variant as in <ref type="bibr" target="#b19">(Mnih et al., 2016)</ref>, while ERAC follows the Q actor critic as in ( <ref type="bibr" target="#b0">Bahdanau et al., 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experiment Settings</head><p>In this work, we focus on two sequence prediction tasks: machine translation and image captioning. Due to the space limit, we only present the infor- mation necessary to compare the empirical results at this moment. For a more detailed description, we refer readers to Appendix B and the code 6 .</p><p>Machine Translation Following <ref type="bibr" target="#b24">Ranzato et al. (2015)</ref>, we evaluate on IWSLT 2014 German-to- English dataset ( <ref type="bibr" target="#b18">Mauro et al., 2012</ref>). The cor- pus contains approximately 153K sentence pairs in the training set. We follow the pre-processing procedure used in ( <ref type="bibr" target="#b24">Ranzato et al., 2015)</ref>.</p><p>Architecture wise, we employ a seq2seq model with dot-product attention ( <ref type="bibr" target="#b1">Bahdanau et al., 2014;</ref><ref type="bibr" target="#b16">Luong et al., 2015)</ref>, where the encoder is a bidirec- tional LSTM (Hochreiter and Schmidhuber, 1997) with each direction being size 128, and the de- coder is another LSTM of size 256. Moreover, we consider two variants of the decoder, one using the input feeding technique ( <ref type="bibr" target="#b16">Luong et al., 2015</ref>) and the other not.</p><p>For all algorithms, the sequence-level BLEU score is employed as the pay-off function R, while the corpus-level BLEU score ( <ref type="bibr" target="#b23">Papineni et al., 2002</ref>) is used for the final evaluation. The sequence-level BLEU score is scaled up by the sentence length so that the scale of the immediate reward at each step is invariant to the length.</p><p>Image Captioning For image captioning, we consider the MSCOCO dataset ( <ref type="bibr" target="#b15">Lin et al., 2014</ref>). We adapt the same preprocessing procedure and the train/dev/test split used by <ref type="bibr">Karpathy and FeiFei (2015)</ref>.</p><p>The NIC ( ) is employed as the baseline model, where a feature vector of the image is extracted by a pre-trained CNN and then used to initialize the LSTM decoder. Different from the original NIC model, we employ a pre- trained 101-layer ResNet ( <ref type="bibr" target="#b8">He et al., 2016</ref>) rather than a GoogLeNet as the CNN encoder.</p><p>For training, each image-caption pair is treated as an i.i.d. sample, and sequence-level BLEU score is used as the pay-off. For testing, the stan- dard multi-reference BLEU4 is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Comparison with the Direct Baseline</head><p>Firstly, we compare ERAC and VAML with their corresponding direct baselines, namely AC (Bah- danau et al., 2016) and RAML ( <ref type="bibr" target="#b22">Norouzi et al., 2016)</ref> respectively. As a reference, the perfor- mance of MLE is also provided.</p><p>Due to non-neglected performance variance ob- served across different runs, we run each algo- rithm for 9 times with different random seeds, <ref type="bibr">7</ref> and report the average performance, the standard deviation and the performance range (min, max).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Machine Translation</head><p>The results on MT are summarized in the left half of Tab. 1. Firstly, all four advanced algorithms significantly outper- form the MLE baseline. More importantly, both VAML and ERAC improve upon their direct base- lines, RAML and AC, by a clear margin on aver- age. The result suggests the two proposed algo- rithms both well combine the benefits of a delicate credit assignment scheme and the entropy regular- ization, achieving improved performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Captioning</head><p>The results on image cap- tioning are shown in the right half of Tab. 1. De- spite the similar overall trend, the improvement of VAML over RAML is smaller compared to that in MT. Meanwhile, the improvement from AC to ERAC becomes larger in comparison. We sus- pect this is due to the multi-reference nature of the MSCOCO dataset, where a larger entropy is preferred. As a result, the explicit entropy regu- larization in ERAC becomes immediately fruitful. On the other hand, with multiple references, it can be more difficult to learn a good oracle Q * (Eqn. <ref type="formula" target="#formula_0">(15)</ref>). Hence, the token-level target can be less ac- curate, resulting in smaller improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Comparison with Existing Work</head><p>To further evaluate the proposed algorithms, we compare ERAC and VAML with the large body of existing algorithms evaluated on IWSTL 2014. As a note of caution, previous works don't employ the exactly same architectures (e.g. number of lay- ers, hidden size, attention type, etc.). Despite that, for VAML and ERAC, we use an architecture that is most similar to the majority of previous works, which is the one described in §6.1 with input feed- ing. Based on the setting, the comparison is summa- rized in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Ablation Study</head><p>Due to the overall excellence of ERAC, we study the importance of various components of it, hope- fully offering a practical guide for readers. As the input feeding technique largely slows down the training, we conduct the ablation based on the model variant without input feeding. Firstly, we study the importance of two tech- niques aimed for training stability, namely the tar- get network and the smoothing technique ( §4.2). Based on the MT task, we vary the update speed β of the target critic, and the λ var , which controls the  • Comparing the two rows of Tab. 3, the smooth- ing technique consistently leads to performance improvement across all values of τ . In fact, re- moving the smoothing objective often causes the training to diverge, especially when β = 0.01 and 1. But interestingly, we find the di- vergence does not happen if we update the tar- get network a little bit faster (β = 0.1) or quite slowly (β = 0.001).</p><p>• In addition, even with the smoothing technique, the target network is still necessary. When the target network is not used (β = 1), the perfor- mance drops below the MLE baseline. How- ever, as long as a target network is employed to ensure the training stability, the specific choice of target network update rate does not matter as much. Empirically, it seems using a slower (β = 0.001) update rate yields the best result.</p><p>Next, we investigate the effect of enforcing dif- ferent levels of entropy by varying the entropy hyper-parameter τ . As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, it seems there is always a sweet spot for the level of en- tropy. On the one hand, posing an over strong en- tropy regularization can easily cause the actor to diverge. Specifically, the model diverges when τ reaches 0.03 on the image captioning task or 0.06 on the machine translation task. On the other hand, as we decrease τ from the best value to 0, the per- formance monotonically decreases as well. This observation further verifies the effectiveness of en- tropy regularization in ERAC, which well matches our theoretical analysis.</p><p>Finally, as discussed in §4.2, ERAC takes the ef- fect of future entropy into consideration, and thus is different from simply adding an entropy term to the standard policy gradient as in A3C ( <ref type="bibr" target="#b19">Mnih et al., 2016)</ref>. To verify the importance of explicitly modeling the entropy from future steps, we com- pared ERAC with the variant that only applies the entropy regularization to the actor but not to the critic. In other words, the τ is set to 0 when per- forming policy evaluating according to <ref type="bibr">Eqn. (4.2)</ref>, while the τ for the entropy gradient in Eqn. <ref type="formula" target="#formula_1">(20)</ref> remains. The comparison result based on 9 runs on test set of IWSTL 2014 is shown in <ref type="table">Table 4</ref>. As we can see, simply adding a local entropy gradient does not even improve upon the AC. This further verifies the difference between ERAC and A3C, and shows the importance of taking future entropy into consideration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm</head><p>Mean Max  <ref type="table">Table 4</ref>: Comparing ERAC with the variant with- out considering future entropy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>In this work, motivated by the intriguing con- nection between the token-level RAML and the entropy-regularized RL, we propose two algo- rithms for neural sequence prediction. Despite the distinct training procedures, both algorithms com- bine the idea of fine-grained credit assignment and the entropy regularization, leading to positive em- pirical results.</p><p>However, many problems remain widely open. In particular, the oracle Q-function Q φ we obtain is far from perfect. We believe the ground-truth reference contains sufficient information for such an oracle, and the current bottleneck lies in the RL algorithm. Given the numerous potential applica- tions of such an oracle, we believe improving its accuracy will be a promising future direction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: ERAC's average performance over multiple runs on two tasks when varying τ .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 .</head><label>2</label><figDesc>8 As we can see, both VAML and ERAC outperform previous methods, with ERAC leading the comparison with a significant margin. This further verifies the effectiveness of the two proposed algorithms.</figDesc><table>Algorithm 
BLEU 

MIXER (Ranzato et al., 2015) 
20.73 
BSO (Wiseman and Rush, 2016) 
27.9 
Q(BLEU) (Li et al., 2017) 
28.3 
AC (Bahdanau et al., 2016) 
28.53 
RAML (Ma et al., 2017) 
28.77 

VAML 
28.94 
ERAC 
29.36 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Comparison with existing algorithms on 
IWSTL 2014 dataset for MT. All numbers of pre-
vious algorithms are from the original work. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Average validation BLEU of ERAC. As 
a reference, the average BLEU is 28.1 for MLE. 
λ var = 0 means not using the smoothing technique. 
β = 1 means not using a target network.  † indi-
cates excluding extreme values due to divergence. 

strength of the smoothness regularization. The av-
erage validation performances of different hyper-
parameter values are summarized in Tab. 3. 

</table></figure>

			<note place="foot" n="1"> In this work, we use the terms &quot;ground-truth&quot; and &quot;reference&quot; to refer to the empirical observations interchangeably.</note>

			<note place="foot" n="2"> To avoid clutter, the conditioning on x * will be omitted in the sequel, assuming it&apos;s clear from the context.</note>

			<note place="foot" n="1"> ; y * ). (8) Proof. See Appendix A.1. Note that, instead of giving an explicit form for the token-level target distribution, Proposition 1 only provides an equivalent condition in the form of an implicit recursion. Thus, we haven&apos;t obtained a practical algorithm yet. However, as we will discuss next, the recursion has a deep connection to entropy regularized RL, which ultimately inspires our proposed algorithms. 3 Connection to Entropy-regularized RL Before we dive into the connection, we first give a brief review of the entropy-regularized RL. For an in-depth treatment, we refer readers to (Ziebart, 2010; Schulman et al., 2017). 3.1 Background: Entropy-regularized RL Following the standard convention of RL, we denote a Markov decision process (MDP) by a tuple M = (S, A, p s , r, γ), where S, A, p s , r, γ are the state space, action space, transition probability, reward function and discounting factor respectively. 3 3 In sequence prediction, we are only interested in the periodic (finite horizon) case.</note>

			<note place="foot" n="4"> See Appendix A.2 for a detailed derivation.</note>

			<note place="foot" n="5"> Different from Bahdanau et al. (2016), we don&apos;t use a delayed actor network to collect trajectories for critic training.</note>

			<note place="foot" n="6"> https://github.com/zihangdai/ERAC-VAML</note>

			<note place="foot" n="7"> For AC, ERAC and VAML, 3 different critics are trained first, and each critic is then used to train 3 actors.</note>

			<note place="foot" n="8"> For a more detailed comparison of performance together with the model architectures, see Table 7 in Appendix C.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">An actor-critic algorithm for sequence prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philemon</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.07086</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Residual algorithms: Reinforcement learning with function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leemon</forename><surname>Baird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Proceedings</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="30" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1171" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Listen, attend and spell: A neural network for large vocabulary conversational speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4960" to="4964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning as search optimization: Approximate large margin methods for structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on Machine learning</title>
		<meeting>the 22nd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuomas</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08165</idno>
		<title level="m">Reinforcement learning with deep energy-based policies</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuomas</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurick</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01290</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Toward neural phrasebased machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep visualsemantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Professor forcing: A new algorithm for training recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex M</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alias</forename><surname>Parth Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4601" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning to decode for future success</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06549</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">J</forename><surname>Timothy P Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.02971</idno>
		<title level="m">Continuous control with deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.04025</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Softmax qdistribution estimation for structured prediction: A theoretical interpretation for raml</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingzhou</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07136</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Wit3: Web inventory of transcribed and translated talks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cettolo</forename><surname>Mauro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girardi</forename><surname>Christian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of European Association for Machine Translation</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="261" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Asynchronous methods for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adria</forename><forename type="middle">Puigdomenech</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1928" to="1937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">K</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page">529</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bridging the gap between value and policy based reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Nachum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2772" to="2782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Reward augmented maximum likelihood for neural structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1723" to="1731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06732</idno>
		<title level="m">Sequence level training with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Self-critical sequence training for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Steven J Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jarret</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaibhava</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00563</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alexander M Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.00685</idno>
		<title level="m">Sumit Chopra, and Jason Weston. 2015. A neural attention model for abstractive sentence summarization</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06440</idno>
		<title level="m">Equivalence between policy gradients and soft qlearning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.02433</idno>
		<title level="m">Minimum risk training for neural machine translation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on. IEEE</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Function optimization using connectionist reinforcement learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Connection Science</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="241" to="268" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Sequence-to-sequence learning as beam-search optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02960</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Modeling purposeful adaptive behavior with the principle of maximum causal entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brian D Ziebart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Maximum entropy inverse reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Brian D Ziebart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anind K</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<meeting><address><addrLine>Chicago, IL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1433" to="1438" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
