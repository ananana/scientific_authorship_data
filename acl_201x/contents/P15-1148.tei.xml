<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:56+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Optimal Shift-Reduce Constituent Parsing with Structured Perceptron</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><forename type="middle">Quang</forename><surname>Thang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">The Graduate University for Advanced Studies</orgName>
								<orgName type="institution" key="instit1">Hanoi University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">National Institute of Informatics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Noji</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">The Graduate University for Advanced Studies</orgName>
								<orgName type="institution" key="instit1">Hanoi University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">National Institute of Informatics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">The Graduate University for Advanced Studies</orgName>
								<orgName type="institution" key="instit1">Hanoi University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">National Institute of Informatics</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Optimal Shift-Reduce Constituent Parsing with Structured Perceptron</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1534" to="1544"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a constituent shift-reduce parser with a structured perceptron that finds the optimal parse in a practical run-time. The key ideas are new feature templates that facilitate state merging of dynamic programming and A* search. Our system achieves 91.1 F1 on a standard English experiment, a level which cannot be reached by other beam-based systems even with large beam sizes. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A parsing system comprises two components: a scoring model for a tree and a search algorithm. In shift-reduce parsing, the focus of most previ- ous studies has been the former, typically by en- riching feature templates, while the search quality has often been taken less seriously. For example, the current state-of-the-art parsers for constituency ( <ref type="bibr" target="#b26">Zhu et al., 2013;</ref><ref type="bibr" target="#b23">Wang and Xue, 2014</ref>) and de- pendency ( <ref type="bibr" target="#b0">Bohnet et al., 2013</ref>) both employ beam search with a constant beam size, which may suf- fer from severe search errors. This is contrary to ordinary PCFG parsing which, while it often uses some approximations, has nearly optimal quality <ref type="bibr" target="#b19">(Petrov and Klein, 2007)</ref>.</p><p>In this paper, we instead investigate the question of whether we can obtain a practical shift-reduce parser with state-of-the-art accuracy by focusing on optimal search quality like PCFG parsing. We base our system on best-first search for shift- reduce parsing formulated in <ref type="bibr" target="#b25">Zhao et al. (2013)</ref>, but it differs from their approach in two points. First, we focus on constituent parsing while they use dependency grammar. Second, and more cru- cially, they use a locally trained MaxEnt model, which is simple but not strong, while we explore a structured perceptron, the current state-of-the-art in shift-reduce parsing ( <ref type="bibr" target="#b26">Zhu et al., 2013)</ref>.</p><p>As we will see, this model change makes search quite hard, which motivates us to invent new fea- ture templates as well as to improve the search algorithm. In existing parsers, features are com- monly exploited from the parsing history, such as the top k elements on the stack. However, such features are expensive in terms of search ef- ficiency. Instead of relying on features primarily from the stack, our features mostly come from the span of the top few nodes, an idea inspired by the recent empirical success in CRF parsing ( <ref type="bibr" target="#b7">Hall et al., 2014</ref>). We show that these span features also fit quite well in the shift-reduce system and lead to state-of-the-art accuracy. We further improve search with new A* heuristics that make optimal search for shift-reduce parsers with a structured perceptron tractable for the first time.</p><p>The primary contribution of this paper is to demonstrate the effectiveness and the practicality of optimal search for shift-reduce parsing, espe- cially when combined with appropriate features and efficient search. In English Penn Treebank ex- periments, our parser achieves an F1 score of 91.1 on test set at a speed of 13.6 sentences per second. This score is in excess of that of a beam-based sys- tem with larger beam size and same speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Shift-Reduce Constituent Parsing</head><p>We first introduce the shift-reduce algorithm for constituent structures. For space reasons, our ex- position is rather informal; See <ref type="bibr" target="#b24">Zhang and Clark (2009)</ref> for details. A shift-reduce parser parses a sentence through transitions between states, each of which consists of two data structures of a stack and a queue. The stack preserves intermediate parse results, while the queue saves unprocessed tokens. At each step, a parser selects an action, which changes the current state into the new one. For example, SHIFT pops the front word from the queue and pushes it onto the stack, while RE- DUCE(X) combines the top two elements on the stack into their parent. <ref type="bibr">2</ref> For example, if the top two elements on the stack are DT and NN, RE- DUCE(NP) combines these by applying the CFG rule NP → DT NN.</p><p>Unary Action The actions above are essentially the same as those in shift-reduce dependency pars- ing <ref type="bibr" target="#b17">(Nivre, 2008)</ref>, but a special action for con- stituent parsing UNARY(X) complicates the sys- tem and search. For example, if the top element on the stack is NN, UNARY(NP) changes it to NP by applying the rule NP → NN. In particular, this causes inconsistency in the numbers of actions be- tween derivations ( <ref type="bibr" target="#b26">Zhu et al., 2013)</ref>, which makes it hard to apply the existing best first search for de- pendency grammar to our system. We revisit this problem in Section 3.1.</p><p>Model The model of a shift-reduce parser gives a score to each derivation, i.e., an action sequence a = (a 1 , · · · , a |a| ), in which each a i is a shift or reduce action. Let p = (p 1 , · · · , p |a| ) be the se- quence of states, where p i is the state after apply- ing a i to p i−1 . p 0 is the initial state for input sen- tence w. Then, the score for a derivation Φ(a) is calculated as the total score of every action:</p><formula xml:id="formula_0">Φ(a) = 1≤i≤|a| φ(a i , p i−1 ).<label>(1)</label></formula><p>There are two well-known models, in which the crucial difference is in training criteria. The Max- Ent model is trained locally to select the correct action at each step. It assigns a probability for each action a i as</p><formula xml:id="formula_1">P (a i |p i−1 ) ∝ exp(θ f (a i , p i−1 )),<label>(2)</label></formula><p>where θ and f (a, p) are weight and feature vec- tors, respectively. Note that the probability of an action sequence a under this model is the product of local probabilities, though we can cast the total score in summation form (1) by using the log of (2) as a local score φ(a i , p i−1 ). The structured perceptron is instead trained globally to select the correct action sequence given an input sentence. It does not use probability and the local score is just φ(a i , p i−1 ) = θ f (a i , p i−1 ). In practice, this global model is much stronger than the local MaxEnt model. However, train- ing this model without any approximation is hard, and the common practice is to rely on well-known heuristics such as an early update with beam search ( <ref type="bibr" target="#b4">Collins and Roark, 2004</ref>). We are not aware of any previous study that succeeded in training a structured perceptron for parsing with- out approximation. We will show how this be- comes possible in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Previous Best-First Shift-Reduce Parsing</head><p>The basic idea behind best-first search (BFS) for shift-reduce parsing is assuming each parser state as a node on a graph and then searching for the minimal cost path from a start state (node) to the final state. This is the idea of <ref type="bibr" target="#b21">Sagae and Lavie (2006)</ref>, and it was later refined by <ref type="bibr" target="#b25">Zhao et al. (2013)</ref>. BFS gives a priority to each state, and a state with the highest priority (lowest cost) is al- ways processed first. BFS guarantees that the first found goal is the best (optimality) if the superior- ity condition is satisfied: a state never has a lower cost than the costs of its previous states.</p><p>Though the found parse is guaranteed to be op- timal, in practice, current BFS-based systems are not stronger than other systems with approximate search ( <ref type="bibr" target="#b26">Zhu et al., 2013;</ref><ref type="bibr" target="#b23">Wang and Xue, 2014</ref>) since all existing systems are based on the MaxEnt model. With this model, the speriority can easily be accomplished by using the negative log of (2), which is always positive and becomes smaller with higher probability. We focus instead on the struc- tured perceptron, but achieving superiority with this model is not trivial. We resolve this problem in Section 3.1.</p><p>In addition to the mathematical convenience, the MaxEnt model itself helps search. Sagae and Lavie ascribe the empirical success of their BFS to the sparseness of the distribution over subsequent actions in the MaxEnt model. In other words, BFS is very efficient when only a few actions have dominant probabilities in each step, and the Max- Ent model facilitates this with its exponential oper- ation (2). Unfortunately, this is not the case in our global structured perceptron because the score of each action is just the sum of the feature weights. Resolving this search difficulty is the central prob- lem of this paper; we illustrate this problem in Sec- tion 4 and resolve it in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Hypergraph Search of Zhao et al. (2013)</head><p>The worst time complexity of BFS in <ref type="bibr" target="#b21">Sagae and Lavie (2006)</ref> is exponential. For dependency pars- ing, <ref type="bibr" target="#b25">Zhao et al. (2013)</ref> reduce it to polynomial by converting the search graph into a hypergraph by using the state merging technique of <ref type="bibr" target="#b8">Huang and Sagae (2010)</ref>. This hypergraph search is the basis of our parser, so we will briefly review it here.</p><p>The algorithm is closely related to agenda- based best-first parsing algorithms for PCFGs ( <ref type="bibr" target="#b9">Klein and Manning, 2001;</ref><ref type="bibr" target="#b18">Pauls and Klein, 2009)</ref>. As in those algorithms, it maintains two data structures: a chart C that preserves processed states as well as a priority queue (agenda) Q. The difference is in the basic items processed in C and Q. In PCFG parsing, they are spans. Each span abstracts many derivations on that span and the chart maps a span to the best (lowest cost) deriva- tion found so far. In shift-reduce parsing, the basic items are not spans but states, i.e., partial represen- tations of the stack. <ref type="bibr">3</ref> We denote p = i, j, s d ...s 0 where s i is the i-th top subtree on the stack and s 0 spans i to j. We extract features from s d ...s 0 . Note that d is constant and a state usually does not con- tain full information about a derivation. In fact, it only keeps atomic features, the minimal informa- tion on the stack necessary to recover the full fea- tures and packs many derivations. The chart maps a state to the current best derivation. For example, if we extract features only from the root symbol of s 0 , each state looks the same as a span of PCFGs.</p><p>Differently from the original shift-reduce algo- rithm, during this search, reduce actions are de- fined between two states p and q. The basic oper- ation of the algorithm is to pop the best (top) state p from the queue, push it into the chart, and then enqueue every state that can be obtained by a re- duce action between p and other states in the chart or a shift action from p. The left states L(p) and right states R(p) are important concepts. L(p) is a set of states in the chart, with which p can reduce from the right side. Formally,</p><formula xml:id="formula_2">L(i, j, s d ...s 0 ) = {{h, i, s d ...s 0 |∀k ∈ [1, d], f k (s k−1 ) = f k (s k )},</formula><p>where f k (·) returns atomic features on the k-th top node. See <ref type="figure" target="#fig_3">Figure 4</ref> for how they look like in con- stituent parsing. R(p) is defined similarly; p can reduce q ∈ R(p) from the left side. When p is popped, it searches for every L(p) and R(p) in the chart and tries to expand the current derivation. The priority for each state is a pair (c, v). c is the prefix cost that is the total cost to reach that state, while v is the inside cost, a cost to build the top node s 0 . The top state in the queue has the lowest prefix cost, or the lowest inside cost if the two prefix costs are the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Best-First Shift-Reduce Constituent Parsing with Structured Perceptron</head><p>This section describes our basic parsing system, i.e., shift-reduce constituent parsing with BFS and the structured perceptron. We have to solve two problems. The first is how to achieve BFS with the structured perceptron, and the second is how to apply that BFS to constituent parsing. Interest- ingly, the solution to the first problem makes the second problem relatively trivial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Superiority of Structured Perceptron</head><p>We must design each priority of a state to sat- isfy the superiority condition.</p><formula xml:id="formula_3">φ(a i , p i−1 ) = θ f (a i , p i−1 )</formula><p>is the usual local score employed in structured perceptrons (Huang and Sagae, 2010) but we cannot use it as a local cost for two rea- sons. First, in our system, the best parse should have the lowest cost; it is opposite in the ordinary setting <ref type="bibr" target="#b6">(Collins, 2002</ref>). We can resolve this con- flict by changing the direction of structured per- ceptron training so that the best parse has the low- est score. <ref type="bibr">4</ref> Second, each φ(a i , p i−1 ) can take a negative value but the cost should always be pos- itive. This is in contrast to the MaxEnt model in which the negative log probability is always pos- itive. Our strategy is to add a constant offset δ to every local cost. If δ is large enough so that ev- ery score is positive, the superiority condition is satisfied. 5</p><p>Unary Merging Though this technique solves the problem with the structured perceptron for a simpler shift-reduce system, say for dependency grammar, the existence of unary actions, as men- tioned in Section 2.1, requires additional effort in order to apply it to constituent parsing. In particu- lar, constituent parsing takes different numbers of SH state p:</p><formula xml:id="formula_4">, j, s d ...s 0 : (c, ) j, j + 1, s d−1 ...s 0 |t j (w j ) : (c + c sh (p), c sh (p)) j &lt; n SHU(X) state p: , j, s d ...s 0 : (c, ) j, j + 1, s d−1 ...s 0 |X(t j (w j )) : (c + c shu(X) (p), c shu(X) (p)) j &lt; n RE(X) state q: k, i, s d ...s 0 : (c , v ) state p: i, j, s d ...s 0 : (c, v) k, j, s d ...s 1 |X(s 0 , s 0 ) : (c + v + c re(X) (p), v + v + c re(X) (p)) q ∈ L(p) REU(Y, X) state q: k, i, s d ...s 0 : (c , v ) state p: i, j, s d ...s 0 : (c, v) k, j, s d ...s 1 |Y(X(s 0 , s 0 )) : (c + v + c reu(Y, X) (p), v + v + c reu(Y, X) (p)) q ∈ L(p)</formula><p>Figure 1: The deductive system of our best-first shift-reduce constituent parsing explaining how the prefix cost and inside cost are calculated. FIN is omitted. | on the stack means an append operation and a(b) means a subtree a → b. t j is the POS tag of j-th token while w j is the surface form. c a (p) is the cost for an action a of which features are extracted from p. Each c a (p) implicitly includes an offset δ.</p><p>actions for each derivation, which means that the scores of two final states may contain different off- set values. The existing modification to alleviate this inconsistency ( <ref type="bibr" target="#b26">Zhu et al., 2013</ref>) cannot be ap- plied here because it is designed for beam search.</p><p>We instead develop a new transition system, in which the number of actions to reach the final state is always 2n (n is the length of sentence). The basic idea is merging a unary action into each shift or reduce action. Our system uses five actions:</p><p>• SH: original shift action;</p><p>• SHU(X): shift a node, then immediately ap- ply a unary rule to that node;</p><p>• RE(X): original reduce action;</p><p>• REU(Y, X): do reduce to X first, then imme- diately apply an unary rule Y → X to it;</p><p>• FIN: finish the process.</p><p>Though the system cannot perform consecutive unary actions, in practice it can generate any unary chains as long as those in the training corpus by collapsing a chain into one rule. We preprocess the corpus in this way along with binarization (See Section 4). Note that this system is quite similar to the tran- sition system for dependency parsing. The only changes are that we have several varieties of shift and reduce actions. This modification also makes it easy to apply an algorithm developed for de- pendency parsing to constituent parsing, such as dynamic programming with beam search <ref type="bibr" target="#b8">(Huang and Sagae, 2010</ref>), which has not been applied into constituent parsing until quite recently <ref type="bibr" target="#b16">(Mi and Huang, 2015)</ref> (See Section 7).</p><p>Algorithm 1 BFS for Constituent Parsing; Only differences from <ref type="bibr" target="#b25">Zhao et al. (2013)</ref> 1: procedure SHIFT(x, Q)</p><p>2:</p><formula xml:id="formula_5">TRYADD(sh(x), Q) 3:</formula><p>for y ∈ shu(x) do for (x, y) ∈ A × B do</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>for z ∈ re(x, y) ∪ reu(x, y) do 8:</p><p>TRYADD(z, Q)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">BFS with Dynamic Programming</head><p>Now applying BFS of <ref type="bibr" target="#b25">Zhao et al. (2013)</ref> for de- pendency parsing into constituent parsing is not hard. <ref type="figure">Figure 1</ref> shows the deductive system of dy- namic programming, which is much similar to that in dependency parsing. One important change is that we include a cost for a shift (SH or SHU) ac- tion in the prefix cost in a shift step, not a reduce step as in <ref type="bibr" target="#b25">Zhao et al. (2013)</ref>, since it is unknown whether the top node s 0 of a state p is instantiated with SH or SHU. This modification keeps the cor- rectness of the algorithm and has been employed in another system ( <ref type="bibr" target="#b13">Kuhlmann et al., 2011</ref>). The algorithm is also slightly changed. We show only the difference from <ref type="bibr" target="#b25">Zhao et al. (2013)</ref> (Algorithm 1) in Algorithm 1. shu(x) is a func- tion which returns the set of states that can be ar- rived at by possible SHU rules applied to the state x. re(x, y) and reu(x, y) are similar, and they re- turn the set of states arrived at through one of RE or REU actions. As a speed up, we can apply a lazy expansion technique (we do so in our experiment).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><note type="other">F1 Speed (Sent./s.) SP (reduced features) 88.9</note><p>0.8 ME (reduced features) 85.1 4.8 ME <ref type="table">(full features)</ref> 86.3 2.5 <ref type="table">Table 1</ref>: Results of BFS systems with dynamic programming for the Penn Treebank development set with different models and features. SP = the structured perceptron; ME = the MaxEnt.</p><p>Another difference is in training. The previ- ous best-first shift-reduce parsers are all trained in the same way as a parser with greedy search since the model is local MaxEnt. In our case, we can use structured perceptron training with exact search <ref type="bibr" target="#b6">(Collins, 2002)</ref>; that is, at each iteration for each sentence, we find the current argmin deriva- tion with BFS, then update the parameters if it dif- fers from the gold derivation. Note that at the be- ginning of training, BFS is inefficient due to the initial flat parameters. We use a heuristic to speed up this process: For a few iterations (five, in our case), we train the model with beam search and an early update ( <ref type="bibr" target="#b4">Collins and Roark, 2004</ref>). We find that this approximation does not affect the perfor- mance, while it greatly reduces the training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation of Best-First Shift-Reduce Constituent Parsing</head><p>This section evaluates the empirical performance of our best-first constituent parser that we built in the previous section. As mentioned in Section 2.2, the previous empirical success of best-first shift- reduce parsers might be due to the sparsity prop- erty of the MaxEnt model, which may not hold true in the structured perceptron. We investigate the validity of this assumption by comparing two systems, a locally trained MaxEnt model and a globally trained structured perceptron.</p><p>Setting We follow the standard practice and train each model on section 2-21 of the WSJ Penn Treebank ( <ref type="bibr" target="#b14">Marcus et al., 1993)</ref>, which is binarized using the algorithm in <ref type="bibr" target="#b24">Zhang and Clark (2009)</ref> with the head rule of Collins (1999). We report the F1 scores for the development set of section 22. The Stanford POS tagger is used for part-of- speech tagging. <ref type="bibr">6</ref> We used the EVALB program to evaluate parsing performance. 7 Every exper- iment reported here was performed on hardware Feature We borrow the feature templates from <ref type="bibr" target="#b21">Sagae and Lavie (2006)</ref>. However, we found the full feature templates make training and decoding of the structured perceptron much slower, and in- stead developed simplified templates by removing some, e.g., that access to the child information on the second top node on the stack. 8</p><p>Result <ref type="table">Table 1</ref> summarizes the results that indi- cate our assumption is true. The structured per- ceptron has the best score even though we restrict the features. However, its parsing speed is much slower than that of the local MaxEnt model. To see the difference in search behaviors between the two models, <ref type="figure" target="#fig_1">Figure 2</ref> plots the number of processed (popped) states during search.</p><p>Discussion This result may seem somewhat de- pressing. We have devised a new method that en- ables optimal search for the structured perceptron, but it cannot handle even modestly large feature templates. As we will see below, the time com- plexity of the system depends on the used fea- tures. We have tried features from Sagae and Lavie (2006), but their features are no longer state-of- the-art. For example, <ref type="bibr" target="#b26">Zhu et al. (2013)</ref> report higher scores by using beam search with much richer feature templates, though, as we have exam- ined, it seems implausible to apply such features to our system. In the following, we find a practi- cal solution for improving both parse accuracy and search efficiency in our system. We will see that our new features not only make BFS tractable, but also lead to comparable or even superior accuracy relative to the current mainstream features. When 2 <ref type="figure">Figure 3</ref>: A snippet of the hypergraph for the sys- tem that simulates a simple PCFG. p is the popped state, which is being expanded with a state of its left states L(p) using a reduce rule. it is combined with A* search, the speed reaches a practical level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Improving Optimal Search Efficiency</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Span Features</head><p>The worst time complexity of hypergraph search for shift-reduce parsing can be analyzed with the deduction rule of the reduce step. <ref type="figure">Figure 3</ref> shows an example. In this case, the time complexity is O(n 3 · |G| · |N |) since there are three indices (i, j, k) and four nonterminals (A, B, C, D), on which three comprise a rule. The extra factor |N | compared with ordinary CKY parsing comes from the restriction that we extract features only from one state <ref type="bibr" target="#b8">(Huang and Sagae, 2010)</ref>.</p><p>Complexity increases when we add new atomic features to each state. For example, if we lexical- ize this model by adding features that depend on the head indices of s 0 and/or s 1 , it increases to O(n 6 · |G| · |N |) since we have to maintain three head indices of A, B, and C. This is why Sagae and Lavie's features are too expensive for our system; they rely on head indices of s 0 , s 1 , s 2 , s 3 , the left and right children of s 0 and s 1 , and so on, lead- ing prohibitively huge complexity. Historically speaking, the success of shift-reduce approach in constituent parsing has been led by its success in dependency parsing <ref type="bibr" target="#b17">(Nivre, 2008)</ref>, in which the head is the primary element, and we suspect this is the reason why the current constituent shift-reduce parsers mainly rely on deeper stack elements and their heads.</p><p>The features we propose here are extracted from fundamentally different parts from these recent trends. <ref type="figure" target="#fig_3">Figure 4</ref> explains how we extract atomic features from a state and <ref type="table">Table 2</ref> shows the full list of feature templates. Our system is unlexicalized;   <ref type="figure">s 1 )</ref>, we extract the surface form and POS tag of the preceding word (bw, bt), the first word (fw, ft), the last word (lw, lt), and the subse- quent word <ref type="bibr">(aw, at)</ref>. shape is the same as that in <ref type="bibr" target="#b7">Hall et al. (2014)</ref>. Bold symbols are additional in- formation from the system of <ref type="figure">Figure 3</ref>. The time complexity is O(n 4 · |G| 3 · |N |). <ref type="table">Table 2</ref>: All feature templates in our span model. See <ref type="figure" target="#fig_3">Figure 4</ref> for a description of each element. q i is the i-th top token on the queue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NNP . NNP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NP IN</head><formula xml:id="formula_6">q0.w • q0.t q1.w • q1.t q2.w • q2.t q3.w • q3.t s0.c • s0.ft s0.c • s0.fw s0.c • s0.lt s0.c • s0.lw s0.c • s0.at s0.c • s0.aw s0.c • s0.ft • s0.lt s0.c • s0.ft • s0.lw s0.c • s0.fw • s0.lt s0.c • s0.fw • s0.lw s0.c • s0.len s0.c • s0.shape s0.rule s0.shape • s0.rule s1.c • s1.ft s1.c • s1.fw s1.c • s1.lt s1.c • s1.lw s1.c • s1.bt s1.c • s1.bw s1.c • s1.ft • s1.lt s1.c • s1.ft • s1.lw s1.c • s1.fw • s1.lt s1.c • s1.fw • s1.lw s1.c • s1.len s1.c • s1.shape s1.rule s1.shape • s1.rule s1.lw • s0.fw s0.ft • s1.lw s1.lt • s0.fw s1.lt • s0.ft s1.c • s0.fw s0.c • s1.fw s1.c • s0.lw s0.c • s1.lw s0.fw • q0.w s0.lw • q0.w q0.t • s0.fw q0.t • s0.lw s0.c • q0.w s0.c • q0.t s1.fw • q0.w s1.lw • q0.w q0.t • s1.fw q0.t • s1.lw s1.c • q0.w s1.c • q0.t q0.w • q1.w q0.t • q1.w q0.w • q1.t q0.t • q1.t s0.c • s1.c • q0.t s0.c • s1.c • q0.w s1.c • q0.t • s0.fw s1.c • q0.t • s0.lw s0.c • q0.t • s1.fw s0.c • q0.t • s1.fw</formula><p>i.e., it does not use any head indices. This feature design is largely inspired by the recent empirical success of span features in CRF parsing ( <ref type="bibr" target="#b7">Hall et al., 2014)</ref>. Their main finding is that the surface in- formation on a subtree, such as the first or the last word of a span, has essentially the same amount of information as its head. For our system, such span features are much cheaper, so we expect they would facilitate our dynamic programming with- out sacrificing accuracy.</p><p>We customize their features for fitting in the shift-reduce framework. Unlike the usual setting of PCFG parsing, shift-reduce parsers receive a POS-tagged sentence as input, so we use both the POS tag and surface form for each word on the span. One difficult part is using features with an applied rule. We include this feature by memoriz-ing the previously applied rule for each span <ref type="bibr">(subtree)</ref>. This is a bit costly, because it means we have to preserve labels of the left and right chil- dren for each node, which lead to an additional |G| 2 factor of complexity. However, we will see that this problem can be alleviated by our heuris- tic cost functions in A* search described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">A* Search</head><p>We now explain our A* search, another key tech- nique for speeding up our search. To our knowl- edge, this is the first work to successfully apply A* search to shift-reduce parsing.</p><p>A* parsing <ref type="bibr" target="#b10">(Klein and Manning, 2003a</ref>) mod- ifies the calculation of priority σ(p i ) for state p i . In BFS, it is basically the prefix cost, the sum of every local cost (Section 3.1), which we denote as β p i :</p><formula xml:id="formula_7">β p i = 1≤j≤i (φ(a j , p j−1 ) + δ). In A* parsing, σ(p i ) = β p i + h(p i ) where h(p i )</formula><p>is a heuristic cost. β p i corresponds to the Viterbi inside cost of PCFG parsing <ref type="bibr" target="#b10">(Klein and Manning, 2003a)</ref> while h(p i ) is the Viterbi outside cost, an approximation of the cost for the future best path (action sequence) from p i . h(p i ) must be a lower bound of the true Viterbi outside cost. In PCFG parsing, this is often achieved with a technique called projection. Let G * be a projected, or relaxed, grammar of the orig- inal G; then, a rule weight in the relaxed gram- mar w r * will become w r * = min r∈G:π(r)=r * w r , where π(r) is a projection function which returns the set of rules that correspond to r in G * .</p><p>In feature-based shift-reduce parsing, a rule weight corresponds to the sum of feature weights for an action a, that is, φ(a, p i ) = θ f (a, p i ). We calculate h(p i ) with a relaxed feature function φ * (a, p i ), which always returns a lower bound:</p><formula xml:id="formula_8">φ * (a, p i ) = θ * f (a, c i ) ≤ θ f (a, p i ) = φ(a, p i ).</formula><p>Note that we only have to modify the weight vec- tor. If a relaxed weight satisfies θ * (k) ≤ θ(k) for all k, that projection is correct.</p><p>Our A* parsing is essentially hierarchical A* parsing <ref type="bibr" target="#b18">(Pauls and Klein, 2009)</ref>, and we calculate a heuristic cost h(p) on the fly using another chart for the relaxed space when a new state p is pushed into the priority queue. Below we introduce two different projection methods, which are orthogo- nal and later combined hierarchically. <ref type="table">Table 3</ref>: Example of our feature projection. θ GP is a weight vector with the GP, which collapses every c. θ LF is with the LF, which collapses all elements in <ref type="table">Table 4</ref>. <ref type="table">Table 4</ref>: List of feature elements ignored in the LF.</p><formula xml:id="formula_9">a • s 1 .c • s 1 .ft θ θ GP θ LF SH • VP • NN 10.53 -5.82 -5.82 SH • SBAR • NN 1.98 -5.82 -5.82 SH • NP • NN -5.82 -5.82 -5.82 · · · SH • VP • DT 3.25 1.12 -5.82 SH • SBAR • DT 1.12 1.12 -5.82 SH • NP • DT 1.98 1.12 -5.82 · · ·</formula><formula xml:id="formula_10">s 1 .c s 1 .ft s 1 .fw s 1 .bt s 1 .bw s 1 .len s 1 .shape s 1 .rule s 0 .rule</formula><p>Grammar Projection (GP) Our first projection borrows the idea from the filter projection of <ref type="bibr" target="#b10">Klein and Manning (2003a)</ref>, in which the grammar sym- bols (nonterminals) are collapsed into a single la- bel X. Our projection, however, does not collapse all the labels into X; instead, we utilize constituent labels in level 2 from , in which labels that tend to be head, such as S or VP are collapsed into HP and others are collapsed into MP. θ G in <ref type="table">Table 3</ref> is an example of how feature weights are relaxed with this projection. Here we show each feature as a tuple including action name (a). Let π GP be a feature projection function: e.g.,</p><formula xml:id="formula_11">((a • s 1 .c • s 1 .ft) = (SH • VP • NN)) → πGP ((a • s 1 .c • s 1 .ft) = (SH • HP • NN)).</formula><p>Formally, for k-th feature, the weight θ GP (k) is determined by minimizing over the features col- lapsed by π GP :</p><formula xml:id="formula_12">θ GP (k) = min 1≤k ≤K:πGP(g k )=g k θ(k ),</formula><p>where g k is the value of the k-th feature.</p><p>Less-Feature Projection (LF) The basic idea of our second projection is to ignore some of the atomic features in a feature template so that we can reduce the time complexity for computing the heuristics. We apply this technique to the feature elements in <ref type="table">Table 4</ref>. We can do so by not filling in the actual value in each feature template: e.g., Avg. parsing time (sec.) BFS A*-LF A*-GP A*-HP <ref type="figure">Figure 5</ref>: Comparison of parsing times between different A* heuristics.</p><formula xml:id="formula_13">((a • s 1 .c • s 1 .ft) = (SH • VP • NN)) → πLF ((a • s 1 .c • s 1 .ft) = (SH • s 1 .c • s 1 .ft)).</formula><p>The elements in <ref type="table">Table 4</ref> are selected so that all bold elements in <ref type="figure" target="#fig_3">Figure 4</ref> would be eliminated; the complexity is O(n 3 · |G| · |N |). In practice, this is still expensive. However, we note that the effects of these two heuristics are complementary:</p><p>The LF reduces complexity to a cubic time bound, while the GP greatly reduces the size of grammar |G|; We combine these two ideas below.</p><p>Hierarchical Projection (HP) The basic idea of this combined projection is to use the heuristics given by the GP to lead search of the LF. This is similar to the hierarchical A* for PCFGs with multilevel symbol refinements <ref type="bibr" target="#b18">(Pauls and Klein, 2009)</ref>. The difference is that their hierarchy is on the grammar symbols while our projection targets are features. When a state p is created, its heuris- tic score h(p) is calculated with the LF, which re- quires search for the outside cost in the space of the LF, but its worst time complexity is cubic. The GP is used to guide this search. For each state p LF in the space of the LF, the GP calculates the heuris- tic score. We will see that this combination works quite well in practice in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiment</head><p>We build our final system by combining the ideas in Section 5 and the system in Section 3. We also build beam-based systems with or without dy- namic programming (DP) and with the ordinary or the new span features. All systems are trained with the structured perceptron. We use the early update for training beam-based systems.</p><p>Effect of A* heuristics <ref type="figure">Figure 5</ref> shows the ef- fects of A* heuristics. In terms of search quality, the LF is better; it prunes 92.5% of states com- pared to naive BFS, while the GP prunes 75%. However, the LF takes more time to calculate  <ref type="table">Table 5</ref>: Results for the Penn Treebank develop- ment set. Z&amp;C = feature set of <ref type="bibr" target="#b24">Zhang and Clark (2009)</ref>. The speeds of non-DP and DP are the same, so we omit them from the comparison.</p><p>heuristics than the GP. The HP combines the ad- vantages of both, achieving the best result.</p><p>Accuracy and Speed The F1 scores for the de- velopment set are summarized in <ref type="table">Table 5</ref>. We can see that the systems with our new feature (span) perform surprisingly well, at a competitive level with the more expensive features of <ref type="bibr">Zhang and Clark (2009) (Z&amp;C)</ref>. This is particularly true with DP; it sometimes outperforms Z&amp;C, probably be- cause our simple features facilitate state merging of DP, which expands search space. However, our main result that the system with optimal search gets a much higher score (90.7 F1) than beam- based systems with a larger beam size (90.2 F1) indicates that ordinary beam-based systems suffer from severe search errors even with the help of DP. Though our naive BFS is slow (1.12 sent./s.), A* search considerably improves parsing speed (13.6 sent./s.), and is faster than the beam-based system with a beam size of 64 ( <ref type="figure">Figure 6</ref>).</p><p>Unary Merging We have not mentioned the ef- fect of our unary merging (Section 3), but the re- sult indicates it has almost the same effect as the previously proposed padding method (Zhu et al.,   Final Experiment <ref type="table" target="#tab_2">Table 6</ref> compares our pars- ing system with those of previous studies. When we look at closed settings, where no external re- source other than the training Penn Treebank is used, our system outperforms all other systems including the Berkeley parser <ref type="bibr" target="#b19">(Petrov and Klein, 2007)</ref> and the Stanford parser ( <ref type="bibr" target="#b22">Socher et al., 2013</ref>) in terms of F1. The parsing systems with exter- nal features or reranking outperform our system. However, it should be noted that our system could also be improved by external features. For exam- ple, the feature of type-level distributional sim- ilarity, such as Brown clustering <ref type="bibr" target="#b1">(Brown et al., 1992)</ref>, can be incorporated with our system with- out changing the theoretical runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work and Discussion</head><p>Though the framework is shift-reduce, we can notice that our system is strikingly similar to the CKY-based discriminative parser ( <ref type="bibr" target="#b7">Hall et al., 2014</ref>) because our features basically come from two nodes on the stack and their spans. From this viewpoint, it is interesting to see that our system outperforms theirs by a large margin ( <ref type="figure">Figure 6</ref>). Identifying the source of this performance change is beyond the scope of this paper, but we believe this is an important question for future parsing research. For example, it is interesting to see whether there is any structural advantage for shift- reduce over CKY by comparing two systems with exactly the same feature set. As shown in Section 4, the previous optimal parser on shift-reduce ( <ref type="bibr" target="#b21">Sagae and Lavie, 2006</ref>) was not so strong because of the locality of the model. Other optimal parsing systems are often based on relatively simple PCFGs, such as unlex- icalized grammar <ref type="bibr" target="#b11">(Klein and Manning, 2003b</ref>) or factored lexicalized grammar ( <ref type="bibr" target="#b12">Klein and Manning, 2003c</ref>) in which A* heuristics from the unlexical- ized grammar guide search. However, those sys- tems are not state-of-the-art probably due to the limited context captured with a simple PCFG. A recent trend has thus been extending the context of each rule <ref type="bibr" target="#b19">(Petrov and Klein, 2007;</ref><ref type="bibr" target="#b22">Socher et al., 2013)</ref>, but the resulting complex grammars make exact search intractable. In our system, the main source of information comes from spans as in CRF parsing. This is cheap yet strong, and leads to a fast and accurate parsing system with optimality.</p><p>Concurrently with this work, <ref type="bibr" target="#b16">Mi and Huang (2015)</ref> have developed another dynamic program- ming for constituent shift-reduce parsing by keep- ing the step size for a sentence to 4n − 2, instead of 2n, with an un-unary (stay) action. Their final score is 90.8 F1 on WSJ. Though they only experi- ment with beam-search, it is possible to build BFS with their transition system as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>To date, all practical shift-reduce parsers have re- lied on approximate search, which suffers from search errors but also allows to utilize unlimited features. The main result of this paper is to show another possibility of shift-reduce by proceeding in an opposite direction: By selecting features and improving search efficiency, a shift-reduce parser with provable search optimality is able to find very high quality parses in a practical runtime.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparison of the average number of the processed states of the structured perceptron with those of the MaxEnt model. equipped with an Intel Corei5 2.5GHz processor and 16GB of RAM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Atomic features of our system largely come from the span of a constituency. For each span (s 0 and s 1 ), we extract the surface form and POS tag of the preceding word (bw, bt), the first word (fw, ft), the last word (lw, lt), and the subsequent word (aw, at). shape is the same as that in Hall et al. (2014). Bold symbols are additional information from the system of Figure 3. The time complexity is O(n 4 · |G| 3 · |N |).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>2013).</head><label></label><figDesc>The score with the non-DP beam size = 16 and Z&amp;C (89.1 F1) is the same as that reported in their paper (the features are the same).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>The final results for section 23 of the 
Penn Treebank. The systems with  † are re-
ported by authors running on different hardware. 
We divide baseline state-of-the-art systems into 
three categories: shift-reduce systems (Sagae and 
Lavie, 2005; Sagae and Lavie, 2006; Zhu et 
al., 2013), other chart-based systems (Petrov and 
Klein, 2007; Socher et al., 2013), and the systems 
with external semi supervised features or rerank-
ing (Charniak and Johnson, 2005; McClosky et al., 
2006; Zhu et al., 2013). 

</table></figure>

			<note place="foot" n="1"> The open source software of our system is available at https://github.com/mynlp/optsr.</note>

			<note place="foot" n="2"> Many existing constituent parsers use two kinds of reduce actions for selecting the direction of its head child while we do not distinguish these two. In our English experiments, we found no ambiguity for head selection in our binarized grammar (See Section 4).</note>

			<note place="foot" n="3"> Although Zhao et al. (2013) explained that the items in Q are derivations (not states), we can implement Q as a set of states by keeping backpointers in a starndard way.</note>

			<note place="foot" n="4"> This is easily accomplished by inverting all signs of the update equations. 5 To find this value, we train our system using beam search with several beam sizes, choosing the maximum value of the action score during training.</note>

			<note place="foot" n="6"> http://nlp.stanford.edu/software/tagger.shtml 7 http://nlp.cs.nyu.edu/evalb</note>

			<note place="foot" n="8"> The other features that we removed are features 9-14 defined in Figure 1 of Sagae and Lavie (2006).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Katsuhiko Hayashi for answering our questions about dynamic program-ming on shift-reduce parsing.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Joint Morphological and Syntactic Analysis for Richly Inflected Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><forename type="middle">M</forename><surname>Boguslavsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richárd</forename><surname>Farkas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="429" to="440" />
			<date type="published" when="2013-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Class-based N-gram Models of Natural Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenifer</forename><forename type="middle">C</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="479" />
			<date type="published" when="1992-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Coarseto-Fine n-Best Parsing and MaxEnt Discriminative Reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)</title>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06" />
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multilevel Coarse-to-Fine PCFG Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha</forename><surname>Elsner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Austerweil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>Haxton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shrivaths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Pozar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Vu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the NAACL, Main Conference</title>
		<meeting>the Human Language Technology Conference of the NAACL, Main Conference<address><addrLine>New York City, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006-06" />
			<biblScope unit="page" from="168" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Incremental Parsing with the Perceptron Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL&apos;04), Main Volume</title>
		<meeting>the 42nd Meeting of the Association for Computational Linguistics (ACL&apos;04), Main Volume<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-07" />
			<biblScope unit="page" from="111" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Head-Driven Statistical Models for Natural Language Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2002 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2002-07" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Less Grammar, More Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="228" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dynamic Programming for Linear-Time Incremental Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-07" />
			<biblScope unit="page" from="1077" to="1086" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Parsing and Hypergraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Workshop on Parsing Technologies (IWPT-2001)</title>
		<meeting>the Seventh International Workshop on Parsing Technologies (IWPT-2001)<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-10-19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A* Parsing: Fast Exact Viterbi Parse Selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, NAACL &apos;03</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, NAACL &apos;03<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="40" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Accurate Unlexicalized Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sapporo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-07" />
			<biblScope unit="page" from="423" to="430" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Factored A * Search for Models over Sequences and Trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI-03, Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence</title>
		<meeting><address><addrLine>Acapulco, Mexico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-08-09" />
			<biblScope unit="page" from="1246" to="1251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dynamic Programming Algorithms for Transition-Based Dependency Parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Kuhlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Gómez-Rodríguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Satta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="673" to="682" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Building a Large Annotated Corpus of English: The Penn Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">COMPUTATIONAL LINGUISTICS</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reranking and Self-Training for Parser Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-07" />
			<biblScope unit="page" from="337" to="344" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Shift-Reduce Constituency Parsing with Dynamic Programming and POS Tag Lattice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on the North American Chapter of the Association for Computational Linguistics Human Language Technologies</title>
		<meeting>the 2015 Conference on the North American Chapter of the Association for Computational Linguistics Human Language Technologies<address><addrLine>Denver, Colorado, May</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithms for Deterministic Incremental Dependency Parsing. Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="513" to="553" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hierarchical Search for Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Pauls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics<address><addrLine>Boulder, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="557" to="565" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improved Inference for Unlexicalized Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference</title>
		<meeting><address><addrLine>Rochester, New York</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007-04" />
			<biblScope unit="page" from="404" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<title level="m">Proceedings of the Ninth International Workshop on Parsing Technology, chapter A Classifier-Based Parser with Linear Run-Time Complexity</title>
		<meeting>the Ninth International Workshop on Parsing Technology, chapter A Classifier-Based Parser with Linear Run-Time Complexity</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="125" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A Best-First Probabilistic Shift-Reduce Parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions</title>
		<meeting>the COLING/ACL 2006 Main Conference Poster Sessions<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-07" />
			<biblScope unit="page" from="691" to="698" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Parsing with Compositional Vector Grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ng</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Joint POS Tagging and Transition-based Constituent Parsing in Chinese with Non-local Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="733" to="742" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Transition-Based Parsing of the Chinese Treebank using a Global Discriminative Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Parsing Technologies (IWPT&apos;09)</title>
		<meeting>the 11th International Conference on Parsing Technologies (IWPT&apos;09)<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009-10" />
			<biblScope unit="page" from="162" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Optimal Incremental Parsing via Best-First Dynamic Programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="758" to="768" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fast and Accurate ShiftReduce Constituent Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="434" to="443" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
