<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:35+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Neural Machine Translation with Weight Sharing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Neural Machine Translation with Weight Sharing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="46" to="55"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>46</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Unsupervised neural machine translation (NMT) is a recently proposed approach for machine translation which aims to train the model without using any labeled data. The models proposed for unsuper-vised NMT often use only one shared en-coder to map the pairs of sentences from different languages to a shared-latent space , which is weak in keeping the unique and internal characteristics of each language, such as the style, terminology, and sentence structure. To address this issue, we introduce an extension by utilizing two independent encoders but sharing some partial weights which are responsible for extracting high-level representations of the input sentences. Besides , two different generative adversarial networks (GANs), namely the local GAN and global GAN, are proposed to enhance the cross-language translation. With this new approach, we achieve significant improvements on English-German, English-French and Chinese-to-English translation tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural machine translation <ref type="bibr" target="#b12">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b21">Sutskever et al., 2014;</ref>), directly applying a single neural network to transform the source sen- tence into the target sentence, has now reached im- pressive performance <ref type="bibr" target="#b19">(Shen et al., 2015;</ref><ref type="bibr" target="#b11">Johnson et al., 2016;</ref><ref type="bibr" target="#b9">Gehring et al., 2017;</ref><ref type="bibr" target="#b23">Vaswani et al., 2017)</ref>. The NMT typically consist- s of two sub neural networks. The encoder net- work reads and encodes the source sentence into a <ref type="bibr">1</ref> Feng Wang is the corresponding author of this paper context vector, and the decoder network generates the target sentence iteratively based on the contex- t vector. NMT can be studied in supervised and unsupervised learning settings. In the supervised setting, bilingual corpora is available for training the NMT model. In the unsupervised setting, we only have two independent monolingual corpora with one for each language and there is no bilin- gual training example to provide alignment infor- mation for the two languages. Due to lack of align- ment information, the unsupervised NMT is con- sidered more challenging. However, this task is very promising, since the monolingual corpora is usually easy to be collected.</p><p>Motivated by recent success in unsupervised cross-lingual embeddings ( <ref type="bibr" target="#b1">Artetxe et al., 2016;</ref><ref type="bibr" target="#b31">Zhang et al., 2017b;</ref><ref type="bibr" target="#b8">Conneau et al., 2017</ref>), the models proposed for unsupervised NMT often as- sume that a pair of sentences from two different languages can be mapped to a same latent repre- sentation in a shared-latent space ( <ref type="bibr" target="#b3">Artetxe et al., 2017b)</ref>. Following this as- sumption,  use a single en- coder and a single decoder for both the source and target languages. The encoder and decoder, act- ing as a standard auto-encoder (AE), are trained to reconstruct the inputs. And Artetxe et al. (2017b) utilize a shared encoder but two independent de- coders. With some good performance, they share a glaring defect, i.e., only one encoder is shared by the source and target languages. Although the shared encoder is vital for mapping sentences from different languages into the shared-latent s- pace, it is weak in keeping the uniqueness and internal characteristics of each language, such as the style, terminology and sentence structure. S- ince each language has its own characteristics, the source and target languages should be encoded and learned independently. Therefore, we conjec- ture that the shared encoder may be a factor limit-ing the potential translation performance.</p><p>In order to address this issue, we extend the encoder-shared model, i.e., the model with one shared encoder, by leveraging two independent en- coders with each for one language. Similarly, t- wo independent decoders are utilized. For each language, the encoder and its corresponding de- coder perform an AE, where the encoder gener- ates the latent representations from the perturbed input sentences and the decoder reconstructs the sentences from the latent representations. To map the latent representations from different languages to a shared-latent space, we propose the weight- sharing constraint to the two AEs. Specifically, we share the weights of the last few layers of two encoders that are responsible for extracting high- level representations of input sentences. Similar- ly, we share the weights of the first few layer- s of two decoders. To enforce the shared-latent space, the word embeddings are used as a rein- forced encoding component in our encoders. For cross-language translation, we utilize the back- translation following . Ad- ditionally, two different generative adversarial net- works (GAN) ( , namely the local and global GAN, are proposed to further improve the cross-language translation. We utilize the local GAN to constrain the source and target latent rep- resentations to have the same distribution, where- by the encoder tries to fool a local discriminator which is simultaneously trained to distinguish the language of a given latent representation. We ap- ply the global GAN to finetune the corresponding generator, i.e., the composition of the encoder and decoder of the other language, where a global dis- criminator is leveraged to guide the training of the generator by assessing how far the generated sen- tence is from the true data distribution <ref type="bibr">1</ref> . In sum- mary, we mainly make the following contribution- s:</p><p>• We propose the weight-sharing constraint to unsupervised NMT, enabling the model to u- tilize an independent encoder for each lan- guage. To enforce the shared-latent space, we also propose the embedding-reinforced en- coders and two different GANs for our mod- el.</p><p>• We conduct extensive experiments on <ref type="bibr">1</ref> The code that we utilized to train and evaluate our models can be found at https://github.com/ZhenYangIACAS/unsupervised-NMT English-German, English-French and Chinese-to-English translation tasks. Ex- perimental results show that the proposed approach consistently achieves great success.</p><p>• Last but not least, we introduce the direction- al self-attention to model temporal order in- formation for the proposed model. Exper- imental results reveal that it deserves more efforts for researchers to investigate the tem- poral order information within self-attention layers of NMT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Several approaches have been proposed to train N- MT models without direct parallel corpora. The scenario that has been widely investigated is one where two languages have little parallel data be- tween them but are well connected by one pivot language. The most typical approach in this sce- nario is to independently translate from the source language to the pivot language and from the piv- ot language to the target language ( <ref type="bibr" target="#b16">Saha et al., 2016;</ref><ref type="bibr" target="#b6">Cheng et al., 2017)</ref>. To improve the transla- tion performance, Johnson et al. <ref type="formula" target="#formula_1">(2016)</ref> propose a multilingual extension of a standard NMT model and they achieve substantial improvement for lan- guage pairs without direct parallel training data. Recently, motivated by the success of cross- lingual embeddings, researchers begin to show in- terests in exploring the more ambitious scenario where an NMT model is trained from monolingual corpora only.  and <ref type="bibr" target="#b3">Artetxe et al. (2017b)</ref> simultaneously propose an approach for this scenario, which is based on pre-trained cross lingual embeddings.  utilizes a single encoder and a single decoder for both languages. The entire system is trained to reconstruct its perturbed input. For cross-lingual translation, they incorporate back-translation into the training procedure. Different from (Lample et al., 2017), Artetxe et al. (2017b) use two in- dependent decoders with each for one language. The two works mentioned above both use a sin- gle shared encoder to guarantee the shared latent space. However, a concomitant defect is that the shared encoder is weak in keeping the uniqueness of each language. Our work also belongs to this more ambitious scenario, and to the best of our knowledge, we are one among the first endeav- ors to investigate how to train an NMT model with monolingual corpora only.  is the translation in reversed direction. D l is utilized to assess whether the hidden representation of the encoder is from the source or target language. D g1 and D g2 are used to evaluate whether the translated sentences are realistic for each language respectively. Z represents the shared-latent space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Architecture</head><p>The model architecture, as illustrated in <ref type="figure" target="#fig_0">figure 1</ref>, is based on the AE and GAN. It consists of sev- en sub networks: including two encoders Enc s and Enc t , two decoders Dec s and Dec t , the lo- cal discriminator D l , and the global discriminators D g1 and D g2 . For the encoder and decoder, we follow the newly emerged Transformer ( <ref type="bibr" target="#b23">Vaswani et al., 2017)</ref>. Specifically, the encoder is com- posed of a stack of four identical layers 2 . Each layer consists of a multi-head self-attention and a simple position-wise fully connected feed-forward network. The decoder is also composed of four i- dentical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub- layer, which performs multi-head attention over the output of the encoder stack. For more details about the multi-head self-attention layer, we refer the reader to ( <ref type="bibr" target="#b23">Vaswani et al., 2017)</ref>. We implement the local discriminator as a multi-layer perceptron and implement the global discriminator based on the convolutional neural network (CNN). Several ways exist to interpret the roles of the sub network- s are summarised in table 1. The proposed system has several striking components , which are criti- cal either for the system to be trained in an unsu- <ref type="bibr">2</ref> The layer number is selected according to our prelimi- nary experiment, which is presented in appendix ??.</p><p>pervised manner or for improving the translation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Networks</head><p>Roles <ref type="table">Table 1</ref>: Interpretation of the roles for the subnet- works in the proposed system.</p><formula xml:id="formula_0">{Enc s , Dec s } AE for source language {Enc t , Dec t } AE for target language {Enc s , Dec t } translation source → target {Enc t , Dec s } translation target → source {Enc s , D l } 1st local GAN (GAN l1 ) {Enc t , D l } 2nd local GAN (GAN l2 ) {Enc t , Dec s , D g1 } 1st global GAN (GAN g1 ) {Enc s , Dec t , D g2 } 2nd global GAN (GAN g2 )</formula><p>Directional self-attention Compared to recur- rent neural network, a disadvantage of the simple self-attention mechanism is that the temporal or- der information is lost. Although the Transformer applies the positional encoding to the sequence be- fore processed by the self-attention, how to mod- el temporal order information within an attention is still an open question. Following <ref type="bibr" target="#b20">(Shen et al., 2017)</ref>, we build the encoders in our model on the directional self-attention which utilizes the posi- tional masks to encode temporal order information into attention output. More concretely, two posi- tional masks, namely the forward mask M f and backward mask M b , are calculated as:</p><formula xml:id="formula_1">M f ij = 0 i &lt; j −∞ otherwise (1) M b ij = 0 i &gt; j −∞ otherwise<label>(2)</label></formula><p>With the forward mask M f , the later token on- ly makes attention connections to the early token- s in the sequence, and vice versa with the back- ward mask. Similar to ( <ref type="bibr" target="#b32">Zhou et al., 2016;</ref>, we utilize a self-attention network to process the input sequence in forward direc- tion. The output of this layer is taken by an upper self-attention network as input, processed in the reverse direction.</p><p>Weight sharing Based on the shared-latent s- pace assumption, we apply the weight sharing constraint to relate the two AEs. Specifically, we share the weights of the last few layers of the Enc s and Enc t , which are responsible for extracting high-level representations of the input sentences. Similarly, we also share the first few layers of the Dec s and Dec t , which are expected to decode high-level representations that are vital for recon- structing the input sentences. Compared to ( <ref type="bibr" target="#b5">Cheng et al., 2016;</ref><ref type="bibr" target="#b16">Saha et al., 2016</ref>) which use the fully shared encoder, we only share partial weights for the encoders and decoders. In the proposed mod- el, the independent weights of the two encoders are expected to learn and encode the hidden fea- tures about the internal characteristics of each lan- guage, such as the terminology, style, and sentence structure. The shared weights are utilized to map the hidden features extracted by the independent weights to the shared-latent space.</p><p>Embedding reinforced encoder We use pre- trained cross-lingual embeddings in the encoder- s that are kept fixed during training. And the fixed embeddings are used as a reinforced encod- ing component in our encoder. Formally, giv- en the input sequence embedding vectors E = {e 1 , . . . , e t } and the initial output sequence of the encoder stack H = {h 1 , . . . , h t }, we compute H r as:</p><formula xml:id="formula_2">H r = g H + (1 − g) E<label>(3)</label></formula><p>where H r is the final output sequence of the en- coder which will be attended by the decoder (In Transformer, H is the final output of the encoder), g is a gate unit and computed as:</p><formula xml:id="formula_3">g = σ(W 1 E + W 2 H + b)<label>(4)</label></formula><p>where W 1 , W 2 and b are trainable parameters and they are shared by the two encoders. The motivation behind is twofold. Firstly, taking the fixed cross-lingual embedding as the other encod- ing component is helpful to reinforce the shared- latent space. Additionally, from the point of multi- channel encoders ( <ref type="bibr" target="#b27">Xiong et al., 2017)</ref>, provid- ing encoding components with different levels of composition enables the decoder to take pieces of source sentence at varying composition levels suit- ing its own linguistic structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Unsupervised Training</head><p>Based on the architecture proposed above, we train the NMT model with the monolingual corpora on- ly using the following four strategies: Denoising auto-encoding Firstly, we train the two AEs to reconstruct their inputs respective- ly. In this form, each encoder should learn to compose the embeddings of its corresponding lan- guage and each decoder is expected to learn to de- compose this representation into its corresponding language. Nevertheless, without any constraint, the AE quickly learns to merely copy every word one by one, without capturing any internal struc- ture of the language involved. To address this problem, we utilize the same strategy of denois- ing AE ( <ref type="bibr" target="#b24">Vincent et al., 2008</ref>) and add some noise to the input sentences ( <ref type="bibr" target="#b10">Hill et al., 2016;</ref><ref type="bibr" target="#b3">Artetxe et al., 2017b)</ref>. To this end, we shuffle the input sentences randomly. Specifically, we apply a ran- dom permutation ε to the input sentence, verifying the condition:</p><formula xml:id="formula_4">|ε(i) − i| ≤ min(k([ steps s ] + 1), n), ∀i ∈ {1, n}<label>(5)</label></formula><p>where n is the length of the input sentence, steps is the global steps the model has been updated, k and s are the tunable parameters which can be set by users beforehand. This way, the system needs to learn some useful structure of the involved lan- guages to be able to recover the correct word order. In practice, we set k = 2 and s = 100000.</p><p>Back-translation In spite of denoising auto- encoding, the training procedure still involves a s- ingle language at each time, without considering our final goal of mapping an input sentence from the source/target language to the target/source lan- guage. For the cross language training, we uti- lize the back-translation approach for our unsu- pervised training procedure. Back-translation has shown its great effectiveness on improving NMT model with monolingual data and has been wide- ly investigated by <ref type="bibr" target="#b17">(Sennrich et al., 2015a;</ref><ref type="bibr" target="#b30">Zhang and Zong, 2016)</ref>. In our approach, given an input sentence in a given language, we apply the cor- responding encoder and the decoder of the other language to translate it to the other language <ref type="bibr">3</ref> . By combining the translation with its original sen- tence, we get a pseudo-parallel corpus which is u- tilized to train the model to reconstruct the original sentence from its translation.</p><p>Local GAN Although the weight sharing con- straint is vital for the shared-latent space assump- tion, it alone does not guarantee that the corre- sponding sentences in two languages will have the same or similar latent code. To further enforce the shared-latent space, we train a discriminative neural network, referred to as the local discrimi- nator, to classify between the encoding of source sentences and the encoding of target sentences. The local discriminator, implemented as a multi- layer perceptron with two hidden layers of size 256, takes the output of the encoder, i.e., H r calcu- lated as equation 3, as input, and produces a bina- ry prediction about the language of the input sen- tence. The local discriminator is trained to predict the language by minimizing the following cross- entropy loss:</p><formula xml:id="formula_5">L D l (θ D l ) = − E x∈xs [log p(f = s|Enc s (x))] − E x∈xt [log p(f = t|Enc t (x))]<label>(6)</label></formula><p>where θ D l represents the parameters of the local discriminator and f ∈ {s, t}. The encoders are trained to fool the local discriminator:</p><formula xml:id="formula_6">L Encs (θ Encs ) = − E x∈xs [log p(f = t|Enc s (x))]<label>(7)</label></formula><formula xml:id="formula_7">L Enct (θ Enct ) = − E x∈xt [log p(f = s|Enc t (x))]<label>(8)</label></formula><p>where θ Encs and θ Enct are the parameters of the two encoders. Global GAN We apply the global GANs to fine tune the whole model so that the model is able to generate sentences undistinguishable from the true data, i.e., sentences in the training corpus. Differ- ent from the local GANs which updates the param- eters of the encoders locally, the global GANs are utilized to update the whole parameters of the pro- posed model, including the parameters of encoder- s and decoders. The proposed model has two glob- al GANs: GAN g1 and GAN g2 . In GAN g1 , the Enc t and Dec s act as the generator, which gener- ates the sentence˜xsentence˜ sentence˜x t 4 from x t . The D g1 , imple- mented based on CNN, assesses whether the gen- erated sentence˜xsentence˜ sentence˜x t is the true target-language sen- tence or the generated sentence. The global dis- criminator aims to distinguish among the true sen- tences and generated sentences, and it is trained to minimize its classification error rate. During training, the D g1 feeds back its assessment to fine- tune the encoder Enc t and decoder Dec s . S- ince the machine translation is a sequence gener- ation problem, following ), we leverage policy gradient reinforcement training to back-propagate the assessment. We apply a simi- lar processing to GAN g2 (The details about the ar- chitecture of the global discriminator and the train- ing procedure of the global GANs can be seen in appendix ?? and ??).</p><p>There are two stages in the proposed unsuper- vised training. In the first stage, we train the pro- posed model with denoising auto-encoding, back- translation and the local GANs, until no improve- ment is achieved on the development set. Specif- ically, we perform one batch of denoising auto- encoding for the source and target languages, one batch of back-translation for the two languages, and another batch of local GAN for the two lan- guages. In the second stage, we fine tune the pro- posed model with the global GANs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head><p>We evaluate the proposed approach on English- German, English-French and Chinese-to-English translation tasks <ref type="bibr">5</ref> . We firstly describe the dataset- s, pre-processing and model hyper-parameters we used, then we introduce the baseline systems, and finally we present our experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data Sets and Preprocessing</head><p>In English-German and English-French transla- tion, we make our experiments comparable with previous work by using the datasets from the <ref type="bibr">4</ref> The˜xtThe˜ The˜xt is˜xis˜ is˜x Enc t −Decs t in <ref type="figure" target="#fig_0">figure 1</ref>. We omit the super- script for simplicity. <ref type="bibr">5</ref> The reason that we do not conduct experiments on English-to-Chinese translation is that we do not get public test sets for English-to-Chinese.</p><p>WMT 2014 and WMT 2016 shared tasks respec- tively. For Chinese-to-English translation, we use the datasets from LDC, which has been widely u- tilized by previous works ( <ref type="bibr" target="#b22">Tu et al., 2017;</ref><ref type="bibr" target="#b29">Zhang et al., 2017a</ref>).</p><p>WMT14 English-French Similar to , we use the full training set of 36M sentence pairs and we lower-case them and re- move sentences longer than 50 words, resulting in a parallel corpus of about 30M pairs of sen- tences. To guarantee no exact correspondence be- tween the source and target monolingual sets, we build monolingual corpora by selecting English sentences from 15M random pairs, and selecting the French sentences from the complementary set. Sentences are encoded with byte-pair encoding <ref type="bibr" target="#b18">(Sennrich et al., 2015b)</ref>, which has an English vo- cabulary of about 32000 tokens, and French vo- cabulary of about 33000 tokens. We report results on newstest2014.</p><p>WMT16 English-German We follow the same procedure mentioned above to create monolingual training corpora for English-German translation, and we get two monolingual training data of 1.8M sentences each. The two languages share a vocab- ulary of about 32000 tokens. We report results on newstest2016.</p><p>LDC Chinese-English For Chinese-to-English translation, our training data consists of 1.6M sen- tence pairs randomly extracted from LDC corpora <ref type="bibr">6</ref> . Since the data set is not big enough, we just build the monolingual data set by randomly shuf- fling the Chinese and English sentences respec- tively. In spite of the fact that some correspon- dence between examples in these two monolingual sets may exist, we never utilize this alignment in- formation in our training procedure (see Section 3.2). Both the Chinese and English sentences are encoded with byte-pair encoding. We get an En- glish vocabulary of about 34000 tokens, and Chi- nese vocabulary of about 38000 tokens. The re- sults are reported on NIST 02.</p><p>Since the proposed system relies on the pre- trained cross-lingual embeddings, we utilize the monolingual corpora described above to train the embeddings for each language independently by using word2vec <ref type="bibr" target="#b14">(Mikolov et al., 2013</ref>). We then apply the public implementation 7 of the method proposed by <ref type="bibr" target="#b2">(Artetxe et al., 2017a</ref>) to map these embeddings to a shared-latent space 8 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model Hyper-parameters and Evaluation</head><p>Following the base model in ( <ref type="bibr" target="#b23">Vaswani et al., 2017)</ref>, we set the dimension of word embedding as 512, dropout rate as 0.1 and the head number as 8. We use beam search with a beam size of 4 and length penalty α = 0.6. The model is im- plemented in TensorFlow ( <ref type="bibr">Abadi et al., 2015)</ref> and trained on up to four K80 GPUs synchronously in a multi-GPU setup on a single machine.</p><p>For model selection, we stop training when the model achieves no improvement for the tenth e- valuation on the development set, which is com- prised of 3000 source and target sentences extract- ed randomly from the monolingual training cor- pora. Following ( , we trans- late the source sentences to the target language, and then translate the resulting sentences back to the source language. The quality of the model is then evaluated by computing the BLEU score over the original inputs and their reconstruction- s via this two-step translation process. The per- formance is finally averaged over two direction- s, i.e., from source to target and from target to source. BLEU ( <ref type="bibr" target="#b15">Papineni et al., 2002</ref>) is utilized as the evaluation metric. For Chinese-to-English, we apply the script mteval-v11b.pl to evaluate the translation performance. For English-German and English-French, we evaluate the translation per- formance with the script multi-belu.pl 9 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baseline Systems</head><p>Word-by-word translation (WBW) The first baseline we consider is a system that perform- s word-by-word translations using the inferred bilingual dictionary. Specifically, it translates a sentence word-by-word, replacing each word with its nearest neighbor in the other language.</p><p>Lample et al. <ref type="formula" target="#formula_1">(2017)</ref> The second baseline is a previous work that uses the same training and test- ing sets with this paper. Their model belongs to the standard attention-based encoder-decoder frame- work, which implements the encoder using a bidi- rectional long short term memory network (LST- M) and implements the decoder using a simple for- ward LSTM. They apply one single encoder and en-de de-en en-fr fr-en zh-en   decoder for the source and target languages. Supervised training We finally consider exact- ly the same model as ours, but trained using the standard cross-entropy loss on the original parallel sentences. This model can be viewed as an upper bound for the proposed unsupervised model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Number of weight-sharing layers</head><p>We firstly investigate how the number of weight- sharing layers affects the translation performance. In this experiment, we vary the number of weight- sharing layers in the AEs from 0 to 4. Shar- ing one layer in AEs means sharing one lay- er for the encoders and in the meanwhile, shar- ing one layer for the decoders. The BLEU s- cores of English-to-German, English-to-French and Chinese-to-English translation tasks are re- ported in <ref type="figure" target="#fig_2">figure 2</ref>. Each curve corresponds to a different translation task and the x-axis denotes the number of weight-sharing layers for the AEs. We find that the number of weight-sharing layers shows much effect on the translation performance. And the best translation performance is achieved when only one layer is shared in our system. When all of the four layers are shared, i.e., only one shared encoder is utilized, we get poor translation performance in all of the three translation tasks. This verifies our conjecture that the shared en- coder is detrimental to the performance of unsu- pervised NMT especially for the translation tasks on distant language pairs. More concretely, for the related language pair translation, i.e., English-to- French, the encoder-shared model achieves -0.53 BLEU points decline than the best model where only one layer is shared. For the more distant lan- guage pair English-to-German, the encoder-shared model achieves more significant decline, i.e., -0.85 BLEU points decline. And for the most distan- t language pair Chinese-to-English, the decline is as large as -1.66 BLEU points. We explain this as that the more distant the language pair is, the more different characteristics they have. And the shared encoder is weak in keeping the unique characteris- tic of each language. Additionally, we also notice that using two completely independent encoders, i.e., setting the number of weight-sharing layers as 0, results in poor translation performance too. This confirms our intuition that the shared layers are vital to map the source and target latent rep- resentations to a shared-latent space. In the rest of our experiments, we set the number of weight- sharing layer as 1.  <ref type="table" target="#tab_2">Table 2</ref> shows the BLEU scores on English- German, English-French and English-to-Chinese test sets. As it can be seen, the proposed ap- proach obtains significant improvements than the word-by-word baseline system, with at least +5.01 BLEU points in English-to-German translation and up to +13.37 BLEU points in English-to- French translation. This shows that the proposed model only trained with monolingual data effec-en-de de-en en-fr fr-en zh-en  <ref type="table">Table 3</ref>: Ablation study on English-German, English-French and Chinese-to-English translation tasks. Without weight sharing means no layers are shared in the two AEs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Translation results</head><p>tively learns to use the context information and the internal structure of each language. Compared to the work of ( , our mod- el also achieves up to +1.92 BLEU points im- provement on English-to-French translation task. We believe that the unsupervised NMT is very promising. However, there is still a large room for improvement compared to the supervised up- per bound. The gap between the supervised and unsupervised model is as large as 12.3-25.5 BLEU points depending on the language pair and transla- tion direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Ablation study</head><p>To understand the importance of different com- ponents of the proposed system, we perform an ablation study by training multiple versions of our model with some missing components: the local GANs, the global GANs, the directional self-attention, the weight-sharing, the embedding- reinforced encoders, etc. Results are reported in table 3. We do not test the the importance of the auto-encoding, back-translation and the pre- trained embeddings because they have been wide- ly tested in ( <ref type="bibr" target="#b3">Artetxe et al., 2017b</ref>). <ref type="table">Table 3</ref> shows that the best performance is obtained with the simultaneous use of all the test- ed elements. The most critical component is the weight-sharing constraint, which is vital to map sentences of different languages to the shared- latent space. The embedding-reinforced encoder also brings some improvement on all of the trans- lation tasks. When we remove the directional self- attention, we get up to -0.3 BLEU points decline. This indicates that it deserves more efforts to in- vestigate the temporal order information in self- attention mechanism. The GANs also significant- ly improve the translation performance of our sys- tem. Specifically, the global GANs achieve im- provement up to +0.78 BLEU points on English- to-French translation and the local GANs also ob- tain improvement up to +0.57 BLEU points on English-to-French translation. This reveals that the proposed model benefits a lot from the cross- domain loss defined by GANs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future work</head><p>The models proposed recently for unsupervised N- MT use a single encoder to map sentences from different languages to a shared-latent space. We conjecture that the shared encoder is problem- atic for keeping the unique and inherent char- acteristic of each language. In this paper, we propose the weight-sharing constraint in unsuper- vised NMT to address this issue. To enhance the cross-language translation performance, we also propose the embedding-reinforced encoders, local GAN and global GAN into the proposed system. Additionally, the directional self-attention is intro- duced to model the temporal order information for our system. We test the proposed model on English- German, English-French and Chinese-to-English translation tasks. The experimental results reveal that our approach achieves significant improve- ment and verify our conjecture that the shared en- coder is really a bottleneck for improving the un- supervised NMT. The ablation study shows that each component of our system achieves some im- provement for the final translation performance.</p><p>Unsupervised NMT opens exciting opportuni- ties for the future research. However, there is still a large room for improvement compared to the supervised NMT. In the future, we would like to investigate how to utilize the monolingual da- ta more effectively, such as incorporating the lan- guage model and syntactic information into unsu- pervised NMT. Besides, we decide to make more efforts to explore how to reinforce the temporal or-</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The architecture of the proposed model. We implement the shared-latent space assumption using a weight sharing constraint where the connection of the last few layers in Enc s and Enc t are tied (illustrated with dashed lines) and the connection of the first few layers in Dec s and Dec t are tied. ˜ x Encs−Decs s</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Supervised</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The effects of the weight-sharing layer number on English-to-German, English-to-French and Chinese-to-English translation tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>The translation performance on English-German, English-French and Chinese-to-English test 
sets. The results of (Lample et al., 2017) are copied directly from their paper. We do not present the 
results of (Artetxe et al., 2017b) since we use different training sets. 

</table></figure>

			<note place="foot" n="3"> Since the quality of the translation shows little effect on the performance of the model (Sennrich et al., 2015a), we simply use greedy decoding for speed.</note>

			<note place="foot" n="6"> LDC2002L27, LDC2002T01, LDC2002E18, LDC2003E07, LDC2004T08, LDC2004E12, LDC2005T10 7 https://github.com/artetxem/vecmap</note>

			<note place="foot" n="8"> The configuration we used to run these open-source toolkits can be found in appendix ?? 9 https://github.com/mosessmt/mosesdecoder/blob/617e8c8/scripts/generic/multibleu.perl;mteval-v11b.pl</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is supported by the National Key Re-search and Development Program of China un-der Grant No. 2017YFB1002102, and Beijing Engineering Research Center under Grant No. Z171100002217015. We would like to thank X-u Shuang for her preparing data used in this work. Additionally, we also want to thank Jiaming Xu, Suncong Zheng and Wenfu Wang for their invalu-able discussions on this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">aoqiang Zheng. 2015. TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<editor>Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas</editor>
		<meeting><address><addrLine>Dan Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner; Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xi</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning principled bilingual mappings of word embeddings while preserving monolingual invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2289" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning bilingual word embeddings with (almost) no bilingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="451" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Unsupervised neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>arX- iv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04928</idno>
		<title level="m">Neural machine translation with pivot languages</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Joint training for pivotbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Sixth International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3974" to="3980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jgou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Word translation without parallel data</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann N</forename><surname>Dauphin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning distributed representations of sentences from unlabelled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>TACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Google&apos;s multilingual neural machine translation system: Enabling zero-shot translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernanda</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corrado</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04558</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1700" to="1709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Unsupervised machine translation using monolingual corpora only</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A correlational encoder decoder architecture for pivot based sequence generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amrita</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarath</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janarthanan</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Rajendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cho</surname></persName>
		</author>
		<idno>arX- iv:1606.04754</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno>arX- iv:1511.06709</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.02433</idno>
		<title level="m">Minimum risk training for neural machine translation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Disan: Directional self-attention network for rnn/cnn-free language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Sequence to sequence learning with neural networks. Advances in neural information processing systems pages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning to remember translation history with a continuous cache</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Attention is all you need</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Deep neural machine translation with linear associative unit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno>arX- iv:1705.00861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno>arX- iv:1609.08144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Multi-channel encoder for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.02109</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Improving neural machine translation with conditional sequence generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Prior knowledge integration for neural machine translation using posterior regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1514" to="1523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Exploiting source-side monolingual data in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1535" to="1545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adversarial training for unsupervised bilingual lexicon induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1959" to="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Deep recurrent models with fast-forward connections for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04199</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
