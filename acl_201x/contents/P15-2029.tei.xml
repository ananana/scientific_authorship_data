<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dependency-based Convolutional Neural Networks for Sentence Embedding *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 26-31, 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingbo</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate Center &amp; Queens College City</orgName>
								<orgName type="laboratory">IBM Watson Group T. J. Watson Research Center</orgName>
								<orgName type="institution" key="instit1">University of New York</orgName>
								<orgName type="institution" key="instit2">IBM</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate Center &amp; Queens College City</orgName>
								<orgName type="laboratory">IBM Watson Group T. J. Watson Research Center</orgName>
								<orgName type="institution" key="instit1">University of New York</orgName>
								<orgName type="institution" key="instit2">IBM</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate Center &amp; Queens College City</orgName>
								<orgName type="laboratory">IBM Watson Group T. J. Watson Research Center</orgName>
								<orgName type="institution" key="instit1">University of New York</orgName>
								<orgName type="institution" key="instit2">IBM</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate Center &amp; Queens College City</orgName>
								<orgName type="laboratory">IBM Watson Group T. J. Watson Research Center</orgName>
								<orgName type="institution" key="instit1">University of New York</orgName>
								<orgName type="institution" key="instit2">IBM</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Dependency-based Convolutional Neural Networks for Sentence Embedding *</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="174" to="179"/>
							<date type="published">July 26-31, 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In sentence modeling and classification, convolutional neural network approaches have recently achieved state-of-the-art results , but all such efforts process word vectors sequentially and neglect long-distance dependencies. To combine deep learning with linguistic structures, we propose a dependency-based convolution approach , making use of tree-based n-grams rather than surface ones, thus utlizing non-local interactions between words. Our model improves sequential baselines on all four sentiment and question classification tasks, and achieves the highest published accuracy on TREC.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Convolutional neural networks (CNNs), originally invented in computer vision ( <ref type="bibr" target="#b12">LeCun et al., 1995)</ref>, has recently attracted much attention in natural language processing (NLP) on problems such as sequence labeling <ref type="bibr" target="#b0">(Collobert et al., 2011</ref>), seman- tic parsing <ref type="bibr" target="#b25">(Yih et al., 2014</ref>), and search query retrieval <ref type="bibr" target="#b21">(Shen et al., 2014</ref>). In particular, recent work on CNN-based sentence modeling <ref type="bibr" target="#b7">(Kalchbrenner et al., 2014;</ref><ref type="bibr" target="#b8">Kim, 2014</ref>) has achieved ex- cellent, often state-of-the-art, results on various classification tasks such as sentiment, subjectivity, and question-type classification. However, despite their celebrated success, there remains a major limitation from the linguistics perspective: CNNs, being invented on pixel matrices in image process- ing, only consider sequential n-grams that are con- secutive on the surface string and neglect long- distance dependencies, while the latter play an im- portant role in many linguistic phenomena such as negation, subordination, and wh-extraction, all of which might dully affect the sentiment, subjectiv- ity, or other categorization of the sentence. * This work was done at both IBM and CUNY, and was supported in part by DARPA FA8750-13-2-0041 (DEFT), and NSF IIS-1449278. We thank Yoon Kim for sharing his code, and James Cross and Kai Zhao for discussions.</p><p>Indeed, in the sentiment analysis literature, re- searchers have incorporated long-distance infor- mation from syntactic parse trees, but the results are somewhat inconsistent: some reported small improvements <ref type="bibr" target="#b2">(Gamon, 2004;</ref><ref type="bibr" target="#b15">Matsumoto et al., 2005</ref>), while some otherwise ( <ref type="bibr" target="#b1">Dave et al., 2003;</ref><ref type="bibr" target="#b10">Kudo and Matsumoto, 2004</ref>). As a result, syn- tactic features have yet to become popular in the sentiment analysis community. We suspect one of the reasons for this is data sparsity (according to our experiments, tree n-grams are significantly sparser than surface n-grams), but this problem has largely been alleviated by the recent advances in word embedding. Can we combine the advan- tages of both worlds?</p><p>So we propose a very simple dependency-based convolutional neural networks (DCNNs). Our model is similar to <ref type="bibr" target="#b8">Kim (2014)</ref>, but while his se- quential CNNs put a word in its sequential con- text, ours considers a word and its parent, grand- parent, great-grand-parent, and siblings on the de- pendency tree. This way we incorporate long- distance information that are otherwise unavail- able on the surface string.</p><p>Experiments on three classification tasks demonstrate the superior performance of our DCNNs over the baseline sequential CNNs. In particular, our accuracy on the TREC dataset outperforms all previously published results in the literature, including those with heavy hand-engineered features.</p><p>Independently of this work, <ref type="bibr">Mou et al. (2015, unpublished)</ref> reported related efforts; see Sec. 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dependency-based Convolution</head><p>The original CNN, first proposed by <ref type="bibr" target="#b12">LeCun et al. (1995)</ref>, applies convolution kernels on a se- ries of continuous areas of given images, and was adapted to NLP by <ref type="bibr" target="#b0">Collobert et al. (2011)</ref>. <ref type="bibr">Following Kim (2014)</ref>, one dimensional convolution operates the convolution kernel in sequential order in Equation 1, where x i ∈ R d represents the d di- mensional word representation for the i-th word in <ref type="table" target="#tab_3">Despite   the  film  's  shortcomings  the  stories  are  quietly  moving  .</ref> ROOT <ref type="figure" target="#fig_0">Figure 1</ref>: Running example from Movie Reviews dataset. mensional word representation for the i-th word in the sentence, and ⊕ is the concatenation operator. Therefore x i,j refers to concatenated word vector from the i-th word to the (i + j)-th word:</p><formula xml:id="formula_0">x i,j = x i ⊕ x i+1 ⊕ · · · ⊕ x i+j<label>(1)</label></formula><p>Sequential word concatenation x i,j works as n-gram models which feeds local information into convolution operations. However, this setting can not capture long-distance relationships unless we enlarge the window indefinitely which would in- evitably cause the data sparsity problem.</p><p>In order to capture the long-distance dependen- cies we propose the dependency tree-based con- volution model (DTCNN). <ref type="figure" target="#fig_0">Figure 1</ref> illustrates an example from the Movie Reviews (MR) dataset <ref type="bibr" target="#b19">(Pang and Lee, 2005</ref>). The sentiment of this sen- tence is obviously positive, but this is quite dif- ficult for sequential CNNs because many n-gram windows would include the highly negative word "shortcomings", and the distance between "De- spite" and "shortcomings" is quite long. DTCNN, however, could capture the tree-based bigram "Despite -shortcomings", thus flipping the senti- ment, and the tree-based trigram "ROOT -moving -stories", which is highly positive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Convolution on Ancestor Paths</head><p>We define our concatenation based on the depen- dency tree for a given modifier x i :</p><formula xml:id="formula_1">x i,k = x i ⊕ x p(i) ⊕ · · · ⊕ x p k−1 (i)<label>(2)</label></formula><p>where function p k (i) returns the i-th word's k-th ancestor index, which is recursively defined as:</p><formula xml:id="formula_2">p k (i) = p(p k−1 (i)) if k &gt; 0 i if k = 0<label>(3)</label></formula><p>Figure 2 (left) illustrates ancestor paths patterns with various orders. We always start the convo- lution with x i and concatenate with its ancestors. If the root node is reached, we add "ROOT" as dummy ancestors (vertical padding).</p><p>For a given tree-based concatenated word se- quence x i,k , the convolution operation applies a filter w ∈ R k×d to x i,k with a bias term b de- scribed in equation 4:</p><formula xml:id="formula_3">c i = f (w · x i,k + b) (4)</formula><p>where f is a non-linear activation function such as rectified linear unit (ReLu) or sigmoid function. The filter w is applied to each word in the sen- tence, generating the feature map c ∈ R l :</p><formula xml:id="formula_4">c = [c 1 , c 2 , · · · , c l ]<label>(5</label></formula><p>) where l is the length of the sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Max-Over-Tree Pooling and Dropout</head><p>The filters convolve with different word concate- nation in Eq. 4 can be regarded as pattern detec- tion: only the most similar pattern between the words and the filter could return the maximum ac- tivation. In sequential CNNs, max-over-time pool- ing <ref type="bibr" target="#b0">(Collobert et al., 2011;</ref><ref type="bibr" target="#b8">Kim, 2014</ref>) operates over the feature map to get the maximum acti- vationˆcvationˆ vationˆc = max c representing the entire feature map. Our DTCNNs also pool the maximum ac- tivation from feature map to detect the strongest activation over the whole tree (i.e., over the whole sentence). Since the tree no longer defines a se- quential "time" direction, we refer to our pooling as "max-over-tree" pooling.</p><p>In order to capture enough variations, we ran- domly initialize the set of filters to detect different structure patterns. Each filter's height is the num- ber of words considered and the width is always equal to the dimensionality d of word representa- tion. Each filter will be represented by only one feature after max-over-tree pooling. After a series of convolution with different filter with different heights, multiple features carry different structural information become the final representation of the input sentence. Then, this sentence representation is passed to a fully connected soft-max layer and outputs a distribution over different labels.</p><p>Neural networks often suffer from overtrain- ing. Following <ref type="bibr" target="#b8">Kim (2014)</ref>, we employ random dropout on penultimate layer ( <ref type="bibr">Hinton et al., 2012)</ref>. in order to prevent co-adaptation of hidden units.</p><p>In our experiments, we set our drop out rate as 0.5 and learning rate as 0.95 by default. Following <ref type="bibr" target="#b8">Kim (2014)</ref>, training is done through stochastic gradient descent over shuffled mini-batches with the Adadelta update rule (Zeiler, 2012).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Convolution on Siblings</head><p>Ancestor paths alone is not enough to capture many linguistic phenomena such as conjunction. the sentence, and ⊕ is the concatenation operator. Therefore x i,j refers to concatenated word vector from the i-th word to the (i + j)-th word:</p><formula xml:id="formula_5">x i,j = x i ⊕ x i+1 ⊕ · · · ⊕ x i+j<label>(1)</label></formula><p>Sequential word concatenation x i,j works as n-gram models which feeds local information into convolution operations. However, this setting can not capture long-distance relationships unless we enlarge the window indefinitely which would in- evitably cause the data sparsity problem.</p><p>In order to capture the long-distance dependen- cies we propose the dependency-based convolu- tion model (DCNN). <ref type="figure" target="#fig_0">Figure 1</ref> illustrates an exam- ple from the Movie Reviews (MR) dataset (Pang and <ref type="bibr" target="#b19">Lee, 2005</ref>). The sentiment of this sentence is obviously positive, but this is quite difficult for sequential CNNs because many n-gram windows would include the highly negative word "short- comings", and the distance between "Despite" and "shortcomings" is quite long. DCNN, however, could capture the tree-based bigram "Despite - shortcomings", thus flipping the sentiment, and the tree-based trigram "ROOT -moving -sto- ries", which is highly positive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Convolution on Ancestor Paths</head><p>We define our concatenation based on the depen- dency tree for a given modifier x i :</p><formula xml:id="formula_6">x i,k = x i ⊕ x p(i) ⊕ · · · ⊕ x p k−1 (i) (2)</formula><p>where function p k (i) returns the i-th word's k-th ancestor index, which is recursively defined as: <ref type="figure" target="#fig_1">Figure 2</ref> (left) illustrates ancestor paths patterns with various orders. We always start the convo- lution with x i and concatenate with its ancestors. If the root node is reached, we add "ROOT" as dummy ancestors (vertical padding).</p><formula xml:id="formula_7">p k (i) = p(p k−1 (i)) if k &gt; 0 i if k = 0 (3)</formula><p>For a given tree-based concatenated word se- quence x i,k , the convolution operation applies a filter w ∈ R k×d to x i,k with a bias term b de- scribed in equation 4:</p><formula xml:id="formula_8">c i = f (w · x i,k + b) (4)</formula><p>where f is a non-linear activation function such as rectified linear unit (ReLu) or sigmoid function. The filter w is applied to each word in the sen- tence, generating the feature map c ∈ R l :</p><formula xml:id="formula_9">c = [c 1 , c 2 , · · · , c l ] (5)</formula><p>where l is the length of the sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Max-Over-Tree Pooling and Dropout</head><p>The filters convolve with different word concate- nation in Eq. 4 can be regarded as pattern detec- tion: only the most similar pattern between the words and the filter could return the maximum ac- tivation. In sequential CNNs, max-over-time pool- ing <ref type="bibr" target="#b0">(Collobert et al., 2011;</ref><ref type="bibr" target="#b8">Kim, 2014</ref>) operates over the feature map to get the maximum acti- vationˆcvationˆ vationˆc = max c representing the entire feature map. Our DCNNs also pool the maximum activa- tion from feature map to detect the strongest ac- tivation over the whole tree (i.e., over the whole sentence). Since the tree no longer defines a se- quential "time" direction, we refer to our pooling as "max-over-tree" pooling. In order to capture enough variations, we ran- domly initialize the set of filters to detect different structure patterns. Each filter's height is the num- ber of words considered and the width is always equal to the dimensionality d of word representa- tion. Each filter will be represented by only one feature after max-over-tree pooling. After a series of convolution with different filter with different heights, multiple features carry different structural information become the final representation of the input sentence. Then, this sentence representation is passed to a fully connected soft-max layer and outputs a distribution over different labels.</p><p>Neural networks often suffer from overtrain- ing. Following <ref type="bibr" target="#b8">Kim (2014)</ref>, we employ random dropout on penultimate layer ( <ref type="bibr" target="#b3">Hinton et al., 2014)</ref>. in order to prevent co-adaptation of hidden units.</p><p>In our experiments, we set our drop out rate as 0.5 and learning rate as 0.95 by default. Following <ref type="bibr" target="#b8">Kim (2014)</ref>, training is done through stochastic gradient descent over shuffled mini-batches with the Adadelta update rule <ref type="bibr" target="#b26">(Zeiler, 2012</ref>  <ref type="table" target="#tab_2">Table 1</ref>: Tree-based convolution patterns. Word concatenation always starts with m, while h, g, and g 2 denote parent, grand parent, and great-grand parent, etc., and " " denotes words excluded in convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Convolution on Siblings</head><p>Ancestor paths alone is not enough to capture many linguistic phenomena such as conjunction.</p><p>Inspired by higher-order dependency parsing <ref type="bibr" target="#b16">(McDonald and Pereira, 2006;</ref><ref type="bibr" target="#b9">Koo and Collins, 2010)</ref>, we also incorporate siblings for a given word in various ways. See <ref type="table" target="#tab_2">Table 1</ref> (right) for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Combined Model</head><p>Powerful as it is, structural information still does not fully cover sequential information. Also, pars- ing errors (which are common especially for in- formal text such as online reviews) directly affect DTCNN performance while sequential n-grams are always correctly observed. To best exploit both information, we want to combine both mod- els. The easiest way of combination is to con- catenate these representations together, then feed into fully connected soft-max neural networks. In these cases, combine with different feature from different type of sources could stabilize the perfor- mance. The final sentence representation is thus:</p><formula xml:id="formula_10">ˆ c = [ˆ c (1) a , ..., ˆ c (N a ) a ancestors ; ˆ c (1) s , ..., ˆ c (N s ) s siblings ; ˆ c (1) , ..., ˆ c (N ) sequential ]</formula><p>where N a , N s , and N are the number of ancestor, sibling, and sequential filters. In practice, we use 100 filters for each template in <ref type="table" target="#tab_2">Table 1</ref>. The fully combined representation is 1100-dimensional by contrast to 300-dimensional for sequential CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We  <ref type="bibr">, 2002</ref>). For all datasets, we first obtain the dependency parse tree from Stanford parser ( <ref type="bibr" target="#b14">Manning et al., 2014</ref>). 2 Different window size for different choice of convolution are shown in <ref type="table" target="#tab_2">Table 1</ref>. For the dataset without a development set (MR), we ran- domly choose 10% of the training data to indicate early stopping. In order to have a fare compari- son with baseline CNN, we also use 3 to 5 as our window size. Most of our results are generated by GPU due to its efficiency, however CPU poten- tially could generate better results. <ref type="bibr">3</ref> Our imple- mentation can be found on Github. <ref type="bibr">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sentiment Analysis</head><p>Both sentiment analysis datasets (MR and SST- 1) are based on movie reviews. The differences between them are mainly in the different num- bers of categories and whether the standard split is given. There are 10,662 sentences in the MR dataset. Each instance is labeled positive or neg- ative, and in most cases contains one sentence. Since no standard data split is given, following the literature we use 10 fold cross validation to include every sentence in training and testing at least once. Concatenating with sibling and sequential infor- mation obviously improves tree-based CNNs, and the final model outperforms the baseline sequen- tial CNNs by 0.4, and ties with <ref type="bibr" target="#b27">Zhu et al. (2015)</ref>.</p><p>Different from MR, the Stanford Sentiment Treebank (SST-1) annotates finer-grained labels, very positive, positive, neutral, negative and very negative, on an extension of the MR dataset. There are 11,855 sentences with standard split. Our model achieves an accuracy of 49.5 which is sec- ond only to <ref type="bibr" target="#b4">Irsoy and Cardie (2014)</ref>. We set batch size to 100 for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>The phrase-structure trees in SST-1 are actually automat- ically parsed, and thus can not be used as gold-standard trees.</p><p>3 GPU only supports float32 while CPU supports float64. Word concatenation always starts with m, while h, g, and g 2 denote parent, grand parent, and great-grand parent, etc., and " " denotes words excluded in convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Convolution on Siblings</head><p>Ancestor paths alone is not enough to capture many linguistic phenomena such as conjunction. Inspired by higher-order dependency parsing <ref type="bibr" target="#b16">(McDonald and Pereira, 2006;</ref><ref type="bibr" target="#b9">Koo and Collins, 2010)</ref>, we also incorporate siblings for a given word in various ways. See <ref type="figure" target="#fig_1">Figure 2</ref> (right) for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Combined Model</head><p>Powerful as it is, structural information still does not fully cover sequential information. Also, pars- ing errors (which are common especially for in- formal text such as online reviews) directly affect DCNN performance while sequential n-grams are always correctly observed. To best exploit both in- formation, we want to combine both models. The easiest way of combination is to concatenate these representations together, then feed into fully con- nected soft-max neural networks. In these cases, combine with different feature from different type of sources could stabilize the performance. The final sentence representation is thus:</p><formula xml:id="formula_11">ˆ c = [ˆ c (1) a , ..., ˆ c (Na) a ancestors ; ˆ c (1) s , ..., ˆ c (Ns) s siblings ; ˆ c (1) , ..., ˆ c (N ) sequential ]</formula><p>where N a , N s , and N are the number of ancestor, sibling, and sequential filters. In practice, we use 100 filters for each template in <ref type="figure" target="#fig_1">Figure 2</ref> . The fully combined representation is 1,100-dimensional by contrast to 300-dimensional for sequential CNN.  <ref type="bibr" target="#b13">Li and Roth, 2002</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>For all datasets, we first obtain the dependency parse tree from Stanford parser ( <ref type="bibr" target="#b14">Manning et al., 2014</ref>). 1 Different window size for different choice of convolution are shown in <ref type="figure" target="#fig_1">Figure 2</ref>. For the dataset without a development set (MR), we ran- domly choose 10% of the training data to indicate early stopping. In order to have a fare compari- son with baseline CNN, we also use 3 to 5 as our window size. Most of our results are generated by GPU due to its efficiency, however CPU could po- tentially get better results. <ref type="bibr">2</ref> Our implementation, on top of Kim (2014)'s code, 3 will be released. <ref type="bibr">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sentiment Analysis</head><p>Both sentiment analysis datasets (MR and SST- 1) are based on movie reviews. The differences between them are mainly in the different num- bers of categories and whether the standard split is given. There are 10,662 sentences in the MR dataset. Each instance is labeled positive or neg- ative, and in most cases contains one sentence. Since no standard data split is given, following the literature we use 10 fold cross validation to include every sentence in training and testing at least once. Concatenating with sibling and sequential infor- mation obviously improves DCNNs, and the final model outperforms the baseline sequential CNNs by 0.4, and ties with <ref type="bibr" target="#b27">Zhu et al. (2015)</ref>.</p><p>Different from MR, the Stanford Sentiment Treebank (SST-1) annotates finer-grained labels, very positive, positive, neutral, negative and very negative, on an extension of the MR dataset. There are 11,855 sentences with standard split. Our model achieves an accuracy of 49.5 which is sec- ond only to <ref type="bibr" target="#b4">Irsoy and Cardie (2014</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Question Classification</head><p>In the TREC dataset, the entire dataset of 5,952 sentences are classified into the following 6 cate- gories: abbreviation, entity, description, location and numeric. In this experiment, DCNNs easily outperform any other methods even with ancestor convolution only. DCNNs with sibling achieve the best performance in the published literature. DC- NNs combined with sibling and sequential infor- mation might suffer from overfitting on the train- ing data based on our observation. One thing to note here is that our best result even exceeds SVM S (Silva et al., 2011) with 60 hand-coded rules.</p><p>The TREC dataset also provides subcategories such as numeric:temperature, numeric:distance, and entity:vehicle. To make our task more real- istic and challenging, we also test the proposed model with respect to the 50 subcategories. There are obvious improvements over sequential CNNs from the last column of <ref type="table" target="#tab_2">Table 1</ref>. Like ours, Silva et al. (2011) is a tree-based system but it uses constituency trees compared to ours dependency trees. They report a higher fine-grained accuracy of 90.8 but their parser is trained only on the Ques- tionBank ( <ref type="bibr" target="#b6">Judge et al., 2006</ref>) while we used the standard Stanford parser trained on both the Penn Treebank and QuestionBank. Moreover, as men- tioned above, their approach is rule-based while ours is automatically learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Discussions and Examples</head><p>Compared with sentiment analysis, the advantage of our proposed model is obviously more substan- tial on the TREC dataset. Based on our error anal- ysis, we conclude that this is mainly due to the   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Question C</head><p>In the TREC dat sentences are cla gories: abbreviat and numeric. In ily outperform an cestor convolutio achieve the best p erature. DTCNN quential informat on the training da thing to note her ceeds SVM S (Si coded rules. We s The TREC dat such as numeric and entity:vehicl istic and challen model with respe are obvious impr from the last colu et al. <ref type="formula" target="#formula_0">(2011)</ref> is constituency tree trees. They repor of 90.8 but their p tionBank (Judge standard Stanford Treebank and Qu tioned above, the ours is automatic batch size to 30.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Discussions</head><p>Compared with s of our proposed m tial on the TREC   ysis, we conclude that this is mainly due to the difference of the parse tree quality between the two tasks. In sentiment analysis, the dataset is collected from the Rotten Tomatoes website which includes many irregular usage of language. Some of the sentences even come from languages other than English. The errors in parse trees inevitably affect the classification accuracy. However, the parser works substantially better on the TREC dataset since all questions are in formal written English, and the training set for Stanford parser 5 already includes the QuestionBank ( <ref type="bibr" target="#b6">Judge et al., 2006</ref>) which includes 2,000 TREC sentences. <ref type="figure" target="#fig_1">Figure 2</ref> visualizes examples where CNN errs while DTCNN does not. For example, CNN la- bels (a) as location due to "Hawaii" and "state", while the long-distance backbone "What -flower" is clearly asking for an entity. Similarly, in (d), DTCNN captures the obviously negative tree- based trigram "Nothing -worth -emailing". Note that our model also works with non-projective de- pendency trees such as the one in (b). The last two examples in <ref type="figure" target="#fig_1">Figure 2</ref> visualize cases where DTCNN outperforms the baseline CNNs in fine- grained TREC. In example (e), the word "temper- ature" is at second from the top and is root of a 8 word span "the ... earth". When we use a win- dow of size 5 for tree convolution, every words in that span get convolved with "temperature" and this should be the reason why DTCNN get correct.  For example, (a) should be numerical but is la- beled entity by DTCNN and description by CNN. <ref type="figure" target="#fig_3">Figure 3</ref> showcases examples where baseline CNNs get better results than DTCNNs. Exam- ple (a) is misclassified as entity by DTCNN due to parsing/tagging error (the Stanford parser per- forms its own part-of-speech tagging). The word "fly" at the end of the sentence should be a verb instead of noun, and "hummingbirds fly" should be a relative clause modifying "speed".</p><p>There are some sentences that are misclassified by both the baseline CNN and DTCNN. <ref type="figure" target="#fig_8">Figure 4</ref> shows three such examples. Example (a) is not classified as numerical by both methods due to the ambiguous meaning of the word "point" which is difficult to capture by word embedding. This word can mean location, opinion, etc. Apparently, the numerical aspect is not captured by word embed- ding. Example (c) might be an annotation error.</p><p>From the mistakes made by DTCNNs, we find the performance of DTCNN is mainly limited by two factors: the accuracy of the parser and the quality of word embedding. Future work will fo- cus on these two issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions and Future Work</head><p>We have presented a very simple dependency tree- based convolution framework which outperforms sequential CNN baselines on various classification tasks. Extensions of this model would consider dependency labels and constituency trees. Also, we would evaluate on gold-standard parse trees. difference of the parse tree quality between the two tasks. In sentiment analysis, the dataset is collected from the Rotten Tomatoes website which includes many irregular usage of language. Some of the sentences even come from languages other than English. The errors in parse trees inevitably affect the classification accuracy. However, the parser works substantially better on the TREC dataset since all questions are in formal written English, and the training set for Stanford parser 5 already includes the QuestionBank ( <ref type="bibr" target="#b6">Judge et al., 2006</ref>) which includes 2,000 TREC sentences. <ref type="figure" target="#fig_3">Figure 3</ref> visualizes examples where CNN errs while DCNN does not. For example, CNN la- bels (a) as location due to "Hawaii" and "state", while the long-distance backbone "What -flower" is clearly asking for an entity. Similarly, in (d), DCNN captures the obviously negative tree-based trigram "Nothing -worth -emailing". Note that our model also works with non-projective depen- dency trees such as the one in (b). The last two ex- amples in <ref type="figure" target="#fig_3">Figure 3</ref> visualize cases where DCNN outperforms the baseline CNNs in fine-grained TREC. In example (e), the word "temperature" is at second from the top and is root of a 8 word span "the ... earth". When we use a window of size 5 for tree convolution, every words in that span get convolved with "temperature" and this should be the reason why DCNN get correct. <ref type="figure" target="#fig_8">Figure 4</ref> showcases examples where baseline CNNs get better results than DCNNs. Example (a) is misclassified as entity by DCNN due to pars- ing/tagging error (the Stanford parser performs its 5 http://nlp.stanford.edu/software/parser-faq.shtml What is the speed hummingbirds fly ? ysis, we conclude that this is mainly due to the difference of the parse tree quality between the two tasks. In sentiment analysis, the dataset is collected from the Rotten Tomatoes website which includes many irregular usage of language. Some of the sentences even come from languages other than English. The errors in parse trees inevitably affect the classification accuracy. However, the parser works substantially better on the TREC dataset since all questions are in formal written English, and the training set for Stanford parser 5 already includes the QuestionBank ( <ref type="bibr" target="#b6">Judge et al., 2006</ref>) which includes 2,000 TREC sentences. <ref type="figure" target="#fig_1">Figure 2</ref> visualizes examples where CNN errs while DTCNN does not. For example, CNN la- bels (a) as location due to "Hawaii" and "state", while the long-distance backbone "What -flower" is clearly asking for an entity. Similarly, in (d), DTCNN captures the obviously negative tree- based trigram "Nothing -worth -emailing". Note that our model also works with non-projective de- pendency trees such as the one in (b). The last two examples in <ref type="figure" target="#fig_1">Figure 2</ref> visualize cases where DTCNN outperforms the baseline CNNs in fine- grained TREC. In example (e), the word "temper- ature" is at second from the top and is root of a 8 word span "the ... earth". When we use a win- dow of size 5 for tree convolution, every words in that span get convolved with "temperature" and this should be the reason why DTCNN get correct.  For example, (a) should be numerical but is la- beled entity by DTCNN and description by CNN. <ref type="figure" target="#fig_3">Figure 3</ref> showcases examples where baseline CNNs get better results than DTCNNs. Exam- ple (a) is misclassified as entity by DTCNN due to parsing/tagging error (the Stanford parser per- forms its own part-of-speech tagging). The word "fly" at the end of the sentence should be a verb instead of noun, and "hummingbirds fly" should be a relative clause modifying "speed".</p><p>There are some sentences that are misclassified by both the baseline CNN and DTCNN. <ref type="figure" target="#fig_8">Figure 4</ref> shows three such examples. Example (a) is not classified as numerical by both methods due to the ambiguous meaning of the word "point" which is difficult to capture by word embedding. This word can mean location, opinion, etc. Apparently, the numerical aspect is not captured by word embed- ding. Example (c) might be an annotation error.</p><p>From the mistakes made by DTCNNs, we find the performance of DTCNN is mainly limited by two factors: the accuracy of the parser and the quality of word embedding. Future work will fo- cus on these two issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions and Future Work</head><p>We have presented a very simple dependency tree- based convolution framework which outperforms sequential CNN baselines on various classification tasks. Extensions of this model would consider dependency labels and constituency trees. Also, we would evaluate on gold-standard parse trees. own part-of-speech tagging). The word "fly" at the end of the sentence should be a verb instead of noun, and "hummingbirds fly" should be a relative clause modifying "speed".</p><p>There are some sentences that are misclassified by both the baseline CNN and DCNN. <ref type="figure" target="#fig_12">Figure 5</ref> shows three such examples. Example (a) is not classified as numerical by both methods due to the ambiguous meaning of the word "point" which is difficult to capture by word embedding. This word can mean location, opinion, etc. Apparently, the numerical aspect is not captured by word embed- ding. Example (c) might be an annotation error.</p><p>Shortly before submitting to ACL 2015 we learned <ref type="bibr">Mou et al. (2015, unpublished)</ref> have inde- pendently reported concurrent and related efforts. Their constituency model, based on their unpub- lished work in programming languages ( <ref type="bibr" target="#b17">Mou et al., 2014</ref>), 6 performs convolution on pretrained re- cursive node representations rather than word em- beddings, thus baring little, if any, resemblance to our dependency-based model. Their dependency model is related, but always includes a node and all its children (resembling <ref type="bibr" target="#b5">Iyyer et al. (2014)</ref>), which is a variant of our sibling model and always flat. By contrast, our ancestor model looks at the vertical path from any word to its ancestors, being linguistically motivated <ref type="bibr" target="#b20">(Shen et al., 2008</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>We have presented a very simple dependency- based convolution framework which outperforms sequential CNN baselines on modeling sentences.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Dependency tree of an example sentence from the Movie Reviews dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Convolution patterns on trees. Word concatenation always starts with m, while h, g, and g 2 denote parent, grand parent, and great-grand parent, etc., and " " denotes words excluded in convolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Examples from TREC (a-c), SST-1 (d) and TREC with fine-grained label (e-f) that are misclassified by the baseline CNN but correctly labeled by our DTCNN. For example, (a) should be entity but is labeled location by CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Examples from TREC (a-c), SST-1 (d) and TREC with fine-grained label (e-f) that are misclassified by the baseline CNN but correctly labeled by our DCNN. For example, (a) should be entity but is labeled location by CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>What is the speed hummingbirds fly ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>(</head><label></label><figDesc>Figure 3: Examples from TREC datasets that are misclassified by DTCNN but correctly labeled by baseline CNN. For example, (a) should be numerical but is labeled entity by DTCNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>5</head><label></label><figDesc>http://nlp.stanford.edu/software/parser-faq.shtml</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Figure 4: Examples from TREC datasets that are misclassified by both DTCNN and baseline CNN. For example, (a) should be numerical but is labeled entity by DTCNN and description by CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Examples from TREC datasets that are misclassified by DCNN but correctly labeled by baseline CNN. For example, (a) should be numerical but is labeled entity by DCNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>(</head><label></label><figDesc>Figure 3: Examples from TREC datasets that are misclassified by DTCNN but correctly labeled by baseline CNN. For example, (a) should be numerical but is labeled entity by DTCNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>5</head><label></label><figDesc>http://nlp.stanford.edu/software/parser-faq.shtml</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Figure 4: Examples from TREC datasets that are misclassified by both DTCNN and baseline CNN. For example, (a) should be numerical but is labeled entity by DTCNN and description by CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Examples from TREC datasets that are misclassified by both DCNN and baseline CNN. For example, (a) should be numerical but is labeled entity by DCNN and description by CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 summarizes</head><label>1</label><figDesc></figDesc><table>results in the context of other 
high-performing efforts in the literature. We use 
three benchmark datasets in two categories: senti-
ment analysis on both Movie Review (MR) (Pang 
and Lee, 2005) and Stanford Sentiment Treebank 
(SST-1) (Socher et al., 2013) datasets, and ques-
tion classification on TREC (</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>) .</head><label>.</label><figDesc></figDesc><table>Category 

Model 
MR 
SST-1 TREC TREC-2 

This work 
DCNNs: ancestor 
80.4  † 47.7  † 
95.4  † 
88.4  † 
DCNNs: ancestor+sibling 
81.7  † 48.3  † 
95.6  † 
89.0  † 
DCNNs: ancestor+sibling+sequential 
81.9 
49.5 
95.4  † 
88.8  † 

CNNs 
CNNs-non-static (Kim, 2014) -baseline 
81.5 
48.0 
93.6 
86.4  *  
CNNs-multichannel (Kim, 2014) 
81.1 
47.4 
92.2 
86.0  *  
Deep CNNs (Kalchbrenner et al., 2014) 
-
48.5 
93.0 
-

Recursive NNs 
Recursive Autoencoder (Socher et al., 2011) 
77.7 
43.2 
-
-
Recursive Neural Tensor (Socher et al., 2013) 
-
45.7 
-
-
Deep Recursive NNs (Irsoy and Cardie, 2014) -
49.8 
-
-
Recurrent NNs 
LSTM on tree (Zhu et al., 2015) 
81.9 
48.0 
-
-
Other deep learning Paragraph-Vec (Le and Mikolov, 2014) 
-
48.7 
-
-
Hand-coded rules 
SVM S (Silva et al., 2011) 
-
95.0 
90.8 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Results on Movie Review (MR), Stanford Sentiment Treebank (SST-1), and TREC datasets. 
TREC-2 is TREC with fine grained labels.  † Results generated by GPU (all others generated by CPU). 
 *  Results generated from Kim (2014)'s implementation. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Results on Movie Review (MR), Stanford Sentiment Treeba 
TREC-2 is TREC with fine grained labels.  † Results generated by GPU 
 *  Results generated from Kim (2014)'s implementation. 

What is Hawaii 's state flower ? 

root 

(a) enty ⇒ loc 

What is natural gas composed of ? 

root 

(b) enty ⇒ desc 

What does a defibrillator do ? 

root 

(c) desc ⇒ enty 

Nothing plot wise is worth emailing home about 

root 

(d) mild negative ⇒ mild positive 

What is the temperature at the center of the earth ? 

root 

(e) NUM:temp ⇒ NUM:dist 

What were Christopher Columbus ' three ships ? 

root 

(f) ENTY:veh ⇒ LOC:other 

</table></figure>

			<note place="foot" n="1"> The phrase-structure trees in SST-1 are actually automatically parsed, and thus can not be used as gold-standard trees. 2 GPU only supports float32 while CPU supports float64. 3 https://github.comw/yoonkim/CNN_sentence 4 https://github.com/cosmmb/DCNN</note>

			<note place="foot" n="6"> Both their 2014 and 2015 reports proposed (independently of each other and independently of our work) the term &quot;tree-based convolution&quot; (TBCNN).</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mining the peanut gallery: Opinion extraction and semantic classification of product reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kushal</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David M</forename><surname>Pennock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of World Wide Web</title>
		<meeting>World Wide Web</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sentiment classification on customer feedback data: noisy data, large feature vectors, and the role of linguistic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Ilya Sutskever, and Ruslan Salakhutdinov</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep recursive neural networks for compositionality in language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2096" to="2104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A neural network for factoid question answering over paragraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><surname>Claudino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Questionbank: Creating a corpus of parseannotated questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Judge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoife</forename><surname>Cahill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Van Genabith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient thirdorder dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A boosting algorithm for classification of semi-structured text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Comparison of learning algorithms for handwritten digit recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brunot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Mller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int&apos;l Conf. on Artificial Neural Nets</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning question classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL: Demonstrations</title>
		<meeting>ACL: Demonstrations</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sentiment classification using word sub-sequences and dependency sub-trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shotaro</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroya</forename><surname>Takamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of PA-KDD</title>
		<meeting>PA-KDD</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Online learning of approximate dependency parsing algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">TBCNN: A tree-based convolutional neural network for programming language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1409.5718" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Discriminative neural sentence modeling by tree-based convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1504.01106v5.Version5dated" />
		<imprint>
			<date type="published" when="2015-04-05" />
		</imprint>
	</monogr>
	<note>Tree-based Convolution: A New Architecture for Sentence Modeling</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">LTAG-spinal and the treebank. Language Resources and Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Champollion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind K</forename><surname>Joshi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning semantic representations using convolutional neural networks for web search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Xiaodong He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregoire</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mesnil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW</title>
		<meeting>WWW</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">From symbolic to sub-symbolic information in question classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Coheur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Wichert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="page">35</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2013</title>
		<meeting>EMNLP 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semantic parsing for single-relation question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Adadelta: An adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mattgew</forename><surname>Zeiler</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1212.5701" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Long short-term memory over tree structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parinaz</forename><surname>Sobhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
