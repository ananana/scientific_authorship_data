<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantically Smooth Knowledge Graph Embedding</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="laboratory">National Computer Network Emergency Response Technical Team Coordination Center of China</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100093, 100029</postCode>
									<settlement>Beijing, Beijing</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="laboratory">National Computer Network Emergency Response Technical Team Coordination Center of China</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100093, 100029</postCode>
									<settlement>Beijing, Beijing</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="laboratory">National Computer Network Emergency Response Technical Team Coordination Center of China</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100093, 100029</postCode>
									<settlement>Beijing, Beijing</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="laboratory">National Computer Network Emergency Response Technical Team Coordination Center of China</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100093, 100029</postCode>
									<settlement>Beijing, Beijing</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="laboratory">National Computer Network Emergency Response Technical Team Coordination Center of China</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100093, 100029</postCode>
									<settlement>Beijing, Beijing</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Semantically Smooth Knowledge Graph Embedding</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="84" to="94"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper considers the problem of embedding Knowledge Graphs (KGs) consisting of entities and relations into low-dimensional vector spaces. Most of the existing methods perform this task based solely on observed facts. The only requirement is that the learned embeddings should be compatible within each individual fact. In this paper, aiming at further discovering the intrinsic geometric structure of the embedding space, we propose Semantically Smooth Embedding (SSE). The key idea of SSE is to take full advantage of additional semantic information and enforce the embedding space to be semantically smooth, i.e., entities belonging to the same semantic category will lie close to each other in the embedding space. Two manifold learning algorithms Laplacian Eigenmaps and Locally Linear Embedding are used to model the smoothness assumption. Both are formulated as geometrically based regularization terms to constrain the embedding task. We empirically evaluate SSE in two benchmark tasks of link prediction and triple classification , and achieve significant and consistent improvements over state-of-the-art methods. Furthermore, SSE is a general framework. The smoothness assumption can be imposed to a wide variety of embedding models, and it can also be constructed using other information besides entities&apos; semantic categories.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge Graphs (KGs) like WordNet (Miller,l use of additional semantic information (i.e. se- mantic categories of entities) and enforce the em- bedding space to be semantically smooth-entities belonging to the same semantic category should lie close to each other in the embedding space. This smoothness assumption is closely related to the local invariance assumption exploited in mani- fold learning theory, which requires nearby points to have similar embeddings or labels <ref type="bibr" target="#b1">(Belkin and Niyogi, 2001</ref>). Thus we employ two manifold learning algorithms Laplacian Eigenmaps <ref type="bibr" target="#b1">(Belkin and Niyogi, 2001</ref>) and Locally Linear Embed- ding <ref type="bibr" target="#b23">(Roweis and Saul, 2000</ref>) to model the s- moothness assumption. The former requires an entity to lie close to every other entity in the same category, while the latter represents that entity as a linear combination of its nearest neighbors (i.e. entities within the same category). Both are for- mulated as manifold regularization terms to con- strain the KG embedding objective function. As such, SSE obtains an embedding space which is semantically smooth and at the same time com- patible with observed facts.</p><p>The advantages of SSE are two-fold: 1) By im- posing the smoothness assumption, SSE success- fully captures the semantic correlation between entities, which exists intrinsically but is over- looked in previous work on KG embedding. 2) KGs are typically very sparse, containing a rela- tively small number of facts compared to the large number of entities and relations. SSE can effec- tively deal with data sparsity by leveraging ad- ditional semantic information. Both aspects lead to more accurate embeddings in SSE. Moreover, our approach is quite general. The smoothness as- sumption can actually be imposed to a wide va- riety of KG embedding models. Besides seman- tic categories, other information (e.g. entity sim- ilarities specified by users or derived from auxil- iary data sources) can also be used to construc- t the manifold regularization terms. And besides KG embedding, similar smoothness assumptions can also be applied in other embedding tasks (e.g. word embedding and sentence embedding).</p><p>Our main contributions can be summarized as follows. First, we devise a novel KG embedding framework that naturally requires the embedding space to be semantically smooth. As far as we know, it is the first work that imposes constraints on the geometric structure of the embedding space during KG embedding. By leveraging addition- al semantic information, our approach can also deal with the data sparsity issue that commonly exists in typical KGs. Second, we evaluate our approach in two benchmark tasks of link predic- tion and triple classification, and achieve signif- icant and consistent improvements over state-of- the-art models.</p><p>In the remainder of this paper, we first provide a brief review of existing KG embedding model- s in Section 2, and then detail the proposed SSE framework in Section 3. Experiments and results are reported in Section 4. Then in Section 5 we discuss related work, followed by the conclusion and future work in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">A Brief Review of KG Embedding</head><p>KG embedding aims to embed entities and rela- tions into a continuous vector space and model the plausibility of each fact in that space. In general, it consists of three steps: 1) representing entities and relations, 2) specifying a scoring function, and 3) learning the latent representations. In the first step, given a KG, entities are represented as points (i.e. vectors) in a continuous vector space, and relation- s as operators in that space, which can be charac- terized by vectors ( <ref type="bibr" target="#b6">Bordes et al., 2014;</ref><ref type="bibr" target="#b31">Wang et al., 2014b</ref>), matrices <ref type="bibr" target="#b4">(Bordes et al., 2011;</ref><ref type="bibr" target="#b13">Jenatton et al., 2012)</ref>, or tensors ( <ref type="bibr" target="#b26">Socher et al., 2013</ref>). In the second step, for each candi- date fact ⟨e i , r k , e j ⟩, an energy function f (e i , r k , e j ) is further defined to measure its plausibility, with the corresponding entity and relation representa- tions as variables. Plausible triples are assumed to have low energies. Then in the third step, to obtain the entity and relation representations, a margin- based ranking loss, i.e.,</p><formula xml:id="formula_0">L = ∑ t + ∈O ∑ t − ∈N t + [ γ+ f (e i , r k , e j )− f (e ′ i , r k , e ′ j ) ] + ,<label>(1)</label></formula><p>is minimized. Here, O is the set of observed (i.e. positive) triples, and t + = ⟨e i , r k , e j ⟩ ∈ O; N t + de- notes the set of negative triples constructed by re- placing entities in t + , and t − = ⟨e ′ i , r k , e ′ j ⟩ ∈ N t + ; γ &gt; 0 is a margin separating positive and nega- tive triples; and [x] + = max(0, x). The ranking loss favors lower energies for positive triples than for negative ones. Stochastic gradient descent (in mini-batch mode) is adopted to solve the mini- mization problem. For details please refer to ) and references therein.</p><p>Different embedding models differ in the first t- wo steps: entity/relation representation and energy <ref type="bibr">TransE (Bordes et al., 2013</ref>) ) represents both en- tities and relations as vectors in the embedding s- pace. For a given triple ⟨e i , r k , e j ⟩, the relation is interpreted as a translation vector r k so that the embedded entities e i and e j can be connected by r k with low error. The energy function is defined as f (e i , r k , e j ) = ∥e i + r k − e j ∥ ℓ 1 /ℓ 2 , where ∥·∥ ℓ 1 /ℓ 2 denotes the ℓ 1 -norm or ℓ 2 -norm. SME ( <ref type="bibr" target="#b6">Bordes et al., 2014</ref>) also represents enti- ties and relations as vectors, but models triples in a more expressive way. Given a triple ⟨e i , r k , e j ⟩, it first employs a function g u (·, ·) to combine r k and e i , and g v (·, ·) to combine r k and e j . Then, the energy function is defined as matching g u (·, ·) and g v (·, ·) by their dot product, i.e., f (e i , r k , e j ) = g u (r k , e i ) T g v (r k , e j ). There are two versions of SME, linear and bilinear (denoted as SME (lin) and SME (bilin) respectively), obtained by defin- ing different g u (·, ·) and g v (·, ·).</p><note type="other">Method Entity/Relation embeddings Energy function</note><formula xml:id="formula_1">e, r ∈ R d f (e i , r k , e j ) = ∥e i + r k − e j ∥ ℓ 1 /ℓ 2 SME (lin) (Bordes et al., 2014) e, r ∈ R d f (e i , r k , e j ) = (W u1 r k + W u2 e i + b u ) T ( W v1 r k + W v2 e j + b v ) SME (bilin) (Bordes et al., 2014) e, r ∈ R d f (e i , r k , e j ) = (( W u ¯ × 3 r k ) e i + b u ) T (( W v ¯ × 3 r k ) e j + b v ) SE (Bordes et al., 2011) e ∈ R d , R u , R v ∈ R d×d f (e i , r k , e j ) = ∥R u k e i − R v k e j ∥ ℓ 1</formula><p>SE <ref type="formula">(</ref>  <ref type="table" target="#tab_0">Table 1</ref> summarizes the entity/relation representa- tions and energy functions used in these models.</p><formula xml:id="formula_2">e i , r k , e j ) = ∥R u k e i − R v k e j ∥ ℓ 1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Semantically Smooth Embedding</head><p>The methods introduced above perform the em- bedding task based solely on observed facts. The only requirement is that the learned embeddings should be compatible within each individual fact. However, they fail to discover the intrinsic geo- metric structure of the embedding space. To deal with this limitation, we introduce Semantically S- mooth Embedding (SSE) which constrains the em- bedding task by incorporating geometrically based regularization terms, constructed by using addi- tional semantic categories of entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>Suppose we are given a KG consisting of n entities and m relations. The facts observed are stored as a set of triples</p><formula xml:id="formula_3">O = { ⟨e i , r k , e j ⟩ } . A triple ⟨e i , r k , e j ⟩</formula><p>indicates that entity e i and entity e j are connected by relation r k . In addition, the entities are classi- fied into multiple semantic categories. Each entity e is associated with a label c e indicating the cate- gory to which it belongs. SSE aims to embed the entities and relations into a continuous vector s- pace which is compatible with the observed facts, and at the same time semantically smooth. To make the embedding space compatible with the observed facts, we make use of the triple set O and follow the same strategy adopted in previous methods. That is, we define an energy function on each candidate triple (e.g. the energy functions listed in <ref type="table" target="#tab_0">Table 1</ref>), and require observed triples to have lower energies than unobserved ones (i.e. the margin-based ranking loss defined in Eq. <ref type="formula" target="#formula_0">(1)</ref>).</p><p>To make the embedding space semantically s- mooth, we further leverage the entity category in- formation {c e }, and assume that entities within the same semantic category should lie close to each other in the embedding space. This smoothness assumption is similar to the local invariance as- sumption exploited in manifold learning theory (i.e. nearby points are likely to have similar em- beddings or labels). So we employ two manifold learning algorithms Laplacian Eigenmaps <ref type="bibr" target="#b1">(Belkin and Niyogi, 2001</ref>) and Locally Linear Embed- ding <ref type="bibr" target="#b23">(Roweis and Saul, 2000</ref>) to model such se- mantic smoothness, termed as LE and LLE for short respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Modeling Semantic Smoothness by LE</head><p>Laplacian Eigenmaps (LE) is a manifold learning algorithm that preserves local invariance between each two data points <ref type="bibr" target="#b1">(Belkin and Niyogi, 2001</ref>). We borrow the idea of LE and enforce semantic smoothness by assuming:</p><p>Smoothness Assumption 1 If two entities e i and e j belong to the same semantic category, they will have embeddings e i and e j close to each other.</p><p>To encode the semantic information, we construct an adjacency matrix W 1 ∈ R n×n among the enti- ties, with the i j-th entry defined as:</p><formula xml:id="formula_4">w (1) i j =        1, if c e i = c e j , 0, otherwise,</formula><p>where c e i /c e j is the category label of entity e i /e j . Then, we use the following term to measure the smoothness of the embedding space:</p><formula xml:id="formula_5">R 1 = 1 2 n ∑ i=1 n ∑ j=1 ∥e i − e j ∥ 2 2 w (1) i j ,</formula><p>where e i and e j are the embeddings of entities e i and e j respectively. By minimizing R 1 , we expect Smoothness Assumption 1: if two entities e i and e j belong to the same semantic category (i.e. w (1) i j = 1), the distance between e i and e j (i.e. ∥e i − e j ∥ 2 2 ) should be small.</p><p>We further incorporate R 1 as a regularization term into the margin-based ranking loss (i.e. Eq.</p><p>(1)) adopted in previous KG embedding methods, and propose our first SSE model. The new mod- el performs the embedding task by minimizing the following objective function:</p><formula xml:id="formula_6">L 1 = 1 N ∑ t + ∈O ∑ t − ∈N t + ℓ ( t + , t − ) + λ 1 2 n ∑ i=1 n ∑ j=1 ∥e i − e j ∥ 2 2 w (1) i j , where ℓ ( t + , t − ) = [ γ+ f (e i , r k , e j )− f (e ′ i , r k , e ′ j )</formula><p>] + is the ranking loss on the positive-negative triple pair</p><formula xml:id="formula_7">( t + , t − )</formula><p>, and N is the total number of such triple pairs. The first term in L 1 enforces the resultant embedding space compatible with all the observed triples, and the second term further requires that space to be semantically smooth. Hyperparameter λ 1 makes a trade-off between the two cases.</p><p>The minimization is carried out by stochastic gradient descent. Given a randomly sampled posi- tive triple t + = ⟨e i , r k , e j ⟩ and the associated nega- tive triple t − = ⟨e ′ i , r k , e ′ j ⟩, 1 the stochastic gradient w.r.t. e s (s ∈ {i, j, i ′ , j ′ }) can be calculated as:</p><formula xml:id="formula_8">∇ e s L 1 = ∇ e s ℓ ( t + , t − ) + 2λ 1 E (D − W 1 ) 1 s , 1</formula><p>The negative triple is constructed by replacing one of the entities in the positive triple.</p><p>where E = [e 1 , e 2 , · · · , e n ] ∈ R d×n is a matrix con- sisting of entity embeddings; D ∈ R n×n is a di- agonal matrix with the i-th entry on the diagonal being d ii = ∑ n j=1 w (1) i j ; and 1 s ∈ R n is a column vector where the s-th entry is 1 and the others are 0. Other parameters are not included in R 1 , and their gradients remain the same as defined in pre- vious work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Modeling Semantic Smoothness by LLE</head><p>As opposed to LE which preserves local invari- ance within data pairs, Locally Linear Embedding (LLE) expects each data point to be roughly re- constructed by a linear combination of its nearest neighbors <ref type="bibr" target="#b23">(Roweis and Saul, 2000</ref>). We borrow the idea of LLE and enforce semantic smoothness by assuming:</p><p>Smoothness Assumption 2 Each entity e i can be roughly reconstructed by a linear combination of its nearest neighbors in the embedding space, i.e.,</p><formula xml:id="formula_9">e i ≈ ∑ e j ∈N(e i ) α j e j .</formula><p>Here nearest neighbors refer to entities belonging to the same semantic catego- ry with e i .</p><p>To model this assumption, for each entity e i , we randomly sample K entities uniformly from the category to which e i belongs, denoted as the n- earest neighbor set N (e i ). We construct a weight matrix W 2 ∈ R n×n by defining:</p><formula xml:id="formula_10">w (2) i j =        1, if e j ∈ N (e i ) , 0, otherwise,</formula><p>and normalize the rows so that ∑ n j=1 w (2) i j = 1 for each row i. Note that W 2 is no longer a symmetric matrix. The smoothness of the embedding space can be measured by the reconstruction error:</p><formula xml:id="formula_11">R 2 = n ∑ i=1 e i − ∑ e j ∈N(e i ) w (2) i j e j 2 2 .</formula><p>Minimizing R 2 results in Smoothness Assump- tion 2: each entity can be linearly reconstructed from its nearest neighbors with low error. By incorporating R 2 as a regularization term in- to the margin-based ranking loss defined in Eq.</p><p>(1), we obtain our second SSE model, which per- forms the embedding task by minimizing:</p><formula xml:id="formula_12">L 2 = 1 N ∑ t + ∈O ∑ t − ∈N t + ℓ ( t + , t − ) +λ 2 n ∑ i=1 e i − ∑ e j ∈N(e i ) w (2) i j e j 2 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>87</head><p>The resultant embedding space is also semanti- cally smooth and compatible with the observed triples. Hyperparameter λ 2 makes a trade-off be- tween the two cases. Similar to the first model, stochastic gradien- t descent is used to solve the minimization prob- lem. Given a positive triple t + = ⟨e i , r k , e j ⟩ and the associated negative triple t − = ⟨e ′ i , r k , e ′ j ⟩, the gradient w.r.t. e s (s ∈ {i, j, i ′ , j ′ }) is calculated as:</p><formula xml:id="formula_13">∇ e s L 2 = ∇ e s ℓ ( t + , t − ) +2λ 2 E (I − W 2 ) T (I − W 2 ) 1 s ,</formula><p>where I ∈ R n×n is the identity matrix. Other pa- rameters are not included in R 2 , and their gradi- ents remain the same as defined in previous work.</p><p>To better capture the cohesion within each cate- gory, during each stochastic step we resample the nearest neighbors for each entity, uniformly from the category to which it belongs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Advantages and Extensions</head><p>The advantages of our approach can be summa- rized as follows: 1) By incorporating geometri- cally based regularization terms, the SSE mod- els are able to capture the semantic correlation between entities, which exists intrinsically but is overlooked in previous work. 2) By leveraging ad- ditional entity category information, the SSE mod- els can deal with the data sparsity issue that com- monly exists in most KGs. Both aspects lead to more accurate embeddings. Entity category information has also been inves- tigated in ( <ref type="bibr" target="#b21">Nickel et al., 2012;</ref><ref type="bibr" target="#b11">Chang et al., 2014;</ref><ref type="bibr" target="#b32">Wang et al., 2015</ref>), but in different manners. <ref type="bibr" target="#b21">Nickel et al. (2012)</ref> take categories as pseudo entities and introduce a specific relation to link entities to categories. <ref type="bibr" target="#b11">Chang et al. (2014)</ref> and <ref type="bibr" target="#b32">Wang et al. (2015)</ref> use entity categories to specify relation- s' argument expectations, removing invalid triples during training and reasoning respectively. None of them considers the intrinsic geometric structure of the embedding space.</p><p>Actually, our approach is quite general. 1) The smoothness assumptions can be imposed to a wide variety of KG embedding models, not only the ones introduced in Section 2, but also those based on matrix/tensor factorization <ref type="bibr" target="#b20">(Nickel et al., 2011;</ref><ref type="bibr" target="#b10">Chang et al., 2013</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We empirically evaluate the proposed SSE models in two tasks: link prediction (  and triple classification (Socher et al., 2013).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data Sets</head><p>We create three data sets with different sizes using NELL <ref type="bibr">(Carlson et al., 2010)</ref>: Location, Sport, and Nell186. Location and Sport are two small-scale data sets, both containing 8 relations on the topics of "location" and "sport" respectively. The corre- sponding relations are listed in <ref type="table" target="#tab_3">Table 2</ref>. Nell186 is a larger data set containing the most frequent 186 relations. On all the data sets, entities appearing only once are removed. We extract the entity cat- egory information from a specific relation called Generalization, and keep non-overlapping cat- egories. <ref type="bibr">2</ref> Categories containing less than 5 entities on Location and Sport as well as categories con- taining less than 50 entities on Nell186 are fur- ther removed.  <ref type="table" target="#tab_0">Location  8  380  718  5  358  Sport  8  1,520  3,826  4  1,506  Nell186  186 14,463 41,134  35  8,590   Table 3</ref>: Statistics of data sets.</p><p>training/validation/test splits, and results averaged over the 5 rounds are reported. On Nell186 ex- periments are conducted only once, using a train- ing/validation/test split with 31,134/5,000/5,000 triples respectively. We will release the data up- on request.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Link Prediction</head><p>This task is to complete a triple ⟨e i , r k , e j ⟩ with e i or e j missing, i.e., predict e i given (r k , e j ) or pre- dict e j given (e i , r k ). Baseline methods. We take TransE, SME (lin), SME (bilin), and SE as our baselines. We then in- corporate manifold regularization terms into these methods to obtain the SSE models. A model with the LE/LLE regularization term is denoted as TransE-LE/TransE-LLE for example. We fur- ther compare our SSE models with the setting pro- posed by <ref type="bibr" target="#b21">Nickel et al. (2012)</ref>, which also takes in- to account the entity category information, but in a more direct manner. That is, given an entity e with its category label c e , we create a new triple ⟨e, Generalization, c e ⟩ and add it into the train- ing set. Such a method is denoted as TransE-Cat for example.</p><p>Evaluation protocol. For evaluation, we adopt the same ranking procedure proposed by . For each test triple ⟨e i , r k , e j ⟩, the head entity e i is replaced by every entity e ′ i in the KG, and the energy is calculated for the corrupted triple ⟨e ′ i , r k , e j ⟩. Ranking the energies in ascending or- der, we get the rank of the correct entity e i . Sim- ilarly, we can get another rank by corrupting the tail entity e j . Aggregated over all test triples, we report three metrics: 1) the averaged rank, denoted as Mean (the smaller, the better); 2) the median of the ranks, denoted as Median (the smaller, the bet- ter); and 3) the proportion of ranks no larger than 10, denoted as Hits@10 (the higher, the better).</p><p>Implementation details. We implement the methods based on the code provided by   <ref type="bibr">3</ref> . For all the methods, we create 100 mini-batches on each data set. On Location and Sport, the dimension of the embedding space d is set in the range of {10, 20, 50, 100}, the margin γ is set in the range of {1, 2, 5, 10}, and the learning rate is fixed to 0.1. On Nell186, the hyperparame- ters d and γ are fixed to 50 and 1 respectively, and the learning rate is fixed to 10. In LE and LLE, the regularization hyperparameters λ 1 and λ 2 are tuned in {10 −4 , 10 −5 , 10 −6 , 10 −7 , 10 −8 }. And the number of nearest neighbors K in LLE is tuned in {5, 10, 15, 20}. The best model is selected by ear- ly stopping on the validation sets (by monitoring Mean), with a total of at most 1000 iterations over the training sets.</p><p>Results. <ref type="table" target="#tab_7">Table 4</ref> reports the results on the test sets of Location, Sport, and Nell186. From the results, we can see that: 1) SSE (regularized vi- a either LE or LLE) outperforms all the baselines on all the data sets and with all the metrics. The improvements are usually quite significant. The metric Mean drops by about 10% to 65%, Medi- an drops by about 5% to 75%, and Hits@10 rises by about 5% to 190%. This observation demon- strates the superiority and generality of our ap- proach. 2) Even if encoded in a direct way (e.g. TransE-Cat), the entity category information can still help the baseline methods in the link predic- tion task. This observation indicates that leverag- ing additional information is indeed useful in deal- ing with the data sparsity issue and hence leads to better performance. 3) Compared to the strategy which incorporates the entity category information directly, formulating such information as manifold regularization terms results in better and more sta- ble results. The *-Cat models sometimes perfor- m even worse than the baselines (e.g. TransE-Cat on Sport data), while the SSE models consistent- ly achieve better results. This observation further demonstrates the superiority of constraining the geometric structure of the embedding space.</p><p>We further visualize and compare the geometric structures of the embedding spaces learned by tra- ditional embedding and semantically smooth em- bedding. We select the 10 largest semantic cate- gories in Nell186 (specified in <ref type="figure" target="#fig_1">Figure 1</ref>) and the 5,740 entities therein. We take the embeddings of these entities learned by TransE, TransE-Cat, TransE-LE, and TransE-LLE, with the optimal hy- perparameter settings determined in the link pre- diction task. Then we create 2D plots using t- SNE (Van der Maaten and Hinton, 2008) <ref type="bibr">4</ref> . The results are shown in <ref type="figure" target="#fig_1">Figure 1</ref>, where a different Location Sport Nell186 Mean Median Hits@10 (%) Mean Median Hits@10 (%) Mean Median Hits@10 (%)     color is used for each category. It is easy to see that imposing the semantic smoothness assump- tions helps in capturing the semantic correlation between entities in the embedding space. Entities within the same category lie closer to each oth- er, while entities belonging to different categories are easily distinguished (see <ref type="figure" target="#fig_1">Figure 1</ref>(c) and <ref type="figure" target="#fig_1">Fig- ure 1(d)</ref>). Incorporating the entity category infor- mation directly could also helps. But it fails on some "hard" entities (i.e., those belonging to d- ifferent categories but mixed together in the cen- ter of <ref type="figure" target="#fig_1">Figure 1(b)</ref>). We have conducted the same experiments with the other methods and observed similar phenomena.</p><note type="other">SME (lin)-Cat 41.12 18.30 42.43 263.88 70.80 35.03 309.60 25.00 36.22 SME (lin)-LE 36.19 16.10 43.75 237.38 50.80 38.35 276.94 25.00 37.14 SME (lin)-LLE 38.22 15.60 43.96 241.70 63.70 36.54 252.87 25.00 37.14 SME (bilin) 47.66 20.90 37.85 314.49 124.00 33.83 848.39 28.00 35.71 SME (bilin)-Cat 40.75 16.20 42.71 298.09 103.80 35.86 560.76 24.00 37.83 SME (bilin)-LE 33.41 14.00 44.24 297.90 116.10 38.95 448.31 24.00 37.80 SME (bilin)-LLE 32.84 13.60 46.25 286.63 110.10 35.</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Triple Classification</head><p>This task is to verify whether a given triple ⟨e i , r k , e j ⟩ is correct or not. We test our SSE mod- els in this task, with the same comparison settings as used in the link prediction task. Evaluation protocol. We follow the same eval- uation protocol used in <ref type="bibr" target="#b26">(Socher et al., 2013;</ref><ref type="bibr" target="#b31">Wang et al., 2014b</ref>). To create labeled data for classifica- tion, for each triple in the test and validation sets, we construct a negative triple for it by randomly corrupting the entities. To corrupt a position (head or tail), only entities that have appeared in that po- sition are allowed. During triple classification, a triple is predicted as positive if the energy is be- low a relation-specific threshold δ r ; otherwise as negative. We report two metrics on the test sets: micro-averaged accuracy and macro-averaged ac- curacy, denoted as Micro-ACC and Macro-ACC respectively. The former is a per-triple average, while the latter is a per-relation average. Implementation details. We use the same hy- perparameter settings as in the link prediction task. The relation-specific threshold δ r is determined by maximizing Micro-ACC on the validation sets. A- gain, training is limited to at most 1000 iterations, and the best model is selected by early stopping on the validation sets (by monitoring Micro-ACC).</p><p>Results.  line methods on all the data sets in both metric- s. The improvements are usually quite substantial. The metric Micro-ACC rises by about 1% to 25%, and Macro-ACC by about 2% to 30%. 2) Incorpo- rating the entity category information directly can also improve the baselines in the triple classifica- tion task, again demonstrating the effectiveness of leveraging additional information to deal with the data sparsity issue.</p><p>3) It is a better choice to in- corporate the entity category information as man- ifold regularization terms as opposed to encoding it directly. The *-Cat models sometimes perfor- m even worse than the baselines (e.g. TransE- Cat on Location data and SE-Cat on Sport data), while the SSE models consistently achieve better results. The observations are similar to those ob- served during the link prediction task, and further demonstrate the superiority and generality of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>This section reviews two lines of related work: KG embedding and manifold learning. KG embedding aims to embed a KG composed of entities and relations into a low-dimensional vector space, and model the plausibility of each fact in that space.  categorized the literature into three major groups: 1) method- s based on neural networks, 2) methods based on matrix/tensor factorization, and 3) methods based on Bayesian clustering. The first group perform- s the embedding task using neural network archi- tectures ( <ref type="bibr" target="#b6">Bordes et al., 2014;</ref><ref type="bibr" target="#b26">Socher et al., 2013)</ref>. Several state-of-the-art neural network-based embedding models have been in- troduced in Section 2. For other work please refer to ( <ref type="bibr" target="#b13">Jenatton et al., 2012;</ref><ref type="bibr" target="#b31">Wang et al., 2014b;</ref><ref type="bibr" target="#b16">Lin et al., 2015</ref>). In the second group, KGs are represent- ed as tensors, and embedding is performed via ten- sor factorization or collective matrix factorization techniques <ref type="bibr" target="#b25">(Singh and Gordon, 2008;</ref><ref type="bibr" target="#b20">Nickel et al., 2011;</ref><ref type="bibr" target="#b11">Chang et al., 2014</ref>). The third group embeds factorized representations of entities and relations into a nonparametric Bayesian clustering frame- work, so as to obtain more interpretable embed- dings ( <ref type="bibr" target="#b14">Kemp et al., 2006;</ref><ref type="bibr" target="#b27">Sutskever et al., 2009)</ref>. Our work falls into the first group, but differs in that it further imposes constraints on the geomet- ric structure of the embedding space, which exists intrinsically but is overlooked in previous work. Although this paper focuses on incorporating ge- ometrically based regularization terms into neural network architectures, it can be easily extended to matrix/tensor factorization techniques.</p><p>Manifold learning is a geometrically motivat- ed framework for machine learning, enforcing the learning model to be smooth w.r.t. the geometric structure of data ( <ref type="bibr" target="#b2">Belkin et al., 2006</ref>). Within this framework, various manifold learning algorithm- s have been proposed, such as ISOMAP <ref type="bibr" target="#b28">(Tenenbaum et al., 2000</ref>), Laplacian Eigenmaps ( <ref type="bibr" target="#b1">Belkin and Niyogi, 2001)</ref>, and Locally Linear Embed- ding <ref type="bibr" target="#b23">(Roweis and Saul, 2000</ref>). All these algo- rithms are based on the so-called local invariance assumption, i.e., nearby points are likely to have similar embeddings or labels. Manifold learning has been widely applied in many different areas, from dimensionality reduction <ref type="bibr">(Belkin and Niyo-gi, 2001;</ref><ref type="bibr" target="#b7">Cai et al., 2008)</ref> and semi-supervised learning ( <ref type="bibr" target="#b35">Zhou et al., 2004;</ref><ref type="bibr" target="#b36">Zhu and Niyogi, 2005</ref>) to recommender systems <ref type="bibr" target="#b17">(Ma et al., 2011)</ref> and community question answering ( <ref type="bibr" target="#b30">Wang et al., 2014a)</ref>. This paper employs manifold learning al- gorithms to model the semantic smoothness as- sumptions in KG embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this paper, we have proposed a novel approach to KG embedding, referred to as Semantically S- mooth Embedding (SSE). The key idea of SSE is to impose constraints on the geometric structure of the embedding space and enforce it to be semanti- cally smooth. The semantic smoothness assump- tions are constructed by using entities' category information, and then formulated as geometrical- ly based regularization terms to constrain the em- bedding task. The embeddings learned in this way are capable of capturing the semantic correlation between entities. By leveraging additional infor- mation besides observed triples, SSE can also deal with the data sparsity issue that commonly exists in most KGs. We empirically evaluate SSE in two benchmark tasks of link prediction and triple clas- sification. Experimental results show that by in- corporating the semantic smoothness assumption- s, SSE significantly and consistently outperforms state-of-the-art embedding methods, demonstrat- ing the superiority of our approach. In addition, our approach is quite general. The smoothness as- sumptions can actually be imposed to a wide vari- ety of embedding models, and it can also be con- structed using other information besides entities' semantic categories.</p><p>As future work, we would like to: 1) Construct the manifold regularization terms using other da- ta sources. The only information required to con- struct the manifold regularization terms is the sim- ilarity between entities (used to define the adja- cency matrix in LE and to select nearest neigh- bors for each entity in LLE). We would try entity similarities derived in different ways, e.g., spec- ified by users or calculated from entities' textual descriptions. 2) Enhance the efficiency and scala- bility of SSE. Processing the manifold regulariza- tion terms can be time-and space-consuming (e- specially the one induced by the LE algorithm). We would investigate how to address this prob- lem, e.g., via the efficient iterative algorithms in- troduced in <ref type="bibr" target="#b24">(Saul and Roweis, 2003)</ref> or via paral- lel/distributed computing. 3) Impose the seman- tic smoothness assumptions on other KG embed- ding methods (e.g. those based on matrix/tensor factorization or Bayesian clustering), and even on other embedding tasks (e.g. word embedding or sentence embedding).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Embeddings of entities belonging to the 10 largest categories in Nell186 (best viewed in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Existing KG embedding models. 

function definition. Three state-of-the-art embed-
ding models, namely TransE (Bordes et al., 2013), 
SME (Bordes et al., 2014), and SE (Bordes et al., 
2011), are detailed below. Please refer to (Jenat-
ton et al., 2012; Socher et al., 2013; Wang et al., 
2014b; Lin et al., 2015) for other methods. 
TransE (Bordes et al., 2013</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Relations in Location and Sport. 

applied in other embedding tasks (e.g. word em-
bedding and sentence embedding). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 gives</head><label>3</label><figDesc></figDesc><table>some statistics of the 
three data sets, where # Rel./# Ent./# Trip./# Cat. 
denotes the number of relations/entities/observed 
triples/categories respectively, and # c-Ent. de-
notes the number of entities that have category la-
bels. Note that our SSE models do not require ev-
ery entity to have a category label. From the statis-
tics, we can see that all the three data sets suffer 
from the data sparsity issue, containing a relative-
ly small number of observed triples compared to 
the number of entities. 
On the two small-scale data sets Location and 
Sport, triples are split into training/validation/test 
sets, with the ratio of 3:1:1. The first set is used 
for modeling training, the second for hyperparam-
eter tuning, and the third for evaluation. All ex-
periments are repeated 5 times by drawing new </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 4 : Link prediction results on the test sets of Location, Sport, and Nell186.</head><label>4</label><figDesc></figDesc><table>Athlete 
Politicianus 
Chemical 
City 
Clothing 
Country 
Sportsteam 
Journalist 
Televisionstation 
Room 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 5 reportsACC Macro-ACC Micro-ACC Macro-ACC Micro-ACC Macro-ACC</head><label>5</label><figDesc></figDesc><table>the results on the test 
sets of Location, Sport, and Nell186. The results 
indicate that: 1) SSE (regularized via either LE or 
LLE) performs consistently better than the base-

90 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Triple classification results (%) on the test sets of Location, Sport, and Nell186. 

</table></figure>

			<note place="foot" n="2"> If two categories overlap, the smaller one is discarded.</note>

			<note place="foot" n="3"> https://github.com/glorotxa/SME</note>

			<note place="foot" n="4"> http://lvdmaaten.github.io/tsne/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous reviewers for their valuable comments and suggestions. This work is supported by the National Natural Science Foundation of China (grant No. 61402465), the S-trategic Priority Research Program of the Chinese Academy of Sciences (grant No. XDA06030200), and the National Key Technology R&amp;D Program (grant No. 2012BAH46B03).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Random walks for knowledge-based word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oier</forename><surname>Lopez De Lacalle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Soroa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="84" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps and spectral techniques for embedding and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="585" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Manifold regularization: A geometric framework for learning from labeled and unlabeled examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Sindhwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2399" to="2434" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Freebase: A collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data</title>
		<meeting>the 2008 ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning structured embeddings of knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 25th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="301" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garciadurán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A semantic matching energy function for learning with multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="233" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Non-negative matrix factorization on manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th IEEE International Conference on Data Mining</title>
		<meeting>the 8th IEEE International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="63" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Estevam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Hruschka</surname><genName>Jr</genName></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Toward an architecture for neverending language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 24th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1306" to="1313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-relational latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1602" to="1612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Typed tensor decomposition of knowledge bases for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1568" to="1579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Knowledge-based weak supervision for information extraction of overlapping relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congle</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A latent factor model for highly multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodolphe</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><forename type="middle">R</forename><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3167" to="3175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning systems of concepts with an infinite relational model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Kemp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naonori</forename><surname>Ueda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st AAAI Conference on Artificial Intelligence</title>
		<meeting>the 21st AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="381" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Dbpedia: A largescale, multilingual knowledge base extracted from wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Isele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anja</forename><surname>Jentzsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Kontokostas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><forename type="middle">N</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Hellmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Morsey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Kleef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sören</forename><surname>Auer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Semantic Web Journal</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 29th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2181" to="2187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recommender systems with social regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th ACM International Conference on Web Search and Data Mining</title>
		<meeting>the 4th ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="287" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A wordnet-based approach to named entities recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Prevete</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hristo</forename><surname>Tanev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2002 Workshop on Building and Using Semantic Networks</title>
		<meeting>the 2002 Workshop on Building and Using Semantic Networks</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Wordnet: A lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning</title>
		<meeting>the 28th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Factorizing yago: Scalable machine learning for linked data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on World Wide Web</title>
		<meeting>the 21st International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="271" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Relation extraction with matrix factorization and universal schemas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">M</forename><surname>Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference on North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="74" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">K</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Think globally, fit locally: Unsupervised learning of low dimensional manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><forename type="middle">T</forename><surname>Saul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="119" to="155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Relational learning via collective matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajit</forename><forename type="middle">P</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="650" to="658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Modelling relational data using bayesian clustered tensor factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1821" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A global geometric framework for nonlinear dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vin</forename><forename type="middle">De</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2319" to="2323" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">85</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A regularized competition model for question difficulty estimation in community question answering services</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1115" to="1126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 28th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1112" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Knowledge base completion using embeddings and rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 24th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Connecting language and knowledge bases with embedding models for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1366" to="1371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning multi-relational semantics using neural-embedding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4072</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning with local and global consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">Navin</forename><surname>Lal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Harmonic mixtures: combining mixture models and graph-based methods for inductive and scalable semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Machine Learning</title>
		<meeting>the 22nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1052" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
