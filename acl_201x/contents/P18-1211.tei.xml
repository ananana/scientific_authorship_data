<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Spatial Model for Extracting and Visualizing Latent Discourse Structure in Text</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018. 2268</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashank</forename><surname>Srivastava</surname></persName>
							<email>ssrivastava@cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nebojsa</forename><surname>Jojic</surname></persName>
							<email>jojic@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Redmond</orgName>
								<address>
									<postCode>98052</postCode>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Spatial Model for Extracting and Visualizing Latent Discourse Structure in Text</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2268" to="2277"/>
							<date type="published">July 15-20, 2018. 2018. 2268</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a generative probabilistic model of documents as sequences of sentences, and show that inference in it can lead to extraction of long-range latent discourse structure from a collection of documents. The approach is based on embedding sequences of sentences from longer texts into a 2-or 3-D spatial grids, in which one or two coordinates model smooth topic transitions , while the third captures the sequential nature of the modeled text. A significant advantage of our approach is that the learned models are naturally visualiz-able and interpretable, as semantic similarity and sequential structure are modeled along orthogonal directions in the grid. We show that the method can capture discourse structures in narrative text across multiple genres, including biographies, stories, and newswire reports. In particular, our method can capture biographical templates from Wikipedia, and is competitive with state-of-the-art generative approaches on tasks such as predicting the outcome of a story, and sentence ordering.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ability to identify discourse patterns and nar- rative themes from language is useful in a wide range of applications and data analysis. From a perspective of language understanding, learning such latent structure from large corpora can pro- vide background information that can aid machine reading. For example, computers can use such knowledge to predict what is likely to happen next * *Work done while first author was an intern at Microsoft Research in a narrative ( <ref type="bibr" target="#b19">Mostafazadeh et al., 2016)</ref>, or rea- son about which narratives are coherent and which do not make sense ( <ref type="bibr" target="#b2">Barzilay and Lapata, 2008)</ref>. Similarly, knowledge of discourse is increasingly important for language generation models. Modern neural generation models, while good at capturing surface properties of text -by fusing elements of syntax and style -are still poor at modeling long range dependencies that go across sentences ( <ref type="bibr" target="#b13">Li and Jurafsky, 2017;</ref><ref type="bibr" target="#b24">Wang et al., 2017)</ref>. Models of long range flow in the text can thus be useful as additional input to such methods.</p><p>Previously, the question of modeling discourse structure in language has been explored through several lenses, including from perspectives of lin- guistics, cognitive science and information re- trieval. Prominent among linguistic approaches are Discourse Representation Theory <ref type="bibr" target="#b0">(Asher, 1986)</ref> and Rhetorical Structure Theory ( <ref type="bibr" target="#b17">Mann and Thompson, 1988)</ref>; which formalize how discourse context can constrain the semantics of a sentence, and lay out ontologies of discourse relation types between parts of a document. This line of research has been largely constrained by the unavailability of corpora of discourse relations, which are ex- pensive to annotate. Another line of research has focused on the task of automatic script induction, building on earlier work in the 1970's ( <ref type="bibr" target="#b22">Schank and Abelson, 1977)</ref>. More recently, methods based on neural distributed representations have been ex- plored ( <ref type="bibr" target="#b10">Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b11">Le and Mikolov, 2014</ref>) to model the flow of discourse. While these methods have had varying degrees of success, they are largely opaque and hard to interpret. In this work, we seek to pro- vide a scalable model that can extract latent sequen- tial structures from a collection of documents, and can be naturally visualized to provide a summary of the learned semantics and discourse trajectories.</p><p>In this work, we present an approach for extract- <ref type="figure">Figure 1</ref>: Modeling principle for Sequential Count- ing Grids. We design the method to capture se- mantic similarities between documents along XY planes (e.g., biographies might be more similar to literary fiction than news reports), as well extract sequential trajectories along the Z axes similar to those shown. The sequence of sentences in a doc- ument is latently aligned to positions in the grid, such that the model prefers alignments of contigu- ous sentences to grid cells that are spatially close.</p><p>ing and visualizing sequential structure from a col- lection of text documents. Our method is based on embedding sentences in a document in a 3- dimensional grid, such that contiguous sentences in the document are likely to be embedded in the same order in the grid. Further, sentences across docu- ments that are semantically similar are also likely to be embedded in the same neighborhood in the grid. By leveraging the sequential order of sentences in a large document collection, the method can induce lexical semantics, as well as extract latent discourse trajectories in the documents. <ref type="figure">Figure 1</ref> shows a con- ceptual schematic of our approach. The method can learn semantic similarity (across XY planes), as well as sequential discourse chains (along the Z-axis). The parameters and latent structure of the grid are learned by optimizing the likelihood of a collection of documents under a generative model. Our method outperforms state-of-the-art generative methods on two tasks: predicting the outcome of a story and coherence prediction; and is seen to yield a flexible range of interpretable visualizations in different domains of text. Our method is scalable, and can incorporate a broad range of features. In particular, the approach can work on simple tok- enized text. The remainder of this paper is organized as fol- lows. In Section 2, we briefly summarize other related work. In Section 3, we describe our method in detail. We present experimental results in Sec- tion 4, and conclude with a brief discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Building on linguistic theories of discourse and text coherence, several computational approaches have attempted to model discourse structure from multiple perspectives. Prominent among these are Narrative Event Chains ( <ref type="bibr" target="#b4">Chambers and Jurafsky, 2008)</ref> which learn chains of events that follow a pattern in a unsupervised framework, and the Entity grid model ( <ref type="bibr" target="#b2">Barzilay and Lapata, 2008)</ref>, which rep- resents sentences in a context in terms of discourse entities occurring in them and trains coherence clas- sifiers over this representation. Other work extends these using better models of events and discourse entities <ref type="bibr" target="#b15">(Lin et al., 2011;</ref><ref type="bibr" target="#b21">Pichotta and Mooney, 2015)</ref>. <ref type="bibr" target="#b16">Louis and Nenkova (2012)</ref> use manually provided syntactic patterns for sentence representa- tion, and model transitions in text as Markov prob- abilities, which is related to our work. However, while they use simple HMMs over discrete topics, our method allows for a richer model that also cap- tures smooth transition across them. Approaches such as <ref type="bibr" target="#b10">Kalchbrenner and Blunsom (2013)</ref>; ; <ref type="bibr" target="#b13">Li and Jurafsky (2017)</ref> model text through recurrent neural architectures, but are hard to in- terpret and visualize. Other approaches have ex- plored applications related to modeling narrative discourse in context of limited tasks such as story cloze <ref type="bibr" target="#b19">(Mostafazadeh et al., 2016</ref>) and identifying similar narratives ( <ref type="bibr" target="#b6">Chaturvedi et al., 2018)</ref>.</p><p>From a large scale document-mining perspective, the question of extracting intra-document structure remains largely underexplored. While early mod- els such as LDA completely ignore ordering and discourse elements of a documents, other methods that use distributed embeddings of documents are opaque ( <ref type="bibr" target="#b11">Le and Mikolov, 2014</ref>), even while they can in principle model sequential structure within a document. Methods such as HMM-LDA ( <ref type="bibr" target="#b7">Griffiths et al., 2005</ref>) and Topics-over-time ( <ref type="bibr" target="#b25">Wang and McCallum, 2006</ref>) address the related question of topic evolution in a stream of documents, but these approaches are too coarse to model intra-document sequential structure. In terms of our technical ap- proach, we build on previous research on grid- based models ( <ref type="bibr" target="#b9">Jojic and Perina, 2011)</ref>, which have previously been used for topic-modeling for images and text as unstructured bags-of-features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Sequential CG model</head><p>In this section, we present our method, which we call Sequential Counting Grids, or Sequential CG. We first present our notation, model formu- lation and training approach. We discuss how the method is designed to incorporate smoothness and sequential structure, and how the method can be efficiently scaled to train on large document collec- tions. In Section 3.2, we present a mixture model variant that combines Sequential CG with a uni- gram language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model description</head><p>We represent a document as a sequence s of sen- tences, s = {s 1 , s 2 . . . s D }, where D represents the number of sentences in the document. In gen- eral, we assume each sentence is represented as a multiset of features s i = {c z } i , where c i z repre- sents the count of the feature indexed by z in the ith sentence in the sequence. <ref type="bibr">1</ref> The Sequential CG consists of a 3-D grid G of size E x × E y × E z , where E x , E y and E z denote the extent of the grid along the X, Y and Z-axes (see <ref type="figure">Figure 1</ref>). Let us denote an index of a position in the grid by an integer-valued vector i = (i x i y i z ). The three components of the index together spec- ify a XY location as well as a depth in the grid. The Sequential CG model is parametrized by two sets of parameters, π i,z and P ij . Here, π i,z repre- sents a multinomial distribution over the vocabu- lary of features z for each cell in the grid G, i.e. z π i,z = 1 ∀ i ∈ G. To induce smoothness across XY planes, we further define histogram dis- tributions h i,z , which average the π distributions in a 2-D neighborhood W i (of size specified by W = [W x , W y ]) around the grid position i. This notation follows <ref type="bibr" target="#b9">Jojic and Perina (2011)</ref>.</p><formula xml:id="formula_0">h i,z = 1 W x W y i ∈W i π i ,z<label>(1)</label></formula><p>The generative model assumes that individual sen- tences in a document are generated by h distribu- tions in the grid. Movements from one position i to another j in the grid are modeled as transition probabilities P ij . The generative process consists of the following. We uniformly sample a starting location i 1 in the grid. We sample words in the first sentence s 1 from π i1 , and sample the next posi- tion i 2 from the distribution P i 1 ,: , and so on till we generate s D . The alignments</p><formula xml:id="formula_1">I = [i 1 , i 2 . . . i D ]</formula><p>of individual sentences in a document with positions in the grid are latent variables in our model. Given the sequence of alignments I for a doc- ument, the conditional likelihood of generating s is given as a product of generating individual sen- tences:</p><formula xml:id="formula_2">p(s| I) = D d p({c d z }| i d ) = D d=1 z (h i d ,z ) c d z (2)</formula><p>Since the alignments of sequences to their posi- tions in the grids I are latent, we marginalize over these to maximize the likelihood of an observed col- lection of documents S := {s t } T t=1 . Here, T is the total number of documents, and t is an index over individual documents. Using Jensen's inequality, any distributions q t I over the hidden alignments I t provide lower-bounds on the data log-likelihood.</p><formula xml:id="formula_3">t log p(s t |π) = t log I p(s t , I|π) = t log I q t I p(s t |I)p(I)) q t I ≥ − t I q t I log q t I + t I q t I log p(s|I, π)p(I))<label>(3)</label></formula><p>Here, q t I denotes a variational distribution for each of the data sequences s t . The learning algorithm consists of an iterative generalized EM procedure (which can be interpreted as a block-coordinate ascent in the latent variables q t I and the model pa- rameters π and P). We maximize the lower bound in Eqn 3 exactly by setting q t I to the posterior dis- tribution of the data for the current values of the parameters π (standard E step). Thus, we have</p><formula xml:id="formula_4">q t I ∝ p(s|I)p(I) = D d=1 z (h i d ,z ) c d z (t) D d=2 P i d−1 ,i d<label>(4)</label></formula><p>We do not need to explicitly compute the poste- rior distribution q t I = p(I|s) at this point, but only use it to compute the relevant expectation statistics in the M-step. This can be done efficiently, as we see next. In the M-step, we consider q t I as fixed, and maximize the objective in terms of the model parameters π. Substituting this in Eqn 3, and focus- ing on terms that depend on the model parameters (π and P), we get</p><formula xml:id="formula_5">L(π, P) ≥ t I q t I log p(s|I, π)p(I)) + H q = t I q t I d z c d z (t) log h i d ,z + d log P i d−1 ,i d = t I E q t I d z I i t d =i c d z (t) log h i d ,z + t I E q t I d I i t d−1 =i,i t d =j log P ij<label>(5)</label></formula><p>Maximizing the likelihood w.r.t. P leads to the following updates for the transition probabilities: 2</p><formula xml:id="formula_6">P ij = t d P (i t d−1 = i, i t d = j) t d P (i t d−1 = i)<label>(6)</label></formula><p>Here, the pairwise state-probabilities</p><formula xml:id="formula_7">P (i t d−1 = i, i t d = j)</formula><p>for adjacent sentences in a sequence can be efficiently calculated using the Forward- Backward algorithm. In Equation 5, rewriting the term containing h in terms of π using Eqn 1 (and ignoring constant terms W x W y ), we get:</p><formula xml:id="formula_8">t I E q t I d z I i t d =i c d z (t) log i ∈W i π i ,z = t I d P (i t d = i) z c d z (t) log i ∈W i π i ,z<label>(7)</label></formula><p>The presence of a summation inside of a loga- rithm makes maximizing this objective for π harder. For this, we simply use Jensen's inequality intro- ducing an additional variational distribution (for the latent grid positions within window W i ), and maximize the lower bound. The final M-step up- date for π becomes:</p><formula xml:id="formula_9">π i,z ∝ t d c d z (t) k|i∈W k P (i t d = k) h k,z π i,z<label>(8)</label></formula><p>2 Since the optimal value for the concave problem j yj log xj s.t. j xj = 1 occurs when x * j ∝ yj As before, the state-probabilities P (i t d = i) can be computed using the Forward Backward algorithm.</p><p>Intuitively, the expected alignments in the E-step are distributions over sequences of positions in the grid that best explain the structure of documents for the current value of Sequential CG parameters. In the M-step, we assume these distributions embedding documents into various parts of the grid as given, and update the multi- nomial parameters and transition probabilities. Modeling the transitions as having a Markov property allows us to use a dynamic programming approach (Forward Backward algorithm) to exactly compute the posterior probabilities required for parameter updates. We note that at the onset of the procedure, we need to initialize π randomly to break symmetries. Unless otherwise stated, in all experiments, we run EM to 200 iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Correlating space with sequential structure:</head><p>The use of histogram distributions h to generate data forces smoothness in the model along XY planes due to adjacent cells in the grid sharing a large number of parameters that contribute to their histograms (due to overlapping windows). On the other hand, in order to induce spatial proximity in the grid to mimic the sequential flow of discourse in documents, we constrain the transition matrix P (which specifies transition preferences from one position in the grid to another) to a sparse banded matrix. In particular, a position i = (i x , i y , i z ) in the grid can only transition to itself, its 4 neighbors in the same XY plane, and two cells in the suc- ceeding two layers along the Z-axis ( (i x , i y , i z+1 ) and (i x , i y , i z+2 )). This is enforced by fixing other elements in the transition matrix to 0, and only updating allowable transitions.</p><p>As an important note about implementation de- tails, we observe here that the Forward-Backward procedure (which is repeatedly invoked during model training) can be naturally formulated in terms of matrix operations. <ref type="bibr">3</ref> This allows training for the Sequential CG approach to be scalable for large document collections.</p><p>In our formulation, we have presented a Sequen- tial CG model for a 3-D grid. This can be adapted to learn 2-D grids (trellis) by setting E y = 1. In our experiments, we found 3-D grids to be better in terms of task performance and visualization (for a comparable number of parameters).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Mixture model</head><p>The Sequential CG model described above can be combined with other generative models (e.g., lan- guage models) to get a mixture model. Here, we show how a unigram language model can be com- bined with Sequential CG. The rationale behind this is that since the Sequential CG is primarily designed to explain elements of documents that re- flect sequential discourse structures, mixing with a context-agnostic distribution can allow it to fo- cus specifically on elements that reflect sequential regularities. In experimental evaluation, we find that such a mixture model shows distinctly differ- ent behavior (see Section 4.1.1). Next, we briefly describe updates for this approach.</p><p>Let µ z denote the multinomial distribution over features for the unigram model to be mixed with the CG. Let β z be the mixing proportion for the feature z, i.e. an occurrence of z is presumed to come from the Sequential CG with probability β z , and from the unigram distribution with probability 1 − β z . Further, let α t z be binary variable that denotes whether a particular instance of z comes from the Sequential CG, or the unigram model. Then, Equation 2 changes to:</p><formula xml:id="formula_10">p(s| I, α) = z,d (h i d ,z ) c d z β z α t z µ c d z z (1−β z ) 1−α t z</formula><p>Since we do not observe α t z (i.e., which distribu- tion generated a particular feature in a particular document), they are additional latent variables in the model. Thus, we need to introduce a Bernoulli variational distribution q αzt . Doing this modifies relevant parts (containing q αzt ) of Equation 5 to:</p><formula xml:id="formula_11">t I q t I z q αzt log β z d h c d z (t) i d ,z + (1 − q αzt ) log 1 − β z )µ d c d z z + d log P i d−1 ,i d + H qα zt<label>(9)</label></formula><p>This leads to the following additional updates for estimating q αzt (in the E-step) <ref type="bibr">4</ref> and β z (in the M- step). <ref type="bibr">4</ref> Since the optimal value for the concave problem j xj log</p><formula xml:id="formula_12">y j x j s.t. j xj = 1 occurs when x * j ∝ yj q αzt = exp I i P (i t d =i)c d z (t) log h i d ,z βz exp I i P (i t d =i)c d z (t) log h i d ,z βz+µ d c d z z (1−βz)</formula><p>In the M-step, β z can be estimated simply as the fraction of times z is generated from the Sequential CG component.</p><formula xml:id="formula_13">β z = t qα zt t Iz</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>In this section, we analyze the performance of our approach on text collections from several do- mains (including short stories, newswire text and biographies). We first qualitatively evaluate our generative method on a dataset of biographical ex- tracts from Wikipedia, which visually illustrates biographical trajectories learned by the model, op- erationalizing our model concept from <ref type="figure">Figure 1</ref> in real data (see <ref type="figure" target="#fig_0">Figure 2</ref>). Next, we evaluate our method on two standard tasks requiring document understanding: story cloze evaluation and sentence ordering. Since our method is completely unsu- pervised and is not tailored to specific tasks, com- petitive performance on these tasks would indicate that the method learns helpful regularities in text structure, useful for general-purpose language un- derstanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Visualizing Wikipedia biographies</head><p>We now qualitatively explore models learned by our method on a dataset of biographies from Wikipedia. <ref type="bibr">5</ref> For this, we use the data previously collected and processed by <ref type="bibr" target="#b1">Bamman and Smith (2014)</ref>. In all, the original dataset consists of ex- tracts from biographies of about 240,000 individu- als. For ease of training, we trained our method on a subset of the 50,0000 shortest documents from this set. The original paper uses the numerical or- der of dates mentioned in the biographies to extract biographical templates, but we do not use this infor- mation. <ref type="figure" target="#fig_0">Figure 2</ref> visualizes a Sequential CG model learned on this dataset for on a grid of dimensions E = 8 × 8 × 5, and a histogram window W of dimensions 3 × 3 . In general, we found that using larger grids leads to smoother transitions and learn- ing more intricate patterns including hierarchies of trajectories, but here we show a model with a smaller grid for ease of visualization. Here, the words in each cell in the grid denote the highest probability assignments in that cell. Larger fonts within a cell indicate higher probabilities. We observe that the method successfully extracts various biographical trajectories, as well as capture a notion of similarity between them. To explain, we observe that the lower-right part of the learned grid largely models documents about sportpersons (with discernable regions focusing on sports like soccer, American football and ice-hockey). On the other hand, the left-half of the grid is domi- nated by biographies of people from the arts and humanities (inlcuding artists, writers, musicians, etc.). The top-center of the grid focuses on aca- demicians and scientists, while the top-right repre- sents biographies of political and military leaders. We note smooth transitions between different re- gions, which is precisely what we would expect from the use of the smoothing filter that incorpo- rates parameter sharing across cells in the method. Further, as we go across the layers in the figure, we note the biographical trajectories learned by the model across the entire grid. For example, from the grid, the life trajectory of a football player can be visualized as being drafted, signing and playing for a team, and eventually becoming a head-coach or a hall-of-fame inductee.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Effects of mixing</head><p>The Sequential-CG method can be combined with other generative models in a mixture model, fol- lowing the approach previously described in Sec- tion 3.2. A major reason to do this might be to allow the base model to handle general content, while allowing the Sequential-CG method to focus on modeling context-sensitive words only. Here, we empirically characterize the mixing behavior for different categories of words. <ref type="figure" target="#fig_1">Figure 3</ref> shows the mixing proportion of differ- ent words when the Sequential-CG model is com- bined with a unigram model. In the figure, the X-axis corresponds to words in the dataset with decreasing frequency of occurrence, whereas the Y- axis denotes the mixing proportions β z learned by the mixture model. We note that the mixture model learns to explain frequent as well as the long-tail of rare words using the simple unigram model (as seen from low mixing proportion of Sequential-CG method). These regimes correspond to (1) stop- words and very common nouns, and (2) rare words respectively. In turn, this allows the Sequential- CG component to preserve more probability mass to explain the intermediate content words. Thus, the Sequential-CG component only needs to model words that reflect useful statistical sequential pat- terns, without expending modeling effort on back- ground content (common words) or noise (rare words). For the long tail of infrequent words, we observe that Sequential CG is much more likely to generate verbs and adjectives, rather than nouns. This is as we would expect, since verbs and adjec- tives often denote events and sentiments, which can be important elements in discourse trajectories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Story-cloze</head><p>We next evaluate our method on the story-cloze task presented by <ref type="bibr" target="#b19">Mostafazadeh et al. (2016)</ref>, which tests common-sense understanding in context of children stories. The task consists of identifying the correct ending to a four-sentence long story (called context in the original paper) and two possi- ble ending options. The dataset for the task consists of a collection of around 45K unlabeled 5-sentence long stories as well as 3742 5-sentence stories with two provided endings, with one labeled as the cor- rect ending. For this task, we train our method on grids of dimension 15 × 15 × 6 (E), and histogram windows W of size 5 × 5 on the unlabeled collec- tion of stories. At test time, for each story, we are provided two versions (a story-version v consists of the provided context c, followed by a possible ending e 1 , i.e. v = <ref type="bibr">[c, e]</ref> ). For prediction, we need to define a goodness score S v for a story-version.</p><p>In the simplest case, this score can simply be the log-likelihood log p SCG (v) of the story-version, ac- cording to the Sequential-CG model. However, this is problematic since this is biased towards choos- ing shorter endings. To alleviate this, we define the goodness score by discounting the log-likelihood by the probability of the ending e itself, under a Accuracy Our Method variants Sequential CG + Unigram Mixture 0.602 Sequential CG + Brown clustering 0.593 Sequential CG + Sentiment 0.581 Sequential CG 0.589 Sequential CG (unnormalized) 0.531 DSSM 0.585 GenSim 0.539 Skip-thoughts 0.552 Narrative-Chain(Stories) 0.494 N-grams 0.494 <ref type="table">Table 1</ref>: Performance of our approach on story- cloze task from Mostafazadeh et al. <ref type="formula" target="#formula_0">(2016)</ref> com- pared with other unsupervised approaches (accu- racy numbers as reported in <ref type="bibr" target="#b19">Mostafazadeh et al. (2016)</ref>).</p><p>simple unigram model.</p><formula xml:id="formula_14">S v = log p SCG (c, e) − log p uni (e)</formula><p>The predicted ending is the story-version with a higher score. <ref type="table">Table 1</ref> shows the performance of variants of our approach for the task. Our base- lines include previous approaches for the same task: DSSM is a deep-learning based approach, which maps the context and ending to the same space, and is the best-performing method in <ref type="bibr" target="#b19">Mostafazadeh et al. (2016)</ref>. GenSim and N-gram return the end- ing that is more similar to the context based on word2vec embeddings ( <ref type="bibr" target="#b18">Mikolov et al., 2013)</ref> and n-grams, respectively. Narrative-Chains computes the probability of each alternative based on event- chains, following the approach of <ref type="bibr" target="#b4">Chambers and Jurafsky (2008)</ref>.</p><p>We note that our method improves on the pre- vious best unsupervised methods for the task. This is quite surprising, since our Sequential-CG model in this case is trained on bag-of-lemma representations, and only needs sentence segmen- tation, tokenization and lemmatization for pre- processing. On the other hand, approaches such as Narrative-Chains require parsing and event- recognition, while approaches such as GenSim re- quire learning word embeddings on large text cor- pora for training. Further, we note that predicting the ending without normalizing for the probability of the words in the ending results in significantly weaker performance, as expected. We train another variant of Sequential-CG with the sentence-level sentiment annotation (from Stanford CoreNLP) also added as a feature. This does not improve per- formance, consistent with findings in <ref type="bibr" target="#b19">Mostafazadeh et al. (2016)</ref>. We also experiment with a variant where we perform Brown clustering <ref type="bibr" target="#b3">(Brown et al., 1992)</ref> of words in the unlabeled stories (K = 500 clusters), and include cluster-annotations as fea- tures for training the method. Doing this explicitly incorporates lexical similarity into the model, lead- ing to a small improvement in performance. Finally, a mixture model consisting of the Sequential-CG and a unigram language model leads to a further improvement in performance. The performance of our unsupervised approach on this task indicates that it can learn discourse structures that are helpful for general language understanding. The story-cloze task has recently also been ad- dressed as a shared task at EACL ( <ref type="bibr" target="#b20">Mostafazadeh et al., 2017</ref>) with a significantly expanded dataset, and achieving much higher performance. How- ever, we note that the proposed best-performing ap- proaches ( <ref type="bibr" target="#b5">Chaturvedi et al., 2017;</ref><ref type="bibr" target="#b23">Schwartz et al., 2017)</ref> for this task are all supervised, and hence not included here for comparison. <ref type="figure" target="#fig_2">Figure 4</ref> shows examples where the model cor- rectly identifies the ending. These show a mix of behavior such as sentiment coherence (iden- tifying dissonance between 'wonderful surprise' and 'stolen') and modeling causation (police being called after being suspected).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Sentence Ordering</head><p>We next evaluate our method on the sentence order- ing task, which requires distinguishing an original  <ref type="formula" target="#formula_0">(2017)</ref> 0.930 0.992 <ref type="bibr">Recursive (2014)</ref> 0.864 0.976 Entity- <ref type="bibr">Grid (2008)</ref> 0.904 0.872 <ref type="bibr">Graph (2013)</ref> 0.846 0.635 <ref type="table">Table 2</ref>: Performance of our approach on sentence ordering dataset from <ref type="bibr" target="#b2">Barzilay and Lapata (2008)</ref>. document from a version consisting of permuta- tions of sentences of the original ( <ref type="bibr" target="#b2">Barzilay and Lapata, 2008;</ref><ref type="bibr" target="#b16">Louis and Nenkova, 2012)</ref>. For this, we use two datasets of documents and their per- mutations from <ref type="bibr" target="#b2">Barzilay and Lapata (2008)</ref>, which are used as standard evaluation for coherence pre- diction tasks. These consist of (i) reports of ac- cidents from the National Transportation Safety Bureau (we refer to this data as accidents), and (ii) newswire reports about earthquake events from the Associated press (we refer to this as earthquakes). Each dataset consists of 100 training documents, and about 100 documents for testing. Also pro- vided are about 20 generated permutations for each document (resulting in 1986 test pairs for accidents, and 1955 test pairs for earthquakes). Documents in accidents consist of between 6 and 19 sentences each, with a median of 11 sentences. Documents in earthquakes consist of between 4 and 30 sentences each, with a median of 10 sentences. Since the datasets for these tasks only have a relatively small number of training documents (100 each), we use Sequential-CG with smaller grids (3×3×15), and don't train a mixture model (which needs to learn a parameter β z for each word in the vocabulary). Further, we train for a much smaller number of iterations to prevent overfitting (K = 3, chosen through cross-validation on the training set). During testing, since provided article pairs are sim- ply permutations of each other and identical in content, we do not need to normalize as needed in Section 4.2. The score of a provided article is sim- ply calculated as its log-likelihood. The article with higher likelihood is predicted to be the original. <ref type="table">Table 2</ref> shows performance of the method com- pared with other approaches for coherence predic- tion. We note that Sequential-CG performs com- petitively with the state-of-the-art for generative approaches for the task, while needing no other annotation. In comparison, the HMM based ap- proaches use significant annotation and syntactic features. Sequential-CG also outperforms several discriminative approaches for the task. In <ref type="figure" target="#fig_3">Figure 5</ref> we illustrate the learned discourse trajectories in terms of the most salient features in each sentence. Words in bold are those identified by the model to be most context-appropriate at the correspond- ing point in the narrative. This is done by ranking words by the ratio between their probabilities (π :,z ) in the grid weighted by alignment locations of the document (q t I ), and unigram probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented a simple model for extracting and visualizing latent discourse structure from un- labeled documents. The approach is coarse, and does not have explicit models for important ele- ments such as entities and events in a discourse. However, the method outperforms some previous approaches on document understanding tasks, even while ignoring syntactic structure within sentences. The ability to visualize learning is a key component of our method, which can find significant applica- tions in data mining and data-discovery in large text collections. More generally, similar approaches can explore a wider range of scenarios involving sequences of text. While here our focus was on learning discourse structures at the document level, similar methods can also be used at other scales, such as for syntactic or morphological analysis.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Visualization of a Sequential-CG model with grid size of 8 × 8 × 5, trained on 50,000 documents from the Wikipedia biographies dataset. Cells in the grid show words with highest probabilities (empty cells may indicate that no word has a substantially higher probability than others).</figDesc><graphic url="image-2.png" coords="6,72.00,62.81,467.16,202.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Learned mixing proportion (β z ) in the mixture model of Section 3.2 for words of different frequencies. β z denotes the probability of a word being generated from the Sequential CG model (rather than from the Unigram model). The Sequential CG learns to model content words (with intermediate ranks), and conserves modeling effort by avoiding modeling both very common words (that occur across contexts), as well as rare words.</figDesc><graphic url="image-3.png" coords="7,72.28,62.81,217.69,172.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Illustrative story-cloze examples where the model correctly identifies the appropriate ending (model score in parentheses).</figDesc><graphic url="image-4.png" coords="8,72.00,62.81,226.78,154.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Example of newswire report about an earthquake event. Bold fonts represent words that align particularly well with the learned model at corresponding points in the narrative.</figDesc><graphic url="image-5.png" coords="9,79.09,62.81,204.09,124.20" type="bitmap" /></figure>

			<note place="foot" n="1"> These may simply consist of tokens (words, entities and MWEs) in the sentence, but can include additional information, such as sentiment or event annotations, or other discrete sentence-level representations</note>

			<note place="foot" n="3"> To explain, if f d 1×G are forward probabilities for step d, and O d+1 G×G are observation probabilities for step d + 1, f d+1 = f d × P × O d computes forward probabilities for the next step in the sequence</note>

			<note place="foot" n="5"> For all our experimental evaluation, we tokenize and lemmatize text using the Stanford CoreNLP pipeline, but retain entity-names and contiguous text-spans representing MWEs as single units</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Belief in discourse representation theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Asher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Philosophical Logic</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="127" to="189" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised discovery of biographical structure from text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bamman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="363" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Modeling local coherence: An entity-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Classbased n-gram models of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter F Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent J Della</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenifer C</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="479" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised learning of narrative event chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Association for Computer Linguistics</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="789" to="797" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Story comprehension for predicting what happens next</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Snigdha</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoruo</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1603" to="1614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Where have I heard this story before?&apos; : Identifying narrative similarity in movie remakes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Snigdha</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashank</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Integrating topics and syntax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Thomas L Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="537" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Graphbased local coherence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Guinaudeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="93" to="103" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multidimensional counting grids: Inferring word order from disordered bags of words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nebojsa</forename><surname>Jojic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Perina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="547" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Recurrent convolutional neural networks for discourse compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">119</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th International Conference on Machine Learning</title>
		<meeting>the 31th International Conference on Machine Learning<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A model of coherence based on distributed sentence representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2039" to="2048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural net models of open-domain discourse coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="198" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recursive deep models for discourse parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rumeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2061" to="2069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatically evaluating text coherence using discourse relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="997" to="1006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A coherence model based on syntactic patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1157" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Rhetorical structure theory: Toward a functional theory of text organization. Text-Interdisciplinary Journal for the Study of Discourse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><forename type="middle">A</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thompson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="243" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A corpus and cloze evaluation for deeper understanding of commonsense stories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACLHLT</title>
		<meeting>NAACLHLT</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="839" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">LSDSem 2017 shared task: The story cloze test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics</title>
		<meeting>the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="46" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning statistical scripts with LSTM recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Pichotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Scripts, plans, goals and understanding: an inquiry into human knowledge structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Roger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robert P Abelson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977" />
			<publisher>Erlbaum</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The effect of different writing tasks on linguistic style: A case study of the roc story cloze task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leila</forename><surname>Zilles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01841</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Steering output style and topic in neural response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nebojsa</forename><surname>Jojic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2140" to="2150" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Topics over time: A non-markov continuous-time model of topical trends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuerui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;06</title>
		<meeting>the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;06<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="424" to="433" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
