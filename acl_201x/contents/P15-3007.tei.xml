<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Map Dependency Parses to Abstract Meaning Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-07-28">July 28, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Te</forename><surname>Chen</surname></persName>
							<email>Weite.Chen@colorado.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Colorado at Boulder</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Map Dependency Parses to Abstract Meaning Representations</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the ACL-IJCNLP 2015 Student Research Workshop</title>
						<meeting>the ACL-IJCNLP 2015 Student Research Workshop <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="41" to="46"/>
							<date type="published" when="2015-07-28">July 28, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Meaning Representation (AMR) is a semantic representation language used to capture the meaning of English sentences. In this work, we propose an AMR parser based on dependency parse rewrite rules. This approach transfers dependency parses into AMRs by integrating the syntactic dependencies, semantic arguments, named entity and co-reference information. A dependency parse to AMR graph aligner is also introduced as a preliminary step for designing the parser.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Abstract Meaning Representation (AMR) ( <ref type="bibr" target="#b0">Banarescu et al., 2013</ref>) is a semantic formalism that expresses the logical meanings of English sen- tences in the form of a directed, acyclic graph. AMR focuses on the semantic concepts (nodes on the graph), and relations (labeled edges on the graph) between those concepts. AMR relies heav- ily on predicate-argument structures defined in the PropBank (PB) <ref type="bibr" target="#b5">(Palmer et al., 2005</ref>). The repre- sentation encodes rich information, including se- mantic roles, named entities, and co-reference in- formation. <ref type="figure" target="#fig_0">Fig. 1</ref> shows an example AMR.</p><p>In this proposal, we focus on the design of an automatic AMR parser in a supervised fashion from dependency parses. In contrast with recent semantic parsing algorithms, we start the parsing process from the dependency parses rather than the sentences. A dependency parse provides both the semantic dependency information for the sen- tence, and the structure of the relations between the head word and their dependencies. These can provide strong features for semantic parsing. By using a binary-branching bottom-up shift-reduced algorithm, the statistical model for the rewrite rules can be learned discriminatively. Although the AMR parser is my thesis topic, in this proposal we will pay more attention to preliminary work - the AMR -Dependency Parse aligner.</p><p>To extract the rewrite rules and the statistical model, we need the links between AMR con- cepts and the word nodes within the dependency parse. An example alignment is shown in <ref type="figure">Fig.  2</ref>. Alignment between an AMR concept and de- pendency node is needed because 1) it represents the meaning of the sub-graph of the concept and its child concepts corresponding to the phrase of the head word node, and 2) the dependency node contains sufficient information for the extraction of rewrite rules. For example, the word node "Vinken" on the dependency parse side in <ref type="figure">Fig. 2</ref> links to the lexical concept "Vinken" and, further- more, links to the "p2/name" and the "p/person" concepts since "Vinken" is the head of the named entity (NE) "Pierre Vinken" and the head of the noun phrase "Pierre Vinken, 61 years old." The secondary aim of this proposal is to design an alignment model between AMR concepts and de- pendency parses. We use EM to search the hidden derivations by combining the features of lexical form, relation label, NE, semantic role, etc. Af- ter EM processing, both the alignments and all the feature probabilities can be estimated.</p><p>The design of a rewrite-based AMR parser is described in Sec. 2, and the aligner is in Sec. 3. Our preliminary experiments and results are pre- <ref type="figure">Figure 2</ref>: The alignment between an AMR sub- graph and a dependency parse. A red line links the corresponding concept and dependency node. sented in Sec. 4, followed by future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Rewrite Based AMR Parser</head><p>AMR is a rooted, directed, acyclic graph. For ex- ample, the concept join-01 in <ref type="figure" target="#fig_0">Fig. 1</ref> is the root meaning of the sentence, which links to the child concepts Arg0, Arg1, and time. AMR adheres to the following principles ( <ref type="bibr" target="#b0">Banarescu et al., 2013</ref>):</p><p>• AMRs are rooted acyclic graphs with labels (re- lations) on edges. These labels indicate the di- rected relation between two concepts.</p><p>• AMRs abstract away from syntactic idiosyncra- cies of a language, and instead attempt to capture only the core meaning of a sentence.</p><p>• AMRs use the PB framesets as relation la- bels ( <ref type="bibr" target="#b5">Palmer et al., 2005</ref>). For example, the rela- tion labels (i.e., ARG0, ARG1) of "join-01" con- cept in <ref type="figure" target="#fig_0">Fig. 1</ref> correspond to the roles of the PB frame "join-v." • AMRs combine multiple layers of linguistic an- notation, like coreference, NE, semantic role, etc., in a single structure.</p><p>The above basic characteristics make the parsing of AMRs a difficult task. First, because AMR abstracts away from syntactic idiosyncrasies, we need a model to link the AMR concepts to words in the original sentence, in order to obtain exter- nal lexical, syntactic and semantic features. Sec- ondly, the parser should learn the different feature transformation probabilities jointly since AMRs combine several linguistic annotations. Moreover, <ref type="table">Table 1</ref>: Sample Rewrite Rules AMR uses graph variables and reentrancy to ex- press coreference (e.g., "p" variable in <ref type="figure" target="#fig_0">Fig 1 ap</ref>- pears twice -in :ARG0 of join-01 and :ARG0 of have-org-role-91). The reentrancy prevents the AMR graph from begin a tree structure. Dur- ing parsing decoding, a polynomial time algorithm should be replaced by alternative algorithms, like beam search, to avoid an exponential running time. JAMR ( <ref type="bibr" target="#b3">Flanigan et al., 2014</ref>) is the first sys- tem for AMR parsing, which identifies the con- cepts, and then searches for a maximum span- ning connected subgraph (MSCG) on a fully con- nected graph to identify the relations between con- cepts. The search algorithm is similar to the maxi- mum spanning tree algorithms. To assure the fi- nal connected graph conforms to linguistic con- straints, JAMR uses Lagrangian relaxation <ref type="bibr" target="#b4">(Geoffrion, 2010</ref>) to supplement the MSCG algorithm. JAMR reaches a 58% Smatch score <ref type="bibr" target="#b2">(Cai and Knight, 2013</ref>) on automatic concept and relation identification data, and 80% on gold concept and automatic relation identification data.</p><formula xml:id="formula_0">(x)/NP → :op(x) -r1 (x) nn (y) → :name (name(x, y)) -r2 (x)/CD → :quant(x) -r3 (x)/NNS → :unit(x) -r4 npadvmod (x)(y) → temporal-quanity(x, y) -r5 old/JJ (x) → :age(x) -r6 NE PERSON(x)(y) → person(x, y) -r7</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Our Shift-Reduce Rewrite Rule Parser</head><p>Rewrite rule based parser is a bottom-up converter from dependency parses to AMRs. The process starts from the leaf word node on the dependency parse. By applying rewrite-rules to each word node, we obtain and assemble the sub-graphs of our target AMR. Sample rewrite rules are listed in <ref type="table">Table 1</ref>. In these rules, the left hand side contains the dependency information (e.g. word lemma, POS, relation label, NE tag, etc). The right hand side is the AMR concept and its template for filling variables from previous parsing steps. The sample derivation steps are listed in <ref type="table">Table 2</ref>. For every step, it shows the derivation rule applied (in <ref type="table">Table  1</ref>), and the concept name, c1-c8.</p><p>This approach to parsing could be implemented with a shift-reduce algorithm ( <ref type="bibr" target="#b7">Wang et al., 2015</ref>). We define a stack and a list of tokens, which stores the dependency words in the order of tree traver- sal. Several actions are defined to operate on the list(L) and the stack(S):</p><formula xml:id="formula_1">Derivation Apply Rule Concept Name Pierre/NNP → :op Pierre r1 c1 Vinken/NNP → :op Vinken r1 c2 (c1) nn (c2) → :name (name :op1 Pierre :op2 Vinekn) r2 c3</formula><p>61/CD → :quant 61 r3 c4 years/NNS → :unit year r4 c5 npadvmod (c4)(c5) r5 c6 → temporal-quanity :quant 61 :unit year old/JJ (c6) → :age (temporal-quanity :quant 61 :unit year) r6 c7</p><formula xml:id="formula_2">NE PERSON (c3)(c7)</formula><p>→ person :name (name :op1 Pierre :op2 Vinekn) :age (temporal-quanity :quant 61 :unit year) r7 c8</p><p>Table 2: The derivation for parsing "Pierre Vinken, 61 years old" from dep. parse to AMR</p><p>• Shift Remove the dependency word from L, ap- ply the rules, and push the new concept to S.</p><p>• Reduce Move the two top sub-concepts from S, apply the rules, and push it back to S.</p><p>• Unary Move the top sub-concept from S, apply the rules, and push it back to S.</p><p>• Finish If no more dependency words are in the list, and one concept is in S, then return.</p><p>The final AMR concept would be stored at the top of the stack. It is guaranteed that all the AMR expressions can be derived from the dependency parses by using the shift-reduce algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dependency Parses to AMR Aligner</head><p>A preliminary step for our rewrite-based parser is the alignment between the AMR and the depen- dency parse. JAMR ( <ref type="bibr" target="#b3">Flanigan et al., 2014</ref>) pro- vides a heuristic aligner between an AMR concept and the word or phrase of a sentence. They use a set of aligner rules, like NE, fuzzy NE, data en- tity, etc., with a greedy strategy to match the align- ments. This aligner achieves a 90% F 1 score on hand aligned AMR-sentence pairs. On the other hand, <ref type="bibr" target="#b6">Pourdamghani et al. (2014)</ref> present a gen- erative model to align from AMR graphs to sen- tence strings. They raises concerns about the lack of sufficient data for learning derivation rules. In- stead, they propose a string-to-string alignment model, which transfers the AMR expression to a linearized string representation. Then they use several IBM word alignment models ( <ref type="bibr" target="#b1">Brown et al., 1993</ref>) on this task. IBM Model-4 with a sym- metric method reaches the highest F 1 score of 83.1%. Separately analyzing the alignments of roles and non-roles (lexical leaf on AMR), the F 1 scores are 49.3% and 89.8%, respectively. In comparison to previous work, our aligner es- timates the alignments by learning the transforma- tion probability of lexical form, relations, named entities and semantic roles features jointly. Both the alignment and transformation probabilities are initialized for the training of parser.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Our Aligner Model with EM Algorithm</head><p>Our approach, based on the existing IBM Model ( <ref type="bibr" target="#b1">Brown et al., 1993)</ref>, is an AMR-to-Dependency parse aligner, which represents one AMR as a list of Concepts C = c 1 , c 2 , . . . , c |C| , and the cor- responding dependency parse as a list of depen- dency word nodes</p><formula xml:id="formula_3">D = d 1 , d 2 , . . . , d |D| .</formula><p>The alignment A is a set of mapping functions a, which link Concept c j to dependency word node d i , a : c j → d i . Our model adopts an asymmetric EM approach, instead of the standard symmetric one. We can always find the dependency label path be- tween any pair of dependency word nodes. How- ever, the number of concept relation label paths is not deterministic. Thus, we select the alignment direction of AMR to dependency parse only, and one-to-one mapping, in our model.</p><p>The objective function is to learn the parameter θ in the AMR-to-Dependency Parse of EM:</p><formula xml:id="formula_4">θ = argmax L θ (AM R|DEP ) L θ (AM R|DEP ) = |S| k=1 A P (C (k) , A|D (k) ; t, q)</formula><p>where L θ is the likelihood that we would like to maximize, S is the training data set. We will explain the transformation probability t and the alignment probability q below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Expectation-Step</head><p>The E-Step estimates the likelihood of the input AMR and dependency parse by giving the trans- formation probability t and alignment probability q. The likelihood can be calculated using:</p><formula xml:id="formula_5">P (A|C, D) = |C| j=1 P (c j |a(c j )) P (c j |d i , |C|, |D|) = t(c j |d i ) * q(d i |c j , |C|, |D|)</formula><p>We would like to calculate all the probabilities of possible alignments A between c j and d i . The transformation probability t is a combination (multiple) probability of several different features:</p><p>• P lemma (c j |d i ): the lemma probability is the probability of the concept name of c j , condi- tioned on the dependency word of d i .</p><p>• P rel (Label(c j , c p j )|RelP ath dep (a(c j ), a(c p j ))): the relation probability is the probability of the relation label between c i and its parent concept c p i , given the relation path between the depen- dency word nodes a(c i ) and a(c p i ). e.g., the relation probability of c j = 61 and a(c j ) = 61 in <ref type="figure">Fig. 2</ref> is P (quant|npadvmod ↓ num ↓).</p><p>• P N E (N ame(c j )|T ype N E (a(c j ))):</p><p>the NE probability is the probability of the name of c j , given the NE type (e.g., PERSON, DATE, ORG, etc.) contained by a(c j )).</p><p>• P SR (Label(c j , c p j )|P red(a(c p j )), Arg(a(c j ))): the semantic role probability is the probability of relation label between c j and its parent c p j , conditioned on the predicate word of a(c p j ) and argument type of a(c j ) if a(c j ) is semantic argument of predicate a(c p j ). On the other hand, the alignment probability q(Dist(a(c j ), a(c p j ))|c j , |C|, |D|) can be inter- preted as the probability of the distance between a(c j ) and a(c p j ) on dependency parse D, condi- tioned on c j , the lengths of D and C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Maximization-Step</head><p>In the M-Step, the parameter θ r is updated from the previous round of θ r−1 , in order to maximize the likelihood L θ (AM R|DEP ):</p><formula xml:id="formula_6">t(C|D; AM R, DEP ) = (AM R,DEP ) cnt(C|D; AM R, DEP ) C (AM R,DEP ) cnt(C|D; AM R, DEP ) q(D|C; AM R, DEP ) = (AM R,DEP ) cnt(D|C; AM R, DEP ) D (AM R,DEP ) cnt(D|C; AM R, DEP )</formula><p>where cnt is the normalized count that is collected from the accumulating probability of all possible alignment from the E-step. EM iterates the E-step and M-step until convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Initialization</head><p>Before iterating, the transformation probability t and alignment probability q must be initialized. We use these steps to initialize the parameters:</p><p>1. Assign a fixed value, say 0.9, to P lemma (c j |d i )</p><p>if the concept name of c j is identical or a partial match to the dependency word node d i . Other- wise, initialize it uniformly; 2. Run the EM algorithm with the initialized P lemma only (Similar to IBM Model 1, which is only concerned with translation probability);</p><p>3. Initialize all the other parameters, i.e., P rel , P N E , P SR , and q with the development data; 4. Run the completed EM algorithm with the P lemma we obtained from Step 2 and other prob- abilities from Step 3.</p><p>The extra EM for the initialization of P lemma is to estimate a more reasonable P lemma , and to speed up the convergence of the second round of EM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decoding</head><p>To find the alignment of C, D, we define the search for alignments as follows:</p><formula xml:id="formula_7">argmax A P (A|C, D) = argmax A |C| j=1 t(c j |a(c j )) * q(a(c j )|c j , |C|, |D|)</formula><p>This decoding problem finds the alignment A with the maximum likelihood. A dynamic program- ming (DP) algorithm is designed to extract the tar- get alignment without exhaustively searching all candidate alignments, which will take O(|D| |C| ). This DP algorithm starts from the leaf concepts and then walks through parent concepts. In c j , we need to produce the following likelihoods:</p><p>1. Accumulated likelihood for aligning to any d i from all the child concepts of c j 2. Likelihood of P lemma and P N E 3. Likelihood of P rel and P SR for parent concept c p j aligned to any dependency word node d l . In step (3), we need to find the d l , aligned by c p j , that maximizes the likelihood. The accumulated likelihood is then stored in a list with size=|D|. We can trace back and find the most likely alignments in the end. The running time of this algorithm is O(|C||D| 2 ). This algorithm does not include reentrancy cases. One solution to be explored in future work is to use a beam-search algorithm in- stead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Preliminary Experiments and Results</head><p>Here, we describe a preliminary experiment for the AMR-Dependency Parse aligner, including the data description, experimental setup, and results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data</head><p>The LDC AMR release 1.0 consists of 13,051 AMR-English sentence pairs 1 . To match an AMR Split Sent. <ref type="table" target="#tab_1">Tokens  # of  NE  # of  Pred.  # of  Args  Train  1,000 19,923 1,510 4,231 7,739  Dev.  100  2,328  239  235  526  Test  100  1,672  80  199  445   Table 3</ref>: The data split of train/dev./test set. "# of NE", "# of Pred." and "# of Args" stand for the number of named entities, predicate and argument annotations in the data set, respectively.   <ref type="table">Table 3</ref>. We man- ually align the AMR concepts and dependency word nodes in the dev. and test sets. We initial- ize P rel , P N E , and P SR with the dev. set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>We run our first round of EM (Step 2 in Initializa- tion of Sec. 3.1) for 100 iterations, then use the second round (Step 4 in Initialization of Sec. 3.1) for another 100 iterations. We run our decoding al- gorithm and evaluation on the test set after the first and second round of EM. Due to time constraints, we did not train the q here. The experimental results are listed in <ref type="table" target="#tab_1">Table 4</ref>. We evaluate the performance on the precision, re- call, and F 1 score. Using just the P lemma (a simi- lar approach to ( <ref type="bibr" target="#b6">Pourdamghani et al., 2014)</ref>), we achieve 53.4% F 1 score on the test set. On the other hand, our aligner reaches 57.0% F 1 score with the full aligner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this research, we briefly introduce AMR. We describe the design principles and characteristics of AMR, and show how the AMR parser task is important, yet difficult. We present the basic idea for a proposed AMR parser, based on the shift-reduce algorithm. We also present an AMR- Dependency Parse aligner, because such an aligner <ref type="bibr">2</ref>  will be a necessary first step before parsing. The alignment and the estimated feature probabilities are obtained by running the EM algorithm, which could be use directly for the AMR parser.</p><p>In the future, we will be following these steps to develop the proposed rewrite-based parser:</p><p>Implemention of our rewrite-based AMR parser: We would like to implement the proposed rewrite-based AMR parser. In comparison to the parser of Flanigan <ref type="formula">(2014)</ref> , we believe our parser could perform better on the runtime. We also plan to experiment with the data generated by an auto- matic dependency parser.</p><p>Expand the experimental data of aligner: One problem discovered in our preliminary ex- periments was that of data sparsity, especially for P lemma . The LDC AMR Release contains 18,779 AMR/English sentences, and 8,996 of them are contained in the OntoNotes release as well. There- fore, increasing the training data size from the re- lease is one solution to improve the performance of our aligner from the unsatisfactory results. Us- ing external lexical resources, like WordNet, is an- other promissing solution to extend to snyonyms.</p><p>Evaluation of the aligner with existing parser: Since our aligner provides the align- ment between the dependency word node and both the AMR leaf concept and role concept, we as- sume that our aligner could improve not only our rewrite-based parser but other parsers as well. To verify this, we hope to submit our improved align- ment results to a state-of-the-art AMR parser, and evaluate the parsing results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The AMR annotation of sentence "Pierre Vinken, 61 years old, will join the board as a nonexecutive director Nov. 29."</figDesc><graphic url="image-1.png" coords="1,329.95,204.54,181.42,77.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Experiment Results 

with its corresponding dependency parse, we se-
lect the sentences which appear in the OntoNotes 
5.0 release 2 as well, then randomly select 1,000 of 
them as our training set. The OntoNotes data con-
tains TreeBank, PB, and NE annotations. Statis-
tics about the AMR and OntoNotes corpus and the 
train/dev./test splits are given in </table></figure>

			<note place="foot" n="1"> LDC AMR release 1.0, Release date: June 16, 2014 https://catalog.ldc.upenn.edu/LDC2014T12</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We gratefully acknowledge the support of the Na-tional Science Foundation Grants IIS-1116782, A Bayesian Approach to Dynamic Lexical Resources for Flexible Language Processing, 0910992 IIS:RI: Richer Representations for Ma-chine Translation, and NSF IIA-0530118 PIRE (a subcontract from Johns Hopkins) for the 2014 Frederick Jelinek Memorial Workshop for Mean-ing Representations in Language and Speech Processing, and funding under the BOLT and Machine Reading programs, HR0011-11-C-0145 (BOLT) FA8750-09-C-0179 (M.R.). Any opin-ions, findings, and conclusions or recommenda-tions expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Abstract meaning representation for sembanking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Banarescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Bonial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madalina</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<pubPlace>Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The mathematics of statistical machine translation: Parameter estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J Della</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">A</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="311" />
			<date type="published" when="1993-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Smatch: an evaluation metric for semantic feature structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="748" to="752" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A discriminative graph-based parser for the abstract meaning representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Lagrangian relaxation for integer programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arthurm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geoffrion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">50 Years of Integer Programming</title>
		<editor>Michael Jnger, Thomas M. Liebling, Denis Naddef, George L. Nemhauser, William R. Pulleyblank, Gerhard Reinelt, Giovanni Rinaldi, and Laurence A. Wolsey</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1958" />
			<biblScope unit="page" from="243" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The Proposition Bank: An annotated corpus of semantic roles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Guildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="105" />
			<date type="published" when="2005-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Aligning english strings with abstract meaning representation graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Pourdamghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="425" to="429" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A transition-based algorithm for amr parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Nianwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradhan</forename><surname>Sameer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
