<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:10+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Faster Phrase-Based Decoding by Refining Feature State</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kayser</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Faster Phrase-Based Decoding by Refining Feature State</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="130" to="135"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We contribute a faster decoding algorithm for phrase-based machine translation. Translation hypotheses keep track of state, such as context for the language model and coverage of words in the source sentence. Most features depend upon only part of the state, but traditional algorithms, including cube pruning, handle state atomically. For example, cube pruning will repeatedly query the language model with hypotheses that differ only in source coverage , despite the fact that source coverage is irrelevant to the language model. Our key contribution avoids this behavior by placing hypotheses into equivalence classes, masking the parts of state that matter least to the score. Moreover, we exploit shared words in hypotheses to iteratively refine language model scores rather than handling language model state atomically. Since our algorithm and cube pruning are both approximate, improvement can be used to increase speed or accuracy. When tuned to attain the same accuracy, our algorithm is 4.0-7.7 times as fast as the Moses decoder with cube pruning.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Translation speed is critical to making suggestions as translators type, mining for parallel data by translating the web, and running on mobile de- vices without Internet connectivity. We contribute a fast decoding algorithm for phrase-based ma- chine translation along with an implementation in a new open-source (LGPL) decoder available at http://kheafield.com/code/.</p><p>Phrase-based decoders ( <ref type="bibr" target="#b9">Koehn et al., 2007;</ref><ref type="bibr" target="#b0">Cer et al., 2010;</ref><ref type="bibr" target="#b15">Wuebker et al., 2012</ref>) keep track of several types of state with translation hypothe- ses: coverage of the source sentence thus far, con- text for the language model, the last position for the distortion model, and anything else features need. Existing decoders handle state atomically: hypotheses that have exactly the same state can be recombined and efficiently handled via dynamic programming, but there is no special handling for partial agreement. Therefore, features are repeat- edly consulted regarding hypotheses that differ only in ways irrelevant to their score, such as cov- erage of the source sentence. Our decoder bun- dles hypotheses into equivalence classes so that features can focus on the relevant parts of state.</p><p>We pay particular attention to the language model because it is responsible for much of the hy- pothesis state. As the decoder builds translations from left to right <ref type="bibr" target="#b10">(Koehn, 2004)</ref>, it records the last N − 1 words of each hypothesis so that they can be used as context to score the first N − 1 words of a phrase, where N is the order of the language model. Traditional decoders <ref type="bibr" target="#b7">(Huang and Chiang, 2007</ref>) try thousands of combinations of hypothe- ses and phrases, hoping to find ones that the lan- guage model likes. Our algorithm instead discov- ers good combinations in a coarse-to-fine manner. The algorithm exploits the fact that hypotheses of- ten share the same suffix and phrases often share the same prefix. These shared suffixes and prefixes allow the algorithm to coarsely reason over many combinations at once.</p><p>Our primary contribution is a new search algo- rithm that exploits the above observations, namely that state can be divided into pieces relevant to each feature and that language model state can be further subdivided. The primary claim is that our algorithm is faster and more accurate than the pop- ular cube pruning algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our previous work <ref type="bibr" target="#b5">(Heafield et al., 2013)</ref> devel- oped language model state refinement for bottom-up decoding in syntatic machine translation. In bottom-up decoding, hypotheses can be extended to the left or right, so hypotheses keep track of both their prefix and suffix. The present phrase- based setting is simpler because sentences are constructed from left to right, so prefix infor- mation is unnecessary. However, phrase-based translation implements reordering by allowing hy- potheses that translate discontiguous words in the source sentence. There are exponentially many ways to cover the source sentence and hypotheses carry this information as additional state. A main contribution in this paper is efficiently ignoring coverage when evaluating the language model. In contrast, syntactic machine translation hypotheses correspond to contiguous spans in the source sen- tence, so in prior work we simply ran the search algorithm in every span.</p><p>Another improvement upon <ref type="bibr" target="#b5">Heafield et al. (2013)</ref> is that we previously made no effort to exploit common words that appear in translation rules, which are analogous to phrases. In this work, we explicitly group target phrases by com- mon prefixes, doing so directly in the phrase table.</p><p>Coarse-to-fine approaches ( <ref type="bibr" target="#b14">Petrov et al., 2008;</ref><ref type="bibr" target="#b16">Zhang and Gildea, 2008)</ref> invoke the decoder multiple times with increasingly detailed models, pruning after each pass. The key difference in our work is that, rather than refining models in lock step, we effectively refine the language model on demand for hypotheses that score well. More- over, their work was performed in syntactic ma- chine translation while we address issues specific to phrase-based translation.</p><p>Our baseline is cube pruning <ref type="bibr" target="#b3">(Chiang, 2007;</ref><ref type="bibr" target="#b7">Huang and Chiang, 2007)</ref>, which is both a way to organize search and an algorithm to search through cross products of sets. We adopt the same search organization (Section 3.1) but change how cross products are searched.</p><p>Chang and Collins (2011) developed an exact decoding algorithm based on Lagrangian relax- ation. However, it has not been shown to tractably scale to 5-gram language models used by many modern translation systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Decoding</head><p>We begin by summarizing the high-level organiza- tion of phrase-based cube pruning <ref type="bibr" target="#b10">(Koehn, 2004;</ref><ref type="bibr" target="#b9">Koehn et al., 2007;</ref><ref type="bibr" target="#b7">Huang and Chiang, 2007</ref>  <ref type="figure">Figure 1</ref>: Stacks to translate the French "le chat ." into English. Filled circles indicate that the source word has been translated. A phrase translates "le chat" as simply "cat", emphasizing that stacks are organized by the number of source words rather than the number of target words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Search Organization</head><p>Phrase-based decoders construct hypotheses from left to right by appending phrases in the target lan- guage. The decoder organizes this search process using stacks ( <ref type="figure">Figure 1</ref>). Stacks contain hypothe- ses that have translated the same number of source words. The zeroth stack contains one hypothe- sis with nothing translated. Subsequent stacks are built by extending hypotheses in preceding stacks. For example, the second stack contains hypothe- ses that translated two source words either sepa- rately or as a phrasal unit. Returning to <ref type="figure">Figure 1</ref>, the decoder can apply a phrase pair to translate "le chat" as "cat" or it can derive "the cat" by translat- ing one word at a time; both appear in the second stack because they translate two source words. To generalize, the decoder populates the ith stack by pairing hypotheses in the i − jth stack with tar- get phrases that translate source phrases of length j. Hypotheses remember which source word they translated, as indicated by the filled circles. The reordering limit prevents hypotheses from jumping around the source sentence too much and dramatically reduces the search space. Formally, the decoder cannot propose translations that would require jumping back more than R words in the source sentence, including multiple small jumps.</p><p>In practice, stacks are limited to k hypothe- ses, where k is set by the user. Small k is faster but may prune good hypotheses, while large k is slower but more thorough, thereby comprising a time-accuracy trade-off. The central question in this paper is how to select these k hypotheses.</p><p>Populating a stack boils down to two steps. First, the decoder matches hypotheses with source phrases subject to three constraints: the total source length matches the stack being populated, none of the source words has already been trans- lated, and the reordering limit. Second, the de- coder searches through these matches to select k high-scoring hypotheses for placement in the stack. We improve this second step.</p><p>The decoder provides our algorithm with pairs consisting of a hypothesis and a compatible source phrase. Each source phrase translates to multiple target phrases. The task is to grow these hypothe- ses by appending a target phrase, yielding new hy- potheses. These new hypotheses will be placed into a stack of size k, so we are interested in se- lecting k new hypotheses that score highly.</p><p>Beam search <ref type="bibr" target="#b12">(Lowerre, 1976;</ref><ref type="bibr" target="#b10">Koehn, 2004</ref>) tries every hypothesis with every compatible tar- get phrase then selects the top k new hypotheses by score. This is wasteful because most hypothe- ses are discarded. Instead, we follow cube pruning <ref type="bibr" target="#b3">(Chiang, 2007</ref>) in using a priority queue to gen- erate k hypotheses. A key difference is that we generate these hypotheses iteratively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Tries</head><p>For each source phrase, we collect the set of com- patible hypotheses. We then place these hypothe- ses in a trie that emphasizes the suffix words be- cause these matter most when appending a target phrase. <ref type="figure" target="#fig_0">Figure 2</ref> shows an example. While it suf- fices to build this trie on the last N − 1 words that matter to the language model, <ref type="bibr" target="#b11">Li and Khudanpur (2008)</ref> have identified cases where fewer words are necessary because the language model will back off. The leaves of the trie are complete hypotheses and reveal information irrelevant to the language model, such as coverage of the source sentence and the state of other features.</p><p>Each source phrase translates to a set of tar- get phrases. Because these phrases will be ap- pended to a hypothesis, the first few words mat- ter the most to the language model. We therefore which have diplomatic are that have diplomatic  <ref type="figure" target="#fig_1">Figure 3</ref>. Similar to the hy- pothesis trie, the depth may be shorter than N − 1 in cases where the language model will provably back off ( <ref type="bibr" target="#b11">Li and Khudanpur, 2008)</ref>. The trie can also be short because the target phrase has fewer than N − 1 words. We currently store this trie data structure directly in the phrase table, though it could also be computed on demand to save mem- ory. Empirically, our phrase table uses less RAM than Moses's memory-based phrase table.</p><p>As an optimization, a trie reveals multiple words when there would otherwise be no branch- ing. This allows the search algorithm to make de- cisions only when needed.</p><p>Following <ref type="bibr" target="#b5">Heafield et al. (2013)</ref>, leaves in the trie take the score of the underlying hypothesis or target phrase. Non-leaf nodes take the maximum score of their descendants. Children of a node are sorted by score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Boundary Pairs</head><p>The idea is that the decoder reasons over pairs of nodes in the hypothesis and phrase tries before de- vling into detail. In this way, it can determine what the language model likes and, conversely, quickly discard combinations that the model does not like.</p><p>A boundary pair consists of a node in the hy- pothesis trie and a node in the target phrase trie. For example, the decoder starts at the root of each trie with the boundary pair (, ). The score of a boundary pair is the sum of the scores of the un- derlying trie nodes. However, once some words have been revealed, the decoder calls the language model to compute a score adjustment. For exam- ple, the boundary pair (country, that) has score ad- justment</p><formula xml:id="formula_0">log p(that | country) p(that)</formula><p>times the weight of the language model. This has the effect of cancelling out the estimate made 132 when the phrase was scored in isolation, replacing it with a more accurate estimate based on avail- able context. These score adjustments are efficient to compute because the decoder retained a pointer to "that" in the language model's data structure ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Splitting</head><p>Refinement is the notion that the boundary pair (, ) divides into several boundary pairs that re- veal specific words from hypotheses or target phrases. The most straightforward way to do this is simply to split into all children of a trie node. Continuing the example from <ref type="figure" target="#fig_0">Figure 2</ref>, we could split (, ) into three boundary pairs: (country, ), (nations, ), and (countries, ). However, it is somewhat inefficient to separately consider the low-scoring child (countries, ). Instead, we con- tinue to split off the best child (country, ) and leave a note that the zeroth child has been split off, denoted ([1 + ], ). The index increases each time a child is split off.</p><p>The the boundary pair ([1 + ], ) no longer counts (country, ) as a child, so its score is lower.</p><p>Splitting alternates sides. For example, (country, ) splits into (country, that) and (country, [1 + ]). If one side has completely revealed words that matter to the language model, then splitting continues with the other side. This procedure ensures that the language model score is completely resolved before considering irrelevant differences, such as coverage of the source sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Priority Queue</head><p>Search proceeds in a best-first fashion controlled by a priority queue. For each source phrase, we convert the compatible hypotheses into a trie. The target phrases were already converted into a trie when the phrase table was loaded. We then push the root (, ) boundary pair into the prior- ity queue. We do this for all source phrases under consideration, putting their root boundary pairs into the same priority queue. The algorithm then loops by popping the top boundary pair. It the top boundary pair uniquely describes a hypothesis and target phrase, then remaining features are evalu- ated and the new hypothesis is output to the de- coder's stack. Otherwise, the algorithm splits the boundary pair and pushes both split versions. Iter- ation continues until k new hypotheses have been found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Overall Algorithm</head><p>We build hypotheses from left-to-right and man- age stacks just like cube pruning. The only dif- ference is how the k elements of these stacks are selected.</p><p>When the decoder matches a hypothesis with a compatible source phrase, we immediately evalu- ate the distortion feature and update future costs, both of which are independent of the target phrase. Our future costs are exactly the same as those used in Moses ( <ref type="bibr" target="#b9">Koehn et al., 2007)</ref>: the highest-scoring way to cover the rest of the source sentence. This includes the language model score within target phrases but ignores the change in language model score that would occur were these phrases to be appended together. The hypotheses compatible with each source phrase are arranged into a trie. Finally, the priority queue algorithm from the pre- ceding section searches for options that the lan- guage model likes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>The primary claim is that our algorithm performs better than cube pruning in terms of the trade-off between time and accuracy. We compare our new decoder implementation with Moses ( <ref type="bibr" target="#b9">Koehn et al., 2007</ref>) by translating 1677 sentences from Chinese to English. These sentences are a deduplicated subset of the NIST Open MT 2012 test set and were drawn from Chinese online text sources, such as discussion forums. We trained our phrase table using a bitext of 10.8 million sentence pairs, which after tokenization amounts to approximately 290 million words on the English side. The bitext con- tains data from several sources, including news ar- ticles, UN proceedings, Hong Kong government documents, online forum data, and specialized sources such as an idiom translation table. We also trained our language model on the English half of this bitext using unpruned interpolated modified Kneser-Ney smoothing <ref type="bibr" target="#b8">(Kneser and Ney, 1995;</ref><ref type="bibr" target="#b2">Chen and Goodman, 1998)</ref>.</p><p>The system has standard phrase table, length, distortion, and language model features. We plan to implement lexicalized reordering in future work; without this, the test system is 0.53 BLEU ( <ref type="bibr" target="#b13">Papineni et al., 2002</ref>) point behind a state-of-the- art system. We set the reordering limit to R = 15.  Moses ( <ref type="bibr" target="#b9">Koehn et al., 2007</ref>) revision d6df825 was compiled with all optimizations recom- mended in the documentation. We use the in- memory phrase table for speed. Tests were run on otherwise-idle identical machines with 32 GB RAM; the processes did not come close to running out of memory. The language model was com- piled into KenLM probing format <ref type="bibr" target="#b6">(Heafield, 2011)</ref> and placed in RAM while text phrase tables were forced into the disk cache before each run. Timing is based on CPU usage (user plus system) minus loading time, as measured by running on empty input; our decoder is also faster at loading. All re- sults are single-threaded. Model score is compa- rable across decoders and averaged over all 1677 sentences; higher is better. The relationship be- tween model score and uncased BLEU ( <ref type="bibr" target="#b13">Papineni et al., 2002</ref>) is noisy, so peak BLEU is not attained by the highest search accuracy. <ref type="figure" target="#fig_2">Figure 4</ref> shows the results for pop limits k rang- ing from 5 to 10000 while <ref type="table">Table 1</ref> shows select results. For Moses, we also set the stack size to k to disable a second pruning pass, as is common. Because Moses is slower, we also ran our decoder with higher beam sizes to fill in the graph. Our decoder is more accurate, but mostly faster. We can interpret accuracy improvments as speed im- provements by asking how much time is required to attain the same accuracy as the baseline. By this metric, our decoder is 4.0 to 7.7 times as fast as Moses, depending on k.  <ref type="table">Table 1</ref>: Results for select stack sizes k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have contributed a new phrase-based search al- gorithm based on the principle that the language model cares the most about boundary words. This leads to two contributions: hiding irrelevant state from features and an incremental refinement algo- rithm to find high-scoring combinations. This al- gorithm is implemented in a new fast phrase-based decoder, which we release as open-source under the LGPL at kheafield.com/code/.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Hypothesis suffixes arranged into a trie. The leaves indicate source coverage and any other hypothesis state.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Target phrases arranged into a trie. Set in italic, leaves reveal parts of the phrase that are irrelevant to the language model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance of our decoder and Moses for various stack sizes k.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>The phrase table was pre-pruned by applying the same heuristic as Moses: select the top 20 target phrases by score, including the language model.</figDesc><table>133 

-29.5 

-29.0 

-28.5 

-28.0 

-27.5 

0 
1 
2 
3 
4 
Average model score 

CPU seconds/sentence 

This Work 
Moses 

13 

14 

15 

0 
1 
2 
3 
4 

Uncased BLEU 

CPU seconds/sentence 

This Work 
Moses 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by the Defense Ad-</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Phrasal: A statistical machine translation toolkit for exploring new model features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL HLT 2010 Demonstration Session</title>
		<meeting>the NAACL HLT 2010 Demonstration Session<address><addrLine>California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-06" />
			<biblScope unit="page" from="9" to="12" />
		</imprint>
	</monogr>
	<note>Los Angeles. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Exact decoding of phrase-based translation models through lagrangian relaxation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin-Wen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Edinburgh, Scotland, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">An empirical study of smoothing techniques for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Goodman</surname></persName>
		</author>
		<idno>TR-10-98</idno>
		<imprint>
			<date type="published" when="1998-08" />
		</imprint>
		<respStmt>
			<orgName>Harvard University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hierarchical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="201" to="228" />
			<date type="published" when="2007-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Left language model state for syntactic machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tetsuo</forename><surname>Kiso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Spoken Language Translation</title>
		<meeting>the International Workshop on Spoken Language Translation<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Grouping language model boundary words to speed k-best extraction from hypergraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">KenLM: Faster and smaller language model queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Workshop on Statistical Machine Translation, Edinburgh, UK, July. Association for Computational Linguistics</title>
		<meeting>the Sixth Workshop on Statistical Machine Translation, Edinburgh, UK, July. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Forest rescoring: Faster decoding with integrated language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improved backing-off for m-gram language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Kneser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="181" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting><address><addrLine>Prague</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06" />
		</imprint>
	</monogr>
	<note>Czech Republic</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pharaoh: a beam search decoder for phrase-based statistical machine translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine translation: From real users to research</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
	<note>September</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A scalable decoder for parsing-based machine translation with equivalent language model state maintenance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2)</title>
		<meeting>the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2)<address><addrLine>Columbus, Ohio</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-06" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The Harpy speech recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowerre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1976" />
			<pubPlace>Pittsburgh, PA, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">BLEU: A method for automatic evalution of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-07" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Coarse-to-fine syntactic machine translation using language projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2008 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-10" />
			<biblScope unit="page" from="108" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Jane 2: Open source phrase-based and hierarchical statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joern</forename><surname>Wuebker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Peitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malte</forename><surname>Nuhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Thorsten</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saab</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2012: Demonstration Papers</title>
		<meeting>COLING 2012: Demonstration Papers<address><addrLine>Mumbai</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-12" />
			<biblScope unit="page" from="483" to="492" />
		</imprint>
	</monogr>
	<note>India</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient multipass decoding for synchronous context free grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT</title>
		<meeting>ACL-08: HLT<address><addrLine>Columbus, Ohio</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="209" to="217" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
