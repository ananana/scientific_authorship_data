<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:55+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Differentiable Scheduled Sampling for Credit Assignment</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartik</forename><surname>Goyal</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University Pittsbrugh</orgName>
								<address>
									<region>PA</region>
									<country>USA, UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Differentiable Scheduled Sampling for Credit Assignment</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="366" to="371"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-2058</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We demonstrate that a continuous relaxation of the argmax operation can be used to create a differentiable approximation to greedy decoding for sequence-to-sequence (seq2seq) models. By incorporating this approximation into the scheduled sampling training procedure (Bengio et al., 2015)-a well-known technique for correcting exposure bias-we introduce a new training objective that is continuous and differentiable everywhere and that can provide informative gradients near points where previous decoding decisions change their value. In addition, by using a related approximation, we demonstrate a similar approach to sampled-based training. Finally , we show that our approach outper-forms cross-entropy training and scheduled sampling procedures in two sequence prediction tasks: named entity recognition and machine translation.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sequence-to-Sequence (seq2seq) models have demonstrated excellent performance in several tasks including machine translation <ref type="bibr" target="#b16">(Sutskever et al., 2014</ref>), summarization <ref type="bibr" target="#b14">(Rush et al., 2015)</ref>, dialogue generation ( <ref type="bibr" target="#b15">Serban et al., 2015)</ref>, and image captioning ( <ref type="bibr" target="#b19">Xu et al., 2015)</ref>. However, the standard cross-entropy training procedure for these models suffers from the well-known prob- lem of exposure bias: because cross-entropy train- ing always uses gold contexts, the states and con- texts encountered during training do not match those encountered at test time. This issue has been addressed using several approaches that try to in- corporate awareness of decoding choices into the training optimization. These include reinforce- ment learning ( <ref type="bibr" target="#b12">Ranzato et al., 2016;</ref><ref type="bibr" target="#b1">Bahdanau et al., 2017)</ref>, imitation learning ( <ref type="bibr" target="#b5">Daumé et al., 2009;</ref><ref type="bibr" target="#b13">Ross et al., 2011;</ref>, and beam-based approaches <ref type="bibr" target="#b18">(Wiseman and Rush, 2016;</ref><ref type="bibr" target="#b0">Andor et al., 2016;</ref><ref type="bibr" target="#b6">Daumé III and Marcu, 2005)</ref>. In this paper, we focus on one the simplest to implement and least computationally expensive approaches, scheduled sampling ( , which stochastically incorporates contexts from previous decoding decisions into training.</p><p>While scheduled sampling has been empiri- cally successful, its training objective has a draw- back: because the procedure directly incorporates greedy decisions at each time step, the objective is discontinuous at parameter settings where previ- ous decisions change their value. As a result, gra- dients near these points are non-informative and scheduled sampling has difficulty assigning credit for errors. In particular, the gradient does not pro- vide information useful in distinguishing between local errors without future consequences and cas- cading errors which are more serious.</p><p>Here, we propose a novel approach based on scheduled sampling that uses a differentiable ap- proximation of previous greedy decoding deci- sions inside the training objective by incorporat- ing a continuous relaxation of argmax. As a re- sult, our end-to-end relaxed greedy training ob- jective is differentiable everywhere and fully con- tinuous. By making the objective continuous at points where previous decisions change value, our approach provides gradients that can respond to cascading errors. In addition, we demonstrate a related approximation and reparametrization for sample-based training (another training scenario considered by scheduled sampling ( ) that can yield stochastic gradients with lower variance than in standard scheduled sam- pling. In our experiments on two different tasks, machine translation (MT) and named entity recog- nition (NER), we show that our approach out- performs both cross-entropy training and standard scheduled sampling procedures with greedy and sampled-based training.</p><formula xml:id="formula_0">↵ = 1 ↵ = 10 ✓ objective(✓) ˆ y i1 (✓) = 'dog' ˆ y i1 (✓) = 'kitten' ˆ y i1 (✓) = 'cat'</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Discontinuity in Scheduled Sampling</head><p>While scheduled sampling ( ) is an effective way to rectify exposure bias, it can- not differentiate between cascading errors, which can lead to a sequence of bad decisions, and local errors, which have more benign effects. Specifi- cally, scheduled sampling focuses on learning op- timal behavior in the current step given the fixed decoding decision of the previous step. If a previ- ous bad decision is largely responsible for the cur- rent error, the training procedure has difficulty ad- justing the parameters accordingly. The following machine translation example highlights this credit assignment issue:</p><p>Ref: The cat purrs . Pred: The dog barks .</p><p>At step 3, the model prefers the word 'barks' af- ter incorrectly predicting 'dog' at step 2. To cor- rect this error, the scheduled sampling procedure would increase the score of 'purrs' at step 3, con- ditioned on the fact that the model predicted (in- correctly) 'dog' at step 2, which is not the ideal learning behaviour. Ideally, the model should be able to backpropagate the error from step 3 to the source of the problem which occurred at step 2, where 'dog' was predicted instead of 'cat'. The lack of credit assignment during training is a result of discontinuity in the objective func- tion used by scheduled sampling, as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. We denote the ground truth target sym- bol at step i by y * i , the embedding representa- tion of word y by e(y), and the hidden state of a seq2seq decoder at step i as h i . Standard cross- entropy training defines the loss at each step to be log p(y * i |h i (e(y * i−1 ), h i−1 )), while scheduled sam- pling uses loss log p(y * i |h i (e(ˆ y i−1 ), h i−1 )), where</p><formula xml:id="formula_1">{ e(dog) e(cat) ↵-soft argmax { peaked softmax argmax h i1 h i e(kitten) ˆ y i1 = dog ¯ e i1 si1(dog) si1(kitten) si1(cat) X y e(y) · exp [↵ · s i1 (y)] Z</formula><p>Figure 2: Relaxed greedy decoder that uses a continuous approximation of argmax as input to the decoder state at next time step.</p><p>ˆ y i−1 refers the model's prediction at the previ- ous step. 1 Here, the model predictionˆypredictionˆ predictionˆy i−1 is obtained by argmaxing over the output softmax layer. Hence, in addition to the intermediate hid- den states and final softmax scores, the previ- ous model prediction, ˆ y i−1 , itself depends on the model parameters, θ, and ideally, should be back- propagated through, unlike the gold target symbol y * i−1 which is independent of model parameters. However, the argmax operation is discontinuous, and thus the training objective (depicted in <ref type="figure" target="#fig_0">Fig- ure 1</ref> as the red line) exhibits discontinuities at pa- rameter settings where the previous decoding deci- sions change value (depicted as changes from 'kit- ten' to 'dog' to 'cat'). Because these change points represent discontinuities, their gradients are unde- fined and the effect of correcting an earlier mistake (for example 'dog' to 'cat') as the training proce- dure approaches such a point is essentially hidden.</p><p>In our approach, described in detail in the next section, we attempt to fix this problem by incorpo- rating a continuous relaxation of the argmax op- eration into the scheduled sampling procedure in order to form an approximate but fully continuous objective. Our relaxed approximate objective is depicted in <ref type="figure" target="#fig_0">Figure 1</ref> as blue and purple lines, de- pending on temperature parameter α which trades- off smoothness and quality of approximation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Credit Assignment via Relaxation</head><p>In this section we explain in detail the continu- ous relaxation of greedy decoding that we will use to build a fully continuous training objective. We also introduce a related approach for sample-based training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Soft Argmax</head><p>In scheduled sampling, the embedding for the best scoring word at the previous step is passed as an input to the current step. This operation 2 can be expressed asê</p><formula xml:id="formula_2">asˆasê i−1 = y e(y)1[∀y = y s i−1 (y) &gt; s i−1 (y )]</formula><p>where y is a word in the vocabulary, s i−1 (y) is the output score of that word at the previous step, andê andˆandê i−1 is the embedding passed to the next step. This operation can be relaxed by replacing the indicator function with a peaked softmax function with hy- perparameter α to define a soft argmax procedure:</p><formula xml:id="formula_3">¯ e i−1 = y e(y) · exp (α s i−1 (y)) y exp (α s i−1 (y ))</formula><p>As α → ∞, the equation above approaches the true argmax embedding. Hence, with a finite and large α, we get a linear combination of all the words (and therefore a continuous function of the parameters) that is dominated heavily by the word with maximum score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Soft Reparametrized Sampling</head><p>Another variant of scheduled sampling is to pass a sampled embedding from the softmax distribu- tion at the previous step to the current step instead of the argmax. This is expected to enable better exploration of the search space during optimiza- tion due to the added randomness and hence re- sult in a more robust model. In this section, we discuss and review an approximation to the Gum- bel reparametrization trick that we use as a mod- ule in our sample-based decoder. This approxima- tion was proposed by <ref type="bibr" target="#b11">Maddison et al. (2017)</ref> and <ref type="bibr" target="#b10">Jang et al. (2016)</ref>, who showed that the same soft argmax operation introduced above can be used for reducing variance of stochastic gradients when sampling from softmax distributions. Unlike soft argmax, this approach is not a fully continuous ap- proximation to the sampling operation, but it does result in much more informative gradients com- pared to naive scheduled sampling procedure. The Gumbel reparametrization trick shows that sampling from a categorical distribution can be refactored into sampling from a simple distribu- tion followed by a deterministic transformation 2 Assuming there are no ties for the sake of simplicity.</p><p>as follows: (i) sampling an independent Gum- bel noise G for each element in the categori- cal distribution, typically done by transforming a sample from the uniform distribution: U ∼ U nif orm(0, 1) as G = −log(−log U ), then (ii) adding it componentwise to the unnormalized score of each element, and finally (iii) taking an argmax over the vector. Using the same argmax softening procedure as above, they arrive at an ap- proximation to the reparametrization trick which mitigates some of the gradient's variance intro- duced by sampling. The approximation is <ref type="bibr">3</ref> :</p><formula xml:id="formula_4">˜ e i−1 = y e(y) · exp (α (s i−1 (y) + G y )) y exp (α (s i−1 (y ) + G y ))</formula><p>We will use this 'concrete' approximation of soft- max sampling in our relaxation of scheduled sam- pling with a sample-based decoder. We discuss details in the next section. Note that our orig- inal motivation based on removing discontinuity does not strictly apply to this sampling procedure, which still yields a stochastic gradient due to sam- pling from the Gumbel distribution. However, this approach is conceptually related to greedy relax- ations since, here, the soft argmax reparametriza- tion reduces gradient variance which may yield a more informative training signal. Intuitively, this approach results in the gradient of the loss to be more aware of the sampling procedure compared to naive scheduled sampling and hence carries for- ward information about decisions made at previ- ous steps. The empirical results, discussed later, show similar gains to the greedy scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Differentiable Relaxed Decoders</head><p>With the argmax relaxation introduced above, we have a recipe for a fully differentiable greedy de- coder designed to produce informative gradients near change points. Our final training network for scheduled sampling with relaxed greedy decod- ing is shown in <ref type="figure">Figure 2</ref>. Instead of conditioning the current hidden state, h i , on the argmax em- bedding from the previous step, ˆ e i−1 , we use the α-soft argmax embedding, ¯ e i−1 , defined in Sec- tion 3.1. This removes the discontinuity in the original greedy scheduled sampling objective by passing a linear combination of embeddings, dom- inated by the argmax, to the next step. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the effect of varying α. As α increases, we more closely approximate the greedy decoder.</p><p>As in standard scheduled sampling, here we minimize the cross-entropy based loss at each time step. Hence the computational complexity of our approach is comparable to standard seq2seq train- ing. As we discuss in Section 5, mixing model predictions randomly with ground truth symbols during training ( <ref type="bibr" target="#b5">Daumé et al., 2009;</ref><ref type="bibr" target="#b13">Ross et al., 2011</ref>), while annealing the prob- ability of using the ground truth with each epoch, results in better models and more stable train- ing. As a result, training is reliant on the anneal- ing schedule of two important hyperparameters: i) ground truth mixing probability and ii) the α pa- rameter used for approximating the argmax func- tion. For output prediction, at each time step, we can still output the hard argmax, depicted in <ref type="figure">Fig- ure 2</ref>.</p><p>For the case of scheduled sampling with sample-based training-where decisions are sam- pled rather than chosen greedily ( )-we conduct experiments using a related training procedure. Instead of using soft argmax, we use the soft sample embedding, ˜ e i−1 , defined in Section 3.2. Apart from this difference, training is carried out using the same procedure. <ref type="bibr" target="#b7">Gormley et al. (2015)</ref>'s approximation-aware training is conceptually related, but focuses on variational decoding procedures. <ref type="bibr" target="#b9">Hoang et al. (2017)</ref> also propose continuous relaxations of de- coders, but are focused on developing better infer- ence procedures. <ref type="bibr" target="#b8">Grefenstette et al. (2015)</ref> suc- cessfully use a soft approximation to argmax in neural stack mechanisms. Finally, <ref type="bibr" target="#b12">Ranzato et al. (2016)</ref> experiment with a similarly motivated ob- jective that was not fully continuous, but found it performed worse than the standard training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head><p>We perform experiments with machine translation (MT) and named entity recognition (NER).</p><p>Data: For MT, we use the same dataset (the German-English portion of the IWSLT 2014 ma- chine translation evaluation campaign ( <ref type="bibr" target="#b4">Cettolo et al., 2014)</ref>), preprocessing and data splits as <ref type="bibr" target="#b12">Ranzato et al. (2016)</ref>. For named entity recognition, we use the CONLL 2003 shared task data <ref type="bibr" target="#b17">(Tjong Kim Sang and De Meulder, 2003)</ref> for German lan- guage and use the provided data splits. We per- form no preprocessing on the data.The output vo- cabulary length for MT is 32000 and 10 for NER.</p><p>Implementation details: For MT, we use a seq2seq model with a simple attention mechanism ( <ref type="bibr" target="#b2">Bahdanau et al., 2015</ref>), a bidirectional LSTM en- coder (1 layer, 256 units), and an LSTM decoder (1 layer, 256 units). For NER, we use a seq2seq model with an LSTM encoder (1 layer, 64 units) and an LSTM decoder (1 layer, 64 units) with a fixed attention mechanism that deterministically attends to the ith input token when decoding the ith output, and hence does not involve learning of attention parameters. 4</p><p>Hyperparameter tuning: We start by training with actual ground truth sequences for the first epoch and decay the probability of selecting the ground truth token as an inverse sigmoid (  of epochs with a decay strength pa- rameter k. We also tuned for different values of α and explore the effect of varying α exponentially (annealing) with the epochs. In table 1, we report results for the best performing configuration of de- cay parameter and the α parameter on the valida- tion set. To account for variance across randomly started runs, we ran multiple random restarts (RR) for all the systems evaluated and always used the RR with the best validation set score to calculate test performance.</p><p>Comparison We report validation and test met- rics for NER and MT tasks in <ref type="table">Table 1, F1</ref> and BLEU respectively. 'Greedy' in the table refers to scheduled sampling with soft argmax decisions (either soft or hard) and 'Sample' refers the cor- responding reparametrized sample-based decod- ing scenario. We compare our approach with two baselines: standard cross-entropy loss minimiza- tion for seq2seq models ('Baseline CE') and the standard scheduled sampling procedure ( ). We report results for two variants of our approach: one with a fixed α parameter throughout the training procedure (α-soft fixed), and the other in which we vary α exponentially with the number of epochs (α-soft annealed). <ref type="table">Table 1</ref>: Result on NER and MT. We compare our approach (α-soft argmax with fixed and annealed temperature) with standard cross entropy training (Baseline CE) and discontinuous scheduled sampling ( ). 'Greedy' and 'Sample' refer to Section 3.1 and Section 3.2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>All three approaches improve over the standard cross-entropy based seq2seq training. Moreover, both approaches using continuous relaxations (greedy and sample-based) outperform standard scheduled sampling ( ). The best results for NER were obtained with the re- laxed greedy decoder with annealed α which yielded an F1 gain of +3.1 over the standard seq2seq baseline and a gain of +1.5 F1 over stan- dard scheduled sampling. For MT, we obtain the best results with the relaxed sample-based de- coder, which yielded a gain of +1.5 BLEU over standard seq2seq and a gain of +0.75 BLEU over standard scheduled sampling. We observe that the reparametrized sample- based method, although not fully continuous end- to-end unlike the soft greedy approach, results in good performance on both the tasks, partic- ularly MT. This might be an effect of stochas- tic exploration of the search space over the out- put sequences during training and hence we ex- pect MT to benefit from sampling due to a much larger search space associated with it. We also ob- serve that annealing α results in good performance which suggests that a smoother approximation to the loss function in the initial stages of training is helpful in guiding the learning in the right direc- tion. However, in our experiments we noticed that k 100 10 1 Always NER (F1) 56.33 55.88 55.30 54.83 <ref type="table">Table 2</ref>: Effect of different schedules for scheduled sampling on NER. k is the decay strength parameter. Higher k cor- responds to gentler decay schedules. Always refers to the case when predictions at the previous predictions are always passed on as inputs to the next step. the performance while annealing α was sensitive to the hyperparameter associated with the anneal- ing schedule of the mixing probability in sched- uled sampling during training.</p><p>The computational complexity of our approach is comparable to that of standard seq2seq train- ing. However, instead of a vocabulary-sized max and lookup, our approach requires a matrix multi- plication. Practically, we observed that on GPU hardware, all the models for both the tasks had similar speeds which suggests that our approach leads to accuracy gains without compromising run-time. Moreover, as shown in <ref type="table">Table 2</ref>, we ob- serve that a gradual decay of mixing probability consistently compared favorably to more aggres- sive decay schedules. We also observed that the 'always sample' case of relaxed greedy decoding, in which we never mix in ground truth inputs (see ), worked well for NER but resulted in unstable training for MT. We reckon that this is an effect of large difference between the search space associated with NER and MT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>Our positive results indicate that mechanisms for credit assignment can be useful when added to the models that aim to ameliorate exposure bias. Further, our results suggest that continuous relax- ations of the argmax operation can be used as ef- fective approximations to hard decoding during training.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Discontinuous scheduled sampling objective (red) and continuous relaxations (blue and purple).</figDesc></figure>

			<note place="foot" n="1"> For the sake of simplicity, the &apos;always sample&apos; variant of scheduled sampling is described (Bengio et al., 2015).</note>

			<note place="foot" n="3"> This is different from using the expected softmax embedding because our approach approximates the actual sampling process instead of linearly weighting the embeddings by their softmax probabilities</note>

			<note place="foot" n="4"> Fixed attention refers to the scenario when we use the bidirectional LSTM encoder representation of the source sequence token at time step t while decoding at time step t instead of using a linear combination of all the input sequences weighted according to the attention parameters in the standard attention mechanism based models.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Graham Neubig for helpful discussions. We also thank the three anonymous reviewers for their valuable feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Globally normalized transition-based neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Presta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An actor-critic algorithm for sequence prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philemon</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1171" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Report on the 11th iwslt evaluation campaign, iwslt 2014</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Stüker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Spoken Language Translation</title>
		<meeting>the International Workshop on Spoken Language Translation<address><addrLine>Hanoi, Vietnam</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Search-based structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="325" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning as search optimization: Approximate large margin methods for structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on Machine learning</title>
		<meeting>the 22nd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Approximation-aware dependency parsing by belief propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Gormley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
			<publisher>TACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to transduce with unbounded memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1828" to="1836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Decoding as continuous optimization in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong Duy Vu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.02854</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Chris J Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sequence level training with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A reduction of imitation learning and structured prediction to no-regret online learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Drew</forename><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Alexander M Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Building end-to-end dialogue systems using generative hierarchical neural network models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Iulian V Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI&apos;16 Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik F Tjong Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien De</forename><surname>Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003</title>
		<meeting>the seventh conference on Natural language learning at HLT-NAACL 2003</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sequence-to-sequence learning as beam-search optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="77" to="81" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
