<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Trainable Spaced Repetition Model for Language Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>August 7-12, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Settles</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Duolingo Pittsburgh</orgName>
								<orgName type="institution">Uber Advanced Technologies Center Pittsburgh</orgName>
								<address>
									<country>PA USA, PA USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Meeder</surname></persName>
							<email>bmeeder@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Duolingo Pittsburgh</orgName>
								<orgName type="institution">Uber Advanced Technologies Center Pittsburgh</orgName>
								<address>
									<country>PA USA, PA USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Trainable Spaced Repetition Model for Language Learning</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1848" to="1858"/>
							<date type="published">August 7-12, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present half-life regression (HLR), a novel model for spaced repetition practice with applications to second language acquisition. HLR combines psycholinguis-tic theory with modern machine learning techniques, indirectly estimating the &quot;half-life&quot; of a word or concept in a student&apos;s long-term memory. We use data from Duolingo-a popular online language learning application-to fit HLR models, reducing error by 45%+ compared to several baselines at predicting student recall rates. HLR model weights also shed light on which linguistic concepts are systematically challenging for second language learners. Finally, HLR was able to improve Duolingo daily student engagement by 12% in an operational user study.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The spacing effect is the observation that people tend to remember things more effectively if they use spaced repetition practice (short study periods spread out over time) as opposed to massed prac- tice (i.e., "cramming"). The phenomenon was first documented by <ref type="bibr" target="#b12">Ebbinghaus (1885)</ref>, using himself as a subject in several experiments to memorize verbal utterances. In one study, after a day of cramming he could accurately recite 12-syllable sequences (of gibberish, apparently). However, he could achieve comparable results with half as many practices spread out over three days.</p><p>The lag effect <ref type="bibr" target="#b18">(Melton, 1970</ref>) is the related ob- servation that people learn even better if the spac- ing between practices gradually increases. For ex- ample, a learning schedule might begin with re-view sessions a few seconds apart, then minutes, then hours, days, months, and so on, with each successive review stretching out over a longer and longer time interval.</p><p>The effects of spacing and lag are well- established in second language acquisition re- search <ref type="bibr" target="#b1">(Atkinson, 1972;</ref><ref type="bibr" target="#b5">Bloom and Shuell, 1981;</ref><ref type="bibr" target="#b6">Cepeda et al., 2006;</ref><ref type="bibr" target="#b20">Pavlik Jr and Anderson, 2008)</ref>, and benefits have also been shown for gym- nastics, baseball pitching, video games, and many other skills. See <ref type="bibr" target="#b24">Ruth (1928)</ref>, <ref type="bibr" target="#b9">Dempster (1989)</ref>, and <ref type="bibr" target="#b10">Donovan and Radosevich (1999)</ref> for thorough meta-analyses spanning several decades.</p><p>Most practical algorithms for spaced repetition are simple functions with a few hand-picked pa- rameters. This is reasonable, since they were largely developed during the 1960s-80s, when people would have had to manage practice sched- ules without the aid of computers. However, the recent popularity of large-scale online learning software makes it possible to collect vast amounts of parallel student data, which can be used to em- pirically train richer statistical models.</p><p>In this work, we propose half-life regression (HLR) as a trainable spaced repetition algorithm, marrying psycholinguistically-inspired models of memory with modern machine learning tech- niques. We apply this model to real student learn- ing data from Duolingo, a popular language learn- ing app, and use it to improve its large-scale, op- erational, personalized learning system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Duolingo</head><p>Duolingo is a free, award-winning, online lan- guage learning platform. Since launching in 2012, more than 150 million students from all over the world have enrolled in a Duolingo course, either via the website 1 or mobile apps for Android, iOS,   and Windows devices. For comparison, that is more than the total number of students in U.S. el- ementary and secondary schools combined. At least 80 language courses are currently available or under development 2 for the Duolingo platform. The most popular courses are for learning English, Spanish, French, and German, although there are also courses for minority languages (Irish Gaelic), and even constructed languages (Esperanto). More than half of Duolingo students live in developing countries, where Internet access has more than tripled in the past three years <ref type="bibr" target="#b16">(ITU and UNESCO, 2015)</ref>. The majority of these students are using Duolingo to learn English, which can significantly improve their job prospects and qual- ity of life ( <ref type="bibr" target="#b22">Pinon and Haydon, 2010</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">System Overview</head><p>Duolingo uses a playfully illustrated, gamified de- sign that combines point-reward incentives with implicit instruction <ref type="bibr" target="#b8">(DeKeyser, 2008)</ref>, mastery learning ( <ref type="bibr" target="#b2">Block et al., 1971)</ref>, explanations (Fahy, 2 https://incubator.duolingo.com 2004), and other best practices. Early research suggests that 34 hours of Duolingo is equivalent to a full semester of university-level Spanish in- struction ( <ref type="bibr" target="#b28">Vesselinov and Grego, 2012)</ref>. <ref type="figure" target="#fig_0">Figure 1</ref>(a) shows an example skill tree for English speakers learning French. This specifies the game-like curriculum: each icon represents a skill, which in turn teaches a set of themati- cally or grammatically related words or concepts. Students tap an icon to access lessons of new material, or to practice previously-learned mate- rial. <ref type="figure" target="#fig_0">Figure 1(b)</ref> shows a screen for the French skill Gerund, which teaches common gerund verb forms such as faisant (doing) andétantand´andétant (being). This skill, as well as several others, have already been completed by the student. However, the Mea- sures skill in the bottom right of <ref type="figure" target="#fig_0">Figure 1</ref>(a) has one lesson remaining. After completing each row of skills, students "unlock" the next row of more advanced skills. This is a gamelike implementa- tion of mastery learning, whereby students must reach a certain level of prerequisite knowledge be- fore moving on to new material.</p><p>Each language course also contains a corpus (large database of available exercises) and a lex- eme tagger (statistical NLP pipeline for automat- ically tagging and indexing the corpus; see the Appendix for details and a lexeme tag reference). <ref type="figure" target="#fig_0">Figure 1(c,d)</ref> shows an example translation exer- cise that might appear in the Gerund skill, and <ref type="figure" target="#fig_1">Fig- ure 2</ref> shows the lexeme tagger output for this sen- tence. Since this exercise is indexed with a gerund lexeme tag (ˆ etre.V.GER in this case), it is available for lessons or practices in this skill.</p><p>The lexeme tagger also helps to provide correc- tive feedback. Educational researchers maintain that incorrect answers should be accompanied by explanations, not simply a "wrong" mark <ref type="bibr" target="#b13">(Fahy, 2004</ref>). In <ref type="figure" target="#fig_0">Figure 1(d)</ref>, the student incorrectly used the 2nd-person verb form es (ˆ etre.V.PRES.P2.SG) instead of the 3rd-person est (ˆ etre.V.PRES.P3.SG). If Duolingo is able to parse the student response and detect a known grammatical mistake such as this, it provides an explanation 3 in plain language. Each lesson continues until the student masters all of the target words being taught in the session, as estimated by a mixture model of short-term learn- ing curves <ref type="bibr" target="#b26">(Streeter, 2015</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Spaced Repetition and Practice</head><p>Once a lesson is completed, all the target words being taught in the lesson are added to the student model. This model captures what the student has learned, and estimates how well she can recall this knowledge at any given time. Spaced repetition is a key component of the student model: over time, the strength of a skill will decay in the student's long-term memory, and this model helps the stu- dent manage her practice schedule.</p><p>Duolingo uses strength meters to visualize the student model, as seen beneath each of the com- pleted skill icons in <ref type="figure" target="#fig_0">Figure 1</ref>(a). These meters represent the average probability that the student can, at any moment, correctly recall a random tar- get word from the lessons in this skill (more on this probability estimate in §3.3). At four bars, the skill is "golden" and considered fresh in the stu- dent's memory. At fewer bars, the skill has grown stale and may need practice. A student can tap the skill icon to access practice sessions and target her weakest words. For example, <ref type="figure" target="#fig_0">Figure 1</ref>(b) shows some weak words from the Gerund skill. Practice sessions are identical to lessons, except that the exercises are taken from those indexed with words (lexeme tags) due for practice according to student model. As time passes, strength meters continu- ously update and decay until the student practices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Spaced Repetition Models</head><p>In this section, we describe several spaced repeti- tion algorithms that might be incorporated into our student model. We begin with two common, estab- lished methods in language learning technology, and then present our half-life regression model which is a generalization of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Pimsleur Method</head><p>Pimsleur (1967) was perhaps the first to make mainstream practical use of the spacing and lag ef- fects, with his audio-based language learning pro- gram (now a franchise by <ref type="bibr">Simon &amp; Schuster)</ref>. He referred to his method as graduated-interval re- call, whereby new vocabulary is introduced and then tested at exponentially increasing intervals, interspersed with the introduction or review of other vocabulary. However, this approach is lim- ited since the schedule is pre-recorded and can- not adapt to the learner's actual ability. Consider an English-speaking French student who easily learns a cognate like pantalon (pants), but strug- gles to remember manteau (coat). With the Pim- sleur method, she is forced to practice both words at the same fixed, increasing schedule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Leitner System</head><p>Leitner (1972) proposed a different spaced repeti- tion algorithm intended for use with flashcards. It is more adaptive than Pimsleur's, since the spac- ing intervals can increase or decrease depending on student performance. <ref type="figure" target="#fig_2">Figure 3</ref> illustrates a pop- ular variant of this method. The main idea is to have a few boxes that corre- spond to different practice intervals: 1-day, 2-day, 4-day, and so on. All cards start out in the 1-day box, and if the student can remember an item after one day, it gets "promoted" to the 2-day box. Two days later, if she remembers it again, it gets pro- moted to the 4-day box, etc. Conversely, if she is incorrect, the card gets "demoted" to a shorter in- terval box. Using this approach, the hypothetical French student from §3.1 would quickly promote pantalon to a less frequent practice schedule, but continue reviewing manteau often until she can regularly remember it.</p><p>Several electronic flashcard programs use the Leitner system to schedule practice, by organiz- ing items into "virtual" boxes. In fact, when it first launched, Duolingo used a variant similar to <ref type="figure" target="#fig_2">Fig- ure 3</ref> to manage skill meter decay and practice. The present research was motivated by the need for a more accurate model, in response to student complaints that the Leitner-based skill meters did not adequately reflect what they had learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Half-Life Regression: A New Approach</head><p>We now describe half-life regression (HLR), start- ing from psychological theory and combining it with modern machine learning techniques.</p><p>Central to the theory of memory is the Ebbing- haus model, also known as the forgetting curve <ref type="bibr" target="#b12">(Ebbinghaus, 1885)</ref>. This posits that memory de- cays exponentially over time:</p><formula xml:id="formula_0">p = 2 −∆/h .<label>(1)</label></formula><p>In this equation, p denotes the probability of cor- rectly recalling an item (e.g., a word), which is a function of ∆, the lag time since the item was last practiced, and h, the half-life or measure of strength in the learner's long-term memory. <ref type="figure">Figure 4</ref>(a) shows a forgetting curve (1) with half-life h = 1. Consider the following cases:</p><p>1. ∆ = 0. The word was just recently practiced, so p = 2 0 = 1.0, conforming to the idea that it is fresh in memory and should be recalled correctly regardless of half-life.</p><p>2. ∆ = h. The lag time is equal to the half-life, so p = 2 −1 = 0.5, and the student is on the verge of being unable to remember.</p><p>3. ∆ h. The word has not been practiced for a long time relative to its half-life, so it has probably been forgotten, e.g., p ≈ 0.</p><p>Let x denote a feature vector that summarizes a student's previous exposure to a particular word, and let the parameter vector Θ contain weights that correspond to each feature variable in x. Under the assumption that half-life should increase expo- nentially with each repeated exposure (a common practice in spacing and lag effect research), we letˆh letˆ letˆh Θ denote the estimated half-life, given by:</p><formula xml:id="formula_1">ˆ h Θ = 2 Θ·x .<label>(2)</label></formula><p>In fact, the Pimsleur and Leitner algorithms can be interpreted as special cases of (2) using a few fixed, hand-picked weights. See the Appendix for the derivation of Θ for these two methods. For our purposes, however, we want to fit Θ em- pirically to learning trace data, and accommodate an arbitrarily large set of interesting features (we discuss these features more in §3.4). Suppose we have a data set D = {{p, ∆,</p><formula xml:id="formula_2">x i } D i=1</formula><p>made up of student-word practice sessions. Each data instance consists of the observed recall rate p 4 , lag time ∆ since the word was last seen, and a feature vector x designed to help personalize the learning expe- rience. Our goal is to find the best model weights Θ * to minimize some loss function :</p><formula xml:id="formula_3">Θ * = arg min Θ D i=1 (p, ∆, x i ; Θ) .<label>(3)</label></formula><p>To illustrate, <ref type="figure">Figure 4</ref>(b) shows a student-word learning trace over the course of a month. Each indicates a data instance: the vertical position is the observed recall rate p for each practice session, and the horizontal distance between points is the lag time ∆ between sessions. Combining (1) and (2), the model predictionˆppredictionˆ predictionˆp Θ = 2 −∆/ ˆ h Θ is plot- ted as a dashed line over time (which resets to 1.0 after each exposure, since ∆ = 0). The training loss function (3) aims to fit the predicted forget- ting curves to observed data points for millions of student-word learning traces like this one.</p><p>We chose the L 2 -regularized squared loss func- tion, which in its basic form is given by:</p><formula xml:id="formula_4">(; Θ) = (p − ˆ p Θ ) 2 + λΘ 2 2 ,</formula><p>where = p, ∆, x is shorthand for the training data instance, and λ is a parameter to control the regularization term and help prevent overfitting. In practice, we found it useful to optimize for the half-life h in addition to the observed recall rate p. Since we do not know the "true" half-life of a given word in the student's memory -this is a hypothetical construct -we approximate it algebraically from (1) using p and ∆. We solve for h = −∆ log 2 (p) and use the final loss function:</p><formula xml:id="formula_5">(; Θ) = (p − ˆ p Θ ) 2 + α(h − ˆ h Θ ) 2 + λΘ 2 2 ,</formula><p>where α is a parameter to control the relative im- portance of the half-life term in the overall train- ing objective function. Since is smooth with re- spect to Θ, we can fit the weights to student-word learning traces using gradient descent. See the Ap- pendix for more details on our training and opti- mization procedures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Feature Sets</head><p>In this work, we focused on features that were eas- ily instrumented and available in the production Duolingo system, without adding latency to the student's user experience. These features fall into two broad categories:</p><p>• Interaction features: a set of counters sum- marizing each student's practice history with each word (lexeme tag). These include the total number of times a student has seen the word x n , the number of times it was correctly recalled x ⊕ , and the number of times incor- rect x . These are intended to help the model make more personalized predictions.</p><p>• Lexeme tag features: a large, sparse set of indicator variables, one for each lexeme tag in the system (about 20k in total). These are intended to capture the inherent difficulty of each particular word (lexeme tag).</p><formula xml:id="formula_6">recall rate lag (days) feature vector x p (⊕/n) ∆ x n x ⊕ x x ˆ etre.V.GER 1.0 (3/3) 0</formula><note type="other">.6 3 2 1 1 0.5 (2/4) 1.7 6 5 1 1 1.0 (3/3) 0.7 10 7 3 1 0.8 (4/5) 4.7 13 10 3 1 0.5 (1/2) 13.5 18 14 4 1 1.0 (3/3)</note><p>2.6 20 15 5 1 <ref type="table">Table 1</ref>: Example training instances. Each row corresponds to a data point in <ref type="figure">Figure 4</ref>(b) above, which is for a student learning the French wordétant word´wordétant (lexeme tagêtretagˆtagêtre.V.GER).</p><p>To be more concrete, imagine that the trace in <ref type="figure">Figure 4</ref>(b) is for a student learning the French wordétantword´wordétant (lexeme tagêtretagˆtagêtre.V.GER). <ref type="table">Table 1</ref> shows what p, ∆, x would look like for each session in the student's history with that word. The inter- action features increase monotonically 5 over time, and x ˆ etre.V.GER is the only lexeme feature to "fire" for these instances (it has value 1, all other lexeme features have value 0). The model also includes a bias weight (intercept) not shown here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we compare variants of HLR with other spaced repetition algorithms in the context of Duolingo. First, we evaluate methods against his- torical log data, and analyze trained model weights for insight. We then describe two controlled user experiments where we deployed HLR as part of the student model in the production system.  Constant ¯ p = 0.859 0.175 n/a n/a <ref type="table">Table 2</ref>: Evaluation results using historical log data (see text). Arrows indicate whether lower (↓) or higher (↑) scores are better. The best method for each metric is shown in bold, and statistically significant effects (p &lt; 0.001) are marked with *.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Historical Log Data Evaluation</head><p>We collected two weeks of Duolingo log data, containing 12.9 million student-word lesson and practice session traces similar to <ref type="table">Table 1</ref> (for all students in all courses). We then compared three categories of spaced repetition algorithms:</p><p>• Half-life regression (HLR), our model from §3.3. For ablation purposes, we consider four variants: with and without lexeme features (-lex), as well as with and without the half- life term in the loss function (-h).</p><p>• Leitner and Pimsleur, two established base- lines that are special cases of HLR, using fixed weights. See the Appendix for a deriva- tion of the model weights we used.</p><p>• Logistic regression (LR), a standard machine learning 6 baseline. We evaluate two variants: with and without lexeme features (-lex).</p><p>We used the first 1 million instances of the data to tune the parameters for our training algorithm. After trying a handful of values, we settled on λ = 0.1, α = 0.01, and learning rate η = 0.001. We used these same training parameters for HLR and LR experiments (the Leitner and Pimsleur models are fixed and do not require training). <ref type="bibr">6</ref> For LR models, we include the lag time x∆ as an addi- tional feature, since -unlike HLR -it isn't explicitly ac- counted for in the model. We experimented with polynomial and exponential transformations of this feature, as well, but found the raw lag time to work best. <ref type="table">Table 2</ref> shows the evaluation results on the full data set of 12.9 million instances, using the first 90% for training and remaining 10% for testing. We consider several different evaluation measures for a comprehensive comparison:</p><p>• Mean absolute error (MAE) measures how closely predictions resemble their observed outcomes: 1</p><formula xml:id="formula_7">D D i=1 |p − ˆ p Θ | i .</formula><p>Since the strength meters in Duolingo's interface are based on model predictions, we use MAE as a measure of prediction quality.</p><p>• Area under the ROC curve (AUC) -or the Wilcoxon rank-sum test -is a measure of ranking quality. Here, it represents the proba- bility that a model ranks a random correctly- recalled word as more likely than a random incorrectly-recalled word. Since our model is used to prioritize words for practice, we use AUC to help evaluate these rankings.</p><p>• Half-life correlation (COR h ) is the Spearman rank correlation betweenˆhbetweenˆ betweenˆh Θ and the alge- braic estimate h described in §3.3. We use this as another measure of ranking quality.</p><p>For all three metrics, HLR with lexeme tag fea- tures is the best (or second best) approach, fol- lowed closely by HLR -lex (no lexeme tags). In fact, these are the only two approaches with MAE lower than a baseline constant prediction of the av- erage recall rate in the training data <ref type="table">(Table 2</ref>, bot- tom row). These HLR variants are also the only methods with positive COR h , although this seems reasonable since they are the only two to directly optimize for it. While lexeme tag features made limited impact, the h term in the HLR loss func- tion is clearly important: MAE more than doubles without it, and the -h variants are generally worse than the other baselines on at least one metric.</p><p>As stated in §3.2, Leitner was the spaced repeti- tion algorithm used in Duolingo's production stu- dent model at the time of this study. The Leitner method did yield the highest AUC 7 values among the algorithms we tried. However, the top two HLR variants are not far behind, and they also re- duce MAE compared to Leitner by least 45%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lg. Word</head><p>Lexeme  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model Weight Analysis</head><p>In addition to better predictions, HLR can cap- ture the inherent difficulty of concepts that are en- coded in the feature set. The "easier" concepts take on positive weights (less frequent practice re- sulting from longer half-lifes), while the "harder" concepts take on negative weights (more frequent practice resulting from shorter half-lifes). <ref type="table" target="#tab_3">Table 3</ref> shows HLR model weights for sev- eral English, Spanish, French, and German lexeme tags. Positive weights are associated with cog- nates and words that are common, short, or mor- phologically simple to inflect; it is reasonable that these would be easier to recall correctly. Negative weights are associated with irregular forms, rare words, and grammatical constructs like past or present participles and imperfective aspect. These model weights can provide insight into the aspects of language that are more or less challenging for students of a second language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Daily Retention Activity</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment</head><p>Any Lesson Practice I. HLR (v. Leitner) +0.3 +0.3 -7.3* II. HLR -lex (v. HLR) +12.0* +1.7* +9.5* <ref type="table">Table 4</ref>: Change (%) in daily student retention for controlled user experiments. Statistically signifi- cant effects (p &lt; 0.001) are marked with *.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">User Experiment I</head><p>The evaluation in §4.1 suggests that HLR is a bet- ter approach than the Leitner algorithm originally used by Duolingo (cutting MAE nearly in half). To see what effect, if any, these gains have on ac- tual student behavior, we ran controlled user ex- periments in the Duolingo production system.</p><p>We randomly assigned all students to one of two groups: HLR (experiment) or Leitner (con- trol). The underlying spaced repetition algorithm determined strength meter values in the skill tree (e.g., <ref type="figure" target="#fig_0">Figure 1(a)</ref>) as well as the ranking of target words for practice sessions (e.g., <ref type="figure" target="#fig_0">Figure 1(b)</ref>), but otherwise the two conditions were identical. The experiment lasted six weeks and involved just un- der 1 million students.</p><p>For evaluation, we examined changes in daily retention: what percentage of students who en- gage in an activity return to do it again the fol- lowing day? We used three retention metrics: any activity (including contributions to crowdsourced translations, online forum discussions, etc.), new lessons, and practice sessions.</p><p>Results are shown in the first row of <ref type="table">Table 4</ref>. The HLR group showed a slight increase in overall activity and new lessons, but a significant decrease in practice. Prior to the experiment, many stu- dents claimed that they would practice instead of learning new material "just to keep the tree gold," but that practice sessions did not review what they thought they needed most. This drop in practice -plus positive anecdotal feedback about stength meter quality from the HLR group -led us to believe that HLR was actually better for student engagement, so we deployed it for all students.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">User Experiment II</head><p>Several months later, active students pointed out that particular words or skills would decay rapidly, regardless of how often they practiced. Upon closer investigation, these complaints could be traced to lexeme tag features with highly negative weights in the HLR model (e.g., <ref type="table" target="#tab_3">Table 3</ref>). This im- plied that some feature-based overfitting had oc- curred, despite the L 2 regularization term in the training procedure. Duolingo was also preparing to launch several new language courses at the time, and no training data yet existed to fit lexeme tag feature weights for these new languages.</p><p>Since the top two HLR variants were virtually tied in our §4.1 experiments, we hypothesized that using interaction features alone might alleviate both student frustration and the "cold-start" prob- lem of training a model for new languages. In a follow-up experiment, we randomly assigned all students to one of two groups: HLR -lex (experi- ment) and HLR (control). The experiment lasted two weeks and involved 3.3 million students.</p><p>Results are shown in the second row of Ta- ble 4. All three retention metrics were signifi- cantly higher for the HLR -lex group. The most substantial increase was for any activity, although recurring lessons and practice sessions also im- proved (possibly as a byproduct of the overall ac- tivity increase). Anecdotally, vocal students from the HLR -lex group who previously complained about rapid decay under the HLR model were also positive about the change.</p><p>We deployed HLR -lex for all students, and be- lieve that its improvements are at least partially re- sponsible for the consistent 5% month-on-month growth in active Duolingo users since the model was launched.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Other Related Work</head><p>Just as we drew upon the theories of Ebbinghaus to derive HLR as an empirical spaced repetition model, there has been other recent work drawing on other (but related) theories of memory.</p><p>ACT-R ( <ref type="bibr" target="#b0">Anderson et al., 2004</ref>) is a cognitive architecture whose declarative memory module 8 takes the form of a power function, in contrast to the exponential form of the Ebbinghaus model and HLR. <ref type="bibr" target="#b20">Pavlik and Anderson (2008)</ref> used ACT-R predictions to optimize a practice schedule for second-language vocabulary, although their set- ting was quite different from ours. They assumed fixed intervals between practice exercises within the same laboratory session, and found that they could improve short-term learning within a ses-sion. In contrast, we were concerned with mak- ing accurate recall predictions between multiple sessions "in the wild" on longer time scales. Ev- idence also suggests that manipulation between sessions can have greater impact on long-term learning ( <ref type="bibr" target="#b6">Cepeda et al., 2006</ref>).</p><p>Motivated by long-term learning goals, the mul- tiscale context model (MCM) has also been pro- posed <ref type="bibr" target="#b19">(Mozer et al., 2009)</ref>. MCM combines two modern theories of the spacing effect ( <ref type="bibr" target="#b25">Staddon et al., 2002;</ref><ref type="bibr" target="#b23">Raaijmakers, 2003)</ref>, assuming that each time an item is practiced it creates an additional item-specific forgetting curve that decays at a dif- ferent rate. Each of these forgetting curves is ex- ponential in form (similar to HLR), but are com- bined via weighted average, which approximates a power law (similar to ACT-R). The authors were able to fit models to controlled laboratory data for second-language vocabulary and a few other memory tasks, on times scales up to several months. We were unaware of MCM at the time of our work, and it is unclear if the additional compu- tational overhead would scale to Duolingo's pro- duction system. Nevertheless, comparing to and integrating with these ideas is a promising direc- tion for future work.</p><p>There has also been work on more heuris- tic spaced repetition models, such as Super- Memo <ref type="bibr" target="#b30">(Wo´zniakWo´zniak, 1990</ref>). Variants of this algo- rithm are popular alternatives to Leitner in some flashcard software, leveraging additional parame- ters with complex interactions to determine spac- ing intervals for practice. To our knowledge, these additional parameters are hand-picked as well, but one can easily imagine fitting them empirically to real student log data, as we do with HLR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have introduced half-life regression (HLR), a novel spaced repetition algorithm with applica- tions to second language acquisition. HLR com- bines a psycholinguistic model of human mem- ory with modern machine learning techniques, and generalizes two popular algorithms used in lan- guage learning technology: Leitner and Pimsleur. We can do this by incorporating arbitrarily rich features and fitting their weights to data. This ap- proach is significantly more accurate at predict- ing student recall rates than either of the previous methods, and is also better than a conventional ma- chine learning approach like logistic regression.</p><p>One result we found surprising was that lexeme tag features failed to improve predictions much, and in fact seemed to frustrate the student learn- ing experience due to over-fitting. Instead of the sparse indicator variables used here, it may be bet- ter to decompose lexeme tags into denser and more generic features of tag components 9 (e.g., part of speech, tense, gender, case), and also use corpus frequency, word length, etc. This representation might be able to capture useful and interesting reg- ularities without negative side-effects.</p><p>Finally, while we conducted a cursory analy- sis of model weights in §4.2, an interesting next step would be to study such weights for even deeper insight. (Note that using lexeme tag com- ponent features, as suggested above, should make this anaysis more robust since features would be less sparse.) For example, one could see whether the ranking of vocabulary and/or grammar compo- nents by feature weight is correlated with external standards such as the CEFR <ref type="bibr" target="#b7">(Council of Europe, 2001</ref>). This and other uses of HLR hold the poten- tial to transform data-driven curriculum design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data and Code</head><p>To faciliatate research in this area, we have pub- licly released our data set and code from §4.1: https://github.com/duolingo/halflife-regression.  hidden Markov model (HMM) to determine which tag is correct in a given context. Consider the following two Spanish sentences: 'Yo como manzanas' ('I eat apples') and 'Corro como el viento' ('I run like the wind'). For both sentences, the FST parses the word como into the lexeme tag candidates comer.V.PRES.P1.SG ([I] eat) and como.ADV.CNJ (like/as). The HMM then disambiguates between the respective tags for each sentence. <ref type="table" target="#tab_5">Table 5</ref> contains a reference of the abbreviations used in this paper for lexeme tags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Pimsleur and Leitner Models</head><p>As mentioned in §3.3, the Pimsleur and Leitner algorithms are special cases of HLR using fixed, hand-picked weights. To see this, consider the original practice interval schedule used by <ref type="bibr" target="#b21">Pimsleur (1967)</ref>: 5 sec, 25 sec, 2 min, 10 min, 1 hr, 5 hr, 1 day, 5 days, 25 days, 4 months, and 2 years. If we interpret this as a sequence ofˆhofˆ ofˆh Θ half-lifes (i.e., students should practice whenˆpwhenˆ whenˆp Θ = 0.5), we can rewrite (2) and solve for log 2 ( ˆ h Θ ) as a linear equation. This yields Θ = {x n : 2.4, x b : -16.5}, where x n and x b are the number of practices and a bias weight (intercept), respectively. This model perfectly reconstructs Pimsleur's original schedule in days (r 2 = 0.999, p 0.001). Analyzing the Leitner variant from <ref type="figure" target="#fig_2">Figure 3</ref> is even simpler: this corresponds to Θ = {x ⊕ : 1, x : -1}, where x ⊕ is the number of past correct responses (i.e., dou- bling the interval), and x is the number of incor- rect responses (i.e., halving the interval).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Training and Optimization Details</head><p>The complete objective function given in §3.3 for half-life regression is:</p><formula xml:id="formula_8">(p, ∆, x; Θ) = (p − ˆ p Θ ) 2 + α(h − ˆ h Θ ) 2 + λΘ 2 2 .</formula><p>Substituting <ref type="formula" target="#formula_0">(1)</ref> and <ref type="formula" target="#formula_1">(2)</ref> into this equation produces the following more explicit formulation:</p><formula xml:id="formula_9">(p, ∆, x; Θ) = p − 2 − ∆ 2 Θ·x 2 + α −∆ log 2 (p) − 2 Θ·x 2 + λΘ 2 2 .</formula><p>In general, the search for Θ * weights to minimize cannot be solved in closed form, but since it is a smooth function, it can be optimized using gradi- ent methods. The partial gradient of with respect to each θ k weight is given by:</p><formula xml:id="formula_10">∂ ∂θ k = 2(ˆ p Θ − p) ln 2 (2)ˆ p Θ ∆ ˆ h Θ x k + 2αˆh 2αˆ 2αˆh Θ + ∆ log 2 (p) ln(2) ˆ h Θ x k + 2λθ k .</formula><p>In order to fit Θ to a large amount of student log data, we use AdaGrad ( <ref type="bibr" target="#b11">Duchi et al., 2011</ref>), an online algorithm for stochastic gradient descent (SGD). AdaGrad is typically less sensitive to the learning rate parameter η than standard SGD, by dynamically scaling each weight update as a func- tion of how often the corresponding feature ap- pears in the training data:</p><formula xml:id="formula_11">θ (+1) k := θ k − η c(x k ) − 1 2 ∂ ∂θ k .</formula><p>Here c(x k ) denotes the number of times feature x k has had a nonzero value so far in the SGD pass through the training data. This is useful for train- ing stability when using large, sparse feature sets (e.g., the lexeme tag features in this study). Note that to prevent computational overflow and under- flow errors, we boundˆpboundˆ boundˆp Θ ∈ [0.0001, 0.9999] andˆh andˆ andˆh Θ ∈ [15 min, 9 months] in practice.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Duolingo screenshots for an English-speaking student learning French (iPhone app, 2016). (a) A course skill tree: golden skills have four bars and are "at full strength," while other skills have fewer bars and are due for practice. (b) A skill screen detail (for the Gerund skill), showing which words are predicted to need practice. (c,d) Grading and explanations for a translation exercise.</figDesc><graphic url="image-1.png" coords="2,72.78,63.30,109.52,194.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The French sentence from Figure 1(c,d) and its lexeme tags. Tags encode the root lexeme, part of speech, and morphological components (tense, gender, person, etc.) for each word in the exercise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The Leitner System for flashcards.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 4: Forgetting curves. (a) Predicted recall rate as a function of lag time ∆ and half-life h = 1. (b) Example student-word learning trace over 30 days: marks the observed recall rate p for each practice session, and half-life regression aims to fit model predictionsˆppredictionsˆ predictionsˆp Θ (dashed lines) to these points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Model</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Lexeme tag weights for English (EN), 
Spanish (ES), French (FR), and German (DE). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 : Lexeme tag component abbreviations.</head><label>5</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> https://www.duolingo.com</note>

			<note place="foot" n="3"> If Duolingo cannot parse the precise nature of the mistake-e.g., because of a gross typographical error-it provides a &quot;diff&quot; of the student&apos;s response with the closest acceptable answer in the corpus (using Levenshtein distance).</note>

			<note place="foot" n="4"> In our setting, each data instance represents a full lession or practice session, which may include multiple exercises reviewing the same word. Thus p represents the proportion of times a word was recalled correctly in a particular session.</note>

			<note place="foot" n="5"> Note that in practice, we found that using the square root of interaction feature counts (e.g., √ x⊕) yielded better results than the raw counts shown here.</note>

			<note place="foot" n="7"> AUC of 0.5 implies random guessing (Fawcett, 2006), so the AUC values here may seem low. This is due in part to an inherently noisy prediction task, but also to a range restriction: ¯ p = 0.859, so most words are recalled correctly and predictions tend to be high. Note that all reported AUC values are statistically significantly better than chance using a Wilcoxon rank sum test with continuity correction.</note>

			<note place="foot" n="8"> Declarative (specifically semantic) memory is widely regarded to govern language vocabulary (Ullman, 2005).</note>

			<note place="foot" n="10"> The lexeme tag set is based on a large morphology dictionary created by the Apertium project (Forcada et al., 2011), which we supplemented with entries from Wiktionary (Wikimedia Foundation, 2002) and other sources. Each Duolingo course teaches about 3,000-5,000 lexeme tags.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Thanks to our collaborators at Duolingo, particu-larly Karin Tsai, Itai Hass, and André Horie for help gathering data from various parts of the sys-tem. We also thank the anonymous reviewers for suggestions that improved the final manuscript.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Lexeme Tagger Details</head><p>We use a lexeme tagger, introduced in §2, to ana- lyze and index the learning corpus and student re- sponses. Since Duolingo courses teach a moderate set of words and concepts, we do not necessarily need a complete, general-purpose, multi-lingual NLP stack. Instead, for each language we use a fi- nite state transducer (FST) to efficiently parse can- didate lexeme tags 10 for each word. We then use a</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An intergrated theory of mind</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bothell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Byrne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Douglass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Libiere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page" from="1036" to="1060" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Optimizing the learning of a second-language vocabulary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Atkinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="124" to="129" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Carroll</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Block</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Airasian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Bloom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mastery Learning: Theory and Practice</title>
		<imprint>
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rinehart</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winston</forename></persName>
		</author>
		<imprint>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">V.GER) is represented by an ID in the system. We used indicator variables in this work since the IDs are readily available; the overhead of retreiving all lexeme components would be inefficient in the production system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Engineering-Wise ; ˆ Etre</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>each lexeme tag. Of course, we could optimize for this if there were evidence of a significant improvement</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Effects of massed and distributed practice on the learning and retention of second language vocabulary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Bloom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Shuell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Educational Psychology</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="245" to="248" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Distributed practice in verbal recall tasks: A review and quantitative synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Cepeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pashler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Wixted</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rohrer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">354</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Common European Framework of Reference for Languages: Learning, Teaching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Council Of Europe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Assessment</title>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Implicit and explicit learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dekeyser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Handbook of Second Language Acquisition</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="313" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spacing effects and their implications for theory and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Dempster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Educational Psychology Review</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="309" to="330" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A metaanalytic review of the distribution of practice effect: Now you see it, now you don&apos;t</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Donovan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Radosevich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Psychology</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="795" to="805" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Memory: A Contribution to Experimental Psychology. Teachers College</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ebbinghaus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1885" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Columbia University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Media characteristics and online learning technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Fahy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Theory and Practice of Online Learning</title>
		<editor>T. Anderson and F. Elloumi</editor>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="137" to="171" />
		</imprint>
		<respStmt>
			<orgName>Athabasca University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An introduction to ROC analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fawcett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="861" to="874" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Apertium: A free/opensource platform for rule-based machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Forcada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ginestí-Rosell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nordfalk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>O&amp;apos;regan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ortiz-Rojas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Pérez-Ortiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sánchez-Martínez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ramírez-Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Tyers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Translation</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="127" to="144" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Unesco</forename><surname>Itu</surname></persName>
		</author>
		<title level="m">The state of broadband 2015</title>
		<imprint>
			<date type="published" when="2015-09" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">So lernt man lernen. Angewandte Lernpsychologie-ein Weg zum Erfolg</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leitner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1972" />
			<publisher>Verlag Herder</publisher>
			<pubPlace>Freiburg im Breisgau, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The situation with respect to the spacing of repetitions and memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Melton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Verbal Learning and Verbal Behavior</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="596" to="606" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Predicting the optimal spacing of study: A multiscale context model of memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Mozer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pashler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cepeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Lindsey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1321" to="1329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Using a model to compute the optimal schedule of practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">I</forename><surname>Pavlik</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Applied</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="101" to="117" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A memory schedule</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pimsleur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Modern Language Journal</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="73" to="75" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pinon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Haydon</surname></persName>
		</author>
		<title level="m">The benefits of the English language for individuals and societies: Quantitative indicators from Cameroon</title>
		<meeting><address><addrLine>Nigeria, Rwanda, Bangladesh and Pakistan</addrLine></address></meeting>
		<imprint>
			<publisher>Euromonitor International for the British Council</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Spacing and repetition effects in human memory: Application of the sam model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G W</forename><surname>Raaijmakers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="431" to="452" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Factors influencing the relative economy of massed and distributed practice in learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Ruth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="19" to="45" />
			<date type="published" when="1928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Habituation, memory and the brain: The dynamics of interval timing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E R</forename><surname>Staddon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">M</forename><surname>Chelaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Higa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioural Processes</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="71" to="88" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mixture modeling of individual learning curves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Streeter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Educational Data Mining (EDM)</title>
		<meeting>the International Conference on Educational Data Mining (EDM)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A cognitive neuroscience perspective on second language acquisition: The declarative/procedural model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adult Second Language Acquisition: Methods, Theory, and Practice</title>
		<editor>C. Sanz</editor>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">141</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Duolingo effectiveness study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vesselinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Grego</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<pubPlace>Queens College, City University of New York</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Wiktionary: A wikibased open content dictionary</title>
		<ptr target="https://www.wiktionary.org" />
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="2012" to="2015" />
		</imprint>
	</monogr>
	<note>Wikimedia Foundation</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Optimization of learning. Master&apos;s thesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Wo´zniakwo´zniak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
		<respStmt>
			<orgName>University of Technology in Pozna´nPozna´n</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
