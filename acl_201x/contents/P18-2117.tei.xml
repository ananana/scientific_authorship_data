<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On the Practical Computational Power of Finite Precision RNNs for Language Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gail</forename><surname>Weiss Technion</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Bar-Ilan University</orgName>
								<address>
									<country>Israel, Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Israel</forename><forename type="middle">Yoav</forename><surname>Goldberg</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Bar-Ilan University</orgName>
								<address>
									<country>Israel, Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Yahav</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Bar-Ilan University</orgName>
								<address>
									<country>Israel, Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">On the Practical Computational Power of Finite Precision RNNs for Language Recognition</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="740" to="745"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>740</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>While Recurrent Neural Networks (RNNs) are famously known to be Turing complete, this relies on infinite precision in the states and unbounded computation time. We consider the case of RNNs with finite precision whose computation time is linear in the input length. Under these limitations, we show that different RNN variants have different computational power. In particular, we show that the LSTM and the Elman-RNN with ReLU activation are strictly stronger than the RNN with a squashing activation and the GRU. This is achieved because LSTMs and ReLU-RNNs can easily implement counting behavior. We show empirically that the LSTM does indeed learn to effectively use the counting mechanism.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recurrent Neural Network (RNNs) emerge as very strong learners of sequential data. A famous re- sult by <ref type="bibr" target="#b15">Siegelmann and Sontag (1992;</ref><ref type="bibr" target="#b16">1994)</ref>, and its extension in <ref type="bibr" target="#b13">(Siegelmann, 1999)</ref>, demonstrates that an Elman-RNN <ref type="bibr" target="#b5">(Elman, 1990</ref>) with a sigmoid activation function, rational weights and infinite precision states can simulate a Turing-machine in real-time, making RNNs Turing-complete. Re- cently, <ref type="bibr" target="#b1">Chen et al (2017)</ref> extended the result to the ReLU activation function. However, these constructions (a) assume reading the entire in- put into the RNN state and only then perform- ing the computation, using unbounded time; and (b) rely on having infinite precision in the net- work states. As argued by <ref type="bibr" target="#b1">Chen et al (2017)</ref>, this is not the model of RNN computation used in NLP applications. Instead, RNNs are often used by feeding an input sequence into the RNN one item at a time, each immediately returning a state- vector that corresponds to a prefix of the sequence and which can be passed as input for a subse- quent feed-forward prediction network operating in constant time. The amount of tape used by a Turing machine under this restriction is linear in the input length, reducing its power to recogni- tion of context-sensitive language. More impor- tantly, computation is often performed on GPUs with 32bit floating point computation, and there is increasing evidence that competitive performance can be achieved also for quantized networks with 4-bit weights or fixed-point arithmetics <ref type="bibr" target="#b11">(Hubara et al., 2016)</ref>. The construction of <ref type="bibr" target="#b13">(Siegelmann, 1999</ref>) implements pushing 0 into a binary stack by the operation g ← g/4 + 1/4. This allows push- ing roughly 15 zeros before reaching the limit of the 32bit floating point precision. Finally, RNN solutions that rely on carefully orchestrated math- ematical constructions are unlikely to be found us- ing backpropagation-based training.</p><p>In this work we restrict ourselves to input- bound recurrent neural networks with finite- precision states (IBFP-RNN), trained using back- propagation. This class of networks is likely to co- incide with the networks one can expect to obtain when training RNNs for NLP applications. An IBFP Elman-RNN is finite state. But what about other RNN variants? In particular, we consider the Elman RNN (SRNN) <ref type="bibr" target="#b5">(Elman, 1990</ref>) with squash- ing and with ReLU activations, the Long Short- Term Memory (LSTM) <ref type="bibr" target="#b10">(Hochreiter and Schmidhuber, 1997</ref>) and the Gated Recurrent Unit (GRU) ( <ref type="bibr" target="#b4">Chung et al., 2014</ref>).</p><p>The common wisdom is that the LSTM and GRU introduce additional gating components that handle the vanishing gradients problem of train- ing SRNNs, thus stabilizing training and making it more robust. The LSTM and GRU are often con- sidered as almost equivalent variants of each other.</p><p>(a) a n b n -LSTM on a 1000 b <ref type="bibr">1000</ref> (b) a n b n c n -LSTM on a 100 b 100 c 100 (c) a n b n -GRU on a 1000 b 1000 (d) a n b n c n -GRU on a 100 b 100 c 100</p><p>Figure 1: Activations -c for LSTM and h for GRU -for networks trained on a n b n and a n b n c n . The LSTM has clearly learned to use an explicit counting mechanism, in contrast with the GRU.</p><p>We show that in the input-bound, finite- precision case, there is a real difference between the computational capacities of the LSTM and the GRU: the LSTM can easily perform unbounded counting, while the GRU (and the SRNN) can- not. This makes the LSTM a variant of a k-counter machine <ref type="bibr" target="#b6">(Fischer et al., 1968)</ref>, while the GRU re- mains finite-state. Interestingly, the SRNN with ReLU activation followed by an MLP classifier also has power similar to a k-counter machine.</p><p>These results suggest there is a class of formal languages that can be recognized by LSTMs but not by GRUs. In section 5, we demonstrate that for at least two such languages, the LSTM manages to learn the desired concept classes using back- propagation, while using the hypothesized control structure. <ref type="figure">Figure 1</ref> shows the activations of 10- d LSTM and GRU trained to recognize the lan- guages a n b n and a n b n c n . It is clear that the LSTM learned to dedicate specific dimensions for count- ing, in contrast to the GRU. 1 1 Is the ability to perform unbounded counting relevant to "real world" NLP tasks? In some cases it might be. For ex- ample, processing linearized parse trees ( <ref type="bibr" target="#b17">Vinyals et al., 2015;</ref><ref type="bibr" target="#b3">Choe and Charniak, 2016;</ref><ref type="bibr" target="#b0">Aharoni and Goldberg, 2017)</ref> re- quires counting brackets and nesting levels. Indeed, previous works that process linearized parse trees report using LSTMs</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The RNN Models</head><p>An RNN is a parameterized function R that takes as input an input vector x t and a state vector h t−1 and returns a state vector h t :</p><formula xml:id="formula_0">h t = R(x t , h t−1 ) (1)</formula><p>The RNN is applied to a sequence x 1 , ..., x n by starting with an initial vector h 0 (often the 0 vec- tor) and applying R repeatedly according to equa- tion (1). Let Σ be an input vocabulary (alphabet), and assume a mapping E from every vocabulary item to a vector x (achieved through a 1-hot encod- ing, an embedding layer, or some other means). Let RN N (x 1 , ..., x n ) denote the state vector h re- sulting from the application of R to the sequence E(x 1 ), ..., E(x n ). An RNN recognizer (or RNN acceptor) has an additional function f mapping states h to 0, 1. Typically, f is a log-linear classi- fier or multi-layer perceptron. We say that an RNN recognizes a language L⊆ Σ * if f (RN N (w)) re- turns 1 for all and only words w = x 1 , ..., x n ∈ L.</p><p>Elman-RNN (SRNN) In the Elman-RNN <ref type="bibr" target="#b5">(Elman, 1990</ref>), also called the Simple RNN (SRNN), and not GRUs for this purpose. Our work here suggests that this may not be a coincidence. the function R takes the form of an affine trans- form followed by a tanh nonlinearity:</p><formula xml:id="formula_1">h t = tanh(W x t + U h t−1 + b)<label>(2)</label></formula><p>Elman-RNNs are known to be at-least finite- state. <ref type="bibr" target="#b14">Siegelmann (1996)</ref> proved that the tanh can be replaced by any other squashing function with- out sacrificing computational power.</p><p>IRNN The IRNN model, explored by ( <ref type="bibr" target="#b12">Le et al., 2015)</ref>, replaces the tanh activation with a non- squashing ReLU:</p><formula xml:id="formula_2">h t = max(0, (W x t + U h t−1 + b))<label>(3)</label></formula><p>The computational power of such RNNs (given in- finite precision) is explored in <ref type="bibr" target="#b1">(Chen et al., 2017</ref>).</p><p>Gated Recurrent Unit (GRU) In the GRU ( ), the function R incorporates a gating mechanism, taking the form:</p><formula xml:id="formula_3">z t = σ(W z x t + U z h t−1 + b z ) (4) r t = σ(W r x t + U r h t−1 + b r ) (5) ˜ h t = tanh(W h x t + U h (r t • h t−1 ) + b h )(6) h t = z t • h t−1 + (1 − z t ) • ˜ h t<label>(7)</label></formula><p>Where σ is the sigmoid function and • is the Hadamard product (element-wise product).</p><p>Long Short Term Memory (LSTM) In the LSTM (Hochreiter and Schmidhuber, 1997), R uses a different gating component configuration:</p><formula xml:id="formula_4">f t = σ(W f x t + U f h t−1 + b f ) (8) i t = σ(W i x t + U i h t−1 + b i ) (9) o t = σ(W o x t + U o h t−1 + b o ) (10) ˜ c t = tanh(W c x t + U c h t−1 + b c ) (11) c t = f t • c t−1 + i t • ˜ c t (12) h t = o t • g(c t )<label>(13)</label></formula><p>where g can be either tanh or the identity.</p><p>Equivalences The GRU and LSTM are at least as strong as the SRNN: by setting the gates of the GRU to z t = 0 and r t = 1 we obtain the SRNN computation. Similarly by setting the LSTM gates to i t = 1,o t = 1, and f t = 0. This is easily achieved by setting the matrices W and U to 0, and the biases b to the (constant) desired gate values. Thus, all the above RNNs can recognize finite- state languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Power of Counting</head><p>Power beyond finite state can be obtained by in- troducing counters. Counting languages and k- counter machines are discussed in depth in <ref type="bibr" target="#b6">(Fischer et al., 1968)</ref>. When unbounded computa- tion is allowed, a 2-counter machine has Turing power. However, for computation bound by in- put length (real-time) there is a more interesting hierarchy. In particular, real-time counting lan- guages cut across the traditional Chomsky hierar- chy: real-time k-counter machines can recognize at least one context-free language (a n b n ), and at least one context-sensitive one (a n b n c n ). How- ever, they cannot recognize the context free lan- guage given by the grammar S → x|aSa|bSb (palindromes).</p><p>SKCM For our purposes, we consider a sim- plified variant of k-counter machines (SKCM). A counter is a device which can be incremented by a fixed amount (INC), decremented by a fixed amount (DEC) or compared to 0 (COMP0). In- formally, <ref type="bibr">2</ref> an SKCM is a finite-state automaton extended with k counters, where at each step of the computation each counter can be incremented, decremented or ignored in an input-dependent way, and state-transitions and accept/reject de- cisions can inspect the counters' states using COMP0. The results for the three languages dis- cussed above hold for the SKCM variant as well, with proofs provided in the supplementary mate- rial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RNNs as SKCMs</head><p>In what follows, we consider the effect on the state-update equations on a single dimension,</p><formula xml:id="formula_5">h t [j]. We omit the index [j] for readability.</formula><p>LSTM The LSTM acts as an SKCM by des- ignating k dimensions of the memory cell c t as counters. In non-counting steps, set i t = 0, f t = 1 through equations (8-9). In counting steps, the counter direction (+1 or -1) is set iñ c t (equation 11) based on the input x t and state h t−1 . The counting itself is performed in equation (12), af- ter setting i t = f t = 1. The counter can be reset to 0 by setting i t = f t = 0.</p><p>Finally, the counter values are exposed through h t = o t g(c t ), making it trivial to compare the counter's value to 0. <ref type="bibr">3</ref> We note that this implementation of the SKCM operations is achieved by saturating the activations to their boundaries, making it relatively easy to reach and maintain in practice.</p><p>SRNN The finite-precision SRNN cannot desig- nate unbounded counting dimensions.</p><p>The SRNN update equation is:</p><formula xml:id="formula_6">h t = tanh(W x + U h t−1 + b) h t [i] = tanh( dx j=1 W ij x[j] + d h j=1 U ij h t−1 [j] + b[i])</formula><p>By properly setting U and W, one can get cer- tain dimensions of h to update according to the value of x, by</p><formula xml:id="formula_7">h t [i] = tanh(h t−1 [i] + w i x + b[i]).</formula><p>However, this counting behavior is within a tanh activation. Theoretically, this means unbounded counting cannot be achieved without infinite pre- cision. Practically, this makes the counting behav- ior inherently unstable, and bounded to a relatively narrow region. While the network could adapt to set w to be small enough such that counting works for the needed range seen in training without over- flowing the tanh, attempting to count to larger n will quickly leave this safe region and diverge.</p><p>IRNN Finite-precision IRNNs can perform un- bounded counting conditioned on input symbols. This requires representing each counter as two di- mensions, and implementing INC as incrementing one dimension, DEC as incrementing the other, and COMP0 as comparing their difference to 0. In- deed, Appendix A in ( <ref type="bibr" target="#b1">Chen et al., 2017</ref>) provides concrete IRNNs for recognizing the languages a n b n and a n b n c n . This makes IBFP-RNN with <ref type="bibr">3</ref> Some further remarks on the LSTM: LSTM supports both increment and decrement in a single dimension. The counting dimensions in ct are exposed through a function g. For both g(x) = x and g(x) = tanh(x), it is trivial to do compare 0. Another operation of interest is compar- ing two counters (for example, checking the difference be- tween them). This cannot be reliably achieved with g(x) = tanh(x), due to the non-linearity and saturation properties of the tanh function, but is possible in the g(x) = x case. LSTM can also easily set the value of a counter to 0 in one step. The ability to set the counter to 0 gives slightly more power for real-time recognition, as discussed by <ref type="bibr" target="#b6">Fischer et al. (1968)</ref>.</p><p>Relation to known architectural variants: Adding peep- hole connections <ref type="bibr" target="#b8">(Gers and Schmidhuber, 2000</ref>) essentially sets g(x) = x and allows comparing counters in a stable way. Coupling the input and the forget gates (it = 1 − ft) ( <ref type="bibr" target="#b9">Greff et al., 2017</ref>) removes the single-dimension unbounded counting ability, as discussed for the GRU.</p><p>ReLU activation more powerful than IBFP-RNN with a squashing activation. Practically, ReLU- activated RNNs are known to be notoriously hard to train because of the exploding gradient problem.</p><p>GRU Finite-precision GRUs cannot implement unbounded counting on a given dimension. The tanh in equation <ref type="formula">(6)</ref> combined with the interpola- tion (tying z t and 1 − z t ) in equation <ref type="formula" target="#formula_3">(7)</ref> restricts the range of values in h to between -1 and 1, pre- cluding unbounded counting with finite precision. Practically, the GRU can learn to count up to some bound m seen in training, but will not generalize well beyond that. <ref type="bibr">4</ref> Moreover, simulating forms of counting behavior in equation <ref type="formula" target="#formula_3">(7)</ref> require consis- tently setting the gates z t , r t and the proposal˜hproposal˜ proposal˜h t to precise, non-saturated values, making it much harder to find and maintain stable solutions.</p><p>Summary We show that LSTM and IRNN can implement unbounded counting in dedicated counting dimensions, while the GRU and SRNN cannot. This makes the LSTM and IRNN at least as strong as SKCMs, and strictly stronger than the SRNN and the GRU. <ref type="bibr">5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>Can the LSTM indeed learn to behave as a k- counter machine when trained using backpropaga- tion? We show empirically that:</p><p>1. LSTMs can be trained to recognize a n b n and a n b n c n .</p><p>2. These LSTMs generalize to much higher n than seen in the training set (though not in- finitely so).</p><p>3. The trained LSTM learn to use the per- dimension counting mechanism.</p><p>4. The GRU can also be trained to recognize a n b n and a n b n c n , but they do not have clear <ref type="bibr">4</ref> One such mechanism could be to divide a given dimen- sion by k &gt; 1 at each symbol encounter, by setting zt = 1/k and˜htand˜ and˜ht = 0. Note that the inverse operation would not be implementable, and counting down would have to be realized with a second counter. <ref type="bibr">5</ref> One can argue that other counting mechanisms- involving several dimensions-are also possible. Intuitively, such mechanisms cannot be trained to perform unbounded counting based on a finite sample as the model has no means of generalizing the counting behavior to dimensions beyond those seen in training. We discuss this more in depth in the supplementary material, where we also prove that an SRNN cannot represent a binary counter. counting dimensions, and they generalize to much smaller n than the LSTMs, often fail- ing to generalize correctly even for n within their training domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Trained LSTM networks outperform trained</head><p>GRU networks on random test sets for the languages a n b n and a n b n c n .</p><p>Similar empirical observations regarding the ability of the LSTM to learn to recognize a n b n and a n b n c n are described also in <ref type="bibr" target="#b7">(Gers and Schmidhuber, 2001</ref>).</p><p>We train 10-dimension, 1-layer LSTM and GRU networks to recognize a n b n and a n b n c n . For a n b n the training samples went up to n = 100 and for a n b n c n up to n = 50. 6</p><p>Results On a n b n , the LSTM generalizes well up to n = 256, after which it accumulates a devia- tion making it reject a n b n but recognize a n b n+1 for a while, until the deviation grows. <ref type="bibr">7</ref> The GRU does not capture the desired concept even within its training domain: accepting a n b n+1 for n &gt; 38, and also accepting a n b n+2 for n &gt; 97. It stops accepting a n b n for n &gt; 198.</p><p>On a n b n c n the LSTM recognizes well until n = 100. It then starts accepting also a n b n+1 c n . At n &gt; 120 it stops accepting a n b n c n and switches to accepting a n b n+1 c n , until at some point the de- viation grows. The GRU accepts already a 9 b 10 c 12 , and stops accepting a n b n c n for n &gt; 63. <ref type="figure">Figure 1a</ref> plots the activations of the 10 dimen- sions of the a n b n -LSTM for the input a 1000 b 1000 . While the LSTM misclassifies this example, the use of the counting mechanism is clear. <ref type="figure">Fig- ure 1b</ref> plots the activation for the a n b n c n LSTM on a 100 b 100 c 100 . Here, again, the two counting dimensions are clearly identified-indicating the LSTM learned the canonical 2-counter solution- although the slightly-imprecise counting also starts to show. In contrast, <ref type="figure">Figures 1c and 1d</ref> show the state values of the GRU-networks. The GRU behavior is much less interpretable than the LSTM. In the a n b n case, some dimensions may be performing counting within a bounded range, but move to erratic behavior at around t = 1750 (the network starts to misclassify on sequences much shorter than that). The a n b n c n state dynamics are even less interpretable.</p><p>Finally, we created 1000-sample test sets for each of the languages. For a n b n we used words with the form a n+i b n+j where n ∈ rand(0, 200) and i, j ∈ rand(−2, 2), and for a n b n c n we use words of the form a n+i b n+j c n+k where n ∈ rand(0, 150) and i, j, k ∈ rand(−2, 2). The LSTM's accuracy was 100% and 98.6% on a n b n and a n b n c n respectively, as opposed to the GRU's 87.0% and 86.9%, also respectively.</p><p>All of this empirically supports our result, showing that IBFP-LSTMs can not only theoret- ically implement "unbounded" counters, but also learn to do so in practice (although not perfectly), while IBFP-GRUs do not manage to learn proper counting behavior, even when allowing floating point computations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We show that the IBFP-LSTM can model a real- time SKCM, both in theory and in practice. This makes it more powerful than the IBFP-SRNN and the IBFP-GRU, which cannot implement un- bounded counting and are hence restricted to rec- ognizing regular languages. The IBFP-IRNN can also perform input-dependent counting, and is thus more powerful than the IBFP-SRNN.</p><p>We note that in addition to theoretical distinc- tions between architectures, it is important to con- sider also the practicality of different solutions: how easy it is for a given architecture to discover and maintain a stable behavior in practice. We leave further exploration of this question for future work.</p></div>
			<note place="foot" n="2"> Formal definition is given in the supplementary material.</note>

			<note place="foot" n="6"> Implementation in DyNet, using the SGD Optimizer. Positive examples are generated by sampling n in the desired range. For negative examples we sample 2 or 3 n values independently, and ensuring at least one of them differs from the others. We dedicate a portion of the examples as the dev set, and train up to 100% dev set accuracy. 7 These fluctuations occur as the networks do not fully saturate their gates, meaning the LSTM implements an imperfect counter that accumulates small deviations during computation, e.g.: increasing the counting dimension by 0.99 but decreasing only by 0.98. Despite this, we see that the its solution remains much more robust than that found by the GRU-the LSTM has learned the essence of the counting based solution, but its implementation is imprecise.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The research leading to the results presented in this paper is supported by the European Union's Seventh Framework Programme (FP7) under grant agreement no. 615688 (PRIME), The Israeli Sci-ence Foundation (grant number 1555/15), and The Allen Institute for Artificial Intelligence.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards string-to-tree neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roee</forename><surname>Aharoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="132" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Recurrent neural networks as weighted language recognizers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sorcha</forename><surname>Gilroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename></persName>
		</author>
		<idno>abs/1711.05408</idno>
		<imprint>
			<date type="published" when="2017-05" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Parsing as language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kook</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2331" to="2336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<title level="m">Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Finding Structure in Time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Counter machines and counter languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><forename type="middle">R</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold</forename><forename type="middle">L</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rosenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical systems theory</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="265" to="283" />
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Lstm recurrent networks learn simple context-free and contextsensitive languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1333" to="1340" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recurrent nets that time and count</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEEINNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium</title>
		<meeting>the IEEEINNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="189" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Lstm: A search space odyssey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Koutnk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Steunebrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2222" to="2232" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Binarized neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4107" to="4115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00941</idno>
		<title level="m">A Simple Way to Initialize Recurrent Networks of Rectified Linear Units</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hava</forename><surname>Siegelmann</surname></persName>
		</author>
		<title level="m">Neural Networks and Analog Computation: Beyond the Turing Limit</title>
		<imprint>
			<publisher>Birkhäuser Basel</publisher>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
	<note>1 edition</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recurrent neural networks and finite automata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Siegelmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="567" to="574" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On the computational power of neural nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><forename type="middle">D</forename><surname>Siegelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Annual ACM Conference on Computational Learning Theory, COLT 1992</title>
		<meeting>the Fifth Annual ACM Conference on Computational Learning Theory, COLT 1992<address><addrLine>Pittsburgh, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992-07-27" />
			<biblScope unit="page" from="440" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Analog computation via neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><forename type="middle">D</forename><surname>Siegelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theor. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="331" to="360" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems</title>
		<meeting>the 28th International Conference on Neural Information Processing Systems<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2773" to="2781" />
		</imprint>
	</monogr>
	<note>NIPS&apos;15</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
