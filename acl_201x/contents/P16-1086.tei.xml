<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:27+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Text Understanding with the Attention Sum Reader Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Kadlec</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM</orgName>
								<address>
									<addrLine>Watson V Parku 4</addrLine>
									<settlement>Prague</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Schmid</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM</orgName>
								<address>
									<addrLine>Watson V Parku 4</addrLine>
									<settlement>Prague</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bajgar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM</orgName>
								<address>
									<addrLine>Watson V Parku 4</addrLine>
									<settlement>Prague</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kleindienst</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM</orgName>
								<address>
									<addrLine>Watson V Parku 4</addrLine>
									<settlement>Prague</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Text Understanding with the Attention Sum Reader Network</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="908" to="918"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Several large cloze-style context-question-answer datasets have been introduced recently: the CNN and Daily Mail news data and the Children&apos;s Book Test. Thanks to the size of these datasets, the associated text comprehension task is well suited for deep-learning techniques that currently seem to outperform all alternative approaches. We present a new, simple model that uses attention to directly pick the answer from the context as opposed to computing the answer using a blended representation of words in the document as is usual in similar models. This makes the model particularly suitable for question-answering problems where the answer is a single word from the document. Ensemble of our models sets new state of the art on all evaluated datasets.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Most of the information humanity has gathered up to this point is stored in the form of plain text. Hence the task of teaching machines how to un- derstand this data is of utmost importance in the field of Artificial Intelligence. One way of testing the level of text understanding is simply to ask the system questions for which the answer can be in- ferred from the text. A well-known example of a system that could make use of a huge collection of unstructured documents to answer questions is for instance IBM's Watson system used for the Jeop- ardy challenge <ref type="bibr" target="#b5">(Ferrucci et al., 2010)</ref>.</p><p>Cloze style questions <ref type="bibr" target="#b17">(Taylor, 1953)</ref>, i.e. ques- tions formed by removing a phrase from a sen- tence, are an appealing form of such questions (for example see <ref type="figure">Figure 1</ref>). While the task is easy to evaluate, one can vary the context, the question Document: What was supposed to be a fantasy sports car ride at Walt Disney World Speedway turned deadly when a Lamborghini crashed into a guardrail. The crash took place Sunday at the Exotic Driving Experi- ence, which bills itself as a chance to drive your dream car on a racetrack. The Lamborghini's passenger, 36- year-old Gary Terry of Davenport, Florida, died at the scene, Florida Highway Patrol said. The driver of the Lamborghini, 24-year-old Tavon Watson of Kissimmee, Florida, lost control of the vehicle, the Highway Patrol said. (...)</p><p>Question: Officials say the driver, 24-year-old Tavon Watson, lost control of a Answer candidates: Tavon Watson, Walt Disney World Speedway, Highway Patrol, Lamborghini, Florida, (...)</p><p>Answer: Lamborghini <ref type="figure">Figure 1</ref>: Each example consists of a context document, question, answer cadidates and, in the training data, the correct answer. This example was taken from the CNN dataset ( <ref type="bibr" target="#b6">Hermann et al., 2015)</ref>. Anonymization of this example that makes the task harder is shown in <ref type="table" target="#tab_4">Table 3.</ref> sentence or the specific phrase missing in the ques- tion to dramatically change the task structure and difficulty.</p><p>One way of altering the task difficulty is to vary the word type being replaced, as in ( <ref type="bibr" target="#b7">Hill et al., 2015)</ref>. The complexity of such variation comes from the fact that the level of context understand- ing needed in order to correctly predict different types of words varies greatly. While predicting prepositions can easily be done using relatively simple models with very little context knowledge, predicting named entities requires a deeper under- standing of the context.</p><p>Also, as opposed to selecting a random sentence from a text (as done in <ref type="bibr" target="#b7">(Hill et al., 2015)</ref>), the questions can be formed from a specific part of a document, such as a short summary or a list of   <ref type="bibr" target="#b6">Hermann et al., 2015)</ref> and the statistics provided with the CBT data set.</p><p>tags. Since such sentences often paraphrase in a condensed form what was said in the text, they are particularly suitable for testing text compre- hension ( <ref type="bibr" target="#b6">Hermann et al., 2015</ref>). An important property of cloze style questions is that a large amount of such questions can be au- tomatically generated from real world documents. This opens the task to data-hungry techniques such as deep learning. This is an advantage com- pared to smaller machine understanding datasets like MCTest ( <ref type="bibr" target="#b13">Richardson et al., 2013</ref>) that have only hundreds of training examples and therefore the best performing systems usually rely on hand- crafted features <ref type="bibr" target="#b14">(Sachan et al., 2015;</ref><ref type="bibr" target="#b10">Narasimhan and Barzilay, 2015)</ref>.</p><p>In the first part of this article we introduce the task at hand and the main aspects of the relevant datasets. Then we present our own model to tackle the problem. Subsequently we compare the model to previously proposed architectures and finally describe the experimental results on the perfor- mance of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task and datasets</head><p>In this section we introduce the task that we are seeking to solve and relevant large-scale datasets that have recently been introduced for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Formal Task Description</head><p>The task consists of answering a cloze style ques- tion, the answer to which is dependent on the un- derstanding of a context document provided with the question. The model is also provided with a set of possible answers from which the correct one is to be selected. This can be formalized as follows:</p><p>The training data consist of tuples (q, d, a, A), where q is a question, d is a document that con- tains the answer to question q, A is a set of possi- ble answers and a ∈ A is the ground truth answer. Both q and d are sequences of words from vocab- ulary V . We also assume that all possible answers are words from the vocabulary, that is A ⊆ V , and that the ground truth answer a appears in the doc- ument, that is a ∈ d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Datasets</head><p>We will now briefly summarize important features of the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">News Articles -CNN and Daily Mail</head><p>The first two datasets 1 ( <ref type="bibr" target="#b6">Hermann et al., 2015)</ref> were constructed from a large number of news articles from the CNN and Daily Mail websites. The main body of each article forms a context, while the cloze style question is formed from one of short highlight sentences, appearing at the top of each article page. Specifically, the question is created by replacing a named entity from the summary sentence (e.g. "Producer X will not press charges against Jeremy Clarkson, his lawyer says.").</p><p>Furthermore the named entities in the whole dataset were replaced by anonymous tokens which were further shuffled for each example so that the model cannot build up any world knowledge about the entities and hence has to genuinely rely on the context document to search for an answer to the question.</p><p>Qualitative analysis of reasoning patterns needed to answer questions in the CNN dataset to- gether with human performance on this task are provided in <ref type="bibr" target="#b2">(Chen et al., 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Children's Book Test</head><p>The third dataset 2 , the Children's Book Test (CBT) ( <ref type="bibr" target="#b7">Hill et al., 2015)</ref>, is built from books that are freely available thanks to Project Gutenberg 3 . Each context document is formed by 20 consecu- tive sentences taken from a children's book story. Due to the lack of summary, the cloze style ques- tion is then constructed from the subsequent (21 st ) sentence.</p><p>One can also see how the task complexity varies with the type of the omitted word (named entity, common noun, verb, preposition). ( <ref type="bibr" target="#b7">Hill et al., 2015)</ref> have shown that while standard LSTM lan- guage models have human level performance on predicting verbs and prepositions, they lack be- hind on named entities and common nouns. In this article we therefore focus only on predicting the first two word types.</p><p>Basic statistics about the CNN, Daily Mail and CBT datasets are summarized in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Model -Attention Sum Reader</head><p>Our model called the Attention Sum Reader (AS Reader) 4 is tailor-made to leverage the fact that the answer is a word from the context document. This is a double-edged sword. While it achieves state- of-the-art results on all of the mentioned datasets (where this assumption holds true), it cannot pro- duce an answer which is not contained in the doc- ument. Intuitively, our model is structured as fol- lows:</p><p>1. We compute a vector embedding of the query.</p><p>2. We compute a vector embedding of each indi- vidual word in the context of the whole doc- ument (contextual embedding).</p><p>3. Using a dot product between the question embedding and the contextual embedding of each occurrence of a candidate answer in the document, we select the most likely answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Formal Description</head><p>Our model uses one word embedding function and two encoder functions. The word embedding function e translates words into vector represen- tations. The first encoder function is a document encoder f that encodes every word from the doc- ument d in the context of the whole document. We call this the contextual embedding. For con- venience we will denote the contextual embedding of the i-th word in d as f i (d). The second encoder g is used to translate the query q into a fixed length representation of the same dimensionality as each f i (d). Both encoders use word embeddings com- puted by e as their input. Then we compute a weight for every word in the document as the dot product of its contextual embedding and the query embedding. This weight might be viewed as an attention over the document d.</p><p>To form a proper probability distribution over the words in the document, we normalize the weights using the softmax function. This way we model probability s i that the answer to query q appears at position i in the document d. In a func- tional form this is:</p><formula xml:id="formula_0">s i ∝ exp (f i (d) · g(q))<label>(1)</label></formula><p>Finally we compute the probability that word w is a correct answer as:</p><formula xml:id="formula_1">P (w|q, d) ∝ i∈I(w,d) s i (2)</formula><p>where I(w, d) is a set of positions where w ap- pears in the document d. We call this mechanism pointer sum attention since we use attention as a pointer over discrete tokens in the context docu- ment and then we directly sum the word's atten- tion across all the occurrences. This differs from the usual use of attention in sequence-to-sequence models (  where attention is used to blend representations of words into a new embedding vector. Our use of attention was in- spired by Pointer Networks (Ptr-Nets) ( <ref type="bibr" target="#b19">Vinyals et al., 2015)</ref>.</p><p>A high level structure of our model is shown in <ref type="figure">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model instance details</head><p>In our model the document encoder f is imple- mented as a bidirectional Gated Recurrent Unit (GRU) network ( <ref type="bibr" target="#b4">Chung et al., 2014</ref>) whose hidden states form the contextual word embeddings, that is </p><formula xml:id="formula_2">f i (d) = − → f i (d) || ← − f i (d),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Embeddings</head><p>Input text ….. í µí±(Obama) í µí± and í µí±(Putin) ….. í µí±(said) í µí±(Obama) í µí±(in) í µí±(Prague) í µí±(XXXXX) í µí± visited í µí±(Prague) <ref type="figure">Figure 2</ref>: Structure of the model.</p><p>... what was supposed to be a fantasy sports car ride at @entity3 turned deadly when a @entity4 crashed into a guardrail . the crash took place sunday at the @en- tity8 , which bills itself as a chance to drive your dream car on a racetrack . the @entity4 's passenger , 36 - year -old @entity14 of @entity15 , @entity16 , died at the scene , @entity13 said . the driver of the @entity4 , 24 -year -old @entity18 of @entity19 , @entity16 , lost control of the vehicle , the @entity13 said .</p><p>...</p><p>officials say the driver , 24 -year -old @entity18 , lost control of a <ref type="figure">Figure 3</ref>: Attention in an example with anonymized entities where our system selected the correct answer. Note that the attention is focused only on named entities.</p><p>← − f i denote forward and backward contextual em- beddings from the respective recurrent networks. The query encoder g is implemented by another bidirectional GRU network. This time the last hidden state of the forward network is concate- nated with the last hidden state of the backward network to form the query embedding, that is g(q) = − → g |q| (q) || ← − g 1 (q). The word embedding function e is implemented in a usual way as a look-up table V. V is a matrix whose rows can be indexed by words from the vocabulary, that is e(w) = V w , w ∈ V . Therefore, each row of V contains embedding of one word from the vocab- ulary. During training we jointly optimize param- eters of f , g and e.</p><p>... @entity11 film critic @entity29 writes in his review that "anyone nostalgic for childhood dreams of trans- formation will find something to enjoy in an uplifting movie that invests warm sentiment in universal themes of loss and resilience , experience and maturity . " more : the best and worst adaptations of "@entity" @entity43, @entity44 and @entity46 star in director @entity48's crime film about a hit man trying to save his estranged son from a revenge plot. @entity11 chief film critic @entity52 writes in his review that the film ... stars in crime film about hit man trying to save his estranged son <ref type="figure">Figure 4</ref>: Attention over an example where our system failed to select the correct answer (en- tity43). The system was probably mislead by the co-occurring word 'film'. Namely, entity11 occurs 7 times in the whole document and 6 times it is to- gether with the word 'film'. On the other hand, the correct answer occurs only 3 times in total and only once together with 'film'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Several recent deep neural network architec- tures ( <ref type="bibr" target="#b6">Hermann et al., 2015;</ref><ref type="bibr" target="#b7">Hill et al., 2015;</ref><ref type="bibr" target="#b2">Chen et al., 2016;</ref><ref type="bibr" target="#b9">Kobayashi et al., 2016</ref>) were applied to the task of text comprehension. The last two architectures were developed independently at the same time as our work. All of these architec- tures use an attention mechanism that allows them to highlight places in the document that might be relevant to answering the question. We will now briefly describe these architectures and compare them to our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Attentive and Impatient Readers</head><p>Attentive and Impatient Readers were proposed in ( <ref type="bibr" target="#b6">Hermann et al., 2015</ref>). The simpler Attentive Reader is very similar to our architecture. It also uses bidirectional document and query encoders to compute an attention in a similar way we do. The more complex Impatient Reader computes atten- tion over the document after reading every word of the query. However, empirical evaluation has shown that both models perform almost identically on the CNN and Daily Mail datasets.</p><p>The key difference between the Attentive Reader and our model is that the Attentive Reader uses attention to compute a fixed length repre- sentation r of the document d that is equal to a weighted sum of contextual embeddings of words in d, that is r = i s i f i (d). A joint query and document embedding m is then a non-linear func- tion of r and the query embedding g(q). This joint embedding m is in the end compared against all candidate answers a ∈ A using the dot product e(a ) · m, in the end the scores are normalized by softmax. That is: P (a |q, d) ∝ exp (e(a ) · m).</p><p>In contrast to the Attentive Reader, we select the answer from the context directly using the com- puted attention rather than using such attention for a weighted sum of the individual representations (see Eq. 2). The motivation for such simplifica- tion is the following.</p><p>Consider a context "A UFO was observed above our city in January and again in March." and question "An observer has spotted a UFO in ."</p><p>Since both January and March are equally good candidates, the attention mechanism might put the same attention on both these candidates in the con- text. The blending mechanism described above would compute a vector between the representa- tions of these two words and propose the clos- est word as the answer -this may well happen to be February (it is indeed the case for Word2Vec trained on Google News). By contrast, our model would correctly propose January or March.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Chen et al. 2016</head><p>A model presented in ( <ref type="bibr" target="#b2">Chen et al., 2016</ref>) is in- spired by the Attentive Reader. One difference is that the attention weights are computed with a bilinear term instead of simple dot-product, that</p><note type="other">is: s i ∝ exp (f i (d) W g(q)). The document em- bedding r is computed using a weighted sum as in the Attentive Reader: r</note><formula xml:id="formula_3">= i s i f i (d).</formula><p>In the end P (a |q, d) ∝ exp (e (a ) · r), where e is a new embedding function.</p><p>Even though it is a simplification of the Atten- tive Reader this model performs significantly bet- ter than the original.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Memory Networks</head><p>MemNNs ( <ref type="bibr">Weston et al., 2014</ref>) were applied to the task of text comprehension in <ref type="figure" target="#fig_2">(Hill et al., 2015)</ref>.</p><p>The best performing memory networks model setup -window memory -uses windows of fixed length (8) centered around the candidate words as memory cells. Due to this limited context window, the model is unable to capture dependencies out of scope of this window. Furthermore, the repre- sentation within such window is computed simply as the sum of embeddings of words in that win- dow. By contrast, in our model the representation of each individual word is computed using a recur- rent network, which not only allows it to capture context from the entire document but also the em- bedding computation is much more flexible than a simple sum.</p><p>To improve on the initial accuracy, a heuristic approach called self supervision is used in ( <ref type="bibr" target="#b7">Hill et al., 2015)</ref> to help the network to select the right supporting "memories" using an attention mechanism showing similarities to the ours. Plain MemNNs without this heuristic are not competi- tive on these machine reading tasks. Our model does not need any similar heuristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Dynamic Entity Representation</head><p>The Dynamic Entity Representation model ( <ref type="bibr" target="#b9">Kobayashi et al., 2016</ref>) has a com- plex architecture also based on the weighted attention mechanism and max pooling over contextual embeddings of vectors for each named entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Pointer Networks</head><p>Our model architecture was inspired by Ptr- Nets ( <ref type="bibr" target="#b19">Vinyals et al., 2015</ref>) in using an attention mechanism to select the answer in the context rather than to blend words from the context into an answer representation. While a Ptr-Net consists of an encoder as well as a decoder, which uses the at- tention to select the output at each step, our model outputs the answer in a single step. Furthermore, the pointer networks assume that no input in the sequence appears more than once, which is not the case in our settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Summary</head><p>Our model combines the best features of the ar- chitectures mentioned above. We use recurrent networks to "read" the document and the query as done in ( <ref type="bibr" target="#b6">Hermann et al., 2015;</ref><ref type="bibr" target="#b2">Chen et al., 2016;</ref><ref type="bibr" target="#b9">Kobayashi et al., 2016</ref>) and we use atten- tion in a way similar to Ptr-Nets. We also use summation of attention weights in a way similar to <ref type="bibr">MemNNs (Hill et al., 2015)</ref>.</p><p>From a high level perspective we simplify all the discussed text comprehension models by re- moving all transformations past the attention step. Instead we use the attention directly to compute the answer probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>In this section we evaluate our model on the CNN, Daily Mail and CBT datasets. We show that despite the model's simplicity its ensembles achieve state-of-the-art performance on each of these datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Training Details</head><p>To train the model we used stochastic gradient de- scent with the ADAM update rule ( <ref type="bibr" target="#b8">Kingma and Ba, 2015)</ref> and learning rate of 0.001 or 0.0005. During training we minimized the following neg- ative log-likelihood with respect to θ:</p><formula xml:id="formula_4">−logP θ (a|q, d)<label>(3)</label></formula><p>where a is the correct answer for query q and doc- ument d, and θ represents parameters of the en- coder functions f and g and of the word embed- ding function e. The optimized probability distri- bution P (a|q, d) is defined in Eq. 2. The initial weights in the word embedding ma- trix were drawn randomly uniformly from the interval [−0.1, 0.1]. Weights in the GRU net- works were initialized by random orthogonal ma- trices ( <ref type="bibr" target="#b15">Saxe et al., 2014</ref>) and biases were ini- tialized to zero. We also used a gradient clip- ping ( ) threshold of 10 and batches of size 32.</p><p>During training we randomly shuffled all exam- ples in each epoch. To speedup training, we al- ways pre-fetched 10 batches worth of examples and sorted them according to document length.</p><p>Hence each batch contained documents of roughly the same length.</p><p>For each batch of the CNN and Daily Mail datasets we randomly reshuffled the assignment of named entities to the corresponding word em- bedding vectors to match the procedure proposed in ( <ref type="bibr" target="#b6">Hermann et al., 2015)</ref>. This guaranteed that word embeddings of named entities were used only as semantically meaningless labels not en- coding any intrinsic features of the represented entities. This forced the model to truly deduce the answer from the single context document as- sociated with the question. We also do not use pre-trained word embeddings to make our training procedure comparable to ( <ref type="bibr" target="#b6">Hermann et al., 2015</ref>).</p><p>We did not perform any text pre-processing since the original datasets were already tokenized.</p><p>We do not use any regularization since in our experience it leads to longer training times of sin- gle models, however, performance of a model en- semble is usually the same. This way we can train the whole ensemble faster when using multiple GPUs for parallel training.</p><p>For Additional details about the training proce- dure see Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation Method</head><p>We evaluated the proposed model both as a single model and using ensemble averaging. Although the model computes attention for every word in the document we restrict the model to select an answer from a list of candidate answers associated with each question-document pair.</p><p>For single models we are reporting results for the best model as well as the average of accura- cies for the best 20% of models with best perfor- mance on validation data since single models dis- play considerable variation of results due to ran- dom weight initialization <ref type="bibr">5</ref>    the best performing model according to validation performance. Then in each step we tried adding the best performing model that had not been pre- viously tried. We kept it in the ensemble if it did improve its validation performance and discarded it otherwise. This way we gradually tried each model once. We call the resulting model a greedy ensemble.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head><p>Performance of our models on the CNN and Daily Mail datasets is summarized in <ref type="table" target="#tab_3">Table 2, Table 3</ref> shows results on the CBT dataset. The tables also list performance of other published models that were evaluated on these datasets. Ensembles of our models set the new state-of-the-art results on all evaluated datasets. <ref type="table" target="#tab_8">Table 4</ref> then measures accuracy as the pro- portion of test cases where the ground truth was among the top k answers proposed by the greedy ensemble model for k = 1, 2, 5.</p><p>CNN and Daily Mail. The CNN dataset is the most widely used dataset for evaluation of text comprehension systems published so far. Perfor-      <ref type="figure">Figure 7</ref>: Subfigure (a) shows the model accu- racy when the correct answer is the n th most fre- quent named entity for n ∈ <ref type="bibr">[1,</ref><ref type="bibr">10]</ref>. Subfigure (b) shows the number of test examples for which the correct answer was the n-th most frequent entity. The plots for CBT look almost identical (see Ap- pendix B). mance of our single model is a little bit worse than performance of simultaneously published models ( <ref type="bibr" target="#b2">Chen et al., 2016;</ref><ref type="bibr" target="#b9">Kobayashi et al., 2016)</ref>. Com- pared to our work these models were trained with Dropout regularization ( <ref type="bibr" target="#b16">Srivastava et al., 2014)</ref> which might improve single model performance. However, ensemble of our models outperforms these models even though they use pre-trained word embeddings.</p><p>On the CNN dataset our single model with best validation accuracy achieves a test accuracy of 69.5%. The average performance of the top 20% models according to validation accuracy is 69.9% which is even 0.5% better than the single best-validation model. This shows that there were many models that performed better on test set than the best-validation model. Fusing multiple models then gives a significant further increase in accu- racy on both CNN and Daily Mail datasets..</p><p>CBT. In named entity prediction our best single model with accuracy of 68.6% performs 2% abso- lute better than the MemNN with self supervision, the averaging ensemble performs 4% absolute bet- ter than the best previous result. In common noun prediction our single models is 0.4% absolute bet- ter than MemNN however the ensemble improves the performance to 69% which is 6% absolute bet- ter than MemNN.  </p><formula xml:id="formula_5">Dataset k = 1 k = 2 k = 5</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Analysis</head><p>To further analyze the properties of our model, we examined the dependence of accuracy on the length of the context document ( <ref type="figure" target="#fig_2">Figure 5</ref>), the number of candidate answers ( <ref type="figure">Figure 6</ref>) and the frequency of the correct answer in the context <ref type="figure">(Figure 7</ref>). On the CNN and Daily Mail datasets, the ac- curacy decreases with increasing document length <ref type="figure" target="#fig_2">(Figure 5a</ref>). We hypothesize this may be due to multiple factors. Firstly long documents may make the task more complex. Secondly such cases are quite rare in the training data <ref type="figure" target="#fig_2">(Figure 5b</ref>) which motivates the model to specialize on shorter con- texts. Finally the context length is correlated with the number of named entities, i.e. the number of possible answers which is itself negatively corre- lated with accuracy (see <ref type="figure">Figure 6</ref>).</p><p>On the CBT dataset this negative trend seems to disappear <ref type="figure" target="#fig_2">(Fig. 5c)</ref>. This supports the later two explanations since the distribution of document lengths is somewhat more uniform <ref type="figure" target="#fig_2">(Figure 5d</ref>) and the number of candidate answers is constant (10) for all examples in this dataset.</p><p>The effect of increasing number of candidate answers on the model's accuracy can be seen in <ref type="figure">Figure 6a</ref>. We can clearly see that as the num- ber of candidate answers increases, the accuracy drops. On the other hand, the amount of examples with large number of candidate answers is quite small <ref type="figure">(Figure 6b)</ref>.</p><p>Finally, since the summation of attention in our model inherently favours frequently occurring to- kens, we also visualize how the accuracy depends on the frequency of the correct answer in the doc- ument. <ref type="figure">Figure 7a</ref> shows that the accuracy signif- icantly drops as the correct answer gets less and less frequent in the document compared to other candidate answers. On the other hand, the correct answer is likely to occur frequently <ref type="figure">(Fig. 7a</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this article we presented a new neural network architecture for natural language text comprehen- sion. While our model is simpler than previously published models, it gives a new state-of-the-art accuracy on all the evaluated datasets.</p><p>An analysis by <ref type="bibr" target="#b2">(Chen et al., 2016)</ref> suggests that on CNN and Daily Mail datasets a significant pro- portion of questions is ambiguous or too difficult to answer even for humans (partly due to entity anonymization) so the ensemble of our models may be very near to the maximal accuracy achiev- able on these datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Training Details</head><p>During training we evaluated the model perfor- mance after each epoch and stopped the training when the error on the validation set started increas- ing.</p><p>The models usually converged after two epochs of training. Time needed to complete a single epoch of training on each dataset on an Nvidia K40 GPU is shown in <ref type="table">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Time per epoch CNN 10h 22min Daily Mail 25h 42min CBT Named Entity 1h 5min CBT Common Noun 0h 56min <ref type="table">Table 5</ref>: Average duration of one epoch of training on the four datasets.</p><p>The hyperparameters, namely the recurrent hid- den layer dimension and the source embedding di- mension, were chosen by grid search. We started with a range of 128 to 384 for both parameters and subsequently kept increasing the upper bound by 128 until we started observing a consistent de- crease in validation accuracy. The region of the parameter space that we explored together with the parameters of the model with best validation accu- racy are summarized in  <ref type="table" target="#tab_9">Table 6</ref>: Dimension of the recurrent hidden layer and of the source embedding for the best model and the range of values that we tested. We report number of hidden units of the unidirectional GRU; the bidirectional GRU has twice as many hidden units.</p><p>Our model was implemented using Theano ( <ref type="bibr" target="#b1">Bastien et al., 2012)</ref> and <ref type="bibr">Blocks (van Merrienboer et al., 2015</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B Dependence of accuracy on the frequency of the correct answer</head><p>In Section 6 we analysed how the test accuracy depends on how frequent the correct answer is compared to other answer candidates for the news datasets. The plots for the Children's Book Test looks very similar, however we are adding it here for completeness.   <ref type="figure">Figure 8</ref>: Subfigure (a) shows the model accu- racy when the correct answer is among n most fre- quent named entities for n ∈ <ref type="bibr">[1,</ref><ref type="bibr">10]</ref>. Subfigure (b) shows the number of test examples for which the correct answer was the n-th most frequent entity.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>CNN</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Sub-figures (a) and (b) plot the test accuracy against the length of the context document. The examples were split into ten buckets of equal size by their context length. Averages for each bucket are plotted on each axis. Sub-figures (c) and (d) show distributions of context lengths in the four datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>, even for identical hy- perparameter values. Single model performance may consequently prove difficult to reproduce. What concerns ensembles, we used simple aver- aging of the answer probabilities predicted by en- semble members. For ensembling we used 14, 16, 84 and 53 models for CNN, Daily Mail and CBT CN and NE respectively. The ensemble models were chosen either as the top 70% of all trained models, we call this avg ensemble. Alternatively we use the following algorithm: We started with</figDesc><table>CNN 

Daily Mail 

valid test valid 
test 

Attentive Reader  † 
61.6 63.0 70.5 
69.0 
Impatient Reader  † 
61.8 63.8 69.0 
68.0 

MemNNs (single model)  ‡ 
63.4 66.8 NA 
NA 
MemNNs (ensemble)  ‡ 
66.2 69.4 NA 
NA 

Dynamic Entity Repres. (max-pool) 
71.2 70.7 NA 
NA 
Dynamic Entity Repres. (max-pool + byway) 70.8 72.0 NA 
NA 
Dynamic Entity Repres. + w2v 
71.3 72.9 NA 
NA 

Chen et al. (2016) (single model) 
72.4 72.4 76.9 
75.8 

AS Reader (single model) 
68.6 69.5 75.0 
73.9 
AS Reader (avg for top 20%) 
68.4 69.9 74.5 
73.5 
AS Reader (avg ensemble) 
73.9 75.4 78.1 
77.1 
AS Reader (greedy ensemble) 
74.5 74.8 78.7 
77.7 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Results of our AS Reader on the CNN and Daily Mail datasets. Results for models marked 
with  † are taken from (Hermann et al., 2015), results of models marked with  ‡ are taken from (Hill et al., 
2015) and results marked with are taken from (Kobayashi et al., 2016). Performance of  ‡ and models 
was evaluated only on CNN dataset. 

Named entity 
Common noun 

valid 
test valid 
test 

Humans (query) ( * ) 
NA 
52.0 NA 
64.4 
Humans (context+query) ( * ) 
NA 
81.6 NA 
81.6 

LSTMs (context+query)  ‡ 
51.2 
41.8 62.6 
56.0 

MemNNs (window memory + self-sup.)  ‡ 70.4 
66.6 64.2 
63.0 

AS Reader (single model) 
73.8 
68.6 68.8 
63.4 
AS Reader (avg for top 20%) 
73.3 
68.4 67.7 
63.2 
AS Reader (avg ensemble) 
74.5 
70.6 71.1 
68.9 
AS Reader (greedy ensemble) 
76.2 
71.0 72.4 
67.5 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Results of our AS Reader on the CBT datasets. Results marked with  ‡ are taken from (Hill et 
al., 2015). ( * ) Human results were collected on 10% of the test set. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Proportion of test examples for which the 
top k answers proposed by the greedy ensemble 
included the correct answer. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="true"><head>Table 6 .</head><label>6</label><figDesc></figDesc><table>Rec. Hid. Layer 
Embedding 
Dataset 
min max best min max best 

CNN 
128 512 
384 128 512 128 
Daily Mail 128 1024 512 128 512 384 
CBT NE 
128 512 
384 128 512 384 
CBT CN 
128 1536 256 128 512 384 

</table></figure>

			<note place="foot" n="1"> The CNN and Daily Mail datasets are available at https://github.com/deepmind/rc-data</note>

			<note place="foot" n="2"> The CBT dataset is available at http://www. thespermwhale.com/jaseweston/babi/ CBTest.tgz 3 https://www.gutenberg.org/ 4 An implementation of AS Reader is available at https: //github.com/rkadlec/asreader</note>

			<note place="foot" n="5"> The standard deviation for models with the same hyperparameters was between 0.6-2.5% in absolute test accuracy.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would also like to thank Tim Klinger for pro-viding us with masked softmax code that we used in our implementation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Neural Machine Translation by Jointly Learning to Align and Translate. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Arnaud Bergeron, Nicolas Bouchard, and Yoshua Bengio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Thorough Examination of the CNN / Daily Mail Reading Comprehension Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning Phrase Representations using RNN EncoderDecoder for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Holger Schwenk, and Yoshua Bengio</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling. arXiv</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Building Watson: An Overview of the DeepQA Project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ferrucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Chu-Carroll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gondek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">A</forename><surname>Kalyanpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">William</forename><surname>Murdock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>Schlaefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Welty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="59" to="79" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1684" to="1692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The goldilocks principle: Reading children&apos;s books with explicit memory representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02301</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adam: a Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dynamic Entity Representation with Max-pooling Improves Machine Reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sosuke</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoaki</forename><surname>Okazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Association for Computational Linguistics and Human Language Technologies (NAACL-HLT)</title>
		<meeting>the North American Chapter of the Association for Computational Linguistics and Human Language Technologies (NAACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<title level="m">Machine Comprehension with Discourse Relations. Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<imprint>
			<publisher>Long Papers</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1253" to="1262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 30th International Conference on Machine Learning</title>
		<meeting>The 30th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Renshaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="193" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning AnswerEntailing Structures for Machine Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrinmaya</forename><surname>Sachan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinava</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="239" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">L</forename><surname>Andrew M Saxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dropout: prevent NN from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cloze procedure: a new tool for measuring readability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journalism and Mass Communication Quarterly</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">415</biblScope>
			<date type="published" when="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Blocks and Fuel : Frameworks for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bart Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2674" to="2682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1410.3916</idno>
		<imprint/>
	</monogr>
<note type="report_type">Bordes. 2014. Memory networks. arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
