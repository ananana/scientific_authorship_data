<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:27+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On the Automatic Generation of Medical Imaging Reports</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyu</forename><surname>Jing</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">† Petuum Inc</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">On the Automatic Generation of Medical Imaging Reports</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2577" to="2586"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Medical imaging is widely used in clinical practice for diagnosis and treatment. Report-writing can be error-prone for unexperienced physicians, and time-consuming and tedious for experienced physicians. To address these issues, we study the automatic generation of medical imaging reports. This task presents several challenges. First, a complete report contains multiple heterogeneous forms of information, including findings and tags. Second, abnormal regions in medical images are difficult to identify. Third, the reports are typically long, containing multiple sentences. To cope with these challenges , we (1) build a multi-task learning framework which jointly performs the prediction of tags and the generation of paragraphs , (2) propose a co-attention mechanism to localize regions containing abnormalities and generate narrations for them, (3) develop a hierarchical LSTM model to generate long paragraphs. We demonstrate the effectiveness of the proposed methods on two publicly available datasets.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Medical images, such as radiology and pathol- ogy images, are widely used in hospitals for the diagnosis and treatment of many diseases, such as pneumonia and pneumothorax. The reading and interpretation of medical images are usually conducted by specialized medical professionals. For example, radiology images are read by ra- diologists. They write textual reports <ref type="figure" target="#fig_0">(Figure 1</ref>) to narrate the findings regarding each area of the body examined in the imaging study, specifically whether each area was found to be normal, abnor- mal or potentially abnormal.</p><p>For less-experienced radiologists and patholo- gists, especially those working in the rural area where the quality of healthcare is relatively low, writing medical-imaging reports is demanding. For instance, to correctly read a chest x-ray im- age, the following skills are needed <ref type="bibr" target="#b1">(Delrue et al., 2011</ref>): (1) thorough knowledge of the normal anatomy of the thorax, and the basic physiology of chest diseases; (2) skills of analyzing the radio- graph through a fixed pattern; (3) ability of eval- uating the evolution over time; (4) knowledge of clinical presentation and history; (5) knowledge of the correlation with other diagnostic results (labo- ratory results, electrocardiogram, and respiratory function tests).</p><p>For experienced radiologists and pathologists, writing imaging reports is tedious and time- consuming. In nations with large population such as China, a radiologist may need to read hundreds of radiology images per day. Typing the findings of each image into computer takes about 5-10 min- utes, which occupies most of their working time. In sum, for both unexperienced and experienced medical professionals, writing imaging reports is unpleasant.</p><p>This motivates us to investigate whether it is possible to automatically generate medical image reports. Several challenges need to be addressed. First, a complete diagnostic report is comprised of multiple heterogeneous forms of information. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the report for a chest x- ray contains impression which is a sentence, find- ings which are a paragraph, and tags which are a list of keywords. Generating this heterogeneous information in a unified framework is technically demanding. We address this problem by building a multi-task framework, which treats the predic- tion of tags as a multi-label classification task, and treats the generation of long descriptions as a text generation task.</p><p>Second, how to localize image-regions and at- tach the right description to them are challeng- ing. We solve these problems by introducing a co-attention mechanism, which simultaneously at- tends to images and predicted tags and explores the synergistic effects of visual and semantic in- formation.</p><p>Third, the descriptions in imaging reports are usually long, containing multiple sentences. Gen- erating such long text is highly nontrivial. Rather than adopting a single-layer LSTM <ref type="bibr" target="#b8">(Hochreiter and Schmidhuber, 1997)</ref>, which is less capable of modeling long word sequences, we leverage the compositional nature of the report and adopt a hierarchical LSTM to produce long texts. Com- bined with the co-attention mechanism, the hierar- chical LSTM first generates high-level topics, and then produces fine-grained descriptions according to the topics.</p><p>Overall, the main contributions of our work are:</p><p>• We propose a multi-task learning framework which can simultaneously predict the tags and generate the text descriptions.</p><p>• We introduce a co-attention mechanism for localizing sub-regions in the image and gen- erating the corresponding descriptions.</p><p>• We build a hierarchical LSTM to generate long paragraphs.</p><p>• We perform extensive experiments to show the effectiveness of the proposed methods.</p><p>The rest of the paper is organized as follows. Section 2 reviews related works. Section 3 intro- duces the method. Section 4 present the experi- mental results and Section 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Textual labeling of medical images There have been several works aiming at attaching "texts" to medical images. In their settings, the target "texts" are either fully-structured or semi-structured (e.g. tags, templates), rather than natural texts. <ref type="bibr" target="#b11">Kisilev et al. (2015)</ref> build a pipeline to predict the at- tributes of medical images. <ref type="bibr" target="#b19">Shin et al. (2016)</ref> adopt a CNN-RNN based framework to predict tags (e.g. locations, severities) of chest x-ray images. The work closest to ours is recently contributed by , which aims at generating semi-structured pathology reports, whose contents are restricted to 5 predefined top- ics.</p><p>However, in the real-world, different physicians usually have different writing habits and different x-ray images will represent different abnormali- ties. Therefore, collecting semi-structured reports is less practical and thus it is important to build models to learn from natural reports. To the best of our knowledge, our work represents the first one that generates truly natural reports written by physicians, which are usually long and cover di- verse topics.</p><p>Image captioning with deep learning Image captioning aims at automatically generating text descriptions for given images. Most recent im- age captioning models are based on a CNN-RNN framework ( <ref type="bibr" target="#b22">Vinyals et al., 2015;</ref><ref type="bibr" target="#b5">Fang et al., 2015;</ref><ref type="bibr" target="#b9">Karpathy and Fei-Fei, 2015;</ref><ref type="bibr" target="#b23">Xu et al., 2015;</ref><ref type="bibr" target="#b24">You et al., 2016;</ref><ref type="bibr" target="#b12">Krause et al., 2017)</ref>.</p><p>Recently, attention mechanisms have been shown to be useful for image captioning ( <ref type="bibr" target="#b23">Xu et al., 2015;</ref><ref type="bibr" target="#b24">You et al., 2016)</ref>. <ref type="bibr" target="#b23">Xu et al. (2015)</ref> introduce a spatial-visual attention mechanism over image features extracted from intermediate layers of the CNN. <ref type="bibr" target="#b24">You et al. (2016)</ref> propose a semantic atten- tion mechanism over tags of given images. To bet- ter leverage both the visual features and semantic tags, we propose a co-attention mechanism for re- port generation.</p><p>Instead of only generating one-sentence caption for images, <ref type="bibr" target="#b12">Krause et al. (2017)</ref> and <ref type="bibr" target="#b13">Liang et al. (2017)</ref> generate paragraph captions using a hier- archical LSTM. Our method also adopts a hierar- chical LSTM for paragraph generation, but unlike <ref type="bibr" target="#b12">Krause et al. (2017)</ref>, we use a co-attention network to generate topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>A complete diagnostic report for a medical image is comprised of both text descriptions (long para- graphs) and lists of tags, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. We propose a multi-task hierarchical model with co- attention for automatically predicting keywords and generating long paragraphs. Given an image which is divided into regions, we use a CNN to learn visual features for these patches. Then these visual features are fed into a multi-label classifi- cation (MLC) network to predict the relevant tags.</p><p>In the tag vocabulary, each tag is represented by a word-embedding vector. Given the predicted tags for a specific image, their word-embedding vec- tors serve as the semantic features of this image. Then the visual features and semantic features are fed into a co-attention model to generate a context vector that simultaneously captures the visual and semantic information of this image. As of now, the encoding process is completed. Next, starting from the context vector, the de- coding process generates the text descriptions. The description of a medical image usually con- tains multiple sentences, and each sentence fo- cuses on one specific topic. Our model leverages this compositional structure to generate reports in a hierarchical way: it first generates a sequence of high-level topic vectors representing sentences, then generates a sentence from each topic vector. Specifically, the context vector is inputted into a sentence LSTM, which unrolls for a few steps and produces a topic vector at each step. A topic vector represents the semantics of a sentence to be gen- erated. Given a topic vector, the word LSTM takes it as input and generates a sequence of words to form a sentence. The termination of the unrolling process is controlled by the sentence LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Tag Prediction</head><p>The first task of our model is predicting the tags of the given image. We treat the tag prediction task as a multi-label classification task. Specifi- cally, given an image I, we first extract its features {v n } N n=1 ∈ R D from an intermediate layer of a CNN, and then feed {v n } N n=1 into a multi-label classification (MLC) network to generate a distri- bution over all of the L tags:</p><formula xml:id="formula_0">p l,pred (li = 1|{vn} N n=1 ) ∝ exp(MLCi({vn} N n=1 )) (1)</formula><p>where l ∈ R L is a tag vector, l i = 1/0 denote the presence and absence of the i-th tag respectively, and MLC i means the i-th output of the MLC net- work. For simplicity, we extract visual features from the last convolutional layer of the VGG-19 model <ref type="bibr" target="#b20">(Simonyan and Zisserman, 2014</ref>) and use the last two fully connected layers of VGG-19 for MLC.</p><p>Finally, the embeddings of the M most likely tags {a m } M m=1 ∈ R E are used as semantic features for topic generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Co-Attention</head><p>Previous works have shown that visual attention alone can perform fairly well for localizing ob- jects ( ) and aiding caption gener- ation ( <ref type="bibr" target="#b23">Xu et al., 2015)</ref>. However, visual attention does not provide sufficient high level semantic in- formation. For example, only looking at the right lower region of the chest x-ray image <ref type="figure" target="#fig_0">(Figure 1</ref>) without accounting for other areas, we might not be able to recognize what we are looking at, not to even mention detecting the abnormalities. In contrast, the tags can always provide the needed high level information. To this end, we propose a co-attention mechanism which can simultaneously attend to visual and semantic modalities.</p><p>In the sentence LSTM at time step s, the joint</p><formula xml:id="formula_1">context vector ctx (s) ∈ R C is generated by a co-attention network f coatt ({v n } N n=1 , {a m } M m=1 , h (s−1) sent ), where h (s−1) sent</formula><p>∈ R H is the sentence LSTM hidden state at time step s − 1. The co- attention network f coatt uses a single layer feed- forward network to compute the soft visual atten- tions and soft semantic attentions over input image features and tags:</p><formula xml:id="formula_2">αv,n ∝ exp(Wv att tanh(Wvvn + W v,h h (s−1) sent )) (2) αa,m ∝ exp(Wa att tanh(Waam + W a,h h (s−1) sent )) (3)</formula><p>where W v , W v,h , and W vatt are parameter ma- trices of the visual attention network. W a , W a,h , and W aatt are parameter matrices of the semantic attention network.</p><p>The visual and semantic context vectors are computed as:</p><formula xml:id="formula_3">v (s) att = N n=1 αv,nvn, a (s) att = M m=1 αa,mam.</formula><p>There are many ways to combine the visual and semantic context vectors such as concatenation and element-wise operations. In this paper, we first concatenate these two vectors as [v </p><formula xml:id="formula_4">ctx (s) = W f c [v (s) att ; a (s) att ].<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Sentence LSTM</head><p>The sentence LSTM is a single-layer LSTM that takes the joint context vector ctx ∈ R C as its input, and generates topic vector t ∈ R K for word LSTM through topic generator and deter- mines whether to continue or stop generating cap- tions by a stop control component.</p><p>Topic generator We use a deep output layer ( <ref type="bibr" target="#b18">Pascanu et al., 2014</ref>) to strengthen the context in- formation in topic vector t (s) , by combining the hidden state h (s) sent and the joint context vector ctx (s) of the current step:</p><formula xml:id="formula_5">t (s) = tanh(W t,h sent h (s) sent + Wt,ctxctx (s) ) (5)</formula><p>where W t,hsent and W t,ctx are weight parame- ters.</p><p>Stop control We also apply a deep output layer to control the continuation of the sentence LSTM. The layer takes the previous and current hidden state h</p><formula xml:id="formula_6">(s−1) sent , h (s)</formula><p>sent as input and produces a distri- bution over {STOP=1, CONTINUE=0}:</p><formula xml:id="formula_7">p(ST OP |h (s−1) sent , h (s) sent ) ∝ exp{Wstop tanh(Wstop,s−1h (s−1) sent + Wstop,sh (s) sent )}<label>(6)</label></formula><p>where W stop , W stop,s−1 and W stop,s are parame- ter matrices. If p(ST OP |h</p><formula xml:id="formula_8">(s−1) sent , h (s) sent )</formula><p>is greater than a predefined threshold (e.g. 0.5), then the sen- tence LSTM will stop producing new topic vec- tors and the word LSTM will also stop producing words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Word LSTM</head><p>The words of each sentence are generated by a word LSTM. Similar to ( <ref type="bibr" target="#b12">Krause et al., 2017)</ref>, the topic vector t produced by the sentence LSTM and the special START token are used as the first and second input of the word LSTM, and the subse- quent inputs are the word sequence.</p><p>The hidden state h word ∈ R H of the word LSTM is directly used to predict the distribution over words:</p><formula xml:id="formula_9">p(word|h word ) ∝ exp(Wouth word )<label>(7)</label></formula><p>where W out is the parameter matrix. After each word-LSTM has generated its word sequences, the final report is simply the concatenation of all the generated sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Parameter Learning</head><p>Each training example is a tuple (I, l, w) where I is an image, l denotes the ground-truth tag vector, and w is the diagnostic paragraph, which is com- prised of S sentences and each sentence consists of T s words.</p><p>Given a training example (I, l, w), our model first performs multi-label classification on I and produces a distribution p l,pred over all tags. Note that l is a binary vector which encodes the pres- ence and absence of tags. We can obtain the ground-truth tag distribution by normalizing l: p l = l/||l|| 1 . The training loss of this step is a cross-entropy loss tag between p l and p l,pred .</p><p>Next, the sentence LSTM is unrolled for S steps to produce topic vectors and also distributions over {STOP, CONTINUE}: p s stop . Finally, the S topic vectors are fed into the word LSTM to generate words w s,t . The training loss of caption gen- eration is the combination of two cross-entropy losses: sent over stop distributions p s stop and word over word distributions p s,t . Combining the pieces together, we obtain the overall training loss:</p><formula xml:id="formula_10">(I, l, w) = λtagtag + λsent S s=1 sent(p s stop , I{s = S}) + λ word S s=1</formula><p>Ts t=1 word (ps,t, ws,t)</p><p>In addition to the above training loss, there is also a regularization term for visual and seman- tic attentions. Similar to ( <ref type="bibr" target="#b23">Xu et al., 2015)</ref>, let α ∈ R N ×S and β ∈ R M ×S be the matrices of vi- sual and semantic attentions respectively, then the regularization loss over α and β is:</p><formula xml:id="formula_12">reg = λreg[ N n (1− S s αn,s) 2 + M m (1− S s βm,s) 2 ] (9)</formula><p>Such regularization encourages the model to pay equal attention over different image regions and different tags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we evaluate the proposed model with extensive quantitative and qualitative experi- ments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We used two publicly available medical image datasets to evaluate our proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IU X-Ray</head><p>The Indiana University Chest X- Ray Collection (IU X-Ray) <ref type="bibr" target="#b2">(Demner-Fushman et al., 2015</ref>) is a set of chest x-ray images paired with their corresponding diagnostic reports. The dataset contains 7,470 pairs of images and reports. Each report consists of the following sections: im- pression, findings, tags 1 , comparison, and indica- tion. In this paper, we treat the contents in impres- sion and findings as the target captions 2 to be gen- erated and the Medical Text Indexer (MTI) anno- tated tags as the target tags to be predicted ( <ref type="figure" target="#fig_0">Figure  1 provides an example)</ref>.</p><p>We preprocessed the data by converting all to- kens to lowercases, removing all of non-alpha to- kens, which resulting in 572 unique tags and 1915 unique words. On average, each image is asso- ciated with 2.2 tags, 5.7 sentences, and each sen- tence contains 6.5 words. Besides, we find that top 1,000 words cover 99.0% word occurrences in the dataset, therefore we only included top 1,000 words in the dictionary. Finally, we randomly se- lected 500 images for validation and 500 images for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PEIR Gross</head><p>The Pathology Education Informa- tional Resource (PEIR) digital library 3 is a pub- lic medical image library for medical education. We collected the images together with their de- scriptions in the Gross sub-collection, resulting in the PEIR Gross dataset that contains 7,442 image- caption pairs from 21 different sub-categories. Different from the IU X-Ray dataset, each caption in PEIR Gross contains only one sentence. We used this dataset to evaluate our model's ability of generating single-sentence report.</p><p>For PEIR Gross, we applied the same prepro- cessing as IU X-Ray, which yields 4,452 unique words. On average, each image contains 12.0 words. Besides, for each caption, we selected 5 words with the highest tf-idf scores as tags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We used the full VGG-19 model <ref type="bibr" target="#b20">(Simonyan and Zisserman, 2014</ref>) for tag prediction. As for the training loss of the multi-label classification (MLC) task, since the number of tags for semantic attention is fixed as 10, we treat MLC as a multi- label retrieval task and adopt a softmax cross- entropy loss (a multi-label ranking loss), similar to ( <ref type="bibr" target="#b6">Gong et al., 2013;</ref><ref type="bibr" target="#b7">Guillaumin et al., 2009</ref>). <ref type="bibr">1</ref> There are two types of tags: manually generated (MeSH) and Medical Text Indexer (MTI) generated. <ref type="bibr">2</ref> The impression and findings sections are concatenated together as a long paragraph, since impression can be viewed as a conclusion or topic sentence of the report.</p><p>3 PEIR is  <ref type="table">Table 1</ref>: Main results for paragraph generation on the IU X-Ray dataset (upper part), and single sentence generation on the PEIR Gross dataset (lower part). BLUE-n denotes the BLEU score that uses up to n-grams.</p><p>In paragraph generation, we set the dimensions of all hidden states and word embeddings as 512. For words and tags, different embedding matri- ces were used since a tag might contain multi- ple words. We utilized the embeddings of the 10 most likely tags as the semantic feature vectors {a m } M =10 m=1 . We extracted the visual features from the last convolutional layer of the VGG-19 net- work, which yields a 14 × 14 × 512 feature map.</p><p>We used the Adam (Kingma and Ba, 2014) optimizer for parameter learning. The learning rates for the CNN <ref type="bibr">(VGG-19)</ref> and the hierarchi- cal LSTM were 1e-5 and 5e-4 respectively. The weights (λ tag , λ sent , λ word and λ reg ) of different losses were set to 1.0. The threshold for stop con- trol was 0.5. Early stopping was used to prevent over-fitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines</head><p>We compared our method with several state- of-the-art image captioning models: CNN-RNN ( <ref type="bibr" target="#b22">Vinyals et al., 2015)</ref>, <ref type="bibr">LRCN (Donahue et al., 2015)</ref>, Soft ATT ( <ref type="bibr" target="#b23">Xu et al., 2015)</ref>, and ATT-RK ( <ref type="bibr" target="#b24">You et al., 2016)</ref>. We re-implemented all of these models and adopt <ref type="bibr">VGG-19 (Simonyan and Zisserman, 2014</ref>) as the CNN encoder. Consider- ing these models are built for single sentence cap- tions and to better show the effectiveness of the hierarchical LSTM and the attention mechanism for paragraph generation, we also implemented a hierarchical model without any attention: Ours- no-Attention. The input of Ours-no-Attention is the overall image feature of VGG-19, which has a dimension of 4096. Ours-no-Attention can be viewed as a CNN-RNN ( <ref type="bibr" target="#b22">Vinyals et al., 2015</ref>) equipped with a hierarchical LSTM decoder. To further show the effectiveness of the proposed co- attention mechanism, we also implemented two ablated versions of our model: Ours-Semantic- only and Ours-Visual-only, which takes solely the semantic attention or visual attention context vec- tor to produce topic vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Quantitative Results</head><p>We report the paragraph generation (upper part of <ref type="table">Table 1</ref>) and one sentence generation (lower part of <ref type="table">Table 1</ref>) results using the standard image cap- tioning evaluation tool 4 which provides evalua- tion on the following metrics: BLEU ( <ref type="bibr" target="#b17">Papineni et al., 2002</ref>), METEOR <ref type="bibr" target="#b3">(Denkowski and Lavie, 2014</ref>), ROUGE <ref type="bibr" target="#b14">(Lin, 2004)</ref>, and CIDER <ref type="bibr" target="#b21">(Vedantam et al., 2015)</ref>.</p><p>For paragraph generation, as shown in the upper part of <ref type="table">Table 1</ref>, it is clear that models with a single LSTM decoder perform much worse than those with a hierarchical LSTM decoder. Note that the only difference between Ours-no-Attention and CNN-RNN ( <ref type="bibr" target="#b22">Vinyals et al., 2015)</ref> is that Ours- no-Attention adopts a hierarchical LSTM decoder while CNN-RNN ( <ref type="bibr" target="#b22">Vinyals et al., 2015</ref>) adopts a single-layer LSTM. The comparison between these two models directly demonstrates the ef- fectiveness of the hierarchical LSTM. This re- sult is not surprising since it is well-known that a single-layer LSTM cannot effectively model long sequences ( <ref type="bibr" target="#b15">Liu et al., 2015;</ref><ref type="bibr" target="#b16">Martin and Cundy, 2018)</ref>. Additionally, employing semantic atten- tion alone (Ours-Semantic-only) or visual atten- tion alone (Ours-Visual-only) to generate topic vectors does not seem to help caption generation a lot. The potential reason might be that visual at- tention can only capture the visual information of sub-regions of the image and is unable to correctly capture the semantics of the entire image. Se- mantic attention is inadequate of localizing small abnormal image-regions. Finally, our full model (Ours-CoAttention) achieves the best results on all of the evaluation metrics, which demonstrates the effectiveness of the proposed co-attention mecha- nism.</p><p>For the single-sentence generation results (shown in the lower part of Table 1), the ab- lated versions of our model (Ours-Semantic-only and Ours-Visual-only) achieve competitive scores compared with the state-of-the-art methods. Our full model (Ours-CoAttention) outperforms all of the baseline, which indicates the effectiveness of the proposed co-attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Paragraph Generation</head><p>An illustration of paragraph generation by three models (Ours-CoAttention, Ours-no-Attention and Soft Attention models) is shown in <ref type="figure" target="#fig_3">Figure 3</ref>.</p><p>We can find that different sentences have different topics. The first sentence is usually a high level de- scription of the image, while each of the following sentences is associated with one area of the image (e.g. "lung", "heart"). Soft Attention and Ours- no-Attention models detect only a few abnormal- ities of the images and the detected abnormali- ties are incorrect. In contrast, Ours-CoAttention model is able to correctly describe many true ab- normalities (as shown in top three images). This comparison demonstrates that co-attention is bet- ter at capturing abnormalities.</p><p>For the third image, Ours-CoAttention model successfully detects the area ("right lower lobe") which is abnormal ("eventration"), however, it fails to precisely describe this abnormality. In ad- dition, the model also finds abnormalities about "interstitial opacities" and "atheroscalerotic calci- fication", which are not considered as true abnor- mality by human experts. The potential reason for this mis-description might be that this x-ray image is darker (compared with the above images), and our model might be very sensitive to this change. The image at the bottom is a failure case of Ours-CoAttention. However, even though the model makes the wrong judgment about the ma- jor abnormalities in the image, it does find some unusual regions: "lateral lucency" and "left lower lobe".</p><p>To further understand models' ability of detect- ing abnormalities, we present the portion of sen- tences which describe the normalities and abnor- malities in <ref type="table" target="#tab_1">Table 2</ref>. We consider sentences which contain "no", "normal", "clear", "stable" as sen- tences describing normalities. It is clear that Ours- CoAttention best approximates the ground truth distribution over normality and abnormality.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Co-Attention Learning</head><p>Figure 4 presents visualizations of co-attention. The first property shown by <ref type="figure" target="#fig_4">Figure 4</ref> is that the sentence LSTM can generate different topics at different time steps since the model focuses on different image regions and tags for different sen- tences. The next finding is that visual attention can guide our model to concentrate on relevant re-gions of the image. For example, the third sen- tence of the first example is about "cardio", and the visual attention concentrates on regions near the heart. Similar behavior can also be found for semantic attention: for the last sentence in the first example, our model correctly concentrates on "de- generative change" which is the topic of the sen- tence. Finally, the first sentence of the last exam- ple presents a mis-description caused by incorrect semantic attention over tags. Such incorrect atten- tion can be reduced by building a better tag pre- diction module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we study how to automatically gen- erate textual reports for medical images, with the goal to help medical professionals produce reports more accurately and efficiently. Our proposed methods address three major challenges: (1) how to generate multiple heterogeneous forms of in- formation within a unified framework, (2) how to localize abnormal regions and produce accurate descriptions for them, (3) how to generate long texts that contain multiple sentences or even para- graphs. To cope with these challenges, we propose a multi-task learning framework which jointly pre- dicts tags and generates descriptions. We intro- duce a co-attention mechanism that can simultane- ously explore visual and semantic information to accurately localize and describe abnormal regions. We develop a hierarchical LSTM network that can more effectively capture long-range semantics and produce high quality long texts. On two medical datasets containing radiology and pathology im- ages, we demonstrate the effectiveness of the pro- posed methods through quantitative and qualita- tive studies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An exemplar chest x-ray report. In the impression section, the radiologist provides a diagnosis. The findings section lists the radiology observations regarding each area of the body examined in the imaging study. The tags section lists the keywords which represent the critical information in the findings. These keywords are identified using the Medical Text Indexer (MTI).</figDesc><graphic url="image-1.png" coords="1,307.28,222.54,226.77,107.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of the proposed model. MLC denotes a multi-label classification network. Semantic features are the word embeddings of the predicted tags. The boldfaced tags "calcified granuloma" and "granuloma" are attended by the co-attention network.</figDesc><graphic url="image-2.png" coords="3,72.00,62.81,453.56,101.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>and then use a fully connected layer W f c to ob- tain a joint context vector:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration of paragraph generated by Ours-CoAttention, Ours-no-Attention, and Soft Attention models. The underlined sentences are the descriptions of detected abnormalities. The second image is a lateral x-ray image. Top two images are positive results, the third one is a partial failure case and the bottom one is failure case. These images are from test dataset.</figDesc><graphic url="image-3.png" coords="7,72.00,62.81,453.54,299.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Visualization of co-attention for three examples. Each example is comprised of four things: (1) image and visual attentions; (2) ground truth tags and semantic attention on predicted tags; (3) generated descriptions; (4) ground truth descriptions. For the semantic attention, three tags with highest attention scores are highlighted. The underlined tags are those appearing in the ground truth.</figDesc><graphic url="image-4.png" coords="8,72.00,62.80,453.56,399.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Portion of sentences which describe the 
normalities and abnormalities in the image. 

</table></figure>

			<note place="foot" n="4"> https://github.com/tylin/coco-caption</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Multiple object recognition with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Difficulties in the interpretation of chest radiography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louke</forename><surname>Delrue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Gosselin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Ilsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Van Landeghem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>De Mey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Duyck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Comparative Interpretation of CT and Standard Radiography of the Chest</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="27" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Preparing a collection of radiology examinations for distribution and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonya</forename><forename type="middle">E</forename><surname>Marc B Rosenman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laritza</forename><surname>Shooshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Antani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement J</forename><surname>George R Thoma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="304" to="310" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ninth workshop on statistical machine translation</title>
		<meeting>the ninth workshop on statistical machine translation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="376" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><forename type="middle">Darrell</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">From captions to visual concepts and back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Hao Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rupesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1473" to="1482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep convolutional ranking for multilabel image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Alexander Toshev, and Sergey Ioffe. ICLR</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tagprop: Discriminative metric learning in nearest neighbor models for image auto-annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="309" to="316" />
		</imprint>
	</monogr>
	<note>Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep visualsemantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">From medical image to automatic medical report generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kisilev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Walach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ella</forename><surname>Barkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boaz</forename><surname>Ophir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Alpert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2/3</biblScope>
			<biblScope unit="page" from="2" to="3" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A hierarchical approach for generating descriptive image paragraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recurrent topictransition gan for visual paragraph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text summarization branches out: Proceedings of the ACL-04 workshop</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-timescale long short-term memory neural network for modelling sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 conference on empirical methods in natural language processing</title>
		<meeting>the 2015 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2326" to="2335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Parallelizing linear recurrent neural nets over sequence length</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Cundy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">How to construct deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to read chest x-rays: recurrent neural cascade model for automated image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoo-Chang</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirk</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dina</forename><surname>Demnerfushman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhua</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2497" to="2506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4566" to="4575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Image captioning with semantic attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanzeng</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4651" to="4659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mdnet: A semantically and visually interpretable medical image diagnosis network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanpu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuyong</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mason</forename><surname>Mcgough</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6428" to="6436" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
