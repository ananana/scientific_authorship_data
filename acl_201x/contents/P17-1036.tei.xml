<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:33+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Unsupervised Neural Attention Model for Aspect Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruidan</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wee</forename><forename type="middle">Sun</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Dahlmeier</surname></persName>
						</author>
						<title level="a" type="main">An Unsupervised Neural Attention Model for Aspect Extraction</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="388" to="397"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1036</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Aspect extraction is an important and challenging task in aspect-based sentiment analysis. Existing works tend to apply variants of topic models on this task. While fairly successful, these methods usually do not produce highly coherent aspects. In this paper, we present a novel neural approach with the aim of discovering coherent aspects. The model improves coherence by exploiting the distribution of word co-occurrences through the use of neural word embeddings. Unlike topic models which typically assume independently generated words, word embedding models encourage words that appear in similar contexts to be located close to each other in the embedding space. In addition, we use an attention mechanism to de-emphasize irrelevant words during training, further improving the coherence of aspects. Experimental results on real-life datasets demonstrate that our approach discovers more meaningful and coherent aspects, and substantially outper-forms baseline methods on several evaluation tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Aspect extraction is one of the key tasks in senti- ment analysis. It aims to extract entity aspects on which opinions have been expressed ( <ref type="bibr" target="#b7">Hu and Liu, 2004;</ref><ref type="bibr" target="#b12">Liu, 2012)</ref>. For example, in the sentence "The beef was tender and melted in my mouth", the aspect term is "beef". Two sub-tasks are per- formed in aspect extraction: (1) extracting all as- pect terms (e.g., "beef") from a review corpus, (2) clustering aspect terms with similar meaning into categories where each category represents a single aspect (e.g., cluster "beef", "pork", "pasta", and "tomato" into one aspect food).</p><p>Previous works for aspect extraction can be cat- egorized into three approaches: rule-based, super- vised, and unsupervised. Rule-based methods usu- ally do not group extracted aspect terms into cate- gories. Supervised learning requires data annota- tion and suffers from domain adaptation problems. Unsupervised methods are adopted to avoid re- liance on labeled data needed for supervised learn- ing.</p><p>In recent years, Latent Dirichlet Allocation (LDA) ( <ref type="bibr" target="#b1">Blei et al., 2003</ref>) and its variants <ref type="bibr" target="#b24">(Titov and McDonald, 2008;</ref><ref type="bibr" target="#b2">Brody and Elhadad, 2010;</ref><ref type="bibr" target="#b30">Zhao et al., 2010;</ref><ref type="bibr" target="#b18">Mukherjee and Liu, 2012</ref>) have become the dominant unsupervised approach for aspect extraction. LDA models the corpus as a mixture of topics (aspects), and topics as distri- butions over word types. While the mixture of aspects discovered by LDA-based models may de- scribe a corpus fairly well, we find that the individ- ual aspects inferred are of poor quality -aspects often consist of unrelated or loosely-related con- cepts. This may substantially reduce users' con- fidence in using such automated systems. There could be two primary reasons for the poor qual- ity. Conventional LDA models do not directly en- code word co-occurrence statistics which are the primary source of information to preserve topic coherence <ref type="bibr" target="#b16">(Mimno et al., 2011</ref>). They implicitly capture such patterns by modeling word genera- tion from the document level, assuming that each word is generated independently. Furthermore, LDA-based models need to estimate a distribution of topics for each document. Review documents tend to be short, thus making the estimation of topic distributions more difficult.</p><p>In this work, we present a novel neural approach to tackle the weaknesses of LDA-based methods. We start with neural word embeddings that al-ready map words that usually co-occur within the same context to nearby points in the embedding space ( <ref type="bibr" target="#b15">Mikolov et al., 2013)</ref>. We then filter the word embeddings within a sentence using an at- tention mechanism ( <ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref> and use the filtered words to construct aspect embed- dings. The training process for aspect embed- dings is analogous to autoencoders, where we use dimension reduction to extract the common fac- tors among embedded sentences and reconstruct each sentence through a linear combination of as- pect embeddings. The attention mechanism de- emphasizes words that are not part of any aspect, allowing the model to focus on aspect words. We call our proposed model Attention-based Aspect Extraction (ABAE).</p><p>In contrast to LDA-based models, our proposed method explicitly encodes word-occurrence statis- tics into word embeddings, uses dimension reduc- tion to extract the most important aspects in the review corpus, and uses an attention mechanism to remove irrelevant words to further improve co- herence of the aspects.</p><p>We have conducted extensive experiments on large review data sets. The results show that ABAE is effective in discovering meaningful and coherent aspects. It substantially outperforms baseline methods on multiple evaluation tasks. In addition, ABAE is intuitive and structurally sim- ple. It can also easily scale to a large amount of training data. Therefore, it is a promising alterna- tive to LDA-based methods proposed previously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The problem of aspect extraction has been well studied in the past decade. Initially, methods were mainly based on manually defined rules. <ref type="bibr" target="#b7">Hu and Liu (2004)</ref> proposed to extract different product features through finding frequent nouns and noun phrases. They also extracted opinion terms by finding the synonyms and antonyms of opinion seed words through WordNet. Following this, a number of methods have been proposed based on frequent item mining and dependency information to extract product aspects ( <ref type="bibr" target="#b31">Zhuang et al., 2006;</ref><ref type="bibr" target="#b22">Somasundaran and Wiebe, 2009;</ref><ref type="bibr" target="#b19">Qiu et al., 2011</ref>). These models heavily depend on predefined rules which work well only when the aspect terms are restricted to a small group of nouns.</p><p>Supervised learning approaches generally model aspect extraction as a standard sequence labeling problem. <ref type="bibr" target="#b9">Jin and Ho (2009)</ref> and  proposed to use hidden Markov models (HMM) and conditional random fields (CRF), respectively with a set of manually-extracted fea- tures. More recently, different neural models ( <ref type="bibr" target="#b29">Yin et al., 2016;</ref><ref type="bibr" target="#b26">Wang et al., 2016)</ref> were proposed to automatically learn features for CRF-based aspect extraction. Rule-based models are usually not refined enough to categorize the extracted aspect terms. On the other hand, supervised learning requires large amounts of labeled data for training purposes.</p><p>Unsupervised approaches, especially topic models, have been proposed subsequently to avoid reliance on labeled data. Generally, the outputs of those models are word distributions or rank- ings for each aspect. Aspects are naturally ob- tained without separately performing extraction and categorization. Most existing works <ref type="bibr" target="#b2">(Brody and Elhadad, 2010;</ref><ref type="bibr" target="#b30">Zhao et al., 2010;</ref><ref type="bibr" target="#b18">Mukherjee and Liu, 2012;</ref><ref type="bibr" target="#b4">Chen et al., 2014</ref>) are based on variants and extensions of LDA ( <ref type="bibr" target="#b1">Blei et al., 2003)</ref>. Recently, <ref type="bibr" target="#b25">Wang et al. (2015)</ref> proposed a re- stricted Boltzmann machine (RBM)-based model to simultaneously extract aspects and relevant sen- timents of a given review sentence, treating as- pects and sentiments as separate hidden variables in RBM. However, the RBM-based model pro- posed in ( <ref type="bibr" target="#b25">Wang et al., 2015</ref>) relies on a substantial amount of prior knowledge such as part-of-speech (POS) tagging and sentiment lexicons. A biterm topic model (BTM) that generates co-occurring word pairs was proposed in ( <ref type="bibr" target="#b28">Yan et al., 2013</ref>). We experimentally compare ABAE and BTM on multiple tasks in this paper.</p><p>Attention models ( <ref type="bibr" target="#b17">Mnih et al., 2014</ref>) have re- cently gained popularity in training neural net- works and have been applied to various nat- ural language processing tasks, including ma- chine translation ( <ref type="bibr" target="#b0">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b13">Luong et al., 2015)</ref>, sentence summarization ( <ref type="bibr" target="#b20">Rush et al., 2015)</ref>, sentiment classification ( <ref type="bibr" target="#b3">Chen et al., 2016;</ref><ref type="bibr" target="#b23">Tang et al., 2016)</ref>, and question answering <ref type="bibr" target="#b6">(Hermann et al., 2015)</ref>. Rather than using all available information, attention mechanism aims to focus on the most pertinent information for a task. Un- like previous works, in this paper, we apply atten- tion to an unsupervised neural model. Our experi- mental results demonstrate its effectiveness under an unsupervised setting for aspect extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model Description</head><p>We describe the Attention-based Aspect Extrac- tion (ABAE) model in this section. The ultimate goal is to learn a set of aspect embeddings, where each aspect can be interpreted by looking at the nearest words (representative words) in the em- bedding space. We begin by associating each word w in our vocabulary with a feature vector e w ∈ R d . We use word embeddings for the feature vectors as word embeddings are designed to map words that often co-occur in a context to points that are close by in the embedding space <ref type="bibr" target="#b15">(Mikolov et al., 2013</ref>). The feature vectors associated with the words correspond to the rows of a word em- bedding matrix E ∈ R V ×d , where V is the vo- cabulary size. We want to learn embeddings of aspects, where aspects share the same embedding space with words. This requires an aspect embed- ding matrix T ∈ R K×d , where K, the number of aspects defined, is much smaller than V . The aspect embeddings are used to approximate the aspect words in the vocabulary, where the aspect words are filtered through an attention mechanism.</p><p>Each input sample to ABAE is a list of indexes for words in a review sentence. Given such an input, two steps are performed as shown in <ref type="figure" target="#fig_0">Fig- ure 1</ref>. First, we filter away non-aspect words by down-weighting them using an attention mecha- nism, and construct a sentence embedding z s from weighted word embeddings. Then, we try to re- construct the sentence embedding as a linear com- bination of aspect embeddings from T. This pro- cess of dimension reduction and reconstruction, where ABAE aims to transform sentence embed- dings of the filtered sentences (z s ) into their re- constructions (r s ) with the least possible amount of distortion, preserves most of the information of the aspect words in the K embedded aspects. We next describe the process in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sentence Embedding with Attention Mechanism</head><p>We construct a vector representation z s for each input sentence s in the first step. In general, we want the vector representation to capture the most relevant information with regards to the aspect (topic) of the sentence. We define the sentence embedding z s as the weighted summation of word embeddings e w i , i = 1, ..., n corresponding to the word indexes in the sentence.</p><formula xml:id="formula_0">z s = n i=1 a i e w i .<label>(1)</label></formula><p>For each word w i in the sentence, we compute a positive weight a i which can be interpreted as the probability that w i is the right word to focus on in order to capture the main topic of the sen- tence. The weight a i is computed by an attention model, which is conditioned on the embedding of the word e w i as well as the global context of the sentence:</p><formula xml:id="formula_1">a i = exp(d i ) n j=1 exp(d j )<label>(2)</label></formula><formula xml:id="formula_2">d i = e w i · M · y s (3) y s = 1 n n i=1 e w i (4)</formula><p>where y s is simply the average of the word em- beddings, which we believe captures the global context of the sentence. M ∈ R d×d is a matrix mapping between the global context embedding y s and the word embedding e w and is learned as part of the training process. We can think of the attention mechanism as a two-step process. Given a sentence, we first construct its representation by averaging all the word representations. Then the weight of a word is assigned by considering two things. First, we filter the word through the trans- formation M which is able to capture the rele- vance of the word to the K aspects. Then we capture the relevance of the filtered word to the sentence by taking the inner product of the filtered word to the global context y s .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sentence Reconstruction with Aspect Embeddings</head><p>We have obtained the sentence embedding. Now we describe how to compute the reconstruction of the sentence embedding. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the reconstruction process consists of two steps of transitions, which is similar to an autoencoder. In- tuitively, we can think of the reconstruction as a linear combination of aspect embeddings from T:</p><formula xml:id="formula_3">r s = T · p t (5)</formula><p>where r s is the reconstructed vector representa- tion, p t is the weight vector over K aspect embed- dings, where each weight represents the probabil- ity that the input sentence belongs to the related aspect. p t can simply be obtained by reducing z s from d dimensions to K dimensions and then ap- plying a softmax non-linearity that yields normal- ized non-negative weights:</p><formula xml:id="formula_4">p t = softmax (W · z s + b)<label>(6)</label></formula><p>where W, the weighted matrix parameter, and b, the bias vector, are learned as part of the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training Objective</head><p>ABAE is trained to minimize the reconstruction error. We adopted the contrastive max-margin ob- jective function used in previous work <ref type="bibr" target="#b27">(Weston et al., 2011;</ref><ref type="bibr" target="#b21">Socher et al., 2014;</ref><ref type="bibr" target="#b8">Iyyer et al., 2016</ref>).</p><p>For each input sentence, we randomly sample m sentences from our training data as negative sam- ples. We represent each negative sample as n i which is computed by averaging its word embed- dings. Our objective is to make the reconstructed embedding r s similar to the target sentence em- bedding z s while different from those negative samples. Therefore, the unregularized objective J is formulated as a hinge loss that maximize the inner product between r s and z s and simultane- ously minimize the inner product between r s and the negative samples:</p><formula xml:id="formula_5">J(θ) = s∈D m i=1 max(0, 1 − r s z s + r s n i ) (7)</formula><p>where D represents the training data set and θ = {E, T, M, W, b} represents the model parame- ters. <ref type="table" target="#tab_0">Restaurant  52,574  3,400  Beer  1,586,259  9,245   Table 1</ref>: Dataset description.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain #Reviews #Labeled sentences</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Regularization Term</head><p>We hope to learn vector representations of the most representative aspects for a review dataset. However, the aspect embedding matrix T may suf- fer from redundancy problems during training. To ensure the diversity of the resulting aspect embed- dings, we add a regularization term to the objective function J to encourage the uniqueness of each as- pect embedding:</p><formula xml:id="formula_6">U (θ) = T n · T n − I<label>(8)</label></formula><p>where I is the identity matrix, and T n is T with each row normalized to have length 1. Any non- diagonal element t ij (i = j) in the matrix T n · T n corresponds to the dot product of two different as- pect embeddings. U reaches its minimum value when the dot product between any two different aspect embeddings is zero. Thus the regularization term encourages orthogonality among the rows of the aspect embedding matrix T and penalizes re- dundancy between different aspect vectors. Our final objective function L is obtained by adding J and U :</p><formula xml:id="formula_7">L(θ) = J(θ) + λU (θ)<label>(9)</label></formula><p>where λ is a hyperparameter that controls the weight of the regularization term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluate our method on two real-word datasets. The detailed statistics of the datasets are summarized in <ref type="table">Table 1</ref>.</p><p>(1) Citysearch corpus: This is a restaurant review corpus widely used by previous works ( <ref type="bibr" target="#b5">Ganu et al., 2009;</ref><ref type="bibr" target="#b2">Brody and Elhadad, 2010;</ref><ref type="bibr" target="#b30">Zhao et al., 2010)</ref>, which contains over 50,000 restaurant reviews from Citysearch New York. <ref type="bibr" target="#b5">Ganu et al. (2009)</ref> also provided a subset of 3,400 sentences from the corpus with manually labeled aspects. These anno- tated sentences are used for evaluation of as- pect identification. There are six manually defined aspect labels: Food, Staff, Ambience, Price, Anecdotes, and Miscellaneous.</p><p>(2) BeerAdvocate: This is a beer review corpus introduced in <ref type="bibr" target="#b14">(McAuley et al., 2012</ref>), con- taining over 1.5 million reviews. A subset of 1,000 reviews, corresponding to 9,245 sen- tences, are annotated with five aspect labels: Feel, Look, Smell, Taste, and Overall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline Methods</head><p>To validate the performance of ABAE, we com- pare it against a number of baselines:</p><p>(1) LocLDA (Brody and Elhadad, 2010): This method uses a standard implementation of LDA. In order to prevent the inference of global topics and direct the model towards rateable aspects, each sentence is treated as a separate document.</p><p>(2) k-means: We initialize the aspect matrix T by using the k-means centroids of the word embeddings. To show the power of ABAE, we compare its performance with using the k- means centroids directly. : This is a biterm topic model that is specially designed for short texts such as texts from social media and review sites. The major advantage of BTM over con- ventional LDA models is that it alleviates the problem of data sparsity in short documents by directly modeling the generation of unordered word-pair co-occurrences (biterms) over the corpus. It has been shown to perform better than conventional LDA models in discovering coherent topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Settings</head><p>Review corpora are preprocessed by removing punctuation symbols, stop words, and words ap- pearing less than 10 times. For LocLDA, we use the open-source implementation GibbsLDA++ <ref type="bibr">1</ref> and for BTM, we use the implementation released by (Yan et al., 2013) 2 . We tune the hyperparame- ters of all topic model baselines on a held-out set with grid search using the topic coherence metric to be introduced later in Eq 10: for LocLDA, the Dirichlet priors α = 0.05 and β = 0.1; for SAS and BTM, α = 50/K and β = 0.1. We run 1,000 iterations of Gibbs sampling for all topic models. For the ABAE model, we initialize the word embedding matrix E with word vectors trained by word2vec with negative sampling on each dataset, setting the embedding size to 200, window size to 10, and negative sample size to 5. The parameters we use for training word embeddings are standard with no specific tuning to our data. We also initial- ize the aspect embedding matrix T with the cen- troids of clusters resulting from running k-means on word embeddings. Other parameters are initial- ized randomly. During the training process, we fix the word embedding matrix E and optimize other parameters using Adam ( <ref type="bibr" target="#b10">Kingma and Ba, 2014</ref>) with learning rate 0.001 for 15 epochs and batch size of 50. We set the number of negative samples per input sample m to 20, and the orthogonality penalty weight λ to 1 by tuning the hyperparam- eters on a held-out set with grid search. The re- sults reported for all models are the average over 10 runs.</p><p>Following <ref type="bibr" target="#b2">(Brody and Elhadad, 2010;</ref><ref type="bibr" target="#b30">Zhao et al., 2010)</ref>, we set the number of aspects for the restaurant corpus to 14. We experimented with different number of aspects from 10 to 20 for the beer corpus. The results showed no major dif- ference, so we also set it to 14. As in previ- ous work <ref type="bibr" target="#b2">(Brody and Elhadad, 2010;</ref><ref type="bibr" target="#b30">Zhao et al., 2010)</ref>, we manually mapped each inferred aspect to one of the gold-standard aspects according to its top ranked representative words. In ABAE, repre- sentative words of an aspect can be found by look- ing at its nearest words in the embedding space using cosine as the similarity metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation and Results</head><p>We describe the evaluation tasks and report the experimental results in this section. We evaluate ABAE on two criteria:</p><p>• Is it able to find meaningful and semantically coherent aspects?</p><p>• Is it able to improve aspect identification per- formance on real-world review datasets?  <ref type="table" target="#tab_0">Table 2</ref>: List of inferred aspects for restaurant reviews (left), with top representative words for each inferred aspect (middle), and the corresponding gold-standard aspect labels (right). Inferred aspect labels (left) were assigned manually. standard labels, the inferred aspects are more fine- grained. For example, it can distinguish main dishes from desserts, and drinks from food.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Aspect Quality Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Coherence Score</head><p>In order to objectively measure the quality of as- pects, we use coherence score as a metric which has been shown to correlate well with human judg- ment ( <ref type="bibr" target="#b16">Mimno et al., 2011)</ref>. Given an aspect z and a set of top N words of z, S z = {w z 1 , ..., w z N }, the coherence score is calculated as follows:</p><formula xml:id="formula_8">C(z; S z ) = N n=2 n−1 l=1 log D 2 (w z n , w z l ) + 1 D 1 (w z l )<label>(10)</label></formula><p>where D 1 (w) is the document frequency of word w and D 2 (w 1 , w 2 ) is the co-document frequency of words w 1 and w 2 . A higher coherence score indicates a better aspect interpretability, i.e., more meaningful and semantically coherent. <ref type="figure" target="#fig_2">Figure 2</ref> shows the average coherence score of each model which is computed as</p><formula xml:id="formula_9">1 K K k=1 C(z k ; S z k )</formula><p>on both the restaurant do- main and beer domain. From the results, we make the following observations: (1) ABAE outperforms previous models for all ranked buckets. (2) BTM performs slightly better than LocLDA and SAS. This may be because BTM directly models the generation of biterms, while conventional LDA just implicitly captures such patterns by modeling word generation from the document level. (3) It is interesting to note that performing k-means on the word embeddings is sufficient to perform better than all topic model baselines, including BTM. This indicates that neural word embedding is a better model for capturing co-occurrence than LDA, even for BTM which specifically models the generation of co-occurring word pairs.  <ref type="table">Table 3</ref>: Number of coherent aspects. K (number of aspects) = 14 for all models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">User Evaluation</head><p>As we want to discover a set of aspects that the human user finds agreeable, it is also necessary to carry out user evaluation directly. Following the experimental setting in <ref type="figure" target="#fig_0">(Chen et al., 2014</ref>), we recruited three human judges. Each aspect is la- beled as coherent if the majority of judges assess that most of its top 50 terms coherently represent a product aspect. The numbers of coherent aspects discovered by each model are shown in <ref type="table">Table 3</ref>. ABAE discovers the most number of coherent as- pects compared with other models. For a coherent aspect, each of its top terms is labeled as correct if and only if the majority of judges assess that it reflects the related aspect. We adopt precision@n (or p@n) to evaluate the re- sults, which was also used in <ref type="bibr" target="#b18">(Mukherjee and Liu, 2012;</ref><ref type="bibr" target="#b4">Chen et al., 2014</ref>). <ref type="figure" target="#fig_1">Figure 3</ref> shows the aver- age p@n results over all coherent aspects for each domain. We can see that the user evaluation results correlate well with the coherence scores shown in <ref type="figure" target="#fig_2">Figure 2</ref>, where ABAE substantially outperforms all other models for all ranked buckets, especially for large values of n.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Aspect Identification</head><p>We evaluate the performance of sentence-level as- pect identification on both domains using the an- notated sentences shown in <ref type="table">Table 1</ref>. The evalua- tion criterion is to judge how well the predictions match the true labels, measured by precision, re- call, and F 1 scores. The results 4 are shown in <ref type="table" target="#tab_2">Ta- ble 4 and Table 5</ref>.</p><p>Given a review sentence, ABAE first assigns an inferred aspect label which corresponds to the highest weight in p t calculated as shown in Equa- tion 6 . And we then assign the gold-standard label to the sentence according to the mapping between inferred aspects and gold-standard labels.</p><p>3 k-means assigns a sentence an inferred aspect whose em- bedding is the closest to the averaged word embeddings of the sentence. <ref type="bibr">4</ref> Note that the values of P/R/F1 reported are the average over 10 runs (except some values taken from published re- sults in  For the restaurant domain, we follow the exper- imental settings of previous work <ref type="bibr" target="#b2">(Brody and Elhadad, 2010;</ref><ref type="bibr" target="#b30">Zhao et al., 2010;</ref><ref type="bibr" target="#b25">Wang et al., 2015)</ref> to make our results comparable. To do that, (1) we only used the single-label sentences for eval- uation to avoid ambiguity (about 83% of labeled sentences have a single label), and (2) we only evaluated on three major aspects, namely Food, Staff, and Ambience. The other aspects do not show clear patterns in either word usage or writ- ing style, which makes these aspects very hard for even humans to identify. Besides the base- line models, we also compare the results with other published models, including MaxEnt-LDA (ME-LDA) ( <ref type="bibr" target="#b30">Zhao et al., 2010</ref>) and SERBM ( <ref type="bibr" target="#b25">Wang et al., 2015)</ref>. SERBM has reported state-of-the-art results for aspect identification on the restaurant corpus to date. However, SERBM relies on a sub- stantial amount of prior knowledge.    We make the following observations from Ta- ble 4: (1) ABAE outperforms all other models on F 1 score for aspects Staff and Ambience. (2) The F 1 score of ABAE for Food is worse than SERBM while its precision is very high. We an- alyzed the errors and found that most of the sen- tences we failed to recognize as Food are general descriptions without specific food words appear- ing. For example, the true label for the sentence "The food is prepared quickly and efficiently." is Food. ABAE assigns Staff to it as the highly fo- cused words according to the attention mechanism are quickly and efficiently which are more related to Staff. In fact, although this sentence contains the word food, we think it is a rather general descrip- tion of service. (3) ABAE substantially outper- forms k-means for this task although both meth- ods perform well for extracting coherent aspects as shown in <ref type="figure" target="#fig_2">Figure 2</ref> and <ref type="figure" target="#fig_1">Figure 3</ref>. This shows the power brought by the attention mechanism, which is able to capture the main topic of a sentence by only focusing on aspect-related words.</p><p>For the beer domain, in addition to the five gold- standard aspect labels, we also combined Taste and Smell to form a single aspect -Taste+Smell. This is because these two aspects are very similar <ref type="figure">Figure 4</ref>: Visualization of the attention layer. and many words can be used to describe both as- pects. For example, the words spicy, bitter, fresh, sweet, etc. are top ranked representative words in both aspects, which makes it very hard even for humans to distinguish them. Since Taste and Smell are highly correlated and difficult to separate in real life, a natural way to evaluate is to treat them as a single aspect.</p><p>We can see from <ref type="table" target="#tab_5">Table 5</ref> that due to the issue de- scribed above, all models perform poorly on Taste and Smell. ABAE outperforms previous models in F 1 scores on all aspects except for Taste. The re- sults demonstrate the capability of ABAE in iden- tifying separable aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Aspect</head><p>Method  <ref type="table">Table 6</ref>: Comparison between ABAE and ABAE − on aspect identification on the restaurant domain. <ref type="figure">Figure 4</ref> shows the weights of words assigned by the attention model for some example sentences. As we can see, the weights learned by the model correspond very strongly with human intuition. In order to evaluate how attention model affects the overall performance of ABAE, we conduct exper- iments to compare ABAE and ABAE − on aspect identification, where ABAE − denotes the model in which the attention layer is switched off and sentence embedding is calculated by averaging its word embeddings: z s = 1 n n i=1 e w i . The re- sults on the restaurant domain are shown in Ta- ble 6. ABAE achieves substantially higher pre- cision and recall on all aspects compared with ABAE − , which demonstrates the effectiveness of the attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Validating the Effectiveness of Attention Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have presented ABAE, a simple yet effective neural attention model for aspect extraction. In contrast to LDA models, ABAE explicitly cap- tures word co-occurrence patterns and overcomes the problem of data sparsity present in review cor- pora. Our experimental results demonstrated that ABAE not only learns substantially higher qual- ity aspects, but also more effectively captures the aspects of reviews than previous methods. To the best of our knowledge, we are the first to propose an unsupervised neural approach for aspect extrac- tion. ABAE is intuitive and structurally simple, and also scales up well. All these benefits make it a promising alternative to LDA-based methods in practice.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of the ABAE structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( 3 )</head><label>3</label><figDesc>SAS (Mukherjee and Liu, 2012): This is a hy- brid topic model that jointly discovers both aspects and aspect-specific opinions. This model has been shown to be competitive among topic models in discovering meaning- ful aspects (Mukherjee and Liu, 2012; Wang et al., 2015). (4) BTM (Yan et al., 2013)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Average coherence score versus number of top n terms for the restaurant domain (top) and beer domain (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Average p@n over all coherent aspects for the restaurant domain (left) and beer domain (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Feel</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 presents all 14 aspects inferred by ABAE for the restaurant domain. Compared to gold-</head><label>2</label><figDesc></figDesc><table>Inferred Aspects 

Representative Words 
Gold Aspects 
Main Dishes 
beef, duck, pork, mahi, filet, veal 

Food 
Dessert 
gelato, banana, caramel, cheesecake, pudding, vanilla 
Drink 
bottle, selection, cocktail, beverage, pinot, sangria 
Ingredient 
cucumber, scallion, smothered, stewed, chilli, cheddar 
General 
cooking, homestyle, traditional, cuisine, authentic, freshness 
Physical Ambience 
wall, lighting, ceiling, wood, lounge, floor 
Ambience 
Adjectives 
intimate, comfy, spacious, modern, relaxing, chic 
Staff 
waitstaff, server, staff, waitress, bartender, waiter 
Staff 
Service 
unprofessional, response, condescending, aggressive, behavior, rudeness 
Price 
charge, paid, bill, reservation, came, dollar 
Price 
Anecdotes 
celebrate, anniversary, wife, fiance, recently, wedding 
Anecdotes 
Location 
park, street, village, avenue, manhattan, brooklyn 
Misc. 
General 
excellent, great, enjoyed, best, wonderful, fantastic 
Other 
aged, reward, white, maison, mediocrity, principle 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 4 ). Thus the F1 values cannot be computed directly from corresponding P/R values</head><label>4</label><figDesc></figDesc><table>Aspect 
Method 
Precision Recall 
F1 
LocLDA 
0.898 
0.648 
0.753 
ME-LDA 
0.874 
0.787 
0.828 
SAS 
0.867 
0.772 
0.817 
Food 
BTM 
0.933 
0.745 
0.816 
SERBM 
0.891 
0.854 
0.872 
k-means 3 
0.931 
0.647 
0.755 
ABAE 
0.953 
0.741 
0.828 
LocLDA 
0.804 
0.585 
0.677 
ME-LDA 
0.779 
0.540 
0.638 
SAS 
0.774 
0.556 
0.647 
Staff 
BTM 
0.828 
0.579 
0.677 
SERBM 
0.819 
0.582 
0.680 
k-means 
0.789 
0.685 
0.659 
ABAE 
0.802 
0.728 
0.757 
LocLDA 
0.603 
0.677 
0.638 
ME-LDA 
0.773 
0.558 
0.648 
SAS 
0.780 
0.542 
0.640 
Ambience 
BTM 
0.813 
0.599 
0.685 
SERBM 
0.805 
0.592 
0.682 
k-means 
0.730 
0.637 
0.677 
ABAE 
0.815 
0.698 
0.740 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Aspect identification results on the restau-
rant domain. The results of LocLDA and ME-
LDA are taken from (Zhao et al., 2010); the results 
of SAS and SERBM are taken from (Wang et al., 
2015). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Aspect identification results on the beer 
domain. 

</table></figure>

			<note place="foot" n="1"> http://gibbslda.sourceforge.net 2 http://code.google.com/p/btm/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research is partially funded by the Economic Development Board and the National Research Foundation of Singapore.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations</title>
		<meeting>the 3rd International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An unsupervised aspect-sentiment model for online reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Brody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noemie</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural sentiment classification with user and product attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cunchao</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Aspect extraction with automated prior knowledge learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Beyond the stars: Improving rating predictions using review text content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gayatree</forename><surname>Ganu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noemie</forename><surname>Elhadad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amélie</forename><surname>Marian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Workshop on the Web and Databases</title>
		<meeting>the 12th International Workshop on the Web and Databases</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočisk´kočisk´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Feuding families and former friends: Unsupervised learning for dynamic fictional relationship</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anupam</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Snigdha</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A novel lexicalized HMM-based learning framework for web opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung</forename><forename type="middle">Hay</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Machine Learning</title>
		<meeting>the 26th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Conference on Learning Representations</title>
		<meeting>the 2nd International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Structure-aware review mining and summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying-Ju</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Sentiment Analysis and Opinion Mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Morgan &amp; Claypool publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Effective approaches to attention based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning attitudes and attributes from multiaspect reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th IEEE International Conference on Data Mining</title>
		<meeting>the 12th IEEE International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Optimizing semantic coherence in topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edmund</forename><surname>Talley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Leenders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Aspect extraction through semi-supervised modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Opinion word expansion and target extraction through double propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="9" to="27" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A neural attention model for sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Grounded compositional semantics for finding and describing images with sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recognizing stances in online debates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swapna</forename><surname>Somasundaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Aspect level sentiment classification with deep memory network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Modeling online reviews with multi-grain topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International World Wide Web Conference</title>
		<meeting>the 17th International World Wide Web Conference</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sentiment-aspect extraction based on restricted Boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Recursive neural conditional random fields for aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenya</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sinno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokui</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Scaling up to large vocabulary image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Second International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A biterm topic model for short texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International World Wide Web Conference</title>
		<meeting>the 22nd International World Wide Web Conference</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised word and dependency path embeddings for aspect term extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Fifth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Jointly modeling aspects and opinions with a MaxEnt-LDA hybrid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Wayne Xin Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongfei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Movie review mining and summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 15th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
