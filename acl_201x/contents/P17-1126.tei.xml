<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:03+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Generate Market Comments from Stock Prices</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soichiro</forename><surname>Murakami</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiko</forename><surname>Watanabe</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Miyazawa</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keiichi</forename><surname>Goshima</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yanase</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroya</forename><surname>Takamura</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
						</author>
						<title level="a" type="main">Learning to Generate Market Comments from Stock Prices</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1374" to="1384"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1126</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper presents a novel encoder-decoder model for automatically generating market comments from stock prices. The model first encodes both short-and long-term series of stock prices so that it can mention short-and long-term changes in stock prices. In the decoding phase, our model can also generate a numerical value by selecting an appropriate arithmetic operation such as subtraction or rounding, and applying it to the input stock prices. Empirical experiments show that our best model generates market comments at the fluency and the informativeness approaching human-generated reference texts.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Various industries such as finance, pharmaceuti- cals, and telecommunications have been increas- ingly providing opportunities to treat various types of large-scale numerical time-series data. Such data are hard for non-specialists to interpret in de- tail and time-consuming even for specialists to con- strue. As a result, there has been a growing interest in automatically generating concise descriptions of such data, i.e., data summarization. This interest in data summarization is encouraged by the recent de- velopment of neural network-based text generation methods. Given an appropriate architecture, a neu- ral network can generate a sentence that is mostly grammatical and semantically reasonable.</p><p>In this study, we focus on the task of gen- erating market comments from a time-series of stock prices. We adopt an encoder-decoder model <ref type="bibr" target="#b31">(Sutskever et al., 2014</ref>) and exploit its capability to learn to capture the behavior of the input and generate a description of it. Although encoder- decoder models can learn to do this, they need to be  provided with an appropriate network-architecture and necessary information. We use <ref type="figure" target="#fig_1">Figure 1</ref> to illustrate the characteristic problems of comment generation for time-series of stock prices. The fig- ure shows the Nikkei Stock Average (Nikkei 225, or simply Nikkei), which is a stock market index calculated from 225 selected issues, on some con- secutive trading days accompanied by the market comments made at some specific time points in the span. The first problem is that market com- ments do not merely describe the increase and de- crease of the price. They also often describe how the price changes compared with the previous pe- riod, such as "continues to fall" in (3) of <ref type="figure" target="#fig_1">Figure 1</ref>, "turns to rise" in (2), and "rebound" in (6). Market comments sometimes describe the change in price compared with the prices in the previous week. The second problem is that market comments also contain expressions that depend on their delivery time: e.g., "opens with" in (1), "closing price of the morning session" in (3), and "beginning of the afternoon session" in (4). The third problem is that market comments typically contain numerical val- ues, which often cannot be copied from the input prices. Such numerical values probably cannot be generated as other words are generated by the stan- dard decoder. This difficulty can be easily under- stood as analogous with the difficulty of generat- ing named entities by encoder-decoder models. To derive such values, the model needs arithmetic op- erations such as subtraction as in examples <ref type="formula" target="#formula_4">(3)</ref> and (6) mentioning the difference in price and rounding as in example (5).</p><p>To address these problems, we present a novel encoder-decoder model to automatically generate market comments from stock prices. To address the first problem of capturing various types of change in different time scales, the model first en- codes data consisting of both short-and long-term time-series, where a multi-layer perceptron, a re- current neural network, or a convolutional network is adopted as a basic encoder. In the decoding phase, we feed our model with the delivery time of the market comment to generate the expressions depending on time of day to address the second problem. To address the third problem regarding with numerical values mentioned in the generated text, we allow our model to choose an arithmetic operation such as subtraction or rounding instead of generating a word.</p><p>The proposed methods are evaluated on the task of generating Japanese market comments on the Nikkei Stock Average. Automatic evaluation with BLEU score <ref type="bibr" target="#b27">(Papineni et al., 2002</ref>) and F-score of time-dependent expressions reveals that our model outperforms a baseline encoder-decoder model significantly. Furthermore, human assessment and error analysis prove that our best model generates characteristic expressions discussed above almost perfectly, approaching the fluency and the informa- tiveness of human-generated market comments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The task of generating descriptions from time- series or structured data has been tackled in vari- ous domains such as weather forecasts <ref type="bibr" target="#b5">(Belz, 2007;</ref><ref type="bibr" target="#b0">Angeli et al., 2010</ref>), healthcare <ref type="bibr" target="#b28">(Portet et al., 2009;</ref><ref type="bibr" target="#b3">Banaee et al., 2013b)</ref>, and sports ( <ref type="bibr" target="#b22">Liang et al., 2009)</ref>. Traditionally, many studies used hand- crafted rules <ref type="bibr" target="#b10">(Goldberg et al., 1994;</ref><ref type="bibr" target="#b8">Dale et al., 2003;</ref><ref type="bibr" target="#b29">Reiter et al., 2005</ref>). On the other hand, in- terest has recently been growing in automatically learning a correspondence relationship from data to text and generating a description of this relation- ship since large-scale data in diversified formats have become easy to acquire. In fact, a data-driven approach has been extensively studied nowadays for various tasks such as image caption generation ( <ref type="bibr" target="#b32">Vinyals et al., 2015</ref>) and weather forecast genera- tion ( <ref type="bibr" target="#b25">Mei et al., 2016b</ref>).</p><p>The task, called data-to-text or concept-to-text, is generally divided into two subtasks: content se- lection and surface realization. Whereas previous studies tackled the subtasks separately ( <ref type="bibr" target="#b4">Barzilay and Lapata, 2005;</ref><ref type="bibr" target="#b33">Wong and Mooney, 2007;</ref><ref type="bibr" target="#b23">Lu et al., 2009)</ref>, recent work has focused on solving them jointly using a single framework <ref type="bibr" target="#b6">(Chen and Mooney, 2008;</ref><ref type="bibr" target="#b14">Kim and Mooney, 2010;</ref><ref type="bibr" target="#b0">Angeli et al., 2010;</ref><ref type="bibr">Lapata, 2012, 2013</ref>).</p><p>More recently, there has been some work on an encoder-decoder model <ref type="bibr" target="#b31">(Sutskever et al., 2014</ref>) for generating a description from time-series or struc- tured data to solve the subtasks jointly in a sin- gle framework, and this model has been proven to be useful ( <ref type="bibr" target="#b25">Mei et al., 2016b;</ref><ref type="bibr" target="#b20">Lebret et al., 2016</ref>). However, the task of generating a descrip- tion from numerical time-series data presents diffi- culties such as the second and third problems men- tioned in Section 1. For the second problem, the model needs to be fed with information on deliv- ery time. Also, the model needs arithmetic op- erations such as subtraction for the third problem because even if we simply apply a copy mecha- nism ( <ref type="bibr" target="#b11">Gu et al., 2016;</ref><ref type="bibr" target="#b12">Gulcehre et al., 2016</ref>) to the model, it cannot derive a calculated value such as (3), (5), or (6) in <ref type="figure" target="#fig_1">Figure 1</ref> from input. Thus, in this work, we tackle these problems and develop a model on the basis of the encoder-decoder model that can mention a specific numerical value by re- ferring to the input data or producing a processed value with mathematical calculation and mention time-dependent expressions by incorporating the information on delivery time into its decoder.</p><p>There has also been some work on generating market comments. <ref type="bibr" target="#b19">Kukich (1983)</ref> developed a sys- tem consisting of rule-based components for gen- erating stock reports from a database of daily stock quotes. Although she used several components individually and had to define a number of rules for the generation, our encoder-decoder model can perform it with fewer and simpler rules for the cal- culation. <ref type="bibr" target="#b1">Aoki and Kobayashi (2016)</ref> developed a method on the basis of a weighted bi-gram lan- guage model for automatically describing trends of time-series data such as the Nikkei Stock Av- erage. However, they did not attempt to refer to specific numerical values such as closing prices and amounts of rises in price although such de- scriptions are often used in market comments as shown in <ref type="figure" target="#fig_1">Figure 1 (3)</ref>, <ref type="formula">(5)</ref>, and (6). In contrast, we present a novel approach to generate natural lan- guage descriptions of time-series data that can not only able to describe trends of the data but also mention specific numerical values by referring to the time-series data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Generating Market Comments</head><p>To generate market comments on stock prices, we introduce an encoder-decoder model. Encoder- decoder models have been widely used and proven useful in various tasks of natural language genera- tion such as machine translation ( <ref type="bibr" target="#b7">Cho et al., 2014</ref>) and text summarization ( <ref type="bibr" target="#b30">Rush et al., 2015)</ref>. Our task is similar to these tasks in that the system takes sequential data and generates text. Therefore, it is natural to use an encoder-decoder model in mod- eling stock prices.</p><p>Figure 2 illustrates our model. In describing time-series data, the model is expected to cap- ture various types of change and important val- ues in the given sequence, such as absolute or rel- ative changes and maximum or minimum value, in different time-scales. Moreover, it is neces- sary to generate time-dependent comments and nu- merical values that require arithmetic operations for derivation, such as "The closing price of the morning session decreases by 5 yen...". To achieve these, we present three strategies that alter the stan- dard encoder-decoder model. First (Section 3.1), we use several encoding methods for time-series data, as in (1) of <ref type="figure" target="#fig_2">Figure 2</ref>, to capture the changes and important values. Sec- ond (Section 3.2), we incorporate delivery-time in- formation into the decoder, as in (2) of <ref type="figure" target="#fig_2">Figure 2</ref>, to generate time-dependent comments. For the de- coder, we use a recurrent neural network language model (RNNLM) ( <ref type="bibr" target="#b26">Mikolov et al., 2010)</ref>, which is widely used in language generation tasks. Finally (Section 3.3), we extend the decoder to estimate arithmetic operations, as in (3) of <ref type="figure" target="#fig_2">Figure 2</ref>, to gen- erate numerical values in market comments.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Encoding Numerical Time-Series Data</head><p>We prepare short-and long-term data, using the five-minute chart of Nikkei 225. A vector for short-term data consists of the prices of one trad- ing day and has N elements. We denote it as</p><formula xml:id="formula_0">x short = x short, i N −1 i=0</formula><p>. On the other hand, a vec- tor for long-term data consists of the closing prices of the M preceding trading days. It is denoted as</p><formula xml:id="formula_1">x long = x long, i M−1 i=0</formula><p>. Data are commonly preprocessed to remove noise and enhance generalizability of a model ( <ref type="bibr" target="#b34">Zhang and Qi, 2005;</ref><ref type="bibr" target="#b2">Banaee et al., 2013a</ref>). We use two preprocessing methods: standardization and moving reference. Standardization substitutes each element x i of input x by</p><formula xml:id="formula_2">x std i = x i − µ σ ,<label>(1)</label></formula><p>where µ and σ are the mean and standard devia- tion of the values in the training data, respectively. Standardized values are less affected by scale. The second method, moving reference ( <ref type="bibr" target="#b9">Freitas et al., 2009</ref>), substitutes each element x i of input x by</p><formula xml:id="formula_3">x move i = x i − r i ,<label>(2)</label></formula><p>where r i is the closing price of the previous trad- ing day of x. This is introduced to capture price fluctuations from the previous day. By applying one of the preprocessing methods to x short and x long , we obtain two vectors of prepro- cessed values l short and l long . Given these, each en- coder emits the corresponding hidden states h short and h long . After obtaining the hidden states, we concatenate the two vectors of the preprocessed values and the outputs of the encoders as a multi- level representation of the input time-series data. The multi-level representation is an approach de- veloped by <ref type="bibr" target="#b24">Mei et al. (2016a)</ref> that enable the de- coder to take into account both the high-level rep- resentation, e.g., h short , h long , and the low-level representation, e.g., l short , l long , at the same time. They have shown that it improves performance in terms of selecting salient objects in input data. We thus set the initial hidden state s 0 of the decoder as</p><formula xml:id="formula_4">s 0 = l short ⊕ l long ⊕ h short ⊕ h long ,<label>(3)</label></formula><p>where ⊕ is the concatenation operator. In this case, we introduce four en- coders, and set the initial hidden state s 0 of the de- coder as</p><formula xml:id="formula_5">s 0 = l move short ⊕ l std short ⊕ l move long ⊕ l std long ⊕ h move short ⊕ h std short ⊕ h move long ⊕ h std long . (4)</formula><p>Since several encoding methods can be used for the time-series data, we use any one of the three conventional neural networks: Multi-Layer Perceptron (MLP), Convolutional Neural Network (CNN), or Recurrent Neural Network (RNN) with Long Short-Term Memory cells <ref type="bibr" target="#b13">(Hochreiter and Schmidhuber, 1997</ref>). In the experiments, we em- pirically evaluate and compare the encoding meth- ods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Incorporating Time Embedding</head><p>Even if identical sequences of values are observed, comments usually vary in accordance with price history or the time they are observed. For instance, when the market opens, comments usually men- tion how much the stock price has increased or de- creased compared with the closing price of the pre- vious trading day, as in (1) and (3) in <ref type="figure" target="#fig_1">Figure 1</ref>.</p><p>Our model creates vectors called time embed- ding vectors T on the basis of the time when the comment is delivered (e.g., 9:00 a.m. or 3:00 p.m.). Then a time embedding vector is added to each hidden state s j in decoding so that words are generated depending on time. This mechanism is inspired by speaker embedding introduced by . They use an encoder-decoder model for a conversational agent that inherits the charac- teristics of a speaker, such as his/her manner of speaking. They encode speaker-specific informa- tion (e.g., dialect, age, and gender) into speaker embedding vectors and used them in decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Estimation of Arithmetic Operations</head><p>Text generation systems based on language models such as RNNLM often generate erroneous words for named entities; that is, they often mention a similar but incorrect entity, e.g., Nissan for Toyota. To overcome this problem, <ref type="bibr" target="#b12">Gulcehre et al. (2016)</ref> developed a text generation method called copy mechanism. The method copies rare words miss- ing from the vocabulary from a given sequence of words using an attention mechanism, and emits the copied words.</p><p>Market comments often mention numerical val- ues that appear in the input data, but they also men- tion values obtained through arithmetic operations, such as differences in prices as in <ref type="formula" target="#formula_4">(3)</ref> and <ref type="formula">(6)</ref>  To enable our model to generate text with values calculated from input values, we add generaliza- tion tags to the vocabulary used in the model. Each generalization tag represents a type of arithmetic operation. When a generalization tag is emitted, the model performs the operations on the desig- nated values in accordance with the tag, replaces the tag with the calculated value, and finally out- puts text containing numerical values. For pre- processing, we replace each numerical value ap- pearing in the market comments in the training data with generalization tags such as &lt;price1&gt;. The tag for a numerical value depends on what the value stands for in the text. Since this comment omits the phrase "than the  Return ∆ &lt;price2&gt;</p><p>Round down ∆ to the nearest 10 &lt;price3&gt;</p><p>Round down ∆ to the nearest 100 &lt;price4&gt;</p><p>Round up ∆ to the nearest 10 &lt;price5&gt;</p><p>Round up ∆ to the nearest 100 &lt;price6&gt;</p><p>Return z as it is &lt;price7&gt;</p><p>Round down z to the nearest 100 &lt;price8&gt;</p><p>Round down z to the nearest 1,000 &lt;price9&gt;</p><p>Round down z to the nearest 10,000 &lt;price10&gt; Round up z to the nearest 100 &lt;price11&gt; Round up z to the nearest 1,000 &lt;price12&gt; Round up z to the nearest 10,000 <ref type="table" target="#tab_1">Table 1</ref>: Generalization tags and corresponding arithmetic operations. Here z and ∆ stand for latest price and difference between z and closing price of previous trading day.</p><p>closing price of the previous day", 227 in this ex- ample indicates the difference between the clos- ing price of the previous trading day x long, M−1 and the latest price x short, N −1 denoted by z in Ta- ble 1. Therefore, we replace 227 with the tag &lt;price1&gt;. Likewise, we replace 16,610 with &lt;price6&gt; because it represents the latest price z. To find the optimal tag for each value, we try all the types of operations listed in <ref type="table" target="#tab_1">Table 1</ref> using the values appearing in the text, i.e., 227 and 16,610 in this case. Then, we select the tag that has the op- eration that yields the value closest to the original one.</p><p>In prediction, the model first generates a ten- tative comment, which includes tags as well as words. Suppose that the input vectors are x short and x long , with x short, N −1 = 14508 and x long, M−1 = 14612, and that the model generates the comment below:</p><p>(b) Nikkei opens turning down. The loss ex- ceeds &lt;price2&gt; yen, and it falls to the &lt;price7&gt; yen level.</p><p>Since the tag &lt;price2&gt; represents "the differ- ence between x short, N −1 and x long, M−1 rounded down to the nearest 10", we replace the tag with 100. Similarly, we replace &lt;price7&gt;, which is "the last price x short, N −1 rounded down to the near- est 100", with 14,500. Finally, we have a market comment containing the numbers as below:</p><p>(c) Nikkei opens turning down. The loss ex- ceeds 100 yen, and it falls to the 14,500 yen level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>We used the five-minute chart of Nikkei 225 from March 2013 to October 2016 as numerical time- series data, which were collected from IBI-Square Stocks 1 , and 7,351 descriptions as market com- ments, which are written in Japanese and provided by Nikkei QUICK News. We divided the dataset into three parts: 5,880 for training, 730 for val- idation, and 741 for testing. For a human eval- uation, we randomly selected 100 comments and their time-series data included in the test set. We set N = 62, which is the number of time steps for stock prices for one trading day, and M = 7, which is the number of the time steps for clos- ing prices of the preceding trading days. We used Adam ( <ref type="bibr" target="#b15">Kingma and Ba, 2015)</ref> for optimization with a learning rate of 0.001 and a mini-batch size of 100. The dimensions of word embeddings, time embeddings, and hidden states for both the encoder and decoder are set to 128, 64, and 256, respec- tively. For CNN, we used a single convolutional layer and set the filter size to 3.</p><p>In the experiments, we conducted three types of evaluation: two for automatic evaluation, and one for human evaluation. For one automatic evalua- tion, we used BLEU ( <ref type="bibr" target="#b27">Papineni et al., 2002</ref>) to mea- sure the matching degree between the market com- ments written by humans as references and out- put comments generated by our model. We ap- plied paired bootstrap resampling <ref type="bibr" target="#b16">(Koehn, 2004</ref>) for a significance test. For the other automatic evaluation metric, we calculate F-measures for time-dependent expressions, using market com- ments written by humans as references, to investi- gate whether our model can correctly output time- dependent expressions such as "open with" and de- scribe how the price changes compared with the previous period referring to the series of preced- ing prices such as "continual fall". Specifically, we calculate F-measures for 13 expressions shown in <ref type="figure" target="#fig_7">Figure 3</ref>.</p><p>For the human evaluation, we recruited a spe- cialist in financial engineering as a judge to eval- uate the quality of generated market comments. To evaluate the difference in the quality of gener- ated comments between our models and human, we showed both system-generated and human- generated market comments together with their   time-series data consisting of x short and x long , with- out letting the judge know which comment is gen- erated by which method. We asked the judge to give each market comment two scores: one for in- formativeness and one for fluency. Both scores have two levels, 0 or 1, where 1 indicates high in- formativeness or fluency. For informativeness, the judge used both generated comments and their in- put stock prices to rate the comments. Specifically, if the judge deem that a generated comment de- scribes an important price movement or an outline of the movement properly, such comments are con- sidered to be informative. For fluency, the judge read only the generated comments and rate them in terms of readability, regardless of their content of the comment.</p><p>In addition, since some of the market comments written by humans sometimes include external in- formation such as "Nikkei opens with a continual fall as yen pressures exporters", we also asked the judge to ignore the correctness of external informa- tion mentioned in comments, for the sake of fair- ness in comparison, because external information cannot be retrieved from the time-series data.</p><p>To assess the effectiveness of the techniques we introduced, we conducted experiments with 11 models. <ref type="table" target="#tab_3">Table 2</ref> shows an overview of the models  <ref type="table">Table 3</ref>: BLEU scores on the test set. Differences between the best model, mlp-enc, and other models are statistically significant at p &lt; 0.05.</p><p>we compared. We compared three types of mod- els: a baseline, full models (e.g., mlp-enc), and ab- lated models (e.g., -short). For example, -short is a model that does not use the short-term time series. <ref type="table">Table 3</ref> shows the BLEU scores on the test set. <ref type="figure" target="#fig_7">Figure 3</ref> presents the F-measure of the models for each phrase. We also present output examples with human-generated market comments (Human) for reference in <ref type="figure" target="#fig_11">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>In the results for the automatic evaluation in BLEU, the model using both MLP as encoders and all the techniques we developed, mlp-enc, outper- formed baseline and the other models. The BLEU scores and F-measure values revealed differences among the models using MLP, CNN, or RNN (mlp-  Long-term data (b) Price movements of Nikkei on July 21, 2016.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model F I Generated comment</head><p>baseline 1 0 Nikkei heikin, han-patsu zen-bike wa 81 en daka no &lt;unk&gt; en Nikkei average, rebound-pop first_half-closing top 81 yen higher gen &lt;unk&gt; yen Nikkei rebounds. The closing price of the morning session is &lt;unk&gt; yen, which is 81 yen higher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>mlp-enc 1 1</head><p>Nikkei heikin, oo-haba han-patsu oo-bike wa 1,069 en daka no 16,022 en Nikkei average, big-range rebound-pop big-closing top 1,069 yen higher gen 16,022 yen Nikkei significantly rebounds. The closing price is 16,022 yen, which is 1,069 yen higher.  enc, cnn-enc, rnn-enc). In the comparison between the models that took two types of the time-series data x short , x long as input (e.g., mlp-enc or rnn-enc) and the models that only used one of them (-short, -long), the models using both types of data such as mlp-enc and rnn-enc gained higher BLEU scores than -short and -long. Also, the models that en- coded the two types of time-series data to capture their short-and long-term changes correctly output more expressions that described the changes such as "turn to rise", "continue to fall", and "rebound" than -short and -long as shown in <ref type="figure" target="#fig_7">Figure 3</ref>.</p><p>According to the comparison between prepro- cessing methods, mlp-enc, which used both stan- dardization and moving reference as preprocessing methods, obtained a higher BLEU score than the models that used neither (-std, -move). In terms of the F-measure values, mlp-enc output phrases mentioning changes more appropriately and there- fore achieved the higher values than the other two models as in "turn to rise" or "turn to fall" in <ref type="figure" target="#fig_7">Fig- ure 3</ref>. Furthermore, we found that the BLEU score of -multi, which did not use the multi-level repre- sentation of the data, was inferior. In other words, incorporating the multi-level representation along with an output of an encoder into a decoder seems  <ref type="figure">Figure 5</ref>: BLEU scores of market comments gen- erated by models for each size of training data on the validation set.</p><p>to contribute to improving the automatic evalua- tion and producing a better representation of the input data. baseline and -num output numerical values as "words" from the vocabulary for RNNLM because these models do not use any arithmetic opera- tion. Therefore, there were many cases including &lt;unk&gt; that should be output as a numerical value as shown in <ref type="figure" target="#fig_11">Figure 4</ref> (a). We found that -num had a lower BLEU score than the models such as mlp-enc and -std that used arithmetic operations. Further- more, we observed that the models with arithmetic operations correctly generated stock prices in most cases.</p><p>By comparing -time, which did not incorporate time-embeddings into a decoder, and other mod- els such as mlp-enc with respect to the F-measure of expressions depending on delivery time (e.g., "open with" or "closing session"), we found that the models that took time information into account, such as mlp-enc, generated those phrases more ac- curately than -time.</p><p>Moreover, we analyzed the effect of different sizes of training data. <ref type="figure">Figure 5</ref> shows BLEU scores of market comments generated by our models for each size of training data on the validation set. Ac- cording to the results, we found that the BLEU scores for the models saturated when we used 3000 training data. In addition, there was not much dif- ference in convergence speed among the models.</p><p>The human evaluation results in <ref type="table">Table 4</ref> in- dicate that market comments generated by our model (mlp-enc) achieved a quality comparable even to that of market comments written by hu- mans. Moreover, we found that mlp-enc signifi-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Informativeness Fluency <ref type="table" target="#tab_1">External   Human  95  95  25  mlp-enc  85  93  1  baseline  28  100  6   Table 4</ref>: Results of human evaluation. Each score indicates number of market comments judged to be level-1. External shows number of market com- ments including external information.</p><p>cantly outperformed baseline in terms of informa- tiveness but was outperformed by baseline in terms of fluency. The reason was that mlp-enc occasion- ally generated a market comment such as "Nikkei gains more than 0 yen" because of an error in the prediction of the operation, and such comments were not considered not to be fluent or informative by the judge, although most of comments gener- ated by mlp-enc were as fluent as those of baseline.</p><p>Note that baseline does not generate expressions like "0 yen" because they are not normally used in market comments and so not included in the vo- cabulary. Therefore, the judge considered all the comments generated by baseline to be fluent. For another possibility to enhance our model, we have to consider that the model should men- tion a difference or gain for a duration from when to when. For example, our current model some- times generated a market comment such as "Nikkei gains more than 200 yen", although Nikkei actu- ally gained more than 300 yen. Such a comment is not incorrect but is imprecise. Therefore, we con- sider that a mechanism is needed to select the pe- riod to be mentioned when the model generates a comment to this problem and increase the general- izability of our model for generating a description from various time-series data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this study, we presented a novel encoder-decoder model to automatically generate market comments from numerical time-series data of stock prices, using the Nikkei Stock Average as an example. Descriptions of numerical time-series data writ- ten by humans such as market comments have sev- eral writing style characteristics. For example, (1) content to be mentioned in the market comments varies depending on short-or long-term changes of the time-series data, (2) expressions depending on delivery time at which text is written are used, and (3) numerical values obtained through arith-metic operations applied to the input data are often described. We developed approaches for generat- ing comments that have these characteristics and showed the effectiveness of the proposed model.</p><p>In future work, we plan to apply our model to descriptions of time-series data in various domains such as weather forecasts and sports, which share the above writing-style characteristics. We also plan to use multiple time-series as input such as multiple brands of stock.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>15:00 9:00 10:00 11:00 12:00 13:00 14:00 15:00 Time Stock price [yen] Time Comment (1) 09:00 Nikkei opens with a continual fall. (2) 09:29 Nikkei turns to rise. (3) 11:30 Nikkei continues to fall. The clos- ing price of the morning session de- creases by 5 yen to 19,386 yen. (4) 12:30 Nikkei rises at the beginning of the afternoon session. (5) 13:54 Nikkei gains more than 100 yen. (6) 15:00 Nikkei rebounds and closes up 102 yen to 19,494 yen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Nikkei 225 and market comments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of our model. Here l short and l long represent two vectors of preprocessed values, and h short and h long indicate hidden states of the encoder. T represents a time embedding vector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>When we use both preprocessing methods, we have four preprocessed input vectors: l move short , l std short , l move long , and l std long .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>in Fig- ure 1, or rounded values as in (5). Thus, another problem arises: what type of operation is suitable for text to be generated? In this work, we solve this problem by extending the idea of copy mechanism.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: F-measure values for the expressions on the test set. Each expression is accompanied by its original Japanese expression transliterated into English alphabet in parenthesis. Out of the 13 expressions, 10 on the left are expressions that describe how the price changes compared with the previous period, and 3 on the right are time-dependent expressions. Model baseline mlp-enc cnn-enc rnn-enc-short-long-std-move-multi-num-time Encoder MLP MLP CNN RNN MLP MLP MLP MLP MLP MLP MLP Input data x short − x long − − Preprocessing Standardization − Moving reference − Multi-level − − Arithmetic operation − − Time-embedding − −</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Model baseline mlp-enc cnn-enc rnn-enc -short -long BLEU 0.243 0.464 0.449 0.454 0.380 0.433 Model -std -move -multi -num -time BLEU 0.455 0.393 0.435 0.318 0.395</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>15000</head><label>15000</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Nikkei heikin, oo-haba han-patsu oo-bike wa 1,069 en daka no 16,022 en Nikkei average, big-range rebound-pop big-closing top 1,069 yen higher gen 16,022 yen Nikkei significantly rebounds. The closing price is 16,022 yen, which is 1,069 yen higher. (c) Comments on price at 3:00 p.m. on February 15, 2016. Model F I Generated comment baseline 1 0 Nikkei heikin, zoku-shin de hajimaru age-haba 100 en koeru Nikkei average, continual-advance instr open-imperf raise-range 100 yen exceed-imperf Nikkei opens with a continual rise. The gain exceeds 100 yen. mlp-enc 1 1 Nikkei heikin, age-haba 200 en koeru Nikkei average, raise-range 200 yen exceed-imperf Nikkei gains more than 200 yen. human 1 1 Nikkei heikin, age-haba 200 en kosu Nikkei average, raise-range 200 yen exceed-imperf Nikkei gains more than 200 yen. (d) Comments on price at 9:00 a.m. on July 21, 2016.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Examples of short-and long-term movements of Nikkei, and comments models made on them, where &lt;unk&gt; represents an unknown word. Columns F and I show scores on fluency and informativeness in human evaluation. Each example is accompanied by original Japanese comment transliterated into English alphabet, its literal translation, and the corresponding English sentence. Abbreviations used here are as follows. top: topic case, gen: genitive case, instr: instrumental case, and imperf: imperfect form of a verb.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 displays</head><label>1</label><figDesc></figDesc><table>all the 
tags and the corresponding types of calculation. To 
illustrate, suppose a market comment says 

(a) Nikkei rebounds. The closing price of the 
morning session is 16,610 yen, which is 
227 yen higher. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Overview of the models we used in the experiments.</figDesc><table></table></figure>

			<note place="foot" n="1"> http://www.ibi-square.jp/index.htm</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This paper is based on results obtained from a project commissioned by the New Energy and Industrial Technology Development Organization (NEDO).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A simple domain-independent probabilistic approach to generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D10-1049" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="502" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Linguistic summarization using a weighted n-gram language model based on the similarity of time-series data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kasumi</forename><surname>Aoki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ichiro</forename><surname>Kobayashi</surname></persName>
		</author>
		<idno type="doi">10.1109/FUZZ-IEEE.2016.7737741</idno>
		<ptr target="https://doi.org/10.1109/FUZZ-IEEE.2016.7737741" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Fuzzy Systems</title>
		<meeting>IEEE International Conference on Fuzzy Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="595" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A framework for automatic text generation of trends in physiological time series data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hadi Banaee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Mobyen Uddin Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loutfi</surname></persName>
		</author>
		<idno type="doi">10.1109/SMC.2013.661</idno>
		<ptr target="https://doi.org/10.1109/SMC.2013.661" />
	</analytic>
	<monogr>
		<title level="m">Processing of IEEE International Conference on Systems, Man, and Cybernetics</title>
		<meeting>essing of IEEE International Conference on Systems, Man, and Cybernetics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3876" to="3881" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards NLG for physiological data monitoring with body area networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hadi Banaee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Mobyen Uddin Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loutfi</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/W13-2127" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th European Workshop on Natural Language Generation. Association for Computational Linguistics</title>
		<meeting>the 14th European Workshop on Natural Language Generation. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="193" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Collective content selection for concept-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/H05-1042" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing</title>
		<meeting>the conference on Human Language Technology and Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="331" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Probabilistic generation of weather forecast texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anja</forename><surname>Belz</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/N07-1021" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 2007 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="164" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to sportscast: A test of grounded language acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
		<idno type="doi">10.1145/1390156.1390173</idno>
		<ptr target="https://doi.org/10.1145/1390156.1390173" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="128" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="doi">10.3115/v1/D14-1179</idno>
		<ptr target="https://doi.org/10.3115/v1/D14-1179" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">CORAL: Using natural language generation for navigational assistance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Geldof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Philippe</forename><surname>Prost</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=783106.783111" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Australasian Computer Science Conference</title>
		<meeting>the 26th Australasian Computer Science Conference</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="35" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Prediction-based portfolio optimization model using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">D</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><forename type="middle">F De</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ailson</forename><forename type="middle">R</forename><surname>De Almeida</surname></persName>
		</author>
		<idno type="doi">10.1016/j.neucom.2008.08.019</idno>
		<ptr target="https://doi.org/10.1016/j.neucom.2008.08.019" />
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2155" to="2170" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Using natural-language processing to produce weather forecasts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norbert</forename><surname>Driedger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">I</forename><surname>Kittredge</surname></persName>
		</author>
		<idno type="doi">10.1109/64.294135</idno>
		<ptr target="https://doi.org/10.1109/64.294135" />
	</analytic>
	<monogr>
		<title level="j">IEEE Expert</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="45" to="53" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">O K</forename><surname>Li</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/P16-1154</idno>
		<ptr target="https://doi.org/10.18653/v1/P16-1154" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1631" to="1640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pointing the unknown words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/P16-1014</idno>
		<ptr target="https://doi.org/10.18653/v1/P16-1014" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="140" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="doi">10.1162/neco.1997.9.8.1735</idno>
		<ptr target="https://doi.org/10.1162/neco.1997.9.8.1735" />
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generative alignment and semantic parsing for learning from ambiguous supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joohyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/C10-2062" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="543" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1412.6980" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations</title>
		<meeting>the 3rd International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Statistical significance tests for machine translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/W04-3250" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2004 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="388" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised concept-to-text generation with hypergraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/N12-1093" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="752" to="761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Inducing document plans for concept-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D13-1157" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1503" to="1514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Design of a knowledge-based report generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Kukich</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P83-1022" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1983" />
			<biblScope unit="page" from="145" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural text generation from structured data with application to the biography domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Lebret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/D16-1128</idno>
		<ptr target="https://doi.org/10.18653/v1/D16-1128" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1203" to="1213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A persona-based neural conversation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Spithourakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/P16-1094</idno>
		<ptr target="https://doi.org/10.18653/v1/P16-1094" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="994" to="1003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning semantic correspondences with less supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P09-1011" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association for Computational Linguistics and International Joint Conference on Natural Language Processing</title>
		<meeting>Association for Computational Linguistics and International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Natural language generation with tree conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wee Sun</forename><surname>Lee</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D09-1042" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="400" to="409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Listen, attend, and walk: Neural mapping of navigational instructions to action sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Walter</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1506" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association for the Advancement of Artificial Intelligence</title>
		<meeting>Association for the Advancement of Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">What to talk about and how? selective generation using lstms with coarse-to-fine alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Walter</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/N16-1086</idno>
		<ptr target="https://doi.org/10.18653/v1/N16-1086" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="720" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukáš</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Černocký</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
		<ptr target="http://www.isca-speech.org/archive/interspeech_2010/i10_1045.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Annual Conference of the International Speech Communication Association. International Speech Communication Association</title>
		<meeting>the 11th Annual Conference of the International Speech Communication Association. International Speech Communication Association</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">BLEU: A method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P02-1040" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Automatic generation of textual summaries from neonatal intensive care data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Portet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somayajulu</forename><surname>Sripada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvonne</forename><surname>Freer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cindy</forename><surname>Sykes</surname></persName>
		</author>
		<idno type="doi">10.1016/j.artint.2008.12.002</idno>
		<ptr target="https://doi.org/10.1016/j.artint.2008.12.002" />
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">173</biblScope>
			<biblScope unit="issue">7-8</biblScope>
			<biblScope unit="page" from="789" to="816" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Choosing words in computergenerated weather forecasts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somayajulu</forename><surname>Sripada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Davy</surname></persName>
		</author>
		<idno type="doi">10.1016/j.artint.2005.06.006</idno>
		<ptr target="https://doi.org/10.1016/j.artint.2005.06.006" />
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">167</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="137" to="169" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/D15-1044</idno>
		<ptr target="https://doi.org/10.18653/v1/D15-1044" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1411.4555" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Generation by inverting a semantic parser that uses statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuk</forename><forename type="middle">Wah</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/N07-1022" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 2007 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="172" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Neural network forecasting for seasonal and trend time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Qi</surname></persName>
		</author>
		<idno type="doi">10.1016/j.ejor.2003.08.037</idno>
		<ptr target="https://doi.org/10.1016/j.ejor.2003.08.037" />
	</analytic>
	<monogr>
		<title level="j">European journal of operational research</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="501" to="514" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
