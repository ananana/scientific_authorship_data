<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:21+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automatic Text Scoring Using Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>August 7-12, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Alikaniotis</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Yannakoudakis</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Rei</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Theoretical and Applied Linguistics</orgName>
								<orgName type="laboratory">The ALTA Institute Computer Laboratory University of Cambridge Cambridge</orgName>
								<orgName type="institution">University of Cambridge Cambridge</orgName>
								<address>
									<country>UK, UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">The ALTA Institute Computer Laboratory University of Cambridge Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Automatic Text Scoring Using Neural Networks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="715" to="725"/>
							<date type="published">August 7-12, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Automated Text Scoring (ATS) provides a cost-effective and consistent alternative to human marking. However, in order to achieve good performance, the pre-dictive features of the system need to be manually engineered by human experts. We introduce a model that forms word representations by learning the extent to which specific words contribute to the text&apos;s score. Using Long-Short Term Memory networks to represent the meaning of texts, we demonstrate that a fully automated framework is able to achieve excellent results over similar approaches. In an attempt to make our results more interpretable, and inspired by recent advances in visualizing neural networks, we introduce a novel method for identifying the regions of the text that the model has found more discriminative.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automated Text Scoring (ATS) refers to the set of statistical and natural language processing tech- niques used to automatically score a text on a marking scale. The advantages of ATS systems have been established since Project Essay Grade (PEG) <ref type="bibr">(Page, 1967;</ref><ref type="bibr" target="#b13">Page, 1968)</ref>, one of the earli- est systems whose development was largely moti- vated by the prospect of reducing labour-intensive marking activities. In addition to providing a cost-effective and efficient approach to large-scale grading of (extended) text, such systems ensure a consistent application of marking criteria, there- fore facilitating equity in scoring.</p><p>There is a large body of literature with re- gards to ATS systems of text produced by non- native English-language learners <ref type="bibr" target="#b13">(Page, 1968;</ref><ref type="bibr" target="#b0">Attali and Burstein, 2006;</ref><ref type="bibr" target="#b15">Rudner and Liang, 2002;</ref><ref type="bibr" target="#b11">Elliot, 2003;</ref><ref type="bibr">Landauer et al., 2003;</ref><ref type="bibr" target="#b2">Briscoe et al., 2010;</ref><ref type="bibr" target="#b28">Yannakoudakis et al., 2011;</ref><ref type="bibr" target="#b16">Sakaguchi et al., 2015</ref>, among others), overviews of which can be found in various studies <ref type="bibr" target="#b26">(Williamson, 2009;</ref><ref type="bibr" target="#b10">Dikli, 2006;</ref><ref type="bibr" target="#b17">Shermis and Hammer, 2012</ref>). Im- plicitly or explicitly, previous work has primarily treated text scoring as a supervised text classifica- tion task, and has utilized a large selection of tech- niques, ranging from the use of syntactic parsers, via vectorial semantics combined with dimension- ality reduction, to generative and discriminative machine learning.</p><p>As multiple factors influence the quality of texts, ATS systems typically exploit a large range of textual features that correspond to different properties of text, such as grammar, vocabulary, style, topic relevance, and discourse coherence and cohesion. In addition to lexical and part-of- speech (POS) ngrams, linguistically deeper fea- tures such as types of syntactic constructions, grammatical relations and measures of sentence complexity are among some of the properties that form an ATS system's internal marking criteria. The final representation of a text typically consists of a vector of features that have been manually se- lected and tuned to predict a score on a marking scale.</p><p>Although current approaches to scoring, such as regression and ranking, have been shown to achieve performance that is indistinguishable from that of human examiners, there is substantial man- ual effort involved in reaching these results on dif- ferent domains, genres, prompts and so forth. Lin- guistic features intended to capture the aspects of writing to be assessed are hand-selected and tuned for specific domains. In order to perform well on different data, separate models with distinct fea- ture sets are typically tuned.</p><p>Prompted by recent advances in deep learning and the ability of such systems to surpass state-of- the-art models in similar areas <ref type="bibr" target="#b25">(Tang, 2015;</ref><ref type="bibr" target="#b24">Tai et al., 2015)</ref>, we propose the use of recurrent neural network models for ATS. Multi-layer neural net- works are known for automatically learning use- ful features from data, with lower layers learn- ing basic feature detectors and upper levels learn- ing more high-level abstract features ( <ref type="bibr">Lee et al., 2009)</ref>. Additionally, recurrent neural networks are well-suited for modeling the compositionality of language and have been shown to perform very well on the task of language modeling <ref type="bibr">(Mikolov et al., 2011;</ref><ref type="bibr" target="#b3">Chelba et al., 2013)</ref>. We therefore propose to apply these network structures to the task of scoring, in order to both improve the per- formance of ATS systems and learn the required feature representations for each dataset automat- ically, without the need for manual tuning. More specifically, we focus on predicting a holistic score for extended-response writing items. <ref type="bibr">1</ref> However, automated models are not a panacea, and their deployment depends largely on the abil- ity to examine their characteristics, whether they measure what is intended to be measured, and whether their internal marking criteria can be in- terpreted in a meaningful and useful way. The deep architecture of neural network models, how- ever, makes it rather difficult to identify and ex- tract those properties of text that the network has identified as discriminative. Therefore, we also describe a preliminary method for visualizing the information the model is exploiting when assign- ing a specific score to an input text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we describe a number of the more influential and/or recent approaches in automated text scoring of non-native English-learner writing.</p><p>Project Essay Grade <ref type="bibr">(Page, 1967;</ref><ref type="bibr" target="#b13">Page, 1968;</ref><ref type="bibr" target="#b14">Page, 2003</ref>) is one of the earliest automated scor- ing systems, predicting a score using linear regres- sion over vectors of textual features considered to be proxies of writing quality. Intelligent Essay Assessor ( <ref type="bibr">Landauer et al., 2003</ref>) uses Latent Se- mantic Analysis to compute the semantic similar- ity between texts at specific grade points and a test text, which is assigned a score based on the ones in the training set to which it is most similar. <ref type="bibr">Lonsdale and Strong-Krause (2003)</ref> use the Link Gram- mar parser <ref type="bibr" target="#b20">(Sleator and Templerley, 1995)</ref> to anal- yse and score texts based on the average sentence- level scores calculated from the parser's cost vec- tor.</p><p>The Bayesian Essay Test Scoring sYstem <ref type="bibr" target="#b15">(Rudner and Liang, 2002</ref>) investigates multinomial and Bernoulli Naive Bayes models to classify texts based on shallow content and style features. e- Rater ( <ref type="bibr" target="#b0">Attali and Burstein, 2006</ref>), developed by the Educational Testing Service, was one of the first systems to be deployed for operational scor- ing in high-stakes assessments. The model uses a number of different features, including aspects of grammar, vocabulary and style (among others), whose weights are fitted to a marking scheme by regression. <ref type="bibr" target="#b4">Chen et al. (2010)</ref> use a voting algorithm and address text scoring within a weakly supervised bag-of-words framework. <ref type="bibr" target="#b28">Yannakoudakis et al. (2011)</ref> extract deep linguistic features and employ a discriminative learning-to-rank model that out- performs regression.</p><p>Recently, <ref type="bibr" target="#b7">McNamara et al. (2015)</ref> used a hier- achical classification approach to scoring, utilizing linguistic, semantic and rhetorical features, among others. <ref type="bibr" target="#b12">Farra et al. (2015)</ref> utilize variants of lo- gistic and linear regression and develop models that score persuasive essays based on features ex- tracted from opinion expressions and topical ele- ments.</p><p>There have also been attempts to incorporate more diverse features to text scoring models. <ref type="bibr">Klebanov and Flor (2013)</ref> demonstrate that essay scoring performance is improved by adding to the model information about percentages of highly associated, mildly associated and dis-associated pairs of words that co-exist in a given text. <ref type="bibr" target="#b22">Somasundaran et al. (2014)</ref> exploit lexical chains and their interaction with discourse elements for evalu- ating the quality of persuasive essays with respect to discourse coherence. <ref type="bibr" target="#b7">Crossley et al. (2015)</ref> identify student attributes, such as standardized test scores, as predictive of writing success and use them in conjunction with textual features to develop essay scoring models.</p><p>In 2012, Kaggle, 2 sponsored by the Hewlett Foundation, hosted the Automated Student As- sessment Prize (ASAP) contest, aiming to demon-strate the capabilities of automated text scoring systems <ref type="bibr" target="#b18">(Shermis, 2015)</ref>. The dataset released consists of around twenty thousand texts (60% of which are marked), produced by middle-school English-speaking students, which we use as part of our experiments to develop our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">C&amp;W Embeddings</head><p>Collobert and Weston (2008) and Collobert et al. (2011) introduce a neural network architecture <ref type="figure">(Fig. 1a)</ref> that learns a distributed representation for each word w in a corpus based on its local context. Concretely, suppose we want to learn a represen- tation for some target word w t found in an n-sized sequence of words S = (w 1 , . . . , w t , . . . , w n ) based on the other words which exist in the same sequence (∀w i ∈ S | w i = w t ). In order to derive this representation, the model learns to discrimi- nate between S and some 'noisy' counterpart S in which the target word w t has been substituted for a randomly sampled word from the vocabu- lary: S = (w 1 , . . . , w c , . . . , w n | w c ∼ V). In this way, every word w is more predictive of its local context than any other random word in the corpus.</p><p>Every word in V is mapped to a real-valued vector in Ω via a mapping function C(·) such that C(w i ) = M i , where M ∈ R D×|V| is the embedding matrix and M i is the ith col- umn of M. The network takes S as input by concatenating the vectors of the words found in it;</p><formula xml:id="formula_0">s t = C(w 1 ) . . . C(w t ) . . . C(w n ) ∈ R nD . Similarly, S is formed by substituting C(w t ) for C(w c ) ∼ M | w c = w t .</formula><p>The input vector is then passed through a hard tanh layer defined as,</p><formula xml:id="formula_1">htanh(x) =      −1 x &lt; −1 x −1 x 1 1 x &gt; 1 (1)</formula><p>which feeds a single linear unit in the output layer. The function that is computed by the network is ultimately given by (4):</p><formula xml:id="formula_2">s t = M 1 . . . M t . . . M n (2) i = σ(W hi s t + b h ) (3) f (s t ) = W oh i + b o (4) f (s), b o ∈ R 1 W oh ∈ R H×1 W hi ∈ R D×H s ∈ R D b o ∈ R H where M, W oh , W hi , b o , b h are learnable param- eters, D, H</formula><p>are hyperparameters controlling the size of the input and the hidden layer, respectively; σ is the application of an element-wise non-linear function (htanh in this case).</p><p>The model learns word embeddings by ranking the activation of the true sequence S higher than the activation of its 'noisy' counterpart S . The objective of the model then becomes to minimize the hinge loss which ensures that the activations of the original and 'noisy' ngrams will differ by at least 1:</p><formula xml:id="formula_3">loss context (target, corrupt) = [1 − f (s t ) + f (s ck )] + , ∀k ∈ Z E (5)</formula><p>where E is another hyperparameter controlling the number of 'noisy' sequences we give along with the correct sequence ( <ref type="bibr" target="#b3">Mikolov et al., 2013;</ref><ref type="bibr">Gutmann and Hyvärinen, 2012</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Augmented C&amp;W model</head><p>Following Tang (2015), we extend the previous model to capture not only the local linguistic en- vironment of each word, but also how each word contributes to the overall score of the essay. The aim here is to construct representations which, along with the linguistic information given by the linear order of the words in each sentence, are able to capture usage information. Words such as is, are, to, at which appear with any essay score are considered to be under-informative in the sense that they will activate equally both on high and low scoring essays. Informative words, on the other hand, are the ones which would have an impact on the essay score (e.g., spelling mistakes).</p><p>In order to capture those score-specific word embeddings (SSWEs), we extend (4) by adding a further linear unit in the output layer that performs linear regression, predicting the essay score. Us- ing (2), the activations of the network (presented in <ref type="figure">Fig. 1b)</ref>   <ref type="figure">Figure 1</ref>: Architecture of the original C&amp;W model (left) and of our extended version (right).</p><formula xml:id="formula_4">f ss (s) = W oh 1 i + b o 1 (6) f context (s) = W oh 2 i + b o 2 (7) f ss (s) ∈ [min(score), max(score)] b o 1 ∈ R 1 W oh 1 ∈ R 1×H</formula><p>The error we minimize for f ss (where ss stands for score specific) is the mean squared error between the predictedˆypredictedˆ predictedˆy and the actual essay score y:</p><formula xml:id="formula_5">loss score (s) = 1 N N i=1 (ˆ y i − y i ) 2<label>(8)</label></formula><p>From <ref type="formula">(5)</ref> and <ref type="formula" target="#formula_5">(8)</ref> we compute the overall loss function as a weighted linear combination of the two loss functions (9), back-propagating the error gradients to the embedding matrix M:</p><formula xml:id="formula_6">loss overall (s) = α · loss context (s, s ) + (1 − α) · loss score (s)<label>(9)</label></formula><p>where α is the hyper-parameter determining how the two error functions should be weighted. α val- ues closer to 0 will place more weight on the score- specific aspect of the embeddings, whereas values closer to 1 will favour the contextual information. <ref type="figure" target="#fig_0">Fig. 2</ref> shows the advantage of using SSWEs in the present setting. Based solely on the informa- tion provided by the linguistic environment, words such as computer and laptop are going to be placed together with their mis-spelled counterparts cop- muter and labtop <ref type="figure" target="#fig_0">(Fig. 2a)</ref>. This, however, does not reflect the fact that the mis-spelled words tend to appear in lower scoring essays. Using SSWEs, the correctly spelled words are pulled apart in the vector space from the incorrectly spelled ones, re- taining, however, the information that labtop and copmuter are still contextually related <ref type="figure" target="#fig_0">(Fig. 2b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Long-Short Term Memory Network</head><p>We use the SSWEs obtained by our model to derive continuous representations for each essay. We treat each essay as a sequence of tokens and explore the use of uni-and bi-directional (Graves, 2012) Long-Short Term Memory net- works (LSTMs) <ref type="bibr">(Hochreiter and Schmidhuber, 1997</ref>) in order to embed these sequences in a vec- tor of fixed size. Both uni-and bi-directional LSTMs have been effectively used for embedding long sequences ( <ref type="bibr">Hermann et al., 2015)</ref>. LSTMs are a kind of recurrent neural network (RNN) ar- chitecture in which the output at time t is condi- tioned on the input s both at time t and at time t − 1:</p><formula xml:id="formula_7">y t = W yh h t + b y (10) h t = H(W hs s t + W hh h t−1 + b h )<label>(11)</label></formula><p>where s t is the input at time t, and H is usually an element-wise application of a non-linear func- tion. In LSTMs, H is substituted for a composite function defining h t as:   The hidden layer that has been formed at the last timestep is used to predict the essay score using linear regression. We also explore the use of bi-directional LSTMs (dashed arrows). For 'deeper' representations, we can stack more LSTM layers after the hidden layer shown here.</p><formula xml:id="formula_8">i t = σ(W is s t + W ih h t−1 + W ic c t−1 + b i )<label>(12)</label></formula><formula xml:id="formula_9">f t = σ(W f s s t + W f h h t−1 + W f c c t−1 + b f )<label>(13)</label></formula><formula xml:id="formula_10">c t = i t g(W cs s t + W ch h t−1 + b c )+ f t c t−1<label>(14)</label></formula><formula xml:id="formula_11">o t = σ(W os s t + W oh h t−1 + W oc c t + b o )<label>(15)</label></formula><formula xml:id="formula_12">h t = o t h(c t )<label>(16)</label></formula><p>where g, σ and h are element-wise non-linear functions such as the logistic sigmoid ( 1 1+e −x ) and the hyperbolic tangent ( e 2z −1</p><formula xml:id="formula_13">e 2z +1</formula><p>); is the Hadamard product; W, b are the learned weights and biases respectively; and i, f, o and c are the input, forget, output gates and the cell activation vectors respec- tively.</p><p>Training the LSTM in a uni-directional manner (i.e., from left to right) might leave out important information about the sentence. For example, our interpretation of a word at some point t i might be different once we know the word at t i+5 . An ef- fective way to get around this issue has been to train the LSTM in a bidirectional manner. This re- quires doing both a forward and a backward pass of the sequence (i.e., feeding the words from left to right and from right to left). The hidden layer element in (10) can therefore be re-written as the concatenation of the forward and backward hidden vectors:</p><formula xml:id="formula_14">y t = W yh ← − h t − → h t + b y<label>(17)</label></formula><p>We feed the embedding of each word found in each essay to the LSTM one at a time, zero-padding shorter sequences. We form D- dimensional essay embeddings by taking the ac- tivation of the LSTM layer at the timestep where the last word of the essay was presented to the net- work. In the case of bi-directional LSTMs, the two independent passes of the essay (from left to right and from right to left) are concatenated together to predict the essay score. These essay embeddings are then fed to a linear unit in the output layer which predicts the essay score <ref type="figure" target="#fig_2">(Fig. 3)</ref>. We use the mean square error between the predicted and the gold score as our loss function, and optimize with RMSprop ( <ref type="bibr" target="#b8">Dauphin et al., 2015)</ref>, propagating the errors back to the word embeddings. <ref type="bibr">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Other Baselines</head><p>We train a Support Vector Regression model (see Section 4), which is one of the most widely used approaches in text scoring. We parse the data us- ing the RASP parser ( <ref type="bibr" target="#b1">Briscoe et al., 2006</ref>) and extract a number of different features for assess- ing the quality of the essays. More specifically, we use character and part-of-speech unigrams, bi- grams and trigrams; word unigrams, bigrams and trigrams where we replace open-class words with their POS; and the distribution of common nouns, prepositions, and coordinators. Additionally, we extract and use as features the rules from the phrase-structure tree based on the top parse for each sentence, as well as an estimate of the error rate based on manually-derived error rules.</p><p>N grams are weighted using tf-idf, while the rest are count-based and scaled so that all features have approximately the same order of magnitude. The final input vectors are unit-normalized to account for varying text-length biases.</p><p>Further to the above, we also explore the use of the Distributed Memory Model of Paragraph Vectors (PV-DM) proposed by <ref type="bibr">Le and Mikolov (2014)</ref>, as a means to directly obtain essay embed- dings. PV-DM takes as input word vectors which make up ngram sequences and uses those to pre- dict the next word in the sequence. A feature of PV-DM, however, is that each 'paragraph' is as- signed a unique vector which is used in the predic- tion. This vector, therefore, acts as a 'memory', retaining information from all contexts that have appeared in this paragraph. Paragraph vectors are then fed to a linear regression model to obtain es- say scores (we refer to this model as doc2vec).</p><p>Additionally, we explore the effect of our score- specific method for learning word embeddings, when compared against three different kinds of word embeddings:</p><p>• word2vec embeddings ( <ref type="bibr" target="#b3">Mikolov et al., 2013</ref>) trained on our training set (see Sec- tion 4).</p><p>• Publicly available word2vec embeddings ( <ref type="bibr" target="#b3">Mikolov et al., 2013</ref>) pre-trained on the Google News corpus (ca. 100 billion words), which have been very effective in capturing solely contextual information.</p><p>• Embeddings that are constructed on the fly by the LSTM, by propagating the errors from its hidden layer back to the embedding matrix (i.e., we do not provide any pre-trained word embeddings). <ref type="bibr">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Dataset</head><p>The Kaggle dataset contains 12.976 essays rang- ing from 150 to 550 words each, marked by two raters (Cohen's κ = 0.86). The essays were writ- ten by students ranging from Grade 7 to Grade 10, comprising eight distinct sets elicited by eight different prompts, each with distinct marking cri- teria and score range. <ref type="bibr">5</ref> For our experiments, we use the resolved combined score between the two raters, which is calculated as the average between the two raters' scores (if the scores are close), or is determined by a third expert (if the scores are far apart). Currently, the state-of-the-art on this dataset has achieved a Cohen's κ = 0.81 (using quadratic weights). However, the test set was re- leased without the gold score annotations, render- ing any comparisons futile, and we are therefore restricted in splitting the given training set to cre- ate a new test set. The sets where divided as follows: 80% of the entire dataset was reserved for train- ing/validation, and 20% for testing. 80% of the training/validation subset was used for actual training, while the remaining 20% for validation (in absolute terms for the entire dataset: 64% train- ing, 16% validation, 20% testing). To facilitate future work, we release the ids of the validation and test set essays we used in our experiments, in addition to our source code and various hyperpa- rameter values. <ref type="bibr">6</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Results</head><p>The hyperparameters for our model were as fol- lows: sizes of the layers H, D, the learning rate η, the window size n, the number of 'noisy' se- quences E and the weighting factor α. Also the hyperparameters of the LSTM were the size of the LSTM layer D LST M as well as the dropout rate r.  <ref type="table">Table 1</ref>: Results of the different models on the Kaggle dataset. All resulting vectors were trained using linear regression. We optimized the parameters using a separate validation set (see text) and report the results on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><note type="other">Spearman's ρ Pearson r RMSE Cohen's κ doc2vec 0</note><p>Since the search space would be massive for grid search, the best hyperparameters were determined using Bayesian Optimization ( <ref type="bibr" target="#b21">Snoek et al., 2012)</ref>. In this context, the performance of our models in the validation set is modeled as a sample from a Gaussian process (GP) by constructing a proba- bilistic model for the error function and then ex- ploiting this model to make decisions about where to next evaluate the function. The hyperparame- ters for our baselines were also determined using the same methodology. All models are trained on our training set (see Section 4), except the one prefixed 'word2vec pre-trained ' which uses pre-trained em- beddings on the Google News Corpus. We re- port the Spearman's rank correlation coefficient ρ, Pearson's product-moment correlation coefficient r, and the root mean square error (RMSE) be- tween the predicted scores and the gold standard on our test set, which are considered more appro- priate metrics for evaluating essay scoring systems ( <ref type="bibr" target="#b27">Yannakoudakis and Cummins, 2015)</ref>. However, we also report Cohen's κ with quadratic weights, which was the evaluation metric used in the Kag- gle competition. Performance of the models is shown in <ref type="table">Table 1</ref>.</p><p>In terms of correlation, SVMs produce com- petitive results (ρ = 0.78 and r = 0.77), out- performing doc2vec, LSTM and BLSTM, as well as their deep counterparts. As described above, the SVM model has rich linguistic knowl- edge and consists of hand-picked features which have achieved excellent performance in similar tasks <ref type="bibr" target="#b28">(Yannakoudakis et al., 2011</ref>). However, in terms of RMSE, it is among the lowest performing models <ref type="bibr">(8.85)</ref>, together with 'BLSTM' and 'Two- layer BLSTM'. Deep models in combination with word2vec (i.e., 'word2vec + Two-layer LSTM' and 'word2vec + Two-layer BLSTM') and SVMs are comparable in terms of r and ρ, though not in terms of RMSE, where the former produce better results, with RMSE improving by half (4.79). doc2vec also produces competitive RMSE results (4.43), though correlation is much lower (ρ = 0.62 and r = 0.63).</p><p>The two BLSTMs trained with word2vec em- beddings are among the most competitive models in terms of correlation and outperform all the mod- els, except the ones using pre-trained embeddings and SSWEs. Increasing the number of hidden lay- ers and/or adding bi-directionality does not always improve performance, but it clearly helps in this case and performance improves compared to their uni-directional counterparts.</p><p>Using pre-trained word embeddings improves the results further. More specifically, we found 'word2vec pre-trained + Two-layer BLSTM' to be the best configuration, increasing correlation to 0.79 ρ and 0.91 r, and reducing RMSE to 3.2. We note however that this is not an entirely fair comparison as these are trained on a much larger corpus than our training set (which we use to train our models). Nevertheless, when we use our SSWEs models we are able to outper- form 'word2vec pre-trained + Two-layer BLSTM', even though our embeddings are trained on fewer data points. More specifically, our best model ('SSWE + Two-layer BLSTM') improves correla- tion to ρ = 0.91 and r = 0.96, as well as RMSE to 2.4, giving a maximum increase of around 10% in correlation. Given the results of the pre-trained model, we believe that the performance of our best SSWE model will further improve should more training data be given to it. <ref type="bibr">7</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Discussion</head><p>Our SSWE + LSTM approach having no prior knowledge of the grammar of the language or the domain of the text, is able to score the essays in a very human-like way, outperforming other state- of-the-art systems. Furthermore, while we tuned the models' hyperparameters on a separate vali- dation set, we did not perform any further pre- processing of the text other than simple tokeniza- tion.</p><p>In the essay scoring literature, text length tends to be a strong predictor of the overall score. In order to investigate any possible effects of essay length, we also calculate the correlation between the gold scores and the length of the essays. We find that the correlations on the test set are rela- tively low (r = 0.3, ρ = 0.44), and therefore con- clude that there are no such strong effects.</p><p>As described above, we used Bayesian Op- timization to find optimal hyperparameter con- figurations in fewer steps than in regular grid search. Using this approach, the optimization model showed some clear preferences for some parameters which were associated with better scoring models: 8 the number of 'noisy' sequences E, the weighting factor α and the size of the LSTM layer D LST M . The optimal α value was consistently set to 0.1, which shows that our SSWE approach was necessary to capture the usage of the words. Performance dropped considerably as α increased (less weight on SSWEs and more on the contextual aspect). When using α = 1, which <ref type="bibr">7</ref> Our approach outperforms all the other models in terms of Cohen's κ too.</p><p>8 For the best scoring model the hyperparameters were as follows: D = 200, H = 100, η = 1e − 7, n = 9, E = 200, α = 0.1, DLST M = 10, r = 0.5.</p><p>is equivalent to using the basic C&amp;W model, we found that performance was considerably lower (e.g., correlation dropped to ρ = 0.15).</p><p>The number of 'noisy' sequences was set to 200, which was the highest possible setting we considered, although this might be related more to the size of the corpus (see <ref type="bibr" target="#b3">Mikolov et al. (2013)</ref> for a similar discussion) rather than to our approach. Finally, the optimal value for D LST M was 10 (the lowest value investigated), which again may be corpus-dependent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Visualizing the black box</head><p>In this section, inspired by recent advances in (de-) convolutional neural networks in computer vision ( <ref type="bibr" target="#b19">Simonyan et al., 2013</ref>) and text summa- rization <ref type="bibr" target="#b9">(Denil et al., 2014</ref>), we introduce a novel method of generating interpretable visualizations of the network's performance. In the present con- text, this is particularly important as one advantage of the manual methods discussed in § 2 is that we are able to know on what grounds the model made its decisions and which features are most discrim- inative.</p><p>At the outset, our goal is to assess the 'qual- ity' of our word vectors. By 'quality' we mean the level to which a word appearing in a particu- lar context would prove to be problematic for the network's prediction. In order to identify 'high' and 'low' quality vectors, we perform a single pass of an essay from left to right and let the LSTM make its score prediction. Normally, we would provide the gold scores and adjust the network weights based on the error gradients. Instead, we provide the network with a pseudo-score by taking the maximum score this specific essay can take <ref type="bibr">9</ref> and provide this as the 'gold' score. If the word vector is of 'high' quality (i.e., associated with higher scoring texts), then there is going to be lit- tle adjustment to the weights in order to predict the highest score possible. Conversely, providing the minimum possible score (here 0), we can assess how 'bad' our word vectors are. Vectors which re- quire minimal adjustment to reach the lowest score are considered of 'lower' quality. Note that since we do a complete pass over the network (without doing any weight updates), the vector quality is going to be essay dependent.</p><p>. . . way to show that Saeng is a determined . . . . . . . sometimes I do . Being patience is being . . .</p><p>. . . which leaves the reader satisfied . . . . . . is in this picture the cyclist is riding a dry and area which could mean that it is very and the looks to be going down hill there looks to be a lot of turns . . . . . . . The only reason im putting this in my own way is because know one is patient in my family . . . . . . . Whether they are building hand-eye coordination , researching a country , or family and friends through @CAPS3 , @CAPS2 , @CAPS6 the internet is highly and I hope you feel the same way . <ref type="table">Table 2</ref>: Several example visualizations created by our LSTM. The full text of the essay is shown in black and the 'quality' of the word vectors appears in color on a range from dark red (low quality) to dark green (high quality).</p><p>Concretely, using the network function f (x) as computed by Eq. <ref type="formula" target="#formula_8">(12)</ref> - <ref type="formula" target="#formula_14">(17)</ref>, we can approximate the loss induced by feeding the pseudo-scores by taking the magnitude of each error vector (18) - (19). Since lim w 2 →0ˆy→0ˆ →0ˆy = y, this magnitude should tell us how much an embedding needs to change in order to achieve the gold score (here pseudo-score). In the case where we provide the minimum as a pseudo-score, a w 2 value closer to zero would indicate an incorrectly used word. For the results reported here, we combine the mag- nitudes produced from giving the maximum and minimum pseudo-scores into a single score, com- puted as L(˜ y max , f (x)) − L(˜ y min , f (x)), where:</p><formula xml:id="formula_15">L(˜ y, f (x)) ≈ w 2 (18) w = L(x) ∂L ∂x (˜ y,f (x))<label>(19)</label></formula><p>where w 2 is the vector Euclidean norm w = N i=1 w 2 i ; L(·) is the mean squared error as in Eq. <ref type="formula" target="#formula_5">(8)</ref>; and˜yand˜ and˜y is the essay pseudo-score.</p><p>We show some examples of this visualization procedure in <ref type="table">Table 2</ref>. The model is capable of providing positive feedback. Correctly placed punctuation or long-distance dependencies (as in Sentence 6 are . . . researching) are particularly favoured by the model. Conversely, the model does not deal well with proper names, but is able to cope with POS mistakes (e.g., Being patience or the internet is highly and . . . ). However, as seen in Sentence 3 the model is not perfect and returns a false negative in the case of satisfied.</p><p>One potential drawback of this approach is that the gradients are calculated only after the end of the essay. This means that if a word appears mul- tiple times within an essay, sometimes correctly and sometimes incorrectly, the model would not be able to distinguish between them. Two possi- ble solutions to this problem are to either provide the gold score at each timestep which results into a very computationally expensive endeavour, or to feed sentences or phrases of smaller size for which the scoring would be more consistent. <ref type="bibr">10</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we introduced a deep neural network model capable of representing both local contex- tual and usage information as encapsulated by es- say scoring. This model yields score-specific word embeddings used later by a recurrent neural net- work in order to form essay representations.</p><p>We have shown that this kind of architecture is able to surpass similar state-of-the-art systems, as well as systems based on manual feature engineer- ing which have achieved results close to the upper bound in past work. We also introduced a novel way of exploring the basis of the network's inter- nal scoring criteria, and showed that such models are interpretable and can be further exploited to provide useful feedback to the author.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparison between standard and score-specific word embeddings. By virtue of appearing in similar environments, standard neural embeddings will place the correct and the incorrect spelling closer in the vector space. However, since the mistakes are found in lower scoring essays, SSWEs are able to discriminate between the correct and the incorrect versions without loss in contextual meaning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A single-layer Long Short Term Memory (LSTM) network. The word vectors w i enter the input layer one at a time. The hidden layer that has been formed at the last timestep is used to predict the essay score using linear regression. We also explore the use of bi-directional LSTMs (dashed arrows). For 'deeper' representations, we can stack more LSTM layers after the hidden layer shown here.</figDesc></figure>

			<note place="foot" n="1"> The task is also referred to as Automated Essay Scoring. Throughout this paper, we use the terms text and essay (scoring) interchangeably.</note>

			<note place="foot" n="2"> http://www.kaggle.com/c/asap-aes/</note>

			<note place="foot" n="3"> The maximum time for jointly training a particular SSWE + LSTM combination took about 55-60 hours on an Amazon EC2 g2.2xlarge instance (average time was 27-30 hours).</note>

			<note place="foot" n="4"> Another option would be to use standard C&amp;W embeddings; however, this is equivalent to using SSWEs with α = 1, which we found to produce low results. 5 Five prompts employed a holistic scoring rubric, one was scored with a two-trait rubric, and two were scored with a multi-trait rubric, but reported as a holistic score (Shermis and Hammer, 2012). 6 The code, by-model hyperparameter configurations and the IDs of the testing set are available at https:// github.com/dimalik/ats/.</note>

			<note place="foot" n="9"> Note the in the Kaggle dataset essays from different essay sets have different maximum scores. Here we take as˜ymax as˜ as˜ymax the essay set maximum rather than the global maximum.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The first author is supported by the Onassis Foun-dation. We would like to thank the three anony-mous reviewers for their valuable feedback. <ref type="bibr">10</ref> We note that the same visualization technique can be used to show the 'goodness' of phrases/sentences. Within the phrase setting, after feeding the last word of the phrase to the network, the LSTM layer will contain the phrase embed-ding. Then, we can assess the 'goodness' of this embedding by evaluating the error gradients after predicting the high-est/lowest score.</p></div>
			</div>

			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automated essay scoring with e-Rater v.2.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yigal</forename><surname>Attali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jill</forename><surname>Burstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Learning, and Assessment</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>Journal of Technology</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The second release of the RASP system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Watson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the COLING/ACL</title>
		<meeting>the COLING/ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Automated assessment of ESOL free text examinations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Medlock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Øistein</forename><forename type="middle">E</forename><surname>Andersen</surname></persName>
		</author>
		<idno>UCAM-CL-TR- 790</idno>
		<imprint>
			<date type="published" when="2010-11" />
		</imprint>
		<respStmt>
			<orgName>University of Cambridge, Computer Laboratory</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Thorsten Brants, Phillipp Koehn, and Tony Robinson. In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An Unsupervised Automated Essay Scoring System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="page" from="61" to="67" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth international conference on Machine Learning</title>
		<meeting>the Twenty-Fifth international conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2008-07" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pssst... textual features... there is more to automatic essay scoring than just you!</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Crossley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erica</forename><forename type="middle">L</forename><surname>Laura K Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danielle</forename><forename type="middle">S</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcnamara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth International Conference on Learning Analytics And Knowledge</title>
		<meeting>the Fifth International Conference on Learning Analytics And Knowledge</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="203" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Equilibrated adaptive learning rates for nonconvex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dauphin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-02" />
		</imprint>
	</monogr>
	<note>Harm de Vries, and Yoshua Bengio</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Modelling, visualising and summarising documents with a single convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Demiraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando De</forename><surname>Freitas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An overview of automated scoring of essays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Semire</forename><surname>Dikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Learning, and Assessment</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>Journal of Technology</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Intellimetric TM : From here to validity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Elliot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automated Essay Scoring: A Cross-Disciplinary Perspective</title>
		<editor>M. D. Shermis and J. Burnstein</editor>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="71" to="86" />
		</imprint>
		<respStmt>
			<orgName>Lawrence Erlbaum Associates</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Scoring persuasive essays using opinions and their targets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noura</forename><surname>Farra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swapna</forename><surname>Somasundaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jill</forename><surname>Burstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the Tenth Workshop on Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="64" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The use of the computer in analyzing student essays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellis</forename><forename type="middle">B</forename><surname>Page</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Review of Education</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="225" />
			<date type="published" when="1968-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Project essay grade: PEG</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Page</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automated essay scoring: A cross-disciplinary perspective</title>
		<editor>M.D. Shermis and J.C. Burstein</editor>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="43" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automated essay scoring using Bayes&apos; theorem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Rudner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tahung</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Technology, Learning and Assessment</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="3" to="21" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Effective feature integration for automated short answer scoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Heilman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Madnani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the Tenth Workshop on Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Contrasting stateof-the-art automated scoring of essays: analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shermis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hammer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
		<respStmt>
			<orgName>The University of Akron and Kaggle</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Contrasting state-of-the-art in the machine scoring of short-form constructed responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shermis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Educational Assessment</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="46" to="65" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Parsing English with a link grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D K</forename><surname>Sleator</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Templerley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Workshop on Parsing Technologies, ACL</title>
		<meeting>the 3rd International Workshop on Parsing Technologies, ACL</meeting>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Practical bayesian optimization of machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Lexical chaining for measuring discourse coherence quality in test-taker essays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swapna</forename><surname>Somasundaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jill</forename><surname>Burstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="950" to="961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sentiment-specific representation learning for document-level sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth ACM International Conference on Web Search and Data Mining-WSDM &apos;15</title>
		<meeting>the Eighth ACM International Conference on Web Search and Data Mining-WSDM &apos;15</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A framework for implementing automated scoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Williamson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
	<note>ucational Testing Service</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Evaluating the performance of automated text scoring systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Cummins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the Tenth Workshop on Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A new dataset and method for automatically grading ESOL texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Medlock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference</title>
		<meeting><address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06-24" />
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
