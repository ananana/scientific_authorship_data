<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploiting Domain Knowledge via Grouped Weight Sharing with Application to Text Categorization</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Lease</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Exploiting Domain Knowledge via Grouped Weight Sharing with Application to Text Categorization</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="155" to="160"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-2024</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>A fundamental advantage of neural models for NLP is their ability to learn representations from scratch. However, in practice this often means ignoring existing external linguistic resources, e.g., Word-Net or domain specific ontologies such as the Unified Medical Language System (UMLS). We propose a general, novel method for exploiting such resources via weight sharing. Prior work on weight sharing in neural networks has considered it largely as a means of model compression. In contrast, we treat weight sharing as a flexible mechanism for incorporating prior knowledge into neural models. We show that this approach consistently yields improved performance on classification tasks compared to baseline strategies that do not exploit weight sharing.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural models are powerful in part due to their ability to learn good representations of raw tex- tual inputs, mitigating the need for extensive task-specific feature engineering <ref type="bibr" target="#b1">(Collobert et al., 2011</ref>). However, a downside of learning from scratch is failing to capitalize on prior linguistic or semantic knowledge, often encoded in existing resources such as ontologies. Such prior knowl- edge can be particularly valuable when estimating highly flexible models. In this work, we address how to exploit known relationships between words when training neural models for NLP tasks.</p><p>We propose exploiting the feature-hashing trick, originally proposed as a means of neural net- work compression <ref type="bibr">(Chen et al., 2015</ref>  <ref type="figure">Figure 1</ref>: An example of grouped partial weight sharing. Here there are two groups. We stochas- tically select embedding weights to be shared be- tween words belonging to the same group(s).</p><p>to be similar a priori. In effect, this acts as a reg- ularizer that constrains the model to learn weights that agree with the domain knowledge codified in external resources like ontologies.</p><p>More specifically, as external resources we use Brown clusters <ref type="bibr">(Brown et al., 1992)</ref>, WordNet <ref type="bibr" target="#b10">(Miller, 1995)</ref> and the Unified Medical Language System (UMLS) <ref type="bibr">(Bodenreider, 2004)</ref>. From these we derive groups of words with similar meaning. We then use feature hashing to share a subset of weights between the embeddings of words that be- long to the same semantic group(s). This forces the model to respect prior domain knowledge, in- sofar as words similar under a given ontology are compelled to have similar embeddings.</p><p>Our contribution is a novel, simple and flexible method for injecting domain knowledge into neu- ral models via stochastic weight sharing. Results on seven diverse classification tasks (three senti- ment and four biomedical) show that our method consistently improves performance over (1) base- lines that fail to capitalize on domain knowledge, and (2) an approach that uses retrofitting <ref type="bibr" target="#b4">(Faruqui et al., 2014</ref>) as a preprocessing step to encode do- main knowledge prior to training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Grouped Weight Sharing</head><p>We incorporate similarity relations codified in ex- isting resources (here derived from Brown clus- ters, SentiWordNet and the UMLS) as prior knowledge in a Convolutional Neural Network (CNN). 1 To achieve this we construct a shared em- bedding matrix such that words known a priori to be similar are constrained to share some fraction of embedding weights.</p><p>Concretely, suppose we have N groups of words derived from an external resource. Note that one could derive such groups in several ways; e.g., using the synsets in SentiWordNet. We denote groups by {g 1 , g 2 , ..., g N }. Each group is associ- ated with an embedding g g i , which we initialize by averaging the pre-trained embeddings of each word in the group.</p><p>To exploit both grouped and independent word weights, we adopt a two-channel CNN model ( <ref type="bibr" target="#b25">Zhang et al., 2016b</ref>). The embedding matrix of the first channel is initialized with pre-trained word vectors. We denote this input by E p ∈ R V ×d (V is the vocabulary size and d the dimension of the word embeddings). The second channel in- put matrix is initialized with our proposed weight- sharing embedding E s ∈ R V ×d . E s is initialized by drawing from both E p and the external resource following the process we describe below.</p><p>Given an input text sequence of length l, we construct sequence embedding representations W p ∈ R l×d and W s ∈ R l×d using the cor- responding embedding matrices. We then apply independent sets of linear convolution filters on these two matrices. Each filter will generate a fea- ture map vector v ∈ R l−h+1 (h is the filter height). We perform 1-max pooling over each v, extract- ing one scalar per feature map. Finally, we con- catenate scalars from all of the feature maps (from both channels) into a feature vector which is fed to a softmax function to predict the label <ref type="figure" target="#fig_0">(Figure 2)</ref>.</p><p>We initialize E s as follows. Each row e i ∈ R d of E s is the embedding of word i. Words may belong to one or more groups. A mapping func- tion G(i) retrieves the groups that word i belongs to, i.e., G(i) returns a subset of {g 1 , g 2 , ..., g N }, which we denote by {g</p><formula xml:id="formula_0">(i) 1 , g (i) 2 ...g (i)</formula><p>K }, where K is the number of groups that contain word i. To initialize E s , for each dimension j of each word embedding e i , we use a hash function h i to map <ref type="bibr">1</ref> The idea of sharing weights to reflect known similarity is general and could be applied with other neural architectures. (hash) the index j to one of the K group IDs: <ref type="bibr">- berger et al., 2009;</ref><ref type="bibr" target="#b14">Shi et al., 2009</ref>), we use a second hash function b to remove bias induced by hashing. This is a signing function, i.e., it maps (i, j) tuples to {+1, −1} 2 . We then set e i,j to the product of g h i (j),j and b(i, j). h and b are both ap- proximately uniform hash functions. Algorithm 1 provides the full initialization procedure.</p><formula xml:id="formula_1">h i : N → {g (i) 1 , g (i) 2 ...g (i) K }. Following (Wein</formula><formula xml:id="formula_2">Algorithm 1 Initialization of E s 1: for i in {1, . . . , V } do 2: {g (i) 1 , g (i) 2 , . . . , g (i) K } := G(i).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3:</head><p>for j ∈ {1, . . . , d} do 4:</p><formula xml:id="formula_3">e i,j := g h i (j),j · b(i, j) 5:</formula><p>end for 6: end for For illustration, consider <ref type="figure">Figure 1</ref>. Here g 1 con- tains three words: good, nice and amazing, while g 2 has two words: good and interesting. The group embeddings g g 1 , g g 2 are initialized as averages over the pre-trained embeddings of the words they comprise. Here, embedding parameters e 1,1 and e 2,1 are both mapped to g g 1 ,1 , and thus share this value. Similarly, e 1,3 and e 2,3 will share value at g g 1 , <ref type="bibr">3</ref> . We have elided the second hash function b from this figure for simplicity. During training, we update E p as usual using back-propagation <ref type="bibr" target="#b13">(Rumelhart et al., 1986</ref>). We up- date E s and group embeddings g in a manner sim- ilar to <ref type="bibr">Chen et al. (2015)</ref>. In the forward propa- gation before each training step (mini-batch), we derive the value of e i,j from g:</p><formula xml:id="formula_4">e i,j := g h i (j),j * b(i, j)<label>(1)</label></formula><p>We use this newly updated e i,j to perform for- ward propagation in the CNN.</p><p>During backward propagation, we first compute the gradient of E s , and then we use this to derive the gradient w.r.t gs. To do this, for each dimen- sion j in g g k , we aggregate the gradients w.r.t E s whose elements are mapped to this dimension:</p><formula xml:id="formula_5">g g k ,j := (i,j) E s i,j · δ h i (j)=g k · b(i, j) (2)</formula><p>where δ h i (j)=g k = 1 when h i (j) = g k , and 0 otherwise. Each training step involves execut- ing Equations 1 and 2. Once the shared gradient is calculated, gradient descent proceeds as usual.</p><p>We update all parameters aside from the shared weights in the standard way.</p><p>The number of parameters in our approach scales linearly with the number of channels. But the gradients can actually be back-propagated in a distributed way for each channel, since the con- volutional and embedding layers are independent across these. Thus training time scales approxi- mately linearly with the number of parameters in one channel (if the gradient is back-propagated in a distributed way).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>We use three sentiment datasets: a movie review (MR) dataset (Pang and <ref type="bibr" target="#b12">Lee, 2005)</ref>  <ref type="bibr">3</ref> ; a customer review (CR) dataset (Hu and Liu, 2004) 4 ; and an opinion dataset (MPQA) ( <ref type="bibr" target="#b18">Wiebe et al., 2005</ref>) <ref type="bibr">5</ref> .</p><p>We also use four biomedical datasets, which concern systematic reviews. The task here is to classify published articles describing clinical tri- als as relevant or not to a well-specified clinical question. Articles deemed relevant are included in the corresponding review, which is a synthesis of all pertinent evidence ( <ref type="bibr" target="#b16">Wallace et al., 2010</ref>). We use data from reviews that concerned: clopido- grel (CL) for cardiovascular conditions <ref type="bibr" target="#b2">(Dahabreh et al., 2013)</ref>; biomarkers for assessing iron de- ficiency in anemia (AN) experienced by patients with kidney disease (Chung et al., 2012); statins (ST) <ref type="bibr" target="#b0">(Cohen et al., 2006</ref>); and proton beam (PB) therapy <ref type="figure" target="#fig_0">(Terasawa et al., 2009</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details and Baselines</head><p>We use SentiWordNet ( <ref type="bibr">Baccianella et al., 2010)</ref>  <ref type="bibr">6</ref> for the sentiment tasks. SentiWordNet assigns to each synset of wordnet three sentiment scores: positivity, negativity and objectivity, constrained to sum to 1. We keep only the synsets with pos- itivity or negativity scores greater than 0, i.e., we remove synsets deemed objective. The synsets in SentiWordNet constitute our groups. We also use the Brown clustering algorithm 7 on the three senti- ment datasets. We generate 1000 clusters and treat each as a group.</p><p>For the biomedical datasets, we use the Medical Subject Headings (MeSH) terms 8 attached to each abstract to classify them. Each MeSH term has a tree number indicating the path from the root in the UMLS. For example, 'Alagille Syndrome' has tree number 'C06.552.150.125'; periods denote tree splits, numbers are nodes. We induce groups com- prising MeSH terms that share the same first three parent nodes, e.g., all terms with 'C06.552.150' as their tree number prefix constitute one group.</p><p>We compare our approach to several baselines. All use pre-trained embeddings to initialize E p , but we explore several approaches to exploiting E s : (1) randomly initialize E s ; (2) initialize E s to reflect the group embedding g, but do not share weights during the training process, i.e., do not constrain their weights to be equal when we perform back-propagation; (3) use linguistic re- sources to retro-fit <ref type="bibr" target="#b4">(Faruqui et al., 2014</ref>) the pre- trained embeddings, and use these to initialize E s . For retro-fitting, we first construct a graph de- rived from SentiWordNet. Then we run belief- propagation on the graph to encourage linked words to have similar vectors. This is a pre- processing step only; we do not impose weight sharing constraints during training. <ref type="table" target="#tab_2">MR  10662  18765  5331  5331  CR  3773  5340  2406  1367  MPQA  10604  6246  3311  7293  AN  5653  5554  653  5000  CL  8288  3684  768  7520  ST  3464  2965  173  3291  PB  4749  3086  243  4506   Table 1</ref>: Corpora statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>total #instances vocabulary size #positive instances #negative instances</head><p>For the sentiment datasets we use three filter heights (3,4,5) for each of the two CNN channels. For the biomedical datasets, we use only one fil- ter height (1), because the inputs are unstructured MeSH terms. <ref type="bibr">9</ref> In both cases we use 100 filters of each unique height. For the sentiment datasets, we use Google word2vec ( <ref type="bibr" target="#b9">Mikolov et al., 2013)</ref>  <ref type="bibr">10</ref> to initialize E p . For the biomedical datasets, we use word2vec trained on biomedical texts <ref type="bibr">(Moen and Ananiadou, 2013)</ref>  <ref type="bibr">11</ref> to initialize E p . For param- eter estimation, we use Adadelta <ref type="bibr" target="#b23">(Zeiler, 2012)</ref>. Because the biomedical datasets are imbalanced, we use downsampling ( <ref type="bibr" target="#b24">Zhang et al., 2016a;</ref><ref type="bibr" target="#b26">Zhang and Wallace, 2015)</ref> to effectively train on balanced subsets of the data.</p><p>We developed our approach using the MR senti- ment dataset, tuning our approach to constructing groups from the available resources -experiments on other sentiment datasets were run after we fi- nalized the model and hyperparameters. Similarly, we used the anemia (AN) review as a development set for the biomedical tasks, especially w.r.t. con- structing groups from MeSH terms using UMLS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>We replicate each experiment five times (each is a 10-fold cross validation), and report the mean (min, max) across these replications. Results on the sentiment and biomedical corpora in are pre- sented in <ref type="table" target="#tab_2">Tables 2 and 3</ref>, respectively. 12 These exploit different external resources to induce the word groupings that in turn inform weight shar- ing. We report AUC for the biomedical datasets because these are highly imbalanced (see <ref type="table">Table 1</ref>).</p><p>Our method improves performance compared to all relevant baselines (including an approach that <ref type="bibr">9</ref> For this work we are ignoring title and abstract texts. 10 code.google.com/archive/p/word2vec/ 11 bio.nlplab.org/ 12 Sentiment task results are not directly comparable to prior work due to different preprocessing steps. also exploits external knowledge via retrofitting) in six of seven cases. Informing weight initial- ization using external resources improves perfor- mance independently, but additional gains are re- alized by also enforcing sharing during training.</p><p>We note that our aim here is not necessarily to achieve state-of-art results on any given dataset, but rather to evaluate the proposed method for in- corporating external linguistic resources into neu- ral models via weight sharing. We have therefore compared to baselines that enable us to assess this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Neural Models for NLP. Recently there has been enormous interest in neural models for NLP gener- ally <ref type="bibr" target="#b1">(Collobert et al., 2011;</ref><ref type="bibr" target="#b5">Goldberg, 2016)</ref>. Most relevant to this work, simple CNN based mod- els (which we have built on here) have proven extremely effective for text categorization <ref type="bibr" target="#b8">(Kim, 2014;</ref><ref type="bibr" target="#b26">Zhang and Wallace, 2015)</ref>. Exploiting Linguistic Resources. A potential drawback to learning from scratch in end-to-end neural models is a failure to capitalize on exist- ing knowledge sources. There have been efforts to exploit such resources specifically to induce bet- ter word vectors ( <ref type="bibr" target="#b21">Yu and Dredze, 2014;</ref><ref type="bibr" target="#b4">Faruqui et al., 2014;</ref><ref type="bibr" target="#b22">Yu et al., 2016;</ref><ref type="bibr" target="#b19">Xu et al., 2014)</ref>. But these models do not attempt to exploit external resources jointly during training for a particular downstream task (which uses word embeddings as inputs), as we do here.</p><p>Past work on sparse linear models has shown the potential of exploiting linguistic knowledge in statistical NLP models. For example, <ref type="bibr" target="#b20">Yogatama and Smith (2014)</ref> used external resources to in- form structured, grouped regularization of log- linear text classification models, yielding improve- ments over standard regularization approaches. Elsewhere, <ref type="bibr" target="#b3">Doshi-Velez et al. (2015)</ref> proposed a variant of LDA that exploits a priori known tree-   <ref type="table">Table 3</ref>: AUC mean (min, max) on the biomedical datasets. Abbreviations are as in <ref type="table" target="#tab_2">Table 2</ref>, except here the external resource is the UMLS MeSH ontology ('U').'U(s)' is the proposed weight sharing method utilizing ULMS. structured relations between tokens (e.g., derived from the UMLS) in topic modeling. Weight-sharing in NNs. Recent work has con- sidered stochastically sharing weights in neural models. Notably, <ref type="bibr">Chen et al. (2015)</ref>. proposed randomly sharing weights in neural networks. Elsewhere, <ref type="bibr" target="#b6">Han et al. (2015)</ref> proposed quantized weight sharing as an intermediate step in their deep compression model. In these works, the pri- mary motivation was model compression, whereas here we view the hashing trick as a mechanism to encode domain knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have proposed a novel method for incorporat- ing prior semantic knowledge into neural models via stochastic weight sharing. We have showed it generally improves text classification performance vs. model variants which do not exploit external resources and vs. an approach based on retrofitting prior to training. In future work, we will inves- tigate generalizing our approach beyond classifi- cation, and to inform weight sharing using other varieties and sources of linguistic knowledge.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Proposed two-channel model. The first channel input is a standard pre-trained embedding matrix. The second channel receives a partially shared embedding matrix constructed using external linguistic resources.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Accuracy mean (min, max) on sentiment datasets. 'p': channel initialized with the pre-trained 
embeddings E p . 'r': channel randomly initialized. 'retro': initialized with retofitted embeddings. 'S/B 
(no sharing)': channel initialized with E s (using SentiWordNet or Brown clusters), but weights are not 
shared during training. 'S/B (sharing)': proposed weight-sharing method. 

Method 
AN 
CL 
ST 
PB 
p only 
86.63 (86.57,86.67) 88.73 (88.51,89.00) 67.15 (66.00, 67.91) 90.11 (89.46, 91.03) 
p + r 
85.67 (85.46,85.95) 88.87 (88.56,89.03) 67.72 (67.65,67.86) 90.12 (89.87,90.47) 
p + retro 86.46 (86.32,86.65) 89.27 (88.89,90.01) 67.78 (67.56,68.00) 90.07 (89.92,90.20) 
p + U 
86.60 (86.32,87.01) 88.93 (88.67,89.13) 67.78 (67.71,67.85) 90.23 (89.84,90.47) 
p + U(s) 87.15 (87.00,87.29) 89.29 (89.09,89.51) 67.73 (67.58,67.88) 90.99 (90.46,91.59) 

</table></figure>

			<note place="foot" n="2"> Empirically, we found that using this signing function does not affect performance.</note>

			<note place="foot" n="3"> www.cs.cornell.edu/people/pabo/ movie-review-data/ 4 www.cs.uic.edu/ ˜ liub/FBS/ sentiment-analysis.html 5 mpqa.cs.pitt.edu/corpora/mpqa_corpus/</note>

			<note place="foot" n="6"> sentiwordnet.isti.cnr.it 7 github.com/percyliang/brown-cluster 8 www.nlm.nih.gov/bsd/disted/ meshtutorial/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head><p>Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-tiani. 2010. Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining. In LREC. volume 10, pages 2200-2204.</p><p>Olivier Bodenreider. 2004. The unified medical lan-guage system (umls): integrating biomedical termi-nology. Nucleic acids research 32(suppl 1):D267-D270.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Reducing workload in systematic review preparation using automated citation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">R</forename><surname>Aaron M Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Poyin</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="206" to="219" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Testing of cyp2c19 variants and platelet reactivity for guiding antiplatelet treatment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denish</forename><surname>Issa J Dahabreh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">L</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lamont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Minghua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Kent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Graph-sparse lda: A topic model with structured sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Finale</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2575" to="2581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Retrofitting word vectors to semantic lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sujay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4166</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A primer on neural network models for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="345" to="420" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William J</forename><surname>Dally</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.00149</idno>
		<title level="m">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 10th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Distributional semantics resources for biomedical text processing</title>
		<editor>SPFGH Moen and Tapio Salakoski2 Sophia Ananiadou</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ACL</title>
		<meeting>of the ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning representations by backpropagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hintont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">6088</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hash kernels for structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinfeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Petterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gideon</forename><surname>Dror</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svn</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="2615" to="2637" />
			<date type="published" when="2009-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Charged Particle Radiation Therapy for Cancer: A Systematic Review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Terasawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dvorak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Raman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Trikalinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Intern. Med</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semi-automated screening of biomedical citations for systematic reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Byron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">A</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Trikalinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carla</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher H</forename><surname>Brodley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">55</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Feature hashing for large scale multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Attenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1113" to="1120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Annotating expressions of opinions and emotions in language. Language resources and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="165" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rcnet: A general framework for incorporating knowledge into word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management</title>
		<meeting>the 23rd ACM International Conference on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1219" to="1228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Linguistic structured sparsity in text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="786" to="796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improving lexical embeddings with semantic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="545" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Retrofitting word vectors of mesh terms to improve semantic similarity measures. Intl</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elmer</forename><forename type="middle">V</forename><surname>Bernstam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron C</forename><surname>Wallace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Health Text Mining and Information Analysis at EMNLP pages</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="43" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adadelta: An adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron C</forename><surname>Wallace</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.04469</idno>
		<title level="m">Rationale-augmented convolutional neural networks for text classification</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">MGNC-CNN: A simple approach to exploiting multiple word embeddings for sentence classification pages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron C</forename><surname>Wallace</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1522" to="1527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A sensitivity analysis of (and practitioners&apos; guide to) convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron C</forename><surname>Wallace</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.03820</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
