<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Models for Documents with Metadata</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018. 2031</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dallas</forename><surname>Card</surname></persName>
							<email>dcard@cmu.edu chenhao.tan@colorado.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Machine Learning Department</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhao</forename><surname>Tan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Colorado</orgName>
								<address>
									<postCode>80309</postCode>
									<settlement>Boulder</settlement>
									<region>CO</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
							<email>nasmith@cs.washington.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Paul G</orgName>
								<orgName type="institution" key="instit1">Allen School of CSE</orgName>
								<orgName type="institution" key="instit2">University of Washington</orgName>
								<address>
									<postCode>98195</postCode>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Models for Documents with Metadata</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2031" to="2040"/>
							<date type="published">July 15-20, 2018. 2018. 2031</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Most real-world document collections involve various types of metadata, such as author, source, and date, and yet the most commonly-used approaches to modeling text corpora ignore this information. While specialized models have been developed for particular applications, few are widely used in practice, as customization typically requires derivation of a custom inference algorithm. In this paper, we build on recent advances in variational inference methods and propose a general neural framework, based on topic models, to enable flexible incorporation of metadata and allow for rapid exploration of alternative models. Our approach achieves strong performance, with a manageable tradeoff between perplex-ity, coherence, and sparsity. Finally, we demonstrate the potential of our framework through an exploration of a corpus of articles about US immigration.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Topic models comprise a family of methods for uncovering latent structure in text corpora, and are widely used tools in the digital humanities, political science, and other related fields <ref type="bibr" target="#b5">(Boyd-Graber et al., 2017)</ref>. Latent Dirichlet allocation (LDA; <ref type="bibr" target="#b4">Blei et al., 2003</ref>) is often used when there is no prior knowl- edge about a corpus. In the real world, however, most documents have non-textual attributes such as author <ref type="bibr" target="#b35">(Rosen-Zvi et al., 2004</ref>), timestamp ), rating <ref type="bibr" target="#b24">(McAuliffe and Blei, 2008)</ref>, or ideology ( <ref type="bibr" target="#b12">Eisenstein et al., 2011;</ref><ref type="bibr" target="#b29">Nguyen et al., 2015b</ref>), which we refer to as metadata.</p><p>Many customizations of LDA have been devel- oped to incorporate document metadata. Two mod- els of note are supervised LDA <ref type="bibr">(SLDA;</ref><ref type="bibr" target="#b24">McAuliffe and Blei, 2008)</ref>, which jointly models words and labels (e.g., ratings) as being generated from a la- tent representation, and sparse additive genera- tive models (SAGE; <ref type="bibr" target="#b12">Eisenstein et al., 2011</ref>), which assumes that observed covariates (e.g., author ide- ology) have a sparse effect on the relative proba- bilities of words given topics. The structural topic model (STM; <ref type="bibr" target="#b34">Roberts et al., 2014</ref>), which adds cor- relations between topics to SAGE, is also widely used, but like SAGE it is limited in the types of metadata it can efficiently make use of, and how that metadata is used. Note that in this work we will distinguish labels (metadata that are gener- ated jointly with words from latent topic represen- tations) from covariates (observed metadata that influence the distribution of labels and words).</p><p>The ability to create variations of LDA such as those listed above has been limited by the expertise needed to develop custom inference algorithms for each model. As a result, it is rare to see such varia- tions being widely used in practice. In this work, we take advantage of recent advances in variational methods ( <ref type="bibr" target="#b17">Kingma and Welling, 2014;</ref><ref type="bibr" target="#b33">Rezende et al., 2014;</ref><ref type="bibr" target="#b25">Miao et al., 2016;</ref><ref type="bibr" target="#b37">Srivastava and Sutton, 2017</ref>) to facilitate approximate Bayesian infer- ence without requiring model-specific derivations, and propose a general neural framework for topic models with metadata, SCHOLAR. <ref type="bibr">1</ref> SCHOLAR combines the abilities of SAGE and SLDA, and allows for easy exploration of the fol- lowing options for customization:</p><p>1. Covariates: as in SAGE and STM, we incorpo- rate explicit deviations for observed covariates, as well as effects for interactions with topics.</p><p>2. Supervision: as in SLDA, we can use metadata as labels to help infer topics that are relevant in predicting those labels.</p><p>3. Rich encoder network: we use the encoding network of a variational autoencoder (VAE) to incorporate additional prior knowledge in the form of word embeddings, and/or to provide interpretable embeddings of covariates.</p><p>4. Sparsity: as in SAGE, a sparsity-inducing prior can be used to encourage more interpretable topics, represented as sparse deviations from a background log-frequency.</p><p>We begin with the necessary background and motivation ( §2), and then describe our basic frame- work and its extensions ( §3), followed by a series of experiments ( §4). In an unsupervised setting, we can customize the model to trade off between perplexity, coherence, and sparsity, with improved coherence through the introduction of word vec- tors. Alternatively, by incorporating metadata we can either learn topics that are more predictive of labels than SLDA, or learn explicit deviations for particular parts of the metadata. Finally, by com- bining all parts of our model we can meaningfully incorporate metadata in multiple ways, which we demonstrate through an exploration of a corpus of news articles about US immigration.</p><p>In presenting this particular model, we empha- size not only its ability to adapt to the characteris- tics of the data, but the extent to which the VAE approach to inference provides a powerful frame- work for latent variable modeling that suggests the possibility of many further extensions. Our im- plementation is available at https://github. com/dallascard/scholar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Motivation</head><p>LDA can be understood as a non-negative Bayesian matrix factorization model: the observed document-word frequency matrix, X ∈ Z D×V (D is the number of documents, V is the vocab- ulary size) is factored into two low-rank matri- ces, Θ D×K and B K×V , where each row of Θ, θ i ∈ ∆ K is a latent variable representing a distri- bution over topics in document i, and each row of B, β k ∈ ∆ V , represents a single topic, i.e., a dis- tribution over words in the vocabulary. <ref type="bibr">2</ref> While it is possible to factor the count data into unconstrained 2 Z denotes nonnegative integers, and ∆ K denotes the set of K-length nonnegative vectors that sum to one. For a proper probabilistic interpretation, the matrix to be factored is actually the matrix of latent mean parameters of the assumed data generating process, Xij ∼ Poisson(Λij). See Cemgil (2009) or <ref type="bibr" target="#b31">Paisley et al. (2014)</ref> for details. matrices, the particular priors assumed by LDA are important for interpretability ( <ref type="bibr" target="#b40">Wallach et al., 2009)</ref>. For example, the neural variational docu- ment model (NVDM; <ref type="bibr" target="#b25">Miao et al., 2016</ref>) allows θ i ∈ R K and achieves normalization by taking the softmax of θ i B. However, the experiments in <ref type="bibr" target="#b37">Srivastava and Sutton (2017)</ref> found the perfor- mance of the NVDM to be slightly worse than LDA in terms of perplexity, and dramatically worse in terms of topic coherence.</p><p>The topics discovered by LDA tend to be parsi- monious and coherent groupings of words which are readily identifiable to humans as being related to each other ( <ref type="bibr" target="#b9">Chang et al., 2009)</ref>, and the resulting mode of the matrix Θ provides a representation of each document which can be treated as a measure- ment for downstream tasks, such as classification or answering social scientific questions <ref type="bibr" target="#b39">(Wallach, 2016)</ref>. LDA does not require -and cannot make use of -additional prior knowledge. As such, the topics that are discovered may bear little connec- tion to metadata of a corpus that is of interest to a researcher, such as sentiment, ideology, or time.</p><p>In this paper, we take inspiration from two mod- els which have sought to alleviate this problem. The first, supervised LDA (SLDA; <ref type="bibr" target="#b24">McAuliffe and Blei, 2008)</ref>, assumes that documents have labels y which are generated conditional on the correspond- ing latent representation, i.e., y i ∼ p(y | θ i ). 3 By incorporating labels into the model, it is forced to learn topics which allow documents to be repre- sented in a way that is useful for the classification task. Such models can be used inductively as text classifiers ( <ref type="bibr" target="#b0">Balasubramanyan et al., 2012)</ref>. <ref type="bibr">SAGE (Eisenstein et al., 2011)</ref>, by contrast, is an exponential-family model, where the key inno- vation was to replace topics with sparse deviations from the background log-frequency of words (d), i.e., p(word | softmax(d + θ i B)). SAGE can also incorporate deviations for observed covariates, as well as interactions between topics and covariates, by including additional terms inside the softmax. In principle, this allows for inferring, for example, the effect on an author's ideology on their choice of words, as well as ideological variations on each underlying topic. Unlike the NVDM, SAGE still constrains θ i to lie on the simplex, as in LDA.</p><p>SLDA and SAGE provide two different ways that users might wish to incorporate prior knowl-edge as a way of guiding the discovery of topics in a corpus: SLDA incorporates labels through a distribution conditional on topics; SAGE includes explicit sparse deviations for each unique value of a covariate, in addition to topics. <ref type="bibr">4</ref> Because of the Dirichlet-multinomial conjugacy in the original model, efficient inference algorithms exist for LDA. Each variation of LDA, however, has required the derivation of a custom inference algorithm, which is a time-consuming and error- prone process. In SLDA, for example, each type of distribution we might assume for p(y | θ) would require a modification of the inference algorithm. SAGE breaks conjugacy, and as such, the authors adopted L-BFGS for optimizing the variational bound. Moreover, in order to maintain compu- tational efficiency, it assumed that covariates were limited to a single categorical label.</p><p>More recently, the variational autoencoder (VAE) was introduced as a way to perform approxi- mate posterior inference on models with otherwise intractable posteriors ( <ref type="bibr" target="#b17">Kingma and Welling, 2014;</ref><ref type="bibr" target="#b33">Rezende et al., 2014</ref>). This approach has previously been applied to models of text by <ref type="bibr" target="#b25">Miao et al. (2016)</ref> and <ref type="bibr" target="#b37">Srivastava and Sutton (2017)</ref>. We build on their work and show how this framework can be adapted to seamlessly incorporate the ideas of both SAGE and SLDA, while allowing for greater flexibility in the use of metadata. Moreover, by exploiting au- tomatic differentiation, we allow for modification of the model without requiring any change to the inference procedure. The result is not only a highly adaptable family of models with scalable inference and efficient prediction; it also points the way to incorporation of many ideas found in the literature, such as a gradual evolution of topics ), and hierarchical models ( <ref type="bibr" target="#b2">Blei et al., 2010;</ref><ref type="bibr" target="#b30">Nguyen et al., 2013</ref><ref type="bibr" target="#b29">Nguyen et al., , 2015b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SCHOLAR: A Neural Topic Model with Covariates, Supervision, and Sparsity</head><p>We begin by presenting the generative story for our model, and explain how it generalizes both SLDA and SAGE ( §3.1). We then provide a general expla- nation of inference using VAEs and how it applies to our model ( §3.2), as well as how to infer docu- 4 A third way of incorporating metadata is the approach used by various "upstream" models, such as Dirichlet- multinomial regression <ref type="bibr" target="#b26">(Mimno and McCallum, 2008)</ref>, which uses observed metadata to inform the document prior. We hy- pothesize that this approach could be productively combined with our framework, but we leave this as future work. ment representations and predict labels at test time ( §3.3). Finally, we discuss how we can incorporate additional prior knowledge ( §3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Generative Story</head><p>Consider a corpus of D documents, where docu- ment i is a list of N i words, w i , with V words in the vocabulary. For each document, we may have observed covariates c i (e.g., year of publication), and/or one or more labels, y i (e.g., sentiment).</p><p>Our model builds on the generative story of LDA, but optionally incorporates labels and covariates, and replaces the matrix product of Θ and B with a more flexible generative network, f g , followed by a softmax transform. Instead of using a Dirichlet prior as in LDA, we employ a logistic normal prior on θ as in <ref type="bibr" target="#b37">Srivastava and Sutton (2017)</ref> to facilitate inference ( §3.2): we draw a latent variable, r, 5 from a multivariate normal, and transform it to lie on the simplex using a softmax transform. <ref type="bibr">6</ref> The generative story is shown in <ref type="figure" target="#fig_0">Figure 1a</ref> and described in equations below:</p><p>For each document i of length N i :</p><p># Draw a latent representation on the sim- plex from a logistic normal prior:</p><formula xml:id="formula_0">r i ∼ N (r | µ 0 (α), diag(σ 2 0 (α))) θ i = softmax(r i )</formula><p># Generate words, incorporating covariates:</p><formula xml:id="formula_1">η i = f g (θ i , c i ) For each word j in document i: w ij ∼ p(w | softmax(η i )) # Similarly generate labels: y i ∼ p(y | f y (θ i , c i )),</formula><p>where p(w | softmax(η i )) is a multinomial distri- bution and p(y | f y (θ i , c i )) is a distribution appro- priate to the data (e.g., multinomial for categorical labels). f g is a model-specific combination of latent variables and covariates, f y is a multi-layer neural network, and µ 0 (α) and σ 2 0 (α) are the mean and diagonal covariance terms of a multivariate nor- mal prior. To approximate a symmetric Dirichlet prior with hyperparameter α, these are given by the Laplace approximation ( <ref type="bibr" target="#b15">Hennig et al., 2012</ref>) to be µ 0,k (α) = 0 and σ 2 0,k = (K − 1)/(αK). If we were to ignore covariates, place a Dirichlet prior on B, and let η = θ i B, this model is equiv- alent to SLDA with a logistic normal prior. Sim- ilarly, we can recover a model that is like SAGE, but lacks sparsity, if we ignore labels, and let</p><formula xml:id="formula_2">η i = d + θ i B + c i B cov + (θ i ⊗ c i ) B int , (1)</formula><p>where d is the V -dimensional background term (representing the log of the overall word frequency), θ i ⊗ c i is a vector of interactions between topics and covariates, and B cov and B int are additional weight (deviation) matrices. The background is included to account for common words with ap- proximately the same frequency across documents, meaning that the B * weights now represent both positive and negative deviations from this back- ground. This is the form of f g which we will use in our experiments.</p><p>To recover the full SAGE model, we can place a sparsity-inducing prior on each B * . As in Eisen- stein et al. (2011), we make use of the compound normal-exponential prior for each element of the weight matrices, B * m,n , with hyperparameter γ, 7</p><formula xml:id="formula_3">τ m,n ∼ Exponential(γ), (2) B * m,n ∼ N (0, τ m,n ).<label>(3)</label></formula><p>We can choose to ignore various parts of this model, if, for example, we don't have any labels or observed covariates, or we don't wish to use interactions or sparsity. <ref type="bibr">8</ref> Other generator networks could also be considered, with additional layers to represent more complex interactions, although this might involve some loss of interpretability.</p><p>In the absence of metadata, and without sparsity, our model is equivalent to the ProdLDA model of <ref type="bibr" target="#b37">Srivastava and Sutton (2017)</ref> with an explicit background term, and ProdLDA is, in turn, a <ref type="bibr">7</ref> To avoid having to tune γ, we employ an improper Jef- fery's prior, p(τm,n) ∝ 1/τm,n, as in SAGE. Although this causes difficulties in posterior inference for the variance terms, τ , in practice, we resort to a variational EM approach, with MAP-estimation for the weights, B, and thus alternate be- tween computing expectations of the τ parameters, and up- dating all other parameters using some variant of stochastic gradient descent. For this, we only require the expectation of each τmn for each E-step, which is given by 1/B 2 m,n . We re- fer the reader to <ref type="bibr" target="#b12">Eisenstein et al. (2011)</ref> for additional details. <ref type="bibr">8</ref> We could also ignore latent topics, in which case we would get a naïve Bayes-like model of text with deviations for each covariate p(wij | ci) special case of SAGE, without background log- frequencies, sparsity, covariates, or labels. In the next section we generalize the inference method used for ProdLDA; in our experiments we validate its performance and explore the effects of regular- ization and word-vector initialization ( §3.4). The NVDM ( <ref type="bibr" target="#b25">Miao et al., 2016)</ref> uses the same approach to inference, but does not not restrict document representations to the simplex.</p><formula xml:id="formula_4">∝ exp(d + c i B cov ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning and Inference</head><p>As in past work, each document i is assumed to have a latent representation r i , which can be in- terpreted as its relative membership in each topic (after exponentiating and normalizing). In order to infer an approximate posterior distribution over r i , we adopt the sampling-based VAE framework developed in previous work <ref type="bibr" target="#b17">(Kingma and Welling, 2014;</ref><ref type="bibr" target="#b33">Rezende et al., 2014</ref>). As in conventional variational inference, we assume a variational approximation to the poste- rior, q Φ (r i | w i , c i , y i ), and seek to minimize the KL divergence between it and the true posterior, p(r i | w i , c i , y i ), where Φ is the set of variational parameters to be defined below. After some ma- nipulations (given in supplementary materials), we obtain the evidence lower bound (ELBO) for a sin-gle document,</p><formula xml:id="formula_5">L(w i ) = E q Φ (r i |w i ,c i ,y i )   N i j=1 log p(w ij | r i , c i )   + E q Φ (r i |w i ,c i ,y i ) [log p(y i | r i , c i )] − D KL [q Φ (r i | w i , c i , y i ) || p(r i | α)] .<label>(4)</label></formula><p>As in the original VAE, we will encode the pa- rameters of our variational distributions using a shared multi-layer neural network. Because we have assumed a diagonal normal prior on r, this will take the form of a network which outputs a mean vector, µ i = f µ (w i , c i , y i ) and diagonal of a covariance matrix,</p><formula xml:id="formula_6">σ 2 i = f σ (w i , c i , y i ), such that q Φ (r i | w i , c i , y i ) = N (µ i , σ 2 i )</formula><p>. Incorporating labels and covariates to the inference network used by <ref type="bibr" target="#b25">Miao et al. (2016)</ref> and <ref type="bibr" target="#b37">Srivastava and Sutton (2017)</ref>, we use:</p><formula xml:id="formula_7">π i = f e ([W x x i ; W c c i ; W y y i ]),<label>(5)</label></formula><formula xml:id="formula_8">µ i = W µ π i + b µ ,<label>(6)</label></formula><formula xml:id="formula_9">log σ 2 i = W σ π i + b σ ,<label>(7)</label></formula><p>where x i is a V -dimensional vector representing the counts of words in w i , and f e is a multilayer perceptron. The full set of encoder parameters, Φ, thus includes the parameters of f e and all weight matrices and bias vectors in Equations 5-7 (see <ref type="figure" target="#fig_0">Figure 1b</ref>). This approach means that the expectations in Equation 4 are intractable, but we can approximate them using sampling. In order to maintain differen- tiability with respect to Φ, even after sampling, we make use of the reparameterization trick ( <ref type="bibr" target="#b17">Kingma and Welling, 2014</ref>), 9 which allows us to reparame- terize samples from q Φ (r | w i , c i , y i ) in terms of samples from an independent source of noise, i.e.,</p><formula xml:id="formula_10">(s) ∼ N (0, I), r (s) i = g Φ (w i , c i , y i , (s) ) = µ i + σ i · (s) .</formula><p>We thus replace the bound in Equation 4 with a Monte Carlo approximation using a single sam- <ref type="bibr">9</ref> The Dirichlet distribution cannot be directly reparame- terized in this way, which is why we use the logistic normal prior on θ to approximate the Dirichlet prior used in LDA. ple 10 of (and thereby of r):</p><formula xml:id="formula_11">L(w i ) ≈ N i j=1 log p(w ij | r (s) i , c i ) + log p(y i | r (s) i , c i ) − D KL [q Φ (r i | w i , c i , y i ) || p(r i | α)] .<label>(8)</label></formula><p>We can now optimize this sampling-based approxi- mation of the variational bound with respect to Φ, B * , and all parameters of f g and f y using stochas- tic gradient descent. Moreover, because of this stochastic approach to inference, we are not re- stricted to covariates with a small number of unique values, which was a limitation of SAGE. Finally, the KL divergence term in Equation 8 can be com- puted in closed form (see supplementary materials).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Prediction on Held-out Data</head><p>In addition to inferring latent topics, our model can both infer latent representations for new docu- ments and predict their labels, the latter of which was the motivation for SLDA. In traditional vari- ational inference, inference at test time requires fixing global parameters (topics), and optimizing the per-document variational parameters for the test set. With the VAE framework, by contrast, the encoder network (Equations 5-7) can be used to directly estimate the posterior distribution for each test document, using only a forward pass (no iterative optimization or sampling).</p><p>If not using labels, we can use this approach di- rectly, passing the word counts of new documents through the encoder to get a posterior q Φ (r i | w i , c i ). When we also include labels to be pre- dicted, we can first train a fully-observed model, as above, then fix the decoder, and retrain the encoder without labels. In practice, however, if we train the encoder network using one-hot encodings of document labels, it is sufficient to provide a vector of all zeros for the labels of test documents; this is what we adopt for our experiments ( §4.2), and we still obtain good predictive performance.</p><p>The label network, f y , is a flexible component which can be used to predict a wide range of out- comes, from categorical labels (such as star ratings; <ref type="bibr" target="#b24">McAuliffe and Blei, 2008)</ref> to real-valued outputs (such as number of citations or box-office returns; <ref type="bibr" target="#b41">Yogatama et al., 2011</ref>). For categorical labels, pre- dictions are given byˆy</p><formula xml:id="formula_12">byˆ byˆy i = argmax y ∈ Y p(y | r i , c i ).<label>(9)</label></formula><p>Alternatively, when dealing with a small set of categorical labels, it is also possible to treat them as observed categorical covariates during training. At test time, we can then consider all possible one-hot vectors, e, in place of c i , and predict the label that maximizes the probability of the words, i.e.,</p><formula xml:id="formula_13">ˆ y i = argmax y ∈ Y N i j=1 log p(w ij | r i , e y ).<label>(10)</label></formula><p>This approach works well in practice (as we show in §4.2), but does not scale to large numbers of labels, or other types of prediction problems, such as multi-class classification or regression.</p><p>The choice to include metadata as covariates, la- bels, or both, depends on the data. The key point is that we can incorporate metadata in two very different ways, depending on what we want from the model. Labels guide the model to infer topics that are relevant to those labels, whereas covari- ates induce explicit deviations, leaving the latent variables to account for the rest of the content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Additional Prior Information</head><p>A final advantage of the VAE framework is that the encoder network provides a way to incorporate additional prior information in the form of word vectors. Although we can learn all parameters start- ing from a random initialization, it is also possible to initialize and fix the initial embeddings of words in the model, W x , in Equation 5. This leverages word similarities derived from large amounts of un- labeled data, and may promote greater coherence in inferred topics. The same could also be done for some covariates; for example, we could embed the source of a news article based on its place on the ideological spectrum. Conversely, if we choose to learn these parameters, the learned values (W y and W c ) may provide meaningful embeddings of these metadata (see section §4.3).</p><p>Other variants on topic models have also pro- posed incorporating word vectors, both as a par- allel part of the generative process ( <ref type="bibr" target="#b28">Nguyen et al., 2015a)</ref>, and as an alternative parameterization of topic distributions ( <ref type="bibr" target="#b11">Das et al., 2015</ref>), but inference is not scalable in either of these models. Because of the generality of the VAE framework, we could also modify the generative story so that word em- beddings are emitted (rather than tokens); we leave this for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head><p>To evaluate and demonstrate the potential of this model, we present a series of experiments below. We first test SCHOLAR without observed meta- data, and explore the effects of using regulariza- tion and/or word vector initialization, compared to LDA, SAGE, and NVDM ( §4.1). We then evaluate our model in terms of predictive performance, in comparison to SLDA and an l 2 -regularized logistic regression baseline <ref type="figure" target="#fig_2">( §4.2)</ref>. Finally, we demonstrate the ability to incorporate covariates and/or labels in an exploratory data analysis ( §4.3).</p><p>The scores we report are generalization to held- out data, measured in terms of perplexity; coher- ence, measured in terms of non-negative point-wise mutual information (NPMI; <ref type="bibr" target="#b9">Chang et al., 2009;</ref><ref type="bibr" target="#b27">Newman et al., 2010)</ref>, and classification accuracy on test data. For coherence we evaluate NPMI us- ing the top 10 words of each topic, both internally (using test data), and externally, using a decade of articles from the English Gigaword dataset ( <ref type="bibr" target="#b13">Graff and Cieri, 2003</ref>). Since our model employs varia- tional methods, the reported perplexity is an upper bound based on the ELBO.</p><p>As datasets we use the familiar 20 newsgroups, the IMDB corpus of 50,000 movie reviews <ref type="bibr" target="#b23">(Maas et al., 2011)</ref>, and the UIUC Yahoo answers dataset with 150,000 documents in 15 categories ( <ref type="bibr" target="#b10">Chang et al., 2008)</ref>. For further exploration, we also make use of a corpus of approximately 4,000 time- stamped news articles about US immigration, each annotated with pro-or anti-immigration tone <ref type="bibr" target="#b7">(Card et al., 2015)</ref>. We use the original author-provided implementations of SAGE 11 and SLDA, 12 while for LDA we use Mallet. <ref type="bibr">13</ref> . Our implementation of SCHOLAR is in TensorFlow, but we have also provided a preliminary PyTorch implementation of the core of our model. <ref type="bibr">14</ref> For additional details about datasets and implementation, please refer to the supplementary material.</p><p>It is challenging to fairly evaluate the relative computational efficiency of our approach compared to past work (due to the stochastic nature of our ap-proach to inference, choices about hyperparameters such as tolerance, and because of differences in im- plementation). Nevertheless, in practice, the perfor- mance of our approach is highly appealing. For all experiments in this paper, our implementation was much faster than SLDA or SAGE (implemented in C and Matlab, respectively), and competitive with Mallet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Unsupervised Evaluation</head><p>Although the emphasis of this work is on incorpo- rating observed labels and/or covariates, we briefly report on experiments in the unsupervised setting. Recall that, without metadata, SCHOLAR equates to ProdLDA, but with an explicit background term. <ref type="bibr">15</ref> We therefore use the same experimental setup as Srivastava and Sutton (2017) (learning rate, mo- mentum, batch size, and number of epochs) and find the same general patterns as they reported (see <ref type="table">Table 1</ref> and supplementary material): our model returns more coherent topics than LDA, but at the cost of worse perplexity. SAGE, by contrast, attains very high levels of sparsity, but at the cost of worse perplexity and coherence than LDA. As expected, the NVDM produces relatively low perplexity, but very poor coherence, due to its lack of constraints on θ.</p><p>Further experimentation revealed that the VAE framework involves a tradeoff among the scores; running for more epochs tends to result in bet- ter perplexity on held-out data, but at the cost of worse coherence. Adding regularization to encour- age sparse topics has a similar effect as in SAGE, leading to worse perplexity and coherence, but it does create sparse topics. Interestingly, initializing the encoder with pretrained word2vec embeddings, and not updating them returned a model with the best internal coherence of any model we considered for IMDB and Yahoo answers, and the second-best for 20 newsgroups.</p><p>The background term in our model does not have much effect on perplexity, but plays an important role in producing coherent topics; as in SAGE, the background can account for common words, so they are mostly absent among the most heavily weighted words in the topics. For instance, words like film and movie in the IMDB corpus are rel- atively unimportant in the topics learned by our  <ref type="table">Table 1</ref>: Performance of our various models in an unsupervised setting (i.e., without labels or co- variates) on the IMDB dataset using a 5,000-word vocabulary and 50 topics. The supplementary ma- terials contain additional results for 20 newsgroups and Yahoo answers.</p><p>model, but would be much more heavily weighted without the background term, as they are in topics learned by LDA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Text Classification</head><p>We next consider the utility of our model in the context of categorical labels, and consider them alternately as observed covariates and as labels generated conditional on the latent representation. We use the same setup as above, but tune number of training epochs for our model using a random 20% of training data as a development set, and similarly tune regularization for logistic regression. <ref type="table" target="#tab_2">Table 2</ref> summarizes the accuracy of various mod- els on three datasets, revealing that our model offers competitive performance, both as a joint model of words and labels (Eq. 9), and a model which condi- tions on covariates (Eq. 10). Although SCHOLAR is comparable to the logistic regression baseline, our purpose here is not to attain state-of-the-art per- formance on text classification. Rather, the high accuracies we obtain demonstrate that we are learn- ing low-dimensional representations of documents that are relevant to the label of interest, outperform- ing SLDA, and have the same attractive properties as topic models. Further, any neural network that is successful for text classification could be incor- porated into f y and trained end-to-end along with topic discovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Exploratory Study</head><p>We demonstrate how our model might be used to explore an annotated corpus of articles about immi- gration, and adapt to different assumptions about the data. We only use a small number of topics in this part (K = 8) for compact presentation.   Tone as a label. We first consider using the an- notations as a label, and train a joint model to infer topics relevant to the tone of the article (pro-or anti-immigration). <ref type="figure" target="#fig_2">Figure 2</ref> shows a set of top- ics learned in this way, along with the predicted probability of an article being pro-immigration con- ditioned on the given topic. All topics are coherent, and the predicted probabilities have strong face validity, e.g., "arrested charged charges agents op- eration" is least associated with pro-immigration.</p><p>Tone as a covariate. Next we consider using tone as a covariate, and build a model using both tone and tone-topic interactions. <ref type="table">Table 3</ref> shows a set of topics learned from the immigration data, along with the most highly-weighted words in the corresponding tone-topic interaction terms. As can be seen, these interaction terms tend to capture dif- ferent frames (e.g., "criminal" vs. "detainees", and "illegals" vs. "newcomers", etc).</p><p>Combined model with temporal metadata. Fi- nally, we incorporate both the tone annotations and the year of publication of each article, treating the former as a label and the latter as a covariate. In this model, we also include an embedding matrix, W c , to project the one-hot year vectors down to a two-dimensional continuous space, with a learned deviation for each dimension. We omit the topics in the interest of space, but <ref type="figure" target="#fig_3">Figure 3</ref> shows the learned embedding for each year, along with the top terms of the corresponding deviations. As can be seen, the model learns that adjacent years tend to produce similar deviations, even though we have not explicitly encoded this information. The left- right dimension roughly tracks a temporal trend with positive deviations shifting from the years of Clinton and INS on the left, to Obama and ICE on the right. <ref type="bibr">16</ref> Meanwhile, the events of 9/11 dom- inate the vertical direction, with the words sept,</p><formula xml:id="formula_14">0 1 p(pro-immigration | topic)</formula><p>arrested charged charges agents operation state gov benefits arizona law bill bills bush border president bill republicans labor jobs workers percent study wages asylum judge appeals deportation court visas visa applications students citizenship boat desert died men miles coast haitian english language city spanish community  hijackers, and attacks increasing in probability as we move up in the space. If we wanted to look at each year individually, we could drop the embed- ding of years, and learn a sparse set of topic-year interactions, similar to tone in <ref type="table">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Additional Related Work</head><p>The literature on topic models is vast; in addition to papers cited throughout, other efforts to incorpo- rate metadata into topic models include Dirichlet- multinomial regression (DMR; <ref type="bibr" target="#b26">Mimno and McCallum, 2008</ref>), Labeled LDA ( <ref type="bibr" target="#b32">Ramage et al., 2009)</ref>, and MedLDA ( <ref type="bibr" target="#b42">Zhu et al., 2009)</ref>. A recent paper also extended DMR by using deep neural networks to embed metadata into a richer document prior <ref type="bibr" target="#b1">(Benton and Dredze, 2018)</ref>. A separate line of work has pursued parame- terizing unsupervised models of documents us- ing neural networks (Hinton and Salakhutdinov, Base topics (each row is a topic) Anti-immigration interactions Pro-immigration interactions ice customs agency enforcement homeland criminal customs arrested detainees detention center agency population born percent americans english jobs million illegals taxpayers english newcomers hispanic city judge case court guilty appeals attorney guilty charges man charged asylum court judge case appeals patrol border miles coast desert boat guard patrol border agents boat died authorities desert border bodies licenses drivers card visa cards applicants foreign sept visas system green citizenship card citizen apply island story chinese ellis international smuggling federal charges island school ellis english story guest worker workers bush labor bill bill border house senate workers tech skilled farm labor benefits bill welfare republican state senate republican california gov state law welfare students tuition <ref type="table">Table 3</ref>: Top words for topics (left) and the corresponding anti-immigration (middle) and pro-immigration (right) variations when treating tone as a covariate, with interactions.</p><p>2009; <ref type="bibr" target="#b21">Larochelle and Lauly, 2012)</ref>, including non- Bayesian approaches <ref type="bibr" target="#b6">(Cao et al., 2015)</ref>. More re- cently, <ref type="bibr" target="#b22">Lau et al. (2017)</ref> proposed a neural language model that incorporated topics, and <ref type="bibr" target="#b14">He et al. (2017)</ref> developed a scalable alternative to the correlated topic model by simultaneously learning topic em- beddings.</p><p>Others have attempted to extend the reparameter- ization trick to the Dirichlet and Gamma distribu- tions, either through transformations ( ) or a generalization of reparameteriza- tion ( <ref type="bibr" target="#b36">Ruiz et al., 2016)</ref>. Black-box and VAE-style inference have been implemented in at least two general purpose tools designed to allow rapid explo- ration and evaluation of models ( <ref type="bibr" target="#b18">Kucukelbir et al., 2015;</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have presented a neural framework for general- ized topic models to enable flexible incorporation of metadata with a variety of options. We take advantage of stochastic variational inference to de- velop a general algorithm for our framework such that variations do not require any model-specific algorithm derivations. Our model demonstrates the tradeoff between perplexity, coherence, and sparsity, and outperforms SLDA in predicting doc- ument labels. Furthermore, the flexibility of our model enables intriguing exploration of a text cor- pus on US immigration. We believe that our model and code will facilitate rapid exploration of docu- ment collections with metadata.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Figure 1a presents the generative story of our model. Figure 1b illustrates the inference network using the reparametrization trick to perform variational inference on our model. Shaded nodes are observed; double circles indicate deterministic transformations of parent nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Topics inferred by a joint model of words and tone, and the corresponding probability of proimmigration tone for each topic. A topic is represented by the top words sorted by word probability throughout the paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Learned embeddings of year-ofpublication (treated as a covariate) from combined model of news articles about immigration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Accuracy of various models on three 
datasets with categorical labels. 

</table></figure>

			<note place="foot" n="3"> Technically, the model conditions on the mean of the perword latent variables, but we elide this detail in the interest of concision.</note>

			<note place="foot" n="5"> r is equivalent to z in the original VAE. To avoid confusion with topic assignment of words in the topic modeling literature, we use r instead of z. 6 Unlike the correlated topic model (CTM; Lafferty and Blei, 2006), which also uses a logistic-normal prior, we fix the parameters of the prior and use a diagonal covariance matrix, rather than trying to infer correlations among topics. However, it would be a straightforward extension of our framework to place a richer prior on the latent document representations, and learn correlations by updating the parameters of this prior after each epoch, analogously to the variational EM approach used for the CTM.</note>

			<note place="foot" n="10"> Alternatively, one can average over multiple samples.</note>

			<note place="foot" n="11"> github.com/jacobeisenstein/SAGE 12 github.com/blei-lab/class-slda 13 mallet.cs.umass.edu 14 github.com/dallascard/scholar</note>

			<note place="foot" n="15"> Note, however, that a batchnorm layer in ProdLDA may play a similar role to a background term, and there are small differences in implementation; please see supplementary material for more discussion of this.</note>

			<note place="foot" n="16"> The Immigration and Naturalization Service (INS) was transformed into Immigration and Customs Enforcement (ICE) and other agencies in 2003.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Charles Sutton, anonymous reviewers, and all members of Noah's ARK for helpful discussions and feedback. This work was made possible by a University of Washington Inno-vation award and computing resources provided by XSEDE.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Modeling polarizing topics: When do different political communities respond differently to the same news?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramnath</forename><surname>Balasubramanyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">P</forename><surname>Redlawsk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICWSM</title>
		<meeting>ICWSM</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep Dirichlet multinomial regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Benton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The nested Chinese restaurant process and Bayesian nonparametric inference of topic hierarchies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JACM</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">57</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dynamic topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Latent Dirichlet allocation. JMLR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Applications of topic models. Foundations and Trends in Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuening</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="143" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A novel neural topic model and its supervised extension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The media frames corpus: Annotations of frames across issues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dallas</forename><surname>Card</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amber</forename><forename type="middle">E</forename><surname>Boydstun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">H</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bayesian inference for nonnegative matrix factorisation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Taylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cemgil</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Intelligence and Neuroscience</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="4" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Reading tea leaves: How humans interpret topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Gerrish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><forename type="middle">L</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Importance of semantic representation: Dataless classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev-Arie</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Srikumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Gaussian LDA for topic models with word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sparse additive generative models of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">English gigaword corpus. Linguistic Data Consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cieri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient correlated topic modeling with topic embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD</title>
		<meeting>KDD</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Kernel topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Hennig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thore</forename><surname>Graepel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AISTATS</title>
		<meeting>AISTATS</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Replicated softmax: An undirected topic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan R</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Autoencoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automatic variational inference in stan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alp</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajesh</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Automatic differentiation variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alp</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajesh</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<idno>ArXiv:1603.00788</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Correlated topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A neural autoregressive topic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislas</forename><surname>Lauly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Topically driven neural language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Jey Han Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Supervised topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">D</forename><surname>Mcauliffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural variational inference for text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Topic models conditioned on arbitrary features with Dirichlet-multinomial regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of UAI</title>
		<meeting>UAI</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Automatic evaluation of topic coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jey</forename><forename type="middle">Han</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Grieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improving topic models with latent feature word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Dat Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Billingsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Tea party in the house: A hierarchical ideal point topic model and its application to Republican legislators in the 112th congress</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Viet-An Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Resnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Lexical and hierarchical topic regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Viet-An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><forename type="middle">L</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bayesian nonnegative matrix factorization with stochastic variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">William</forename><surname>Paisley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Mixed Membership Models and Their Applications</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="205" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Labeled LDA: A supervised topic model for credit attribution in multilabeled corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Shakir Mohamed, and Daan Wierstra</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Structural topic models for open ended survey responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Molly</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tingley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jetson</forename><surname>Leder-Luis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shana</forename><surname>Gadarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bethany</forename><surname>Albertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Rand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Political Science</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="1064" to="1082" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The author-topic model for authors and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Rosen-Zvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padhraic</forename><surname>Smyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of UAI</title>
		<meeting>UAI</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The generalized reparameterization gradient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J R</forename><surname>Francisco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Michalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David M</forename><surname>Titsias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Autoencoding variational inference for topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akash</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Edward: A library for probabilistic modeling, inference, and criticism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alp</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adji</forename><forename type="middle">B</forename><surname>Dieng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<idno>ArXiv:1610.09787</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Interpretability and measurement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP Workshop on Natural Language Processing and Computational Social Science</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Rethinking LDA: Why priors matter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Predicting a scientific community&apos;s response to an article</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Heilman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Bryan R Routledge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">MedLDA: Maximum margin supervised topic models for regression and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
