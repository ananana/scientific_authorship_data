<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Radical Embedding: Delving Deeper to Chinese Radicals</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 26-31, 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sogou Technology Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Zhai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sogou Technology Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sogou Technology Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehua</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sogou Technology Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sogou Technology Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Radical Embedding: Delving Deeper to Chinese Radicals</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="594" to="598"/>
							<date type="published">July 26-31, 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Languages using Chinese characters are mostly processed at word level. Inspired by recent success of deep learning , we delve deeper to character and radical levels for Chinese language processing. We propose a new deep learning technique, called &quot;radical embed-ding&quot;, with justifications based on Chi-nese linguistics, and validate its feasibility and utility through a set of three experiments: two in-house standard experiments on short-text catego-rization (STC) and Chinese word seg-mentation (CWS), and one in-field experiment on search ranking. We show that radical embedding achieves comparable , and sometimes even better, results than competing methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Chinese is one of the oldest written languages in the world, but it does not attract much at- tention in top NLP research forums, proba- bly because of its peculiarities and drastic dif- ferences from English. There are sentences, words, characters in Chinese, as illustrated in <ref type="figure">Figure 1</ref>. The top row is a Chinese sentence, whose English translation is at the bottom. In between is the pronunciation of the sentence in Chinese, called PinYin, which is a form of Romanian phonetic representation of Chinese, similar to the International Phonetic Alpha- bet (IPA) for English. Each squared symbol is a distinct Chinese character, and there are no separators between characters calls for Chi- nese Word Segmentation (CWS) techniques to group adjacent characters into words.</p><p>In most current applications (e.g., catego- rization and recommendation etc.), Chinese is English: It is a nice day today.</p><p>Pinyin: jīn tiān/ tiān qì/ zhēn/ hǎo! Chinese: !"#"##"$"#%"#&amp;! a word a character <ref type="figure">Figure 1</ref>: Illustration of Chinese Language represented at the word level. Inspired by re- cent success of delving deep ( <ref type="bibr" target="#b10">Szegedy et al., 2014;</ref><ref type="bibr" target="#b12">Zhang and LeCun, 2015;</ref><ref type="bibr" target="#b0">Collobert et al., 2011</ref>), an interesting question arises then: can we delve deeper than word level represen- tation for better Chinese language processing? If the answer is yes, how deep can it be done for fun and for profit?</p><p>Intuitively, the answer should be positive. Nevertheless, each Chinese character is seman- tically meaningful, thanks to its pictographic root from ancient Chinese as depicted in <ref type="figure">Fig- ure 2</ref>. We could delve deeper by decomposing each character into character radicals.</p><p>The right part of <ref type="figure">Figure 2</ref> illustrates the de- composition. This Chinese character (mean- ing "morning") is decomposed into 4 radicals that consists of 12 strokes in total. In Chi- nese linguistics, each Chinese character can be decomposed into no more than four radicals based on a set of preset rules 1 . As depicted by the pictograms in the right part of <ref type="figure">Figure 2</ref>, the 1st radical (and the 3rd that happens to be the same) means "grass", and the 2nd and the 4th mean the "sun" and the "moon", re- spectively. These four radicals altogether con- vey the meaning that "the moment when sun arises from the grass while the moon wanes away", which is exactly "morning". On the other hand, it is hard to decipher the seman- tics of strokes, and radicals are the minimum semantic unit for Chinese. Building deep mod- Figure 2: Decomposition of Chinese Character els from radicals could lead to interesting re- sults. In sum, this paper makes the following three-fold contributions: (1) we propose a new deep learning technique, called "radical embedding", for Chinese language processing with proper justifications based on Chinese linguistics; (2) we validate the feasibility and utility of radical embedding through a set of three experiments, which include not only two in-house standard experiments on short-text categorization (STC) and Chinese word seg- mentation (CWS), but an in-field experiment on search ranking as well; (3) this initial suc- cess of radical embedding could shed some light on new approaches to better language processing for Chinese and other languages alike.</p><formula xml:id="formula_0">! " ! #</formula><p>The rest of this paper is organized as fol- lows. Section 2 presents the radical embed- ding technique and the accompanying deep neural network components, which are com- bined and stacked to solve three application problems. Section 3 elaborates on the three applications and reports on the experiment re- sults. With related work briefly discussed in Section 4, Section 5 concludes this study. For clarity, we limit the study to Simplified Chi- nese in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Deep Networks with Radical Embeddings</head><p>This section presents the radical embedding technique, and the accompanying deep neu- ral network components. These components are combined to solve the three applications in Section 3. Word embedding is a popular technique in NLP <ref type="bibr" target="#b0">(Collobert et al., 2011</ref>). It maps words to vectors of real numbers in a relatively low di- mensional space. It is shown that the proxim- ity in this numeric space actually embodies al- gebraic semantic relationship, such as "Queen</p><formula xml:id="formula_1">input output Convolution f ∈ R m k ∈ R n y ∈ R m+n−1 y i = i+n−1 s=i fs · k s−i 0 ≤ i ≤ m − n + 1 Max-pooling x ∈ R d y = max(x) ∈ R Lookup Table M ∈ R d×|D| I i ∈ R |D|×1 v i = M I i ∈ R d Tanh x ∈ R d y ∈ R d y i = e x i −e −x i e x i +e −x i 0 ≤ i ≤ d − 1 Linear x ∈ R d y = x ∈ R d ReLU x ∈ R d y ∈ R d y i = 0 if x i ≤ 0 y i = x i if x i &gt; 0 0 ≤ i ≤ d − 1 Softmax x ∈ R d y ∈ R d y i = e x i d j=1 e x j 0 ≤ i ≤ d − 1 Concatenate x i ∈ R d 0 ≤ i ≤ n − 1 y = (x 0 , x 1 , ..., x n−1 ) ∈ R d×n D: radical vocabulary M : a matrix containing |D| columns, each column is a d-dimensional vector represent radical in D.</formula><p>I i : a one hot vector stands for the ith radical in vocabulary  <ref type="bibr" target="#b7">Mikolov et al., 2013)</ref>. As demonstrated in previous work, this numeric representation of words has led to big improvements in many NLP tasks such as machine translation <ref type="bibr" target="#b9">(Sutskever et al., 2014</ref>), question answering ( <ref type="bibr" target="#b3">Iyyer et al., 2014</ref>) and document ranking <ref type="bibr" target="#b8">(Shen et al., 2014</ref>).</p><p>Radical embedding is similar to word em- bedding except that the embedding is at rad- ical level. There are two ways of embedding: CBOW and skip-gram ( <ref type="bibr" target="#b7">Mikolov et al., 2013</ref>). We here use CBOW for radical embedding be- cause the two methods exhibit few differences, and CBOW is slightly faster in experiments. Specifically, a sequence of Chinese characters is decomposed into a sequence of radicals, to which CBOW is applied. We use the word2vec package ( <ref type="bibr" target="#b7">Mikolov et al., 2013</ref>) to train radical vectors, and then initialize the lookup table with these radical vectors.</p><p>We list the network components in <ref type="table" target="#tab_1">Table 1</ref>, which are combined and stacked in <ref type="figure">Figure 3</ref> to solve different problems in Section 3. Each component is a function, the input column of <ref type="table" target="#tab_1">Table 1</ref> demonstrates input parameters and their dimensions of these functions, the out- put column shows the formulas and outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Applications and Experiments</head><p>In this section, we explain how to stack the components in <ref type="table" target="#tab_1">Table 1</ref> to solve three prob- lems: short-text categorization, Chinese word segmentation and search ranking, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convolution 1×3</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ReLU 256</head><p>Lookup  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Short-Text Categorization</head><p>Figure 3(a) presents the network structure of the model for short-text categorization, where the width of each layer is marked out as well.</p><p>From the top down, a piece of short text, e.g., the title of a URL, is fed into the net- work, which goes through radical decomposi- tion, table-lookup (i.e., locating the embed- ding vector corresponding to each radical), convolution, max pooling, two ReLU layers and one fully connected layer, all the way to the final softmax layer, where the loss is cal- culated against the given label. The stan- dard back-propagation algorithm is used to fine tune all the parameters.</p><p>The experiment uses the top-3 categories of the SogouCA and SogouCS news corpus ( <ref type="bibr" target="#b11">Wang et al., 2008)</ref>. 100,000 samples of each category are randomly selected for training and 10,000 for testing. Hyper-parameters for SVM and LR are selected through cross- validation. <ref type="table" target="#tab_3">Table 2</ref> presents the accuracy of different methods, where "wrd", "chr", and "rdc" denote word, character, and radical em- bedding, respectively. As can be seen, embed- ding methods outperform competing LR and SVM algorithms uniformly, and the fusion of radicals with words and characters improves both.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Chinese Word Segmentation</head><p>Figure 3(b) presents the CWS network ar- chitecture. It uses softmax as well because it essentially classifies whether each charac- ter should be a segmentation boundary. The input is firstly decomposed into a radical se- quence, on which a sliding window of size 3 is applied to extract features, which are pipelined to downstream levels of the network.</p><p>We evaluate the performance using two standard datasets: PKU and MSR, as pro- vided by <ref type="bibr" target="#b2">(Emerson, 2005)</ref>. The PKU dataset contains 1.1M training words and 104K test words, and the MSR dataset contains 2.37M training words and 107K test words. We use the first 90% sentences for training and the rest 10% sentences for testing. We compare radical embedding with the CRF method 2 , FNLM ( <ref type="bibr" target="#b6">Mansur et al., 2013</ref>) and PSA ( <ref type="bibr" target="#b13">Zheng et al., 2013)</ref>, and present the results in <ref type="table" target="#tab_5">Table  3</ref>. Note that no dictionary is used in any of these algorithms.</p><p>We see that the radical embedding (RdE) method, as the first attempt to segment words at radical level, actually achieves very compet- itive results. It outperforms both CRF and FNLM on both datasets, and is comparable with PSA.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Web Search Ranking</head><p>Finally, we report on an in-field experiment with Web search ranking. Web search lever- ages many kinds of ranking signals, an impor- tant one of which is the preference signals ex- tracted from click-through logs. Given a set of triplets {query, title a , title b } discovered from click logs, where the URL title a is preferred to title b for the query. The goal of learning is to produce a matching model between query and title that maximally agrees with the pref- erence triplets. This learnt matching model is combined with other signals, e.g., PageRank, BM25F, etc. in the general ranking. The deep network model for this task is depicted in <ref type="figure">Fig- ure 3(c)</ref>, where each triplet goes through seven layers to compute the loss using Equation <ref type="formula">(1)</ref>, where q i , a i , b i are the output vectors for the query and two titles right before computing the loss. The calculated loss is then back prop- agated to fine tune all the parameters.</p><formula xml:id="formula_2">m i=1 log 1 + exp −c * q T i a i |q i ||a i | − q T i b i |q i ||b i | (1)</formula><p>The evaluation is carried out on a propri- etary data set provided by a leading Chi- nese search engine company.</p><p>It contains 95,640,311 triplets, which involve 14,919,928 distinct queries and 65,125,732 distinct titles. 95,502,506 triplets are used for training, with the rest 137,805 triplets as testing. It is worth noting that the testing triplets are hard cases, mostly involving long queries and short title texts. <ref type="figure">Figure 4</ref> presents the results, where we vary the amount of training data to see how the per- formance varies. The x-axis lists the percent- age of training dataset used, and 100% means using the entire training dataset, and the y- axis is the accuracy of the predicted prefer- ences. We see that word embedding is over- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>This paper presents the first piece of work on embedding radicals for fun and for profit, and we are mostly inspired by fellow researchers delving deeper in various domains ( <ref type="bibr" target="#b13">Zheng et al., 2013;</ref><ref type="bibr" target="#b12">Zhang and LeCun, 2015;</ref><ref type="bibr" target="#b0">Collobert et al., 2011;</ref><ref type="bibr" target="#b5">Kim, 2014;</ref><ref type="bibr" target="#b4">Johnson and Zhang, 2014;</ref><ref type="bibr" target="#b1">dos Santos and Gatti, 2014</ref>). Although working at PinYin level might be a viable approach, using radicals should be more reasonable from a linguistic point of view. Nevertheless, PinYin only represents the pronunciation, which is arguably further away from semantics than radicals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This study presents the first piece of evidence on the feasibility and utility of radical embed- ding for Chinese language processing. It is in- spired by recent success of delving deep in var- ious domains, and roots on the rationale that radicals, as the minimum semantic unit, could be appropriate for deep learning. We demon- strate the utility of radical embedding through two standard in-house and one in-field exper- iments. While some promising results are ob- tained, there are still many problems to be ex- plored further, e.g., how to leverage the lay- out code in radical decomposition that is cur- rently neglected to improve performance. An even more exciting topic could be to train rad- ical, character and word embedding in a uni- fied hierarchical model as they are naturally hierarchical. In sum, we hope this preliminary work could shed some light on new approaches to Chinese language processing and other lan- guages alike.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>For exam- ple, Huang et al.'s work (Huang et al., 2013) on DSSM uses letter trigram as the basic repre- sentation, which somehow resembles radicals. Zhang and Yann's recent work (Zhang and Le- Cun, 2015) represents Chinese at PinYin level, thus taking Chinese as a western language.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Neural Network Components 

− Woman + Man ≈ King" (</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 30K Max-Pooling</head><label>30K</label><figDesc></figDesc><table>Short Text 

ReLU 
256 

Linear 
128 

Softmax 
3 

LossCal 

Input Text 

Lookup Table 30K 

Concatenate 

Tanh 
256 

ReLU 
256 

Softmax 
2 

LossCal 

Label 
3 

Label 
2 

Query 
Title a 
Title b 

Lookup Table 
30K 

Convolution 1×3 Convolution 1×3 Convolution 1×3 

Linear 
100 Linear 
100 Linear 
100 

ReLU 
512 ReLU 
512 ReLU 
512 

ReLU 
512 ReLU 
512 ReLU 
512 

Linear 
256 Linear 
256 Linear 
256 

Max-Pooling 
Max-Pooling 
Max-Pooling 

LossCal 

300 

(a) STC 
(b) CWS 
(c) Search Ranking 

Figure 3: Application Models using Radical Embedding 

Accuracy(%) 
Competing Methods Deep Neural Networks with Embedding 
LR 
SVM 
wrd 
chr 
rdc 
wrd+rdc 
chr+rdc 
Finance 
93.52 
94.06 
94.89 95.85 94.75 
95.70 
95.74 
Sports 
92.40 
92.83 
95.10 
95.01 
92.24 
95.87 
95.91 
Entertainment 
91.72 
92.24 
94.32 
94.77 
93.21 
95.11 
94.78 
Average 
92.55 
93.04 
94.77 
95.21 
93.40 
95.56 
95.46 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 : Short Text Categorization Results</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 : CWS Result Comparison</head><label>3</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> http://en.wikipedia.org/wiki/Wubi_method</note>

			<note place="foot" n="2"> http://crfpp.googlecode.com/svn/trunk/doc/ index.html?source=navbar</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><forename type="middle">P</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for sentiment analysis of short texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cícero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maira</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gatti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="69" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The second international chinese word segmentation bakeoff</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Emerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth SIGHAN workshop on Chinese language Processing</title>
		<meeting>the fourth SIGHAN workshop on Chinese language Processing</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">133</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A neural network for factoid question answering over paragraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><surname>Claudino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="633" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Effective use of word order for text categorization with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rie</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/1412.1058</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Feature-based neural language model and chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mairgup</forename><surname>Mansur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth International Joint Conference on Natural Language Processing</title>
		<meeting><address><addrLine>Nagoya, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1418" />
			<biblScope unit="page" from="1271" to="1277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A latent semantic model with convolutional-pooling structure for information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregoire</forename><surname>Mesnil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management</title>
		<meeting>the 23rd ACM International Conference on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="101" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Going deeper with convolutions. CoRR, abs/1409.4842</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic online news issue construction in web environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoping</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyun</forename><surname>Ru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on World Wide Web</title>
		<meeting>the 17th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="457" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Text understanding from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno>abs/1502.01710</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep learning for chinese word segmentation and POS tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqing</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="647" to="657" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
