<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ConnotationWordNet: Learning Connotation over the Word+Sense Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><forename type="middle">Seok</forename><surname>Kang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stony Brook University Stony Brook</orgName>
								<address>
									<postCode>11794-4400</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stony Brook University Stony Brook</orgName>
								<address>
									<postCode>11794-4400</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stony Brook University Stony Brook</orgName>
								<address>
									<postCode>11794-4400</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stony Brook University Stony Brook</orgName>
								<address>
									<postCode>11794-4400</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ConnotationWordNet: Learning Connotation over the Word+Sense Network</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1544" to="1554"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We introduce ConnotationWordNet, a connotation lexicon over the network of words in conjunction with senses. We formulate the lexicon induction problem as collective inference over pairwise-Markov Random Fields, and present a loopy belief propagation algorithm for inference. The key aspect of our method is that it is the first unified approach that assigns the polarity of both word-and sense-level connotations, exploiting the innate bipar-tite graph structure encoded in WordNet. We present comprehensive evaluation to demonstrate the quality and utility of the resulting lexicon in comparison to existing connotation and sentiment lexicons.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We introduce ConnotationWordNet, a connotation lexicon over the network of words in conjunction with senses, as defined in WordNet. A connotation lexicon, as introduced first by <ref type="bibr" target="#b8">Feng et al. (2011)</ref>, aims to encompass subtle shades of sentiment a word may conjure, even for seemingly objective words such as "sculpture", "Ph.D.", "rosettes". Understanding the rich and complex layers of con- notation remains to be a challenging task. As a starting point, we study a more feasible task of learning the polarity of connotation.</p><p>For non-polysemous words, which constitute a significant portion of English vocabulary, learning the general connotation at the word-level (rather than at the sense-level) would be a natural oper- ational choice. However, for polysemous words, which correspond to most frequently used words, it would be an overly crude assumption that the same connotative polarity should be assigned for all senses of a given word. For example, consider "abound", for which lexicographers of WordNet prescribe two different senses:</p><p>• (v) abound: (be abundant of plentiful; exist in large quantities) • (v) abound, burst, bristle: (be in a state of movement or action) "The room abounded with screaming children"; "The garden bris- tled with toddlers" For the first sense, which is the most commonly used sense for "abound", the general overtone of the connotation would seem positive. That is, al- though one can use this sense in both positive and negative contexts, this sense of "abound" seems to collocate more often with items that are good to be abundant (e.g., "resources"), than unfortunate items being abundant (e.g., "complaints").</p><p>However, as for the second sense, for which "burst" and "bristle" can be used interchangeably with respect to this particular sense, 1 the general overtone is slightly more negative with a touch of unpleasantness, or at least not as positive as that of the first sense. Especially if we look up the Word- Net entry for "bristle", there are noticeably more negatively connotative words involved in its gloss and examples.</p><p>This word sense issue has been a universal chal- lenge for a range of Natural Language Processing applications, including sentiment analysis. Recent studies have shown that it is fruitful to tease out subjectivity and objectivity corresponding to dif- ferent senses of the same word, in order to improve computational approaches to sentiment analysis (e.g. <ref type="bibr" target="#b22">Pestian et al. (2012)</ref>, <ref type="bibr" target="#b16">Mihalcea et al. (2012)</ref>  <ref type="bibr" target="#b4">Balahur et al. (2014)</ref>). Encouraged by these recent successes, in this study, we investigate if we can attain similar gains if we model the connotative polarity of senses separately.</p><p>There is one potential practical issue we would like to point out in building a sense-level lexical resource, however. End-users of such a lexicon may not wish to deal with Word Sense Disam-biguation (WSD), which is known to be often too noisy to be incorporated into the pipeline with re- spect to other NLP tasks. As a result, researchers often would need to aggregate labels across differ- ent senses to derive the word-level label. Although such aggregation is not entirely unreasonable, it does not seem to be the most optimal and princi- pled way of integrating available resources.</p><p>Therefore, in this work, we present the first uni- fied approach that learns both sense-and word- level connotations simultaneously. This way, end- users will have access to more accurate sense-level connotation labels if needed, while also having ac- cess to more general word-level connotation la- bels. We formulate the lexicon induction problem as collective inference over pairwise-Markov Ran- dom Fields (pairwise-MRF) and derive a loopy be- lief propagation algorithm for inference.</p><p>The key aspect of our approach is that we ex- ploit the innate bipartite graph structure between words and senses encoded in WordNet. Although our approach seems conceptually natural, previous approaches, to our best knowledge, have not di- rectly exploited these relations between words and senses for the purpose of deriving lexical knowl- edge over words and senses collectively. In ad- dition, previous studies (for both sentiment and connotation lexicons) aimed to produce only ei- ther of the two aspects of the polarity: word-level or sense-level, while we address both.</p><p>Another contribution of our work is the intro- duction of loopy belief propagation (loopy-BP) as a lexicon induction algorithm. Loopy-BP in our study achieves statistically significantly better performance over the constraint optimization ap- proaches previously explored. In addition, it runs much faster and it is considerably easier to imple- ment. Last but not least, by using probabilistic rep- resentation of pairwise-MRF in conjunction with Loopy-BP as inference, the resulting solution has the natural interpretation as the intensity of con- notation. This contrasts to approaches that seek discrete solutions such as Integer Linear Program- ming( <ref type="bibr" target="#b20">Papadimitriou and Steiglitz, 1998)</ref>.</p><p>ConnotationWordNet, the final outcome of our study, is a new lexical resource that has conno- tation labels over both words and senses follow- ing the structure of WordNet. The lexicon is pub- licly available at: http://www.cs.sunysb. edu/ ˜ junkang/connotation_wordnet.)</p><p>In what follows, we will first describe the net- Figure 1: G WORD+SENSE with words and senses.</p><p>work of words and senses (Section 2), then intro- duce the representation of the network structure as pairwise Markov Random Fields, and a loopy be- lief propagation algorithm as collective inference (Section 3). We then present comprehensive eval- uation (Section 4 &amp; 5 &amp; 6), followed by related work (Section 7) and conclusion (Section 8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Network of Words and Senses</head><p>The connotation graph, called G WORD+SENSE , is a heterogeneous graph with multiple types of nodes and edges. As shown in <ref type="figure">Figure 1</ref>, it contains two types of nodes; (i) lemmas (i.e., words, 115K) and (ii) synsets (63K), and four types of edges; (t 1 ) predicate-argument (179K), (t 2 ) argument- argument (144K), (t 3 ) argument-synset (126K), and (t 4 ) synset-synset (3.4K) edges. The predicate-argument edges, first introduced by <ref type="bibr" target="#b8">Feng et al. (2011)</ref>, depict the selectional prefer- ence of connotative predicates (i.e., the polarity of a predicate indicates the polarity of its arguments) and encode their co-occurrence relations based on the Google Web 1T corpus. The argument- argument edges are based on the distributional similarities among the arguments. The argument- synset edges capture the synonymy between argu- ment nodes through the corresponding synsets. Fi- nally, the synset-synset edges depict the antonym relations between synset pairs.</p><p>In general, our graph construction is similar to that of <ref type="bibr" target="#b9">Feng et al. (2013)</ref>, but there are a few im- portant differences. Most notably, we model both words and synsets explicitly, and exploit the mem- bership relations between words and senses. We expect that edges between words and senses will encourage senses that belong to the same word to receive the same connotation label. Conversely, we expect that these edges will also encourage words that belong to the same sense (i.e., synset definition) to receive the same connotation label.</p><p>Another benefit of our approach is that for var- ious WordNet relations (e.g., antonym relations), which are defined over synsets (not over words), we can add edges directly between corresponding synsets, rather than projecting (i.e., approximat- ing) those relations over words. Note that the lat- ter, which has been employed by several previous studies (e.g., <ref type="bibr" target="#b12">Kamps et al. (2004)</ref>, <ref type="bibr" target="#b29">Takamura et al. (2005)</ref>, <ref type="bibr" target="#b2">Andreevskaia and Bergler (2006)</ref>, <ref type="bibr" target="#b28">Su and Markert (2009)</ref>, <ref type="bibr" target="#b14">Lu et al. (2011)</ref>, <ref type="bibr" target="#b11">Kaji and Kitsuregawa (2007)</ref>, <ref type="bibr" target="#b9">Feng et al. (2013)</ref>), could be a source of noise, as one needs to assume that the semantic relation between a pair of synsets trans- fers over the pair of words corresponding to that pair of synsets. For polysemous words, this as- sumption may be overly strong.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Pairwise Markov Random Fields and Loopy Belief Propagation</head><p>We formulate the task of learning sense-and word- level connotation lexicon as a graph-based clas- sification task <ref type="bibr" target="#b25">(Sen et al., 2008</ref>). More formally, we denote the connotation graph G WORD+SENSE by G = (V, E), in which a total of n word and synset nodes V = {v 1 , . . . , v n } are connected with typed edges e(v i , v j , t) ∈ E, where edge types t ∈ {pred-arg, arg-arg, syn-arg, syn-syn} de- pict the four edge types as described in Section 2. A neighborhood function N , where N v = {u| e(u, v) ∈ E} ⊆ V , describes the underlying network structure. In our collective classification formulation, each node in V is represented as a random variable that takes a value from an appropriate class label do- main; in our case, L = {+, −} for positive and negative connotation. In this classification task, we denote by Y the nodes the labels of which need to be assigned, and let y i refer to Y i 's label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pairwise Markov Random Fields</head><p>We next define our objective function. We pro- pose to use an objective formulation that utilizes pairwise Markov Random Fields (MRFs) <ref type="bibr" target="#b13">(Kindermann and Snell, 1980)</ref>, which we adapt to our problem setting. MRFs are a class of probabilistic graphical models that are suited for solving infer- ence problems in networked data. An MRF con- sists of an undirected graph where each node can be in any of a finite number of states (i.e., class labels). The state of a node is assumed to be de- pendent on each of its neighbors and independent of other nodes in the graph. <ref type="bibr">2</ref> In pairwise MRFs, the joint probability of the graph can be written as a product of pairwise factors, parameterized over the edges. These factors are referred to as clique potentials in general MRFs, which are essentially functions that collectively determine the graph's joint probability.</p><p>Specifically, let G = (V, E) denote a network of random variables, where V consists of the un- observed variables Y that need to be assigned val- ues from label set L. Let Ψ denote a set of clique potentials that consists of two types of factors:</p><formula xml:id="formula_0">• For each Y i ∈ Y, ψ i ∈ Ψ is a prior map- ping ψ i : L → R ≥0 , where R ≥0 denotes non- negative real numbers. • For each e(Y i , Y j , t) ∈ E, ψ t ij ∈ Ψ is a com- patibility mapping ψ t ij : L × L → R ≥0 .</formula><p>Objective formulation Given an assignment y to all the unobserved variables Y and x to ob- served ones X (variables with known labels, if any), our objective function is associated with the following joint probability distribution</p><formula xml:id="formula_1">P (y|x) = 1 Z(x) Y i ∈Y ψ i (y i ) e(Y i ,Y j ,t)∈E ψ t ij (y i , y j )<label>(1)</label></formula><p>where Z(x) is the normalization function. Our goal is then to infer the maximum likelihood as- signment of states (i.e., labels) to unobserved vari- ables (i.e., nodes) that will maximize Equation (1).</p><p>Problem Definition Having introduced our graph-based classification task and objective for- mulation, we define our problem more formally. Given -a connotation graph G = (V, E) of words and synsets connected with typed edges, -prior knowledge (i.e., probabilities) of (some or all) nodes belonging to each class, -compatibility of two nodes with a given pair of labels being connected to each other; Classify the nodes Y i ∈ Y, into one of two classes; L = {+, −}, such that the class assignments y i maximize our objective in Equation (1).</p><p>We can further rank the network objects by the probability of their connotation polarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Loopy Belief Propagation</head><p>Finding the best assignments to unobserved vari- ables in our objective function is the inference problem. The brute force approach through enu- meration of all possible assignments is exponen- tial and thus intractable. In general, exact in- ference is known to be NP-hard and there is no known algorithm which can be theoretically shown to solve the inference problem for gen- eral MRFs. Therefore in this work, we em- ploy a computationally tractable (in fact linearly scalable with network size) approximate infer- ence algorithm called Loopy Belief Propagation (LBP) ( <ref type="bibr" target="#b36">Yedidia et al., 2003)</ref>, which we extend to handle typed graphs like our connotation graph.</p><p>Our inference algorithm is based on iterative message passing and the core of it can be concisely expressed as the following two equations:</p><formula xml:id="formula_2">m i→j (y j ) = α y i ∈L ψ t ij (y i , y j ) ψ i (y i ) Y k ∈N i ∩Y\Y j m k→i (y i ) , ∀y j ∈ L (2) b i (y i ) = β ψ i (y i ) Y j ∈N i ∩Y m j→i (y i ), ∀y i ∈ L<label>(3)</label></formula><p>A message m i→j is sent from node i to node j and captures the belief of i about j, which is the probability distribution over the labels of j; i.e. what i "thinks" j's label is, given the current la- bel of i and the type of the edge that connects i and j. Beliefs refer to marginal probability dis- tributions of nodes over labels; for example b i (y i ) denotes the belief of node i having label y i . α and β are the normalization constants, which respec- tively ensure that each message and each set of marginal probabilities sum to 1. At every iteration, each node computes its belief based on messages received from its neighbors, and uses the compat- ibility mapping to transform its belief into mes- sages for its neighbors. The key idea is that after enough iterations of message passes between the nodes, the "conversations" are likely to come to a consensus, which determines the marginal proba- bilities of all the unknown variables.</p><p>The pseudo-code of our method is given in Al- gorithm 1. It first initializes all messages to 1 and priors to unbiased (i.e., equal) probabilities for all nodes except the seed nodes for which the sentiment is known (lines 3-9). It then proceeds by making each Y i ∈ Y communicate messages Algorithm 1: CONNOTATION INFERENCE 1 Input: Connotation graph G=(V, E), prior potentials ψ s for seed words s ∈ S, and compatibility potentials ψ t ij 2 Output: Connotation label probabilities for each node i ∈ V \P</p><formula xml:id="formula_3">3 foreach e(Y i , Y j , t) ∈ E do // initialize msg.s 4 foreach y j ∈ L do 5 m i→j (y j ) ← 1 6 foreach i ∈ V do // initialize priors 7 foreach y j ∈ L do 8 if i ∈ S then φ i (y j ) ← ψ i (y j ) else φ i (y j ) ← 1/|L| 9 repeat // iterative message passing 10 foreach e(Y i , Y j , t) ∈ E, Y j ∈ Y V \S do 11 foreach y j ∈ L do 12</formula><p>Use Equation <ref type="formula">(2)</ref> 13 until all messages stop changing</p><formula xml:id="formula_4">14 foreach Y i ∈ Y V \S do // compute final beliefs 15 foreach y i ∈ L do 16</formula><p>Use Equation <ref type="formula" target="#formula_2">(3)</ref> with their neighbors in an iterative fashion until the messages stabilize (lines 10-14), i.e. conver- gence is reached. 3 At convergence, we calculate the marginal probabilities, that is of assigning Y i with label y i , by computing the final beliefs b i (y i ) (lines 15-17). We use these maximum likelihood probabilities for label assignment; for each node i, we assign the label L i ← max y i b i (y i ).</p><p>To completely define our algorithm, we need to instantiate the potentials Ψ, in particular the priors and the compatibilities, which we discuss next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Priors</head><p>The prior beliefs ψ i of nodes can be suit- ably initialized if there is any prior knowledge for their connotation sentiment (e.g., enjoy is posi- tive, suffer is negative). As such, our method is flexible to integrate available side information. In case there is no prior knowledge available, each node is initialized equally likely to have any of the possible labels, i.e., <ref type="bibr">1</ref> |L| as in Algorithm 1 (line 9). Compatibilities The compatibility potentials can be thought of as matrices, with entries ψ t ij (y i , y j ) that give the likelihood of a node hav- ing label y i , given that it has a neighbor with label y j to which it is connected through a type t edge.</p><p>A key difference of our method from earlier mod- els is that we use clique potentials that differ for edge types, since the connotation graph is hetero- geneous. This is exactly because the compatibil- ity of class labels of two adjacent nodes depends on the type of the edge connecting them: e.g., A sample instantiation of the compatibilities is shown in <ref type="table">Table 1</ref>. Notice that the potentials for pred-arg, arg-arg, and syn-arg capture ho- mophily, i.e., nodes with the same label are likely to connect to each other through these types of edges. <ref type="bibr">4</ref> On the other hand, syn-syn edges con- nect nodes that are antonyms of each other, and thus the compatibilities capture the reverse rela- tionship among their labels. <ref type="table">Table 1</ref>: Instantiation of compatibility potentials. Entry ψ t ij (y i , y j ) is the compatibility of a node with label y i having a neighbor labeled y j , given the edge between i and j is type t, for small .</p><formula xml:id="formula_5">t: t1 A P + − + 1- − 1- t: t2 A A + − + 1-2 2 − 2 1-2 (t1) pred-arg (t2) arg-arg t: t3 A S + − + 1- − 1- t: t4 S S + − + 1- − 1- (t3) syn-arg (t4) syn-syn (synonym relations) (antonym relations)</formula><p>Complexity analysis Most demanding compo- nent of Algorithm 1 is the iterative message pass- ing over the edges (lines 10-14), with time com- plexity O(ml 2 r), where m = |E| is the num- ber of edges in the connotation graph, l = |L|, the classes, and r, the iterations until convergence. Often, l is quite small (in our case, l = 2) and r m. Thus running time grows linearly with the number of edges and is scalable to large datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation I: Agreement with Sentiment Lexicons</head><p>ConnotationWordNet is expected to be the super- set of a sentiment lexicon, as it is highly likely for any word with positive/negative sentiment to carry connotation of the same polarity. Thus, we use two conventional sentiment lexicons, General In- quirer (GENINQ) ( <ref type="bibr" target="#b26">Stone et al., 1966</ref>) and MPQA ( <ref type="bibr" target="#b35">Wilson et al., 2005b)</ref>, as surrogates to measure the performance of our inference algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Variants of Graph Construction</head><p>The construction of the connotation graph, de- noted by G WORD+SENSE , which includes words and synsets, has been described in Section 2. In ad- dition to this graph, we tried several other graph constructions, the first three of which have previ- ously been used in <ref type="bibr" target="#b9">(Feng et al., 2013</ref>). We briefly describe these graphs below, and compare perfor- mance on all the graphs in the proceeding.</p><p>G WORD W/ PRED-ARG: This is a (bipartite) subgraph of G WORD+SENSE , which only includes the connotative predicates and their arguments. As such, it contains only type t 1 edges. The edges between the predicates and the arguments can be weighted by their Point-wise Mutual Information (PMI) 5 based on the Google Web 1T corpus.</p><p>G WORD W/ OVERLAY: The second graph is also a proper subgraph of G WORD+SENSE , which in- cludes the predicates and all the argument words. Predicate words are connected to their arguments as before. In addition, argument pairs (a 1 , a 2 ) are connected if they occurred together in the "a 1 and a 2 " or "a 2 and a 1 " coordination ( <ref type="bibr" target="#b10">Hatzivassiloglou and McKeown, 1997;</ref><ref type="bibr" target="#b23">Pickering and Branigan, 1998)</ref>. This graph contains both type t 1 and t 2 edges. The edges can also be weighted based on the distributional similarities of the word pairs.</p><formula xml:id="formula_6">G WORD :</formula><p>The third graph is a super-graph of G WORD W/ OVERLAY, with additional edges, where argument pairs in synonym and antonym relation are connected to each other. Note that un- like the connotation graph G WORD+SENSE , it does not contain any synset nodes. Rather, the words that are synonyms or antonyms of each other are directly linked in the graph. As such, this graph contains all edge types t 1 through t 4 .</p><p>G WORD+SENSE W/ SYNSIM: This is a super- graph of our original G WORD+SENSE graph; that is, it has all the predicate, arguments, and synset nodes, as well as the four types of edges between them. In addition, we add edges of a fifth type t 5 between the synset nodes to capture their similar- ity. To define similarity, we use the glossary def- initions of the synsets and derive three different scores. Each score utilizes the count(s 1 , s 2 ) of overlapping nouns, verbs, and adjectives/adverbs among the glosses of the two synsets s 1 and s 2 . G WORD+SENSE W/ SYNSIM1: We discard edges with count less than 3. The weighted version has the counts normalized between 0 and 1.</p><p>G WORD+SENSE W/ SYNSIM2: We normalize the counts by the length of the gloss (the avg of two lengths), that is, p = count / avg(len gloss(s 1 ), len gloss(s 2 )) and discard edges with p &lt; 0.5. The weighted version contains p values as edge weights.</p><p>G WORD+SENSE W/ SYNSIM3: To further sparsify the graph we discard edges with p &lt; 0.6. To weigh the edges, we use the cosine similarity be- tween the gloss vectors of the synsets based on the TF-IDF values of the words the glosses contain.</p><p>Note that the connotation inference algorithm, as given in Algorithm 1, remains exactly the same for all the graphs described above. The only dif- ference is the set of parameters used; while G WORD W/ PRED-ARG and G WORD W/ OVERLAY contain one and two edge types, respectively and only use compatibilities (t 1 ) and (t 2 ), G WORD uses all four as given in <ref type="table">Table 1</ref>. The G WORD+SENSE W/ SYN- SIM graphs use an additional compatibility matrix for the synset similarity edges of type t 5 , which is the same as the one used for t 1 , i.e., similar synsets are likely to have the same connotation label. This flexibility is one of the key advantages of our al- gorithm as new types of nodes and edges can be added to the graph seamlessly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Sentiment-Lexicon based Performance</head><p>In this section, we first compare the performance of our connotation graph G WORD+SENSE to graphs that do not include synset nodes but only words. Then we analyze the performance when the addi- tional synset similarity edges are added. First, we briefly describe our performance measures.</p><p>The sentiment lexicons we use as gold standard are small, compared to the size (i.e., number of words) our graphs contain. Thus, we first find the overlap between each graph and a senti-  <ref type="table">Table 2</ref>: Connotation inference performance on various graphs. '-W' indicates weighted versions (see §4.1). P: precision, R: recall, F: F1-score (%). ment lexicon. Note that the overlap size may be smaller than the lexicon size, as some sen- timent words may be missing from our graphs. Then, we calculate the number of correct la- bel assignments. As such, precision is defined as (correct / overlap), and recall as (correct / lexicon size). Finally, F1-score is their har- monic mean and reflects the overall accuracy. As shown in <ref type="table">Table 2</ref> (top), we first observe that including the synonym and antonym relations in the graph, as with G WORD and G WORD+SENSE , im- prove the performance significantly, almost by an order of magnitude, over graphs G WORD W/ PRED- ARG and G WORD W/ OVERLAY that do not contain those relation types. Furthermore, we notice that the performances on the G WORD+SENSE graph are better than those on the word-only graphs. This shows that including the synset nodes explicitly in the graph structure is beneficial. What is more, it gives us a means to obtain connotation labels for the synsets themselves, which we use in the evaluations in the next sections. Finally, we note that using the unweighted versions of the graphs provide relatively more robust performance, po- tentially due to noise in the relative edge weights.</p><p>Next we analyze the performance when the new edges between synsets are introduced, as given in <ref type="table">Table 2</ref> (bottom). We observe that connecting the synset nodes by their gloss-similarity (at least in the ways we tried) does not yield better perfor- mance than on our original G WORD+SENSE graph. Different from earlier, the weighted versions of the similarity based graphs provide better perfor-mance than their unweighted counterparts. This suggests that glossary similarity would be a more robust means to correlate nodes; we leave it as fu- ture work to explore this direction for predicate- argument and argument-argument relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Parameter Sensitivity</head><p>Our belief propagation based connotation senti- ment inference algorithm has one user-specified parameter (see <ref type="table">Table 1</ref>). To study the sensitivity of its performance to the choice of , we reran our experiments for = {0.02, 0.04, . . . , 0.24} 6 and report the accuracy results on our G WORD+SENSE in <ref type="figure" target="#fig_1">Figure 2</ref> for the two lexicons. The results indicate that the performances remain quite stable across a wide range of the parameter choice. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation II: Human Evaluation on ConnotationWordNet</head><p>In this section, we present the result of human evaluation we executed using Amazon Mechani- cal Turk (AMT). We collect two separate sets of labels: a set of labels at the word-level, and an- other set at the sense-level. We first describe the labeling process of sense-level connotation: We selected 350 polysemous words and one of their senses, and each Turker was asked to rate the con- notative polarity of a given word (or of a given sense), from -5 to 5, 0 being the neutral. <ref type="bibr">7</ref> For each word, we asked 5 Turkers to rate and we took the average of the 5 ratings as the connotative inten- sity score of the word. We labeled a word as nega- tive if its intensity score is less than 0 and positive otherwise. For word-level labels we apply similar procedure as above. <ref type="bibr">6</ref> Note that for &gt; 0.25, compatibilities of ψ t 2 in <ref type="table">Table 1</ref> are reversed, hence the maximum of 0.24. <ref type="bibr">7</ref> Because senses in WordNet can be tricky to understand, care should be taken in designing the task so that the Turkers will focus only on the corresponding sense of a word. There- fore, we provided the part of speech tag, the WordNet gloss of the selected sense, and a few examples as given in Word- Net. As an incentive, each Turker was rewarded $0.07 per hit which consists of 10 words to label.   <ref type="table">Table 3</ref>: Word-/Sense-level evaluation results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Word-Level Evaluation</head><p>We first evaluate the word-level assignment of connotation, as shown in <ref type="table">Table 3</ref>. The agreement between the new lexicon and human judges varies between 84% and 86.98%. Sentiment lexicons such as SentiWordNet ( <ref type="bibr" target="#b3">Baccianella et al. (2010)</ref>) and OpinionFinder ( <ref type="bibr" target="#b34">Wilson et al. (2005a)</ref>) show low agreement rate with human, which is some- what as expected: human judges in this study are labeling for subtle connotation, not for more ex- plicit sentiment. OpinionFinder's low agreement rate was mainly due to the low hit rate of the words (successful look-up rate, 33.43%). Feng2013 is the lexicon presented in <ref type="bibr" target="#b9">(Feng et al., 2013</ref>) and it showed a relatively higher 72.13% hit rate. Note that belief propagation was run until 95% and 99% of the nodes were converged in their beliefs. In addition, the seed words with known connotation labels originally consist of 20 positive and 20 negative predicates. We also extended the seed set with the sentiment lexicon words and de- note these runs with E-for 'Extended'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Sense-Level Evaluation</head><p>We also examined the agreement rates on the sense-level. Since OpinionFinder and Feng2013 do not provide the polarity scores at the sense- level, we excluded them from this evaluation. Be- cause sense-level polarity assignment is a harder (more subtle) task, the performance of all lexicons decreased to some degree in comparison to that of word-level evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Pair-wise Intensity Ranking</head><p>A notable goodness of our induction algorithm is that the outcome of the algorithm can be inter- preted as an intensity of the corresponding conno- tation. But are these values meaningful? We an- swer this question in this section. We formulate a pair-wise ranking task as a binary decision task as follows: given a pair of words, we ask which one is more positive (or more negative) than the other. Since we collect human labels based on scales, we  <ref type="table">Table 4</ref>: Results of pair-wise intensity evaluation, for intensity difference threshold = 2.0 already have this information at hand. Because different human judges have different notion of scales however, subtle differences are more likely to be noisy. Therefore, we experiment with vary- ing degrees of differences in their scales, as shown in <ref type="figure" target="#fig_3">Figure 3</ref>. Threshold values (ranging from 0.5 to 3.0) indicate the minimum differences in scales for any pair of words, for the pair to be included in the test set. As expected, we observe that the perfor- mance improves as we increase the threshold (as pairs get better separated). Within range [0.5, 1.5] (249 pairs examined), the accuracies are as high as 68.27%, which shows that even the subtle differ- ences of the connotative intensities are relatively well reflected in the new lexicons.  <ref type="table">Table 4</ref>. Despite that intensity is generally a harder prop- erty to measure (than the coarser binary catego- rization of polarities), our connotation lexicons perform surprisingly well, reaching up to 74.83% accuracy. Further study on the incorrect cases re- veals that SentiWordNet has many pair of words with the same polarity score (23.34%). Such cases seems to be due to the limited score patterns of SentiWordNet. The ratio of such cases are ac- counted as Undecided in <ref type="table">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation III: Sentiment Analysis using ConnotationWordNet</head><p>Finally, to show the utility of the resulting lexi- con in the context of a concrete sentiment analysis task, we perform lexicon-based sentiment analy- sis. We experiment with SemEval dataset <ref type="bibr" target="#b27">(Strapparava and Mihalcea, 2007</ref>) that includes the hu- man labeled dataset for predicting whether a news headline is a good news or a bad news, which we expect to have a correlation with the use of con- notative words that we focus on in this paper. The good/bad news are annotated with scores (ranging from -100 to 87). We construct several data sets by applying different thresholds on scores. For exam- ple, with the threshold set to 60, we discard the in- stances whose scores lie between -60 and 60. For comparison, we also test the connotation lexicon from <ref type="bibr" target="#b9">(Feng et al., 2013</ref>) and the combined senti- ment lexicon GENINQ+MPQA. Note that there is a difference in how humans judge the orientation and the degree of connota- tion for a given word out of context, and how the use of such words in context can be perceived as good/bad news. In particular, we conjecture that humans may have a bias toward the use of posi- tive words, which in turn requires calibration from the readers' minds <ref type="bibr" target="#b21">(Pennebaker and Stone, 2003)</ref>. That is, we might need to tone down the level of positiveness in order to correctly measure the ac- tual intended positiveness of the message.</p><p>With this in mind, we tune the appropriate cali- bration from a small training data, by using 1 fold from N fold cross validation, and using the re- maining N − 1 folds as testing. We simply learn the mixture coefficient λ to scale the contribution of positive and negative connotation values. We tune this parameter λ 8 for other lexicons we com- pare against as well. Note that due to this param- eter learning, we are able to report better perfor- mance for the connotation lexicon of <ref type="bibr" target="#b9">(Feng et al., 2013)</ref> than what the authors have reported in their paper (labeled with *) in <ref type="table" target="#tab_5">Table 5</ref>. <ref type="table" target="#tab_5">Table 5</ref> shows the results for N =15, where the new lexicon consistently outperforms other com- petitive lexicons. In addition, <ref type="figure" target="#fig_4">Figure 4</ref> shows that the performance does not change much based on the size of training data used for parameter tuning (N ={5, 10, 15, 20}).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Several previous approaches explored the use of graph propagation for sentiment lexicon induction ( <ref type="bibr" target="#b31">Velikovich et al., 2010)</ref>    The key difference, however, is that in our MRF representation, we can explicitly model various types of word-word, sense-sense and word-sense relations as edge po- tentials. In particular, we can naturally encode re- lations that encourage the same assignment (e.g., synonym) as well as the opposite assignment (e.g., antonym) of the polarity labels. Note that integra- tion of the latter is not straightforward in the graph propagation framework. There have been a number of previous studies that aim to construct a word-level sentiment lex- icon ( <ref type="bibr" target="#b24">Qiu et al., 2009</ref>) and a sense-level sentiment lexicon <ref type="bibr" target="#b7">(Esuli and Sebastiani, 2006</ref>). But none of these approaches con- sidered to induce the polarity labels at both the word-level and sense-level. Although we focus on learning connotative polarity of words and senses in this paper, the same approach would be applica- ble to constructing a sentiment lexicon as well.</p><p>There have been recent studies that address word sense disambiguation issues for sentiment analysis. <ref type="bibr">SentiWordNet (Esuli and Sebastiani, 2006</ref>) was the very first lexicon developed for sense-level labels of sentiment polarity. In recent years, <ref type="bibr" target="#b0">Akkaya et al. (2009)</ref> report a successful em- pirical result where WSD helps improving senti- ment analysis, while <ref type="bibr" target="#b32">Wiebe and Mihalcea (2006)</ref> study the distinction between objectivity and sub- jectivity in each different sense of a word, and their empirical effects in the context of sentiment analysis. Our work shares the high-level spirit of accessing the sense-level polarity, while also de- riving the word-level polarity.</p><p>In recent years, there has been a growing re- search interest in investigating more fine-grained aspects of lexical sentiment beyond positive and negative sentiment. For example, Mohammad and Turney (2010) study the affects words can evoke in people's minds, while Bollen et al. (2011) study various moods, e.g., "tension", "depression", be- yond simple dichotomy of positive and negative sentiment. Our work, and some recent work by <ref type="bibr" target="#b8">Feng et al. (2011) and</ref><ref type="bibr" target="#b9">Feng et al. (2013)</ref> share this spirit by targeting more subtle, nuanced sentiment even from those words that would be considered as objective in early studies of sentiment analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We have introduced a novel formulation of lexicon induction operating over both words and senses, by exploiting the innate structure between the words and senses as encoded in WordNet. In addi- tion, we introduce the use of loopy belief propaga- tion over pairwise-Markov Random Fields as an effective lexicon induction algorithm. A notable strength of our approach is its expressiveness: var- ious types of prior knowledge and lexical relations can be encoded as node potentials and edge po- tentials. In addition, it leads to a lexicon of bet- ter quality while also offering faster run-time and easiness of implementation. The resulting lexi- con, called ConnotationWordNet, is the first lex- icon that has polarity labels over both words and senses. ConnotationWordNet is publicly available for research and practical use.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>+ syn-arg − −−−− → + is highly compatible, whereas + syn-syn − −−−− → + is unlikely; as syn-arg edges capture synonymy; i.e., words-sense memberships, while syn-syn edges depict antonym relations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Performance is stable across various .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Lexicon</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Trend of accuracy for pair-wise intensity evaluation over threshold The results for pair-wise intensity evaluation (threshold=2.0, 1,208 pairs) are given in Table 4. Despite that intensity is generally a harder property to measure (than the coarser binary categorization of polarities), our connotation lexicons perform surprisingly well, reaching up to 74.83% accuracy. Further study on the incorrect cases reveals that SentiWordNet has many pair of words with the same polarity score (23.34%). Such cases seems to be due to the limited score patterns of SentiWordNet. The ratio of such cases are accounted as Undecided in Table 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Trend of SemEval performance over N , the number of CV folds induction (Feng et al., 2013). Our work introduces the use of loopy belief propagation over pairwise-MRF as an alternative solution to these tasks. At a high-level, both approaches share the general idea of propagating confidence or belief over the graph connectivity. The key difference, however, is that in our MRF representation, we can explicitly model various types of word-word, sense-sense and word-sense relations as edge potentials. In particular, we can naturally encode relations that encourage the same assignment (e.g., synonym) as well as the opposite assignment (e.g., antonym) of the polarity labels. Note that integration of the latter is not straightforward in the graph propagation framework. There have been a number of previous studies that aim to construct a word-level sentiment lexicon (Wiebe et al., 2005; Qiu et al., 2009) and a sense-level sentiment lexicon (Esuli and Sebastiani, 2006). But none of these approaches considered to induce the polarity labels at both the word-level and sense-level. Although we focus on learning connotative polarity of words and senses in this paper, the same approach would be applicable to constructing a sentiment lexicon as well. There have been recent studies that address word sense disambiguation issues for sentiment analysis. SentiWordNet (Esuli and Sebastiani, 2006) was the very first lexicon developed for</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>and connotation lexicon</figDesc><table>Lexicon 

SemEval Threshold 

20 
40 
60 
80 

Instance Size 
955 
649 
341 
86 
Feng2013 71.5 77.1 81.6 90.5 
GENINQ+MPQA 72.8 77.2 80.4 86.7 
G WORD+SENSE (95%) 74.5 79.4 86.5 91.9 
G WORD+SENSE (99%) 74.6 79.4 86.8 91.9 

E-G 

WORD+SENSE (95%) 72.5 76.8 82.3 87.2 

E-G 

WORD+SENSE (99%) 72.6 76.9 82.5 87.2 
Feng2013* 70.8 74.6 80.8 93.5 
GENINQ+MPQA* 64.5 69.0 74.0 80.5 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>SemEval evaluation results, for N =15 

</table></figure>

			<note place="foot" n="1"> Hence a sense in WordNet is defined by synset (= synonym set), which is the set of words sharing the same sense.</note>

			<note place="foot" n="2"> This assumption yields a pairwise Markov Random Field (MRF); a special case of general MRFs (Yedidia et al., 2003).</note>

			<note place="foot" n="3"> Although convergence is not theoretically guaranteed, in practice LBP converges to beliefs within a small threshold of change (e.g., 10 −6 ) fairly quickly with accurate results (Pandit et al., 2007; McGlohon et al., 2009; Akoglu et al., 2013).</note>

			<note place="foot" n="4"> arg-arg edges are based on co-occurrence (see Section 2), which does not carry as strong indication of the same connotation as e.g., synonymy. Thus, we enforce less homophily for nodes connected through edges of arg-arg type.</note>

			<note place="foot" n="5"> PMI scores are widely used in previous studies to measure association between words (e.g., (Church and Hanks, 1990), (Turney, 2001), (Newman et al., 2009)).</note>

			<note place="foot" n="8"> What is reported is based on λ ∈ {20, 40, 60, 80}. More detailed parameter search does not change the results much.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported by the Army Re-search Office under Contract No. W911NF-14-1-0029, Stony Brook University Office of Vice Pres-ident for Research, and gifts from Northrop Grum-man Aerospace Systems and Google. We thank reviewers for many insightful comments and sug-gestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Subjectivity word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cem</forename><surname>Akkaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="190" to="199" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Opinion fraud detection in online reviews by network effects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishi</forename><surname>Chandy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mining wordnet for a fuzzy sentiment: Sentiment tag extraction from wordnet glosses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Andreevskaia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Bergler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="209" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Baccianella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="2200" to="2204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Computational approaches to subjectivity and sentiment analysis: Present and envisaged methods and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Balahur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrés</forename><surname>Montoyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Modeling public mood and emotion: Twitter sentiment and socio-economic phenomena</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bollen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huina</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Pepe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICWSM</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Word association norms, mutual information, and lexicography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="22" to="29" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sentiwordnet: A publicly available lexical resource for opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Conference on Language Resources and Evaluation</title>
		<meeting>the 5th Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">06</biblScope>
			<biblScope unit="page" from="417" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning general connotation of words using graph-based algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ritwik</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1092" to="1103" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Connotation lexicon: A dash of sentiment beneath the surface meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><forename type="middle">Seok</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Polina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Association for Computer Linguistics</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1774" to="1784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Predicting the semantic orientation of adjectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Hatzivassiloglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint ACL/EACL Conference</title>
		<meeting>the Joint ACL/EACL Conference</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="174" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Building lexicon for sentiment analysis from massive collection of html documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nobuhiro</forename><surname>Kaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaru</forename><surname>Kitsuregawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1075" to="1083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Using wordnet to measure semantic orientations of adjectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Robert J Mokken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De Rijke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Kindermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Snell</surname></persName>
		</author>
		<title level="m">Markov Random Fields and Their Applications</title>
		<imprint>
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automatic construction of a context-aware sentiment lexicon: an optimization approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malu</forename><surname>Castellanos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umeshwar</forename><surname>Dayal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on World wide web</title>
		<meeting>the 20th international conference on World wide web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="347" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Snare: a link analytic system for graph labeling and risk detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><surname>Mcglohon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><forename type="middle">G</forename><surname>Anderle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Steier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
		<editor>John F. Elder IV, Franoise Fogelman-Souli, Peter A. Flach, and Mohammed Zaki</editor>
		<imprint>
			<date type="published" when="2009" />
			<publisher>ACM</publisher>
			<biblScope unit="page" from="1265" to="1274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multilingual subjectivity and sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tutorial Abstracts of ACL 2012</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="4" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Emotions evoked by common words and phrases: Using mechanical turk to create an emotion lexicon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saif</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text</title>
		<meeting>the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text<address><addrLine>Los Angeles, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-06" />
			<biblScope unit="page" from="26" to="34" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">External evaluation of topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarvnaz</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Cavedon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Australasian Document Computing Symposium</title>
		<meeting><address><addrLine>Sydney</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-12" />
			<biblScope unit="page" from="11" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Netprobe: a fast and scalable system for fraud detection in online auction networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashank</forename><surname>Pandit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horng</forename><surname>Duen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="201" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Combinatorial optimization: algorithms and complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Christos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Papadimitriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steiglitz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Courier Dover Publications</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Words of wisdom: language use over the life span</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lori</forename><forename type="middle">D</forename><surname>Pennebaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of personality and social psychology</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">291</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sentiment analysis of suicide notes: A shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pawel</forename><surname>John P Pestian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><surname>Matykiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brett</forename><surname>Linngust</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozlem</forename><surname>South</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Uzuner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Hurdle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomedical Informatics Insights</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Suppl. 1</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The representation of verbs: Evidence from syntactic priming in language production</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holly</forename><forename type="middle">P</forename><surname>Pickering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Branigan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="633" to="651" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Expanding domain sentiment lexicon through double propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1199" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="106" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The General Inquirer: A Computer Approach to Content Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dexter</forename><forename type="middle">C</forename><surname>Dunphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marshall</forename><forename type="middle">S</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ogilvie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1966" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semeval2007 task 14: Affective text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Strapparava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Workshop on Semantic Evaluations</title>
		<meeting>the 4th International Workshop on Semantic Evaluations</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="70" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Subjectivity recognition on word senses via semi-supervised mincuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangzhong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Markert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Extracting semantic orientations of words using spin model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroya</forename><surname>Takamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Inui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="133" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Mining the Web for synonyms: PMI-IR versus LSA on TOEFL</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth European Conference on Machine Learning (ECML-01)</title>
		<meeting>the Twelfth European Conference on Machine Learning (ECML-01)<address><addrLine>Freiburg, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="491" to="502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The viability of web-derived polarity lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Velikovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasha</forename><surname>Blair-Goldensohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kerry</forename><surname>Hannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Word sense and subjectivity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1065" to="1072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Annotating expressions of opinions and emotions in language. Language Resources and Evaluation (formerly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and the Humanities)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2/3</biblScope>
			<biblScope unit="page" from="164" to="210" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Opinionfinder: A system for subjectivity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swapna</forename><surname>Somasundaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Kessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Patwardhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT/EMNLP on Interactive Demonstrations</title>
		<meeting>HLT/EMNLP on Interactive Demonstrations</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="34" to="35" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Recognizing contextual polarity in phraselevel sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies Conference/Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Human Language Technologies Conference/Conference on Empirical Methods in Natural Language Processing<address><addrLine>Vancouver, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Understanding belief propagation and its generalizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">S</forename><surname>Yedidia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Exploring AI in the new millennium</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="239" to="269" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
