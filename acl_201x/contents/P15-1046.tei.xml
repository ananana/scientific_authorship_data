<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:18+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">New Transfer Learning Techniques for Disparate Label Sets</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
							<email>stratos@cs.columbia.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Columbia University</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwoo</forename><surname>Jeong</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">New Transfer Learning Techniques for Disparate Label Sets</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="473" to="482"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In natural language understanding (NLU), a user utterance can be labeled differently depending on the domain or application (e.g., weather vs. calendar). Standard domain adaptation techniques are not directly applicable to take advantage of the existing annotations because they assume that the label set is invariant. We propose a solution based on label embeddings induced from canonical correlation analysis (CCA) that reduces the problem to a standard domain adaptation task and allows use of a number of transfer learning techniques. We also introduce a new transfer learning technique based on pretrain-ing of hidden-unit CRFs (HUCRFs). We perform extensive experiments on slot tagging on eight personal digital assistant domains and demonstrate that the proposed methods are superior to strong baselines.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The main goal of NLU is to automatically extract the meaning of spoken or typed queries. In recent years, this task has become increasingly impor- tant as more and more speech-based applications have emerged. Recent releases of personal dig- ital assistants such as Siri, Google Now, Dragon Go and Cortana in smart phones provide natu- ral language based interface for a variety of do- mains (e.g. places, weather, communications, re- minders). The NLU in these domains are based on statistical machine learned models which re- quire annotated training data. Typically each do- main has its own schema to annotate the words and queries. However the meaning of words and utter- ances could be different in each domain. For ex- ample, "sunny" is considered a weather condition in the weather domain but it may be a song title in a music domain. Thus every time a new applica- tion is developed or a new domain is built, a sig- nificant amount of resources is invested in creating annotations specific to that application or domain.</p><p>One might attempt to apply existing techniques <ref type="bibr" target="#b2">(Blitzer et al., 2006;</ref><ref type="bibr" target="#b8">Daumé III, 2007</ref>) in domain adaption to this problem, but a straightforward ap- plication is not possible because these techniques assume that the label set is invariant.</p><p>In this work, we provide a simple and effec- tive solution to this problem by abstracting the la- bel types using the canonical correlation analysis (CCA) by <ref type="bibr" target="#b13">Hotelling (Hotelling, 1936</ref>) a powerful and flexible statistical technique for dimensional- ity reduction. We derive a low dimensional rep- resentation for each label type that is maximally correlated to the average context of that label via CCA. These shared label representations, or label embeddings, allow us to map label types across different domains and reduce the setting to a stan- dard domain adaptation problem. After the map- ping, we can apply the standard transfer learning techniques to solve the problem.</p><p>Additionally, we introduce a novel pretraining technique for hidden-unit CRFs (HUCRFs) to ef- fectively transfer knowledge from one domain to another. In our experiments, we find that our pretraining method is almost always superior to strong baselines such as the popular domain adap- tation method of Daumé III (2007).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem description and related work</head><p>Let D be the number of distinct domains. Let X i be the space of observed samples for the i-th do- main. Let Y i be the space of possible labels for the i-th domain. In most previous works in domain adaptation ( <ref type="bibr" target="#b2">Blitzer et al., 2006;</ref><ref type="bibr" target="#b8">Daumé III, 2007)</ref>, observed data samples may vary but label space is invariant 1 . That is,</p><formula xml:id="formula_0">Y i = Y j ∀i, j ∈ {1 . . . D}</formula><p>but X i = X j for some domains i and j. For exam- ple, in part-of-speech (POS) tagging on newswire and biomedical domains, the observed data sam- ple may be radically different but the POS tag set remains the same. In practice, there are cases, where the same query is labeled differently depending on the do- main or application and the context. For example, Fred Myer can be tagged differently; "send a text message to Fred Myer" and "get me driving direc- tion to Fred Myer ". In the first case, Fred Myer is person in user's contact list but it is a grocery store in the second one.</p><p>So, we relax the constraint that label spaces must be the same. Instead, we assume that sur- face forms (i.e words) are similar. This is a natu- ral setting in developing multiple applications on speech utterances; input spaces (service request utterances) do not change drastically but output spaces (slot tags) might.</p><p>Multi-task learning differs from our task. In general multi-task learning aims to improve per- formance across all domains while our domain adaptation objective is to optimize the perfor- mance of semantic slot tagger on the target do- main.</p><p>Below, we review related work in domain adap- tion and natural language understanding (NLU).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Related Work</head><p>Domain adaptation has been widely used in many natural language processing (NLP) applications including part-of-speech tagging ( <ref type="bibr" target="#b31">Schnabel and Schütze, 2014</ref>), parsing <ref type="bibr" target="#b28">(McClosky et al., 2010)</ref>, and machine translation ( <ref type="bibr" target="#b12">Foster et al., 2010</ref>). Most of the work can be classified either su- pervised domain adaptation ( <ref type="bibr" target="#b4">Chelba and Acero, 2006;</ref><ref type="bibr" target="#b2">Blitzer et al., 2006;</ref><ref type="bibr" target="#b7">Daume III and Marcu, 2006;</ref><ref type="bibr" target="#b8">Daumé III, 2007;</ref><ref type="bibr" target="#b11">Finkel and Manning, 2009;</ref><ref type="bibr" target="#b5">Chen et al., 2011</ref>) or semi-supervised adap- tation ( <ref type="bibr" target="#b1">Ando and Zhang, 2005;</ref><ref type="bibr" target="#b16">Jiang and Zhai, 2007;</ref><ref type="bibr" target="#b21">Kumar et al., 2010;</ref><ref type="bibr" target="#b14">Huang and Yates, 2010)</ref>. Our problem setting falls into the former.</p><p>Multi-task learning has become popular in NLP. <ref type="bibr" target="#b32">Sutton and McCallum (2005)</ref> showed that joint learning and/or decoding of sub-tasks helps to im- prove performance. <ref type="bibr" target="#b6">Collobert and Weston (2008)</ref> proved the similar claim in a deep learning archi- tecture. While our problem resembles their set- tings, there are two clear distinctions. First, we aim to optimize performance on the target domain by minimizing the gap between source and target domain while multi-task learning jointly learns the shared tasks. Second, in our problem the domains are different, but they are closely related. On the other hand, prior work focuses on multiple sub- tasks of the same data.</p><p>Despite the increasing interest in NLU <ref type="bibr" target="#b9">(De Mori et al., 2008;</ref><ref type="bibr" target="#b34">Xu and Sarikaya, 2013;</ref><ref type="bibr" target="#b35">Xu and Sarikaya, 2014;</ref><ref type="bibr" target="#b0">Anastasakos et al., 2014;</ref><ref type="bibr" target="#b10">El-Kahky et al., 2014;</ref><ref type="bibr" target="#b27">Marin et al., 2014;</ref><ref type="bibr" target="#b3">Celikyilmaz et al., 2015;</ref><ref type="bibr" target="#b25">Ma et al., 2015;</ref><ref type="bibr" target="#b20">Kim et al., 2015)</ref>, transfer learn- ing in the context of NLU has not been much ex- plored. The most relevant previous work is <ref type="bibr" target="#b33">Tur (2006)</ref> and <ref type="bibr" target="#b23">Li et al. (2011)</ref>, which described both the effectiveness of multi-task learning in the con- text of NLU. For multi-task learning, they used shared slots by associating each slot type with ag- gregate active feature weight vector based on an existing domain specific slot tagger. Our empiri- cal results shows that these vector representation might be helpful to find shared slots across do- main, but cannot find bijective mapping between domains.</p><p>Also, Jeong and Lee (2009) presented a transfer learning approach in multi-domain NLU, where the model jointly learns slot taggers in multiple domains and simultaneously predicts domain de- tection and slot tagging results. <ref type="bibr">2</ref> To share parame- ters across domains, they added an additional node for domain prediction on top of the slot sequence. However, this framework also limited to a setting in which the label set remains invariant. In con- trast, our method is restricted to this setting with- out any modification of models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Sequence Modeling Technique</head><p>The proposed techniques in Section 4 and 5 are generic methodologies and not tied to any partic- ular models such as any sequence models and in- stanced based models. However, because of supe- rior performance over CRF, we use a hidden unit CRF (HUCRF) of <ref type="bibr" target="#b26">Maaten et al. (2011)</ref>. While popular and effective, a CRF is still a lin- ear model. In contrast, a HUCRF benefits from nonlinearity, leading to superior performance over CRF <ref type="bibr" target="#b26">(Maaten et al., 2011</ref>). Thus we will focus on HUCRFs to demonstrate our techniques in experi- ments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Hidden Unit CRF (HUCRF)</head><p>A HUCRF introduces a layer of binary-valued hid- den units z = z 1 . . . z n ∈ {0, 1} for each pair of label sequence y = y 1 . . . y n and observation se- quence x = x 1 . . . x n . A HUCRF parametrized by θ ∈ R d and γ ∈ R d defines a joint probability of y and z conditioned on x as follows:</p><formula xml:id="formula_1">p θ,γ (y, z|x) = exp(θ Φ(x, z) + γ Ψ(z, y)) z ∈{0,1} n y ∈Y(x,z ) exp(θ Φ(x, z ) + γ Ψ(z , y )) (1)</formula><p>where Y(x, z) is the set of all possible label se- quences for x and z, and Φ(x, z) ∈ R d and Ψ(z, y) ∈ R d are global feature functions that de- compose into local feature functions:</p><formula xml:id="formula_2">Φ(x, z) = n j=1 φ(x, j, z j ) Ψ(z, y) = n j=1 ψ(z j , y j−1 , y j )</formula><p>HUCRF forces the interaction between the obser- vations and the labels at each position j to go through a latent variable z j : see <ref type="figure" target="#fig_0">Figure 1</ref> for illus- tration. Then the probability of labels y is given by marginalizing over the hidden units,</p><formula xml:id="formula_3">p θ,γ (y|x) = z∈{0,1} n p θ,γ (y, z|x)</formula><p>As in restricted Boltzmann machines <ref type="bibr" target="#b22">(Larochelle and Bengio, 2008)</ref>, hidden units are conditionally independent given observations and labels. This allows for efficient inference with HUCRFs de- spite their richness (see <ref type="bibr" target="#b26">Maaten et al. (2011)</ref> for details). We use a perceptron-style algorithm of Maaten et al. <ref type="formula">(2011)</ref> for training HUCRFs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Transfer learning between domains with different label sets</head><p>In this section, we describe three methods for uti- lizing annotations in domains with different la- bel types. First two methods are about transfer- ring features and last method is about transfer- ring model parameters. Each of these methods re- quires some sort of mapping for label types. A fine-grained label type needs to be mapped to a coarse one; a label type in one domain needs to be mapped to the corresponding label type in another domain. We will provide a solution to obtaining these label mappings automatically in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Coarse-to-fine prediction</head><p>This approach has some similarities to the method of <ref type="bibr" target="#b23">Li et al. (2011)</ref> in that shared slots are used to transfer information between domains. In this two-stage approach, we train a model on the source domain, make predictions on the target do- main, and then use the predicted labels as addi- tional features to train a final model on the target domain. This can be helpful if there is some cor- relation between the label types in the source do- main and the label types in the target domain. However, it is not desirable to directly use the label types in the source domain since they can be highly specific to that particular domain. An effective way to combat this problem is to re- duce the original label types such start-time, contract-info, and restaurant as to a set of coarse label types such as name, date, time, and location that are universally shared across all domains. By doing so, we can use the first model to predict generic labels such as time and then use the second model to use this information to predict fine-grained labels such as start-time and end-time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Method of Daumé III (2007)</head><p>In this popular technique for domain adapta- tion, we train a model on the union of the source domain data and the target domain data but with the following preprocessing step: each feature is duplicated and the copy is conjoined with a domain indicator. For example, in a WEATHER domain dataset, a feature that indi- cates the identity of the string "Sunny" will generate both w(0) = Sunny and (w(0) = Sunny) ∧ (domain = W EAT HER) as fea- ture types. This preprocessing allows the model to utilize all data through the common features and at the same time specialize to specific do- mains through the domain specific features. This is especially helpful when there is label ambigu- ity on particular features (e.g., "Sunny" might be a weather-condition in a WEATHER domain dataset but a music-song-name in a MUSIC domain dataset).</p><p>Note that a straightforward application of this technique is in general not feasible in our situation. This is because we have features conjoined with label types and our domains do not share label types. This breaks the sharing of features across domains: many feature types in the source domain are disjoint from those in the target domain due to different labeling.</p><p>Thus it is necessary to first map source domain label types to target domain label type. After the mapping, features are shared across domains and we can apply this technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Transferring model parameter</head><p>In this approach, we train HUCRF on the source domain and transfer the learned parameters to ini- tialize the training process on the target domain. This can be helpful for at least two reasons:</p><p>1. The resulting model will have parameters for feature types observed in the source domain as well as the target domain. Thus it has bet- ter feature coverage.</p><p>2. If the training objective is non-convex, this initialization can be helpful in avoiding bad local optima.</p><p>Since the training objective of HUCRFs is non- convex, both benefits can apply. We show in our experiments that this is indeed the case: the model benefits from both better feature coverage and bet- ter initialization.</p><p>Note that in order to use this approach, we need to map source domain label types to target domain label type so that we know which parameter in the source domain corresponds to which param- eter in the target domain. This can be a many-to- one, one-to-many, one-to-one mapping depending on the label sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Pretraining with HUCRFs</head><p>In fact, pretraining HUCRFs in the source domain can be done in various ways. Recall that there are two parameter types: θ ∈ R d for scoring obser- vations and hidden states and γ ∈ R d for scoring hidden states and labels (Eq. <ref type="formula">(1)</ref>). In pretraining, we first train a model (θ 1 , γ 1 ) on the source data</p><formula xml:id="formula_4">{(x (i) src , y (i) src )} nsrc i=1</formula><p>:</p><formula xml:id="formula_5">(θ 1 , γ 1 ) ≈ arg max θ,γ nsrc i=1 log p θ,γ (y (i) src |x (i) src )</formula><p>Then we train a model (θ 2 , γ 2 ) on the target data {(x</p><formula xml:id="formula_6">(i) trg , y (i) trg )} ntrg i=1 by initializing (θ 2 , γ 2 ) ← (θ 1 , γ 1 ): (θ 2 , γ 2 ) ≈ arg max θ,γ ntrg i=1 log p θ,γ (y (i) trg |x (i) trg )</formula><p>Here, we can choose to initialize only θ 2 ← θ 1 and discard the parameters for hidden states and labels since they may not be the same. The θ 1 parame- ters model the hidden structures in the source do- main data and serve as a good initialization point for learning the θ 2 parameters in the target domain. This can be helpful if the mapping between the la- bel types in the source data and the label types in the target data is unreliable. This process is illus- trated in <ref type="figure" target="#fig_1">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Automatic generation of label mappings</head><p>All methods described in Section 4 require a way to propagate the information in label types across different domains. A straightfor- ward solution would be to manually construct such mappings by inspection. For instance, we can specify that start-time and end-time are grouped as the same label time, or that the label public-transportation-route in the PLACES domain maps to the label implicit-location in the CALENDAR do- main.</p><p>Instead</p><note type="other">, we propose a technique that automat- ically generates the label mappings. We induce vector representations for all label types through canonical correlation analysis (CCA) -a pow- erful and flexible technique for deriving low- dimensional representation. We give a review of CCA in Section 5.1 and describe how we use the technique to construct label mappings in Sec- tion 5.2.</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Canonical Correlation Analysis (CCA)</head><p>CCA is a general technique that operates on a pair of multi-dimensional variables. CCA finds k dimensions (k is a parameter to be specified) in which these variables are maximally correlated.</p><p>Let x 1 . . . x n ∈ R d and y 1 . . . y n ∈ R d be n samples of the two variables. For simplicity, as- sume that these variables have zero mean. Then CCA computes the following for i = 1 . . . k:</p><p>arg max</p><formula xml:id="formula_7">u i ∈R d , v i ∈R d : u i u i =0 ∀i &lt;i v i v i =0 ∀i &lt;i n l=1 (u i x l )(v i y l ) n l=1 (u i x l ) 2 n l=1 (v i y l ) 2</formula><p>In other words, each (u i , v i ) is a pair of projec- tion vectors such that the correlation between the projected variables u i x l and v i y l (now scalars) is maximized, under the constraint that this projec- tion is uncorrelated with the previous i − 1 pro- jections. This is a non-convex problem due to the inter- action between u i and v i . Fortunately, a method based on singular value decomposition (SVD) pro- vides an efficient and exact solution to this prob- lem <ref type="bibr" target="#b13">(Hotelling, 1936)</ref>. The resulting solution u 1 . . . u k ∈ R d and v 1 . . . v k ∈ R d can be used to project the variables from the original d-and d -dimensional spaces to a k-dimensional space:</p><formula xml:id="formula_8">x ∈ R d −→ ¯ x ∈ R k : ¯ x i = u i x y ∈ R d −→ ¯ y ∈ R k : ¯ y i = v i y</formula><p>The new k-dimensional representation of each variable now contains information about the other variable. The value of k is usually selected to be much smaller than d or d , so the representation is typically also low-dimensional.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Inducing label embeddings</head><p>We now describe how to use CCA to induce vec- tor representations for label types. Using the same notation, let n be the number of instances of la- bels in the entire data. Let x 1 . . . x n be the original representations of the label samples and y 1 . . . y n be the original representations of the associated words set contained in the labels. We employ the following definition for the orig- inal representations for reasons we explain below. Let d be the number of distinct label types and d be the number of distinct word types.</p><p>• x l ∈ R d is a zero vector in which the entry corresponding to the label type of the l-th in- stance is set to 1.</p><p>• y l ∈ R d is a zero vector in which the entries corresponding to words spanned by the label are set to 1.</p><p>The motivation for this definition is that similar label types often have similar or same word. For instance, consider two label types start-time, (start time of a calendar event) and end-time, meaning (the end time of a cal- endar event). Each type is frequently associated with phrases about time. The phrases {"9 pm", "7", "8 am"} might be labeled as start-time; the phrases {"9 am", "7 pm"} might be labeled as end-time. In these examples, both label types share words "am", "pm", "9", and "7" even though phrases may not match exactly. <ref type="figure" target="#fig_3">Figure 3</ref> gives the CCA algorithm for inducing label embeddings. It produces a k-dimensional vector for each label type corresponding to the CCA projection of the one-hot encoding of that label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Discussion on alternative label representations</head><p>We point out that there are other options for in- ducing label representations besides CCA. For instance, one could simply use the sparse fea- ture vector representation of each label. How- ever, CCA's low-dimensional projection is com- putationally more convenient and arguably more generalizable. One can also consider training a predictive model similar to word2vec (Mikolov  In contrast, CCA is simple, efficient, and effec- tive and can be readily implemented. Also, CCA is theoretically well understood while methods in- spired by neural networks are not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Constructing label mappings</head><p>Vector representations of label types allow for nat- ural solutions to the task of constructing label mappings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Mapping to a coarse label set</head><p>Given a domain and the label types that occur in the domain, we can reduce the number of la- bel types by simply clustering their vector repre- sentations. For instance, if the embeddings for start-time and end-time are close together, they will be grouped as a single label type. We run the k-means algorithm on the label embeddings to obtain this coarse label set.  <ref type="table" target="#tab_0">Table 1</ref>: Some of cluster examples</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Bijective mapping between label sets</head><p>Given a pair of domains and their label sets, we can create a bijective label mapping by finding the nearest neighbor of each label type. <ref type="figure" target="#fig_2">Figure 4</ref> shows some actual examples of CCA-based bijec- tive maps, where the label set in the REMINDER domain is mapped to the PLACES and ALARM domains. One particularly interesting example is that move earlier time in REMINDER do- main is mapped to Travel time in PLACES and Duration in ALARM domain. This is a tag used in a user utterance requesting to move an <ref type="table" target="#tab_0">Domains   # of label Source Training  Test  Description  Alarm  7  27865  3334  Set alarms  Calendar  20  50255  7017  Set appointments &amp; meetings in the calendar  Communication  18  104881  14484  Make calls, send texts, and communication related user request  Note  4  17445  2342  Note taking  Ondevice  7  60847  9704  Phone settings  Places  32  150348  20798  Find places &amp; get direction  Reminder  16  62664  8235  Setting time, person &amp; place based reminder  Weather  9  53096  9114</ref> Weather forecasts &amp; historical information about weather patterns <ref type="table">Table 2</ref>: Size of number of label, labeled data set size and description for Alarm, Calendar, Communica- tion, Note, Ondevice, Places, Reminder and Weather domains partitioned into training and test set.</p><p>appointment to an earlier time. For example, in the query "move the dentist's appointment up by 30 minutes.", the phrase "30 minutes" is tagged with move earlier time. The role of this tag is very similar to the role of Travel time in PLACES (not Time) and Duration in ALARMS (not Start date), and CCA is able to recover this relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>In this section, we turn to experimental findings to provide empirical support for our proposed meth- ods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Setup</head><p>To test the effectiveness of our approach, we apply it to a suite of eight Cortana personal assistant do- mains for slot sequence tagging tasks, where the goal is to find the correct semantic tagging of the words in a given user utterance. The data statistics and short descriptions are shown in <ref type="table">Table 2</ref>. As the table indicates, the do- mains have very different granularity and diverse semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Baselines</head><p>In all our experiments, we trained HUCRF and only used n-gram features, including unigram, bi- gram, and trigram within a window of five words (±2 words) around the current word as binary fea- ture functions. With these features, we compare the following methods for slot tagging:</p><p>• NoAdapt: train only on target training data.</p><p>• Union: train on the union of source and target training data.</p><p>• Daume: train with the feature duplication method described in 4.2.</p><p>• C2F: train with the coarse-to-fine prediction method described in 4.1.</p><p>• Pretrain: train with the pretraining method described in 4.3.1.</p><p>To apply these methods except for Target, we treat each of the eight domains in turn as the test domain, with one of remaining seven domain as the source domain. As in general domain adap- tation setting, we assume that the source domain has a sufficient amount of labeled data but the tar- get domain has an insufficient amount of labeled data. Specifically, For each test or target domain, we only use 10% of the training examples to sim- ulate data scarcity. In the following experiments, we report the slot F-measure, using the standard CoNLL evaluation script <ref type="bibr">3</ref>   To assess the quality of our automatic mapping methods via CCA described in Section 5, we com- pared against manually established mappings and also the mapping method of <ref type="bibr" target="#b23">Li et al. (2011)</ref>   In contrast, the proposed CCA based technique consistently outperforms the NoAdapt baselines by significant margins. More importantly, it also outperforms manual results under all conditions. It is perhaps not so surprising -the CCA derived mapping is completely data driven, while human annotators have nothing but the prior linguistic knowledge about the slot tags and the domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Results on mappings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Main Results</head><p>The full results are shown in <ref type="table" target="#tab_5">Table 5</ref>, where all pairs of source and target languages are consid- ered for domain adaptation. It is clear from the ta- ble that we can always achieve better results using adaptation techniques than the non-adapted mod- els trained only on the target data. Also, our pro- posed pretraining method outperforms other types of adaptation in most cases.</p><p>The overall result of our experiments are shown in <ref type="table" target="#tab_4">Table 4</ref>. In this experiment, we compare dif- ferent adaptation techniques using our suggested CCA-based mapping. Here, except for NoAdapt, we use both the target and the nearest source do- main data. To find the nearest domain, we first map fine grained label set to coarse label set by using the method described in Section 5.4.1 and then count how many coarse labels are used in a domain. And then we can find the nearest source domain by calculating the l 2 distance between the multinomial distributions of the source domain and the target domain over the set of coarse labels.</p><p>For example, for CALENDAR, we identify REMINDER as the nearest domain and vice versa because most of their labels are attributes related to time. In all experiments, the domain adapted models perform better than using only target do- main data which achieves 75.1% F1 score. Sim- ply combining source and target domain using our automatically mapped slot labels performs slightly better than baseline. C2F boosts the performance up to 77.61% and Daume is able to reach 78.99%. <ref type="bibr">4</ref> Finally, our proposed method, pretrain achieves nearly 81.02% F1 score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We presented an approach to take advantage of ex- isting annotations when the data are similar but the label sets are different. This approach was based on label embeddings from CCA, which re- duces the setting to a standard domain adapta- tion problem. Combined with a novel pretrain- ing scheme applied to hidden-unit CRFs, our ap- proach is shown to be superior to strong baselines in extensive experiments for slot tagging on eight distinct personal assistant domains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Graphical representation of hidden unit CRFs.</figDesc><graphic url="image-1.png" coords="3,72.00,62.81,218.25,93.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of a pretraining scheme for HUCRFs.</figDesc><graphic url="image-2.png" coords="4,307.28,62.81,218.27,95.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Bijective mapping: labels in REMINDER domain (orange box) are mapped into those in PLACES and ALARM domains.</figDesc><graphic url="image-3.png" coords="6,83.34,62.80,430.88,164.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: CCA algorithm for inducing label embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 shows</head><label>1</label><figDesc>examples of this clustering. It demonstrates that the CCA representations ob- tained by the procedure described in Section 5.2 are indeed informative of the labels' properties.</figDesc><table>Cluster 
Labels 
Cluster 
Labels 

Time 

start time 

Person 

contact info 
end time 
artist 
original start time 
from contact name 
travel time 
relationship name 

Loc 

absolute loc 

Loc ATTR 

prefer route 
leaving loc 
public trans route 
from loc 
nearby 
position ref 
distance 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Comparison of slot F1 scores using 
the proposed CCA-derived mapping versus other 
mapping methods combined with different adap-
tation techniques. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>. The method of Li et al. (2011) is to associate each slot type with the aggregate active feature weight vectors based on an existing domain specific slot tagger (a CRF). Manual mapping were performed</figDesc><table>Target 

Source 
Minimum distance domain performance 
Domain 
Nearest Domain NoAdapt Union Daume 
C2F 
Pretrain 
Alarm 
Calendar 
74.82 
84.46 
84.97 
81.54 
84.88 
Calendar 
Reminder 
70.51 
73.94 
73.07 
72.82 
77.08 
Note 
Reminder 
65.38 
56.39 
69.89 
66.6 
69.55 
Ondevice 
Weather 
70.86 
66.66 
71.17 
71.49 
73.5 
Reminder 
Calendar 
77.3 
83.38 
82.19 
81.29 
83.22 
Communication 
Reminder 
79.31 
74.28 
80.33 
79.66 
82.96 
Places 
Weather 
73.93 
73.74 
75.86 
73.73 
80.11 
Weather 
Places 
92.78 
92.88 
94.43 
93.75 
97.18 
Average 
-
75.61 
75.72 
78.99 
77.61 
81.06 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Slot F1 scores on each target domain using adapted models from the nearest source domain. 

h h h h h h h h h h h h 

Source 
Target Alarm Calendar Note Ondevice Reminder Communication Places Weather Average 

NoAdapt 
74.82 
70.51 
65.38 
70.86 
77.3 
79.31 
73.93 
92.78 
75.61 

Alarm 

Union 
-
72.26 
59.92 
67.32 
79.45 
77.91 
73.78 
92.67 
74.76 
Daume 
-
72.77 
66.28 
70.94 
81.12 
80.38 
75.62 
93.12 
77.18 
C2F 
-
70.59 
64.06 
71 
78.8 
79.5 
74.29 
92.75 
75.86 
Pretrain 
-
76.68 
68.12 
71.8 
81.25 
81.5 
77.1 
95.03 
78.78 

Calendar 

Union 
84.46 
-
50.64 
64.7 
83.38 
75.02 
71.13 
93.2 
74.65 
Daume 
84.97 
-
65.43 
70.12 
82.19 
79.78 
75.21 
93.1 
78.69 
C2F 
81.54 
-
66.08 
71.22 
81.29 
80.11 
73.75 
93.18 
78.17 
Pretrain 
84.88 
-
69.21 
72.3 
83.22 
82.75 
77.89 
95.8 
80.86 

Note 

Union 
60.26 
60.42 
-
65.79 
69.81 
76.85 
70.56 
90.02 
70.53 
Daume 
66.03 
67.38 
-
69.54 
76.65 
77.83 
73.49 
92.09 
74.72 
C2F 
74.68 
70.51 
-
71.34 
77.49 
79.48 
74.17 
92.89 
77.22 
Pretrain 
75.52 
72.4 
-
71.4 
80.1 
82.06 
76.53 
94.22 
78.89 

Ondevice 

Union 
63.72 
66.28 
55.67 
-
75.16 
74.85 
70.59 
90.7 
71.00 
Daume 
71.01 
69.39 
64.02 
-
75.75 
77.92 
74.41 
92.62 
75.02 
C2F 
74.02 
70.33 
64.99 
-
77.43 
79.53 
73.84 
92.71 
76.12 
Pretrain 
76.27 
71.59 
67.21 
-
78.67 
82.34 
77.45 
95.04 
78.37 

Reminder 

Union 
84.74 
73.94 
56.39 
61.27 
-
74.28 
68.14 
92.22 
73.00 
Daume 
84.66 
73.07 
69.89 
67.94 
-
80.33 
73.36 
93.19 
77.49 
C2F 
80.42 
72.82 
66.6 
71.36 
-
79.66 
74.35 
92.38 
76.80 
Pretrain 
84.75 
77.08 
69.55 
71.9 
-
82.96 
78.57 
95.37 
80.03 

Communication 

Union 
58.25 
54.69 
65.28 
62.95 
63.98 
-
68.16 
87.13 
65.78 
Daume 
70.4 
67.41 
69.14 
69.26 
77.67 
-
73.33 
92.82 
74.29 
C2F 
74.54 
70.84 
65.48 
70.81 
77.68 
-
74.15 
92.79 
75.18 
Pretrain 
76.04 
74.01 
68.76 
73.2 
80.74 
-
76.83 
94.58 
77.74 

Places 

Union 
71.7 
67.56 
45.37 
53.93 
67.78 
63.67 
-
92.88 
66.13 
Daume 
75.69 
69.01 
66.11 
65.46 
79.01 
78.42 
-
94.43 
75.45 
C2F 
78.9 
71.64 
66.93 
71.26 
79.2 
79.19 
-
93.75 
77.27 
Pretrain 
76.8 
74.12 
67.5 
72.7 
81 
81.89 
-
97.18 
78.74 

Weather 

Union 
69.43 
58.53 
56.76 
66.66 
74.98 
77.53 
73.74 
-
68.23 
Daume 
75 
71.73 
66.54 
71.17 
79.36 
80.57 
75.86 
-
74.32 
C2F 
77.61 
71.47 
63.24 
71.49 
78.44 
79.43 
73.73 
-
73.63 
Pretrain 
77.37 
74.5 
68.23 
73.5 
80.96 
82.05 
80.11 
-
76.67 

Average 

Union 
70.37 
64.81 
55.72 
63.23 
73.51 
74.3 
70.87 
91.26 
70.51 
Daume 
75.4 
70.23 
66.77 
69.2 
78.32 
79.32 
74.47 
93.05 
75.85 
C2F 
77.39 
71.17 
65.4 
71.21 
78.62 
79.56 
74.04 
92.92 
76.29 
Pretrain 
78.80 
74.34 
68.37 
72.40 
80.85 
82.22 
77.78 
95.32 
78.76 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Slot F1 scores of using Union, Daume, Coarse-to-Fine and pretraining on all pairs of source and 
target data. The numbers in boldface are the best performing adaptation technique in each pair. 

by two experienced annotators who have PhD in 
linguistics and machine learning. Each annotator 
first assigned mapping slot labels independently 
and then both annotators collaborated to reduce 
disagreement of their mapping results. Initially, 
the disagreement of their mapping rate between 
two annotators was about 30% because labels of 
slot tagging are very diverse; furthermore, in some 
cases it is not clear for human annotators if there 
exists a valid mapping. 

The results are shown at Table 3. Vector repre-

sentation of Li et al. (2011) increases the F1 score 
slightly from 75.13 to 75.69 in C2F, but it does not 
help as much in cases that require bijective map-
ping: Daume, Union and Pretrain. 

</table></figure>

			<note place="foot" n="1"> Multilingual learning (Kim et al., 2011; Kim and Snyder, 2012; Kim and Snyder, 2013) has same setting.</note>

			<note place="foot" n="2"> Jeong and Lee (2009) pointed out that if the domain is given, their method is the same as that of Daumé III (2007).</note>

			<note place="foot" n="3"> http://www.cnts.ua.ac.be/conll2000/chunking/output.html</note>

			<note place="foot" n="4"> It is known that Daume is less beneficial when the source and target domains are similar due to the increased number of features.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Task specific continuous word representations for mono and multi-lingual spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tasos</forename><surname>Anastasakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Deoras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the ICASSP</title>
		<meeting>eeding of the ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3246" to="3250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A framework for learning predictive structures from multiple tasks and unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kubota</forename><surname>Rie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Ando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1817" to="1853" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Domain adaptation with structural correspondence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="120" to="128" />
		</imprint>
	</monogr>
	<note>Proceedings of the EMNLP</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Enriching word embeddings using knowledge graph for semantic tagging in conversational dialog systems. AAAI-Association for the Advancement of Artificial Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptation of maximum entropy capitalizer: Little data can help a lot</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="382" to="399" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Co-training for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2456" to="2464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICML</title>
		<meeting>the ICML</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Domain adaptation for statistical classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="page" from="101" to="126" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Frustratingly easy domain adaptation. proceedings of the ACL</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">256</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renato</forename><forename type="middle">De</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Bechet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mctear</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Riccardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokhan</forename><surname>Tur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spoken language understanding. Signal Processing Magazine</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="50" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Extending domain coverage of language understanding systems via intent transfer between domains using knowledge graphs and search query click logs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>El-Kahky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokhan</forename><surname>Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hierarchical bayesian domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL</title>
		<meeting>the ACL</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="602" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Discriminative instance weighting for domain adaptation in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyril</forename><surname>Goutte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP</title>
		<meeting>the EMNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="451" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Relations between two sets of variates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harold</forename><surname>Hotelling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3/4</biblScope>
			<biblScope unit="page" from="321" to="377" />
			<date type="published" when="1936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exploring representation-learning approaches to domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing</title>
		<meeting>the 2010 Workshop on Domain Adaptation for Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="23" to="30" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multidomain spoken language understanding with transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwoo</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary Geunbae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="412" to="424" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Instance weighting for domain adaptation in nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL</title>
		<meeting>the ACL</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="264" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Universal grapheme-to-phoneme prediction over latin alphabets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Snyder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP</title>
		<meeting>the EMNLP<address><addrLine>Jeju Island, South Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-07" />
			<biblScope unit="page" from="332" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised consonant-vowel prediction over hundreds of languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Snyder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL</title>
		<meeting>the ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1527" to="1536" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Universal morphological analysis using structured nearest neighbor prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>João</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graça</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snyder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP</title>
		<meeting>the EMNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="322" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Weakly supervised slot tagging with partially labeled sequences from web search click logs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwoo</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL. Association for Computational Linguistics</title>
		<meeting>the NAACL. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Co-regularization based semi-supervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avishek</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daume</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="478" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Classification using discriminative restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICML</title>
		<meeting>the ICML</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multitask learning for spoken language understanding with shared slots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye-Yi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gökhan</forename><surname>Tür</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the INTERSPEECH</title>
		<meeting>eeding of the INTERSPEECH</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="701" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A discriminative model based entity dictionary weighting approach for spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>IEEE Institute of Electrical and Electronics Engineers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Knowledge graph inference for spoken dialog systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">A</forename><surname>Crook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Foslerlussier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICASSP</title>
		<meeting>the ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hidden-unit conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning phrase patterns for text classification using a knowledge graph and unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Holenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA-International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Automatic domain adaptation for parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL</title>
		<meeting>the NAACL</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="28" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Shrinkage based features for slot tagging with conditional random fields. Proceeding of ISCA-International Speech Communication Association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Asli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Deoras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwoo</forename><surname>Jeong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Flors: Fast and simple domain adaptation for part-ofspeech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="15" to="26" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Composition of conditional random fields for transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP</title>
		<meeting>the EMNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="748" to="754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multitask learning for spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokhan</forename><surname>Tur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICASSP</title>
		<meeting>the ICASSP<address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Convolutional neural network based triangular crf for joint intent detection and slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding (ASRU)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="78" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Targeted feature dropout for robust slot filling in natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA-International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
