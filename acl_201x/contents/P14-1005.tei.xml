<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Structured Perceptrons for Coreference Resolution with Latent Antecedents and Non-local Features</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Björkelund</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Natural Language Processing</orgName>
								<orgName type="institution">University of Stuttgart</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Kuhn</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Natural Language Processing</orgName>
								<orgName type="institution">University of Stuttgart</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Structured Perceptrons for Coreference Resolution with Latent Antecedents and Non-local Features</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="47" to="57"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We investigate different ways of learning structured perceptron models for coref-erence resolution when using non-local features and beam search. Our experimental results indicate that standard techniques such as early updates or Learning as Search Optimization (LaSO) perform worse than a greedy baseline that only uses local features. By modifying LaSO to delay updates until the end of each instance we obtain significant improvements over the baseline. Our model obtains the best results to date on recent shared task data for Arabic, Chinese, and English.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper studies and extends previous work us- ing the structured perceptron <ref type="bibr" target="#b12">(Collins, 2002</ref>) for complex NLP tasks. We show that for the task of coreference resolution the straightforward combi- nation of beam search and early update ( <ref type="bibr" target="#b11">Collins and Roark, 2004</ref>) falls short of more limited fea- ture sets that allow for exact search. This contrasts with previous work on, e.g., syntactic parsing ( <ref type="bibr" target="#b11">Collins and Roark, 2004;</ref><ref type="bibr" target="#b26">Huang, 2008;</ref><ref type="bibr" target="#b41">Zhang and Clark, 2008</ref>) and linearization <ref type="bibr" target="#b5">(Bohnet et al., 2011)</ref>, and even simpler structured prediction problems, where early updates are not even nec- essary, such as part-of-speech tagging <ref type="bibr" target="#b12">(Collins, 2002</ref>) and named entity recognition <ref type="bibr" target="#b33">(Ratinov and Roth, 2009)</ref>.</p><p>The main reason why early updates underper- form in our setting is that the task is too difficult and that the learning algorithm is not able to profit from all training data. Put another way, early up- dates happen too early, and the learning algorithm rarely reaches the end of the instances as it halts, updates, and moves on to the next instance.</p><p>An alternative would be to continue decod- ing the same instance after the early updates, which is equivalent to Learning as Search Opti- mization ( <ref type="bibr">LaSO;</ref><ref type="bibr" target="#b17">Daumé III and Marcu (2005b)</ref>). The learning task we are tackling is however further complicated since the target structure is under-determined by the gold standard annotation. Coreferent mentions in a document are usually an- notated as sets of mentions, where all mentions in a set are coreferent. We adopt the recently pop- ularized approach of inducing a latent structure within these sets <ref type="bibr" target="#b24">(Fernandes et al., 2012;</ref><ref type="bibr" target="#b8">Chang et al., 2013;</ref><ref type="bibr" target="#b21">Durrett and Klein, 2013)</ref>. This approach provides a powerful boost to the performance of coreference resolvers, but we find that it does not combine well with the LaSO learning strategy. We therefore propose a modification to LaSO, which delays updates until after each instance. The com- bination of this modification with non-local fea- tures leads to further improvements in the cluster- ing accuracy, as we show in evaluation results on all languages from the CoNLL 2012 Shared Task - Arabic, Chinese, and English. We obtain the best results to date on these data sets. <ref type="bibr">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Coreference resolution is the task of grouping re- ferring expressions (or mentions) in a text into dis- joint clusters such that all mentions in a cluster refer to the same entity. An example is given in <ref type="figure">Figure 1</ref> below, where mentions from two clusters are marked with brackets: <ref type="bibr">[</ref>  <ref type="figure">Figure 1</ref>: An excerpt of a document with the men- tions from two clusters marked.</p><p>In recent years much work on coreference res- olution has been devoted to increasing the ex- pressivity of the classical mention-pair model, in which each coreference classification decision is limited to information about two mentions that make up a pair. This shortcoming has been ad- dressed by entity-mention models, which relate a candidate mention to the full cluster of mentions predicted to be coreferent so far (for more discus- sion on the model types, see, e.g., <ref type="bibr" target="#b30">(Ng, 2010)</ref>).</p><p>Nevertheless, the two best systems in the lat- est CoNLL Shared Task on coreference resolu- tion ( <ref type="bibr" target="#b31">Pradhan et al., 2012)</ref> were both variants of the mention-pair model. While the second best system (Björkelund and Farkas, 2012) followed the widely used baseline of <ref type="bibr">Soon et al. (2001)</ref>, the winning system <ref type="bibr" target="#b24">(Fernandes et al., 2012)</ref> proposed the use of a tree representation.</p><p>The tree-based model of <ref type="bibr" target="#b24">Fernandes et al. (2012)</ref> construes the representation of coreference clus- ters as a rooted tree. <ref type="figure">Figure 2</ref> displays an example tree over the clusters from <ref type="figure">Figure 1</ref>. Every men- tion corresponds to a node in the tree, and arcs be- tween mentions indicate that they are coreferent. The tree additionally has a dummy root node. Ev- ery subtree under the root node corresponds to a cluster of coreferent mentions.</p><p>Since coreference training data is typically not annotated with trees, <ref type="bibr" target="#b24">Fernandes et al. (2012)</ref> pro- posed the use of latent trees that are induced dur- ing the training phase of a coreference resolver. The latent tree provides more meaningful an- tecedents for training. <ref type="bibr">2</ref> For instance, the popular pair-wise instance creation method suggested by <ref type="bibr">Soon et al. (2001)</ref> assumes non-branching trees, where the antecedent of every mention is its lin- ear predecessor (i.e., he b 2 is the antecedent of Gary Wilber b 3 ). Comparing the two alternative antecedents of Gary Wilber b 3 , the tree in <ref type="figure">Fig- ure 2</ref> provides a more reliable basis for training a coreference resolver, as the two mentions of Gary Wilber are both proper names and have an exact string match.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Representation and Learning</head><p>Let M = {m 0 , m 1 , ..., m n } denote the set of men- tions in a document, including the artificial root mention (denoted by m 0 ). We assume that the <ref type="bibr">2</ref> We follow standard practice and overload the terms anaphor and antecedent to be any type of mention, i.e., names as well as pronouns. An antecedent is simply the mention to the left of the anaphor. Figure 2: A tree representation of <ref type="figure">Figure 1</ref>.</p><p>mentions are ordered ascendingly with respect to the linear order of the document, where the docu- ment root precedes all other mentions. 3 For each mention m j , let A j denote the set of potential an- tecedents. That is, the set of all mentions that precede m j according to the linear order includ- ing the root node, or, A j = {m i | i &lt; j}. Fi- nally, let A denote the set of all antecedent sets {A 0 , A 1 , ..., A n }. In the tree model, each mention corresponds to a node, and an antecedent-anaphor pair a i , m i , where a i ∈ A i , corresponds to a directed edge (or arc) pointing from antecedent to anaphor.</p><p>The score of an arc a i , m i is defined as the scalar product between a weight vector w and a feature vector Φ(a i , m i ), where Φ is a feature extraction function over an arc (thus extracting features from the antecedent and the anaphor). The score of a coreference tree y = {{a 1 , m 1 , a 2 , m 2 , ..., a n , m n } is defined as the sum of the scores of all the mention pairs:</p><formula xml:id="formula_0">score(ai, mi) = w · Φ(ai, mi)<label>(1)</label></formula><formula xml:id="formula_1">score(y) = a i ,m i ∈y score(ai, mi)</formula><p>The objective is to find the outputˆyoutputˆ outputˆy that maxi- mizes the scoring function:</p><formula xml:id="formula_2">ˆ y = arg max y∈Y(A) score(y)<label>(2)</label></formula><p>where Y(A) denotes the set of possible trees given the antecedent sets A. By treating the mentions as nodes in a directed graph and assigning scores to the arcs according to (1), <ref type="bibr" target="#b24">Fernandes et al. (2012)</ref> solved the search problem using the Chu-Liu- Edmonds (CLE) algorithm (Chu and <ref type="bibr" target="#b10">Liu, 1965;</ref><ref type="bibr" target="#b23">Edmonds, 1967)</ref>, which is a maximum spanning tree algorithm that finds the optimal tree over a connected directed graph. CLE, however, has the drawback that the scores of the arcs must remain fixed and can not change depending on other arcs and it is not clear how to include non-local features in a CLE decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Online learning</head><p>We find the weight vector w by online learning us- ing a variant of the structured perceptron <ref type="bibr" target="#b12">(Collins, 2002</ref>). Specifically, we use the passive-aggressive (PA) algorithm <ref type="bibr" target="#b13">(Crammer et al., 2006</ref>), since we found that this performed slightly better in prelim- inary experiments. <ref type="bibr">4</ref> The structured perceptron iterates over train- ing instances x i , y i , where x i are inputs and y i are outputs. For each instance it uses the current weight vector w to make a predictionˆypredictionˆ predictionˆy i given the input x i . If the prediction is incorrect, the weight vector is updated in favor of the correct structure. Otherwise the weight vector is left untouched. In our setting inputs x i correspond to documents and outputs y i are trees over mentions in a document. The training data is, however, not annotated with trees, but only with clusters of mentions. That is, the y i 's are not defined a priori.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Latent antecedents</head><p>In order to have a tree structure to update against, we use the current weight vector and apply the decoder to a constrained antecedent set and ob- tain a latent tree over the mentions in a docu- ment, where each mention is assigned a single cor- rect antecedent <ref type="bibr" target="#b24">(Fernandes et al., 2012</ref>). We con- strain the antecedent sets such that only trees that correspond to the correct clustering can be built. Specifically, let˜Alet˜ let˜A j denote the set of correct an- tecedents for a mention m j , or˜Aj or˜ or˜Aj = {m0} if mj has no correct antecedent {ai | COREF(ai, mj), ai ∈ Aj} otherwise that is, if mention m j is non-referential or the first mention of its cluster, ˜ A j contains only the docu- ment root. Otherwise it is the set of all mentions to the left that belong to the same cluster as m j . Analogously to A, let˜Alet˜ let˜A denote the set of con- strained antecedent sets. The latent tree˜ytree˜ tree˜y needed <ref type="bibr">4</ref> We also implement the feature mapping function Φ as a hash kernel <ref type="bibr" target="#b6">(Bohnet, 2010)</ref> and apply averaging <ref type="bibr" target="#b12">(Collins, 2002</ref>), though for brevity we omit this from the pseudocode. for updates is then defined to be the optimal tree over Y( ˜ A), subject to the current weight vector:</p><formula xml:id="formula_3">˜ y = arg max y∈Y( ˜ A) score(y)</formula><p>The intuition behind the latent tree is that during online learning, the weight vector will start favor- ing latent trees that are easier to learn (such as the one in <ref type="figure">Figure 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 PA algorithm with latent trees</head><p>Input: Training data D, number of iterations T Output: Weight vector w 1:</p><formula xml:id="formula_4">w = − → 0 2: for t ∈ 1..T do 3: for Mi, Ai, ˜ Ai ∈ D do 4: ˆ yi = arg max Y(A) score(y) Predict 5: if ¬ CORRECT(ˆ yi) then 6: ˜ yi = arg max Y( ˜ A) score(y) Latent tree 7: ∆ = Φ(ˆ yi) − Φ(˜ yi) 8: τ = ∆·w+LOSS(ˆ y i ) ∆ 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PA weight 9:</head><p>w = w + τ ∆ PA update 10: return w Algorithm 1 shows pseudocode for the learn- ing algorithm, which we will refer to as the base- line learning algorithm. Instead of looping over pairs x, y of documents and trees, it loops over triples M, A, ˜ AA that comprise the set of men- tions M and the two sets of antecedent candidates (line 3). Moreover, rather than checking that the tree is identical to the latent tree, it only requires the tree to correctly encode the gold clustering (line 5). The update that occurs in lines 7-9 is the passive-aggressive update. A loss function LOSS that quantifies the error in the prediction is used to compute a scalar τ that controls how much the weights are moved in each update. If τ is set to 1, the update reduces to the standard structured per- ceptron update. The loss function can be an arbi- trarily complex function that returns a numerical value of how bad the prediction is. In the sim- plest case, Hamming loss can be used, i.e., for each incorrect arc add 1. We follow <ref type="bibr" target="#b24">Fernandes et al. (2012)</ref> and penalize erroneous root attach- ments, i.e., mentions that erroneously get the root node as their antecedent, with a loss of 1.5. For all other arcs we use Hamming loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Incremental Search</head><p>We now show that the search problem in (2) can equivalently be solved by the more intuitive best- first decoder <ref type="bibr" target="#b29">(Ng and Cardie, 2002</ref>), rather than using the CLE decoder. The best-first decoder works incrementally by making a left-to-right pass over the mentions, selecting for each mention the highest scoring antecedent.</p><p>The key aspect that makes the best-first decoder equivalent to the CLE decoder is that all arcs point from left to right, both in this paper and in the work of <ref type="bibr" target="#b24">Fernandes et al. (2012)</ref>. We sketch a proof that this decoder also returns the highest scoring tree.</p><p>First, note that this algorithm indeed returns a tree. This can be shown by assuming the opposite, in which case the tree has to have a cycle. Then there must be a mention that has its antecedent to the right. Though this is not possible since all arcs point from left to right.</p><p>Second, this tree is the highest scoring tree. Again, assume the contrary, i.e., that there is a higher scoring tree in Y(A). This implies that for some mention there is a higher scoring antecedent than the one selected by the decoder. This contra- dicts the fact that the best-first decoder selects the highest scoring antecedent for each mention. <ref type="bibr">5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Introducing Non-local Features</head><p>Since the best-first decoder makes a left-to-right pass, it is possible to extract features on the partial structure on the left. Such non-local features are able to capture information beyond that of a men- tion and its potential antecedent, e.g., the size of a partially built cluster, or features extracted from the antecedent of the antecedent.</p><p>When only local features are used, greedy search (either with CLE or the best-first decoder) suffices to find the highest scoring tree. That is, greedy search provides an exact solution to equa- tion 2. Non-local features, however, render the ex- act search problem intractable. This is because with non-local features, locally suboptimal (i.e., non-greedy) antecedents for some mentions may lead to a higher total score over a whole document.</p><p>In order to keep some options around during search, we extend the best-first decoder with beam search. Beam search works incrementally by keeping an agenda of state items. At each step, all items on the agenda are expanded. The subset of size k (the beam size) of the highest scoring ex- pansions are retained and put back into the agenda for the next step. The feature extraction function Φ 5 In case there are multiple maximum spanning trees, the best-first decoder will return one of them. This also holds for the CLE algorithm. With proper definitions, the proof can be constructed to show that both search algorithms return trees belonging to the set of maximum spanning trees over a graph.</p><p>is also extended such that it also receives the cur- rent state s as an argument: Φ(m i , m j , s). The state encodes the previous decisions and enables Φ to extract features from the partial tree on the left.</p><p>We now outline three different ways of learning the weight vector w with non-local features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Early updates</head><p>The beam search decoder can be plugged into the training algorithm, replacing the calls to arg max. Since state items leading to the best tree may be pruned from the agenda before the decoder reaches the end of the document, the introduc- tion of non-local features may cause the decoder to return a non-optimal tree. This is problem- atic as it might cause updates although the correct tree has a higher score than the predicted one. It has previously been observed ( <ref type="bibr" target="#b25">Huang et al., 2012</ref>) that substantial gains can be made by applying an early update strategy ( <ref type="bibr" target="#b11">Collins and Roark, 2004)</ref>: if the correct item is pruned before reaching the end of the document, then stop and update.</p><p>While beam search and early updates have been successfully applied to other NLP applications, our task differs in two important aspects: First, coreference resolution is a much more difficult task, which relies on more (world) knowledge than what is available in the training data. In other words, it is unlikely that we can devise a feature set that is informative enough to allow the weight vector to converge towards a solution that lets the learning algorithm see the entire documents dur- ing training, at least in the situation when no ex- ternal knowledge sources are used.</p><p>Second, our gold structure is not known but is induced latently, and may vary from iteration to iteration. With non-local features this is trou- blesome since the best latent tree of a complete document may not necessarily coincide with the best partial tree at some intermediate mention m j , j &lt; n, i.e., a mention before the last in a docu- ment. We therefore also apply beam search to find the latent tree to have a partial gold structure for every mention in a document.</p><p>Algorithm 2 shows pseudocode for the beam search and early update training procedure. The algorithm maintains two parallel agendas, one for gold items and one for predicted items. At ev- ery mention, both agendas are expanded and thus cover the same set of mentions. Then the predicted agenda is checked to see if it contains any correct</p><note type="other">Algorithm 2 Beam search and early update Input: Data set D, epochs T , beam size k Output: weight vector w 1: w = − → 0 2: for t ∈ 1..T do 3: for Mi, Ai, ˜ Ai ∈ D do 4: AgendaG = {} 5: AgendaP = {} 6: for j ∈ 1..n do 7: AgendaG = EXPAND(AgendaG , ˜ Aj, mj, k) 8: AgendaP = EXPAND(AgendaP , Aj, mj, k) 9: if ¬ CONTAINSCORRECT(AgendaP ) then 10: ˜ y = EXTRACTBEST(AgendaG ) 11: ˆ y = EXTRACTBEST(AgendaP ) 12: update PA update 13: GOTO 3 Skip and move to next instance 14: ˆ y = EXTRACTBEST(AgendaP ) 15: if ¬ CORRECT(ˆ y) then 16: ˜ y = EXTRACTBEST(AgendaG ) 17:</note><p>update PA update item. If there is no correct item in the predicted agenda, search is halted and an update is made against the best item from the gold agenda. The algorithm then moves on to the next document. If the end of a document is reached, the top scoring predicted item is checked for correctness. If it is not, an update is made against the best gold item. A drawback of early updates is that the remain- der of the document is skipped when an early up- date is applied, effectively discarding some train- ing data. <ref type="bibr">6</ref> An alternative strategy that makes bet- ter use of the training data is to apply the max- violation procedure suggested by <ref type="bibr" target="#b25">Huang et al. (2012)</ref>. However, since our gold trees change from iteration to iteration, and even inside of a single document, it is not entirely clear with respect to what gold tree the maximum violation should be computed. Initial experiments with max-violation updates indicated that they did not improve much over early updates, and also had a tendency to only consider a smaller portion of the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">LaSO</head><p>To make full use of the training data we imple- mented Learning as Search Optimization ( <ref type="bibr">LaSO;</ref><ref type="bibr" target="#b17">Daumé III and Marcu, 2005b)</ref>. It is very similar to early updates, but differs in one crucial respect: When an early update is made, search is continued rather than aborted. Thus the learning algorithm always reaches the end of a document, avoiding the problem that early updates discard parts of the training data.</p><p>Correct items are computed the same way as with early updates, where an agenda of gold items is maintained in parallel. When search is resumed after an intermediate LaSO update, the prediction agenda is re-seeded with gold items (i.e., items that are all correct). This is necessary since the update influences what the partial gold structure looks like, and the gold agenda therefore needs to be recreated from the beginning of the document. Specifically, after each intermediate LaSO update, the gold agenda is expanded repeatedly from the beginning of the document to the point where the update was made, and is then copied over to seed the prediction agenda. In terms of pseudocode, this is accomplished by replacing lines 12 and 13 in Algorithm 2 with the following:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>update PA update 13:</p><formula xml:id="formula_5">AgendaG = {} 14:</formula><p>for mi ∈ {m1, ..., mj} Recreate gold agenda 15:</p><formula xml:id="formula_6">AgendaG = EXPAND(AgendaG , ˜ Ai, mi, k) 16: AgendaP = COPY(AgendaG ) 17:</formula><p>GOTO 6 Continue</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Delayed LaSO updates</head><p>When we applied LaSO, we noticed that it per- formed worse than the baseline learning algorithm when only using local features. We believe that the reason is that updates are made in the middle of documents which means that lexical forms of an- tecedents are "fresh in memory" of the weight vec- tor. This results in fewer mistakes during training and leads to fewer updates. While this feedback makes it easier during training, such feedback is not available during test time, and the LaSO learn- ing setting therefore mimics the testing setting to a lesser extent. We also found that LaSO updates change the shape of the latent tree and that the average dis- tance between mentions connected by an arc in- creased. This problem can also be attributed to how lexical items are fresh in memory. Such trees tend to deviate from the intuition that the latent trees are easier to learn. They also render distance- based features (which are standard practice and generally rather useful) less powerful, as distance in sentences or mentions becomes less of a reliable indicator for coreference.</p><p>To cope with this problem, we devised the delayed LaSO update, which differs from LaSO only in the respect that it postpones the actual up- dates until the end of a document. This is accom- plished by summing the distance vectors ∆ at ev- ery point where LaSO would make an update. At</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3 Delayed LaSO update</head><p>Input: Data set D, iterations T , beam size k Output: weight vector w 1: w = − → 0 2: for t ∈ 1..T do 3:</p><p>for Mi, Ai, ˜ Ai ∈ D do 4: AgendaG = {} 5: AgendaP = {} 6: ∆acc = − → 0 7: lossacc = 0 8:</p><p>for j ∈ 1..n do 9: AgendaG = EXPAND(AgendaG , ˜ Aj, mj, k) 10: AgendaP = EXPAND(AgendaP , Aj, mj, k) 11:</p><p>if ¬ CONTAINSCORRECT(AgendaP ) then 12:</p><formula xml:id="formula_7">˜ y = EXTRACTBEST(AgendaG ) 13: ˆ y = EXTRACTBEST(AgendaP ) 14: ∆acc = ∆acc + Φ(ˆ y) − Φ(˜ y) 15: lossacc = lossacc + LOSS(ˆ y) 16: AgendaP = AgendaG 17: ˆ y = EXTRACTBEST(AgendaP ) 18:</formula><p>if ¬ CORRECT(ˆ y) then 19:</p><formula xml:id="formula_8">˜ y = EXTRACTBEST(AgendaG ) 20: ∆acc = ∆acc + Φ(ˆ y) − Φ(˜ y) 21: lossacc = lossacc + LOSS(ˆ y) 22: if ∆acc = − → 0 then 23:</formula><p>update w.r.t. ∆acc and lossacc the end of a document, an update is made with re- spect to the sum of all ∆'s. Similarly, a running sum of the partial loss is maintained within a doc- ument. Since the PA update only depends on the distance vector ∆ and the loss, it can be applied with respect to these sums at the end of the doc- ument. When only local features are used, this update is equivalent to the updates in the baseline learning algorithm. This follows because greedy search finds the optimal tree when only local fea- tures are used. Similarly, using only local features, the beam-based best-first decoder will also return the optimal tree. Algorithm 3 shows the pseu- docode for the delayed LaSO learning algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Features</head><p>In this section we briefly outline the type of fea- tures we use. The feature sets are customized for each language. As a baseline we use the features from <ref type="bibr" target="#b4">Björkelund and Farkas (2012)</ref>, who ranked second in the 2012 CoNLL shared task and is pub- licly available. The exact definitions and feature sets that we use are available as part of the down- load package of our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Local features</head><p>Basic features that can be extracted on one or both mentions in a pair include (among oth- ers): Mention type, which is either root, pro- noun, name, or common; Distance features, e.g., the distance in sentences or mentions; Rule-based features, e.g., StringMatch or SubStringMatch; Syntax-based features, e.g., category labels or paths in the syntax tree; Lexical features, e.g., the head word of a mention or the last word of a men- tion.</p><p>In order to have a strong local baseline, we ap- plied greedy forward/backward feature selection on the training data using a large set of local fea- ture templates. Specifically, the training set of each language was split into two parts where 75% was used for training, and 25% for testing. Feature templates were incrementally added or removed in order to optimize the mean of MUC, B 3 , and CEAF e (i.e., the CoNLL average).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Non-local Features</head><p>We experimented with non-local features drawn from previous work on entity-mention mod- els ( <ref type="bibr" target="#b27">Luo et al., 2004;</ref><ref type="bibr" target="#b32">Rahman and Ng, 2009)</ref>, how- ever they did not improve performance in prelimi- nary experiments. The one exception is the size of a cluster ( <ref type="bibr" target="#b14">Culotta et al., 2007</ref>). Additional features we use are Shape encodes the linear "shape" of a cluster in terms of mention type. For instance, the clusters representing Gary Wilber and Drug Emporium Inc. from the example in <ref type="figure">Figure 1</ref>, would be repre- sented as RNPN and RNCCC, respectively. Where R, N, P, and C denote the root node, names, pro- nouns, and common noun phrases, respectively. Local syntactic context is inspired by the Entity Grid ( <ref type="bibr" target="#b1">Barzilay and Lapata, 2008)</ref>, where the ba- sic assumption is that references to an entity fol- low particular syntactic patterns. For instance, an entity may be introduced as an object in one sen- tence, whereas in subsequent sentences it is re- ferred to in subject position. Grammatical func- tions are approximated by the path in the syntax tree from a mention to its closest S node. The par- tial paths of a mention and its linear predecessor, given the cluster of the current antecedent, informs the model about the local syntactic context. Cluster start distance denotes the distance in mentions from the beginning of the document where the cluster of the antecedent in considera- tion begins.</p><p>Additionally, the non-local model also has ac- cess to the basic properties of other mentions in the partial tree structure, such as head words. The non-local features were selected with the same greedy forward strategy as the local features, start- ing from the optimized local feature sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experimental Setup</head><p>We apply our model to the CoNLL 2012 Shared Task data, which includes a training, develop- ment, and test set split for three languages: Ara- bic, Chinese and English. We follow the closed track setting where systems may only be trained on the provided training data, with the exception of the English gender and number data compiled by <ref type="bibr" target="#b3">Bergsma and Lin (2006)</ref>. We use automatically extracted mentions using the same mention extrac- tion procedure as <ref type="bibr" target="#b4">Björkelund and Farkas (2012)</ref>. We evaluate our system using the CoNLL 2012 scorer, which computes several coreference met- rics: MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), and CEAF e and CEAF m ( <ref type="bibr" target="#b28">Luo, 2005</ref>). We also report the CoNLL average (also known as MELA; Denis and Baldridge <ref type="formula" target="#formula_2">(2009)</ref>), i.e., the arithmetic mean of MUC, B 3 , and CEAF e . It should be noted that for B 3 and the CEAF met- rics, multiple ways of handling twinless mentions <ref type="bibr">7</ref> have been proposed <ref type="bibr" target="#b32">(Rahman and Ng, 2009;</ref><ref type="bibr" target="#b37">Stoyanov et al., 2009</ref>). We use the most recent ver- sion of the CoNLL scorer (version 7), which im- plements the original definitions of these metrics. <ref type="bibr">8</ref> Our system is evaluated on the version of the data with automatic preprocessing information (e.g., predicted parse trees). Unless otherwise stated we use 25 iterations of perceptron training and a beam size of 20. We did not attempt to tune either of these parameters. We experiment with two feature sets for each language: the optimized local feature sets (denoted local), and the opti- mized local feature sets extended with non-local features (denoted non-local).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Results</head><p>Learning strategies. We begin by looking at the different learning strategies. Since early updates do not always make use of the complete docu- ments during training, it can be expected that it will require either a very wide beam or more iter- ations to get up to par with the baseline learning algorithm. <ref type="figure" target="#fig_1">Figure 3</ref> shows the CoNLL average on Iterations Baseline Early (local), k=20 Early (local), k=100 Early (non-local), k=20 Early (non-local), k=100 the English development set as a function of num- ber of training iterations with two different beam sizes, 20 and 100, over the local and non-local fea- ture sets. The figure shows that even after 50 itera- tions, early update falls short of the baseline, even when the early update system has access to more informative non-local features. <ref type="bibr">9</ref> In <ref type="figure">Figure 4</ref> we compare early update with LaSO and delayed LaSO on the English development set. The left half uses the local feature set, and the right the extended non-local feature set. Recall that with only local features, delayed LaSO is equivalent to the baseline learning algorithm. As before, early update is considerably worse than other learning strategies. We also see that delayed LaSO out- performs LaSO, both with and without non-local features. Note that plain LaSO with non-local fea- tures only barely outperforms the delayed LaSO with only local features (i.e., the baseline), which indicates that only delayed LaSO is able to fully leverage non-local features. From these results we conclude that we are better off when the learning algorithm handles one document at a time, instead of getting feedback within documents.</p><p>Local vs. Non-local feature sets. Final results. In <ref type="table" target="#tab_4">Table 2</ref> we compare the re- sults of the non-local system (This paper) to the best results from the CoNLL 2012 Shared Task. <ref type="bibr">10</ref> Specifically, this includes Fernandes et al.'s (2012) system for Arabic and English (denoted Fernan- des), and Chen and Ng's (2012) system for Chi- nese (denoted C&amp;N). For English we also com- pare it to the Berkeley system <ref type="bibr" target="#b21">(Durrett and Klein, 2013)</ref>, which, to our knowledge, is the best pub- licly available system for English coreference res- olution (denoted D&amp;K). As a general baseline, we also include Björkelund and Farkas' (2012) sys- tem (denoted B&amp;F), which was the second best system in the shared task. For almost all met- rics our system is significantly better than the best competitor. For a few metrics the best competitor outperforms our results for either precision or re- call, but in terms of F-measures and the CoNLL average our system is the best for all languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Related Work</head><p>On the machine learning side <ref type="bibr" target="#b11">Collins and Roark's (2004)</ref> work on the early update constitutes our starting point. The LaSO framework was intro- duced by <ref type="bibr" target="#b17">Daumé III and Marcu (2005b)</ref>, but has, to our knowledge, only been applied to the related task of entity detection and tracking <ref type="bibr" target="#b16">(Daumé III and Marcu, 2005a</ref>). The theoretical motivation for early updates was only recently explained rigor- ously ( <ref type="bibr" target="#b25">Huang et al., 2012</ref>). The delayed LaSO update that we propose decomposes the predic- tion task of a complex structure into a number of subproblems, each of which guarantee violation, using Huang et al.'s (2012) terminology. We be- lieve this is an interesting novelty, as it leverages the complete structures for every training instance during every iteration, and expect it to be applica- ble also to other structured prediction tasks. Our approach also resembles imitation learning techniques such as SEARN <ref type="bibr" target="#b18">(Daumé III et al., 2009)</ref> and DAGGER <ref type="bibr" target="#b34">(Ross et al., 2011)</ref>, where the search problem is reduced to a sequence of classification steps that guide the search algorithm through the search space. These frameworks, however, rely on the notion of an expert policy which provides an optimal decision at each point during search. In our context that would require antecedents for ev- ery mention to be given a priori, rather than using latent antecedents as we do.</p><p>Perceptrons for coreference. The perceptron has previously been used to train coreference re- solvers either by casting the problem as a binary classification problem that considers pairs of men- tions in isolation <ref type="bibr" target="#b2">(Bengtson and Roth, 2008;</ref><ref type="bibr" target="#b37">Stoyanov et al., 2009;</ref><ref type="bibr">Chang et al., 2012, inter alia)</ref> or in the structured manner, where a clustering for an entire document is predicted in one go <ref type="bibr" target="#b24">(Fernandes et al., 2012</ref>). However, none of these works use non-local features. <ref type="bibr" target="#b36">Stoyanov and Eisner (2012)</ref> train an Easy-First coreference system with the perceptron to learn a sequence of join operations between arbitrary mentions in a document and ac- cesses non-local features through previous merge operations in later stages. <ref type="bibr" target="#b14">Culotta et al. (2007)</ref> also apply online learning in a first-order logic frame- work that enables non-local features, though using a greedy search algorithm.</p><p>Latent antecedents. The use of latent an- tecedents goes back to the work of <ref type="bibr" target="#b40">Yu and Joachims (2009)</ref>, although the idea of determining   meaningful antecedents for mentions can be traced back to <ref type="bibr" target="#b29">Ng and Cardie (2002)</ref> who used a rule- based approach. Latent antecedents have recently gained popularity and were used by two systems in the CoNLL 2012 Shared Task, including the win- ning system <ref type="bibr" target="#b24">(Fernandes et al., 2012;</ref><ref type="bibr" target="#b7">Chang et al., 2012)</ref>. <ref type="bibr" target="#b21">Durrett and Klein (2013)</ref> present a corefer- ence resolver with latent antecedents that predicts clusterings over entire documents and fit a log- linear model with a custom task-specific loss func- tion using AdaGrad <ref type="bibr" target="#b20">(Duchi et al., 2011</ref>). <ref type="bibr" target="#b8">Chang et al. (2013)</ref> use a max-margin approach to learn a pairwise model and rely on stochastic gradient descent to circumvent the costly operation of de- coding the entire training set in order to compute the gradients and the latent antecedents. None of the aforementioned works use non-local features in their models, however.</p><p>Entity-mention models. Entity-mention mod- els that compare a single mention to a (partial) cluster have been studied extensively and several works have evaluated non-local entity-level fea- tures ( <ref type="bibr" target="#b27">Luo et al., 2004;</ref><ref type="bibr" target="#b39">Yang et al., 2008;</ref><ref type="bibr" target="#b32">Rahman and Ng, 2009)</ref>. <ref type="bibr" target="#b27">Luo et al. (2004)</ref> also apply beam search at test time, but use a static assign- ment of antecedents and learns log-linear model using batch learning. Moreover, these works al- ter the basic feature definitions from their pair- wise models when introducing entity-level fea- tures. This contrasts with our work, as our mention-pair model simply constitutes a special case of the non-local system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Conclusion</head><p>We presented experiments with a coreference re- solver that leverages non-local features to improve its performance. The application of non-local fea- tures requires the use of an approximate search al- gorithm to keep the problem tractable. We eval- uated standard perceptron learning techniques for this setting both using early updates and LaSO. We found that the early update strategy is considerably worse than a local baseline, as it is unable to ex- ploit all training data. LaSO resolves this issue by giving feedback within documents, but still under- performs compared to the baseline as it distorts the choice of latent antecedents. We introduced a modification to LaSO, where updates are delayed until each document is pro- cessed. In the special case where only local fea- tures are used, this method coincides with stan- dard structured perceptron learning that uses exact search. Moreover, it is also able to profit from non- local features resulting in improved performance. We evaluated our system on all three languages from the CoNLL 2012 Shared Task and present the best results to date on these data sets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparing early update training with the baseline training algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 dis</head><label>1</label><figDesc>- plays the differences in F-measures and CoNLL average between the local and non-local systems when applied to the development sets for each lan- guage. All metrics improve when more informa- tive non-local features are added to the local fea- ture set. Arabic and English show considerable improvements, and the CoNLL average increases</figDesc><table>58 

59 

60 

61 

62 

63 

64 

65 

Local 
Non-local 

CoNLL avg. 

Early 
LaSO 
Delayed LaSO 

Figure 4: Comparison of learning algorithms eval-
uated on the English development set. 

MUC B 3 
CEAFm CEAFe CoNLL 
Arabic 
local 
47.33 42.51 49.71 
46.49 
45.44 
non-local 49.31 43.52 50.96 
47.18 
46.67 
Chinese 
local 
65.84 57.94 62.23 
57.05 
60.27 
non-local 66.4 
57.99 62.37 
57.12 
60.5 
English 
local 
69.95 58.7 
62.91 
56.03 
61.56 
non-local 70.74 60.03 65.01 
56.8 
62.52 

Table 1: Comparison of local and non-local fea-
ture sets on the development sets. 

about one point. For Chinese the gains are gen-
erally not as pronounced, though the MUC metric 
goes up by more than half a point. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Comparison with other systems on the test sets. Bold numbers indicate significance at the 
p &lt; 0.05 level between the best and the second best systems (according to the CoNLL average) using 
a Wilcoxon signed rank sum test. We refrain from significance tests on the CoNLL average, as it is an 
average over other F-measures. 

</table></figure>

			<note place="foot" n="1"> Our system is available at http://www.ims. uni-stuttgart.de/ ˜ anders/coref.html</note>

			<note place="foot" n="3"> We impose a total order on mentions. In case of nested mentions, the mention that begins first is assumed to precede the embedded one. If two mentions begin at the same token, the longer one is taken to precede the shorter one.</note>

			<note place="foot" n="6"> In fact, after 50 iterations about 70% of the mentions in the training data are still being ignored due to early updates.</note>

			<note place="foot" n="7"> i.e., mentions that appear in the prediction but not in gold, or the other way around 8 Available at http://conll.cemantix.org/ 2012/software.html</note>

			<note place="foot" n="9"> Although the Early systems still seem to show slight increases after 50 iterations, it needs a considerable number of iterations to catch up with the baseline-after 100 iterations the best early system is still more than half a point behind the baseline.</note>

			<note place="foot" n="10"> Thanks to Sameer Pradhan for providing us with the outputs of the other systems for significance testing.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We are grateful to the anonymous reviewers as well as Christian Scheible and Wolfgang Seeker for comments on earlier versions of this paper. This research has been funded by the DFG via SFB 732, project D8.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Algorithms for scoring coreference chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Bagga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Breck</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The First International Conference on Language Resources and Evaluation Workshop on Linguistics Coreference</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="563" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Modeling local coherence: An entity-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Understanding the value of features for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Bengtson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2008 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Honolulu, Hawaii</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008-10" />
			<biblScope unit="page" from="294" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bootstrapping path-based pronoun resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Bergsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-07" />
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Datadriven multilingual coreference resolution using resolver stacking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Björkelund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richárd</forename><surname>Farkas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Conference on EMNLP and CoNLL-Shared Task</title>
		<meeting><address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-07" />
			<biblScope unit="page" from="49" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">&lt;stumaba &gt;: From deep representation to surface</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Mille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoˆıtbenoˆıt</forename><surname>Favre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Wanner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Generation Challenges Session at the 13th European Workshop on Natural Language Generation</title>
		<meeting>the Generation Challenges Session at the 13th European Workshop on Natural Language Generation<address><addrLine>Nancy, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-09" />
			<biblScope unit="page" from="232" to="235" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Top accuracy and fast dependency parsing is not a contradiction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics<address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-08" />
			<biblScope unit="page" from="89" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Illinoiscoref: The ui system in the conll-2012 shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajhans</forename><surname>Samdani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alla</forename><surname>Rozovskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sammons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Conference on EMNLP and CoNLL-Shared Task</title>
		<meeting><address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-07" />
			<biblScope unit="page" from="113" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A constrained latent variable model for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajhans</forename><surname>Samdani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="601" to="612" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Combining the best of two worlds: A hybrid approach to multilingual coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Conference on EMNLP and CoNLL-Shared Task</title>
		<meeting><address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-07" />
			<biblScope unit="page" from="56" to="63" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the shortest aborescence of a directed graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoeng-Jin</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tseng-Hong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science Sinica</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1396" to="1400" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Incremental parsing with the perceptron algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL&apos;04), Main Volume</title>
		<meeting>the 42nd Meeting of the Association for Computational Linguistics (ACL&apos;04), Main Volume<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-07" />
			<biblScope unit="page" from="111" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2002 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2002-07" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Shai Shalev-Shwartz, and Yoram Singer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofer</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Keshet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Reseach</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="551" to="585" />
			<date type="published" when="2006-03" />
		</imprint>
	</monogr>
	<note>Online passive-aggressive algorithms</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">First-order probabilistic models for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aron</forename><surname>Culotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Association for Computational Linguistics</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Main Conference</title>
		<meeting>the Main Conference<address><addrLine>Rochester, New York</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A largescale exploration of effective global features for a joint entity detection and tracking model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing<address><addrLine>Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005-10" />
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning as search optimization: approximate large margin methods for structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Search-based structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="297" to="325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Global Joint Models for Coreference Resolution and Named Entity Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Procesamiento del Lenguaje Natural 42</title>
		<meeting>esamiento del Lenguaje Natural 42<address><addrLine>Barcelona</addrLine></address></meeting>
		<imprint>
			<publisher>SEPLN</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="87" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Easy victories and uphill battles in coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1971" to="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Washington</forename><surname>Seattle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Usa</forename><surname>October</surname></persName>
		</author>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Edmonds</surname></persName>
		</author>
		<title level="m">Optimum branchings. Journal of Research of the National Bureau of Standards</title>
		<imprint>
			<date type="published" when="1967" />
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="233" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Latent structure perceptron with feature induction for unrestricted coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eraldo</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruy</forename><surname>Cícero Dos Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Milidiú</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Conference on EMNLP and CoNLL-Shared Task</title>
		<meeting><address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-07" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Structured perceptron with inexact search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suphan</forename><surname>Fayong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="142" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Forest reranking: Discriminative parsing with non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT</title>
		<meeting>ACL-08: HLT<address><addrLine>Columbus, Ohio</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008-06" />
			<biblScope unit="page" from="586" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A mentionsynchronous coreference resolution algorithm based on the bell tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Ittycheriah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyan</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanda</forename><surname>Kambhatla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Meeting of the Association for Computational Linguistics</title>
		<meeting>the 42nd Meeting of the Association for Computational Linguistics<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-07" />
			<biblScope unit="page" from="135" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On coreference resolution performance metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing<address><addrLine>Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005-10" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improving machine learning approaches to coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002-07" />
			<biblScope unit="page" from="104" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Supervised noun phrase coreference research: The first fifteen years</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-07" />
			<biblScope unit="page" from="1396" to="1411" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Conll2012 shared task: Modeling multilingual unrestricted coreference in ontonotes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Conference on EMNLP and CoNLL-Shared Task</title>
		<meeting><address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-07" />
			<biblScope unit="page" from="1" to="40" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Supervised models for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Altaf</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Singapore, August</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="968" to="977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Design challenges and misconceptions in named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL-2009)</title>
		<meeting>the Thirteenth Conference on Computational Natural Language Learning (CoNLL-2009)<address><addrLine>Boulder, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="147" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A reduction of imitation learning and structured prediction to no-regret online learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Andrew</forename><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="627" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A machine learning approach to coreference resolution of noun phrases</title>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<editor>Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong Lim</editor>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="521" to="544" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Easy-first coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2012</title>
		<meeting>COLING 2012<address><addrLine>Mumbai, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-12" />
			<biblScope unit="page" from="2519" to="2534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Conundrums in noun phrase coreference resolution: Making sense of the stateof-the-art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP<address><addrLine>Suntec, Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009-08" />
			<biblScope unit="page" from="656" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dennis Connolly, and Lynette Hirschman. 1995. A model theoretic coreference scoring scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Vilain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Aberdeen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings MUC-6</title>
		<meeting>MUC-6<address><addrLine>Columbia, Maryland</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="45" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">An entitymention model for coreference resolution with inductive logic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chew</forename><surname>Lim Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL08: HLT</title>
		<meeting>ACL08: HLT<address><addrLine>Columbus, Ohio</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008-06" />
			<biblScope unit="page" from="843" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning structural svms with latent variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Nam</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2008 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Honolulu, Hawaii</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008-10" />
			<biblScope unit="page" from="562" to="571" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
