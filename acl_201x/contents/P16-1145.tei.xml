<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:59+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">WIKIREADING: A Novel Large-scale Language Understanding Task over Wikipedia</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hewlett</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Lacoste</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Fandrianto</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Kelcey</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">WIKIREADING: A Novel Large-scale Language Understanding Task over Wikipedia</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1535" to="1545"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present WIKIREADING, a large-scale natural language understanding task and publicly-available dataset with 18 million instances. The task is to predict textual values from the structured knowledge base Wikidata by reading the text of the corresponding Wikipedia articles. The task contains a rich variety of challenging classification and extraction sub-tasks, making it well-suited for end-to-end models such as deep neural networks (DNNs). We compare various state-of-the-art DNN-based architectures for document classification , information extraction, and question answering. We find that models supporting a rich answer space, such as word or character sequences, perform best. Our best-performing model, a word-level sequence to sequence model with a mechanism to copy out-of-vocabulary words, obtains an accuracy of 71.8%.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A growing amount of research in natural language understanding (NLU) explores end-to-end deep neural network (DNN) architectures for tasks such as text classification ( <ref type="bibr" target="#b26">Zhang et al., 2015)</ref>, rela- tion extraction <ref type="bibr" target="#b17">(Nguyen and Grishman, 2015)</ref>, and question answering ( ). These models offer the potential to remove the interme- diate steps traditionally involved in processing nat- ural language data by operating on increasingly raw forms of text input, even unprocessed char- acter or byte sequences. Furthermore, while these tasks are often studied in isolation, DNNs have the potential to combine multiple forms of reasoning within a single model. Supervised training of DNNs often requires a large amount of high-quality training data. To this end, we introduce a novel prediction task and ac- companying large-scale dataset with a range of sub-tasks combining text classification and infor- mation extraction. The dataset is made publicly- available at http://goo.gl/wikireading. The task, which we call WIKIREADING, is to pre- dict textual values from the open knowledge base <ref type="bibr">Wikidata (Vrandeči´Vrandeči´c and Krötzsch, 2014)</ref> given text from the corresponding articles on Wikipedia ( <ref type="bibr" target="#b1">Ayers et al., 2008)</ref>. Example instances are shown in <ref type="table">Table 1</ref>, illustrating the variety of subject mat- ter and sub-tasks. The dataset contains 18.58M in- stances across 884 sub-tasks, split roughly evenly between classification and extraction (see Section 2 for more details). In addition to its diversity, the WIKIREADING dataset is also at least an order of magnitude larger than related NLU datasets. Many natural lan- guage datasets for question answering (QA), such as WIKIQA ( <ref type="bibr" target="#b25">Yang et al., 2015)</ref>, have only thou- sands of examples and are thus too small for train- ing end-to-end models. <ref type="bibr" target="#b8">Hermann et al. (2015)</ref> proposed a task similar to QA, predicting entities in news summaries from the text of the original news articles, and generated a NEWS dataset with 1M instances. The bAbI dataset ) requires multiple forms of reasoning, but is composed of synthetically generated documents. WIKIQA and NEWS only involve pointing to lo- cations within the document, and text classifica- tion datasets often have small numbers of output classes. In contrast, WIKIREADING has a rich out- put space of millions of answers, making it a chal- lenging benchmark for state-of-the-art DNN archi- tectures for QA or text classification.</p><p>We implemented a large suite of recent models, and for the first time evaluate them on common grounds, placing the complexity of the task in con- text and illustrating the tradeoffs inherent in each Categorization Extraction Document Folkart Towers are twin skyscrapers in the Bayrakli district of the Turkish city of Izmir. Reaching a structural height of 200 m (656 ft) above ground level, they are the tallest . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Angeles blancos is a</head><p>Mexican telenovela pro- duced by Carlos So- tomayor for Televisa in 1990. Jacqueline An- dere, Rogelio Guerra and Alfonso Iturralde star as the main . . .</p><p>Canada is a country in the northern part of North America. Its ten provinces and three ter- ritories extend from the Atlantic to the Pacific and northward into the Arctic Ocean, . . . <ref type="table">American crime drama  television series created  and produced by Vince  Gilligan.  The show  originally aired on the  AMC network for</ref>   <ref type="table">Table 1</ref>: Examples instances from WIKIREADING. The task is to predict the answer given the document and property. Answer tokens that can be extracted are shown in bold, the remaining instances require classification or another form of inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Breaking Bad is an</head><p>approach. The highest score of 71.8% is achieved by a sequence to sequence model <ref type="bibr" target="#b10">(Kalchbrenner and Blunsom, 2013;</ref>) operating on word-level input and output sequences, with spe- cial handing for out-of-vocabulary words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">WIKIREADING</head><p>We now provide background information relating to Wikidata, followed by a detailed description of the WIKIREADING prediction task and dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Wikidata</head><p>Wikidata is a free collaborative knowledge base containing information about approximately 16M items <ref type="bibr" target="#b22">(Vrandeči´Vrandeči´c and Krötzsch, 2014)</ref>. Knowledge related to each item is expressed in a set of statements, each consisting of a (property, value) tuple. For example, the item Paris might have associated state- ments asserting (instance of, city) or (country, France). Wikidata contains over 80M such statements across 884 properties. Items may be linked to articles on Wikipedia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dataset</head><p>We constructed the WIKIREADING dataset from Wikidata and Wikipedia as follows: We consoli- dated all Wikidata statements with the same item and property into a single (item, property, answer) triple, where answer is a set of val- ues. Replacing each item with the text of the linked Wikipedia article (discarding unlinked items) yields a dataset of 18.58M (document, property, answer) instances. Importantly, all elements in each instance are human-readable strings, making the task entirely textual. The only modification we made to these strings was to convert timestamps into a human-readable format (e.g., "4 July 1776").</p><p>The WIKIREADING task, then, is to predict the answer string for each tuple given the document and property strings. This setup can be seen as similar to information extraction, or question an- swering where the property acts as a "question". We assigned all instances for each document ran- domly to either training (12.97M instances), val- idation (1.88M), and test (3.73M ) sets following a 70/10/20 distribution. This ensures that, during validation and testing, all documents are unseen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Documents</head><p>The dataset contains 4.7M unique Wikipedia ar- ticles, meaning that roughly 80% of the English- language Wikipedia is represented. Multiple in- stances can share the same document, with a mean of 5.31 instances per article (median: 4, max: 879). The most common categories of docu- ments are human, taxon, film, album, and human settlement, making up 48.8% of the documents and 9.1% of the instances. The mean and median document lengths are 489.2 and 203 words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Properties</head><p>The dataset contains 884 unique properties, though the distribution of properties across in- stances is highly skewed: The top 20 proper- ties cover 75% of the dataset, with 99% cov- erage achieved after 180 properties. We divide the properties broadly into two groups: Categor- ical properties, such as instance of, gender and country, require selecting between a rel- atively small number of possible answers, while relational properties, such as date of birth,  parent, and capital, typically require ex- tracting rare or totally unique answers from the document.</p><p>To quantify this difference, we compute the en- tropy of the answer distribution A for each prop- erty p, scaled to the <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> range by dividing by the entropy of a uniform distribution with the same number of values, i.e., ˆ H(p) = H(A p )/ log |A p |. Properties that represent essentially one-to-one mappings score near 1.0, while a property with just a single answer would score 0.0. <ref type="table" target="#tab_2">Table 2</ref> lists entropy values for a subset of properties, showing that the dataset contains a spectrum of sub-tasks. We label properties with an entropy less than 0.7 as categorical, and those with a higher entropy as relational. Categorical properties cover 56.7% of the instances in the dataset, with the remaining 43.3% being relational.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Answers</head><p>The distribution of properties described above has implications for the answer distribution. There are a relatively small number of very high frequency "head" answers, mostly for categorical properties, and a vast number of very low frequency "tail" an- swers, such as names and dates. At the extremes, the most frequent answer human accounts for al- most 7% of the dataset, while 54.7% of the an- swers in the dataset are unique. There are some special categories of answers which are systemati- cally related, in particular dates, which comprise 8.9% of the dataset (with 7.2% being unique). This distribution means that methods focused on either head or tail answers can each perform mod- erately well, but only a method that handles both types of answers can achieve maximum perfor- mance. Another consequence of the long tail of answers is that many (30.0%) of the answers in the test set never appear in the training set, meaning they must be read out of the document. An answer is present verbatim in the document for 45.6% of the instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>Recently, neural network architectures for NLU have been shown to meet or exceed the perfor- mance of traditional methods ( <ref type="bibr" target="#b26">Zhang et al., 2015;</ref><ref type="bibr" target="#b6">Dai and Le, 2015)</ref>. The move to deep neural networks also allows for new ways of combin- ing the property and document, inspired by recent research in the field of question answering (with the property serving as a question). In sequen- tial models such as Recurrent Neural Networks (RNNs), the question could be prepended to the document, allowing the model to "read" the doc- ument differently for each question ( <ref type="bibr" target="#b8">Hermann et al., 2015)</ref>. Alternatively, the question could be used to compute a form of attention ( ) over the document, to effectively focus the model on the most predictive words or phrases ( <ref type="bibr" target="#b20">Sukhbaatar et al., 2015;</ref><ref type="bibr" target="#b8">Hermann et al., 2015)</ref>. As this is currently an ongoing field of research, we implemented a range of recent models and for the first time compare them on common grounds. We now describe these methods, grouping them into broad categories by general approach and not- ing necessary modifications. Later, we introduce some novel variations of these models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Answer Classification</head><p>Perhaps the most straightforward approach to WIKIREADING is to consider it as a special case of document classification. To fit WIKIREAD- ING into this framework, we consider each pos- sible answer as a class label, and incorporate fea- tures based on the property so that the model can make different predictions for the same document. While the number of potential answers is too large to be practical (and unbounded in principle), a sub- stantial portion of the dataset can be covered by a model with a tractable number of answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Baseline</head><p>The most common approach to document classi- fication is to fit a linear model (e.g., Logistic Re- gression) over bag of words (BoW) features. To serve as a baseline for our task, the linear model needs to make different predictions for the same Wikipedia article depending on the property. We enable this behavior by computing two N w ele- ment BoW vectors, one each for the document and property, and concatenating them into a sin- gle 2N w feature vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Neural Network Methods</head><p>All of the methods described in this section en- code the property and document into a joint rep- resentation y ∈ R dout , which serves as input for a final softmax layer computing a probability dis- tribution over the top N ans answers. Namely, for each answer i ∈ {1, . . . , N ans }, we have:</p><formula xml:id="formula_0">P (i|x) = e y a i / Nans j=1 e y a j ,<label>(1)</label></formula><p>where a i ∈ R dout corresponds to a learned vec- tor associated with answer i. Thus, these models differ primarily in how they combine the property and document to produce the joint representation. For existing models from the literature, we provide a brief description and note any important differ- ences in our implementation, but refer the reader to the original papers for further details. Except for character-level models, documents and properties are tokenized into words. The N w most frequent words are mapped to a vector in R d in using a learned embedding matrix 1 . Other words are all mapped to a special out of vocabu- lary (OOV) token, which also has a learned em- bedding. d in and d out are hyperparameters for these models.</p><p>Averaged Embeddings (BoW): This is the neu- ral network version of the baseline method de- scribed in Section 3.1.1. Embeddings for words in the document and property are separately aver- aged. The concatenation of the resulting vectors forms the joint representation of size 2d in .</p><p>Paragraph Vector: We explore a variant of the previous model where the document is encoded as a paragraph vector ( <ref type="bibr" target="#b13">Le and Mikolov, 2014</ref>). We apply the PV-DBOW variant that learns an embed- ding for a document by optimizing the prediction of its constituent words. These unsupervised doc- ument embeddings are treated as a fixed input to the supervised classifier, with no fine-tuning.</p><p>LSTM Reader: This model is a simplified ver- sion of the Deep LSTM Reader proposed by <ref type="bibr" target="#b8">Hermann et al. (2015)</ref>. In this model, an LSTM (Hochreiter and Schmidhuber, 1997) reads the property and document sequences word-by-word and the final state is used as the joint representa- tion. This is the simplest model that respects the order of the words in the document. In our imple- mentation we use a single layer instead of two and a larger hidden size. More details on the architec- ture can be found in Section 4.1 and in <ref type="table">Table 4</ref>.</p><p>Attentive Reader: This model, also presented in <ref type="bibr" target="#b8">Hermann et al. (2015)</ref>, uses an attention mech- anism to better focus on the relevant part of the document for a given property. Specifically, At- tentive Reader first generates a representation u of the property using the final state of an LSTM while a second LSTM is used to read the document and generate a representation z t for each word. Then, conditioned on the property encoding u, a normal- ized attention is computed over the document to produce a weighted average of the word represen- tations z t , which is then used to generate the joint representation y. More precisely:</p><formula xml:id="formula_1">m t = tanh(W 1 concat(z t , u)) α t = exp (v m t ) r = t αt τ ατ z t y = tanh(W 2 concat(r, u)),</formula><p>where W 1 , W 2 , and v are learned parameters.</p><p>Memory Network: Our implementation closely follows the End-to-End Memory Network pro- posed in <ref type="bibr" target="#b20">Sukhbaatar et al. (2015)</ref>. This model maps a property p and a list of sentences x 1 , . . . , x n to a joint representation y by attend- ing over sentences in the document as follows:</p><p>The input encoder I converts a sequence of words x i = (x i1 , . . . , x iL i ) into a vector using an embed- ding matrix (equation 2), where L i is the length of sentence i. <ref type="bibr">2</ref> The property is encoded with the em- bedding matrix U (eqn. 3). Each sentence is en- coded into two vectors, a memory vector (eqn. 4) and an output vector (eqn. 5), with embedding ma- trices M and C, respectively. The property encod- ing is used to compute a normalized attention vec- tor over the memories (eqn. 6). <ref type="bibr">3</ref> The joint repre- sentation is the sum of the output vectors weighted <ref type="bibr">2</ref> Our final results use the position encoding method pro- posed by <ref type="bibr" target="#b20">Sukhbaatar et al. (2015)</ref>, which incorporates posi- tional information in addition to word embeddings.</p><p>3 Instead of the linearization method of <ref type="bibr" target="#b20">Sukhbaatar et al. (2015)</ref>, we applied an entropy regularizer for the softmax at- tention as described in <ref type="bibr" target="#b12">Kurach et al. (2015)</ref>.   by this attention (eqn. 7).</p><formula xml:id="formula_2">I(x i , W ) = j W x ij (2) u = I(p, U )<label>(3)</label></formula><formula xml:id="formula_3">m i = I(x i , M )<label>(4)</label></formula><formula xml:id="formula_4">c i = I(x i , C)<label>(5)</label></formula><formula xml:id="formula_5">p i = softmax(q m i )<label>(6)</label></formula><formula xml:id="formula_6">y = u + i p i c i<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Answer Extraction</head><p>Relational properties involve mappings between arbitrary entities (e.g., date of birth, mother, and author) and thus are less amenable to document classification. For these, approaches from information extraction (es- pecially relation extraction) are much more appropriate. In general, these methods seek to identify a word or phrase in the text that stands in a particular relation to a (possibly implicit) subject. Section 5 contains a discussion of prior work applying NLP techniques involving entity recognition and syntactic parsing to this problem. RNNs provide a natural fit for extraction, as they can predict a value at every position in a sequence, conditioned on the entire previous se- quence. The most straightforward application to WIKIREADING is to predict the probability that a word at a given location is part of an answer. We test this approach using an RNN that operates on the sequence of words. At each time step, we use a sigmoid activation for estimating whether the cur- rent word is part of the answer or not. We refer to this model as the RNN Labeler and present it graphically in <ref type="figure" target="#fig_1">Figure 1a</ref>.</p><p>For training, we label all locations where any answer appears in the document with a 1, and other positions with a 0 (similar to distant super- vision ( <ref type="bibr" target="#b16">Mintz et al., 2009)</ref>). For multi-word an- swers, the word sequences in the document and answer must fully match <ref type="bibr">4</ref> . Instances where no an- swer appears in the document are discarded for training. The cost function is the average cross- entropy for the outputs across the sequence. When performing inference on the test set, sequences of consecutive locations scoring above a threshold are chunked together as a single answer, and the top-scoring answer is recorded for submission. <ref type="bibr">5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Sequence to Sequence</head><p>Recently, sequence to sequence learning (or seq2seq) has shown promise for natural language tasks, especially machine translation ( ). These models combine two RNNs: an en- coder, which transforms the input sequence into a vector representation, and a decoder, which con- verts the encoder vector into a sequence of output tokens, one token at a time. This makes them ca- pable, in principle, of approximating any function mapping sequential inputs to sequential outputs. Importantly, they are the first model we consider that can perform any combination of answer clas- sification and extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Basic seq2seq</head><p>This model resembles LSTM Reader augmented with a second RNN to decode the answer as a se- quence of words. The embedding matrix is shared across the two RNNs but their state to state tran- sition matrices are different <ref type="figure" target="#fig_1">(Figure 1b)</ref>. This method extends the set of possible answers to any sequence of words from the document vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Placeholder seq2seq</head><p>While Basic seq2seq already expands the expres- siveness of LSTM Reader, it still has a limited vocabulary and thus is unable to generate some answers. As mentioned in Section 3.2, RNN La- beler can extract any sequence of words present in the document, even if some are OOV. We extend the basic seq2seq model to handle OOV words by adding placeholders to our vocabulary, increasing the vocabulary size from N w to N w + N doc . Then, when an OOV word occurs in the document, it is replaced at random (without replacement). by one of these placeholders. We also replace the corresponding OOV words in the target output se- This makes the input and output sequences a mixture of known words and placeholders, and al- lows the model to produce any answer the RNN Labeler can produce, in addition to the ones that the basic seq2seq model could already produce. This approach is comparable to entity anonymiza- tion used in <ref type="bibr" target="#b8">Hermann et al. (2015)</ref>, which replaces named entities with random ids, but simpler be- cause we use word-level placeholders without en- tity recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Basic Character seq2seq</head><p>Another way of handling rare words is to process the input and output text as sequences of charac- ters or bytes. RNNs have shown some promise working with character-level input, including state-of-the-art performance on a Wikipedia text classification benchmark <ref type="bibr" target="#b6">(Dai and Le, 2015)</ref>. A model that outputs answers character by character can in principle generate any of the answers in the test set, a major advantage for WIKIREADING.</p><p>This model, shown in <ref type="figure" target="#fig_2">Figure 2</ref>, operates only on sequences of mixed-case characters. The property encoder RNN transforms the property, as a charac- ter sequence, into a fixed-length vector. This prop- erty encoding becomes the initial hidden state for the second layer of a two-layer document encoder RNN, which reads the document, again, charac- ter by character. Finally, the answer decoder RNN uses the final state of the previous RNN to decode the character sequence for the answer. <ref type="bibr">6</ref> The same OOV word may occur several times in the document. Our simplified approach will attribute a different placeholder for each of these and will use the first occurrence for the target answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4">Character seq2seq with Pretraining</head><p>Unfortunately, at the character level the length of all sequences (documents, properties, and an- swers) is greatly increased. This adds more se- quential steps to the RNN, requiring gradients to propagate further, and increasing the chance of an error during decoding. To address this issue in a classification context, <ref type="bibr" target="#b6">Dai and Le (2015)</ref> showed that initializing an LSTM classifier with weights from a language model (LM) improved its accu- racy. Inspired by this result, we apply this prin- ciple to the character seq2seq model with a two- phase training process: In the first phase, we train a character-level LM on the input character se- quences from the WIKIREADING training set (no new data is introduced). In the second phase, the weights from this LM are used to initialize the first layer of the encoder and the decoder (purple and green blocks in <ref type="figure" target="#fig_2">Figure 2</ref>). After initialization, training proceeds as in the basic character seq2seq model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluated all methods from Section 3 on the full test set with a single scoring framework. An answer is correct when there is an exact string match between the predicted answer and the gold answer. However, as describe in Section 2.2, some answers are composed from a set of values (e.g. third example in <ref type="table">Table 1</ref>). To handle this, we de- fine the Mean F1 score as follows: For each in- stance, we compute the F1-score (harmonic mean of precision and recall) as a measure of the degree of overlap between the predicted answer set and the gold set for a given instance. The resulting per- instance F1 scores are then averaged to produce a single dataset-level score. This allows a method to obtain partial credit for an instance when it an- swers with at least one value from the golden set. In this paper, we only consider methods for an- swering with a single value, and most answers in the dataset are also composed of a single value, so this Mean F1 metric is closely related to accuracy. More precisely, a method using a single value as answer is bounded by a Mean F1 of 0.963.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training Details</head><p>We implemented all models in a single frame- work based on TensorFlow ( <ref type="bibr" target="#b0">Abadi et al., 2015</ref>) with shared pre-processing and comparable hyper- parameters whenever possible. All documents are  <ref type="table">Table 3</ref>: Results for all methods described in Section 3 on the test set. F1 is the Mean F1 score described in 4. Bound is the upper bound on Mean F1 imposed by constraints in the method (see text for details). The remaining columns provide score breakdowns by property type and the number of model parameters.</p><note type="other">Method Mean F1 Bound Categorical Relational Date Params Answer Classifier Sparse BoW Baseline 0</note><note type="other">Answer Extraction RNN Labeler 0.357 0.471 0.240 0.536 0.626 41M Sequence to Sequence Basic seq2seq 0.708 0.925 0.844 0.530 0.738 32M Placeholder seq2seq 0.718 0.948 0.835 0.565 0.730 32M Character seq2seq 0.677 0.963 0.841 0.462 0.731 4.1M Character seq2seq (LM) 0.699 0.963 0.851 0.501 0.733 4.1M</note><p>truncated to the first 300 words except for Charac- ter seq2seq, which uses 400 characters. The em- bedding matrix used to encode words in the doc- ument uses d in = 300 dimensions for the N w = 100, 000 most frequent words. Similarly, answer classification over the N ans = 50, 000 most fre- quent answers is performed using an answer rep- resentation of size d out = 300. 7 The first 10 words of the properties are embedded using the document embedding matrix. Following , RNNs in seq2seq models use a GRU cell with a hidden state size of 1024. More details on parameters are reported in <ref type="table">Table 4</ref>.  <ref type="table">Table 4</ref>: Structural model parameters. Note that the Para- graph Vector method uses the output from a separate, unsu- pervised model as a document encoding, which is not counted in these parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Optimization was performed with the Adam stochastic optimizer 8 ( <ref type="bibr" target="#b11">Kingma and Adam, 2015)</ref> over mini-batches of 128 samples. Gradient clip- ping 9 <ref type="bibr" target="#b7">(Graves, 2013</ref>) is used to prevent instability in training RNNs. We performed a search over <ref type="bibr">7</ref> For models like Averaged Embedding and Paragraph Vector, the concatenation imposes a greater dout.</p><p>8 Using β1 = 0.9, β2 = 0.999 and = 10 −8 . <ref type="bibr">9</ref> When the norm of gradient g exceeds a threshold C, it is 50 randomly-sampled hyperparameter configura- tions for the learning rate and gradient clip thresh- old, selecting the one with the highest Mean F1 on the validation set. Learning rate and clipping threshold are sampled uniformly, on a logarithmic scale, over the range [10 −5 , 10 −2 ] and [10 −3 , 10 1 ] respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results and Discussion</head><p>Results for all models on the held-out set of test in- stances are presented in <ref type="table">Table 3</ref>. In addition to the overall Mean F1 scores, the model families differ significantly in Mean F1 upper bound, and their relative performance on the relational and categor- ical properties defined in Section 2.4. We also re- port scores for properties containing dates, a sub- set of relational properties, as a separate column since they have a distinct format and organization. For examples of model performance on individual properties, see <ref type="table" target="#tab_6">Table 5</ref>. As expected, all classifier models perform well for categorical properties, with more sophisticated classifiers generally outperforming simpler ones. The difference in precision reading ability be- tween models that use broad document statistics, like Averaged Embeddings and Paragraph Vectors, and the RNN-based classifiers is revealed in the scores for relational and especially date proper- ties. As shown in  <ref type="table" target="#tab_6">Table 5</ref>: Property-level Mean F1 scores on the test set for selected methods and properties. For each property type, the two most frequent properties are shown followed by two less frequent properties to illustrate long-tail behavior. upper bound, as perfect classification across the 50, 000 most frequent answers would yield a Mean F1 of 0.831. However, none of them approaches this limit. Part of the reason is that their accuracy for a given answer decreases quickly as the fre- quency of the answer in the training set decreases, as illustrated in <ref type="figure" target="#fig_3">Figure 3</ref>. As these models have to learn a separate weight vector for each answer as part of the softmax layer (see Section 3.1), this may suggest that they fail to generalize across an- swers effectively and thus require significant num- ber of training examples per answer.</p><p>The only answer extraction model evaluated, RNN Labeler, shows a complementary set of strengths, performing better on relational proper- ties than categorical ones. While the Mean F1 up- per bound for this model is just 0.434 because it can only produce answers that are present verba- tim in the document text, it manages to achieve most of this potential. The improvement on date properties over the classifier models demonstrates its ability to identify answers that are typically present in the document. We suspect that answer extraction may be simpler than answer classifica- tion because the model can learn robust patterns that indicate a location without needing to learn about each answer, as the classifier models must.</p><p>The sequence to sequence models show a greater degree of balance between relational and categorical properties, reaching performance con- sistent with classifiers on the categorical questions and with RNN Labeler on relational questions. Placeholder seq2seq can in principle produce any answer that RNN Labeler can, and the perfor- mance on relational properties is indeed similar. As shown in <ref type="table" target="#tab_6">Table 5</ref>, Placeholder seq2seq per- forms especially well for properties where the an- swer typically contains rare words such as the name of a place or person. When the set of possible answer tokens is more constrained, such as in categorical or date properties, the Basic seq2seq often performs slightly better. Character seq2seq has the highest upper bound, limited to 0.963 only because it cannot produce an answer set with multiple elements. LM pretraining con- sistently improves the performance of the Charac- ter seq2seq model, especially for relational prop- erties as shown in <ref type="table" target="#tab_6">Table 5</ref>. The performance of the Character seq2seq, especially with LM pre- training, is a surprising result: It performs com- parably to the word-level seq2seq models even though it must copy long character strings when doing extraction and has access to a smaller por- tion of the document. We found the character based models to be particularly sensitive to hyper- parameters. However, using a pretrained language model reduced this issue and significantly accel- erated training while improving the final score. We believe that further research on pretraining for character based models could improve this result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>The goal of automatically extracting structured in- formation from unstructured Wikipedia text was first advanced by <ref type="bibr" target="#b24">Wu and Weld (2007)</ref>. As Wiki- data did not exist at that time, the authors re- lied on the structured infoboxes included in some Wikipedia articles for a relational representation of Wikipedia content. Wikidata is a cleaner data source, as the infobox data contains many slight variations in schema related to page formatting. Partially to get around this issue, the authors re- strict their prediction model Kylin to 4 specific in- fobox classes, and only common attributes within each class.</p><p>A substantial body of work in relation extrac- tion (RE) follows the distant supervision paradigm <ref type="bibr" target="#b5">(Craven and Kumlien, 1999)</ref>, where sentences containing both arguments of a knowledge base (KB) triple are assumed to express the triple's re- lation. Broadly, these models use these distant la- bels to identify syntactic features relating the sub- ject and object entities in text that are indicative of the relation. <ref type="bibr" target="#b16">Mintz et al. (2009)</ref> apply distant su- pervision to extracting Freebase triples <ref type="bibr" target="#b3">(Bollacker et al., 2008</ref>) from Wikipedia text, analogous to the relational part of WIKIREADING. Extensions to distant supervision include explicitly modelling whether the relation is actually expressed in the sentence ( <ref type="bibr" target="#b18">Riedel et al., 2010)</ref>, and jointly reason- ing over larger sets of sentences and relations <ref type="bibr" target="#b21">(Surdeanu et al., 2012)</ref>. Recently, <ref type="bibr" target="#b19">Rocktäschel et al. (2015)</ref> developed methods for reducing the num- ber of distant supervision examples required by sharing information between relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have demonstrated the complexity of the WIKIREADING task and its suitability as a bench- mark to guide future development of DNN models for natural language understanding. After compar- ing a diverse array of models spanning classifica- tion and extraction, we conclude that end-to-end sequence to sequence models are the most promis- ing. These models simultaneously learned to clas- sify documents and copy arbitrary strings from them. In light of this finding, we suggest some focus areas for future research.</p><p>Our character-level model improved substan- tially after language model pretraining, suggest- ing that further training optimizations may yield continued gains. Document length poses a prob- lem for RNN-based models, which might be ad- dressed with convolutional neural networks that are easier to parallelize. Finally, we note that these models are not intrinsically limited to English, as they rely on little or no pre-processing with tradi- tional NLP systems. This means that they should generalize effectively to other languages, which could be demonstrated by a multilingual version of WIKIREADING.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Ada</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of RNN models. Blocks with same color share parameters. Red words are out of vocabulary and all share a common embedding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Character seq2seq model. Blocks with the same color share parameters. The same example as in Figure 1 is fed character by character.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Per-answer Mean F1 scores for Attentive Reader (moving average of 1000), illustrating the decline in prediction quality as the number of training examples per answer decreases.</figDesc><graphic url="image-1.png" coords="8,79.09,394.86,204.09,109.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Training set frequency and scaled answer entropy 
for the 10 most frequent properties. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 ,</head><label>5</label><figDesc></figDesc><table>this difference is mag-
nified in situations that are more difficult for a 
classifier, such as relational properties or proper-
ties with fewer training examples, where Attentive 
Reader outperforms Averaged Embeddings by a 
wide margin. This model family also has a high 

scaled down i.e. g ← g · min 

1, C 

||g|| 


. </table></figure>

			<note place="foot" n="1"> Limited experimentation with initialization from publicly-available word2vec embeddings (Mikolov et al., 2013) yielded no improvement in performance.</note>

			<note place="foot" n="4"> Dates were matched semantically to increase recall. 5 We chose an arbitrary threshold of 0.5 for chunking. The score of each chunk is obtained from the harmonic mean of the predicted probabilities of its elements.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Jonathan Berant for many helpful com-ments on early drafts of the paper, and Cather-ine Finegan-Dollak for an early implementation of RNN Labeler.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martın</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Software available from tensorflow. org</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">How Wikipedia works: And how you can be a part of it</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phoebe</forename><surname>Ayers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Yates</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>No Starch Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>International Conference on Learning Representations (ICLR</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Freebase: A collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data, SIGMOD &apos;08</title>
		<meeting>the 2008 ACM SIGMOD International Conference on Management of Data, SIGMOD &apos;08<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
	<note>, October. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Constructing biological knowledge bases by extracting information from text sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Craven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Kumlien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Conference on Intelligent Systems for Molecular Biology</title>
		<meeting>the Seventh International Conference on Intelligent Systems for Molecular Biology</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="77" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semisupervised sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3061" to="3069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno>abs/1308.0850</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1684" to="1692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for discourse compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association of Computational Linguistics</title>
		<meeting><address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Proceedings of the CVSC Workshop</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy Ba</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural random-access machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 31st International Conference on Machine Learning</title>
		<meeting>The 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Addressing the rare word problem in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="11" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
	<note>ACL &apos;09. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Relation extraction: Perspective from convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huu</forename><surname>Thien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Injecting Logical Background Knowledge into Embeddings for Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2431" to="2439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-instance multi-label learning for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Wikidata: A free collaborative knowledgebase. Commun</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Vrandeči´vrandeči´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Krötzsch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ACM</publisher>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="78" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Towards ai-complete question answering: A set of prerequisite toy tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Autonomously semantifying wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniel S Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixteenth ACM conference on Conference on information and knowledge management</title>
		<meeting>the sixteenth ACM conference on Conference on information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="41" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Wikiqa: A challenge dataset for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2013" to="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
