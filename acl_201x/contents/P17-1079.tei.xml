<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:17+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Machine Translation via Binary Code Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Oda</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Arthur</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koichiro</forename><surname>Yoshino</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Nakamura</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Nara Institute of Science and Technoloty</orgName>
								<address>
									<postCode>8916-5, 630-0192</postCode>
									<settlement>Takayama-cho, Ikoma</settlement>
									<region>Nara</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Machine Translation via Binary Code Prediction</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="850" to="860"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1079</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we propose a new method for calculating the output layer in neural machine translation systems. The method is based on predicting a binary code for each word and can reduce computation time/memory requirements of the output layer to be logarithmic in vocabulary size in the best case. In addition, we also introduce two advanced approaches to improve the robustness of the proposed model: using error-correcting codes and combining softmax and binary codes. Experiments on two English ↔ Japanese bidirectional translation tasks show proposed models achieve BLEU scores that approach the softmax, while reducing memory usage to the order of less than 1/10 and improving decoding speed on CPUs by x5 to x10.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>When handling broad or open domains, machine translation systems usually have to handle a large vocabulary as their inputs and outputs. This is par- ticularly a problem in neural machine translation (NMT) models ), such as the attention-based models ( <ref type="bibr" target="#b0">Bahdanau et al., 2014;</ref><ref type="bibr" target="#b18">Luong et al., 2015)</ref> shown in <ref type="figure" target="#fig_0">Figure 1</ref>. In these models, the output layer is required to generate a specific word from an internal vector, and a large vocabulary size tends to require a large amount of computation to predict each of the candidate word probabilities.</p><p>Because this is a significant problem for neural language and translation models, there are a num- ber of methods proposed to resolve this problem, which we detail in Section 2.2. However, none of these previous methods simultaneously satisfies the following desiderata, all of which, we argue, are desirable for practical use in NMT systems: Memory efficiency: The method should not re- quire large memory to store the parameters and calculated vectors to maintain scalability in resource-constrained environments.</p><p>Time efficiency: The method should be able to train the parameters efficiently, and possible to perform decoding efficiently with choos- ing the candidate words from the full proba- bility distribution. In particular, the method should be performed fast on general CPUs to suppress physical costs of computational re- sources for actual production systems.</p><p>Compatibility with parallel computation: It should be easy for the method to be mini- batched and optimized to run efficiently on GPUs, which are essential for training large NMT models.</p><p>In this paper, we propose a method that satis- fies all of these conditions: requires significantly less memory, fast, and is easy to implement mini- batched on GPUs. The method works by not pre- dicting a softmax over the entire output vocab-ulary, but instead by encoding each vocabulary word as a vector of binary variables, then indepen- dently predicting the bits of this binary represen- tation. In order to represent a vocabulary size of 2 n , the binary representation need only be at least n bits long, and thus the amount of computation and size of parameters required to select an output word is only O(log V ) in the size of the vocabu- lary V , a great reduction from the standard linear increase of O(V ) seen in the original softmax.</p><p>While this idea is simple and intuitive, we found that it alone was not enough to achieve competitive accuracy with real NMT models. Thus we make two improvements: First, we propose a hybrid model, where the high frequency words are pre- dicted by a standard softmax, and low frequency words are predicted by the proposed binary codes separately. Second, we propose the use of con- volutional error correcting codes with Viterbi de- coding <ref type="bibr" target="#b32">(Viterbi, 1967)</ref>, which add redundancy to the binary representation, and even in the face of localized mistakes in the calculation of the repre- sentation, are able to recover the correct word.</p><p>In experiments on two translation tasks, we find that the proposed hybrid method with error correc- tion is able to achieve results that are competitive with standard softmax-based models while reduc- ing the output layer to a fraction of its original size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Description and Prior Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Formulation and Standard Softmax</head><p>Most of current NMT models use one-hot repre- sentations to represent the words in the output vo- cabulary -each word w is represented by a unique sparse vector e id(w) ∈ R V , in which only one ele- ment at the position corresponding to the word ID id(w) ∈ {x ∈ N | 1 ≤ x ≤ V } is 1, while oth- ers are 0. V represents the vocabulary size of the target language. NMT models optimize network parameters by treating the one-hot representation e id(w) as the true probability distribution, and min- imizing the cross entropy between it and the soft- max probability v:</p><formula xml:id="formula_0">L H (v, id(w)) := H(e id(w) , v),<label>(1)</label></formula><p>= log sum exp u − u id(w) , (2)</p><formula xml:id="formula_1">v := exp u/ sum exp u,<label>(3)</label></formula><formula xml:id="formula_2">u := W hu h + β u ,<label>(4)</label></formula><p>where sum x represents the sum of all elements in x, x i represents the i-th element of x, W hu ∈ R V ×H and β u ∈ R V are trainable parameters and H is the total size of hidden layers directly con- nected to the output layer. According to Equation (4), this model clearly requires time/space computation in proportion to O(HV ), and the actual load of the computation of the output layer is directly affected by the size of vocabulary V , which is typically set around tens of thousands ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Prior Work on Suppressing Complexity of NMT Models</head><p>Several previous works have proposed methods to reduce computation in the output layer. The hi- erarchical softmax <ref type="bibr" target="#b22">(Morin and Bengio, 2005</ref>) pre- dicts each word based on binary decision and re- duces computation time to O(H log V ). However, this method still requires O(HV ) space for the parameters, and requires calculation much more complicated than the standard softmax, particu- larly at test time.</p><p>The differentiated softmax (Chen et al., 2016) divides words into clusters, and predicts words us- ing separate part of the hidden layer for each word clusters. This method make the conversion matrix of the output layer sparser than a fully-connected softmax, and can reduce time/space computation amount by ignoring zero part of the matrix. How- ever, this method restricts the usage of hidden layer, and the size of the matrix is still in propor- tion to V .</p><p>Sampling-based approximations <ref type="bibr" target="#b21">(Mnih and Teh, 2012;</ref><ref type="bibr" target="#b20">Mikolov et al., 2013</ref>) to the denomina- tor of the softmax have also been proposed to re- duce calculation at training. However, these meth- ods are basically not able to be applied at test time, still require heavy computation like the standard softmax.</p><p>Vocabulary selection approaches ( <ref type="bibr" target="#b19">Mi et al., 2016;</ref><ref type="bibr" target="#b15">L'Hostis et al., 2016)</ref> can also reduce the vocabulary size at testing, but these methods aban- don full search over the target space and the quality of picked vocabularies directly affects the translation quality.</p><p>Other methods using characters ( <ref type="bibr" target="#b16">Ling et al., 2015</ref>) or subwords ( <ref type="bibr" target="#b27">Sennrich et al., 2016;</ref><ref type="bibr" target="#b3">Chitnis and DeNero, 2015)</ref> can be applied to suppress the vocabulary size, but these methods also make for longer sequences, and thus are not a direct solution to problems of computational efficiency.   shows the binary code prediction model proposed in this study. Unlike the conventional softmax, the proposed method predicts each output word indirectly using dense bit arrays that correspond to each word. Let b(w) := [b 1 (w), b 2 (w), · · · , b B (w)] ∈ {0, 1} B be the target bit array obtained for word w, where each b i (w) ∈ {0, 1} is an independent binary function given w, and B is the number of bits in whole array. For convenience, we introduce some constraints on b. First, a word w is mapped to only one bit array b(w). Second, all unique words can be discriminated by b, i.e., all bit arrays satisfy that: 1</p><formula xml:id="formula_3">id(w) = id(w ) ⇒ b(w) = b(w ).<label>(5)</label></formula><p>Third, multiple bit arrays can be mapped to the same word as described in Section 3.5. By considering second constraint, we can also con- strain B ≥ log 2 V , because b should have at least V unique representations to distinguish each word. The output layer of the network in- dependently predicts B probability values q : <ref type="bibr">1</ref> We designed this injective condition using the id(·) func- tion to ignore task-specific sensitivities between different word surfaces (e.g. cases, ligatures, etc.). current hidden values h by logistic regressions:</p><formula xml:id="formula_4">= [q 1 (h), q 2 (h), · · · , q B (h)] ∈ [0, 1] B using the</formula><formula xml:id="formula_5">q(h) = σ(W hq h + β q ),<label>(6)</label></formula><formula xml:id="formula_6">σ(x) := 1/(1 + exp(−x)),<label>(7)</label></formula><p>where W hq ∈ R B×H and β q ∈ R B are train- able parameters. When we assume that each q i is the probability that "the i-th bit becomes 1," the joint probability of generating word w can be rep- resented as:</p><formula xml:id="formula_7">Pr(b(w)|q(h)) := B i=1 b i q i + ¯ b i ¯ q i ,<label>(8)</label></formula><p>where ¯ x := 1 − x. We can easily obtain the maximum-probability bit array from q by simply assuming the i-th bit is 1 if q i ≥ 1/2, or 0 other- wise. However, this calculation may generate in- valid bit arrays which do not correspond to actual words according to the mapping between words and bit arrays. For now, we simply assume that w = UNK (unknown) when such bit arrays are ob- tained, and discuss alternatives later in Section 3.5.</p><p>The constraints described here are very general requirements for bit arrays, which still allows us to choose between a wide variety of mapping func- tions. However, designing the most appropriate mapping method for NMT models is not a triv- ial problem. In this study, we use a simple map- ping method described in Algorithm 1, which was empirically effective in preliminary experiments. <ref type="bibr">2</ref> Here, V is the set of V target words including 3 extra markers: UNK, BOS (begin-of-sentence), and EOS (end-of-sentence), and rank(w) ∈ N &gt;0 is the rank of the word according to their frequen- cies in the training corpus. Algorithm 1 is one of the minimal mapping methods (i.e., satisfying B = log 2 V ), and generated bit arrays have the characteristics that their higher bits roughly repre- sents the frequency of corresponding words (e.g., if w is frequently appeared in the training corpus, higher bits in b(w) tend to become 0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Loss Functions</head><p>For learning correct binary representations, we can use any loss functions that is (sub-)differentiable and satisfies a constraint that:</p><formula xml:id="formula_8">L B (q, b) = L , if q = b, ≥ L , otherwise,<label>(9)</label></formula><p>Algorithm 1 Mapping words to bit arrays.</p><p>Require: w ∈ V Ensure: b ∈ {0, 1} B = Bit array representing w</p><formula xml:id="formula_9">x :=      0, if w = UNK 1, if w = BOS 2, if w = EOS 2 + rank(w), otherwise bi := x/2 i−1 mod 2 b ← [b1, b2, · · · , bB]</formula><p>where L is the minimum value of the loss func- tion which typically does not affect the gradi- ent descent methods. For example, the squared- distance:</p><formula xml:id="formula_10">L B (q, b) := B i=1 (q i − b i ) 2 ,<label>(10)</label></formula><p>or the cross-entropy:</p><formula xml:id="formula_11">L B (q, b) := − B i=1 b i log q i + ¯ b i log ¯ q i ,<label>(11)</label></formula><p>are candidates for the loss function. We also examined both loss functions in the preliminary experiments, and in this paper, we only used the squared-distance function (Equation <ref type="formula" target="#formula_0">(10)</ref>), be- cause this function achieved higher translation ac- curacies than Equation (11). 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Efficiency of the Binary Code Prediction</head><p>The computational complexity for the parame- ters W hq and β q is O(HB). This is equal to O(H log V ) when using a minimal mapping method like that shown in Algorithm 1, and is sig- nificantly smaller than O(HV ) when using stan- dard softmax prediction. For example, if we chose V = 65536 = 2 16 and use Algorithm 1's mapping method, then B = 16 and total amount of com- putation in the output layer could be suppressed to 1/4096 of its original size. On a different note, the binary code prediction model proposed in this study shares some ideas with the hierarchical softmax <ref type="bibr" target="#b22">(Morin and Bengio, 2005</ref>) approach. Actually, when we used a binary- tree based mapping function for b, our model can be interpreted as the hierarchical softmax with two strong constraints for guaranteeing independence between all bits: all nodes in the same level of the hierarchy share their parameters, and all levels of the hierarchy are predicted independently of each other. By these constraints, all bits in b can be calculated in parallel. This is particularly impor- tant because it makes the model conducive to be- ing calculated on parallel computation backends such as GPUs.</p><p>However, the binary code prediction model also introduces problems of robustness due to these strong constraints. As the experimental results show, the simplest prediction model which di- rectly maps words into bit arrays seriously de- creases translation quality. In Sections 3.4 and 3.5, we introduce two additional techniques to prevent reductions of translation quality and improve ro- bustness of the binary code prediction model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Hybrid Softmax/Binary Model</head><p>According to the Zipf's law <ref type="bibr" target="#b33">(Zipf, 1949)</ref>, the dis- tribution of word appearances in an actual cor- pus is biased to a small subset of the vocabu- lary. As a result, the proposed model mostly learns characteristics for frequent words and can- not obtain enough opportunities to learn for rare words. To alleviate this problem, we introduce a hybrid model using both softmax prediction and binary code prediction as shown in <ref type="figure" target="#fig_1">Figure 2</ref>(c). In this model, the output layer calculates a stan- dard softmax for the N − 1 most frequent words and an OTHER marker which indicates all rare words. When the softmax layer predicts OTHER, then the binary code layer is used to predict the representation of rare words. In this case, the ac- tual probability of generating a particular word can be separated into two equations according to the frequency of words:</p><formula xml:id="formula_12">Pr(w|h) v id(w) , if id(w) &lt; N, v N · π(w, h), otherwise,<label>(12)</label></formula><formula xml:id="formula_13">v := exp u / sum exp u ,<label>(13)</label></formula><formula xml:id="formula_14">u := W hu h + β u ,<label>(14)</label></formula><p>π(w, h) := Pr(b(w)|q(h)),</p><p>where W hu ∈ R N ×H and β u ∈ R N are trainable parameters, and id(w) assumes that the value cor- responds to the rank of frequency of each word. We also define the loss function for the hybrid <ref type="figure">Figure 3</ref>: Example of the classification problem using redundant bit array mapping.</p><p>model using both softmax and binary code losses:</p><formula xml:id="formula_16">L := l H (id(w)), if id(w) &lt; N, l H (N ) + l B , otherwise,<label>(16)</label></formula><formula xml:id="formula_17">l H (i) := λ H L H (v , i),<label>(17)</label></formula><formula xml:id="formula_18">l B := λ B L B (q, b),<label>(18)</label></formula><p>where λ H and λ B are hyper-parameters to deter- mine strength of both softmax/binary code losses. These also can be adjusted according to the train- ing data, but in this study, we only used λ H = λ B = 1 for simplicity. The computational complexity of the hybrid model is O(H(N + log V )), which is larger than the original binary code model O(H log V ). How- ever, N can be chosen as N V because the soft- max prediction is only required for a few frequent words. As a result, we can control the actual com- putation for the hybrid model to be much smaller than the standard softmax complexity O(HV ),</p><p>The idea of separated prediction of frequent words and rare words comes from the differenti- ated softmax <ref type="bibr" target="#b2">(Chen et al., 2016)</ref> approach. How- ever, our output layer can be configured as a fully- connected network, unlike the differentiated soft- max, because the actual size of the output layer is still small after applying the hybrid model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Applying Error-correcting Codes</head><p>The 2 methods proposed in previous sections im- pose constraints for all bits in q, and the value of each bit must be estimated correctly for the cor- rect word to be chosen. As a result, these models may generate incorrect words due to even a sin- gle bit error. This problem is the result of dense mapping between words and bit arrays, and can be avoided by creating redundancy in the bit ar- ray. <ref type="figure">Figure 3</ref> shows a simple example of how this idea works when discriminating 2 words using 3 bits. In this case, the actual words are obtained by estimating the nearest centroid bit array accord- ing to the Hamming distance between each cen- troid and the predicted bit array. This approach can predict correct words as long as the predicted bit arrays are in the set of neighbors for the cor- rect centroid (gray regions in the <ref type="figure">Figure 3)</ref>, i.e., up to a 1-bit error in the predicted bits can be cor- rected. This ability to be robust to errors is a cen- tral idea behind error-correcting codes <ref type="bibr" target="#b28">(Shannon, 1948)</ref>. In general, an error-correcting code has the ability to correct up to (d − 1)/2 bit errors when all centroids differ d bits from each other <ref type="bibr" target="#b8">(Golay, 1949)</ref>. d is known as the free distance determined by the design of error-correcting codes. Error- correcting codes have been examined in some pre- vious work on multi-class classification tasks, and have reported advantages from the raw classifica- tion ( <ref type="bibr" target="#b4">Dietterich and Bakiri, 1995;</ref><ref type="bibr" target="#b11">Klautau et al., 2003;</ref><ref type="bibr" target="#b17">Liu, 2006;</ref><ref type="bibr" target="#b14">Kouzani and Nasireding, 2009;</ref><ref type="bibr" target="#b13">Kouzani, 2010;</ref><ref type="bibr">Lin, 2011, 2013)</ref>. In this study, we applied an error-correcting algo- rithm to the bit array obtained from Algorithm 1 to improve robustness of the output layer in an NMT system. A challenge in this study is try- ing a large classification (#classes &gt; 10,000) with error-correction, unlike previous studies focused on solving comparatively small tasks (#classes &lt; 100). And this study also tries to solve a genera- tion task unlike previous studies. As shown in the experiments, we found that this approach is highly effective in these tasks. <ref type="figure" target="#fig_4">Figure 4</ref> (a) and (b) illustrate the training and generation processes for the model with error- correcting codes. In the training, we first con- vert the original bit arrays b(w) to a center bit array b in the space of error-correcting code:</p><formula xml:id="formula_19">b (b) := [b 1 (b), b 2 (b), · · · , b B (b)] ∈ {0, 1} B ,</formula><p>where B (B) ≥ B is the number of bits in the error-correcting code. The NMT model learns its Algorithm 2 Encoding into a convolutional code.</p><formula xml:id="formula_20">Require: b ∈ {0, 1} B Ensure: b ∈ {0, 1} 2(B+6) = Redundant bit array x[t] := b t , if 1 ≤ t ≤ B 0, otherwise y 1 t := x[t − 6 .. t] · [1001111] mod 2 y 2 t := x[t − 6 .. t] · [1101101] mod 2 b ← [y 1 1 , y 2 1 , y 1 2 , y 2 2 , · · · , y 1 B+6 , y 2 B+6 ]</formula><p>parameters based on the loss between predicted probabilities q and b . Note that typical error- correcting codes satisfy O(B /B) = O(1), and this characteristic efficiently suppresses the in- crease of actual computation cost in the output layer due to the application of the error-correcting code. In the generation of actual words, the decod- ing method of the error-correcting code converts the redundant predicted bits q into a dense rep- resentatioñ q := [ ˜ q 1 (q), ˜ q 2 (q), · · · , ˜ q B (q)], and uses˜quses˜ uses˜q as the bits to restore the word, as is done in the method described in the previous sections.</p><p>It should be noted that the method for perform- ing error correction directly affects the quality of the whole NMT model. For example, the map- ping shown in <ref type="figure">Figure 3</ref> has only 3 bits and it is clear that these bits represent exactly the same in- formation as each other. In this case, all bits can be estimated using exactly the same parameters, and we can not expect that we will benefit signif- icantly from applying this redundant representa- tion. Therefore, we need to choose an error correc- tion method in which the characteristics of origi- nal bits should be distributed in various positions of the resulting bit arrays so that errors in bits are not highly correlated with each-other. In addition, it is desirable that the decoding method of the ap- plied error-correcting code can directly utilize the probabilities of each bit, because q generated by the network will be a continuous probabilities be- tween zero and one.</p><p>In this study, we applied convolutional codes <ref type="bibr" target="#b32">(Viterbi, 1967)</ref> to convert between original and re- dundant bits. Convolutional codes perform a set of bit-wise convolutions between original bits and weight bits (which are hyper-parameters). They are well-suited to our setting here because they distribute the information of original bits in dif- ferent places in the resulting bits, work robustly for random bit errors, and can be decoded using Algorithm 3 Decoding from a convolutional code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Require: q ∈ (0, 1) 2(B+6)</head><p>Ensure: ˜ q ∈ {0, 1} B = Restored bit array g(q, b) := b log q</p><formula xml:id="formula_21">+ (1 − b) log(1 − q) φ 0 [s | s ∈ {0, 1} 6 ] ← 0, if s = [000000] −∞, otherwise for t = 1 → B + 6 do for s cur ∈ {0, 1} 6 do s prev (x) := [x] • s cur [1 .. 5] o 1 (x) := ([x] • s cur ) · [1001111] mod 2 o 2 (x) := ([x] • s cur ) · [1101101] mod 2 g (x) := g(q 2t−1 , o 1 (x)) + g(q 2t , o 2 (x)) φ (x) := φ t−1 [s prev (x)] + g (x) ˆ x ← arg max x∈{0,1} φ (x) r t [s cur ] ← s prev (ˆ x) φ t [s cur ] ← φ (ˆ x) end for end for s ← [000000] for t = B → 1 do s ← r t+6 [s ] ˜ q t ← s 1 end for˜q for˜ for˜q ← [˜ q 1 , ˜ q 2 , · · · , ˜ q B ]</formula><p>bit probabilities directly.</p><p>Algorithm 2 describes the particular convolu- tional code that we applied in this study, with two convolution weights <ref type="bibr">[1001111]</ref> and <ref type="bibr">[1101101]</ref> as fixed hyper-parameters. <ref type="bibr">4</ref> Where x[i .. j] := [x i , · · · , x j ] and x · y := i x i y i . On the other hand, there are various algorithms to decode con- volutional codes with the same format which are based on different criteria. In this study, we use the decoding method described in Algorithm 3, where x • y represents the concatenation of vectors x and y. This method is based on the Viterbi al- gorithm <ref type="bibr" target="#b32">(Viterbi, 1967</ref>) and estimates original bits by directly using probability of redundant bits. Al- though Algorithm 3 looks complicated, this algo- rithm can be performed efficiently on CPUs at test time, and is not necessary at training time when we are simply performing calculation of Equation (6). Algorithm 2 increases the number of bits from B into B = 2(B +6), but does not restrict the actual value of B. 4 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>We examined the performance of the proposed methods on two English-Japanese bidirectional translation tasks which have different translation difficulties: ASPEC (Nakazawa et al., 2016) and BTEC <ref type="bibr" target="#b31">(Takezawa, 1999)</ref>. <ref type="table" target="#tab_0">Table 1</ref> describes details of two corpora. To prepare inputs for training, we used tokenizer.perl in Moses ( <ref type="bibr" target="#b12">Koehn et al., 2007)</ref> and <ref type="bibr">KyTea (Neubig et al., 2011</ref>) for En- glish/Japanese tokenizations respectively, applied lowercase.perl from Moses, and replaced out-of-vocabulary words such that rank(w) &gt; V − 3 into the UNK marker.</p><p>We implemented each NMT model using C++ in the DyNet framework ( <ref type="bibr">Neubig et al., 2017)</ref> and trained/tested on 1 GPU (GeForce GTX TITAN X). Each test is also performed on CPUs to com- pare its processing time. We used a bidirectional RNN-based encoder applied in <ref type="bibr" target="#b0">Bahdanau et al. (2014)</ref>, unidirectional decoder with the same style of ( <ref type="bibr" target="#b18">Luong et al., 2015)</ref>, and the concat global attention model also proposed in <ref type="bibr" target="#b18">Luong et al. (2015)</ref>. Each recurrent unit is constructed using a 1-layer LSTM (input/forget/output gates and non- peepholes) ( <ref type="bibr" target="#b7">Gers et al., 2000</ref>) with 30% dropout ( <ref type="bibr" target="#b29">Srivastava et al., 2014</ref>) for the input/output vec- tors of the LSTMs. All word embeddings, recur- rent states and model-specific hidden states are de- signed with 512-dimentional vectors. Only output layers and loss functions are replaced, and other network architectures are identical for the conven- tional/proposed models. We used the Adam op- timizer ( <ref type="bibr" target="#b10">Kingma and Ba, 2014</ref>) with fixed hyper- parameters α = 0.001, β 1 = 0.9 β 2 = 0.999, ε = 10 −8 , and mini-batches with 64 sentences sorted according to their sequence lengths. For eval- uating the quality of each model, we calculated case-insensitive BLEU ( <ref type="bibr" target="#b26">Papineni et al., 2002</ref>) ev- ery 1000 mini-batches. <ref type="table" target="#tab_1">Table 2</ref> lists summaries of all methods we examined in experiments. Softmax prediction <ref type="figure" target="#fig_1">(Fig. 2(a)</ref>) Binary <ref type="figure" target="#fig_1">Fig. 2(b)</ref> w/ raw bit array Hybrid-N <ref type="figure" target="#fig_1">Fig. 2</ref>(c) w/ softmax size N Binary-EC Binary w/ error-correction Hybrid-N-EC Hybrid-N w/ error-correction (a) ASPEC (En → Ja) (b) BTEC (En → Ja) <ref type="figure">Figure 5</ref>: Training curves over 180,000 epochs. <ref type="table" target="#tab_2">Table 3</ref> shows the BLEU on the test set (bold and italic faces indicate the best and second places in each task), number of bits B (or B ) for the binary code, actual size of the output layer # out , number of parameters in the output layer # W,β , as well as the ratio of # W,β or amount of whole parameters compared with Softmax, and averaged processing time at training (per mini-batch on GPUs) and test (per sentence on GPUs/CPUs), respectively. <ref type="figure">Figure 5</ref>(a) and 5(b) shows training curves up to 180,000 epochs about some English→Japanese settings. To relax instabilities of translation qual- ities while training (as shown in <ref type="figure">Figure 5</ref>(a) and 5(b)), each BLEU in <ref type="table" target="#tab_2">Table 3</ref> is calculated by av- eraging actual test BLEU of 5 consecutive results  around the epoch that has the highest dev BLEU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results and Discussion</head><p>First, we can see that each proposed method largely suppresses the actual size of the output layer from ten to one thousand times compared with the standard softmax. By looking at the to- tal number of parameters, we can see that the proposed models require only 70% of the actual memory, and the proposed model reduces the to- tal number of parameters for the output layers to a practically negligible level. Note that most of remaining parameters are used for the embedding lookup at the input layer in both encoder/decoder. These still occupy O(EV ) memory, where E rep- resents the size of each embedding layer and usu- ally O(E/H) = O(1). These are not targets to be reduced in this study because these values rarely are accessed at test time because we only need to access them for input words, and do not need them to always be in the physical memory. It might be possible to apply a similar binary representation as that of output layers to the input layers as well, then express the word embedding by multiplying this binary vector by a word embedding matrix. This is one potential avenue of future work.</p><p>Taking a look at the BLEU for the simple Bi- nary method, we can see that it is far lower than other models for all tasks. This is expected, as described in Section 3, because using raw bit ar- rays causes many one-off estimation errors at the output layer due to the lack of robustness of the output representation. In contrast, Hybrid-N and Binary-EC models clearly improve BLEU from Binary, and they approach that of Softmax. This demonstrates that these two methods effectively improve the robustness of binary code prediction models. Especially, Binary-EC generally achieves higher quality than Hybrid-512 despite the fact that it suppress the number of parameters by about 1/10. These results show that introducing redun- dancy to target bit arrays is more effective than incremental prediction. In addition, the Hybrid-N- EC model achieves the highest BLEU in all pro- posed methods, and in particular, comparative or higher BLEU than Softmax in BTEC. This behav- ior clearly demonstrates that these two methods are orthogonal, and combining them together can be effective. We hypothesize that the lower qual- ity of Softmax in BTEC is caused by an over-fitting due to the large number of parameters required in the softmax prediction.</p><p>The proposed methods also improve actual computation time in both training and test. In par- ticular on CPU, where the computation speed is directly affected by the size of the output layer, the proposed methods translate significantly faster than Softmax by x5 to x20. In addition, we can also see that applying error-correcting code is also effictive with respect to the decoding speed. <ref type="figure" target="#fig_5">Figure 6</ref> shows the trade-off between the trans- lation quality and the size of softmax layers in the hybrid prediction model <ref type="figure" target="#fig_1">(Figure 2(c)</ref>) with- out error-correction. According to the model def- inition in Section 3.4, the softmax prediction and raw binary code prediction can be assumed to be the upper/lower-bound of the hybrid prediction model. The curves in <ref type="figure" target="#fig_5">Figure 6</ref> move between Soft- max and Binary models, and this behavior intu- itively explains the characteristics of the hybrid prediction. In addition, we can see that the BLEU score in BTEC quickly improves, and saturates at N = 1024 in contrast to the ASPEC model, which is still improving at N = 2048. We presume that the shape of curves in <ref type="figure" target="#fig_5">Figure 6</ref> is also affected by the difficulty of the corpus, i.e., when we train the hybrid model for easy datasets (e.g., BTEC is eas- ier than ASPEC), it is enough to use a small soft- max layer (e.g. N ≤ 1024).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this study, we proposed neural machine transla- tion models which indirectly predict output words via binary codes, and two model improvements: a hybrid prediction model using both softmax and binary codes, and introducing error-correcting codes to introduce robustness of binary code pre- diction. Experiments show that the proposed model can achieve comparative translation quali- ties to standard softmax prediction, while signif- icantly suppressing the amount of parameters in the output layer, and improving calculation speeds while training and especially testing.</p><p>One interesting avenue of future work is to au- tomatically learn encodings and error correcting codes that are well-suited for the type of binary code prediction we are performing here. In Al- gorithms 2 and 3 we use convolutions that were determined heuristically, and it is likely that learn- ing these along with the model could result in im- proved accuracy or better compression capability.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Encoder-decoder-attention NMT model and computation amount of the output layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Designs of output layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 (</head><label>2</label><figDesc>Figure 2(a) shows the conventional softmax prediction, and Figure 2(b) shows the binary code prediction model proposed in this study. Unlike the conventional softmax, the proposed method predicts each output word indirectly using dense bit arrays that correspond to each word. Let b(w) := [b 1 (w), b 2 (w), · · · , b B (w)] ∈ {0, 1} B be the target bit array obtained for word w, where each b i (w) ∈ {0, 1} is an independent binary function given w, and B is the number of bits in whole array. For convenience, we introduce some constraints on b. First, a word w is mapped to only one bit array b(w). Second, all unique words can be discriminated by b, i.e., all bit arrays satisfy that: 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Training and generation processes with error-correcting code.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: BLEU changes in the Hybrid-N methods according to the softmax size (En→Ja).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Details of the corpus. 
Name 
ASPEC BTEC 
Languages 
En ↔ Ja 

#sentences 

Train 2.00 M 465. k 
Dev 
1,790 
510 
Test 
1,812 
508 
Vocabulary size V 
65536 25000 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Evaluated methods. 
Name 
Summary 
Softmax 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 : Comparison of BLEU, size of output layers, number of parameters and processing time.</head><label>3</label><figDesc></figDesc><table>Corpus Method 
BLEU % 
B 
#out # W,β 
Ratio of #params 
Time (En→Ja) [ms] 
EnJa 
JaEn 
# W,β 
All 
Train Test: GPU / CPU 

ASPEC 

Softmax 
31.13 21.14 -65536 33.6 M 1/1 
1 
1026. 
121.6 / 2539. 
Binary 
13.78 6.953 16 
16 8.21 k 
1/4.10 k 0.698 711.2 
73.08 / 122.3 
Hybrid-512 
22.81 13.95 16 
528 271. k 
1/124. 
0.700 843.6 
81.28 / 127.5 
Hybrid-2048 
27.73 16.92 16 
2064 1.06 M 1/31.8 
0.707 837.1 
82.28 / 159.3 
Binary-EC 
25.95 18.02 44 
44 22.6 k 
1/1.49 k 0.698 712.0 
78.75 / 164.0 
Hybrid-512-EC 
29.07 18.66 44 
556 285. k 
1/118. 
0.700 850.3 
80.30 / 180.2 
Hybrid-2048-EC 30.05 19.66 44 
2092 1.07 M 1/31.4 
0.707 851.6 
77.83 / 201.3 

BTEC 

Softmax 
47.72 45.22 -25000 12.8 M 1/1 
1 
325.0 
34.35 / 323.3 
Binary 
31.83 31.90 15 
15 7.70 k 
1/1.67 k 0.738 250.7 
27.98 / 54.62 
Hybrid-512 
44.23 43.50 15 
527 270. k 
1/47.4 
0.743 300.7 
28.83 / 66.13 
Hybrid-2048 
46.13 45.76 15 
2063 1.06 M 1/12.1 
0.759 307.7 
28.25 / 67.40 
Binary-EC 
44.48 41.21 42 
42 21.5 k 
1/595. 
0.738 255.6 
28.02 / 69.76 
Hybrid-512-EC 
47.20 46.52 42 
554 284. k 
1/45.1 
0.744 307.8 
28.44 / 56.98 
Hybrid-2048-EC 48.17 46.58 42 
2090 1.07 M 1/12.0 
0.760 311.0 
28.47 / 69.44 

</table></figure>

			<note place="foot" n="2"> Other methods examined included random codes, Huffman codes (Huffman, 1952) and Brown clustering (Brown et al., 1992) with zero-padding to adjust code lengths, and some original allocation methods based on the word2vec embeddings (Mikolov et al., 2013).</note>

			<note place="foot" n="3"> In terms of learning probabilistic models, we should remind that using Eq. (10) is an approximation of Eq. (11). The output bit scores trained by Eq. (10) do not represent actual word perplexities, and this characteristics imposes some practical problems when comparing multiple hypotheses (e.g., reranking, beam search, etc.). We could ignore this problem in this paper because we only evaluated the one-best results in experiments.</note>

			<note place="foot" n="4"> We also examined many configurations of convolutional codes which have different robustness and computation costs, and finally chose this one.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Part of this work was supported by JSPS KAKENHI Grant Numbers JP16H05873 and JP17H00747, and Grant-in-Aid for JSPS Fellows Grant Number 15J10649.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Class-based n-gram models of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter F Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent J Della</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenifer C</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="479" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Strategies for training large vocabulary neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P16-1186" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1975" to="1985" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Variablelength word encodings for neural translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Chitnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D15-1249" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2088" to="2093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Solving multiclass learning problems via errorcorrecting output codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghulum</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bakiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="263" to="286" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multilabel classification with error-correcting codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Sung</forename><surname>Ferng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsuan-Tien</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="281" to="295" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multilabel classification using error-correcting codes of hard or soft bits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Sung</forename><surname>Ferng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsuan-Tien</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1888" to="1900" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to forget: Continual prediction with LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Felix A Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cummins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2451" to="2471" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Marcel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Golay</surname></persName>
		</author>
		<title level="m">Notes on digital coding. Proceedings of the Institute of Radio Engineers</title>
		<imprint>
			<date type="published" when="1949" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">657</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A method for the construction of minimum-redundancy codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Huffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the Institute of Radio Engineers</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1098" to="1101" />
			<date type="published" when="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On nearest-neighbor error-correcting output codes with application to all-pairs multiclass support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aldebaro</forename><surname>Klautau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Jevti´cjevti´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Orlitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2003-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P07-2045" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions. Association for Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume the Demo and Poster Sessions. Association for Computational Linguistics<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multilabel classification using error correction codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Abbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kouzani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Intelligence Computation and Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="444" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multilabel classification by bch code and random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Abbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gulisong</forename><surname>Kouzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nasireding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of recent trends in engineering</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="113" to="116" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Vocabulary selection strategies for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gurvan L&amp;apos;hostis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.00072</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04586</idno>
		<title level="m">Character-based neural machine translation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Using svm and error-correcting codes for multiclass dialog act classification in meeting corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D15-1166" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Vocabulary manipulation for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Ittycheriah</surname></persName>
		</author>
		<ptr target="http://anthology.aclweb.org/P16-2021" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="124" to="129" />
		</imprint>
	</monogr>
	<note>Short Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A fast and simple algorithm for training neural probabilistic language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine Learning</title>
		<meeting>the 29th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hierarchical probabilistic neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Tenth International Workshop on Artificial Intelligence and Statistics</title>
		<meeting>Tenth International Workshop on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="246" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Aspec: Asian scientific paper excerpt corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiaki</forename><surname>Nakazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Yaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyotaka</forename><surname>Uchimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hitoshi</forename><surname>Isahara ; Khalid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Choukri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marko</forename><surname>Declerck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grobelnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC 2016). European Language Resources Association (ELRA)</title>
		<editor>Nicoletta Calzolari</editor>
		<meeting>the Ninth International Conference on Language Resources and Evaluation (LREC 2016). European Language Resources Association (ELRA)<address><addrLine>Bente Maegaard, Joseph Mariani, Asuncion Moreno, Jan Odijk; Portoro, Slovenia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2204" to="2208" />
		</imprint>
	</monogr>
	<note>and Stelios Piperidis</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonios</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Clothiaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Michel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.03980</idno>
		<title level="m">Swabha Swayamdipta, and Pengcheng Yin. 2017. Dynet: The dynamic neural network toolkit</title>
		<meeting><address><addrLine>Yusuke Oda, Matthew Richardson, Naomi Saphra</addrLine></address></meeting>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pointwise prediction for robust, adaptable japanese morphological analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yosuke</forename><surname>Nakata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinsuke</forename><surname>Mori</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P11-2093" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="529" to="533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="doi">10.3115/1073083.1073135</idno>
		<ptr target="https://doi.org/10.3115/1073083.1073135" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P16-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A mathematical theory of communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claude</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell System Technical Journal</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="379" to="423" />
			<date type="published" when="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Building a bilingual travel conversation database for speech translation research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiyuki</forename><surname>Takezawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2nd international workshop on East-Asian resources and evaluation conference on language resources and evaluation</title>
		<meeting>of the 2nd international workshop on East-Asian resources and evaluation conference on language resources and evaluation</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="17" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Error bounds for convolutional codes and an asymptotically optimum decoding algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Viterbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="260" to="269" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Human behavior and the principle of least effort</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">K</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zipf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1949" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
