<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Predict Distributions of Words Across Domains</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
							<email>danushka.bollegala@ liverpool.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Informatics University of Sussex Falmer</orgName>
								<orgName type="department" key="dep3">Department of Informatics University of Sussex Falmer</orgName>
								<orgName type="institution">University of Liverpool Liverpool</orgName>
								<address>
									<postCode>L69 3BX, BN1 9QJ, BN1 9QJ</postCode>
									<settlement>Brighton, Brighton</settlement>
									<country>UK, UK, UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weir</surname></persName>
							<email>d.j.weir@ sussex.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Informatics University of Sussex Falmer</orgName>
								<orgName type="department" key="dep3">Department of Informatics University of Sussex Falmer</orgName>
								<orgName type="institution">University of Liverpool Liverpool</orgName>
								<address>
									<postCode>L69 3BX, BN1 9QJ, BN1 9QJ</postCode>
									<settlement>Brighton, Brighton</settlement>
									<country>UK, UK, UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Carroll</surname></persName>
							<email>j.a.carroll@ sussex.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Informatics University of Sussex Falmer</orgName>
								<orgName type="department" key="dep3">Department of Informatics University of Sussex Falmer</orgName>
								<orgName type="institution">University of Liverpool Liverpool</orgName>
								<address>
									<postCode>L69 3BX, BN1 9QJ, BN1 9QJ</postCode>
									<settlement>Brighton, Brighton</settlement>
									<country>UK, UK, UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Predict Distributions of Words Across Domains</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="613" to="623"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Although the distributional hypothesis has been applied successfully in many natural language processing tasks, systems using distributional information have been limited to a single domain because the distribution of a word can vary between domains as the word&apos;s predominant meaning changes. However, if it were possible to predict how the distribution of a word changes from one domain to another , the predictions could be used to adapt a system trained in one domain to work in another. We propose an unsuper-vised method to predict the distribution of a word in one domain, given its distribution in another domain. We evaluate our method on two tasks: cross-domain part-of-speech tagging and cross-domain sentiment classification. In both tasks, our method significantly outperforms competitive baselines and returns results that are statistically comparable to current state-of-the-art methods, while requiring no task-specific customisations.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Distributional Hypothesis, summarised by the memorable line of <ref type="bibr" target="#b12">Firth (1957)</ref> -You shall know a word by the company it keeps -has inspired a diverse range of research in natural language pro- cessing. In such work, a word is represented by the distribution of other words that co-occur with it. Distributional representations of words have been successfully used in many language process- ing tasks such as entity set expansion ( <ref type="bibr" target="#b26">Pantel et al., 2009)</ref>, part-of-speech (POS) tagging and chunk- ing ( <ref type="bibr" target="#b16">Huang and Yates, 2009)</ref>, ontology learning <ref type="bibr" target="#b10">(Curran, 2005)</ref>, computing semantic textual sim- ilarity <ref type="bibr" target="#b2">(Besançon et al., 1999</ref>), and lexical infer- ence ( <ref type="bibr" target="#b18">Kotlerman et al., 2012</ref>).</p><p>However, the distribution of a word often varies from one domain 1 to another. For example, in the domain of portable computer reviews the word lightweight is often associated with positive sen- timent bearing words such as sleek or compact, whereas in the movie review domain the same word is often associated with negative sentiment- bearing words such as superficial or formulaic. Consequently, the distributional representations of the word lightweight will differ considerably be- tween the two domains. In this paper, given the distribution w S of a word w in the source domain S, we propose an unsupervised method for pre- dicting its distribution w T in a different target do- main T .</p><p>The ability to predict how the distribution of a word varies from one domain to another is vital for numerous adaptation tasks. For example, un- supervised cross-domain sentiment classification <ref type="bibr" target="#b4">(Blitzer et al., 2007;</ref><ref type="bibr" target="#b0">Aue and Gamon, 2005</ref>) in- volves using sentiment-labeled user reviews from the source domain, and unlabeled reviews from both the source and the target domains to learn a sentiment classifier for the target domain. Do- main adaptation (DA) of sentiment classification becomes extremely challenging when the distribu- tions of words in the source and the target domains are very different, because the features learnt from the source domain labeled reviews might not ap- pear in the target domain reviews that must be classified. By predicting the distribution of a word across different domains, we can find source do- main features that are similar to the features in target domain reviews, thereby reducing the mis- match of features between the two domains.</p><p>We propose a two-step unsupervised approach to predict the distribution of a word across do- mains. First, we create two lower dimensional la-tent feature spaces separately for the source and the target domains using Singular Value Decom- position (SVD). Second, we learn a mapping from the source domain latent feature space to the tar- get domain latent feature space using Partial Least Square Regression (PLSR). The SVD smoothing in the first step both reduces the data sparseness in distributional representations of individual words, as well as the dimensionality of the feature space, thereby enabling us to efficiently and accurately learn a prediction model using PLSR in the sec- ond step. Our proposed cross-domain word dis- tribution prediction method is unsupervised in the sense that it does not require any labeled data in either of the two steps.</p><p>Using two popular multi-domain datasets, we evaluate the proposed method in two prediction tasks: (a) predicting the POS of a word in a tar- get domain, and (b) predicting the sentiment of a review in a target domain. Without requiring any task specific customisations, systems based on our distribution prediction method significantly out- perform competitive baselines in both tasks. Be- cause our proposed distribution prediction method is unsupervised and task independent, it is poten- tially useful for a wide range of DA tasks such en- tity extraction <ref type="bibr" target="#b14">(Guo et al., 2009</ref>) or dependency parsing ( <ref type="bibr" target="#b21">McClosky et al., 2010)</ref>. Our contribu- tions are summarised as follows:</p><p>• Given the distribution w S of a word w in a source domain S, we propose a method for learning its distribution w T in a target do- main T .</p><p>• Using the learnt distribution prediction model, we propose a method to learn a cross- domain POS tagger.</p><p>• Using the learnt distribution prediction model, we propose a method to learn a cross- domain sentiment classifier.</p><p>To our knowledge, ours is the first successful at- tempt to learn a model that predicts the distribu- tion of a word across different domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Learning semantic representations for words us- ing documents from a single domain has received much attention lately <ref type="bibr" target="#b34">(Vincent et al., 2010;</ref><ref type="bibr" target="#b32">Socher et al., 2013;</ref><ref type="bibr" target="#b1">Baroni and Lenci, 2010)</ref>. As we have already discussed, the semantics of a word varies across different domains, and such variations are not captured by models that only learn a single se- mantic representation for a word using documents from a single domain.</p><p>The POS of a word is influenced both by its context (contextual bias), and the domain of the document in which it appears (lexical bias). For example, the word signal is predominately used as a noun in MEDLINE, whereas it appears pre- dominantly as an adjective in the Wall Street Jour- nal (WSJ) <ref type="bibr" target="#b3">(Blitzer et al., 2006</ref>). Consequently, a tagger trained on WSJ would incorrectly tag sig- nal in MEDLINE. <ref type="bibr" target="#b3">Blitzer et al. (2006)</ref> append the source domain labeled data with predicted piv- ots (i.e. words that appear in both the source and target domains) to adapt a POS tagger to a tar- get domain. <ref type="bibr" target="#b8">Choi and Palmer (2012)</ref> propose a cross-domain POS tagging method by training two separate models: a generalised model and a domain-specific model. At tagging time, a sen- tence is tagged by the model that is most similar to that sentence. <ref type="bibr" target="#b16">Huang and Yates (2009)</ref> train a Conditional Random Field (CRF) tagger with fea- tures retrieved from a smoothing model trained us- ing both source and target domain unlabeled data. Adding latent states to the smoothing model fur- ther improves the POS tagging accuracy <ref type="bibr" target="#b17">(Huang and Yates, 2012)</ref>. <ref type="bibr" target="#b31">Schnabel and Schütze (2013)</ref> propose a training set filtering method where they eliminate shorter words from the training data based on the intuition that longer words are more likely to be examples of productive linguistic pro- cesses than shorter words.</p><p>The sentiment of a word can vary from one do- main to another. In Structural Correspondence Learning (SCL) <ref type="bibr" target="#b3">(Blitzer et al., 2006;</ref><ref type="bibr" target="#b4">Blitzer et al., 2007</ref>), a set of pivots are chosen using point- wise mutual information. Linear predictors are then learnt to predict the occurrence of those piv- ots, and SVD is used to construct a lower dimen- sional representation in which a binary classifier is trained. Spectral Feature Alignment (SFA) ( <ref type="bibr" target="#b25">Pan et al., 2010</ref>) also uses pivots to compute an align- ment between domain specific and domain inde- pendent features. Spectral clustering is performed on a bipartite graph representing domain specific and domain independent features to find a lower- dimensional projection between the two sets of features. The cross-domain sentiment-sensitive thesaurus (SST) <ref type="bibr" target="#b5">(Bollegala et al., 2011</ref>) groups together words that express similar sentiments in different domains. The created thesaurus is used to expand feature vectors during train and test stages in a binary classifier. However, unlike our method, SCL, SFA, or SST do not learn a prediction model between word distributions across domains.</p><p>Prior knowledge of the sentiment of words, such as sentiment lexicons, has been incorporated into cross-domain sentiment classification. <ref type="bibr" target="#b15">He et al. (2011)</ref> propose a joint sentiment-topic model that imposes a sentiment-prior depending on the oc- currence of a word in a sentiment lexicon. Pono- mareva and Thelwall (2012) represent source and target domain reviews as nodes in a graph and ap- ply a label propagation algorithm to predict the sentiment labels for target domain reviews from the sentiment labels in source domain reviews. A sentiment lexicon is used to create features for a document. Although incorporation of prior senti- ment knowledge is a promising technique to im- prove accuracy in cross-domain sentiment classi- fication, it is complementary to our task of distri- bution prediction across domains.</p><p>The unsupervised DA setting that we consider does not assume the availability of labeled data for the target domain. However, if a small amount of labeled data is available for the target domain, it can be used to further improve the performance of DA tasks ( <ref type="bibr" target="#b37">Xiao et al., 2013;</ref><ref type="bibr" target="#b11">Daumé III, 2007</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Distribution Prediction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">In-domain Feature Vector Construction</head><p>Before we tackle the problem of learning a model to predict the distribution of a word across do- mains, we must first compute the distribution of a word from a single domain. For this purpose, we represent a word w using unigrams and bigrams that co-occur with w in a sentence as follows.</p><p>Given a document H, such as a user-review of a product, we split H into sentences, and lemma- tize each word in a sentence using the RASP sys- tem ( <ref type="bibr" target="#b6">Briscoe et al., 2006</ref>). Using a standard stop word list, we filter out frequent non-content un- igrams and select the remainder as unigram fea- tures to represent a sentence. Next, we generate bigrams of word lemmas and remove any bigrams that consists only of stop words. Bigram features capture negations more accurately than unigrams, and have been found to be useful for sentiment classification tasks. <ref type="table">Table 1</ref> shows the unigram and bigram features we extract for a sentence us- ing this procedure. Using data from a single do- sentence This is an interesting and well researched book unigrams this, is, an, interesting, and, well, researched, (surface) book unigrams this, be, an, interest, and, well, research, book (lemma) unigrams interest, well, research, book (features) bigrams this+be, be+an, an+interest, interest+and, (lemma) and+well, well+research, research+book bigrams an+interest, interest+and, and+well, (features) well+research, research+book <ref type="table">Table 1</ref>: Extracting unigram and bigram features.</p><p>main, we construct a feature co-occurrence ma- trix A in which columns correspond to unigram features and rows correspond to either unigram or bigram features. The value of the element a ij in the co-occurrence matrix A is set to the number of sentences in which the i-th and j-th features co- occur.</p><p>Typically, the number of unique bigrams is much larger than that of unigrams. Moreover, co- occurrences of bigrams are rare compared to co- occurrences of unigrams, and co-occurrences in- volving a unigram and a bigram. Consequently, in matrix A, we consider co-occurrences only be- tween unigrams vs. unigrams, and bigrams vs. unigrams. We consider each row in A as repre- senting the distribution of a feature (i.e. unigrams or bigrams) in a particular domain over the uni- gram features extracted from that domain (repre- sented by the columns of A). We apply Positive Pointwise Mutual Information (PPMI) to the co- occurrence matrix A. This is a variation of the Pointwise Mutual Information (PMI) <ref type="bibr" target="#b9">(Church and Hanks, 1990)</ref>, in which all PMI values that are less than zero are replaced with zero <ref type="bibr" target="#b19">(Lin, 1998;</ref><ref type="bibr" target="#b7">Bullinaria and Levy, 2007)</ref>. Let F be the matrix that results when PPMI is applied to A. Matrix F has the same number of rows, n r , and columns, n c , as the raw co-occurrence matrix A.</p><p>Note that in addition to the above-mentioned representation, there are many other ways to rep- resent the distribution of a word in a particular do- main ( <ref type="bibr" target="#b33">Turney and Pantel, 2010)</ref>. For example, one can limit the definition of co-occurrence to words that are linked by some dependency relation ( <ref type="bibr" target="#b23">Pado and Lapata, 2007)</ref>, or extend the window of co-occurrence to the entire document ( <ref type="bibr" target="#b1">Baroni and Lenci, 2010)</ref>. Since the method we propose in Section 3.2 to predict the distribution of a word across domains does not depend on the particular feature representation method, any of these alter- native methods could be used.</p><p>To reduce the dimensionality of the feature space, and create dense representations for words, we perform SVD on F. We use the left singu- lar vectors corresponding to the k largest singular values to compute a rank k approximationˆFapproximationˆ approximationˆF, of F. We perform truncated SVD using SVDLIBC 2 . Each row inˆFinˆ inˆF is considered as representing a word in a lower k (n c ) dimensional feature space cor- responding to a particular domain. Distribution prediction in this lower dimensional feature space is preferrable to prediction over the original fea- ture space because there are reductions in overfit- ting, feature sparseness, and the learning time. We created two matrices, ˆ F S andˆFandˆ andˆF T from the source and target domains, respectively, using the above mentioned procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Cross-Domain Feature Vector Prediction</head><p>We propose a method to learn a model that can predict the distribution w T of a word w in the target domain T , given its distribution w S in the source domain S. We denote the set of features that occur in both domains by W = {w (1) , . . . , w (n) }. In the literature, such features are often referred to as pivots, and they have been shown to be useful for DA, allowing the weights learnt to be transferred from one domain to an- other. Various criteria have been proposed for se- lecting a small set of pivots for DA, such as the mutual information of a word with the two do- mains ( <ref type="bibr" target="#b4">Blitzer et al., 2007</ref>). However, we do not impose any further restrictions on the set of pivots W other than that they occur in both domains.</p><p>For each word w (i) ∈ W, we denote the cor- responding rows inˆFinˆ inˆF S andˆFandˆ andˆF T by column vec- tors w T need not be equal, and we may select different numbers of singular vectors to approximatê F S andˆFandˆ andˆF T . We model distribu- tion prediction as a multivariate regression prob- lem where, given a set {(w</p><formula xml:id="formula_0">(i) S , w (i) T )} n i=1</formula><p>consist- ing of pairs of feature vectors selected from each domain for the pivots in W, we learn a mapping from the inputs (w   <ref type="bibr" target="#b36">(Wold, 1985)</ref> to learn a regression model using pairs of vectors. PLSR has been applied in</p><formula xml:id="formula_1">Input: X, Y, L. Output: Prediction matrix M. 1: Randomly select γ l from columns in Y l . 2: v l = X l γ l / X l γ l 3: λ l = X l v l 4: q l = Y l λ l / Y l λ l 5: γ l = Y l q l 6:</formula><p>If γ l is unchanged go to Line 7; otherwise go to Line 2 7: Chemometrics ( <ref type="bibr" target="#b13">Geladi and Kowalski, 1986)</ref>, pro- ducing stable prediction models even when the number of samples is considerably smaller than the dimensionality of the feature space. In particu- lar, PLSR fits a smaller number of latent variables (10 − 100 in practice) such that the correlation be- tween the feature vectors for pivots in the two do- mains are maximised in this latent space.</p><formula xml:id="formula_2">c l = λ l γ l / λ l γ l 8: p l = X l λ l /λ l λ l 9: X l+1 = X l − λ l p l and Y l+1 = Y l − c l λ l q l . 10:</formula><p>Let X and Y denote matrices formed by ar- ranging respectively the vectors w T in rows. PLSR decomposes X and Y into a series of products between rank 1 matrices as follows:</p><formula xml:id="formula_3">X ≈ L l=1 λ l p l = ΛP (1) Y ≈ L l=1 γ l q l = ΓQ .<label>(2)</label></formula><p>Here, λ l , γ l , p l , and q l are column vectors, and the summation is taken over the rank 1 matrices that result from the outer product of those vectors. The matrices, Λ, Γ, P, and Q are constructed re- spectively by arranging λ l , γ l , p l , and q l vectors as columns.</p><p>Our method for learning a distribution predic- tion model is shown in Algorithm 1. It is based on the two block NIPALS routine <ref type="bibr" target="#b35">(Wold, 1975;</ref><ref type="bibr" target="#b29">Rosipal and Kramer, 2006</ref>) and iteratively discovers L pairs of vectors (λ l , γ l ) such that the covariances, Cov(λ l , γ l ), are maximised under the constraint ||p l || = ||q l || = 1. Finally, the prediction matrix, M is computed using λ l , γ l , p l , q l . The predicted distributionˆwdistributionˆ distributionˆw T of a word w in T is given byˆw</p><formula xml:id="formula_4">byˆ byˆw T = Mw S .<label>(3)</label></formula><p>Our distribution prediction learning method is un- supervised in the sense that it does not require manually labeled data for a particular task from any of the domains. This is an important point, and means that the distribution prediction method is independent of the task to which it may subse- quently be applied. As we go on to show in Sec- tion 6, this enables us to use the same distribution prediction method for both POS tagging and sen- timent classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Domain Adaptation</head><p>The main reason that a model trained only on the source domain labeled data performs poorly in the target domain is the feature mismatch -few features in target domain test instances appear in source domain training instances. To overcome this problem, we use the proposed distribution pre- diction method to find those related features in the source domain that correspond to the features ap- pearing in the target domain test instances. We consider two DA tasks: (a) cross-domain POS tagging (Section 4.1), and (b) cross-domain sentiment classification (Section 4.2). Note that our proposed distribution prediction method can be applied to numerous other NLP tasks that in- volve sequence labelling and document classifica- tion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Cross-Domain POS Tagging</head><p>We represent each word using a set of features such as capitalisation (whether the first letter of the word is capitalised), numeric (whether the word contains digits), prefixes up to four letters, and suffixes up to four letters <ref type="bibr" target="#b22">(Miller et al., 2011</ref>). Next, for each word w in a source domain labeled (i.e. manually POS tagged) sentence, we select its neighbours u (i) in the source domain as additional features. Specifically, we measure the similarity, sim(u (i) S , w S ), between the source domain distri- butions of u (i) and w, and select the top r simi- lar neighbours u (i) for each word w as additional features for w. We refer to such features as dis- tributional features in this work. The value of a neighbour u (i) selected as a distributional feature is set to its similarity score sim(u (i) S , w S ). Next, we train a CRF model using all features (i.e. cap- italisation, numeric, prefixes, suffixes, and distri- butional features) on source domain labeled sen- tences.</p><p>We train a PLSR model, M, that predicts the target domain distribution Mu</p><formula xml:id="formula_5">(i)</formula><p>S of a word u (i) in the source domain labeled sentences, given its dis- tribution, u (i) S . At test time, for each word w that appears in a target domain test sentence, we mea- sure the similarity, sim(Mu (i) S , w T ), and select the most similar r words u (i) in the source domain labeled sentences as the distributional features for w, with their values set to sim(Mu (i) S , w T ). Fi- nally, the trained CRF model is applied to a target domain test sentence.</p><p>Note that distributional features are always se- lected from the source domain during both train and test times, thereby increasing the number of overlapping features between the trained model and test sentences. To make the inference tractable and efficient, we use a first-order Markov factori- sation, in which we consider all pairwise combi- nations between the features for the current word and its immediate predecessor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Cross-Domain Sentiment Classification</head><p>Unlike in POS tagging, where we must individ- ually tag each word in a target domain test sen- tence, in sentiment classification we must classify the sentiment for the entire review. We modify the DA method presented in Section 4.1 to satisfy this requirement as follows.</p><p>Let us assume that we are given a set {(x</p><formula xml:id="formula_6">(i) S , y (i) )} n i=1 of n labeled reviews x (i)</formula><p>S for the source domain S. For simplicity, let us consider binary sentiment classification where each review x (i) is labeled either as positive (i.e. y (i) = 1) or negative (i.e. y (i) = −1). Our cross-domain bi- nary sentiment classification method can be easily extended to the multi-class setting as well. First, we lemmatise each word in a source domain la- beled review x (i) S , and extract both unigrams and bigrams as features to represent x (i) S by a binary- valued feature vector. Next, we train a binary clas- sification model, θ, using those feature vectors. Any binary classification algorithm can be used to learn θ. In our experiments, we used L2 reg- ularised logistic regression.</p><p>Next, we train a PLSR model, M, as described in Section 3.2 using unlabeled reviews in the source and target domains. At test time, we rep- resent a test target review H using a binary-valued feature vector h of unigrams and bigrams of lem- mas of the words in H, as we did for source do- main labeled train reviews. Next, for each feature w (j) extracted from H, we measure the similarity,</p><formula xml:id="formula_7">sim(Mu (i) S , w (j)</formula><p>T ), between the target domain dis- tribution of w (j) , and each feature (unigram or bi- gram) u (i) in the source domain labeled reviews. We score each source domain feature u (i) for its relatedness to H using the formula:</p><formula xml:id="formula_8">score(u (i) , H) = 1 |H| |H| j=1 sim(Mu (i) S , w (j) T ) (4)</formula><p>where |H| denotes the total number of features ex- tracted from the test review H. We select the top scoring r features u (i) as distributional features for H, and append those to h. The corresponding val- ues of those distributional features are set to the scores given by Equation 4. Finally, we classify h using the trained binary classifier θ. Note that given a test review, we find the distributional fea- tures that are similar to all the words in the test re- view from the source domain. In particular, we do not find distributional features independently for each word in the test review. This enables us to find distributional features that are consistent with all the features in a test review.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model Choices</head><p>For both POS tagging and sentiment classifica- tion, we experimented with several alternative approaches for feature weighting, representation, and similarity measures using development data, which we randomly selected from the training in- stances from the datasets described in Section 5. For feature weighting for sentiment classifica- tion, we considered using the number of occur- rences of a feature in a review and tf-idf weight- ing <ref type="bibr" target="#b30">(Salton and Buckley, 1983)</ref>. For representa- tion, we considered distributional features u (i) in descending order of their scores given by Equa- tion 4, and then taking the inverse-rank as the val- ues for the distributional features <ref type="bibr" target="#b5">(Bollegala et al., 2011</ref>). However, none of these alternatives re- sulted in performance gains. With respect to simi- larity measures, we experimented with cosine sim- ilarity and the similarity measure proposed by <ref type="bibr" target="#b19">Lin (1998)</ref>; cosine similarity performed consistently well over all the experimental settings. The feature representation was held fixed during these similar- ity measure comparisons.</p><p>For POS tagging, we measured the effect of varying r, the number of distributional features, using a development dataset. We observed that setting r larger than 10 did not result in signifi- cant improvements in tagging accuracy, but only increased the train time due to the larger feature space. Consequently, we set r = 10 in POS tag- ging. For sentiment analysis, we used all features in the source domain labeled reviews as distri- butional features, weighted by their scores given by Equation 4, taking the inverse-rank. In both tasks, we parallelised similarity computations us- ing BLAS 3 level-3 routines to speed up the com- putations. The source code of our implementation is publicly available 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Datasets</head><p>To evaluate DA for POS tagging, following <ref type="bibr" target="#b3">Blitzer et al. (2006)</ref>, we use sections 2 − 21 from Wall Street Journal (WSJ) as the source domain labeled data. An additional 100, 000 WSJ sentences from the 1988 release of the WSJ corpus are used as the source domain unlabeled data. Following <ref type="bibr" target="#b31">Schnabel and Schütze (2013)</ref>, we use the POS labeled sentences in the SACNL dataset <ref type="bibr" target="#b27">(Petrov and McDonald, 2012</ref>) for the five target domains: QA fo- rums, Emails, Newsgroups, Reviews, and Blogs. Each target domain contains around 1000 POS labeled test sentences and around 100, 000 unla- beled sentences.</p><p>To evaluate DA for sentiment classification, we use the Amazon product reviews collected by <ref type="bibr" target="#b4">Blitzer et al. (2007)</ref> for four different product cat- egories: books (B), DVDs (D), electronic items (E), and kitchen appliances (K). There are 1000 positive and 1000 negative sentiment labeled re- views for each domain. Moreover, each domain has on average 17, 547 unlabeled reviews. We use the standard split of 800 positive and 800 negative labeled reviews from each domain as training data, and the remainder for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments and Results</head><p>For each domain D in the SANCL (POS tag- ging) and Amazon review (sentiment classifica- tion) datasets, we create a PPMI weighted co- occurrence matrix F D . On average, F D created for a target domain in the SANCL dataset con- tains 104, 598 rows and 65, 528 columns, whereas those numbers in the Amazon dataset are <ref type="bibr">27, 397 and 35, 200</ref> respectively. In cross-domain senti- ment classification, we measure the binary senti- ment classification accuracy for the target domain test reviews for each pair of domains (12 pairs in total for 4 domains). On average, we have 40, 176 pivots for a pair of domains in the Amazon dataset.</p><p>In cross-domain POS tagging, WSJ is always the source domain, whereas the five domains in SANCL dataset are considered as the target do- mains. For this setting we have 9822 pivots on average. The number of singular vectors k se- lected in SVD, and the number of PLSR dimen- sions L are set respectively to 1000 and 50 for the remainder of the experiments described in the pa- per. Later we study the effect of those two param- eters on the performance of the proposed method. The L-BFGS ( <ref type="bibr" target="#b20">Liu and Nocedal, 1989)</ref> method is used to train the CRF and logistic regression mod- els. <ref type="table" target="#tab_2">Table 2</ref> shows the token-level POS tagging accu- racy for unseen words (i.e. words that appear in the target domain test sentences but not in the source domain labeled train sentences). By limiting the evaluation to unseen words instead of all words, we can evaluate the gain in POS tagging accuracy solely due to DA. The NA (no-adapt) baseline sim- ulates the effect of not performing any DA. Specif- ically, in POS tagging, a CRF trained on source domain labeled sentences is applied to target do- main test sentences, whereas in sentiment classi- fication, a logistic regression classifier trained us- ing source domain labeled reviews is applied to the target domain test reviews. The S pred baseline di- rectly uses the source domain distributions for the words instead of projecting them to the target do- main. This is equivalent to setting the prediction matrix M to the unit matrix. The T pred baseline uses the target domain distribution w T for a word w instead of Mw S . If w does not appear in the target domain, then w T is set to the zero vector. The S pred and T pred baselines simulate the two al- ternatives of using source and target domain dis- tributions instead of learning a PLSR model. The DA method proposed in Section 4.1 is shown as the Proposed method. Filter denotes the train- ing set filtering method proposed by <ref type="bibr" target="#b31">Schnabel and Schütze (2013)</ref> for the DA of POS taggers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">POS Tagging Results</head><p>From <ref type="table" target="#tab_2">Table 2</ref>, we see that the Proposed method achieves the best performance in all five domains, followed by the T pred baseline. Recall that the T pred baseline cannot find source domain words that do not appear in the target domain as distri-  butional features for the words in the target do- main test reviews. Therefore, when the overlap be- tween the vocabularies used in the source and the target domains is small, T pred cannot reduce the mismatch between the feature spaces. Poor perfor- mance of the S pred baseline shows that the distri- butions of a word in the source and target domains are different to the extent that the distributional features found using source domain distributions are inadequate. The two baselines S pred and T pred collectively motivate our proposal to learn a distri- bution prediction model from the source domain to the target. The improvements of Proposed over the previously proposed Filter are statistically sig- nificant in all domains except the Emails domain (denoted by † in <ref type="table" target="#tab_2">Table 2</ref> according to the Bino- mial exact test at 95% confidence). However, the differences between the T pred and Proposed meth- ods are not statistically significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Sentiment Classification Results</head><p>In <ref type="figure" target="#fig_4">Figure 1</ref>, we compare the Proposed cross- domain sentiment classification method (Section 4.2) against several baselines and the current state- of-the-art methods. The baselines NA, S pred , and T pred are defined similarly as in Section 6.1. SST is the Sentiment Sensitive Thesaurus proposed by <ref type="bibr" target="#b5">Bollegala et al. (2011)</ref>. SST creates a single distri- bution for a word using both source and target do- main reviews, instead of two separate distributions as done by the Proposed method. SCL denotes the Structural Correspondence Learning method proposed by <ref type="bibr" target="#b3">Blitzer et al. (2006)</ref>. SFA denotes the Spectral Feature Alignment method proposed by <ref type="bibr" target="#b25">Pan et al. (2010)</ref>. SFA and SCL represent the current state-of-the-art methods for cross-domain sentiment classification. All methods are evalu- ated under the same settings, including train/test split, feature spaces, pivots, and classification al- gorithms so that any differences in performance can be directly attributable to their domain adapt- ability. For each domain, the accuracy obtained by a classifier trained using labeled data from that domain is indicated by a solid horizontal line in each sub-figure. This upper baseline represents the classification accuracy we could hope to obtain if we were to have labeled data for the target do- main. Clopper-Pearson 95% binomial confidence intervals are superimposed on each vertical bar. From <ref type="figure" target="#fig_4">Figure 1</ref> we see that the Proposed method reports the best results in 8 out of the 12 domain pairs, whereas SCL, SFA, and S pred report the best results in other cases. Except for the D-E set- ting in which Proposed method significantly out- performs both SFA and SCL, the performance of the Proposed method is not statistically signifi- cantly different to that of SFA or SCL.</p><p>The selection of pivots is vital to the perfor- mance of SFA. However, unlike SFA, which re- quires us to carefully select a small subset of pivots (ca. less than 500) using some heuristic approach, our Proposed method does not require any pivot selection. Moreover, SFA projects source domain reviews to a lower-dimensional latent space, in which a binary sentiment classifier is subsequently trained. At test time SFA projects a target review into this lower-dimensional latent space and ap- plies the trained classifier. In contrast, our Pro- posed method predicts the distribution of a word in the target domain, given its distribution in the source domain, thereby explicitly translating the source domain reviews to the target. This property enables us to apply the proposed distribution pre- diction method to tasks other than sentiment anal- ysis such as POS tagging where we must identify distributional features for individual words. Unlike our distribution prediction method, which is unsupervised, SST requires labeled data for the source domain to learn a feature mapping between a source and a target domain in the form of a thesaurus. However, from <ref type="figure" target="#fig_4">Figure 1</ref> we see that in 10 out of the 12 domain-pairs the Proposed method returns higher accuracies than SST.</p><p>To evaluate the overall effect of the number of singular vectors k used in the SVD step, and the number of PLSR components L used in Algorithm 1, we conduct two experiments. To evaluate the ef- fect of the PLSR dimensions, we fixed k = 1000 and measured the cross-domain sentiment classi- fication accuracy over a range of L values. As shown in <ref type="figure" target="#fig_5">Figure 2,</ref>    ues in practice.</p><p>To evaluate the effect of the SVD dimensions, we fixed L = 100 and measured the cross-domain sentiment classification accuracy for different k values as shown in <ref type="figure">Figure 3</ref>. We see an overall decrease in classification accuracy when k is in- creased. Because the dimensionality of the source and target domain feature spaces is equal to k, the complexity of the least square regression problem increases with k. Therefore, larger k values result in overfitting to the train data and classification ac- curacy is reduced on the target test data.</p><p>As an example of the distribution prediction method, in <ref type="table" target="#tab_4">Table 3</ref> we show the top 3 similar distributional features u in the books (source) do- main, predicted for the electronics (target) domain word w = lightweight, by different similarity measures. Bigrams are indicted by a + sign and the similarity scores of the distributional features are shown within brackets.</p><p>Using the source domain distributions for both u and w (i.e. sim(u S , w S )) produces distribu- tional features that are specific to the books do- main, or to the dominant adjectival sense of hav- ing no importance or influence. On the other hand, using target domain distributions for u and w (i.e. sim(u T , w T )) returns distributional fea- tures of the dominant nominal sense of lower in weight frequently associated with electronic de- vices. Simply using source domain distributions u S (i.e. sim(u S , w T )) returns totally unrelated dis- tributional features. This shows that word distribu- tions in source and target domains are very differ- ent and some adaptation is required prior to com- puting distributional features.</p><p>Interestingly, we see that by using the dis- tributions predicted by the proposed method (i.e. sim(Mu S , w T )) we overcome this problem and find relevant distributional features from the source domain. Although for illustrative purposes we used the word lightweight, which occurs in both the source and the target domains, our pro- posed method does not require the source domain distribution w S for a word w in a target domain document. Therefore, it can find distributional fea- tures even for words occurring only in the target domain, thereby reducing the feature mismatch between the two domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We proposed a method to predict the distribution of a word across domains. We first create a distri- butional representation for a word using the data from a single domain, and then learn a Partial Least Square Regression (PLSR) model to pre- dict the distribution of a word in a target domain given its distribution in a source domain. We eval- uated the proposed method in two domain adapta- tion tasks: cross-domain POS tagging and cross- domain sentiment classification. Our experiments show that without requiring any task-specific cus- tomisations to our distribution prediction method, it outperforms competitive baselines and achieves comparable results to the current state-of-the-art domain adaptation methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Stop if l = L; otherwise l = l + 1 and return to Line 1. 11: Let C = diag(c1, . . . , cL), and V = [v1 . . . vL] 12: M = V(P V) −1 CQ 13: return M</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Cross-Domain sentiment classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The effect of PLSR dimensions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Figure 3: The effect of SVD dimensions. Measure Distributional features sim(uS , wS ) thin (0.1733), digestible (0.1728), small+print (0.1722) sim(uT , wT ) travel+companion (0.6018), snap-in (0.6010), touchpad (0.6016) sim(uS , wT ) segregation (0.1538), participation (0.1512), depression+era (0.1508) sim(MuS , wT ) small (0.2794), compact (0.2641), sturdy (0.2561)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>POS tagging accuracies on SANCL. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Top 3 distributional features u ∈ S for 
the word lightweight (w). 

</table></figure>

			<note place="foot" n="1"> In this paper, we use the term domain to refer to a collection of documents about a particular topic, for example reviews of a particular kind of product.</note>

			<note place="foot" n="2"> http://tedlab.mit.edu/ ˜ dr/SVDLIBC/ Algorithm 1 Learning a prediction model.</note>

			<note place="foot" n="3"> http://www.openblas.net/ 4 http://www.csc.liv.ac.uk/ ˜ danushka/ software.html</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Customizing sentiment classifiers to new domains: a case study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Aue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<pubPlace>Microsoft Research</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Distributional memory: A general framework for corpus-based semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="673" to="721" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Textual similarities based on a 621 distributional approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romaric</forename><surname>Besançon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Rajman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Cédric</forename><surname>Chappelier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of DEXA</title>
		<meeting>of DEXA</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="180" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Domain adaptation with structural correspondence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="120" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="440" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Using multiple sources to construct a sentiment sensitive thesaurus for cross-domain sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL/HLT</title>
		<meeting>of ACL/HLT</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="132" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The second release of the RASP system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Watson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING/ACL Interactive Presentation Sessions</title>
		<meeting>of COLING/ACL Interactive Presentation Sessions</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Extracting semantic representations from word cooccurrence statistics: A computational study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">A</forename><surname>Bullinaria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jospeh</forename><forename type="middle">P</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="510" to="526" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast and robust part-of-speech tagging using dynamic model selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jinho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL Short Papers</title>
		<meeting>of ACL Short Papers</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="363" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Word association norms, mutual information, and lexicography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hanks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="29" />
			<date type="published" when="1990-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Supersense tagging of unknown nouns using semantic similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Curran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="26" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="256" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A synopsis of linguistic theory 1930-55</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Firth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Studies in Linguistic Analysis</title>
		<imprint>
			<date type="published" when="1957" />
			<biblScope unit="page" from="1" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Partial least-squares regression: a tutorial</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Geladi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><forename type="middle">R</forename><surname>Kowalski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Analytica Chimica Acta</title>
		<imprint>
			<biblScope unit="volume">185</biblScope>
			<biblScope unit="issue">0</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Domain adaptation with latent semantic association for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijia</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhili</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="281" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatically extracting polarity-bearing topics for cross-domain sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenghua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harith</forename><surname>Alani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL/HLT</title>
		<meeting>of ACL/HLT</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="123" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributional representations for handling sparsity in supervised sequence-labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-IJCNLP&apos;09</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="495" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Biased representation learning for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP/CoNLL</title>
		<meeting>of EMNLP/CoNLL</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1313" to="1323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Directional distributional similarity for lexical inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Kotlerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Szpektor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maayan Zhitomirsky-Geffet</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="359" to="389" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic retrieval and clustering of similar words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="768" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On the limited memory BFGS method for large scale optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nocedal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="503" to="528" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automatic domain adaptation for parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL/HLT</title>
		<meeting>of NAACL/HLT</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="28" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Building domain-specific taggers without annotated (domain) data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">E</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vijay-Shanker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP/CoNLL</title>
		<meeting>of EMNLP/CoNLL</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1103" to="1111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Pado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dependency-based construction of semantic space models</title>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="161" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cross-domain sentiment classification via spectral feature alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochuan</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Tao</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WWW</title>
		<meeting>of WWW</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="751" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Web-scale distributional similarity and entity set expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Crestan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arkady</forename><surname>Borkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anamaria</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishnu</forename><surname>Vyas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="938" to="947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Overview of the 2012 shared task on parsing the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Notes of the 1st SANCL Workshop</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Do neighbours help? an exploration of graph-based algorithms for cross-domain sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Ponomareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Thelwall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="655" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Overview and recent advances in partial least squares</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Rosipal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicole</forename><surname>Kramer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SLSFS&apos;05</title>
		<editor>C. Saunders et al.</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>SpringerVerlag</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">3940</biblScope>
			<biblScope unit="page" from="34" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Introduction to Modern Information Retreival</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983" />
			<publisher>McGraw-Hill Book Company</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Towards robust cross-domain domain adaptation for part-ofspeech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCNLP</title>
		<meeting>of IJCNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="198" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">From frequency to meaning: Vector space models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Aritificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antonie</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Path models with latent variables: the NIPALS approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herman</forename><surname>Wold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">editor, Quantitative socialogy: international perspective on mathematical and statistical modeling</title>
		<editor>H. M. Blalock et al.,</editor>
		<imprint>
			<publisher>Academic</publisher>
			<date type="published" when="1975" />
			<biblScope unit="page" from="307" to="357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Partial least squares</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herman</forename><surname>Wold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Encyclopedia of the Statistical Sciences</title>
		<editor>Samel Kotz and Norman L. Johnson</editor>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1985" />
			<biblScope unit="page" from="581" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning latent word representations for domain adaptation using supervised word clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feipeng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="152" to="162" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
