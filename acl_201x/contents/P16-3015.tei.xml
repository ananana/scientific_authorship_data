<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">From Extractive to Abstractive Summarization: A Journey</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parth</forename><surname>Mehta</surname></persName>
							<email>parth me@daiict.ac.in</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Information Retrieval and Language Processing Lab Dhirubhai Ambani Institute of Information and Communication Technology Gandhinagar</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">From Extractive to Abstractive Summarization: A Journey</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics-Student Research Workshop</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics-Student Research Workshop <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="100" to="106"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The availability of large document-summary corpora have opened up new possibilities for using statistical text generation techniques for abstractive summarization. Progress in Extractive text summarization has become stagnant for a while now and in this work we compare the two possible alternates to it. We present an argument in favor of abstractive summarization compared to an ensemble of extractive techniques. Further we explore the possibility of using statistical machine translation as a generative text summarization technique and present possible research questions in this direction. We also report our initial findings and future direction of research.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Motivation for proposed research</head><p>Extractive techniques of text summarization have long been the primary focus of research compared to abstractive techniques. But recent reports tend to suggest that advances in extractive text summa- rization have slowed down in the past few years ( <ref type="bibr" target="#b9">Nenkova and McKeown, 2012</ref>). Only marginal improvements are being reported over previous techniques, and more often than not these seem to be a result of variation in the parameters used during evaluation using ROUGE, and some times due to other factors like a better redundancy re- moval module (generally used after the sentences are ranked according to their importance) rather than the actual algorithm. Overall it seems that the current state of the art techniques for extractive summarization have more or less achieved their peak performance and only some small improve- ments can be further achieved. In such a scenario there seem to be two possible directions of fur- ther research. One approach that could be used is making an ensemble of these techniques which might prove to be better than the individual meth- ods. The other option is to focus on abstractive techniques instead.</p><p>A large number of extractive summarization techniques have been developed in the past decade especially after the advent of conferences like Document Understanding Conference (DUC) <ref type="bibr">1</ref> and Text Analysis Conference (TAC) 2 . But very few inquiries have been made as to how these differ from each other and what are the salient features on some which are absent in others. ( <ref type="bibr" target="#b4">Hong et al., 2014</ref>) is first such attempt to com- pare summaries beyond merely comparing the ROUGE <ref type="bibr" target="#b7">(Lin, 2004</ref>) scores. They show that many systems, although having a similar ROUGE score indeed have very different content and have lit- tle overlap among themselves. This difference, at least theoretically, opens up a possibility of com- bining these summaries at various levels, like fus- ing rank lists( <ref type="bibr" target="#b10">Wang and Li, 2012)</ref>, choosing the best combination of sentences from several sum- maries( <ref type="bibr" target="#b5">Hong et al., 2015)</ref> or using learning-to- rank techniques to generate rank lists of sentences and then choosing the top-k sentences as a sum- mary, to get a better result. In the next section we report our initial experiments and show that a meaningful ensemble of these techniques can help in improving the coverage of existing techniques. But such a scenario is not always guaranteed, as shown in the next section, and given that such fu- sion techniques do have a upper bound to the ex- tent to which they can improve the summarization performance as shown by <ref type="bibr" target="#b5">(Hong et al., 2015)</ref>, an ensemble approach would be of limited interest.</p><p>Keeping this in mind we plan to focus on both approaches for abstractive text summariza- tion, those that depend on initial extractive sum- mary and those that do not (text generation ap- proach). Also availability of large document- summary corpora, as we discuss in section 3, has opened up new possibilities for applying statistical text generation approaches to summarization. In the next section we present a brief overview of the initial experiments that we have performed with an ensemble of extractive techniques. In section 3 we then propose further research directions using the generative approach towards text summariza- tion. In the final section we present some prelim- inary results of summarizing documents using a machine translation system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Fusion of Summarization systems</head><p>In this section we report some of our experiments with fusion techniques for combining extractive summarization systems. For the first experiment we consider five basic techniques mentioned in ( <ref type="bibr" target="#b4">Hong et al., 2014</ref>) for the simple reason that they are tested extensively and are simple yet effective. These systems include LexRank, the much pop- ular graph based summarization technique( <ref type="bibr" target="#b1">Erkan and Radev, 2004</ref>), and Greedy-KL( <ref type="bibr" target="#b3">Haghighi and Vanderwende, 2009)</ref>, which iteratively chooses the sentence that has least KL-divergence com- pared to the remaining document. Other systems are FreqSum, a word frequency based system, and TsSum, which uses topic signatures computed by comparing the documents to a background corpus. A Centroid based technique finds the sentences most similar to the document based on cosine sim- ilarity. We also combine the rank lists from these systems using the Borda count <ref type="bibr">3</ref>  We evaluated the techniques based on ROUGE-1, ROUGE-2 and ROUGE-4 Recall <ref type="bibr" target="#b7">(Lin, 2004</ref>) using the parameters mentioned in ( <ref type="bibr" target="#b4">Hong et al., 2014</ref>). We report only ROUGE-1 results due to space constraints. We also computed Average- Rank for each system. Average-Rank indicates the average number of systems that the given sys- tem outperformed. The higher the average-rank the more consistent a given system. When systems are ranked based on ROUGE-1 metric, both Borda and Reciprocal Rank perform better than four of the five systems but couldn't beat the Greedy- KL method. Both combination techniques outper- formed all five methods when systems are ranked based on ROUGE-2 and ROUGE-4. Even in case where Borda and Reciprocal Rank did outperform all the other systems, the increase in ROUGE scores were negligible. These results are con- trary to what has been reported previously ( <ref type="bibr" target="#b10">Wang and Li, 2012</ref>) as neither of the fusion techniques performed significantly better than the candidate systems. The only noticeable improvement in all cases was in the Average-Rank. The combined systems were more consistent than the individual systems. These results indicate that Fusion can at least help us in improving the consistency of the meta-system.</p><p>One clear trend we observed was that not all combinations performed poorly, and summaries from certain techniques when fused together per- formed well (on both ROUGE score and consis- tency). To further investigate this issue we con- ducted another experiment where we try to make an informed fusion of various extractive tech- niques.</p><p>Due to space constraints we report results only on two families of summarization techniques: one is a graph based iterative method as suggested in ( <ref type="bibr" target="#b1">Erkan and Radev, 2004)</ref> and <ref type="bibr" target="#b8">(Mihalcea and Tarau, 2004</ref>) and the other is the 'Greedy approach' where we greedily add a sentence that is most sim- ilar to the entire document, remove the sentence from the document and repeat the process until we have the desired number of sentences. We then chose three commonly used sentence similar- ity measures: Cosine similarity, Word overlap and KL-Divergence. Several other similar approaches are possible, for example TsSum and FreqSum are related in the sense that each method rates a sentence based on the average number of impor- tant words in it, the difference being in the way in which importance of the word is computed.</p><p>We perform this experiment in a very constrained manner and leave it to the future experimenting with other such possible combinations.  We generate summaries using all the possible 6 combinations of two approaches and three sen- tence similarity metrics. We then combine the summaries resulting from a particular sentence similarity metric or from a particular sentence ranking algorithm. The results in <ref type="table" target="#tab_2">table 2</ref> show that techniques that have a similar ranking algo- rithm but use different sentence similarity metrics, when combined produce an aggregate summary whose coverage is much better than the original summary. The aggregate summaries from the sys- tems that have different ranking algorithm but the same sentence similarity measure do not beat the best performing system. Figures in bold indicate the maximum score for that particular approach. We have tested this for several other ranking algo- rithms like centroid based and LSA based and sen- tence similarity measures. The hypothesis holds in most cases. We consider this experiment to be in- dicative of a future direction of research and do not consider it in any way to be conclusive. But it definitely indicates the difficulties that might be encountered when attempting to fuse summaries from different sources compared to the limited im- provement in the coverage (ROUGE scores). This combined with availability of a larger training set of document-summary pairs, which enables us to use several text generation approaches, is our prin- ciple motivation behind the proposed research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Greedy Borda</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Abstractive Summarization</head><p>Abstractive Summarization covers techniques which can generate summaries by rewriting the content in a given text, rather than simply extract- ing important sentences from it. But most of the current abstractive summarization techniques still use sentence extraction as a first step for abstract generation. In most cases, extractive summaries reach their limitation primarily because only a part of every sentence selected is informative and the other part is redundant. Abstractive techniques try to tackle this issue by either dropping the re- dundant part altogether or fusing two similar sen- tences in such a way as to maximize the informa- tion content and minimize the sentence lengths. We discuss some experiments we plan to do in this direction. An alternative to this technique is what is known as the Generative approach for text summarization. These techniques extract concepts (instead of sentences or phrases) from the given text and generate new sentences using those con- cepts and the relationships between them. We pro- pose a novel approach of using statistical machine translation for document summarization. We dis- cuss the possibilities of exploiting Statistical ma- chine translation techniques, which in themselves are generative techniques and have a sound math- ematical formulation, for translating a text in Doc- ument Language to Summary Language. In this section we highlight the research questions we are trying to address and issues that we might face in doing so. We also mention another approach we would like to explore which uses topic modeling for generating summaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sentence Fusion</head><p>Most abstractive summarization techniques rely on sentence fusion to remove redundancy and create a new concise sentence. Graph based techniques similar to ( <ref type="bibr" target="#b2">Ganesan et al., 2010)</ref> and ( <ref type="bibr" target="#b0">Banerjee et al., 2015</ref>) have become very popu- lar recently. These techniques rely on extractive summarization to get important sentences, clus- ter lexically similar sentences together, create a word graph from this cluster and try to generate a new meaningful sentence by selecting a best suited path from this word graph. Several factors like the linguistic quality of the sentence, informativeness, length of the sentence are considered when select- ing an appropriate path form the word graph.</p><p>Informativeness of the selected path can be de- fined in several ways, and the choice defines how good my summary would be (at least when using ROUGE as a evaluation measure). In one of our experiments we changed the informativeness cri- teria from TextRank scores of words as used in the original approach in ( <ref type="bibr" target="#b0">Banerjee et al., 2015)</ref> to Log- Likelihood ratio of the words compared to a large background corpus as suggested in <ref type="bibr" target="#b6">(Lin and Hovy, 2000</ref>). We observed that changing measure of in- formativeness produces a dramatic change in the quality of the summaries. We would like to con- tinue working in this direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Summarization as a SMT problem</head><p>The idea is to model the text summarization prob- lem as a Statistical Machine Translation (SMT) problem of translating text written in a Docu- ment Language to that in a Summary Language. Machine translation techniques have well defined and well accepted generative models which have been researched extensively over more than two decades. At least on the surface, the idea of model- ing a text summarization problem as that of trans- lation between two pairs of texts might enable us to leverage this progress in the field of SMT and extend it to abstractive text summarization, albeit with several modifications. We expect this area to be our primary research focus. While a simi- lar approach has been used in the case of Question Answering ( <ref type="bibr" target="#b11">Zhang et al., 2014)</ref>, to the best of our knowledge it has not yet been used for Document Summarization.</p><p>While the idea seems very intuitive and appeal- ing, there are several roadblocks to it. The first and perhaps the biggest issue has been the lack of availability of a large training corpus. Tradition- ally SMT systems have depended on large vol- umes of parallel texts that are used to learn the phrase level alignment between sentences from two languages and the probability with which a particular phrase in the source language might be translated to another in the target language. The Text Summarization community on the other hand has relied on more linguistic approaches or sta- tistical approaches which use limited amount of training data. Most of the evaluation benchmark datasets generated by conferences like DUC or TAC are limited to less than a hundred Document- Summary pairs and the focus has mainly been on short summaries of very few sentences. This makes the available data too small (especially when considering the number of sentences).</p><p>We hope to solve this problem partially using the Supreme Court Judgments dataset released by the organizers of Information Access in Legal Do- main Track 4 at FIRE 2014. The dataset has 1500 Judgments with a corresponding summary known as a headnote, manually written by legal experts. The organizers released another dataset of addi-tional 10,000 judgment-headnote pairs from the Supreme court of India spread over four decades, that are noisy and need to be curated. The average judgment length is 150 sentences while a head- note is 30 sentence long on an average. Using this we can create a parallel corpus of approximately 45,000 sentences using the clean data, and an ad- ditional 300,000 sentences after curating the entire dataset. This is comparable to the size of standard datasets used for training SMT systems.</p><p>Given this data is only semi-parallel and aligned at document level and not at sentence level, the next issue is extracting pairs of source sentence and target sentence. The exception being that both the source sentence and target sentence can actu- ally be several sentences instead of a single sen- tence, the possibility being higher in case of the source than the target. This might seem to be a classic example of the problem of extracting par- allel sentences from a comparable corpus. But there are several important differences, the biggest one being that it is almost guaranteed that several sentences from the text written in Document Lan- guage will map to a single sentence in the Sum- mary Language. This itself makes this task more challenging compared to the already daunting task of finding parallel sentences in a comparable cor- pora. Another notable difference is that unlike in case of SMT, the headnotes (or the Summary Lan- guage) are influenced a lot by the stylistic qual- ity of its author. The nature of headnotes seems to vary to a large extent over the period of four decades, and we are in the process of trying to fig- ure out how this affects the sentence alignment as well as the overall translation process. The other major difference can actually be used as leverage to improve the quality of sentence level alignment. The headnotes tend to follow a general format, in the sense that there are certain points about the Court judgment that should always occur in the headnote and certain phrases or certain types of sentences are always bound to be excluded. How to leverage this information is one of the research questions we plan to address in the proposed work.</p><p>Another issue that we plan to address in par- ticular is how to handle the mismatch between lengths of a sentence (i.e. multiple sentences con- sidered to be a single sentence) in the Document Language when compared to the Summary Lan- guage. Currently two different languages do vary in the average sentence lengths, for example Ger-man sentences are in general longer than English. But in our case the ratio of sentence lengths would be almost 3:1 with the Document Language be- ing much longer than their Summary Language counterparts. While most current translation mod- els do have a provision for a penalty on sentence lengths which can make the target sentence longer or shorter, the real challenge lies in finding phrase level alignments when either the source sentence or the target sentence is too long compared to the other. This leads to a large number of phrases hav- ing no alignment at all which is not a common phenomenon in cases of SMT.</p><p>In effect we propose to address the following research questions:</p><p>• Exploring the major challenges that one might face when modeling Summarization as a Machine translation problem ? • How to create a sentence aligned parallel cor- pus from a given document and its handwrit- ten summary ? • How to handle the disparity in lengths of sen- tence of Document Language and Summary Language ? • How to reduce the sparsity in training data created due to the stylistic differences present within the Documents and Summaries ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Topic model based sentence generation</head><p>The graph based approaches of sentence fusion mentioned above assumes availability of a num- ber of similar sentences from which a word graph can be formed. It might not always be easy to get such similar sentences, especially in case of sin- gle document summarization. We wish to explore the possibility of using topic modeling to extract informative phrases and entities and then use stan- dard sentence generation techniques to generate representative sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Preliminary experiment</head><p>We would like to conclude by reporting results of a very preliminary experiment wherein we used simple cosine similarity to align sentences be- tween the original Judgments and the manually generated headnotes (summaries). For a small training set of 1000 document-summary pairs, we compute the cosine similarity of each sentence in the judgment to each sentence in the correspond- ing headnote. Sentences in the judgment which do not have a cosine similarity of at least 0.5 with any sentence in the headnote are considered to have no alignment at all. The remaining sentences are aligned to a single best matching sentence in the headnote. Hence each sentence in the judg- ment is aligned to exactly one or zero sentences in the headnote, while each sentence in the headnote can have a many to one mapping. All the judg- ment sentences aligned to the same headnote sen- tence are combined to form a single sentence, thus forming a parallel corpus between Judgment Lan- guage and Headnote Language. Further we used the Moses 5 machine translation toolkit to gener- ate a translation model with the source language as the judgment (or the document Language) and the target language as the headnote (or summary language). Since we have not yet used the entire training data, the results in the current experiment were not very impressive. But there are certain ex- amples worth reporting, where good results were indeed obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Example Translation</head><p>Original: There can in my opinion be no escape from the conclusion that section 12 of the Act by which a most important protection or safeguard conferred on the subject by the Constitution has been taken away is not a valid provision since it contravenes the very provision in the Constitu- tion under which the Parliament derived its com- petence to enact it. Translation: There can be no escape from the conclusion that section 12 of the Act by which safeguard conferred on the subject by the Consti- tution has been taken away is not valid since it con- travened the very provision in the Constitution un- der which the Parliament derived its competence to enact it. The highlighted parts in the original sentence are the ones that have been changed in the cor- responding translation. We can attribute the ex- clusion of 'in my opinion' solely to the language model of the Summary Language. Since the sum- maries are in third person while many statements in the original judgment would be in first person, such a phrase which is common in the Judgment will never occur in the headnote. Similarly the headnotes are usually written in past tense and that might account for changing 'contravenes' to 'con- travened'. We are not sure what the reasons might be behind the other changes. We plan to do an exhaustive error analysis on the results of this ex- periment, which will provide further insights and ideas. We have reported some more examples in the appendix section.</p><p>Although not all translations are linguistically correct and many of them don't make much sense, we believe that by using a larger training cor- pus (which we are currently curating) and a bet- ter technique for creating a sentence aligned cor- pus the results can be significantly improved. Also currently the target sentences are not much shorter than their source, and we need to further work on that issue. Overall the idea of using SMT for doc- ument summarization seems to be promising and worth pursuing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional Examples</head><p>• The underlined parts in the original sentence are the ones that are correctly omitted in the target sentence. The striked out part in the original sentences are wrongly missing in the translation, affecting the comprehensibility of the sentence.</p><p>• The striked out parts in the Translation are the ones that are misplaced in the sentence. Boldfaced parts in the Translation are the ones newly introduced.</p><p>• The boldfaced parts in the Expected Translations are the corrections that are made compared to the actual translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original:</head><p>The Act provides for levy of two kinds of taxes called the general tax and the special tax by the two charging sections 5 and 10 respectively. Seervai attempted to make out that the provisions of the charging sections 5 and 10 fixing Rs. 30000 and Rs. 5000 as the minimum taxable turnover for general tax and special tax respectively were found discriminatory and void under article 14 read with article 13 of the Constitution and he gave us several tables of figures showing how the imposition of the tax actually works out in practice in hypothetical cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Translation:</head><p>The Act provides for the levy of the general tax and special tax by the two charging sections 5 and 10 respectively. that the provisions of the charging sections 5 and 10 fixing Rs. 30000 and Rs. 5000 as the minimum taxable turnover for general tax and special tax respectively are discriminatory and void under art of the Constitution and he gave the several tables of figures showing how the imposition of the tax actually works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Expected Tranlsation:</head><p>The Act provides for the levy of the general tax and special tax by the two charging sections 5 and 10 respectively. Seervai attempted to make out that the provisions of the charging sections 5 and 10 fixing Rs. 30000 and Rs. 5000 as the minimum taxable turnover for general tax and special tax respectively are discriminatory and void under article 14 read with article 13 of the Constitution and he gave the several tables of figures showing how the imposition of the tax actually works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original:</head><p>The learned trial magistrate believed the prosecution evidence rejected the pleas raised by the defe- nce convicted the appellants of the charge framed and sentenced them to undergo simple imprisonment for two months each. The appellate court confirmed the conviction of the appellants but reduced their sentence from simple imprisonment for two months to a fine of Rs. 50 or in default simple imprisonment for one month each.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Translation:</head><p>The Magistrate found the appellants of the charge framed and sentenced them to undergo simple im- prisonment for two months guilty. confirmed the conviction of the appellants but reduced their sentence from simple imprisonment for two months to a fine of Rs. 50 or in default simple imprisonment for one month each.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Expected Tranlsation:</head><p>The Magistrate found the appellants guilty of the charge framed and sentenced them to undergo simple imprisonment for two months. The appellate court confirmed the conviction of the appellants but reduced their sentence from simple imprisonment for two months to a fine of Rs. 50 or in default simple imprisonment for one month each.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Effect of 'Informed' Fusion 

</table></figure>

			<note place="foot" n="1"> duc.nist.gov 2 www.nist.gov/tac</note>

			<note place="foot" n="3"> https://en.wikipedia.org/wiki/Borda_ count</note>

			<note place="foot" n="4"> http://www.isical.ac.in/ ˜ fire/2014/ legal.html</note>

			<note place="foot" n="5"> www.statmt.org/moses</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-document abstractive summarization using ilp based multi-sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasenjit</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazunari</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24th International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Lexrank: Graph-based lexical centrality as salience in text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Günes</forename><surname>Erkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dragomir R Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="page" from="457" to="479" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Opinosis: a graph-based approach to abstractive summarization of highly redundant opinions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kavita</forename><surname>Ganesan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on computational linguistics</title>
		<meeting>the 23rd international conference on computational linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="340" to="348" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploring content models for multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="362" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A repository of state of the art and competitive baseline summaries for generic news summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Conroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Favre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">System combination for multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="107" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The automated acquisition of topic signatures for text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th conference on Computational linguistics</title>
		<meeting>the 18th conference on Computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="495" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text summarization branches out: Proceedings of the ACL-04 workshop</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Textrank: Bringing order into texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tarau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A survey of text summarization techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mining Text Data</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="43" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Weighted consensus multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingding</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="513" to="523" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Question retrieval with high quality answers in community question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haocheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management</title>
		<meeting>the 23rd ACM International Conference on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="371" to="380" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
