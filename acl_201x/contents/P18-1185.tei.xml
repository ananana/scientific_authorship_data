<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:04+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visual Attention Model for Name Tagging in Multimodal Social Media</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018. 1990</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><surname>Neves</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Snap Research</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitor</forename><surname>Carvalho</surname></persName>
							<email>vitor carvalho@intuit.com</email>
							<affiliation key="aff2">
								<address>
									<settlement>Intuit</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Snap Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Visual Attention Model for Name Tagging in Multimodal Social Media</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1990" to="1999"/>
							<date type="published">July 15-20, 2018. 2018. 1990</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Everyday billions of multimodal posts containing both images and text are shared in social media sites such as Snapchat, Twitter or Instagram. This combination of image and text in a single message allows for more creative and expressive forms of communication, and has become increasingly common in such sites. This new paradigm brings new challenges for natural language understanding, as the tex-tual component tends to be shorter, more informal, and often is only understood if combined with the visual context. In this paper, we explore the task of name tagging in multimodal social media posts. We start by creating two new multimodal datasets: one based on Twitter posts 1 and the other based on Snapchat captions (ex-clusively submitted to public and crowd-sourced stories). We then propose a novel model based on Visual Attention that not only provides deeper visual understanding on the decisions of the model, but also significantly outperforms other state-of-the-art baseline methods for this task. 2</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Social platforms, like Snapchat, Twitter, Insta- gram and Pinterest, have become part of our lives and play an important role in making com- munication easier and accessible. Once text- centric, social media platforms are becoming in- * * This work was mostly done during the first author's in- ternship at Snap Research. <ref type="bibr">1</ref> The Twitter data and associated images presented in this paper were downloaded from https://archive.org/ details/twitterstream 2 We will make the annotations on Twitter data available for research purpose upon request. creasingly multimodal, with users combining im- ages, videos, audios, and texts for better expres- siveness. As social media posts become more mul- timodal, the natural language understanding of the textual components of these messages becomes in- creasingly challenging. In fact, it is often the case that the textual component can only be understood in combination with the visual context of the mes- sage.</p><p>In this context, here we study the task of Name Tagging for social media containing both image and textual contents. Name tagging is a key task for language understanding, and provides input to several other tasks such as Question Answering, Summarization, Searching and Recommendation. Despite its importance, most of the research in name tagging has focused on news articles and longer text documents, and not as much in mul- timodal social media data ( <ref type="bibr" target="#b4">Baldwin et al., 2015)</ref>.</p><p>However, multimodality is not the only chal- lenge to perform name tagging on such data. The textual components of these messages are often very short, which limits context around names. Moreover, there linguistic variations, slangs, ty- pos and colloquial language are extremely com- mon, such as using 'looooove' for 'love', 'LosAn- geles' for 'Los Angeles', and '#Chicago #Bull' for 'Chicago Bulls'. These characteristics of social media data clearly illustrate the higher difficulty of this task, if compared to traditional newswire name tagging.</p><p>In this work, we modify and extend the current state-of-the-art model ( <ref type="bibr">Lample et al., 2016;</ref><ref type="bibr" target="#b10">Ma and Hovy, 2016</ref>) in name tagging to incorporate the visual information of social media posts us- ing an Attention mechanism. Although the usually short textual components of social media posts provide limited contextual information, the ac- companying images often provide rich informa- tion that can be useful for name tagging. For ex- ample, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>, both captions in- clude the phrase 'Modern Baseball'. It is not easy to tell if each Modern Baseball refers to a name or not from the textual evidence only. However using the associated images as reference, we can easily infer that Modern Baseball in the first sen- tence should be the name of a band because of the implicit features from the objects like instruments and stage, and the Modern Baseball in the second sentence refers to the sport of baseball because of the pitcher in the image.</p><p>In this paper, given an image-sentence pair as input, we explore a new approach to leverage vi- sual context for name tagging in text. First, we propose an attention-based model to extract visual features from the regions in the image that are most related to the text. It can ignore irrelevant visual information. Secondly, we propose to use a gate to combine textual features extracted by a Bidirectional Long Short Term Memory (BLSTM) and extracted visual features, before feed them into a Conditional Random Fields(CRF) layer for tag predication. The proposed gate architecture plays the role to modulate word-level multimodal features.</p><p>We evaluate our model on two labeled datasets collected from Snapchat and Twitter respectively. Our experimental results show that the proposed model outperforms state-for-the-art name tagger in multimodal social media.</p><p>The main contributions of this work are as fol- lows:</p><p>• We create two new datasets for name tag- ging in multimedia data, one using Twitter and the other using crowd-sourced Snapchat posts. These new datasets effectively consti- tute new benchmarks for the task.</p><p>• We propose a visual attention model specif- ically for name tagging in multimodal social media data. The proposed end-to-end model only uses image-sentence pairs as input with- out any human designed features, and a Vi- sual Attention component that helps under- stand the decision making of the model. <ref type="figure" target="#fig_1">Figure 2</ref> shows the overall architecture of our model. We describe three main components of our model in this section: BLSTM-CRF sequence labeling model (Section 2.1), Visual Attention Model (Section 2.3) and Modulation Gate (Sec- tion 2.4). Given a pair of sentence and image as input, the Visual Attention Model extracts regional vi- sual features from the image and computes the weighted sum of the regional visual features as the visual context vector, based on their relatedness with the sentence. The BLSTM-CRF sequence la- beling model predicts the label for each word in the sentence based on both the visual context vec- tor and the textual information of the words. The modulation gate controls the combination of the visual context vector and the word representations for each word before the CRF layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">BLSTM-CRF Sequence Labeling</head><p>We model name tagging as a sequence labeling problem. Given a sequence of words: S = {s 1 , s 2 , ..., s n }, we aim to predict a sequence of labels: L = {l 1 , l 2 , ..., l n }, where l i ∈ L and L is a pre-defined label set. Bidirectional LSTM. Long Short-term Memory Networks (LSTMs) <ref type="bibr">(Hochreiter and Schmidhuber, 1997</ref>) are variants of Recurrent Neural Networks (RNNs) designed to capture long-range dependen- cies of input. The equations of a LSTM cell are as follows:</p><formula xml:id="formula_0">i t = σ(W xi x t + W hi h t−1 + b i ) f t = σ(W xf x t + W hf h t−1 + b f ) ˜ c t = tanh(W xc x t + W hc h t−1 + b c ) c t = f t c t−1 + i t ˜ c t o t = σ(W xo x t + W ho h t−1 + b o ) h t = o t tanh(c t )</formula><p>where x t , c t and h t are the input, memory and hid- den state at time t respectively.  Name Tagging benefits from both of the past (left) and the future (right) contexts, thus we im- plement the Bidirectional LSTM ( <ref type="bibr">Graves et al., 2013;</ref><ref type="bibr">Dyer et al., 2015</ref>) by concatenating the left and right context representations,</p><formula xml:id="formula_1">W xi , W hi , W xf , W hf , W xc , W hc ,</formula><formula xml:id="formula_2">h t = [ − → h t , ← − h t ], for each word. Character-level Representation.</formula><p>Follow- ing ( <ref type="bibr">Lample et al., 2016)</ref>, we generate the character-level representation for each word using another BLSTM. It receives character embeddings as input and generates representations combining implicit prefix, suffix and spelling information. The final word representation x i is the concate- nation of word embedding e i and character-level representation c i .</p><formula xml:id="formula_3">c i = BLST M char (s i ) s i ∈ S x i = [e i , c i ]</formula><p>Conditional random fields (CRFs). For name tagging, it is important to consider the constraints of the labels in neighborhood (e.g., I-LOC must follow B-LOC). CRFs ( <ref type="bibr">Lafferty et al., 2001</ref>) are effective to learn those constraints and jointly pre- dict the best chain of labels. We follow the imple- mentation of CRFs in (Ma and Hovy, 2016).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Visual Feature Representation</head><p>We use Convolutional Neural Networks (CNNs) ( <ref type="bibr">LeCun et al., 1989)</ref> to obtain the representations of images. Particularly, we use Residual Net (ResNet) (He et al., 2016), which  <ref type="bibr">2014</ref>) detection, and COCO segmentation tasks. Given an input pair (S, I), where S represents the word sequence and I represents the image rescaled to 224x224 pixels, we use ResNet to extract visual features for regional areas as well as for the whole image <ref type="figure" target="#fig_2">(Fig 3)</ref>:</p><formula xml:id="formula_4">V g = ResN et g (I) V r = ResN et r (I)</formula><p>where the global visual vector V g , which repre- sents the whole image, is the output before the last fully connected layer 3 . The dimension of V g is 1,024. V r are the visual representations for re- gional areas and they are extracted from the last convolutional layer of ResNet, and the dimension is 1,024x7x7 as shown in <ref type="figure" target="#fig_2">Figure 3</ref>. 7x7 is the number of regions in the image and 1,024 is the dimension of the feature vector. Thus each feature vector of V r corresponds to a 32x32 pixel region of the rescaled input image. The global visual representation is a reason- able representation of the whole input image, but not the best. Sometimes only parts of the im- age are related to the associated sentence. For example, the visual features from the right part of the image in <ref type="figure" target="#fig_3">Figure 4</ref> cannot contribute to in- ferring the information in the associated sentence 'I have just bought Jeremy Pied.' In this work we utilize visual attention mechanism to combat the problem, which has been proven effective for vision-language related tasks such as Image Cap- tioning (  and Visual Question An- swering ( <ref type="bibr" target="#b23">Yang et al., 2016b;</ref><ref type="bibr" target="#b7">Lu et al., 2016)</ref>, by enforcing the model to focus on the regions in im- ages that are mostly related to context textual in- formation while ignoring irrelevant regions. Also the visualization of attention can also help us to understand the decision making of the model. At- tention mechanism is mapping a query and a set of key-value pairs to an output. The output is a weighted sum of the values and the assigned weight for each value is computed by a function of the query and corresponding key. We encode the sentence into a query vector using an LSTM, and use regional visual representations V r as both keys and values. Text Query Vector. We use an LSTM to encode the sentence into a query vector, in which the in- puts of the LSTM are the concatenations of word embeddings and character-level word representa- tions. Different from the LSTM model used for sequence labeling in Section 2.1, the LSTM here aims to get the semantic information of the sen- tence and it is unidirectional:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Visual Attention Model</head><formula xml:id="formula_5">Q = LST M query (S)<label>(1)</label></formula><p>Attention Implementation. There are many im- plementations of visual attention mechanism such as Multi-layer Perceptron ( <ref type="bibr" target="#b3">Bahdanau et al., 2014</ref>), Bilinear ( <ref type="bibr" target="#b9">Luong et al., 2015)</ref>, dot product (Lu- ong et al., 2015), Scaled Dot Product ( <ref type="bibr" target="#b19">Vaswani et al., 2017)</ref>, and linear projection after summa- tion ( <ref type="bibr" target="#b23">Yang et al., 2016b</ref>). Based on our experi- mental results, dot product implementations usu- ally result in more concentrated attentions and lin- ear projection after summation results in more dis- persed attentions. In the context of name tagging, we choose the implementation of linear projec- tion after summation because it is beneficial for the model to utilize as many related visual fea- tures as possible, and concentrated attentions may make the model bias. For implementation, we first project the text query vector Q and regional visual features V r into the same dimensions:</p><formula xml:id="formula_6">P t = tanh(W t Q) P v = tanh(W v V r )</formula><p>then we sum up the projected query vector with each projected regional visual vector respectively:</p><formula xml:id="formula_7">A = P t ⊕ P v</formula><p>the weights of the regional visual vectors:</p><formula xml:id="formula_8">E = sof tmax(W a A + b a )</formula><p>where W a is weights matrix. The weighted sum of the regional visual features is:</p><formula xml:id="formula_9">v c = α i v i α i ∈ E, v i ∈ V r</formula><p>We use v c as the visual context vector to initial- ize the BLSTM sequence labeling model in Sec- tion 2.1. We compare the performances of the models using global visual vector V g and attention based visual context vector V c for initialization in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Visual Modulation Gate</head><p>The BLSTM-CRF sequence labeling model ben- efits from using the visual context vector to ini- tialize the LSTM cell. However, the better way to utilize visual features for sequence labeling is to incorporate the features at word level individ- ually. However visual features contribute quite differently when they are used to infer the tags of different words. For example, we can easily find matched visual patterns from associated im- ages for verbs such as 'sing', 'run', and 'play'. Words/Phrases such as names of basketball play- ers, artists, and buildings are often well-aligned with objects in images. However it is difficult to align function words such as 'the', 'of ' and 'well' with visual features. Fortunately, most of the chal- lenging cases in name tagging involve nouns and verbs, the disambiguation of which can benefit more from visual features. We propose to use a visual modulation gate, similar to ( <ref type="bibr" target="#b11">Miyamoto and Cho, 2016;</ref><ref type="bibr" target="#b22">Yang et al., 2016a)</ref>, to dynamically control the combination of visual features and word representation generated by BLSTM at word-level, before feed them into the CRF layer for tag prediction. The equations for the implementation of modulation gate are as follows:</p><formula xml:id="formula_10">β v = σ(W v h i + U v v c + b v ) β w = σ(W w h i + U w v c + b w ) m = tanh(W m h i + U m v c + b m ) w m = β w · h i + β v · m</formula><p>where h i is the word representation generated by BLSTM, v c is the computed visual context vector, W v , W w , W m , U v , U w and U m are weight matri- ces, σ is the element-wise sigmoid function, and w m is the modulated word representations fed into the CRF layer in Section 2.1. We conduct experi- ments to evaluate the impact of modulation gate in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Datasets</head><p>We evaluate our model on two multimodal datasets, which are collected from Twitter and Snapchat respectively.  <ref type="bibr">2017</ref> and June 2017. We use sports and social event related key words, such as concert, festi- val, soccer, basketball, as queries. We don't take into consideration messages without images for this experiment. If a tweet has more than one im- age associated to it, we randomly select one of the images. Snap name tagging. The Snap name tagging dataset consists of caption and image pairs exclu- sively extracted from snaps submitted to public and live stories. They were collected between May and July of 2017. The data contains captions sub- mitted to multiple community curated stories like the Electric Daisy Carnival (EDC) music festival and the Golden State Warrior's NBA parade.</p><p>Both Twitter and Snapchat are social media with plenty of multimodal posts, but they have ob- vious differences with sentence length and image styles. In Twitter, text plays a more important role, and the sentences in the Twitter dataset are much longer than those in the Snap dataset (16.0 tokens vs 8.1 tokens). The image is often more related to the content of the text and added with the purpose of illustrating or giving more context. On the other hand, as users of Snapchat use cameras to commu- nicate, the roles of text and image are switched. Captions are often added to complement what is being portrayed by the snap. On our experiment section we will show that our proposed model out- performs baseline on both datasets.</p><p>We believe the Twitter dataset can be an im- portant step towards more research in multimodal name tagging and we plan to provide it as a bench- mark upon request.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training</head><p>Tokenization. To tokenize the sentences, we use the same rules as ( <ref type="bibr" target="#b12">Owoputi et al., 2013</ref>), except we separate the hashtag '#' with the words after. Labeling Schema. We use the standard BIO schema <ref type="bibr" target="#b18">(Sang and Veenstra, 1999</ref>), because we see little difference when we switch to BIOES schema ( <ref type="bibr" target="#b15">Ratinov and Roth, 2009</ref> Optimization. The models achieve the best per- formance by using mini-batch stochastic gradient descent (SGD) with batch size 20 and momentum 0.9 on both datasets. We set an initial learning rate of η 0 = 0.03 with decay rate of ρ = 0.01. We use a gradient clipping of 5.0 to reduce the effects of gradient exploding.</p><p>Hyper-parameters. We summarize the hyper- parameters in <ref type="table">Table 2</ref>.</p><p>Hyper-parameter Value LSTM hidden state size 300 Char LSTM hidden state size 50 visual vector size 100 dropout rate 0.5 <ref type="table">Table 2</ref>: Hyper-parameters of the networks. <ref type="table" target="#tab_4">Table 3</ref> shows the performance of the baseline, which is BLSTM-CRF with sentences as input only, and our proposed models on both datasets. is more likely to include a person name in the as- sociated sentence, but when there is a soccer field in the image, it is more likely to include a sports team name.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>All the models get better scores on Twitter dataset than on Snap dataset, because the average length of the sentences in Snap dataset (8.1 tokens) is much smaller than that of Twitter dataset (16.0 tokens), which means there is much less contex- tual information in Snap dataset.</p><p>Also comparing the gains from visual features on different datasets, we find that the model bene- fits more from visual features on Twitter dataset, considering the much higher baseline scores on Twitter dataset. Based on our observation, users of Snapchat often post selfies with captions, which means some of the images are not strongly related to their associated captions. In contrast, users of Twitter prefer to post images to illustrate texts <ref type="figure" target="#fig_6">Figure 5</ref> shows some good examples of the atten- tion visualization and their corresponding name tagging results. The model can successfully focus on appropriate regions when the images are well aligned with the associated sentences. Based on our observation, the multimodal contexts in posts related to sports, concerts or festival are usually better aligned with each other, therefore the visual features easily contribute to these cases. For ex- ample, the ball and shoot action in example (a) in <ref type="figure" target="#fig_6">Figure 5</ref> indicates that the context should be re- lated to basketball, thus the 'Warriors' should be the name of a sports team. A singing person with a microphone in example (b) indicates that the name of an artist or a band ('Radiohead') may appear in the sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Attention Visualization</head><p>The second and the third rows in <ref type="figure" target="#fig_6">Figure 5</ref> show some more challenging cases whose tagging re- sults benefit from visual features. In example (d), the model pays attention to the big Apple logo, thus tags the 'Apple' in the sentence as an Orga- nization name. In example (e) and (i), a small      image in example (c) is about a baseball pitcher, but our model pays attention to the top right cor- ner of the image. The visual context feature com- puted by our model is not related to the sentence, and results in missed tagging of 'SBU', which is an organization name.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Error Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>In this section, we summarize relevant background on previous work on name tagging and visual at- tention. Name Tagging. In recent years, ( <ref type="bibr">Chiu and Nichols, 2015;</ref><ref type="bibr">Lample et al., 2016;</ref><ref type="bibr" target="#b10">Ma and Hovy, 2016)</ref> proposed several neural network architec- tures for named tagging that outperform tradi- tional explicit features based methods ( <ref type="bibr">Chieu and Ng, 2002;</ref><ref type="bibr">Florian et al., 2003;</ref><ref type="bibr" target="#b2">Ando and Zhang, 2005;</ref><ref type="bibr" target="#b15">Ratinov and Roth, 2009;</ref><ref type="bibr" target="#b5">Lin and Wu, 2009;</ref><ref type="bibr" target="#b13">Passos et al., 2014;</ref><ref type="bibr" target="#b8">Luo et al., 2015)</ref>. They all use Bidirectional LSTM (BLSTM) to extract fea- tures from a sequence of words. ) applied attention mechanism for VQA, to find the regions in images that are most related to the questions. ( <ref type="bibr" target="#b24">Yu et al., 2016</ref>) applied the visual attention mechanism on video captioning. Our at- tention implementation approach in this work is similar to those used for VQA. The model finds the regions in images that are most related to the accompanying sentences, and then feed the visual features into an BLSTM-CRF sequence labeling model. The differences are: (1) we add visual con- text feature at each step of sequence labeling; and (2) we propose to use a gate to control the combi- nation of the visual information and textual infor- mation based on their relatedness. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>We propose a gated Visual Attention for name tagging in multimodal social media. We con- struct two multimodal datasets from Twitter and Snapchat. Experiments show an absolute 3%-4% F-score gain. We hope this work will encour- age more research on multimodal social media in the future and we plan on making our benchmark available upon request. Name Tagging for more fine-grained types (e.g. soccer team, basketball team, politician, artist) can benefit more from visual features. For example, an image including a pitcher indicates that the 'Gi- ants' in context should refer to the baseball team 'San Francisco Giants'. We plan to expand our model to tasks such as fine-grained Name Tagging or Entity Liking in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Examples of Modern Baseball associated with different images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overall Architecture of the Visual Attention Name Tagging Model.</figDesc><graphic url="image-4.png" coords="3,99.20,207.23,81.36,81.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: CNN for visual features extraction.</figDesc><graphic url="image-5.png" coords="3,318.19,333.11,68.90,68.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Example of partially related image and sentence. ('I have just bought Jeremy Pied.')</figDesc><graphic url="image-6.png" coords="4,122.20,146.05,117.86,117.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>BLSTM-CRF + Global Image Vector: use global image vector to initialize the BLSTM-CRF. BLSTM-CRF + Visual attention: use atten- tion based visual context vector to initialize the BLSTM-CRF. BLSTM-CRF + Visual attention + Gate: modu- late word representations with visual vector. Our final model BLSTM-CRF + VISUAL AT- TENTION + GATE, which has visual attention component and modulation gate, obtains the best F1 scores on both datasets. Visual features suc- cessfully play a role of validating entity types. For example, when there is a person in the image, it</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 shows</head><label>6</label><figDesc>Figure 6 shows some failed examples that are categorized into three types: (1) bad alignments between visual and textual information; (2) blur images; (3) wrong attention made by the model. Name tagging greatly benefits from visual fea</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Examples of visual attentions and NER outputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Nice image of [PER Kevin Love] and [PER Kyle Korver] during 1st half #NBAFinals #Cavsin9 #[LOC Cleveland] (b). Very drunk in a #magnum concert (c). Looking forward to editing some SBU baseball shots from Saturday.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Examples of Failed Visual Attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>For character- level representations, (Lample et al., 2016) pro- posed to use another BLSTM to capture prefix and suffix information of words, and (Chiu and Nichols, 2015; Ma and Hovy, 2016) used CNN to extract position-independent character features. On top of BLSTM, (Chiu and Nichols, 2015) used a softmax layer to predict the label for each word, and (Lample et al., 2016; Ma and Hovy, 2016) used a CRF layer for joint prediction. Com- pared with traditional approaches, neural networks based approaches do not require hand-crafted fea- tures and achieved state-of-the-art performance on name tagging (Ma and Hovy, 2016). How- ever, these methods were mainly developed for newswire and paid little attention to social me- dia. For name tagging in social media, (Ritter et al., 2011) leveraged a large amount of unla- beled data and many dictionaries into a pipeline model. (Limsopatham and Collier, 2016) adapted the BLSTM-CRF model with additional word shape information, and (Aguilar et al., 2017) uti- lized an effective multi-task approach. Among these methods, our model is most similar to (Lam- ple et al., 2016), but we designed a new visual at- tention component and a modulation control gate. Visual Attention. Since the attention mechanism was proposed by (Bahdanau et al., 2014), it has been widely adopted to language and vision re- lated tasks, such as Image Captioning and Visual Question Answering (VQA), by retrieving the vi- sual features most related to text context (Zhu et al., 2016; Anderson et al., 2017; Xu and Saenko, 2016; Chen et al., 2015). (Xu et al., 2015) pro- posed to predict a word based on the visual patch that is most related to the last predicted word for image captioning. (Yang et al., 2016b; Lu et al., 2016</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 summarizes</head><label>1</label><figDesc>the data statistics. Both datasets contain four types of named entities: Location, Person, Organization and Miscellaneous. Each data instance contains a pair of sentence and im- age, and the names in sentences are manually tagged by three expert labelers. Twitter name tagging. The Twitter name tagging dataset contains pairs of tweets and their associ- ated images extracted from May 2016, January</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>). Word embeddings. We use the 100-dimensional GloVe 4 (Pennington et al., 2014) embeddings trained on 2 billions tweets to initialize the lookup table and do fine-tuning during training. Character embeddings. As in (Lample et al., 2016), we randomly initialize the character em- beddings with uniform samples. Based on exper- imental results, the size of the character embed- dings affects little, and we set it as 50.</figDesc><table>Training Development Testing 

Snapchat 
Sentences 
4,817 
1,032 
1,033 
Tokens 
39,035 
8,334 
8,110 

Twitter 
Sentences 
4,290 
1,432 
1,459 
Tokens 
68,655 
22,872 
23,051 

Table 1: Sizes of the datasets in numbers of sentence and token. 

Pretrained CNNs. We use the pretrained ResNet-
152 (He et al., 2016) from Pytorch. 
Early Stopping. We use early stopping (Caruana 
et al., 2001; Graves et al., 2013) with a patience of 
15 to prevent the model from over-fitting. 
Fine Tuning. The models are optimized with fine-
tuning on both the word-embeddings and the pre-
trained ResNet. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Results of our models on noisy social media data. 

group of people indicates that it is likely to include 
names of bands ('Florence and the Machine' and 
'BTS'). And a crowd can indicate an organization 
('Warriorette' in example (i)). A jersey shirt on 
the table indicates a sports team. ('Leicester' in 
example (h) can refer to both a city and a soccer 
club based in it.) 

</table></figure>

			<note place="foot" n="3"> the last fully connect layer outputs the probabilities over 1,000 classes of objects.</note>

			<note place="foot" n="4"> https://nlp.stanford.edu/projects/glove/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was partially supported by the U.S. DARPA AIDA Program No. FA8750-18-2-0014 and U.S. ARL NS-CTA No. W911NF-09-2-0053. The views and conclusions contained in this doc-ument are those of the authors and should not be interpreted as representing the official policies, ei-ther expressed or implied, of the U.S. Govern-ment. The U.S. Government is authorized to re-produce and distribute reprints for Government purposes notwithstanding any copyright notation here on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Alex Graves, Abdel-rahman Mohamed, and Geoffrey</head><p>Hinton </p></div>
			</div>

			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A multi-task approach for named entity recognition in social media data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Aguilar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suraj</forename><surname>Maharjan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Noisy User-generated Text</title>
		<meeting>the 3rd Workshop on Noisy User-generated Text</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Adrian Pastor López Monroy, and Thamar Solorio</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07998</idno>
		<title level="m">Bottom-up and top-down attention for image captioning and vqa</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A framework for learning predictive structures from multiple tasks and unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kubota</forename><surname>Rie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Ando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 International Conference on Learning Representations</title>
		<meeting>the 2015 International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Shared tasks of the 2015 workshop on noisy user-generated text: Twitter lexical normalization and named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Noisy User-generated Text</title>
		<meeting>the Workshop on Noisy User-generated Text</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Phrase clustering for discriminative learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 European Conference on Computer Vision</title>
		<meeting>the 2014 European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hierarchical question-image coattention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Advances In Neural Information Processing Systems</title>
		<meeting>the 2016 Advances In Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Joint entity recognition and disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiqing</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">End-to-end sequence labeling via bi-directional lstm-cnns-crf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Gated word-character recurrent language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasumasa</forename><surname>Miyamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improved part-of-speech tagging for online conversational text with word clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olutobi</forename><surname>Owoputi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Lexicon infused phrase embeddings for named entity resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing</title>
		<meeting>the 2014 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Design challenges and misconceptions in named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Thirteenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Named entity recognition in tweets: an experimental study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Representing text chunks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Erik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorn</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Veenstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ninth conference on European chapter of the Association for Computational Linguistics</title>
		<meeting>the ninth conference on European chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Advances in Neural Information Processing Systems</title>
		<meeting>the 2017 Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ask, attend and answer: Exploring question-guided spatial attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 European Conference on Computer Vision</title>
		<meeting>the 2016 European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 International Conference on Machine Learning</title>
		<meeting>the 2015 International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Words or characters? fine-grained gating for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>William W Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 International Conference on Learning Representations</title>
		<meeting>the 2016 International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Video paragraph captioning using hierarchical recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haonan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Visual7w: Grounded question answering in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Feifei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
