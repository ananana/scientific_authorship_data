<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:12+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Latent Predictor Networks for Code Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google DeepMind</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♦</forename><forename type="middle">Edward</forename><surname>Grefenstette</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google DeepMind</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google DeepMind</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><forename type="middle">♦</forename><surname>Tomáš</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google DeepMind</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kočisk´</forename><surname>Kočisk´y</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google DeepMind</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♦</forename></persName>
							<affiliation key="aff0">
								<orgName type="department">Google DeepMind</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google DeepMind</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fumin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google DeepMind</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google DeepMind</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Latent Predictor Networks for Code Generation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="599" to="609"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Many language generation tasks require the production of text conditioned on both structured and unstructured inputs. We present a novel neural network architecture which generates an output sequence conditioned on an arbitrary number of input functions. Crucially, our approach allows both the choice of conditioning context and the granularity of generation, for example characters or tokens, to be marginalised, thus permitting scalable and effective training. Using this framework, we address the problem of generating programming code from a mixed natural language and structured specification. We create two new data sets for this paradigm derived from the collectible trading card games Magic the Gathering and Hearth-stone. On these, and a third preexisting corpus, we demonstrate that marginalis-ing multiple predictors allows our model to outperform strong benchmarks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The generation of both natural and formal lan- guages often requires models conditioned on di- verse predictors ( <ref type="bibr" target="#b13">Koehn et al., 2007;</ref><ref type="bibr" target="#b33">Wong and Mooney, 2006</ref>). Most models take the restrictive approach of employing a single predictor, such as a word softmax, to predict all tokens of the output sequence. To illustrate its limitation, suppose we wish to generate the answer to the question "Who wrote The Foundation?" as "The Foundation was written by Isaac Asimov". The generation of the words "Issac Asimov" and "The Foundation" from a word softmax trained on annotated data is un- likely to succeed as these words are sparse. A ro- bust model might, for example, employ one pre- dictor to copy "The Foundation" from the input, and a another one to find the answer "Issac Asi- mov" by searching through a database. However, training multiple predictors is in itself a challeng- ing task, as no annotation exists regarding the pre- dictor used to generate each output token. Fur- thermore, predictors generate segments of differ- ent granularity, as database queries can generate multiple tokens while a word softmax generates a single token. In this work we introduce Latent Predictor Networks (LPNs), a novel neural archi- tecture that fulfills these desiderata: at the core of the architecture is the exact computation of the marginal likelihood over latent predictors and gen- erated segments allowing for scalable training.</p><p>We introduce a new corpus for the automatic generation of code for cards in Trading Card Games (TCGs), on which we validate our model <ref type="bibr">1</ref> . TCGs, such as Magic the Gathering (MTG) and Hearthstone (HS), are games played between two players that build decks from an ever expanding pool of cards. Examples of such cards are shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Each card is identified by its attributes (e.g., name and cost) and has an effect that is de- scribed in a text box. Digital implementations of these games implement the game logic, which in- cludes the card effects. This is attractive from a data extraction perspective as not only are the data annotations naturally generated, but we can also view the card as a specification communi- cated from a designer to a software engineer.</p><p>This dataset presents additional challenges to prior work in code generation ( <ref type="bibr" target="#b33">Wong and Mooney, 2006;</ref><ref type="bibr" target="#b10">Jones et al., 2012;</ref><ref type="bibr" target="#b18">Lei et al., 2013;</ref><ref type="bibr" target="#b2">Artzi et al., 2015;</ref><ref type="bibr" target="#b26">Quirk et al., 2015)</ref>, including the handling of structured input-i.e. cards are com- posed by multiple sequences (e.g., name and description)-and attributes (e.g., attack and cost), and the length of the generated sequences. Thus, we propose an extension to attention-based neu- ral models ( <ref type="bibr" target="#b3">Bahdanau et al., 2014</ref>) to attend over structured inputs. Finally, we propose a code com- pression method to reduce the size of the code without impacting the quality of the predictions.</p><p>Experiments performed on our new datasets, and a further pre-existing one, suggest that our ex- tensions outperform strong benchmarks.</p><p>The paper is structured as follows: We first describe the data collection process (Section 2) and formally define our problem and our base- line method (Section 3). Then, we propose our extensions, namely, the structured attention mech- anism (Section 4) and the LPN architecture (Sec- tion 5). We follow with the description of our code compression algorithm (Section 6). Our model is validated by comparing with multiple bench- marks (Section 7). Finally, we contextualize our findings with related work (Section 8) and present the conclusions of this work (Section 9).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dataset Extraction</head><p>We obtain data from open source implementations of two different TCGs, MTG in Java 2 and HS in Python. <ref type="bibr">3</ref> The statistics of the corpora are illus- trated in <ref type="table">Table 1</ref>. In both corpora, each card is im- plemented in a separate class file, which we strip of imports and comments. We categorize the con- tent of each card into two different groups: sin- gular fields that contain only one value; and text fields, which contain multiple words representing different units of meaning. In MTG, there are six singular fields (attack, defense, rarity, set, id, and <ref type="bibr">2</ref>  health) and four text fields (cost, type, name, and description), whereas HS cards have eight singu- lar fields (attack, health, cost and durability, rar- ity, type, race and class) and two text fields (name and description). Text fields are tokenized by splitting on whitespace and punctuation, with ex- ceptions accounting for domain specific artifacts (e.g., Green mana is described as "{G}" in MTG). Empty fields are replaced with a "NIL" token. The code for the HS card in <ref type="figure" target="#fig_0">Figure 1</ref> is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. The effect of "drawing cards until the player has as many cards as the opponent" is im- plemented by computing the difference between the players' hands and invoking the draw method that number of times. This illustrates that the map- ping between the description and the code is non- linear, as no information is given in the text regard- ing the specifics of the implementation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Definition</head><p>Given the description of a card x, our decoding problem is to find the codê y so that:</p><formula xml:id="formula_0">ˆ y = argmax y log P (y | x)<label>(1)</label></formula><p>Here log P (y | x) is estimated by a given model. We define y = y 1 ..y |y| as the sequence of char- acters of the code with length |y|. We index each input field with k = 1..|x|, where |x| quantifies the number of input fields. |x k | denotes the number of tokens in x k and x ki selects the i-th token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Structured Attention</head><p>Background When |x| = 1, the atten- tion model of <ref type="bibr" target="#b3">Bahdanau et al. (2014)</ref> ap- plies. Following the chain rule, log P (y|x) = t=1..|y| log P (y t |y 1 ..y t−1 , x), each token y t is predicted conditioned on the previously gener- ated sequence y 1 ..y t−1 and input sequence x 1 = x 11 ..x 1|x 1 | . Probability are estimated with a soft- max over the vocabulary Y :</p><formula xml:id="formula_1">p(y t |y 1 ..y t−1 , x 1 ) = softmax yt∈Y (h t )<label>(2)</label></formula><p>where h t is the Recurrent Neural Network (RNN) state at time stamp t, which is modeled as</p><formula xml:id="formula_2">g(y t−1 , h t−1 , z t ). g(·)</formula><p>is a recurrent update func- tion for generating the new state h t based on the previous token y t−1 , the previous state h t−1 , and the input text representation z t . We imple- ment g using a Long Short-Term Memory (LSTM) RNNs <ref type="bibr" target="#b9">(Hochreiter and Schmidhuber, 1997</ref>).</p><p>The attention mechanism generates the repre- sentation of the input sequence x = x 11 ..x 1|x 1 | , and z t is computed as the weighted sum</p><formula xml:id="formula_3">z t = i=1..|x 1 | a i h(x 1i )</formula><p>, where a i is the attention co- efficient obtained for token x 1i and h is a func- tion that maps each x 1i to a continuous vector. In general, h is a function that projects x 1i by learn- ing a lookup table, and then embedding contex- tual words by defining an RNN. Coefficients a i are computed with a softmax over input tokens x 11 ..x 1|x 1 | :</p><formula xml:id="formula_4">a i = softmax x 1i ∈x (v(h(x 1i ), h t−1 ))<label>(3)</label></formula><p>Function v computes the affinity of each token x 1i and the current output context h t−1 . A common implementation of v is to apply a linear projection from h(x 1i ) : h t−1 (where : is the concatenation operation) into a fixed size vector, followed by a tanh and another linear projection.</p><p>Our Approach We extend the computation of z t for cases when x corresponds to multiple fields. <ref type="figure" target="#fig_2">Figure 3</ref> illustrates how the MTG card "Serra An- gel" is encoded, assuming that there are two singu- lar fields and one text field. We first encode each token x ki using the C2W model described in <ref type="bibr" target="#b19">Ling et al. (2015)</ref>, which is a replacement for lookup ta- bles where word representations are learned at the character level (cf. C2W row). A context-aware representation is built for words in the text fields using a bidirectional LSTM (cf. Bi-LSTM row).</p><p>Computing attention over multiple input fields is problematic as each input field's vectors have dif- ferent sizes and value ranges. Thus, we learn a linear projection mapping each input token x ki to a vector with a common dimensionality and value range (cf. Linear row). Denoting this process as f (x ki ), we extend Equation 3 as:</p><formula xml:id="formula_5">a ki = softmax x ki ∈x (v(f (x ki ), h t−1 ))<label>(4)</label></formula><p>Here a scalar coefficient a ki is computed for each input token x ki (cf. "Tanh", "Linear", and "Soft- max" rows). Thus, the overall input representation z t is computed as:</p><formula xml:id="formula_6">z t = k=1..|x|,i=1..|x k | a ij f (x ki )<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Latent Predictor Networks</head><p>Background In order to decode from x to y, many words must be copied into the code, such as the name of the card, the attack and the cost values. If we observe the HS card in <ref type="figure" target="#fig_0">Figure 1</ref> and the respective code in <ref type="figure" target="#fig_1">Figure 2</ref>, we observe that the name "Divine Favor" must be copied into the class name and in the constructor, along with the cost of the card "3". As explained earlier, this problem is not specific to our task: for in- stance, in the dataset of <ref type="bibr" target="#b24">Oda et al. (2015)</ref>, a model must learn to map from timeout = int ( timeout ) to "convert timeout into an integer.", where the name of the variable "timeout" must be copied into the output sequence. The same is- sue exists for proper nouns in machine translation which are typically copied from one language to the other. Pointer networks ( <ref type="bibr" target="#b31">Vinyals et al., 2015)</ref> address this by defining a probability distribution over a set of units that can be copied c = c 1 ..c |c| .</p><p>The probability of copying a unit c i is modeled as:</p><formula xml:id="formula_7">p(c i ) = softmax c i ∈c (v(h(c i ), q))<label>(6)</label></formula><p>As in the attention model (Equation 3), v is a func- tion that computes the affinity between an embed- ded copyable unit h(c i ) and an arbitrary vector q.</p><p>Our Approach Combining pointer networks with a character-based softmax is in itself difficult as these generate segments of different granularity and there is no ground truth of which predictor to use at each time stamp. We now describe Latent Predictor Networks, which model the conditional probability log P (y|x) over the latent sequence of predictors used to generate y.</p><p>We assume that our model uses multiple pre- dictors r ∈ R, where each r can generate multiple segments s t = y t ..y t+|st|−1 with ar- bitrary length |s t | at time stamp t. An ex- ample is illustrated in <ref type="figure" target="#fig_3">Figure 4</ref>, where we ob- serve that to generate the code init('Tirion Fordring',8,6,6), a pointer network can be used to generate the sequences y 13 7 =Tirion and y <ref type="bibr">22</ref> 14 =Fordring (cf. "Copy From Name" row). These sequences can also be generated us- ing a character softmax (cf. "Generate Characters" row). The same applies to the generation of the attack, health and cost values as each of these pre- dictors is an element in R. Thus, we define our ob- jective function as a marginal log likelihood func- tion over a latent variable ω:</p><formula xml:id="formula_8">log P (y | x) = log ω∈¯ ω P (y, ω | x)<label>(7)</label></formula><p>Formally, ω is a sequence of pairs r t , s t , where r t ∈ R denotes the predictor that is used at time- stamp t and s t the generated string. We decom- pose P (y, ω | x) as the product of the probabilities of segments s t and predictors r t :</p><formula xml:id="formula_9">P (y, ω | x) = rt,st∈ω P (s t , r t | y 1 ..y t−1 , x) = rt,st∈ω P (s t | y 1 ..y t−1 , x, r t )P (r t | y 1 ..y t−1 , x)</formula><p>where the generation of each segment is per- formed in two steps: select the predictor r t with probability P (r t | y 1 ..y t−1 , x) and then gener- ate s t conditioned on predictor r t with probabil- ity log P (s t | y 1 ..y t−1 , x, r t ). The probability of each predictor is computed using a softmax over all predictors in R conditioned on the previous state h t−1 and the input representation z t (cf. "Se- lect Predictor" box). Then, the probability of gen- erating the segment s t depends on the predictor type. We define three types of predictors:</p><p>Character Generation Generate a single char- acter from observed characters from the training data. Only one character is generated at each time stamp with probability given by Equation 2.</p><p>Copy Singular Field For singular fields only the field itself can be copied, for instance, the value of the attack and cost attributes or the type of card. The size of the generated segment is the number of characters in the copied field and the segment is generated with probability 1.</p><p>Copy Text Field For text fields, we allow each of the words x ki within the field to be copied. The probability of copying a word is learned with a pointer network (cf. "Copy From Name" box), where h(c i ) is set to the representation of the word f (x ki ) and q is the concatenation h t−1 : z t of the state and input vectors. This predictor generates a segment with the size of the copied word.</p><p>It is important to note that the state vector h t−1 is generated by building an RNN over the se- quence of characters up until the time stamp t − 1, i.e. the previous context y t−1 is encoded at the character level. This allows the number of pos- sible states to remain tractable at training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Inference</head><p>At training time we use back-propagation to max- imize the probability of observed code, according to Equation 7. Gradient computation must be per- formed with respect to each computed probabil- ity P (r t | y 1 ..y t−1 , x) and P (s t | y 1 ..y t−1 , x, r t ). The derivative ∂ log P (y|x) ∂P (rt|y 1 ..y t−1 ,x) yields:</p><formula xml:id="formula_10">∂α t P (r t | y 1 ..y t−1 , x)β t,rt + ξ rt P (y | x)∂P (r t | y 1 ..y t−1 , x) = α t β t,rt α |y|+1</formula><p>Here α t denotes the cumulative probability of all values of ω up until time stamp t and α |y|+1 yields the marginal probability P (y | x). β t,rt = P (s t | y 1 ..y t−1 )β t+|st|−1 denotes the cumulative proba- bility starting from predictor r t at time stamp t, ex- clusive. This includes the probability of the gener- ated segment P (s t | y 1 ..y t−1 , x, r t ) and the proba- bility of all values of ω starting from timestamp t+ |s t |−1, that is, all possible sequences that generate segment y after segment s t is produced. For com- pleteness, ξ r denotes the cumulative probabilities of all ω that do not include r t . To illustrate this, we refer to <ref type="figure" target="#fig_3">Figure 4</ref> and consider the timestamp t = 14, where the segment s 14 =Fordring is generated. In this case, the cumulative probability α 14 is the sum of the path that generates the se- quence init('Tirion with characters alone, and the path that generates the word Tirion by copying from the input. β 21 includes the prob- ability of all paths that follow the generation of Fordring, which include 2×3×3 different paths due to the three decision points that follow (e.g. generating 8 using a character softmax vs. copy- ing from the cost). Finally, ξ r refers to the path that generates Fordring character by character. While the number of possible paths grows ex- ponentially, α and β can be computed efficiently using the forward-backward algorithm for Semi- Markov models <ref type="bibr" target="#b27">(Sarawagi and Cohen, 2005</ref>), where we associate P (r t | y 1 ..y t−1 , x) to edges and P (s t | y 1 ..y t−1 , x, r t ) to nodes in the Markov chain.</p><p>The derivative</p><formula xml:id="formula_11">∂ log P (y|x)</formula><p>∂P (st|y 1 ..y t−1 ,x,rt) can be com- puted using the same logic:</p><formula xml:id="formula_12">∂α t,st P (s t | y 1 ..y t−1 , x, r t )β t+|st|−1 + ξ rt P (y | x)∂P (s t | y 1 ..y t−1 , x, r t ) = α t,rt β t+|st|−1 α |y|+1</formula><p>Once again, we denote α t,rt = α t P (r t | y 1 ..y t−1 , x) as the cumulative probability of all values of ω that lead to s t , exclusive.</p><p>An intuitive interpretation of the derivatives is that gradient updates will be stronger on prob- ability chains that are more likely to generate the output sequence. For instance, if the model learns a good predictor to copy names, such as Fordring, other predictors that can also gener- ate the same sequences, such as the character soft- max will allocate less capacity to the generation of names, and focus on elements that they excel at (e.g. generation of keywords).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Decoding</head><p>Decoding is performed using a stack-based de- coder with beam search. Each state S corre- sponds to a choice of predictor r t and segment s t at a given time stamp t. This state is scored as V (S) = log P (s t | y 1 ..y t−1 , x, r t ) + log P (r t | y 1 ..y t−1 , x) + V (prev(S)), where prev(S) de- notes the predecessor state of S. At each time stamp, the n states with the highest scores V are expanded, where n is the size of the beam. For each predictor r t , each output s t generates a new state. Finally, at each timestamp t, all states which produce the same output up to that point are merged by summing their probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Code Compression</head><p>As the attention-based model traverses all input units at each generation step, generation becomes quite expensive for datasets such as MTG where the average card code contains 1,080 characters. While this is not the essential contribution in our paper, we propose a simple method to compress the code while maintaining the structure of the code, allowing us to train on datasets with longer code (e.g., MTG).</p><p>The idea behind that method is that many keywords in the programming language (e.g., public and return) as well as frequently used functions and classes (e.g., Card) can be learned without character level information. We exploit this by mapping such strings onto additional sym- bols X i (e.g., public class copy() → "X 1 X 2 X 3 ()"). Formally, we seek the stringˆvstringˆ stringˆv among all strings V (max) up to length max that maximally reduces the size of the corpus:</p><formula xml:id="formula_13">ˆ v = argmax v∈V (max) (len(v) − 1)C(v)<label>(8)</label></formula><p>where C(v) is the number of occurrences of v in the training corpus and len(v) its length.</p><p>(len(v) − 1)C(v) can be seen as the number of characters reduced by replacing v with a non- terminal symbol. To find q(v) efficiently, we lever- age the fact that C(v) ≤ C(v ) if v contains v . It follows that (max − 1)C(v) ≤ (max − 1)C(v ), which means that the maximum compression ob- tainable for v at size max is always lower than that of v . Thus, if we can find a ¯ v such that (len(¯ v) − 1)C(¯ v) &gt; (max − 1)C(v ), that is ¯ v at the current size achieves a better compression rate than v at the maximum length, then it fol- lows that all sequences that contain v can be dis- carded as candidates. Based on this idea, our itera- tive search starts by obtaining the counts C(v) for all segments of size s = 2, and computing the best scoring segment ¯ v. Then, we build a list L(s) of all segments that achieve a better compression rate than ¯ v at their maximum size. At size s + 1, only segments that contain a element in L(s − 1) need to be considered, making the number of substrings to be tested to be tractable as s increases. The al- gorithm stops once s reaches max or the newly generated list L(s) contains no elements.    Oncê v is obtained, we replace all occurrences ofˆvofˆ ofˆv with a new non-terminal symbol. This pro- cess is repeated until a desired average size for the code is reached. While training is performed on the compressed code, the decoding will undergo an additional step, where the compressed code is restored by expanding the all X i . <ref type="table" target="#tab_2">Table 2</ref> shows the first 10 replacements from the MTG dataset, reducing its average size from 1080 to 794.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experiments</head><p>Datasets Tests are performed on the two datasets provided in this paper, described in Ta- ble 1. Additionally, to test the model's ability of generalize to other domains, we report results in the Django dataset ( <ref type="bibr" target="#b24">Oda et al., 2015</ref>), comprising of 16000 training, 1000 development and 1805 test annotations. Each data point consists of a line of Python code together with a manually created nat- ural language description.</p><p>Neural Benchmarks We implement two stan- dard neural networks, namely a sequence-to- sequence model <ref type="bibr" target="#b29">(Sutskever et al., 2014</ref>) and an attention-based model ( <ref type="bibr" target="#b3">Bahdanau et al., 2014</ref>). The former is adapted to work with multiple in- put fields by concatenating them, while the latter uses our proposed attention model. These models are denoted as "Sequence" and "Attention".</p><p>Machine Translation Baselines Our problem can also be viewed in the framework of seman- tic parsing <ref type="bibr" target="#b33">(Wong and Mooney, 2006;</ref><ref type="bibr" target="#b20">Lu et al., 2008;</ref><ref type="bibr" target="#b10">Jones et al., 2012;</ref><ref type="bibr" target="#b2">Artzi et al., 2015)</ref>. Unfor- tunately, these approaches define strong assump- tions regarding the grammar and structure of the output, which makes it difficult to generalize for other domains ( <ref type="bibr" target="#b15">Kwiatkowski et al., 2010)</ref>. How- ever, the work in <ref type="bibr" target="#b1">Andreas et al. (2013)</ref> provides evidence that using machine translation systems without committing to such assumptions can lead to results competitive with the systems described above. We follow the same approach and create a phrase-based ( <ref type="bibr" target="#b13">Koehn et al., 2007)</ref> model and a hierarchical model (or PCFG) <ref type="bibr" target="#b6">(Chiang, 2007)</ref> as benchmarks for the work presented here. As these models are optimized to generate words, not char- acters, we implement a tokenizer that splits on all punctuation characters, except for the " " charac- ter. We also facilitate the task by splitting Camel- Case words (e.g., class TirionFordring → class Tirion Fordring). Otherwise all class names would not be generated correctly by these methods. We used the models implemented in Moses to generate these baselines using stan- dard parameters, using IBM Alignment Model 4 for word alignments <ref type="bibr" target="#b23">(Och and Ney, 2003)</ref>, MERT for tuning ( <ref type="bibr" target="#b28">Sokolov and Yvon, 2011</ref>) and a 4-gram Kneser-Ney Smoothed language model <ref type="bibr" target="#b8">(Heafield et al., 2013</ref>). These models will be denoted as "Phrase" and "Hierarchical", respectively.</p><p>Retrieval Baseline It was reported in <ref type="bibr" target="#b26">(Quirk et al., 2015</ref>) that a simple retrieval method that out- puts the most similar input for each sample, mea- sured using Levenshtein Distance, leads to good results. We implement this baseline by computing the average Levenshtein Distance for each input field. This baseline is denoted "Retrieval".</p><p>Evaluation A typical metric is to compute the accuracy of whether the generated code exactly matches the reference code. This is informative as it gives an intuition of how many samples can be used without further human post-editing. How- ever, it does not provide an illustration on the de- gree of closeness to achieving the correct code. Thus, we also test using BLEU-4 ( <ref type="bibr" target="#b25">Papineni et al., 2002</ref>) at the token level. There are clearly problems with these metrics. For instance, source code can be correct without matching the refer- ence. The code in <ref type="figure" target="#fig_1">Figure 2</ref>, could have also been implemented by calling the draw function in an cycle that exists once both players have the same number of cards in their hands. Some tasks, such as the generation of queries ( <ref type="bibr" target="#b35">Zelle and Mooney, 1996)</ref>, have overcome this problem by executing the query and checking if the result is the same as the annotation. However, we shall leave the study of these methologies for future work, as adapting these methods for our tasks is not triv- ial. For instance, the correctness cards with con- ditional (e.g. if player has no cards, then draw a card) or non-deterministc (e.g. put a random card in your hand) ef- fects cannot be simply validated by running the code.</p><p>Setup The multiple input types <ref type="figure" target="#fig_2">(Figure 3</ref>) are hyper-parametrized as follows: The C2W model (cf. "C2W" row) used to obtain continuous vec- tors for word types uses character embeddings of size 100 and LSTM states of size 300, and gener- ates vectors of size 300. We also report on results using word lookup tables of size 300, where we replace singletons with a special unknown token with probability 0.5 during training, which is then used for out-of-vocabulary words. For text fields, the context (cf. "Bi-LSTM" row) is encoded with a Bi-LSTM of size 300 for the forward and back- ward states. Finally, a linear layer maps the differ- ent input tokens into a common space with of size 300 (cf. "Linear" row). As for the attention model, we used an hidden layer of size 200 before ap- plying the non-linearity (row "Tanh"). As for the decoder <ref type="figure" target="#fig_3">(Figure 4)</ref>, we encode output characters with size 100 (cf. "output (y)" row), and an LSTM state of size 300 and an input representation of size 300 (cf. "State(h+z)" row). For each pointer network (e.g., "Copy From Name" box), the inter- section between the input units and the state units are performed with a vector of size 200. Train- ing is performed using mini-batches of 20 sam- ples using AdaDelta <ref type="bibr" target="#b34">(Zeiler, 2012)</ref> and we report results using the iteration with the highest BLEU score on the validation set (tested at intervals of 5000 mini-batches). Decoding is performed with a beam of 1000. As for compression, we performed a grid search over compressing the code from 0% to 80% of the original average length over inter- vals of 20% for the HS and Django datasets. On the MTG dataset, we are forced to compress the code up to 80% due to performance issues when training with extremely long sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Results</head><p>Baseline Comparison Results are reported in <ref type="table" target="#tab_4">Table 3</ref>. Regarding the retrieval results (cf. "Re- trieval" row), we observe the best BLEU scores among the baselines in the card datasets (cf. "MTG" and "HS" columns). A key advantage of this method is that retrieving existing entities guarantees that the output is well formed, with no   <ref type="table">Table 4</ref>: Results with increasing compression rates with a regular softmax (cf. "Softmax") and a LPN (cf. "LPN"). Performance values (cf. "Seconds Per Card" block) are computed using one CPU.</p><p>syntactic errors such as producing a non-existent function call or generating incomplete code. As BLEU penalizes length mismatches, generating code that matches the length of the reference pro- vides a large boost. The phrase-based transla- tion model (cf. "Phrase" row) performs well in the Django (cf. "Django" column), where map- ping from the input to the output is mostly mono- tonic, while the hierarchical model (cf. "Hierar- chical" row) yields better performance on the card datasets as the concatenation of the input fields needs to be reordered extensively into the out- put sequence. Finally, the sequence-to-sequence model (cf. "Sequence" row) yields extremely low results, mainly due to the lack of capacity needed to memorize whole input and output sequences, while the attention based model (cf. "Attention" row) produces results on par with phrase-based systems. Finally, we observe that by including all the proposed components (cf. "Our System" row), we obtain significant improvements over all base- lines in the three datasets and is the only one that obtains non-zero accuracies in the card datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Component Comparison</head><p>We present ablation results in order to analyze the contribution of each of our modifications. Removing the C2W model (cf. "-C2W" row) yields a small deterioration, as word lookup tables are more susceptible to spar- sity. The only exception is in the HS dataset, where lookup tables perform better. We believe that this is because the small size of the training set does not provide enough evidence for the char- acter model to scale to unknown words. Surpris- ingly, running our model compression code (cf. "- Compress" row) actually yields better results. <ref type="table">Ta- ble 4</ref> provides an illustration of the results for dif- ferent compression rates. We obtain the best re- sults with an 80% compression rate (cf. "BLEU Scores" block), while maximising the time each card is processed (cf. "Seconds Per Card" block).</p><p>While the reason for this is uncertain, it is simi- lar to the finding that language models that output characters tend to under-perform those that output words <ref type="bibr">(Józefowicz et al., 2016)</ref>. This applies when using the regular optimization process with a char- acter softmax (cf. "Softmax" rows), but also when using the LPN (cf. "LPN" rows). We also note that the training speed of LPNs is not significantly lower as marginalization is performed with a dy- namic program. Finally, a significant decrease is observed if we remove the pointer networks (cf. "- LPN" row). These improvements also generalize to sequence-to-sequence models (cf. "-Attention" row), as the scores are superior to the sequence-to- sequence benchmark (cf. "Sequence" row).</p><p>Result Analysis Examples of the code gener- ated for two cards are illustrated in <ref type="figure" target="#fig_6">Figure 5</ref>. We obtain the segments that were copied by the pointer networks by computing the most likely predictor for those segments. We observe from the marked segments that the model effectively copies the attributes that match in the output, including the name of the card that must be collapsed. As expected, the majority of the errors originate from inaccuracies in the generation of the effect of the card. While it is encouraging to observe that a small percentage of the cards are generated cor- rectly, it is worth mentioning that these are the re- sult of many cards possessing similar effects. The "Madder Bomber" card is generated correctly as there is a similar card "Mad Bomber" in the train- ing set, which implements the same effect, except that it deals 3 damage instead of 6. Yet, it is a promising result that the model was able to capture this difference. However, in many cases, effects that radically differ from seen ones tend to be gen- erated incorrectly. In the card "Preparation", we observe that while the properties of the card are generated correctly, the effect implements a unre- lated one, with the exception of the value 3, which is correctly copied. Yet, interestingly, it still gener- ates a valid effect, which sets a minion's attack to 3. Investigating better methods to accurately gen- erate these effects will be object of further studies. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related Work</head><p>While we target widely used programming lan- guages, namely, Java and Python, our work is related to studies on the generation of any ex- ecutable code. These include generating regu- lar expressions <ref type="bibr" target="#b14">(Kushman and Barzilay, 2013)</ref>, and the code for parsing input documents ( <ref type="bibr" target="#b18">Lei et al., 2013)</ref>. Much research has also been in- vested in generating formal languages, such as database queries <ref type="bibr" target="#b35">(Zelle and Mooney, 1996;</ref><ref type="bibr" target="#b4">Berant et al., 2013)</ref>, agent specific language ( <ref type="bibr" target="#b12">Kate et al., 2005</ref>) or smart phone instructions ( <ref type="bibr" target="#b17">Le et al., 2013</ref>). Finally, mapping natural language into a sequence of actions for the generation of executable code <ref type="bibr" target="#b5">(Branavan et al., 2009</ref>). Fi- nally, a considerable effort in this task has fo- cused on semantic parsing ( <ref type="bibr" target="#b33">Wong and Mooney, 2006;</ref><ref type="bibr" target="#b10">Jones et al., 2012;</ref><ref type="bibr" target="#b18">Lei et al., 2013;</ref><ref type="bibr" target="#b2">Artzi et al., 2015;</ref><ref type="bibr" target="#b26">Quirk et al., 2015)</ref>. Recently pro- posed models focus on Combinatory Categorical Grammars ( <ref type="bibr" target="#b14">Kushman and Barzilay, 2013;</ref><ref type="bibr" target="#b2">Artzi et al., 2015)</ref>, Bayesian Tree Transducers ( <ref type="bibr" target="#b10">Jones et al., 2012;</ref><ref type="bibr" target="#b18">Lei et al., 2013</ref>) and Probabilistic Con- text Free Grammars ( <ref type="bibr" target="#b1">Andreas et al., 2013</ref>). The work in natural language programming <ref type="bibr" target="#b30">(Vadas and Curran, 2005;</ref><ref type="bibr" target="#b21">Manshadi et al., 2013)</ref>, where users write lines of code from natural language, is also related to our work. Finally, the reverse map- ping from code into natural language is explored in ( <ref type="bibr" target="#b24">Oda et al., 2015)</ref>.</p><p>Character-based sequence-to-sequence models have previously been used to generate code from natural language in ( <ref type="bibr" target="#b22">Mou et al., 2015)</ref>. Inspired by these works, LPNs provide a richer framework by employing attention models ( <ref type="bibr" target="#b3">Bahdanau et al., 2014</ref>), pointer networks ( <ref type="bibr" target="#b31">Vinyals et al., 2015)</ref> and character-based embeddings ( <ref type="bibr" target="#b19">Ling et al., 2015)</ref>. Our formulation can also be seen as a generaliza- tion of <ref type="bibr" target="#b0">Allamanis et al. (2016)</ref>, who implement a special case where two predictors have the same granularity (a sub-token softmax and a pointer net- work). Finally, HMMs have been employed in neural models to marginalize over label sequences in <ref type="bibr" target="#b7">(Collobert et al., 2011;</ref><ref type="bibr" target="#b16">Lample et al., 2016)</ref> by modeling transitions between labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>We introduced a neural network architecture named Latent Prediction Network, which allows efficient marginalization over multiple predictors. Under this architecture, we propose a generative model for code generation that combines a char- acter level softmax to generate language-specific tokens and multiple pointer networks to copy key- words from the input. Along with other exten- sions, namely structured attention and code com- pression, our model is applied on on both exist- ing datasets and also on a newly created one with implementations of TCG game cards. Our experi- ments show that our model out-performs multiple benchmarks, which demonstrate the importance of combining different types of predictors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example MTG and HS cards.</figDesc><graphic url="image-1.png" coords="1,318.19,222.54,196.44,138.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Code for the HS card "Divine Favor".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration of the structured attention mechanism operating on a single time stamp t.</figDesc><graphic url="image-2.png" coords="3,307.27,62.81,218.29,120.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Generation process for the code init('Tirion Fordring',8,6,6) using LPNs.</figDesc><graphic url="image-3.png" coords="4,80.51,62.81,436.52,228.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Examples of decoded cards from HS. Copied segments are marked in green and incorrect segments are marked in red.</figDesc><graphic url="image-4.png" coords="9,72.00,211.66,218.26,164.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>First 10 compressed units in MTG. We 
replaced newlines with ⇓ and spaces with . 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>BLEU and Accuracy scores for the pro-
posed task on two in-domain datasets (HS and 
MTG) and an out-of-domain dataset (Django). 

Compression 
0% 20% 40% 60% 80% 

Seconds Per Card 
Softmax 
2.81 2.36 1.88 1.42 0.94 
LPN 
3.29 2.65 2.35 1.93 1.41 
BLEU Scores 
Softmax 
44.2 46.9 47.2 51.4 52.7 
LPN 
59.7 62.8 61.1 66.4 67.1 

</table></figure>

			<note place="foot" n="1"> Dataset available at https://deepmind.com/publications.html</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A Convolutional Attention Network for Extreme Summarization of Source Code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-02" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic parsing as machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="page" from="47" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Broad-coverage ccg semantic parsing with amr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="1699" to="1710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic parsing on freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1533" to="1544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reinforcement learning for mapping instructions to actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R K</forename><surname>Branavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harr</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="82" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hierarchical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="228" />
			<date type="published" when="2007-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Scalable modified Kneser-Ney language model estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Pouzyrevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 51th Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="690" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semantic parsing with bayesian tree transducers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keeley</forename><surname>Bevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goldwater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="488" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Exploring the limits of language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Józefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno>abs/1602.02410</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to transform natural to formal languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuk</forename><forename type="middle">Wah</forename><surname>Kate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twentieth National Conference on Artificial Intelligence (AAAI05)</title>
		<meeting>the Twentieth National Conference on Artificial Intelligence (AAAI05)<address><addrLine>Pittsburgh, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-07" />
			<biblScope unit="page" from="1062" to="1068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions</title>
		<meeting>the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Using semantic unification to generate regular expressions from natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="826" to="836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Inducing probabilistic ccg grammars from logical form with higherorder unification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1223" to="1233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<title level="m">Neural Architectures for Named Entity Recognition</title>
		<imprint>
			<date type="published" when="2016-03" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Smartsynth: Synthesizing smartphone automation scripts from natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Vu Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Gulwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the 11th Annual International Conference on Mobile Systems, Applications, and Services</title>
		<meeting>eeding of the 11th Annual International Conference on Mobile Systems, Applications, and Services</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="193" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">From natural language specifications to program input parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Rinard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2013-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1294" to="1303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Finding function in form: Compositional character models for open vocabulary word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Luís</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luís</forename><surname>Marujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rámon</forename><surname>Fernandez Astudillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A generative model for parsing natural language to meaning representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wee</forename><surname>Sun Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;08</title>
		<meeting>the 2008 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;08<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="783" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Integrating programming by example and natural language programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Mehdi Hafezi Manshadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">F</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Allen</surname></persName>
		</author>
		<editor>Marie desJardins and Michael L. Littman</editor>
		<imprint>
			<date type="published" when="2013" />
			<publisher>AAAI. AAAI Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">On end-to-end program generation from user intention by deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
		<idno>abs/1510.07211</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A systematic comparison of various statistical alignment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="51" />
			<date type="published" when="2003-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to generate pseudo-code from source code using statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Oda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Fudaba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideaki</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sakriani</forename><surname>Sakti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoki</forename><surname>Toda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th IEEE/ACM International Conference on Automated Software Engineering (ASE)</title>
		<meeting><address><addrLine>Lincoln, Nebraska, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bleu: A method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Language to code: Learning semantic parsers for if-this-then-that recipes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="page" from="878" to="888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semimarkov conditional random fields for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>L. K. Saul, Y. Weiss, and L. Bottou</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1185" to="1192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Minimum Error Rate Semi-Ring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Sokolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Yvon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Machine Translation</title>
		<editor>Mikel Forcada and Heidi Depraetere</editor>
		<meeting>the European Conference on Machine Translation<address><addrLine>Leuven, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="241" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno>abs/1409.3215</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Programming with unrestricted natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vadas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Australasian Language Technology Workshop 2005</title>
		<meeting>the Australasian Language Technology Workshop 2005<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-12" />
			<biblScope unit="page" from="191" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<editor>C. Cortes, N.D</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garnett</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2674" to="2682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning for semantic parsing with statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuk</forename><forename type="middle">Wah</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics</title>
		<meeting>the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="439" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">ADADELTA: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno>abs/1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning to parse database queries using inductive logic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Zelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI/IAAI</title>
		<meeting><address><addrLine>Portland, OR, August</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press/MIT Press</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="1050" to="1055" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
