<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:00+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Accurate Linear-Time Chinese Word Segmentation via Embedding Matching</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Linguistics</orgName>
								<orgName type="department" key="dep2">SFB 833 and Department of Linguistics</orgName>
								<orgName type="institution" key="instit1">SFB 833</orgName>
								<orgName type="institution" key="instit2">University of Tübingen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erhard</forename><surname>Hinrichs</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Tübingen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Accurate Linear-Time Chinese Word Segmentation via Embedding Matching</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1733" to="1743"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper proposes an embedding matching approach to Chinese word segmenta-tion, which generalizes the traditional sequence labeling framework and takes advantage of distributed representations. The training and prediction algorithms have linear-time complexity. Based on the proposed model, a greedy segmenter is developed and evaluated on benchmark corpora. Experiments show that our greedy segmenter achieves improved results over previous neural network-based word seg-menters, and its performance is competitive with state-of-the-art methods, despite its simple feature set and the absence of external resources for training.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Chinese sentences are written as character se- quences without word delimiters, which makes word segmentation a prerequisite of Chinese lan- guage processing. Since <ref type="bibr" target="#b26">Xue (2003)</ref>, most work has formulated Chinese word segmentation (CWS) as sequence labeling <ref type="bibr" target="#b17">(Peng et al., 2004</ref>) with char- acter position tags, which has lent itself to struc- tured discriminative learning with the benefit of allowing rich features of segmentation configura- tions, including (i) context of character/word n- grams within local windows, (ii) segmentation his- tory of previous characters, or the combinations of both. These feature-based models still form the backbone of most state-of-the art systems.</p><p>Nevertheless, many feature weights in such models are inevitably poorly estimated because the number of parameters is so large with respect to the limited amount of training data. This has mo- tivated the introduction of low-dimensional, real- valued vectors, known as embeddings, as a tool to deal with the sparseness of the input. Em- beddings allow linguistic units appearing in sim- ilar contexts to share similar vectors. The suc- cess of embeddings has been observed in many NLP tasks. For CWS, <ref type="bibr" target="#b36">Zheng et al. (2013</ref><ref type="bibr">) adapted Collobert et al. (2011</ref> and uses character embed- dings in local windows as input for a two-layer net- work. The network predicts individual character position tags, the transitions of which are learned separately.  also developed a similar architecture, which labels individual char- acters and uses character bigram embeddings as additional features to compensate the absence of sentence-level modeling. <ref type="bibr" target="#b16">Pei et al. (2014)</ref> im- proved upon <ref type="bibr" target="#b36">Zheng et al. (2013)</ref> by capturing the combinations of context and history via a tensor neural network.</p><p>Despite their differences, these CWS ap- proaches are all sequence labeling models. In such models, the target character can only influence the prediction as features. Consider the the segmen- tation configuration in (1), where the dot appears before the target character in consideration and the box (2) represents any character that can occur in the configuration. In that example, the known his- tory is that the first two characters 中国 'China' are joined together, which is denoted by the underline.</p><p>(1) 中国·2 格外 (where 2 ∈ {风, 规, ...}) (2) 中国风 格外 'China-style especially' (3) 中国 规格 外 'besides Chinese spec.'</p><p>For possible target characters, 风 'wind' and 规 'rule', the correct segmentation decisions for them are opposite, as shown in (2) and (3), respectively. In order to correctly predict both, current models can set higher weights for target character-specific features. However, in general, 风 is more likely to start a new word instead of joining the exist- ing one as in this example. Given such conflicting evidence, models can rarely find optimal feature weights, if they exist at all.</p><p>The crux of this conflicting evidence problem is that similar configurations can suggest opposite decisions, depending on the target character and vice versa. Thus it might be useful to treat segmen- tation decisions for distinct characters separately. And instead of predicting general segmentation de- cisions given configurations, it could be beneficial to model the matching between configurations and character-specific decisions.</p><p>To this end, this paper proposes an embed- ding matching approach (Section 2) to CWS, in which embeddings for both input and output are learned and used as representations to counteract sparsities. Thanks to embeddings of character- specific decisions (actions) serving as both input features and output, our hidden-layer-free archi- tecture (Section 2.2) is capable of capturing pre- diction histories in similar ways as the hidden lay- ers in recurrent neural networks <ref type="bibr" target="#b14">(Mikolov et al., 2010)</ref>. We evaluate the effectiveness of the model via a linear-time greedy segmenter (Section 3) im- plementation. The segmenter outperforms previ- ous embedding-based models (Section 4.2) and achieves state-of-the-art results (Section 4.3) on a benchmark dataset. The main contributions of this paper are:</p><p>• A novel embedding matching model for Chi- nese word segmentation.</p><p>• Developing a greedy word segmenter, which is based on the matching model and achieves competitive results.</p><p>• Introducing the idea of character-specific seg- mentation action embeddings as both feature and output, which are cornerstones of the model and the segmenter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Embedding Matching Models for Chinese Word Segmentation</head><p>We propose an embedding based matching model for CWS, the architecture of which is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The model employs trainable embed- dings to represent both sides of the matching, which will be specified shortly, followed by details of the architecture in Section 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Segmentation as Configuration-Action Matching</head><p>Output. The word segmentation output of a char- acter sequence can be described as a sequence of character-specific segmentation actions. We use separation (s) and combination (c) as possible actions for each character, where a separation ac- tion starts a new word with the current character, while a combination action appends the character to the preceding ones. We model character-action combinations instead of atomic, character inde- pendent actions. As a running example, sentence (4b) is the correct segmentation for (4a), which can be represented as the sequence (猫 -s, 占 -s, 领 -c,</p><formula xml:id="formula_0">了 -s, 婴 -s, 儿 -c, 床 -c) . (4) a. 猫占领了婴儿床 b. 猫 占领 了 婴儿床 c. '</formula><p>The cat occupied the crib'</p><p>Input. The input are the segmentation configura- tions for each character under consideration, which are described by context and history features. The context features of captures the characters that are in the same sentence of the current character and the history features encode the segmentation ac- tions of previous characters.</p><p>• Context features. These refer to character unigrams and bigrams that appear in the lo- cal context window of h characters that cen- ters at c i , where c i is 领 in example (4) and h = 5 is used in this paper. The template for features are shown in <ref type="table" target="#tab_0">Table 1</ref>. For our exam- ple, the uni-and bi-gram features would be: 猫, 占, 领, 了, 婴 and 猫占, 占领, 领了, 了 婴, respectively.</p><p>• History features.</p><p>To make inference tractable, we assume that only previous l character-specific actions are relevant, where l = 2 for this study. In our example, 猫 -s and 战 -s are the history features. Such fea- tures capture partial information of syntactic and semantic dependencies between previous words, which are clues for segmentation that pure character contexts could not provide. A dummy character START is used to represent the absent (left) context characters in the case of the first l characters in a sentence. And the predicted action for the START symbol is al- ways s.</p><p>Matching. CWS is now modeled as the match- ing of the input (segmentation configuration) and output (two possible character-specific actions) for each character. Formally, a matching model learns </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Group</head><p>Feature template </p><formula xml:id="formula_1">unigram c i−2 , c i−1 , c i , c i+1 , c i+2 bigram c i−2 c i−1 , c i−1 c i , c i c i+1 , c i+1 c i+2</formula><formula xml:id="formula_2">g ( b 1 b 2 ...b n , a 1 a 2 ...a n ) = n ∏ j=1 f ( b j (a j−2 , a j−1 ; c j− h 2 ...c j+ h 2 ), a j )<label>(1)</label></formula><p>where c 1 c 2 ...c n is the character sequence, b j and a j are the segmentation configuration and action for character c j , respectively. In (1),</p><formula xml:id="formula_3">b j (a j−2 , a j−1 ; c j− h 2 ...c j+ h 2 )</formula><p>indicates that the con- figuration for each character is a function that de- pends on the actions of the previous l characters and the characters in the local window of size h. Why embedding. The above matching model would suffer from sparsity if these outputs (character-specific action a j ) were directly en- coded as one-hot vectors, since the matching model can be seen as a sequence labeling model with C ×L outputs, where L is the number of orig- inal labels while C is the number of unique char- acters. For Chinese, C is at the order of 10 3 − 10 4 .</p><p>The use of embeddings, however, can serve the matching model well thanks to their low dimen- sionality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Architecture</head><p>The proposed architecture <ref type="figure" target="#fig_0">(Figure 1</ref>) has three components, namely look-up table, concatenation and softmax function for matching. We will go through each of them in this section.</p><p>Look-up table. The mapping between fea- tures/outputs to their corresponding embeddings are kept in a look-up table, as in many previous embedding related work ( <ref type="bibr" target="#b0">Bengio et al., 2003;</ref><ref type="bibr" target="#b16">Pei et al., 2014</ref>). Such features are extracted from the training data. Formally, the embedding for each distinct feature d is denoted as Embed(d) ∈ R N , which is a real valued vector of dimension N . Each feature is retrieved by its unique index. The retrieval of the embeddings for the output actions is similar.</p><p>Concatenation. To predict the segmentation for the target character c j , its feature vectors are con- catenated into a single vector, the input embed- ding, i(b j ) ∈ R N ×K , where K is the number of features used to describe the configuration b j .</p><p>Softmax. The model then computes the dot product of the input embedding i(b j ) and each of the two output embeddings, o(a j,1 ) and o(a j,2 ), which represent the two possible segmentation ac- tions for the target character c j , respectively. The exponential of the two raw scores are normalized to obtain probabilistic values ∈ <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>.</p><p>We call the resulting scores matching probabili- ties, which denote probabilities that actions match the given segmentation configuration. In our ex- ample, 领 -c has the probability of 0.7 to be the cor- rect action, while 领 -s is less likely with a lower probability of 0.3. Formally, the above matching procedure can be described as a softmax function, as shown in <ref type="formula">(2)</ref>, which is also an individual f term in (1).</p><formula xml:id="formula_4">f ( b j , a j,k ) = exp (i(b j ) · o(a j,k )) ∑ k ′ exp ( i(b j ) · o(a j,k ′ )</formula><p>) <ref type="formula">(2)</ref> In <ref type="formula">(2)</ref>, a j,k (1 ≤ k ≤ 2) represent two possible actions, such as 领 -c and 领 -s for 领 in our ex- ample. Note that, to ensure the input and output are of the same dimension, for each character specific action, the model trains two distinct embeddings, one ∈ R N as feature and the other ∈ R N ×K as output, where K is the number of features for each input.</p><p>Best word segmentation of sentence. After plugging (2) into (1) and applying (and then drop- ping) logarithms for computational convenience, finding the best segmentation for a sentence be- comes an optimization problem as shown in (3). In the formula, ˆ Y is the best action sequence found by the model among all the possible ones, Y = a 1 a 2 ...a n , where a j is the predicted action for the character c j (1 ≤ j ≤ n), which is either c j -s or c j -c, such as 领 -s and 领 -c.</p><formula xml:id="formula_5">ˆ Y = argmax Y n ∑ j=1 exp (i(b j ) · o(a j )) ∑ k exp (i(b j ) · o(a j,k ))<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Greedy Segmenter</head><p>Our model depends on the actions predicted for the previous two characters as history features. Tradi- tionally, such scenarios call for dynamic program- ming for exact inference. However, preliminary experiments showed that, for our model, a Viterbi search based segmenter, even supported by con- ditional random field ( <ref type="bibr" target="#b8">Lafferty et al., 2001</ref>) style training, yields similar results as the greedy search based segmenter in this section. Since the greedy segmenter is much more efficient in training and testing, the rest of the paper will focus on the pro- posed greedy segmenter, the details of which will be described in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Greedy Search</head><p>Initialization. The first character in the sentence is made to have two left side characters that are dummy symbols of START, whose predicted ac- tions are always START-s, i.e. separation.</p><p>Iteration. The algorithms predicts the action for each character c j , one at a time, in a left-to-right, incremental manner, where 1 ≤ j ≤ n and n is the sentence length. To do so, it first extracts context features and history features, the latter of which are the predicted character-specific actions for the pre- vious two characters. Then the model matches the concatenated feature embedding with embeddings of the two possible character-specific actions, c j -s and c i -c. The one with higher matching probability is predicted as segmentation action for the charac- ter, which is irreversible. After the action for the last character is predicted, the segmented word se- quence of the sentence is built from the predicted actions deterministically.</p><p>Hybrid matching. Character-specific embed- dings are capable of capturing subtle word forma- tion tendencies of individual characters, but such representations are incapable of covering match- ing cases for unknown target characters. An- other minor issue is that the action embeddings for certain low frequent characters may not be suf- ficiently trained. To better deal with these sce- narios, We also train two embeddings to repre- sent character-independent segmentation actions, ALL-s and ALL-c, and use them to average with or substitute embeddings of infrequent or unknown characters, which are either insufficiently trained or nonexistent. Such strategy is called hybrid matching, which can improve accuracy.</p><p>Complexity. Although the total number of ac- tions is large, the matching for each target charac- ter only requires the two actions that correspond to that specific character, such as 领 -s and 领 -c for 领 in our example. Each prediction is thus similar to a softmax computation with two outputs, which costs constant time C. Greedy search ensures that the total time for predicting a sentence of n char- acters is n × C, i.e. linear time complexity, with a minor overhead for mapping actions to segmenta- tions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training</head><p>The training procedure first predicts the action for the current character with current parameters, and then optimizes the log likelihood of correct seg- mentation actions in the gold segmentations to up- date parameters. Ideally, the matching probability for the correct action embedding should be 1 while that of the incorrect one should be 0. We minimize the cross-entropy loss function as in <ref type="formula" target="#formula_6">(4)</ref> for the seg- mentation prediction of each character c j to pursue this goal. The loss function is convex, similar to that of maximum entropy models.</p><formula xml:id="formula_6">J = − K ∑ k=1 δ (a j,k ) log exp (i · o(a j,k )) ∑ k ′ exp ( i · o(a j,k ′ ) )<label>(4)</label></formula><p>where a j,k denotes a possible action for c j and i is a compact notation for i(b j ). In (4), δ(a j,k ) is an in- dicator function defined by the following formula, wherê a j denotes the correct action.</p><formula xml:id="formula_7">δ(a j,k ) = { 1, if a j,k = ˆ a j 0, otherwise</formula><p>To counteract over-fitting, we add L2 regulariza- tion term to the loss function, as follows:</p><formula xml:id="formula_8">J = J + K ∑ k=1 λ 2 ( ||i|| 2 + ||o(a j,k )|| 2 )<label>(5)</label></formula><p>The formula in (4) and (5) are similar to that of a standard softmax regression, except that both in- put and output embeddings are parameters to be trained. We perform stochastic gradient descent to update input and output embeddings in turn, each time considering the other as constant. We give the gradient (6) and the update rule <ref type="formula" target="#formula_9">(7)</ref> for the input embedding i(b j ) (i for short), where o k is a short notation for o(a j,k ). The gradient and update for output embeddings are similar. The α in <ref type="formula" target="#formula_9">(7)</ref> is the learning rate, which we use a linear decay scheme to gradually shrink it from its initial value to zero. Note that the update for the input embedding i is actually performed for the feature embeddings that form i in the concatenation step.</p><formula xml:id="formula_9">∂J ∂i = ∑ k ( f (b j , a j,k ) − δ (a j,k )) · o k + λi (6) i = i − α ∂J ∂i<label>(7)</label></formula><p>Complexity. For each iteration of the training pro- cess, the time complexity is also linear to the input character number, as compared with search, only a few constant time operations of gradient computa- tion and parameter updates are performed for each character.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data and Evaluation Metric</head><p>In the experiments, we use two widely used and freely available 1 manually word-segmented cor- pora, namely, PKU and MSR, from the second SIGHAN international Chinese word segmenta- tion bakeoff <ref type="bibr" target="#b3">(Emerson, 2005)</ref>. <ref type="table" target="#tab_2">Table 2</ref>  Character types 5 × 10 3 5 × 10 3</p><p>Character tokens 1.8 × 10 6 4.1 × 10 6 The segmentation accuracy is evaluated by pre- cision (P ), recall (R), F-score and R oov , the re- call for out-of-vocabulary words. Precision is de- fined as the number of correctly segmented words divided by the total number of words in the seg- mentation result. Recall is defined as the number of correctly segmented words divided by the total number of words in the gold standard segmenta- tion. In particular, R oov reflects the model gen- eralization ability. The metric for overall perfor- mance, the evenly-weighted F-score is calculated as in <ref type="formula" target="#formula_10">(8)</ref>:</p><formula xml:id="formula_10">F = 2 × P × R P + R<label>(8)</label></formula><p>To comply with CWS evaluation conventions and make comparisons fair, we distinguish the follow- ing two settings:</p><p>• closed-set: no extra resource other than train- ing corpora is used.</p><p>• open-set: additional lexicon, raw corpora, etc are used.</p><p>We will report the final results of our model 3 on PKU and MSR corpora in comparison with pre- vious embedding based models (Section 4.2) and state-of-the-art systems (Section 4.3), before go- ing into detailed experiments for model analyses (Section 4.5). <ref type="table" target="#tab_3">Table 3</ref> shows the results of our greedy segmenter on the PKU and MSR datasets, which are com- pared with embedding-based segmenters in previ- ous studies. <ref type="bibr">4</ref> In the table, results for both closed- set and open-set setting are shown for previous models. In the open-set evaluations, all three previous work use pre-training to train character ngram embeddings from large unsegmented cor- pora to initialize the embeddings, which will be later trained with the manually word-segmented training data. For our model, we report the close- set results only, as pre-training does not signifi- cant improve the results in our experiments (Sec- tion 4.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with Previous Embedding-Based Models</head><p>As shown in <ref type="table" target="#tab_3">Table 3</ref>, under close-set evaluation, our model significantly outperform previous em- bedding based models in all metrics. Compared with the previous best embedding-based model, our greedy segmenter has achieved up to 2.2% and 25.8% absolute improvements (MSR) on F-score and R oov , respectively. Surprisingly, our close-set results are also comparable to the best open-set re- sults of previous models. As we will see in (Sec- tion 4.4), when using same or less character uni- and bi-gram features, our model still outperforms previous embedding based models in closed-set evaluation, which shows the effectiveness of our matching model. Significance test. <ref type="table" target="#tab_4">Table 4</ref> shows the 95% con- fidence intervals (CI) for close-set results of our model and the best performing previous model <ref type="bibr" target="#b16">(Pei et al., 2014</ref>), which are computed by formula (9), following <ref type="bibr" target="#b3">(Emerson, 2005)</ref>.</p><formula xml:id="formula_11">CI = 2 √ F (1 − F ) N (9)</formula><p>where F is the F-score value and the N is the word token count of the testing set, which is 104,372 and 106,873 for PKU and MSR, respectively. We see that the confidence intervals of our results do not overlap with that of ( <ref type="bibr" target="#b16">Pei et al., 2014)</ref>, meaning that our improvements are statistically significant. <ref type="table" target="#tab_5">Table 5</ref> shows that the results of our greedy seg- menter are competitive with the state-of-the-art su- pervised systems (Best05 closed-set, <ref type="bibr" target="#b29">Zhang and Clark, 2007)</ref>, although our feature set is much simpler. More recent state-of-the-art systems rely on both extensive feature engineering and ex- tra raw corpora to boost performance, which are semi-supervised learning. For example, Zhang et al (2013) developed 8 types of static and dy- namic features to maximize the co-training system that used extra corpora of Chinese Gigaword and Baike, each of which contains more than 1 bil- lion character tokens. Such systems are not di- rectly comparable with our supervised model. We leave the development of semi-supervised learning methods for our model as future work. <ref type="table" target="#tab_7">Table 6</ref> shows the F-scores of our model on PKU dataset when different features are removed ('w/o') or when only a subset of features are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with the State-of-the-Art Systems</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Features Influence</head><p>Features complement each other and removing any group of features leads to a limited drop of F- score up to 0.7%. Note that features of previ- ous (two) actions are even more informative than all unigram features combined, suggesting that intra-an inter-word dependencies reflected by ac- tion features are strong evidence for segmentation. Moreover, using same or less character ngram fea- tures, our model outperforms previous embedding based models, which shows the effectiveness of our matching model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Model Analysis</head><p>Learning curve. <ref type="figure" target="#fig_2">Figure 2</ref> shows that the training procedure coverages quickly. After the first iter- ation, the testing F-scores are already 93.5% and 95.7% for PKU and MSR, respectively, which then gradually reach their maximum within the next 9 iterations before the curve flats out.</p><p>Speed. With an unoptimized single-thread Python implementation running on a laptop with intel Core-i5 CPU (1.9 GHZ), each iteration of the training procedure on PKU dataset takes about 5 minutes, or 6,000 characters per second. The pre- <ref type="bibr" target="#b36">Zheng et al.(2013)</ref> 92.8 92.0 92.4 63.3 92.9 93.6 93.3 55.7 + pre-training † 93.5 92.2 92.8 69.0 94.2 93.7 93.9 64.1  93.6 92.8 93.2 57.9 92.3 92.2 92.2 53.7 + pre-training † 94.0 93.9 94.0 69.5 93.1 93.1 93.1 59.7 <ref type="bibr" target="#b16">Pei et al. (2014)</ref> 93 <ref type="figure">(closed-set)</ref> 95.5 94.6 95.1 76.0 96.6 96.5 96.6 87.2   diction speed is above 13,000 character per second.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PKU Corpus MSR Corpus</head><formula xml:id="formula_12">P R F R oov P R F R oov</formula><note type="other">.7 93.4 93.5 64.2 94.6 94.2 94.4 61.4 + pre-training † 94.4 93.6 94.0 69.0 95.2 94.6 94.9 64.8 + pre-training &amp; bigram † - - 95.2 - - - 97.2 - This work</note><p>Hyper parameters. The hyper parameters used in the experiments are shown in <ref type="table">Table 7</ref>. We ini- tialized hyper parameters with recommendations in literature before tuning with dev-set experi- ments, each of which change one parameter by a magnitude. We fixed the hyper parameter to the current setting without spending too much time on tuning, since that is not the main purpose of this paper.</p><p>• Embedding size determines the number of parameters to be trained, thus should fit the    training data size to achieve good perfor- mance. We tried the size of 30 and 100, both of which performs worse than 50. A possible tuning is to use different embedding size for different groups of features instead of setting N 1 = 50 for all features.</p><p>• Context window size. A window size of 3-5 characters achieves comparable results. <ref type="bibr" target="#b36">Zheng et al. (2013)</ref> suggested that context window larger than 5 may lead to inferior re- sults.</p><p>• Initial Learning rate. We found that several learning rates between 0.04 to 0.15 yielded very similar results as the one reported here. The training is not very sensitive to reason-able values of initial learning rate. However, Instead of our simple linear decay of learning rate, it might be useful to try more sophisti- cated techniques, such as AdaGrad and expo- nential decaying ( ).</p><p>• Regularization. Our model suffers a little from over-fitting, if no regularization is used.</p><p>In that case, the F-score on PKU drops from 95.1% to 94.7%.</p><p>• Pre-training. We tried pre-training charac- ter embeddings using word2vec 5 with Chi- nese Gigaword Corpus 6 and use them to ini- tialize the corresponding embeddings in our model, as previous work did. However, we were only able to see insignificant F-score improvements within 0.1% and observed that the training F-score reached 99.9% much ear- lier. We hypothesize that pre-training leads to sub-optimal local maximums for our model.</p><p>• Hybrid matching. We tried applying hy- brid matching (Section 3.1) for target char- acters which are less frequent than the top f top characters, including unseen characters, which leads to about 0.15% of F-score im- provements.</p><p>Size of feature embed' N 1 = 50 Size of output embed' N 2 = 550 Window size h = 5</p><p>Initial learning rate α = 0.1</p><formula xml:id="formula_13">Regularization λ = 0.001</formula><p>Hybrid matching f top = 8% <ref type="table">Table 7</ref>: Hyper parameters of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Word segmentation. Most modern segmenters followed Xue (2003) to model CWS as sequence labeling of character position tags, using condi- tional random fields ( <ref type="bibr" target="#b17">Peng et al. 2004</ref>), structured perceptron ( <ref type="bibr" target="#b7">Jiang et al., 2008)</ref>, etc. Some notable exceptions are ( <ref type="bibr" target="#b29">Zhang and Clark, 2007;</ref><ref type="bibr" target="#b32">Zhang et al., 2012)</ref>, which exploited rich word-level fea- tures and ( <ref type="bibr" target="#b11">Ma et al., 2012;</ref><ref type="bibr" target="#b12">Ma, 2014;</ref>), which explicitly model word structures. Our work generalizes the sequence labeling to a more flexible framework of matching, and predicts actions as in ( <ref type="bibr" target="#b29">Zhang and Clark, 2007;</ref><ref type="bibr" target="#b32">Zhang et al., 2012</ref>) instead of position tags to prevent the greedy search from suffering tag inconsistencies. To bet- ter utilize resources other than training data, our model might benefit from techniques used in recent state-of-the-art systems, such as semi-supervised learning ( <ref type="bibr" target="#b35">Zhao and Kit, 2008;</ref><ref type="bibr" target="#b20">Sun and Xu, 2011;</ref><ref type="bibr" target="#b28">Zeng et al., 2013</ref>), joint models ( <ref type="bibr" target="#b9">Li and Zhou, 2012;</ref><ref type="bibr" target="#b18">Qian and Liu, 2012)</ref>, and par- tial annotations ( <ref type="bibr" target="#b27">Yang and Vozila, 2014)</ref>. Distributed representation and CWS. Dis- tributed representation are useful for various NLP tasks, such as POS tagging <ref type="bibr" target="#b1">(Collobert et al., 2011</ref>), machine translation ( <ref type="bibr" target="#b2">Devlin et al., 2014</ref>) and pars- ing ( <ref type="bibr" target="#b19">Socher et al., 2013</ref>). Influenced by <ref type="bibr" target="#b1">Collobert et al. (2011)</ref>, <ref type="bibr" target="#b36">Zheng et al. (2013)</ref> modeled CWS as tagging and treated sentence-level tag sequence as the combination of individual tag predictions and context-independent tag transition.  was inspired by <ref type="bibr" target="#b0">Bengio et al. (2003)</ref> and used character bigram embeddings to compensate for the absence of sentence level optimization. To model interactions between tags and characters, which are absent in these two CWS models, <ref type="bibr" target="#b16">Pei et al. (2014)</ref> introduced the tag embedding and used a tensor hidden layer in the neural net. In con- trast, our work uses character-specific action em- beddings to explicitly capture such interactions. In addition, our work gains efficiency by avoiding hidden layers, similar as <ref type="bibr" target="#b15">Mikolov et al. (2013)</ref>.</p><p>Learning to match. Matching heterogeneous objects has been studied in various contexts before, and is currently flourishing, thanks to embedding- based deep ( <ref type="bibr" target="#b4">Gao et al., 2014</ref>) and convolutional <ref type="bibr" target="#b6">(Huang et al., 2013;</ref><ref type="bibr" target="#b5">Hu et al., 2014</ref>) neural net- works. This work develops a matching model for CWS and differs from others in its "shallow"yet effective architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>Simple architecture. It is possible to adopt stan- dard feed-forward neural network for our embed- ding matching model with character-action em- beddings as both feature and output. Nevertheless, we designed the proposed architecture to avoid hidden layers for simplicity, efficiency and easy- tuning, inspired by word2vec. Our simple archi- tecture is effective, demonstrated by the improved results over previous neural-network word seg-menters, all of which use feed-forward architecture with different features and/or layers. It might be interesting to directly compare the performances of our model with same features on the current and feed-forward architectures, which we leave for fu- ture work.</p><p>Greedy and exact search-based models. As mentioned in Section 3, we implemented and pre- liminarily experimented with a segmenter that trains a similar model with exact search via Viterbi algorithm. On the PKU corpus, its F-score is 0.944, compared with greedy segmenter's 0.951. Its training and testing speed are up to 7.8 times slower than that of the greedy search segmenter. It is counter-intuitive that the performance of the exact-search segmenter is no better or even worse than that of the greedy-search segmenter. We hypothesize that since the training updates pa- rameters with regard to search errors, the final model is "tailored" for the specific search method used, which makes the model-search combination of greedy search segmenter not necessarily worse than that of exact search segmenter. Another way of looking at it is that search is less important when the model is accurate. In this case, most step-wise decisions are correct in the first place, which requires no correction from the search algo- rithm. Empirically, Zhang and Clark (2011) also reported exact-search segmenter performing worse than beam-search segmenters.</p><p>Despite that the greedy segmenter is incapable of considering future labels, this rarely causes problems in practice. Our greedy segmenter has good results, compared with the exact-search seg- menter above and previous approaches, most of which utilize exact search. Moreover, the greedy segmenter has additional advantages of faster training and prediction.</p><p>Sequence labeling and matching. A tradi- tional sequence labeling model such as CRF has K (number of labels) target-character-independent weight vectors, where the target character influ- ences the prediction via the weights of the features that contain it. In a way, a matching model can be seen as a family of "sub-models", which keeps a group of weight vectors (the output embeddings) for each unique target character. Different target characters activate different sub-models, allowing opposite predictions for similar input features, as the target weight vectors used are different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>In this paper, we have introduced the matching formulation for Chinese word segmentation and proposed an embedding matching model to take advantage of distributed representations. Based on the model, we have developed a greedy seg- menter, which outperforms previous embedding- based methods and is competitive with state-of- the-art systems. These results suggest that it is promising to model CWS as configuration-action matching using distributed representations. In ad- dition, linear-time training and testing complexity of our simple architecture is very desirable for in- dustrial application. To the best of our knowledge, this is the first greedy segmenter that is competi- tive with the state-of-the-art discriminative learn- ing models.</p><p>In the future, we plan to investigate methods for our model to better utilize external resources. We would like to try using convolutional neural net- work to automatically encode ngram-like features, in order to further shrink parameter space. It is also interesting to study whether extending our model with deep architectures can benefit CWS. Lastly, it might be useful to adapt our model to tasks such as POS tagging and name entity recognition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :architecture of the embedding matching model for CWS.</head><label>1</label><figDesc>Figure 1: The architecture of the embedding matching model for CWS. The model predicts the segmentation for the character 领 in sentence (4), which is the second character of word 占领 'occupy'. Both feature and output embeddings are trainable parameters of the model.</figDesc><graphic url="image-1.png" coords="3,108.96,62.81,379.62,259.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Feature</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The learning curve of our model.</figDesc><graphic url="image-2.png" coords="7,307.28,376.12,225.99,136.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Uni-and bi-gram feature template 

the following function: 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Corpus details of PKU and MSR</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Comparison with previous embedding based models. Numbers in percentage. Results with  † 
used extra corpora for (pre-)training. 

Models 

PKU 
MSR 
F 
CI 
F 
CI 
Pei et al. 
93.5 ±0.15 94.4 ±0.14 
This work 95.1 ±0.13 96.6 ±0.11 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Significance test of closed-set results of 
Pei et al (2014) and our model. 

Model 
PKU MSR 
Best05 closed-set 
95.0 
96.4 
Zhang et al. (2006) 
95.1 
97.1 
Zhang and Clark (2007) 94.5 
97.2 
Wang et al. (2012) 
94.1 
97.2 
Sun et al. (2009) 
95.2 
97.3 
Sun et al. (2012) 
95.4 
97.4 
Zhang et al. (2013)  † 
96.1 
97.4 
This work 
95.1 
96.6 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Comparison with the state-of-the-art sys-
tems. Results with  † used extra lexicon/raw cor-
pora for training, i.e. in open-set setting. Best05 
refers to the best closed-set results in 2nd SIGHAN 
bakeoff. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>The influence of features. F-score in per-
centage on the PKU corpus. 

</table></figure>

			<note place="foot" n="1"> http://www.sighan.org/bakeoff2005/ 2 http://www.sighan.org/bakeoff2003/score</note>

			<note place="foot" n="3"> Our implementation: https://zenodo.org/record/17645. 4 The results for Zheng et al. (2013) are from the reimplementation of Pei et al. (2014).</note>

			<note place="foot" n="5"> https://code.google.com/p/word2vec/ 6 https://catalog.ldc.upenn.edu/LDC2005T14</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank the anonymous reviewers for their very helpful and constructive suggestions. We are indebted to Çağrı Çöltekin for discussion and comments, to Dale Gerdemann, Cyrus Shaoul, Corina Dima, Sowmya Vajjala and Helmut Schmid for their useful feedback on an ear-lier version of the manuscript. Financial support for the research reported in this paper was provided by the German Research Foundation (DFG) as part of the Collaborative Research Center "Emergence of Meaning" (SFB 833) and by the German Min-istry of Education and Technology (BMBF) as part of the research grant CLARIN-D.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast and robust neural network joint models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rabih</forename><surname>Zbib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1370" to="1380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The second international chinese word segmentation bakeoff</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Emerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing</title>
		<meeting>the Fourth SIGHAN Workshop on Chinese Language Processing</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">133</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Modeling interestingness with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional neural network architectures for matching natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2042" to="2050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2333" to="2338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A cascaded linear model for joint Chinese word segmentation and part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajuan</forename><surname>Lü</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="897" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Conditional random fields: probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unified dependency parsing of Chinese morphological and syntactic structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1445" to="1454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Domain adaptation for CRF-based Chinese word segmentation using free annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="864" to="874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semi-automatic annotation of Chinese word structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Kit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dale</forename><surname>Gerdemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second CIPSSIGHAN Joint Conference on Chinese Language Processing</title>
		<meeting>the Second CIPSSIGHAN Joint Conference on Chinese Language Processing</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="9" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic refinement of syntactic categories in Chinese word structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Feature-based neural language model and Chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mairgup</forename><surname>Mansur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCNLP</title>
		<meeting>IJCNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1271" to="1277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Cernockỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Maxmargin tensor neural network for Chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Baobao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="239" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Chinese segmentation and new word detection using conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangfang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="562" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Joint Chinese word segmentation, POS tagging and parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-CoNLL</title>
		<meeting>EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="501" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Parsing with compositional vector grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Enhancing Chinese word segmentation using unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="970" to="979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A discriminative latent variable Chinese segmenter with hybrid word/character information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaozhong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Matsuzaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="56" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fast online training with frequency-adaptive learning rates for Chinese word segmentation and new word detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="253" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Probabilistic Chinese word segmentation with non-local information and stochastic training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaozhong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Matsuzaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="626" to="636" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Stochastic gradient descent training for L1-regularized log-linear models with cumulative penalty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Jun&amp;apos;ichi Tsujii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ananiadou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-IJCNLP</title>
		<meeting>ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="477" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Integrating generative and discriminative characterbased models for Chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keh-Yih</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Asian Language Information Processing (TALIP)</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Chinese word segmentation as character tagging. Computational Linguistics and Chinese Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="29" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semi-supervised Chinese word segmentation using partial-label learning With conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Vozila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="90" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Graph-based semi-supervised model for joint Chinese word segmentation and partof-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidia</forename><forename type="middle">S</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="770" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Chinese segmentation with a word-based perceptron algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="840" to="847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Syntactic processing using the generalized perceptron and beam search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="105" to="151" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Subword-based tagging by conditional random fields for Chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Genichiro</forename><surname>Kikui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="193" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Word segmentation on Chinese mirco-blog data with a linear-time incremental model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaixu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changle</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd CIPS-SIGHAN Joint Conference on Chinese Language Processing</title>
		<meeting>the 2nd CIPS-SIGHAN Joint Conference on Chinese Language Processing</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="41" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Exploring representations from unlabeled data with co-training for Chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maigup</forename><surname>Mansur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="311" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Character-level Chinese dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1326" to="1336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised segmentation helps supervised learning of character tagging for word segmentation and named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Kit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCNLP</title>
		<meeting>IJCNLP</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="106" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep Learning for Chinese Word Segmentation and POS Tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqing</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="647" to="657" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
