<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:31+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Language to Code: Learning Semantic Parsers for If-This-Then-That Recipes</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
							<email>chrisq@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Redmond</orgName>
								<address>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
							<email>mooney@cs.utexas.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">UT Austin Austin TX</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
							<email>mgalley@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Redmond</orgName>
								<address>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Language to Code: Learning Semantic Parsers for If-This-Then-That Recipes</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="878" to="888"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Using natural language to write programs is a touchstone problem for computational linguistics. We present an approach that learns to map natural-language descriptions of simple &quot;if-then&quot; rules to executable code. By training and testing on a large corpus of naturally-occurring programs (called &quot;recipes&quot;) and their natural language descriptions , we demonstrate the ability to effectively map language to code. We compare a number of semantic parsing approaches on the highly noisy training data collected from ordinary users, and find that loosely synchronous systems perform best.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ability to program computers using natural lan- guage would clearly allow novice users to more effectively utilize modern information technology. Work in semantic parsing has explored mapping natural language to some formal domain-specific programming languages such as database queries <ref type="bibr" target="#b20">(Woods, 1977;</ref><ref type="bibr" target="#b22">Zelle and Mooney, 1996;</ref><ref type="bibr" target="#b1">Berant et al., 2013</ref>), commands to robots <ref type="bibr" target="#b9">(Kate et al., 2005</ref>), operating systems ( <ref type="bibr" target="#b3">Branavan et al., 2009)</ref>, smart- phones ( <ref type="bibr" target="#b13">Le et al., 2013)</ref>, and spreadsheets <ref type="bibr" target="#b7">(Gulwani and Marron, 2014</ref>). Developing such language- to-code translators has generally required specific dedicated efforts to manually construct parsers or large corpora of suitable training examples. An interesting subset of the possible program space is if-then "recipes," simple rules that allow users to control many aspects of their digital life including smart devices. Automatically parsing * Work performed while visiting Microsoft Research. these recipes represents a step toward complex nat- ural language programming, moving beyond single commands toward compositional statements with control flow.</p><p>Several services, such as Tasker and IFTTT, al- low users to create simple programs with "triggers" and "actions." For example, one can program their Phillips Hue light bulbs to flash red and blue when the Cubs hit a home run. A somewhat complicated GUI allows users to construct these recipes based on a set of information "channels." These chan- nels represent many types of information. Weather, news, and financial services have provided constant updates through web services. Home automation sensors and controllers such as motion detectors, thermostats, location sensors, garage door openers, etc. are also available. Users can then describe the recipes they have constructed in natural language and publish them.</p><p>Our goal is to build semantic parsers that al- low users to describe recipes in natural language and have them automatically mapped to exe- cutable code. We have collected 114,408 recipe- description pairs from the http://ifttt.com website. Because users often provided short or incomplete English descriptions, the resulting data is extremely noisy for the task of training a semantic parser. Therefore, we have constructed semantic-parser learners that utilize and adapt ideas from several previous approaches <ref type="bibr" target="#b8">(Kate and Mooney, 2006</ref>; <ref type="bibr" target="#b17">Wong and Mooney, 2006</ref>) to learn an effective in- terpreter from such noisy training data. We present results on our collected IFTTT corpus demonstrat- ing that our best approach produces more accurate programs than several competing baselines. By exploiting such "found data" on the web, seman- tic parsers for natural-language programming can potentially be developed with minimal effort. <ref type="bibr">878</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>We take an approach to semantic parsing that directly exploits the formal grammar of the tar- get meaning representation language, in our case IFTTT recipes. Given supervised training data in the form of natural-language sentences each paired with their corresponding IFTTT recipe, we learn to introduce productions from the formal-language grammar into the derivation of the target program based on expressions in the natural-language input. This approach originated with the SILT system ( <ref type="bibr" target="#b9">Kate et al., 2005</ref>) and was further developed in the WASP ( <ref type="bibr" target="#b17">Wong and Mooney, 2006;</ref><ref type="bibr" target="#b19">Wong and Mooney, 2007b</ref>) and KRISP ( <ref type="bibr" target="#b8">Kate and Mooney, 2006</ref>) systems.</p><p>WASP casts semantic parsing as a syntax-based statistical machine translation (SMT) task, where a synchronous context-free grammar (SCFG) <ref type="bibr" target="#b21">(Wu, 1997;</ref><ref type="bibr" target="#b4">Chiang, 2005;</ref><ref type="bibr" target="#b6">Galley et al., 2006</ref>) is used to model the translation of natural language into a formal meaning representation. It uses statistical models developed for syntax-based SMT for lexical learning and parse disambiguation. Productions in the formal-language grammar are used to construct synchronous rules that simultaneously model the generation of the natural language. WASP was sub- sequently "inverted" to use the same synchronous grammar to generate natural language from the for- mal language <ref type="bibr" target="#b18">(Wong and Mooney, 2007a)</ref>.</p><p>KRISP uses classifiers trained using a Support- Vector Machine (SVM) to introduce productions in the derivation of the formal translation. The productions of the formal-language grammar are treated like semantic concepts to be recognized from natural-language expressions. For each pro- duction, an SVM classifier is trained using a string subsequence kernel ( <ref type="bibr" target="#b15">Lodhi et al., 2002</ref>). Each clas- sifier can then estimate the probability that a given natural-language substring introduces a production into the derivation of the target representation. Dur- ing semantic parsing, these classifiers are employed to estimate probabilities on different substrings of the sentence to compositionally build the most probable meaning representation for the sentence. Unlike WASP whose synchronous grammar needs to be able to directly parse the input, KRISP's ap- proach to "soft matching" productions allows it to produce a parse for any input sentence. Conse- quently, KRISP was shown to be much more robust to noisy training data than previous approaches to semantic parsing ( <ref type="bibr" target="#b8">Kate and Mooney, 2006</ref>).</p><p>Since our "found data" for IFTTT is extremely noisy, we have taken an approach similar to KRISP; however, we use a probabilistic log-linear text clas- sifier rather than an SVM to recognize productions.</p><p>This method of assembling well-formed pro- grams guided by a natural language query bears some resemblance to Keyword Programming <ref type="bibr" target="#b14">(Little and Miller, 2007)</ref>. In that approach, users en- ter natural language queries in the middle of an existing program; this query drives a search for programs that are relevant to the query and fit within the surrounding program. However, the function used to score derivations is a simple match- ing heuristic relying on the overlap between query terms and program identifiers. Our approach uses machine learning to build a correspondence be- tween queries and recipes based on parallel data.</p><p>There is also a large body of work applying Com- binatory Categorical Grammars to semantic pars- ing, starting with Zettlemoyer and <ref type="bibr" target="#b23">Collins (2005)</ref>. Depending on the set of combinators used, this ap- proach can capture more expressive languages than synchronous context-free MT. In practice, however, synchronous MT systems have competitive accu- racy scores ( <ref type="bibr" target="#b0">Andreas et al., 2013)</ref>. Therefore, <ref type="bibr">we</ref> have not yet evaluated CCG on this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">If-this-then-that recipes</head><p>The recipes considered in this paper are diverse and powerful despite being simple in structure. Each recipe always contains exactly one trigger and one action. Whenever the conditions of the trigger are satisfied, the action is performed. The resulting recipes can perform tasks such as home automation ("turn on my lights when I arrive home"), home security ("text me if the door opens"), organization ("add receipt emails to a spreadsheet"), and much more ("remind me to drink water if I've been at a bar for more than two hours"). Triggers and actions are drawn from a wide range of channels that must be activated by each user. These channels can represent many entities and services, including devices (such as Android devices or WeMo light switches) and knowledge sources (such as ESPN or Gmail). Each channel exposes a set of functions for both trigger and action.</p><p>Several services such as IFTTT, Tasker, and Llama allow users to author if-this-then-that recipes. IFTTT is unique in that it hosts a large set of recipes along with descriptions and other metadata. Users of this site construct recipes using a GUI interface to select the trigger, action, and the parameters for both trigger and action. After the recipe is authored, the user must provide a descrip- tion and optional set of notes for this recipe and publish the recipe. Other users can browse and use these published recipes; if a user particularly likes a recipe, they can mark it as a favorite.</p><p>As of January 2015, we found 114,408 recipes on http://ifttt.com. Among the available recipes we encountered a total of 160 channels. In total, we found 552 trigger functions from 128 of those chan- nels, and 229 action functions from 99 channels, for a total of 781 functions. Each recipe includes a number of pieces of information: description 1 , note, author, number of uses, etc. 99.98% of the entries have a description, and 35% contain a note. Based on availability, we focused primarily on the description, though there are cases where the note is a more explicit representation of program intent.</p><p>The recipes at http://ifttt.com are represented as HTML forms, with combo boxes, inline maps, and other HTML UI components allowing end users to select functions and their parameters. This is convenient for end users, but difficult for automated approaches. We constructed a formal grammar of possible program structures, and from each HTML form we extracted an abstract syntax tree (AST) conforming to this grammar. We model this as a context-free grammar, though this assumption is violated in some cases. Consider the program in <ref type="figure" target="#fig_0">Figure 1</ref>, where some of the parameters used the action are provided by the trigger. This data could be used in a variety of ways. Recipes could be suggested to users based on their activities or interests, for instance, or one could train a natural language generation system to give a readable description of code.</p><p>In this paper, the paired natural language descrip- tions and abstract syntax trees serve as training data for semantic parsing. Given a description, a system must produce the AST for an IFTTT recipe. We note in passing that the data was constructed in the opposite direction: users first implemented the recipe and then provided a description afterwards. Ideal data for our application would instead start with the description and construct the recipe based on this description. Yet the data is unusually large and diverse, making it interesting training data for mapping natural language to code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Program synthesis methods</head><p>We consider a number of methods to map the natu- ral language description of a problem into its for- mal program representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Program retrieval</head><p>One natural baseline is retrieval. Multiple users could potentially have similar needs and therefore author similar or even identical programs. Given a novel description, we can search for the closest description in a table of program-description pairs, and return the associated program. We explored several text-similarity metrics, and found that string edit distance over the unmodified character se- quence achieved best performance on the devel- opment set. As the corpus of program-description pairs becomes larger, this baseline should increase in quality and coverage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Machine Translation</head><p>The downside to retrieval is that it cannot general- ize. Phrase-based SMT systems( <ref type="bibr" target="#b16">Och et al., 1999;</ref><ref type="bibr" target="#b10">Koehn et al., 2003)</ref> can be seen as an incremental step beyond retrieval: they segment the training data and attempt to match and assemble those seg- ments at runtime. If the phrase length is unbounded, retrieval is almost a special case: it could return whole programs from the training data when the description matches exactly. In addition, they can find subprograms that are relevant to portions of the input, and assemble those subprograms into whole programs.</p><p>As a baseline, we adopt a recent approach <ref type="bibr" target="#b0">(Andreas et al., 2013</ref>) that casts semantic parsing as phrasal translation. First, the ASTs are converted into flat sequences of code tokens using a pre-order left-to-right traversal. The tokens are annotated with their arity, which is sufficient to reconstruct the tree given a well formed sequence of tokens using a simple stack algorithm. Given this paral- lel corpus of language and code tokens, we train a conventional statistical machine translation sys- tem that is similar in structure and performance to Moses ( <ref type="bibr" target="#b11">Koehn et al., 2007)</ref>. We gather the k-best translations, retaining the first such output that can be successfully converted into a well-formed pro- gram according to the formal grammar. Integration of the well-formedness constraint into decoding would likely produce better translations, but would require more modifications to the MT system.</p><p>Approaches to semantic parsing inspired by ma- chine translation have proven effective when the 880 data is very parallel. In the IFTTT dataset, however, the available pairs are not particularly clean. Word alignment quality suffers, and production extrac- tion suffers in turn. Descriptions in this corpus are often quite telegraphic (e.g., "Instagram to Face- book") or express unnecessary pieces of informa- tion, or are downright unintelligible (" 2Mrl14"). Approaches that rely heavily on lexicalized infor- mation and assume a one-to-one correspondence between source and target (at the phrase, if not the word level) struggle in this setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Generation without alignment</head><p>An alternate approach is to treat the source lan- guage as context and a general direction, rather than a hard constraint. The target derivation can be pro- duced primarily according to the formal grammar while guided by features from the source language.</p><p>For each production in the formal grammar, we can train a binary classifier intended to predict whether that production should be present in the derivation. This classifier uses general features of the source sentence. Note how this allows produc- tions to be inferred based on context: although a description might never explicitly say that a pro- duction is necessary, the surrounding context might strongly imply it.</p><p>We assign probabilities to derivations by looking at each production independently. A derivation ei- ther uses or does not use each production. For each production used in the derivation, we multiply by the probability of its inclusion. Likewise for each production not used in the derivation, we multiply by one minus the probability of its inclusion.</p><p>Let G = (V, Σ, R, S) be the formal grammar with non-terminals V , terminal vocabulary Σ, pro- ductions R and start symbol S. E represents a source sentence, and D, a formal derivation tree for that sentence. R(D) is the set of productions in that derivation. The score of a derivation is the following product:</p><formula xml:id="formula_0">P (D|E) = r∈R(D) P (r|E) r∈R\R(D) P (¬r|E)</formula><p>The binary classifiers are log-linear models over features, F , of the input string: P (r|E) ∝ exp θ r F (E) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Training</head><p>For each production, we train a binary classifier predicting its presence or absence. Given a train- ing set of parallel descriptions and programs, we create |R| binary classifier training sets, one for each classifier. We currently use a small set of simple features: word unigrams and bigrams, and character trigrams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Inference</head><p>When presented with a novel utterance, E, our sys- tem must find the best code corresponding to that utterance. We use a top-down, left-to-right gener- ation strategy, where each search node contains a stack of symbols yet to be expanded and a log prob- ability. The initial node is [S] , 0; and a node is complete when its stack of non-terminals is empty. Given a search node with a non-terminal as its first symbol on the stack, we expand with any pro- duction for that symbol, putting its yield onto the stack and updating the node cost to include its derivation score:</p><formula xml:id="formula_1">[X, α] , p (X → β) ∈ R [β, α] , p + log P (X → β|E)</formula><p>If the first stack item is a terminal, it is scanned:</p><formula xml:id="formula_2">[a, α] , p a ∈ Σ [α] , p</formula><p>Using these inference rules, we utilize a simple greedy approach that only accounts for the produc- tions included in the derivation. To account for the negative r∈R\R(D) P (¬r|E) factors, we use a beam search, and rerank the n-best final outcomes from this search based on the probability of all pro- ductions that are not included. Partial derivations are grouped into beams according to the number of productions in that derivation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Loosely synchronous generation</head><p>The above method learns distributions over pro- ductions given the input, but treats the sentence as an undifferentiated bag of linguistic features. The syntax of the source sentence is not leveraged at all, nor is any correspondence between the language syntax and the program structure used. Often the pairs are not in sufficient correspondence to sug- gest synchronous approaches, but some loose corre- spondence to maintain at least a notion of coverage could be helpful.</p><p>We pursue an approach similar to KRISP <ref type="bibr" target="#b8">(Kate and Mooney, 2006</ref>), with several differences. First, rather than a string kernel SVM, we use a log-linear model with character and word n-gram features. <ref type="bibr">2</ref> Second, we allow the model to consider both span- internal features and contextual features.</p><p>This approach explicitly models the correspon- dence between nodes in the code side and tokens in the language. Unlike standard MT systems, word alignment is not used as a hard constraint. Instead, this phrasal correspondence is induced as part of model training.</p><p>We define a semantic derivation D of a natu- ral language sentence E as a program AST where each production in the AST is augmented with a span. The substrings covered by the children of a production must not overlap, and the substring covered by the parent must be the concatenation of the substrings covered by the children. <ref type="figure" target="#fig_1">Figure 2</ref> shows a sample semantic derivation.</p><p>IF <ref type="bibr">[1]</ref><ref type="bibr">[2]</ref><ref type="bibr">[3]</ref><ref type="bibr">[4]</ref><ref type="bibr">[5]</ref><ref type="bibr">[6]</ref> ACTION <ref type="bibr">[1]</ref><ref type="bibr">[2]</ref> Phone call <ref type="bibr">[1]</ref><ref type="bibr">[2]</ref> Call my phone <ref type="bibr">[1]</ref><ref type="bibr">[2]</ref> TRIGGER <ref type="bibr">[3]</ref><ref type="bibr">[4]</ref><ref type="bibr">[5]</ref><ref type="bibr">[6]</ref> ESPN <ref type="bibr">[3]</ref><ref type="bibr">[4]</ref><ref type="bibr">[5]</ref><ref type="bibr">[6]</ref> New in-game update <ref type="bibr">[3]</ref><ref type="bibr">[4]</ref><ref type="bibr">[5]</ref><ref type="bibr">[6]</ref> Chicago Cubs  1 2 3 4 5 6 Call me if the Cubs score The core components of KRISP are string-kernel classifiers P (r, i..j|E) denoting the probability that a production r in the AST covers the span of words i..j in the sentence E. Here, i &lt; j are positions in the sentence indicating the span of tokens most relevant to this production. In other words, the substring E[i..j] denotes the production r with probability P (r, i..j|E). The probability of a semantic derivation D is defined as follows:</p><formula xml:id="formula_3">P (D|E) = (r,i..j)∈D P (r, i..j|E)</formula><p>That is, we assume that each production is indepen- dent of all others, and is conditioned only on the string to which it is aligned. This can be seen as a refinement of the above production classification approach using a notion of correspondence.</p><p>Rather than using string kernels, we use logis- tic regression classifiers with word unigram, word bigram, and character trigram features. Unlike KRISP, we include features from both inside and outside the substring. Consider the production "Phone call → Call my phone" with span 1-2 from <ref type="figure" target="#fig_1">Figure 2</ref>. Word unigram features indicate that "call" and "me" are inside the span; the remaining words are outside the span. Word bigram features indicate that "call me" is inside the span, "me if" is on the boundary of the span, and all remaining bigrams are outside the span.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Training</head><p>These classifiers are trained in an iterative EM- like manner <ref type="bibr" target="#b8">(Kate and Mooney, 2006</ref>). Starting with some initial classifiers and a training set of NL and AST pairs, we search for the most likely derivation. If the AST underlying this derivation matches the gold AST, then this derivation is added to the set of positive instances. Otherwise, it is added to the set of negative instances, and the best derivation constrained to match the gold standard AST is found and added to the positive instances. Given this revised training data, the classifiers are retrained. After each pass through the training data, we evaluate the current model on the development set. This procedure is repeated until development- set performance begins to fall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Inference</head><p>To find the most probable derivation according to the grammar, KRISP uses a variation on Earley parsing. This is similar to the inference method from Section 4.3.2, but each item now additionally maintains a position and a span. Inference proceeds left-to-right through the source string. The natural language may present information in a different order than the formal language, so all permutations of rules are considered during inference.</p><p>We found this inference procedure to be quite slow for larger data sets, especially because wide beams were needed to prevent search failure. To speed up inference, we used scores from the position-independent classifiers as completion-cost estimates.</p><p>The completion-cost estimate for a given sym- bol is defined recursively. Terminals have a cost of zero. Productions have a completion cost of the log probability of the production given the sentence, plus the completion cost of all non-terminal sym- bols. The completion cost for a non-terminal is the max cost of any production rooted in that non- terminal. Computing this cost requires traversing all productions in the grammar for each sentence.</p><p>Given a partial hypothesis, we use exact scores for the left-corner subtree that has been fully con- structed, and completion estimates for all the sym- bols and productions whose left and right spans are not yet fully instantiated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Evaluation</head><p>Next we evaluate the accuracy of these approaches. The 114,408 recipes described in Section 3 were first cleaned and tokenized. We kept only one recipe per unique description, after mapping to low- ercase and normalizing punctuation. <ref type="bibr">3</ref> Finally the recipes were split by author, randomly assigning each to training, development, or test, to prevent overfitting to the linguistic style of a particular au- thor. <ref type="table">Table 1</ref> presents summary statistics for the resulting data.</p><p>Although certain trigger-action pairs occur much more often than others, the recipes in this data are quite diverse. The top 10 trigger-action pairs account for 14% of the recipes; the top 100 account for 37%; the top 1000 account for 72%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Metrics</head><p>To evaluate system performance, several different measures are employed. Ideally a system would output exactly the correct abstract syntax tree. One measure is to count the number of exact matches, though almost all methods receive a score of 0. <ref type="bibr">4</ref> Alternatively, we can look at the AST as a set of productions, computing balanced F-measure. This is a much more forgiving measure, giving partial credit for partially correct results, though it has the caveat that all errors are counted equally.</p><p>Correctly assigning the trigger and action is the most important, especially because some of the pa- rameter values are tailored for particular users. For example, "turn off my lights when I leave home" requires a "home" location, which varies for each user. Therefore, we also measure accuracy at iden- tifying the correct trigger and action, both at the channel and function level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Human comparison</head><p>One remaining difficulty is that multiple programs may be equally correct. Some descriptions are very difficult to interpret, even for humans. Second, multiple channels may provide similar functional- ity: both Phillips Hue and WeMo channels provide the ability to turn on lights. Even a well-authored description may not clarify which channel should be used. Finally, many descriptions are underspec- ified. For instance, the description "notify me if it rains" does not specify whether the user should receive an Android notification, an iOS notification, an email, or an SMS. This is difficult to capture with an automatic metric.</p><p>To address the prevalence and impact of under- specification and ambiguity in descriptions, we asked humans to perform a very similar task. Human annotators on Amazon Mechanical Turk ("turkers") were presented with recipe descriptions and asked to identify the correct channel and func- tion (but not parameters). Turkers received careful instructions and several sample description-recipe pairs, then were asked to specify the best recipe for each input. We requested they try their best to find an action and a trigger even when presented with vague or ambiguous descriptions, but they could tag inputs as 'unintelligible' if they were unable to make an educated guess. Turkers created recipes only for English descriptions, applying the label 'non-English' otherwise. Five recipes were gath- ered for each description. The resulting recipes are not exactly gold, as they have limited training at the task. However, we imposed stringent qualification requirements to control the annotation quality. <ref type="bibr">5</ref> Our workers were in fair agreement with one an- other and the gold standard, producing high quality annotation at wages calibrated to local minimum wage. We measure turker agreement with <ref type="bibr">Krippendorff's α (Krippendorff, 1980)</ref>, which is a statis- tical measure of agreement between any number of coders. Unlike Cohen's κ <ref type="bibr" target="#b5">(Cohen, 1960)</ref>, the α statistic does not require that coders be the same for each unit of analysis. This property is particularly desirable in our case, since turkers generally differ across HITs. A value of α = 1 indicates perfect agreement, while α ≤ 0 suggests the absence of agreement or systematic disagreement. Agreement measures on the Mechanical Turk data are shown in <ref type="table">Table 2</ref>. This shows encouraging levels of agree- ment for both the trigger and the action, especially considering the large number of categories. <ref type="bibr" target="#b12">Krippendorff (1980)</ref> advocates a 0.67 cutoff to allow <ref type="bibr">5</ref> Turkers must have 95% HIT approval rating and be native speakers of English (As an approximation of the latter, we required Turkers be from the U.S.). Manual inspection of an- notation on a control set drawn from the training data ensured there was no apparent spam.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Trigger</head><p>Action C C+F C C+F</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># of categories 128 552 99 229</head><p>all .592 .492 .596 .532 Intelligible English .687 .528 .731 .627 <ref type="table">Table 2</ref>: Annotator agreement as measured by Krippendorff's α coefficient <ref type="bibr" target="#b12">(Krippendorff, 1980)</ref>. Agreement is measured on either channel (C) or channel and function (C+F), and on either the full test set (4294 recipes) or its English and intelligible subset (2262 recipes). "tentative conclusion" of agreement, and turkers are relatively close to that level for both trigger and action channels. However, it is important to note that the coding scheme used by turkers is not mutu- ally exclusive, as several triggers and actions (e.g., "SMS" vs. "Android SMS" actions) accomplish similar effects. Thus, our levels of agreement are likely to be greater than suggested by measures in the table. Finally, we also measured agreement on the English and intelligible subset of the data, as we found that confusion between the two labels "non- English" and "unintelligible" was relatively high. As shown in the table, this substantially increased levels of agreement, up to the point where α for both trigger and action channels are above the 0.67 cutoff drawing tentative conclusion of agreement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Systems and baselines</head><p>The retrieval method searches for the closest de- scription in the training data based on character string-edit-distance and returns the recipe for that training program. The phrasal method uses phrase- based machine translation to generate candidate outputs, searching the resulting n-best candidates for the first well-formed recipe. After exploring multiple word alignment approaches, we found that an unsupervised feature-rich method <ref type="bibr">(BergKirkpatrick et al., 2010</ref>) worked best, leverag- ing features of string similarity between the de- scription and the code. We ran MERT on the de- velopment data to tune parameters. We used a phrasal decoder with performance similar to Moses. The synchronous grammar method, a recreation of WASP, uses the same word alignment as above, but extracts a synchronous grammar rules from the parallel data ( <ref type="bibr" target="#b17">Wong and Mooney, 2006</ref>). The classifier approach described in Section 4.3 is in- dependent of word alignment. Finally, the posclass approach from Section 4.4 derives its own deriva-tion structure from the data.</p><p>The human annotations are used to establish the mturk human-performance baseline by taking the majority selection of the trigger and action over 5 HITs for each description and comparing the result to the gold standard. The oracleturk human- performance baseline shows how often at least one of the turkers agreed with the gold standard.</p><p>In addition, we evaluated all systems on a sub- set of the test data where at least three human- generated recipes agreed with the gold standard. This subset represents those programs that are easily reproducible by human workers. A good method should strive to achieve 100% accuracy on this set, and we should perhaps not be overly concerned about the remaining examples where humans disagree about the correct interpretation. <ref type="table" target="#tab_2">Table 3</ref> summarizes the main evaluation results. Most of the measures are in concordance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results and discussion</head><p>Interestingly, retrieval outperforms the phrasal MT baseline. With a sufficiently long phrase limit, phrasal MT approaches retrieval, but with a few crucial differences. First, phrasal requires an exact match of some substring of the input to some sub- string of the training data, where retrieval can skip over words. Second, the phrases are heavily depen- dent on word alignment; we find the word align- ment techniques struggle with the noisy IFTTT descriptions. Sync performs similarly to phrasal. The underspecified descriptions challenge assump- tions in synchronous grammars: much of the target structure is implied rather than stated.</p><p>In contrast, the classification method performs quite well. Some productions may be very likely given a prior alone, or may be inferred given other productions and the need for a well-formed deriva- tion. Augmenting this information with positional information as in posclass can help with the attri- bution problem. Consider the input "Download Facebook Photos you're tagged in to Dropbox": we would like the token "Facebook" to invoke only the trigger, not the action. We believe further gains could come from better modeling of the correspon- dence between derivation and natural language.</p><p>We find that semantic parsing systems have ac- curacy nearly as high or even higher than turkers in certain conditions. There are several reasons for this. First, many of the channels overlap in func- tionality (Gmail vs. email, or Android SMS vs. SMS); likewise functions may be very closely re- Channel +Func Prod F1    <ref type="table">Table 4</ref>: Example output from the posclass system. For each input instance, we show the original query, the recipe originally authored through IFTTT, and our system output. Instance (a) demonstrates a case where the correct program is produced even though the input is rather tricky. Even the Portuguese query of (b) is correctly predicted, though keywords help here. In instance (c), the query is underspecified, and the system predicts that archiving should be done in Google Drive rather than evernote. Instance (d) shows how we sometimes confuse the trigger and action. Certain queries, such as (e), would require very deep inference: the IFTTT recipe sets up an endless email loop, where our system assembles a strange interpretation based on keyword match.</p><p>instance, more effectively break such ties by learn- ing a prior over which channels are more likely. Turkers, on the other hand, have neither specific training at this job nor a background corpus and more frequently disagree with the gold standard. Second, there are a number of non-English and unintelligible descriptions. Although the turkers were asked to skip these sentences, the machine- learning systems may still correctly predict the channel and action, since the training set also con- tains non-English and cryptic descriptions. For the cases where humans agree with each other and with the gold standard, the best automated system (posclass) does fairly well, getting 81% channel and 71% function accuracy. <ref type="table">Table 4</ref> has some sample outputs from the posclass system, showing both examples where the system is effective and where it struggles to find the intended interpretation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>The primary goal of this paper is to highlight a new application and dataset for semantic pars- ing. Although if-this-then-that recipes have a lim- ited structure, many potential recipes are possible. This is a small step toward broad program synthe- sis from natural language, but is driven by real user data for modern hi-tech applications. To en- courage further exploration, we are releasing the URLs of recipes along with turker annotations at http://research.microsoft.com/lang2code/.</p><p>The best performing results came from a loosely synchronous approach. We believe this is a very promising direction: most work inspired by pars- ing or machine translation has assumed a strong connection between the description and the opera- ble semantic representation. In practical situations, however, many elements of the semantic representa- tion may only be implied by the description, rather than explicitly stated. As we tackle domains with greater complexity, identifying implied but neces- sary information will be even more important.</p><p>Underspecified descriptions open up new inter- face possibilities as well. This paper considered only single-turn interactions, where the user de- scribes a request and the system responds with an interpretation. An important next step would be to engage the user in an interactive dialogue to confirm and refine the user's intent and develop a fully-functional correct program.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example recipe with description, with nodes corresponding to (a) Channels, (b) Functions, and (c) Parameters indicated with specific boxes. Note how some of the fields in braces, such as OccurredAt, depend on the trigger.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An example training pair with its semantic derivation. Note the correspondence between formal language and natural language denoted with indices and spans.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>(</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :Tomorrow's forecast calls for =⇒ SMS : Send me an SMS OUTPUT Weather : Tomorrow's forecast calls for =⇒ SMS : Send me an SMS INPUT Suas fotos do instagr.am salvas no dropbox (b) IFTTT Instagram : Any new photo by you =⇒ Dropbox : Add file from URL OUTPUT Instagram : Any new photo by you =⇒ Dropbox : Add file from URL</head><label>3</label><figDesc></figDesc><table>Evaluation results. The first column measures how 
often the channels are selected correctly for both trigger and 
action (e.g. Android Phone Call and Google Drive in Fig-
ure 1). The next column measures how often both the channel 
and function are correctly selected for both trigger and ac-
tion (e.g. Android Phone Call::Any phone call missed and 
Google Drive::Add row to spreadsheet). The last column 
shows balanced F-measure against the gold tree over all pro-
ductions in the proposed derivation, from the root production 
down to the lowest parameter. We show results on (a) the 
full test data; (b) omitting descriptions marked as non-English 
by a majority of the crowdsourced workers; (c) omitting de-
scriptions marked as either non-English or unintelligible by 
the crowd; and (d) only recipes where at least three of five 
workers agreed with the gold standard. 

lated (Post a tweet vs. Post a tweet with an image). 
All the systems with access to thousands of train-
ing pairs are at a strong advantage; they can, for 

885 

</table></figure>

			<note place="foot" n="1"> The IFTTT site refers to this as &quot;title&quot;.</note>

			<note place="foot" n="2"> We have a preference for log-linear models given their robustness to hyperparameter settings, ease of optimization, and flexible incorporation of features. An SVM trained with similar features should have similar performance, though.</note>

			<note place="foot" n="3"> We found many recipes with the same description, likely copies of some initial recipe made by different users. We selected one representative using a deterministic heuristic.</note>

			<note place="foot" n="4"> Retrieval gets an exact match 3.7% of the time, likely due to near-duplicates from copied recipes.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank William Dolan and the anonymous reviewers for their helpful advice and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semantic parsing as machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-08" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="47" to="52" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic parsing on Freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Painless unsupervised learning with features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Bouchard-Côté</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Los Angeles, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-06" />
			<biblScope unit="page" from="582" to="590" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reinforcement learning for mapping instructions to actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R K</forename><surname>Branavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harr</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing (ACL-IJCNLP)</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A hierarchical phrase-based model for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 43nd Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Ann Arbor, MI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="263" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A coefficient of agreement for nominal scales. Educational and Psychological Measurement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1960" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="37" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scalable inference and training of context-rich syntactic translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Graehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Deneefe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Thayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING/ACL)</title>
		<meeting>the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING/ACL)<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-07" />
			<biblScope unit="page" from="961" to="968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Nlyze: Interactive programming by natural language for spreadsheet data analysis and manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Gulwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Marron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Using string-kernels for learning semantic parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Kate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006-07" />
			<biblScope unit="page" from="913" to="920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to transform natural to formal languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Kate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twentieth National Conference on Artificial Intelligence (AAAI-05)</title>
		<meeting>the Twentieth National Conference on Artificial Intelligence (AAAI-05)<address><addrLine>Pittsburgh, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-07" />
			<biblScope unit="page" from="1062" to="1068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, NAACL &apos;03</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, NAACL &apos;03<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="48" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL), demonstration session</title>
		<meeting><address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Content Analysis: an Introduction to its Methodology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Krippendorff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980" />
			<publisher>Sage Publications</publisher>
			<pubPlace>Beverly Hills, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Smartsynth: Synthesizing smartphone automation scripts from natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Vu Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Gulwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>In MobiSys</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Keyword programming in java</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">C</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twentysecond IEEE/ACM International Conference on Automated Software Engineering, ASE &apos;07</title>
		<meeting>the Twentysecond IEEE/ACM International Conference on Automated Software Engineering, ASE &apos;07<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="84" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Text classification using string kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huma</forename><surname>Lodhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nello</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Watkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="419" to="444" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improved alignment models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Tillmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conference on Empirical Methods in Natural Language Processing and Very Large Corpora</title>
		<meeting>of the Conference on Empirical Methods in Natural Language essing and Very Large Corpora<address><addrLine>College Park, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-06" />
			<biblScope unit="page" from="20" to="28" />
		</imprint>
		<respStmt>
			<orgName>University of Maryland</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning for semantic parsing with statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuk</forename><forename type="middle">Wah</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the NAACL, Main Conference</title>
		<meeting>the Human Language Technology Conference of the NAACL, Main Conference<address><addrLine>New York City, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006-06" />
			<biblScope unit="page" from="439" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generation by inverting a semantic parser that uses statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuk</forename><forename type="middle">Wah</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT)</title>
		<meeting>Human Language Technologies: The Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT)<address><addrLine>Rochester, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="172" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning synchronous grammars for semantic parsing with lambda calculus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuk</forename><forename type="middle">Wah</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 45th Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="960" to="967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Lunar rocks in natural English: Explorations in natural language question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woods</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Linguistic Structures Processing</title>
		<editor>Antonio Zampoli</editor>
		<meeting><address><addrLine>North-Holland, New York</addrLine></address></meeting>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Stochastic inversion transduction grammars and bilingual parsing of parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekai</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="377" to="403" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning to parse database queries using inductive logic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Zelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth National Conference on Artificial Intelligence (AAAI96)</title>
		<meeting>the Thirteenth National Conference on Artificial Intelligence (AAAI96)<address><addrLine>Portland, OR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996-08" />
			<biblScope unit="page" from="1050" to="1055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Conference on Uncertainty in AI</title>
		<meeting>the 21st Conference on Uncertainty in AI</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
