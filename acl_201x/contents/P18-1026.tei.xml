<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph-to-Sequence Learning using Gated Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Beck</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computing and Information Systems</orgName>
								<orgName type="department" key="dep2">Faculty of Information Technology</orgName>
								<orgName type="institution">University of Melbourne</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
							<email>gholamreza.haffari@monash.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computing and Information Systems</orgName>
								<orgName type="department" key="dep2">Faculty of Information Technology</orgName>
								<orgName type="institution">University of Melbourne</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Graph-to-Sequence Learning using Gated Graph Neural Networks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="273" to="283"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>273</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Many NLP applications can be framed as a graph-to-sequence learning problem. Previous work proposing neural architec-tures on this setting obtained promising results compared to grammar-based approaches but still rely on linearisation heuristics and/or standard recurrent networks to achieve the best performance. In this work, we propose a new model that encodes the full structural information contained in the graph. Our architecture couples the recently proposed Gated Graph Neural Networks with an input transformation that allows nodes and edges to have their own hidden representations , while tackling the parameter explosion problem present in previous work. Experimental results show that our model outperforms strong baselines in generation from AMR graphs and syntax-based neu-ral machine translation.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph structures are ubiquitous in representations of natural language. In particular, many whole- sentence semantic frameworks employ directed acyclic graphs as the underlying formalism, while most tree-based syntactic representations can also be seen as graphs. A range of NLP applications can be framed as the process of transducing a graph structure into a sequence. For instance, lan- guage generation may involve realising a semantic graph into a surface form and syntactic machine translation involves transforming a tree-annotated source sentence to its translation.</p><p>Previous work in this setting rely on grammar- based approaches such as tree transducers <ref type="bibr" target="#b13">(Flanigan et al., 2016</ref>) and hyperedge replacement gram- mars ( <ref type="bibr" target="#b22">Jones et al., 2012)</ref>. A key limitation of these approaches is that alignments between graph nodes and surface tokens are required. These alignments are usually automatically generated so they can propagate errors when building the grammar. More recent approaches transform the graph into a linearised form and use off-the-shelf methods such as phrase-based machine translation ( <ref type="bibr" target="#b38">Pourdamghani et al., 2016)</ref> or neural sequence- to-sequence (henceforth, s2s) models ( <ref type="bibr" target="#b28">Konstas et al., 2017)</ref>. Such approaches ignore the full graph structure, discarding key information.</p><p>In this work we propose a model for graph-to- sequence (henceforth, g2s) learning that lever- ages recent advances in neural encoder-decoder architectures. Specifically, we employ an encoder based on Gated Graph Neural Networks ( <ref type="bibr">Li et al., 2016, GGNNs)</ref>, which can incorporate the full graph structure without loss of information. Such networks represent edge information as label-wise parameters, which can be problematic even for small sized label vocabularies (in the order of hun- dreds). To address this limitation, we also intro- duce a graph transformation that changes edges to additional nodes, solving the parameter explosion problem. This also ensures that edges have graph- specific hidden vectors, which gives more infor- mation to the attention and decoding modules in the network.</p><p>We benchmark our model in two graph-to- sequence problems, generation from Abstract Meaning Representations (AMRs) and Neural Machine Translation (NMT) with source depen- dency information. Our approach outperforms strong s2s baselines in both tasks without relying on standard RNN encoders, in contrast with pre- vious work. In particular, for NMT we show that we avoid the need for RNNs by adding sequen- tial edges between contiguous words in the depen- dency tree. This illustrates the generality of our Figure 1: Left: the AMR graph representing the sentence "The boy wants the girl to believe him.". Right: Our proposed architecture using the same AMR graph as input and the surface form as output. The first layer is a concatenation of node and positional embeddings, using distance from the root node as the position. The GGNN encoder updates the embeddings using edge-wise parameters, represented by different colors (in this example, ARG0 and ARG1). The encoder also add corresponding reverse edges (dotted arrows) and self edges for each node (dashed arrows). All parameters are shared between layers. Attention and decoder components are similar to standard s2s models. This is a pictorial representation: in our experiments the graphs are transformed before being used as inputs (see §3).</p><p>approach: linguistic biases can be added to the in- puts by simple graph transformations, without the need for changes to the model architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural Graph-to-Sequence Model</head><p>Our proposed architecture is shown in <ref type="figure">Figure 1</ref>, with an example AMR graph and its transforma- tion into its surface form. Compared to standard s2s models, the main difference is in the encoder, where we employ a GGNN to build a graph repre- sentation. In the following we explain the compo- nents of this architecture in detail. <ref type="bibr">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Gated Graph Neural Networks</head><p>Early approaches for recurrent networks on graphs ( <ref type="bibr" target="#b16">Gori et al., 2005;</ref><ref type="bibr" target="#b40">Scarselli et al., 2009</ref>) assume a fixed point representation of the parameters and learn using contraction maps. <ref type="bibr" target="#b30">Li et al. (2016)</ref> ar- gues that this restricts the capacity of the model and makes it harder to learn long distance rela- tions between nodes. To tackle these issues, they propose Gated Graph Neural Networks, which ex- tend these architectures with gating mechanisms <ref type="bibr">1</ref> Our implementation uses MXNet ( <ref type="bibr" target="#b6">Chen et al., 2015)</ref> and is based on the Sockeye toolkit ( <ref type="bibr" target="#b20">Hieber et al., 2017)</ref>. Code is available at github.com/beckdaniel/acl2018_ graph2seq.</p><p>in a similar fashion to Gated Recurrent Units ( <ref type="bibr" target="#b8">Cho et al., 2014</ref>). This allows the network to be learnt via modern backpropagation procedures.</p><p>In following, we formally define the version of GGNNs we employ in this study. Assume a di- rected graph G = {V, E, L V , L E }, where V is a set of nodes (v, v ), E is a set of edges (v i , v j , e ) and L V and L E are respectively vocabularies for nodes and edges, from which node and edge la- bels ( v and e ) are defined. Given an input graph with nodes mapped to embeddings X, a GGNN is defined as</p><formula xml:id="formula_0">h 0 v = x v r t v = σ c r v u∈Nv W r e h (t−1) u + b r e z t v = σ c z v u∈Nv W z e h (t−1) u + b z e h t v = ρ c v u∈Nv W e r t u h (t−1) u + b e h t v = (1 − z t v ) h (i−1) v + z t v h t v</formula><p>where e = (u, v, e ) is the edge between nodes u and v, N (v) is the set of neighbour nodes for v, ρ is a non-linear function, σ is the sigmoid function</p><formula xml:id="formula_1">and c v = c z v = c r v = |N v | −1 are normalisation constants.</formula><p>Our formulation differs from the original GGNNs from <ref type="bibr" target="#b30">Li et al. (2016)</ref> in some aspects: 1) we add bias vectors for the hidden state, re- set gate and update gate computations; 2) label- specific matrices do not share any components; 3) reset gates are applied to all hidden states before any computation and 4) we add normalisation con- stants. These modifications were applied based on preliminary experiments and ease of implementa- tion.</p><p>An alternative to GGNNs is the model from , which add edge label information to Graph Convolutional Net- works (GCNs). According to <ref type="bibr" target="#b30">Li et al. (2016)</ref>, the main difference between GCNs and GGNNs is analogous to the difference between convolu- tional and recurrent networks. More specifically, GGNNs can be seen as multi-layered GCNs where layer-wise parameters are tied and gating mecha- nisms are added. A large number of layers can propagate node information between longer dis- tances in the graph and, unlike GCNs, GGNNs can have an arbitrary number of layers without in- creasing the number of parameters. Nevertheless, our architecture borrows ideas from GCNs as well, such as normalising factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Using GGNNs in attentional encoder-decoder models</head><p>In s2s models, inputs are sequences of tokens where each token is represented by an embedding vector. The encoder then transforms these vec- tors into hidden states by incorporating context, usually through a recurrent or a convolutional net- work. These are fed into an attention mechanism, generating a single context vector that informs de- cisions in the decoder.</p><p>Our model follows a similar structure, where the encoder is a GGNN that receives node embeddings as inputs and generates node hidden states as out- puts, using the graph structure as context. This is shown in the example of <ref type="figure">Figure 1</ref>, where we have 4 hidden vectors, one per node in the AMR graph. The attention and decoder components fol- low similar standard s2s models, where we use a bilinear attention mechanism ( <ref type="bibr" target="#b32">Luong et al., 2015)</ref> and a 2-layered LSTM (Hochreiter and Schmid- huber, 1997) as the decoder. Note, however, that other decoders and attention mechanisms can be easily employed instead. <ref type="bibr" target="#b3">Bastings et al. (2017)</ref> employs a similar idea for syntax-based NMT, but using GCNs instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Bidirectionality and positional embeddings</head><p>While our architecture can in theory be used with general graphs, rooted directed acyclic graphs (DAGs) are arguably the most common kind in the problems we are addressing. This means that node embedding information is propagated in a top down manner. However, it is desirable to have information flow from the reverse direction as well, in the same way RNN-based encoders benefit from right-to-left propagation (as in bidi- rectional RNNs). Marcheggiani and Titov <ref type="formula">(2017)</ref> and <ref type="bibr" target="#b3">Bastings et al. (2017)</ref> achieve this by adding reverse edges to the graph, as well as self-loops edges for each node. These extra edges have spe- cific labels, hence their own parameters in the net- work.</p><p>In this work, we also follow this procedure to ensure information is evenly propagated in the graph. However, this raises another limitation: be- cause the graph becomes essentially undirected, the encoder is now unaware of any intrinsic hier- archy present in the input. Inspired by <ref type="bibr" target="#b15">Gehring et al. (2017)</ref> and <ref type="bibr" target="#b45">Vaswani et al. (2017)</ref>, we tackle this problem by adding positional embeddings to every node. These embeddings are indexed by in- teger values representing the minimum distance from the root node and are learned as model pa- rameters. <ref type="bibr">2</ref> This kind of positional embedding is restricted to rooted DAGs: for general graphs, dif- ferent notions of distance could be employed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Levi Graph Transformation</head><p>The g2s model proposed in §2 has two key defi- ciencies. First, GGNNs have three linear transfor- mations per edge type. This means that the num- ber of parameters can explode: AMR, for instance, has around 100 different predicates, which corre- spond to edge labels. Previous work deal with this problem by explicitly grouping edge labels into a single one <ref type="bibr" target="#b3">Bastings et al., 2017</ref>) but this is not an ideal solution since it incurs in loss of information. The second deficiency is that edge label in- formation is encoded in the form of GGNN pa- rameters in the network. This means that each label will have the same "representation" across all graphs. However, the latent information in edges can depend on the content in which they appear in a graph. Ideally, edges should have instance-specific hidden states, in the same way as nodes, and these should also inform decisions made in the decoder through the attention mod- ule. For instance, in the AMR graph shown in <ref type="figure">Fig- ure 1</ref>, the ARG1 predicate between want-01 and believe-01 can be interpreted as the prepo- sition "to" in the surface form, while the ARG1 predicate connecting believe-01 and boy is realised as a pronoun. Notice that edge hidden vectors are already present in s2s networks that use linearised graphs: we would like our architec- ture to also have this benefit.</p><p>Instead of modifying the architecture, we pro- pose to transform the input graph into its equiv- alent Levi graph <ref type="bibr" target="#b29">(Levi, 1942;</ref><ref type="bibr">Gross and Yellen, 2004, p. 765)</ref>. Given a graph</p><formula xml:id="formula_2">G = {V, E, L V , L E }, a Levi graph 3 is defined as G = {V , E , L V , L E }, where V = V ∪ E, L V = L V ∪ L E and L E = ∅.</formula><p>The new edge set E contains a edge for every (node, edge) pair that is present in the original graph. By definition, the Levi graph is bipartite.</p><p>Intuitively, transforming a graph into its Levi graph equivalent turns edges into additional nodes. While simple in theory, this transformation ad- dresses both modelling deficiencies mentioned above in an elegant way. Since the Levi graph has no labelled edges there is no risk of parame- ter explosion: original edge labels are represented as embeddings, in the same way as nodes. Further- more, the encoder now naturally generates hidden states for original edges as well.</p><p>In practice, we follow the procedure in §2.3 and add reverse and self-loop edges to the Levi graph, so the practical edge label vocabulary is L E = {default, reverse, self}. This still keeps the parameter space modest since we have only three labels. <ref type="figure" target="#fig_1">Figure 2</ref> shows the transformation steps in detail, applied to the AMR graph shown in <ref type="figure">Figure 1</ref>. Notice that the transformed graphs are the ones fed into our architecture: we show the original graph in <ref type="figure">Figure 1</ref> for simplicity.</p><p>It is important to note that this transformation can be applied to any graph and therefore is inde- pendent of the model architecture. We speculate this can be beneficial in other kinds of graph-based encoder such as GCNs and leave further investiga- tion to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Generation from AMR Graphs</head><p>Our first g2s benchmark is language genera- tion from AMR, a semantic formalism that repre- sents sentences as rooted DAGs ( <ref type="bibr">Banarescu et al., 2013)</ref>. Because AMR abstracts away from syntax, graphs do not have gold-standard alignment infor- mation, so generation is not a trivial task. There- fore, we hypothesize that our proposed model is ideal for this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental setup</head><p>Data and preprocessing We use the latest AMR corpus release (LDC2017T10) with the default split of 36521/1368/1371 instances for training, development and test sets. Each graph is prepro- cessed using a procedure similar to what is per- formed by <ref type="bibr" target="#b28">Konstas et al. (2017)</ref>, which includes entity simplification and anonymisation. This pre- processing is done before transforming the graph into its Levi graph equivalent. For the s2s base- lines, we also add scope markers as in <ref type="bibr" target="#b28">Konstas et al. (2017)</ref>. We detail these procedures in the Supplementary Material.</p><p>Models Our baselines are attentional s2s mod- els which take linearised graphs as inputs. The architecture is similar to the one used in <ref type="bibr" target="#b28">Konstas et al. (2017)</ref> for AMR generation, where the en- coder is a BiLSTM followed by a unidirectional LSTM. All dimensionalities are fixed to 512.</p><p>For the g2s models, we fix the number of layers in the GGNN encoder to 8, as this gave the best results on the development set. Dimensionalities are also fixed at 512 except for the GGNN encoder which uses 576. This is to ensure all models have a comparable number of parameters and therefore similar capacity.</p><p>Training for all models uses Adam ( <ref type="bibr" target="#b25">Kingma and Ba, 2015</ref>) with 0.0003 initial learning rate and 16 as the batch size. <ref type="bibr">4</ref> To regularise our models we perform early stopping on the dev set based on perplexity and apply 0.5 dropout ( <ref type="bibr" target="#b44">Srivastava et al., 2014</ref>) on the source embeddings. We detail addi- tional model and training hyperparameters in the Supplementary Material.</p><p>Evaluation Following previous work, we eval- uate our models using BLEU ( <ref type="bibr" target="#b35">Papineni et al., 2001</ref>) and perform bootstrap resampling to check statistical significance.</p><p>However, since re- cent work has questioned the effectiveness of BLEU with bootstrap resampling ( <ref type="bibr" target="#b17">Graham et al., 2014</ref>), we also report results using sentence-level CHRF++ <ref type="bibr" target="#b37">(Popovi´cPopovi´c, 2017)</ref>, using the Wilcoxon signed-rank test to check significance. Evaluation is case-insensitive for both metrics.</p><p>Recent work has shown that evaluation in neu- ral models can lead to wrong conclusions by just changing the random seed <ref type="bibr" target="#b39">(Reimers and Gurevych, 2017)</ref>. In an effort to make our con- clusions more robust, we run each model 5 times using different seeds. From each pool, we report</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BLEU CHRF++ #params</head><p>Single models s2s</p><note type="other">21.7 49.1 28.4M s2s (-s) 18.4 46.3 28.4M g2s 23.3 50.4 28.3M Ensembles s2s 26.6 52.5 142M s2s (-s) 22.0 48.9 142M g2s 27.5 53.5 141M Previous work (early AMR treebank versions) KIYCZ17 22.0 - - Previous work (as above + unlabelled data) KIYCZ17</note><p>33.8 - - PKH16</p><p>26.9 - - SPZWG17</p><p>25.6 - - FDSC16</p><p>22.0 - - <ref type="table">Table 1</ref>: Results for AMR generation on the test set. All score differences between our models and the corresponding baselines are significantly dif- ferent (p&lt;0.05). "(-s)" means input without scope marking. KIYCZ17, PKH16, SPZWG17 and results using the median model according to per- formance on the dev set (simulating what is ex- pected from a single run) and using an ensemble of the 5 models. Finally, we also report the number of parame- ters used in each model. Since our encoder archi- tectures are quite different, we try to match the number of parameters between them by chang- ing the dimensionality of the hidden layers (as ex- plained above). We do this to minimise the effects of model capacity as a confounder. <ref type="table">Table 1</ref> shows the results on the test set. For the s2s models, we also report results without the scope marking procedure of <ref type="bibr" target="#b28">Konstas et al. (2017)</ref>. Our approach significantly outperforms the s2s baselines both with individual models and ensembles, while using a comparable number of parameters. In particular, we obtain these results without relying on scoping heuristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results and analysis</head><p>On <ref type="figure" target="#fig_3">Figure 3</ref> we show an example where our model outperforms the baseline. The AMR graph contains four reentrancies, predicates that refer-Original AMR graph (p / propose-01 :ARG0 (c / country :wiki "Russia" :name (n / name :op1 "Russia")) :ARG1 (c5 / cooperate-01 :ARG0 c :ARG1 (a / and :op1 (c2 / country :wiki "India" :name (n2 / name :op1 "India")) :op2 (c3 / country :wiki "China" :name (n3 / name :op1 "China")))) :purpose (i / increase-01 :ARG0 c5 :ARG1 (s / security) :location (a2 / around :op1 (c4 / country :wiki "Afghanistan" :name (n4 / name :op1 "Afghanistan"))) :purpose (b / block-01 :ARG0 (a3 / and :op1 c :op2 c2 :op3 c3 :ARG1 (s2 / supply-01 :</p><formula xml:id="formula_3">ARG1 (d / drug)))))</formula><p>Reference surface form Russia proposes cooperation with India and China to in- crease security around Afghanistan to block drug supplies.</p><p>s2s output (CHRF++ 61.8)</p><p>Russia proposed cooperation with India and China to in- crease security around the Afghanistan to block security around the Afghanistan , India and China.</p><p>g2s output (CHRF++ 78.2)</p><p>Russia proposed cooperation with India and China to in- crease security around Afghanistan to block drug supplies. ence previously defined concepts in the graph. In the s2s models including <ref type="bibr" target="#b28">Konstas et al. (2017)</ref>, reentrant nodes are copied in the linearised form, while this is not necessary for our g2s models. We can see that the s2s prediction overgenerates the "India and China" phrase. The g2s predic- tion avoids overgeneration, and almost perfectly matches the reference. While this is only a sin- gle example, it provides evidence that retaining the full graphical structure is beneficial for this task, which is corroborated by our quantitative results. <ref type="table">Table 1</ref> also show BLEU scores reported in pre- vious work. These results are not strictly com- parable because they used different training set versions and/or employ additional unlabelled cor- pora; nonetheless some insights can be made. In particular, our g2s ensemble performs better than many previous models that combine a smaller training set with a large unlabelled corpus. It is also most informative to compare our s2s model with <ref type="bibr" target="#b28">Konstas et al. (2017)</ref>, since this baseline is very similar to theirs. We expected our single model baseline to outperform theirs since we use a larger training set but we obtained similar per- formance. We speculate that better results could be obtained by more careful tuning, but neverthe- less we believe such tuning would also benefit our proposed g2s architecture.</p><p>The best results with unlabelled data are ob- tained by <ref type="bibr" target="#b28">Konstas et al. (2017)</ref> using Gigaword sentences as additional data and a paired trained procedure with an AMR parser. It is important to note that this procedure is orthogonal to the in- dividual models used for generation and parsing. Therefore, we hypothesise that our model can also benefit from such techniques, an avenue that we leave for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Syntax-based Neural Machine Translation</head><p>Our second evaluation is NMT, using as graphs source language dependency syntax trees. We fo- cus on a medium resource scenario where addi- tional linguistic information tends to be more ben- eficial. Our experiments comprise two language pairs: English-German and English-Czech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental setup</head><p>Data and preprocessing We employ the same data and settings from <ref type="bibr" target="#b3">Bastings et al. (2017)</ref>, <ref type="bibr">5</ref> which use the News Commentary V11 corpora from the WMT16 translation task. <ref type="bibr">6</ref> English text is tokenised and parsed using SyntaxNet 7 while German and Czech texts are tokenised and split into subwords using byte-pair encodings (Sen- nrich et al., 2016, BPE) (8000 merge operations).</p><p>We refer to <ref type="bibr" target="#b3">Bastings et al. (2017)</ref> for further infor- mation on the preprocessing steps. Labelled dependency trees in the source side are transformed into Levi graphs as a preprocessing step. However, unlike AMR generation, in NMT the inputs are originally surface forms that contain important sequential information. This informa- tion is lost when treating the input as dependency trees, which might explain why <ref type="bibr" target="#b3">Bastings et al. (2017)</ref> obtain the best performance when using an initial RNN layer in their encoder. To investigate this phenomenon, we also perform experiments adding sequential connections to each word in the dependency tree, corresponding to their order in the original surface form (henceforth, g2s+). These connections are represented as edges with specific left and right labels, which are added af- ter the Levi graph transformation. <ref type="figure" target="#fig_4">Figure 4</ref> shows an example of an input graph for g2s+, with the additional sequential edges connecting the words (reverse and self edges are omitted for simplicity).</p><p>Models Our s2s and g2s models are almost the same as in the AMR generation experiments ( §4.1). The only exception is the GGNN encoder dimensionality, where we use 512 for the experi- ments with dependency trees only and 448 when the inputs have additional sequential connections. As in the AMR generation setting, we do this to ensure model capacity are comparable in the num- ber of parameters. Another key difference is that the s2s baselines do not use dependency trees: they are trained on the sentences only.</p><p>In addition to neural models, we also report re- sults for Phrase-Based Statistical MT (PB-SMT), using Moses ( <ref type="bibr" target="#b27">Koehn et al., 2007)</ref>. The PB-SMT models are trained using the same data conditions as s2s (no dependency trees) and use the standard setup in Moses, except for the language model, where we use a 5-gram LM trained on the target side of the respective parallel corpus. 8</p><p>Evaluation We report results in terms of BLEU and CHRF++, using case-sensitive versions of both metrics. Other settings are kept the same as in the AMR generation experiments ( §4.1). For PB- SMT, we also report the median result of 5 runs, obtained by tuning the model using MERT <ref type="bibr" target="#b34">(Och and Ney, 2002</ref>) 5 times. <ref type="bibr">8</ref> Note that target data is segmented using BPE, which is not the usual setting for PB-SMT. We decided to keep the segmentation to ensure data conditions are the same.</p><p>There is a deeper issue at stake .  <ref type="table">Table 2</ref> shows the results on the respective test set for both language pairs. The g2s models, which do not account for sequential information, lag be- hind our baselines. This is in line with the findings of <ref type="bibr" target="#b3">Bastings et al. (2017)</ref>, who found that having a BiRNN layer was key to obtain the best results. However, the g2s+ models outperform the base- lines in terms of BLEU scores under the same pa- rameter budget, in both single model and ensem- ble scenarios. This result show that it is possible to incorporate sequential biases in our model without relying on RNNs or any other modification in the architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results and analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>English-German</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BLEU CHRF++ #params</head><p>Single  <ref type="table">Table 2</ref>: Results for syntax-based NMT on the test sets. All score differences between our models and the corresponding baselines are significantly dif- ferent (p&lt;0.05), including the negative CHRF++ result for En-Cs.</p><p>Interestingly, we found different trends when analysing the CHRF++ numbers. In particular, this metric favours the PB-SMT models for both lan- guage pairs, while also showing improved perfor- mance for s2s in En-Cs. CHRF++ has been shown to better correlate with human judgments com- pared to BLEU, both at system and sentence level for both language pairs ( <ref type="bibr" target="#b4">Bojar et al., 2017)</ref>, which motivated our choice as an additional metric. We leave further investigation of this phenomena for future work.</p><p>We also show some of the results reported by <ref type="bibr" target="#b3">Bastings et al. (2017)</ref> in <ref type="table">Table 2</ref>. Note that their results were based on a different implementation, which may explain some variation in performance. Their BoW+GCN model is the most similar to ours, as it uses only an embedding layer and a GCN encoder. We can see that even our sim- pler g2s model outperforms their results. A key difference between their approach and ours is the Levi graph transformation and the resulting hidden vectors for edges. We believe their architecture would also benefit from our proposed transforma- tion. In terms of baselines, s2s performs better than their BiRNN model for En-De and compara- bly for En-Cs, which corroborates that our base- lines are strong ones. Finally, our g2s+ single models outperform their BiRNN+GCN results, in particular for En-De, which is further evidence that RNNs are not necessary for obtaining the best performance in this setting.</p><p>An important point about these experiments is that we did not tune the architecture: we simply employed the same model we used in the AMR generation experiments, only adjusting the dimen- sionality of the encoder to match the capacity of the baselines. We speculate that even better re- sults would be obtained by tuning the architecture to this task. Nevertheless, we still obtained im- proved performance over our baselines and previ- ous work, underlining the generality of our archi- tecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related work</head><p>Graph-to-sequence modelling Early NLP ap- proaches for this problem were based on Hy- peredge Replacement Grammars ( <ref type="bibr">Drewes et al., 1997, HRGs)</ref>. These grammars assume the trans- duction problem can be split into rules that map portions of a graph to a set of tokens in the out- put sequence. In particular, Chiang et al. (2013) defines a parsing algorithm, followed by a com- plexity analysis, while <ref type="bibr" target="#b22">Jones et al. (2012)</ref> report experiments on semantic-based machine transla- tion using HRGs. HRGs were also used in pre- vious work on AMR parsing ( <ref type="bibr" target="#b36">Peng et al., 2015)</ref>. The main drawback of these grammar-based ap- proaches though is the need for alignments be- tween graph nodes and surface tokens, which are usually not available in gold-standard form.</p><p>Neural networks for graphs Recurrent net- works on general graphs were first proposed un-der the name Graph Neural Networks ( <ref type="bibr" target="#b16">Gori et al., 2005;</ref><ref type="bibr" target="#b40">Scarselli et al., 2009)</ref>. Our work is based on the architecture proposed by <ref type="bibr" target="#b30">Li et al. (2016)</ref>, which add gating mechanisms. The main differ- ence between their work and ours is that they fo- cus on problems that concern the input graph it- self such as node classification or path finding while we focus on generating strings. The main alternative for neural-based graph representations is Graph Convolutional Networks ( <ref type="bibr" target="#b5">Bruna et al., 2014;</ref><ref type="bibr" target="#b10">Duvenaud et al., 2015;</ref>, which have been applied in a range of prob- lems. In NLP,  use a similar architecture for Semantic Role Labelling. They use heuristics to mitigate the parameter ex- plosion by grouping edge labels, while we keep the original labels through our Levi graph trans- formation. An interesting alternative is proposed by <ref type="bibr" target="#b41">Schlichtkrull et al. (2017)</ref>, which uses tensor factorisation to reduce the number of parameters.</p><p>Applications Early work on AMR generation employs grammars and transducers <ref type="bibr" target="#b13">(Flanigan et al., 2016;</ref><ref type="bibr" target="#b43">Song et al., 2017)</ref>. Linearisation ap- proaches include ( <ref type="bibr" target="#b38">Pourdamghani et al., 2016)</ref> and ( <ref type="bibr" target="#b28">Konstas et al., 2017)</ref>, which showed that graph simplification and anonymisation are key to good performance, a procedure we also employ in our work. However, compared to our approach, lin- earisation incurs in loss of information. MT has a long history of previous work that aims at incor- porating syntax <ref type="bibr" target="#b46">(Wu, 1997;</ref><ref type="bibr" target="#b47">Yamada and Knight, 2001;</ref><ref type="bibr" target="#b14">Galley et al., 2004;</ref><ref type="bibr" target="#b31">Liu et al., 2006</ref>, inter alia). This idea has also been investigated in the context of NMT. <ref type="bibr" target="#b3">Bastings et al. (2017)</ref> is the most similar work to ours, and we benchmark against their approach in our NMT experiments. <ref type="bibr" target="#b11">Eriguchi et al. (2016)</ref> also employs source syntax, but us- ing constituency trees instead. Other approaches have investigated the use of syntax in the target language <ref type="bibr" target="#b0">(Aharoni and Goldberg, 2017;</ref><ref type="bibr" target="#b12">Eriguchi et al., 2017)</ref>. Finally, <ref type="bibr" target="#b19">Hashimoto and Tsuruoka (2017)</ref> treats source syntax as a latent variable, which can be pretrained using annotated data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion and Conclusion</head><p>We proposed a novel encoder-decoder architec- ture for graph-to-sequence learning, outperform- ing baselines in two NLP tasks: generation from AMR graphs and syntax-based NMT. Our approach addresses shortcomings from previous work, including loss of information from lineari- sation and parameter explosion. In particular, we showed how graph transformations can solve is- sues with graph-based networks without chang- ing the underlying architecture. This is the case of the proposed Levi graph transformation, which ensures the decoder can attend to edges as well as nodes, but also to the sequential connections added to the dependency trees in the case of NMT. Overall, because our architecture can work with general graphs, it is straightforward to add linguis- tic biases in the form of extra node and/or edge information. We believe this is an interesting re- search direction in terms of applications.</p><p>Our architecture nevertheless has two major limitations. The first one is that GGNNs have a fixed number of layers, even though graphs can vary in size in terms of number of nodes and edges. A better approach would be to allow the encoder to have a dynamic number of layers, pos- sibly based on the diameter (longest path) in the input graph. The second limitation comes from the Levi graph transformation: because edge la- bels are represented as nodes they end up shar- ing the vocabulary and therefore, the same seman- tic space. This is not ideal, as nodes and edges are different entities. An interesting alternative is Weave Module Networks ( <ref type="bibr" target="#b23">Kearnes et al., 2016)</ref>, which explicitly decouples node and edge repre- sentations without incurring in parameter explo- sion. Incorporating both ideas to our architecture is an research direction we plan for future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Top: the AMR graph from Figure 1 transformed into its corresponding Levi graph. Bottom: Levi graph with added reverse and self edges (colors represent different edge labels).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FDSC16 are respectively the results reported in Konstas et al. ( 2017 )</head><label>2017</label><figDesc>, Pourdamghani et al. (2016), Song et al. (2017) and Flanigan et al. (2016).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Example showing overgeneration due to reentrancies. Top: original AMR graph with key reentrancies highlighted. Bottom: reference and outputs generated by the s2s and g2s models, highlighting the overgeneration phenomena.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Top: a sentence with its corresponding dependency tree. Bottom: the transformed tree into a Levi graph with additional sequential connections between words (dashed lines). The full graph also contains reverse and self edges, which are omitted in the figure.</figDesc></figure>

			<note place="foot" n="2"> Vaswani et al. (2017) also proposed fixed positional embeddings based on sine and cosine wavelengths. Preliminary experiments showed that this approach did not work in our case: we speculate this is because wavelengths are more suitable to sequential inputs.</note>

			<note place="foot" n="3"> Formally, a Levi graph is defined over any incidence structure, which is a general concept usually considered in a geometrical context. Graphs are an example of incidence structures but so are points and lines in the Euclidean space, for instance.</note>

			<note place="foot" n="4"> Larger batch sizes hurt dev performance in our preliminary experiments. There is evidence that small batches can lead to better generalisation performance (Keskar et al., 2017). While this can make training time slower, it was doable in our case since the dataset is small.</note>

			<note place="foot" n="5"> We obtained the data from the original authors to ensure results are comparable without any influence from preprocessing steps. 6 http://www.statmt.org/wmt16/ translation-task.html 7 https://github.com/tensorflow/models/ tree/master/syntaxnet</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by the Australian Re-search Council (DP160102686). The research reported in this paper was partly conducted at the 2017 Frederick Jelinek Memorial Summer Workshop on Speech and Language Technolo-gies, hosted at Carnegie Mellon University and sponsored by Johns Hopkins University with un-restricted gifts from Amazon, Apple, Facebook, Google, and Microsoft. The authors would also like to thank Joost Bastings for sharing the data from his paper's experiments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards String-to-Tree Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roee</forename><surname>Aharoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="132" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Banarescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Bonial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madalina</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Griffitt</surname></persName>
		</author>
		<imprint>
			<pubPlace>Ulf Hermjakob, Kevin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Abstract Meaning Representation for Sembanking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse</title>
		<meeting>the 7th Linguistic Annotation Workshop and Interoperability with Discourse</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="178" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Graph Convolutional Encoders for Syntax-aware Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Bastings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilker</forename><surname>Aziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Sima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1947" to="1957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Results of the WMT17 Metrics Shared Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Kamran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WMT</title>
		<meeting>WMT</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="293" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Spectral Networks and Locally Connected Networks on Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Machine Learning Systems</title>
		<meeting>the Workshop on Machine Learning Systems</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Parsing Graphs with Hyperedge Replacement Grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bevan</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="924" to="932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning Phrase Representations using RNN EncoderDecoder for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Hyperedge Replacement Graph Grammars. Handbook of Graph Grammars and Computing by Graph Transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Drewes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><forename type="middle">Jörg</forename><surname>Kreowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annegret</forename><surname>Habel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional Networks on Graphs for Learning Molecular Fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Aguileraiparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Gómez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alán</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2215" to="2223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Tree-to-Sequence Attentional Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akiko</forename><surname>Eriguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to Parse and Translate Improves Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akiko</forename><surname>Eriguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generation from Abstract Meaning Representation using Tree Transducers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="731" to="739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">What&apos;s in a translation rule?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hopkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="273" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<title level="m">Convolutional Sequence to Sequence Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A New Model for Learning in Graph Domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCNN</title>
		<meeting>IJCNN</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Randomized Significance Tests in Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitika</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WMT</title>
		<meeting>WMT</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="266" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Handbook of Graph Theory</title>
		<editor>Jonathan Gross and Jay Yellen</editor>
		<imprint>
			<date type="published" when="2004" />
			<publisher>CRC Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural Machine Translation with Source-Side Latent Graph Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="125" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Domhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vilar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Sokolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<title level="m">Sockeye: A Toolkit for Neural Machine Translation. arXiv preprint pages</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semantics-Based Machine Translation with Hyperedge Replacement Grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bevan</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1359" to="1376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Molecular Graph Convolutions: Moving Beyond Fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Berndl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer-Aided Molecular Design</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="595" to="608" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheevatsa</forename><surname>Nitish Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mudigere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
	<note>Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">SemiSupervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL Demo Session</title>
		<meeting>ACL Demo Session<address><addrLine>Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Neural AMR: Sequence-to-Sequence Models for Parsing and Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivasan</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Finite Geometrical Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Friedrich</forename><forename type="middle">Wilhelm</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levi</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Gated Graph Sequence Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR. 1</title>
		<meeting>ICLR. 1</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Treeto-string alignment template for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouxun</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the ACL-ACL &apos;06</title>
		<meeting>the 21st International Conference on Computational Linguistics and the 44th annual meeting of the ACL-ACL &apos;06</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="609" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Effective Approaches to Attentionbased Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Discriminative training and maximum entropy models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
		<idno type="doi">10.3115/1073083.1073133</idno>
		<ptr target="https://doi.org/10.3115/1073083.1073133" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics-ACL &apos;02</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics-ACL &apos;02</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A Synchronous Hyperedge Replacement Grammar based approach for AMR parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="32" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">chrF ++: words helping character n-grams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Popovi´cpopovi´c</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WMT</title>
		<meeting>WMT</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="612" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Generating English from Abstract Meaning Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Pourdamghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INLG</title>
		<meeting>INLG</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">0</biblScope>
			<biblScope unit="page" from="21" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Reporting Score Distributions Makes a Difference: Performance Study of LSTM-networks for Sequence Tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="338" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The Graph Neural Network Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Ching Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Modeling Relational Data with Graph Convolutional Networks</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Neural Machine Translation of Rare Words with Subword Units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">AMR-to-text Generation with Synchronous Node Replacement Grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Attention Is All You Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Stochastic inversion transduction grammars and bilingual parsing of parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekai</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="377" to="403" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A Syntaxbased Statistical Translation Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="523" to="530" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
