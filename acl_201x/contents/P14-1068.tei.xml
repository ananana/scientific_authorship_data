<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Grounded Meaning Representations with Autoencoders</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carina</forename><surname>Silberer</surname></persName>
							<email>c.silberer@ed.ac.uk, mlap@inf.ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Language, Cognition and Computation School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton Street</addrLine>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Language, Cognition and Computation School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton Street</addrLine>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Grounded Meaning Representations with Autoencoders</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="721" to="732"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper we address the problem of grounding distributional representations of lexical meaning. We introduce a new model which uses stacked autoencoders to learn higher-level embeddings from tex-tual and visual input. The two modalities are encoded as vectors of attributes and are obtained automatically from text and images, respectively. We evaluate our model on its ability to simulate similarity judgments and concept categorization. On both tasks, our approach outperforms baselines and related models.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent years have seen a surge of interest in sin- gle word vector spaces <ref type="bibr" target="#b51">(Turney and Pantel, 2010;</ref><ref type="bibr" target="#b12">Collobert et al., 2011;</ref><ref type="bibr" target="#b33">Mikolov et al., 2013)</ref> and their successful use in many natural language ap- plications. Examples include information retrieval ( <ref type="bibr" target="#b30">Manning et al., 2008)</ref>, search query expansions ( <ref type="bibr" target="#b26">Jones et al., 2006</ref>), document classification <ref type="bibr" target="#b42">(Sebastiani, 2002</ref>), and question answering ( <ref type="bibr" target="#b55">Yih et al., 2013)</ref>. Vector spaces have been also popular in cognitive science figuring prominently in simula- tions of human behavior involving semantic prim- ing, deep dyslexia, text comprehension, synonym selection, and similarity judgments (see <ref type="bibr" target="#b20">Griffiths et al., 2007)</ref>. In general, these models specify mechanisms for constructing semantic representa- tions from text corpora based on the distributional hypothesis <ref type="bibr" target="#b22">(Harris, 1970)</ref>: words that appear in similar linguistic contexts are likely to have related meanings.</p><p>Word meaning, however, is also tied to the physical world. Words are grounded in the exter- nal environment and relate to sensorimotor experi- ence <ref type="bibr" target="#b39">(Regier, 1996;</ref><ref type="bibr" target="#b27">Landau et al., 1998;</ref><ref type="bibr" target="#b3">Barsalou, 2008)</ref>. To account for this, new types of perceptu- ally grounded distributional models have emerged.</p><p>These models learn the meaning of words based on textual and perceptual input. The latter is ap- proximated by feature norms elicited from humans ( <ref type="bibr" target="#b1">Andrews et al., 2009;</ref><ref type="bibr" target="#b49">Steyvers, 2010;</ref><ref type="bibr" target="#b44">Silberer and Lapata, 2012)</ref>, visual information extracted automatically from images, <ref type="bibr" target="#b16">(Feng and Lapata, 2010;</ref><ref type="bibr" target="#b8">Bruni et al., 2012a;</ref><ref type="bibr" target="#b43">Silberer et al., 2013)</ref> or a combination of both <ref type="bibr" target="#b40">(Roller and Schulte im Walde, 2013</ref>). Despite differences in formulation, most existing models conceptualize the problem of meaning representation as one of learning from multiple views corresponding to different modali- ties. These models still represent words as vectors resulting from the combination of representations with different statistical properties that do not nec- essarily have a natural correspondence (e.g., text and images).</p><p>In this work, we introduce a model, illus- trated in <ref type="figure">Figure 1</ref>, which learns grounded mean- ing representations by mapping words and im- ages into a common embedding space. Our model uses stacked autoencoders ( <ref type="bibr" target="#b4">Bengio et al., 2007)</ref> to induce semantic representations integrating vi- sual and textual information. The literature de- scribes several successful approaches to multi- modal learning using different variants of deep networks <ref type="bibr" target="#b36">(Ngiam et al., 2011;</ref><ref type="bibr" target="#b48">Srivastava and Salakhutdinov, 2012</ref>) and data sources including text, images, audio, and video. Unlike most pre- vious work, our model is defined at a finer level of granularity -it computes meaning representa- tions for individual words and is unique in its use of attributes as a means of representing the textual and visual modalities. We follow <ref type="bibr" target="#b43">Silberer et al. (2013)</ref> in arguing that an attribute-centric repre- sentation is expedient for several reasons.</p><p>Firstly, attributes provide a natural way of ex- pressing salient properties of word meaning as demonstrated in norming studies (e.g., <ref type="bibr" target="#b31">McRae et al., 2005</ref>) where humans often employ attributes when asked to describe a concept. Secondly, from a modeling perspective, attributes allow for eas- ier integration of different modalities, since these are rendered in the same medium, namely, lan- guage. Thirdly, attributes are well-suited to de- scribing visual phenomena (e.g., objects, scenes, actions). They allow to generalize to new in- stances for which there are no training exam- ples available and to transcend category and task boundaries whilst offering a generic description of visual data <ref type="bibr" target="#b14">(Farhadi et al., 2009)</ref>.</p><p>Our model learns multimodal representations from attributes which are automatically inferred from text and images. We evaluate the embed- dings it produces on two tasks, namely word sim- ilarity and categorization. In the first task, model estimates of word similarity (e.g., gem-jewel are similar but glass-magician are not) are compared against elicited similarity ratings. We performed a large-scale evaluation on a new dataset consist- ing of human similarity judgments for 7,576 word pairs. Unlike previous efforts such as the widely used WordSim353 collection ( <ref type="bibr" target="#b17">Finkelstein et al., 2002</ref>), our dataset contains ratings for visual and textual similarity, thus allowing to study the two modalities (and their contribution to meaning rep- resentation) together and in isolation. We also assess whether the learnt representations are ap- propriate for categorization, i.e., grouping a set of objects into meaningful semantic categories (e.g., peach and apple are members of FRUIT, whereas chair and table are FURNITURE). On both tasks, our model outperforms baselines and related models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The presented model has connections to several lines of work in NLP, computer vision research, and more generally multimodal learning. We re- view related work in these areas below.</p><p>Grounded Semantic Spaces Grounded seman- tic spaces are essentially distributional models augmented with perceptual information. A model akin to Latent Semantic Analysis <ref type="bibr" target="#b28">(Landauer and Dumais, 1997</ref>) is proposed in <ref type="bibr" target="#b11">Bruni et al. (2012b)</ref> who concatenate two independently constructed textual and visual spaces and subsequently project them onto a lower-dimensional space using Singu- lar Value Decomposition.</p><p>Several other models have been extensions of Latent Dirichlet Allocation ( <ref type="bibr" target="#b7">Blei et al., 2003)</ref> where topic distributions are learned from words and other perceptual units. <ref type="bibr" target="#b16">Feng and Lapata (2010)</ref> use visual words which they extract from a corpus of multimodal documents (i.e., BBC news articles and their associated images), whereas oth- ers <ref type="bibr" target="#b49">(Steyvers, 2010;</ref><ref type="bibr" target="#b1">Andrews et al., 2009;</ref><ref type="bibr" target="#b44">Silberer and Lapata, 2012</ref>) use feature norms obtained in longitudinal elicitation studies (see <ref type="bibr" target="#b31">McRae et al. (2005)</ref> for an example) as an approximation of the visual environment. More recently, topic mod- els which combine both feature norms and vi- sual words have also been introduced <ref type="bibr" target="#b40">(Roller and Schulte im Walde, 2013)</ref>. Drawing inspiration from the successful application of attribute clas- sifiers in object recognition, <ref type="bibr" target="#b43">Silberer et al. (2013)</ref> show that automatically predicted visual attributes act as substitutes for feature norms without any critical information loss.</p><p>The visual and textual modalities on which our model is trained are decoupled in that they are not derived from the same corpus (we would expect co-occurring images and text to correlate to some extent) but unified in their representation by natu- ral language attributes. The use of stacked autoen- coders to extract a shared lexical meaning repre- sentation is new to our knowledge, although, as we explain below related to a large body of work on deep learning.</p><p>Multimodal Deep Learning Our work employs deep learning (a.k.a deep networks) to project lin- guistic and visual information onto a unified rep- resentation that fuses the two modalities together. The goal of deep learning is to learn multiple lev- els of representations through a hierarchy of net- work architectures, where higher-level representa- tions are expected to help define higher-level con- cepts.</p><p>A large body of work has focused on projecting words and images into a common space using a va- riety of deep learning methods ranging from deep and restricted Boltzman machines <ref type="bibr" target="#b48">(Srivastava and Salakhutdinov, 2012;</ref><ref type="bibr" target="#b15">Feng et al., 2013)</ref>, to au- toencoders ( <ref type="bibr" target="#b54">Wu et al., 2013)</ref>, and recursive neural networks ( <ref type="bibr" target="#b46">Socher et al., 2013b</ref>). Similar methods have been employed to combine other modalities such as speech and video <ref type="bibr" target="#b36">(Ngiam et al., 2011</ref>) or images ( <ref type="bibr" target="#b25">Huang and Kingsbury, 2013</ref>). Although our model is conceptually similar to these studies (especially those applying stacked autoencoders), it differs considerably from them in at least two aspects. Firstly, most of these approaches aim to learn a shared representation between modalities so as to infer some missing modality from others (e.g., to infer text from images and vice versa); in contrast, we aim to learn an optimal representa- tion for each modality and their optimal combi- nation. Secondly, our problem setting is different from the former studies, which usually deal with classification tasks and fine-tune the deep neural networks using training data with explicit class la- bels; in contrast we fine-tune our autoencoders us- ing a semi-supervised criterion. That is, we use indirect supervision in the form of object classifi- cation in addition to the objective of reconstruct- ing the attribute-centric input representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Autoencoders for Grounded Semantics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Background</head><p>Our model learns higher-level meaning represen- tations for single words from textual and visual input in a joint fashion. We first briefly review autoencoders in Section 3.1 with emphasis on as- pects relevant to our model which we then de- scribe in Section 3.2.</p><p>Autoencoders An autoencoder is an unsuper- vised neural network which is trained to recon- struct a given input from its latent representation <ref type="bibr" target="#b5">(Bengio, 2009)</ref>. It consists of an encoder f θ which maps an input vector x (i) to a latent representa- tion y (i) = f θ (x (i) ) = s(Wx (i) + b), with s being a non-linear activation function, such as a sig- moid function. A decoder g θ then aims to recon- struct input</p><formula xml:id="formula_0">x (i) from y (i) , i.e., ˆ x (i) = g θ (y (i) ) = s(W y (i) + b ).</formula><p>The training objective is the de- termination of parametersˆθparametersˆ parametersˆθ = {W, b} andˆθandˆ andˆθ = {W , b } that minimize the average reconstruction error over a set of input vectors {x (1) , ..., x (n) }:</p><formula xml:id="formula_1">ˆ θ, ˆ θ = arg min θ,θ 1 n n ∑ i=1 L(x (i) , g θ ( f θ (x (i) ))), (1)</formula><p>where L is a loss function, such as cross-entropy. Parameters θ and θ can be optimized by gradient descent methods. Autoencoders are a means to learn representa- tions of some input by retaining useful features in the encoding phase which help to reconstruct the input, whilst discarding useless or noisy ones. To this end, different strategies have been employed to guide parameter learning and constrain the hid- den representation. Examples include imposing a bottleneck to produce an under-complete rep- resentation of the input, using sparse representa- tions, or denoising.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Denoising Autoencoders</head><p>The training criterion with denoising autoencoders is the reconstruction of clean input x (i) given a corrupted versioñ x (i) ( <ref type="bibr" target="#b52">Vincent et al., 2010)</ref>. The underlying idea is that the learned latent representation is good if the au- toencoder is capable of reconstructing the actual input from its corruption. The reconstruction error for an input x (i) with loss function L then is:</p><formula xml:id="formula_2">L(x (i) , g θ ( f θ (˜ x (i) )))<label>(2)</label></formula><p>One possible corruption process is masking noise, where the corrupted versioñ x (i) results from ran- domly setting a fraction v of x (i) to 0.</p><p>Stacked Autoencoders Several (denoising) au- toencoders can be used as building blocks to form a deep neural network ( <ref type="bibr" target="#b4">Bengio et al., 2007;</ref><ref type="bibr" target="#b52">Vincent et al., 2010)</ref>. For that purpose, the autoen- coders are pre-trained layer by layer, with the cur- rent layer being fed the latent representation of the previous autoencoder as input. Using this unsuper- vised pre-training procedure, initial parameters are found which approximate a good solution. Subse- quently, the original input layer and hidden repre- sentations of all the autoencoders are stacked and all network parameters are fine-tuned with back- propagation.</p><p>To further optimize the parameters of the net- work, a supervised criterion can be imposed on top of the last hidden layer such as the minimization of a prediction error on a supervised task <ref type="bibr" target="#b5">(Bengio, 2009)</ref>. Another approach is to unfold the stacked autoencoders and fine-tune them with respect to the minimization of the global reconstruction error <ref type="bibr" target="#b23">(Hinton and Salakhutdinov, 2006</ref>). Alternatively, a semi-supervised criterion can be used <ref type="bibr" target="#b38">(Ranzato and Szummer, 2008;</ref><ref type="bibr" target="#b47">Socher et al., 2011</ref>) through combination of the unsupervised training criterion (global reconstruction) with a supervised criterion (prediction of some target given the latent repre- sentation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Semantic Representations</head><p>To learn meaning representations of single words from textual and visual input, we employ stacked (denoising) autoencoders (SAEs). Both input modalities are vector-based representations of words, or, more precisely, the objects they refer to (e.g., canary, trolley). The vector dimensions cor- respond to textual and visual attributes, examples of which are shown in <ref type="table">Table 1</ref>. We explain how these representations are obtained in more detail</p><formula xml:id="formula_3">... ... ... input x TEXT W (1) W (3) ... ... ... IMAGES W (2) W (4) ... bimodal coding ˘ y W (5 ) W (5)</formula><p>...</p><p>softmaxˆtsoftmaxˆsoftmaxˆt W <ref type="bibr">(6)</ref> ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>...</head><p>W <ref type="bibr">(3 )</ref> ...</p><p>...</p><formula xml:id="formula_4">W (4 )</formula><p>...</p><formula xml:id="formula_5">reconstructionˆxreconstructionˆ reconstructionˆx W (1 )</formula><p>...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>W (2 )</head><p>Figure 1: Stacked autoencoder trained with semi-supervised objective. Input to the model are single- word vector representations obtained from text and images. Vector dimensions correspond to textual and visual attributes, respectively (see <ref type="table">Table 1</ref>).</p><p>in Section 4.1. We first train SAEs with two hid- den layers (codings) for each modality separately. Then, we join these two SAEs by feeding their re- spective second coding simultaneously to another autoencoder, whose hidden layer thus yields the fused meaning representation. Finally, we stack all layers and unfold them in order to fine-tune the SAE. <ref type="figure">Figure 1</ref> illustrates the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unimodal Autoencoders</head><p>For both modalities, we use the hyperbolic tangent function as activa- tion function for encoder f θ and decoder g θ and an entropic loss function for L. The weights of each autoencoder are tied, i.e., W = W T . We employ denoising autoencoders (DAEs) for pre-training the textual modality. Regarding the visual autoen- coder, we derive a new ('denoised') target vector to be reconstructed for each input vector x (i) , and treat x (i) itself as corrupted input. The unimodal autoencoder is thus trained to denoise a given in- put. The target vector is derived as follows: each object o in our data is represented by multiple im- ages, and each image is in turn represented by a visual attribute vector x (i) . The target vector is the sum of x (i) and the centroid x (j) of the remaining attribute vectors representing object o.</p><p>Bimodal Autoencoder The bimodal autoen- coder is fed with the concatenated final hidden codings of the visual and textual modalities as in- put and maps these inputs to a joint hidden layer ˘ y with B units. We normalize both unimodal input codings to unit length. Again, we use tied weights for the bimodal autoencoder. We also encourage the autoencoder to detect dependencies between the two modalities while learning the mapping to the bimodal hidden layer. We therefore apply masking noise to one modality with a masking fac- tor v (see Section 3.1), so that the corrupted modal- ity optimally has to rely on the other modality in order to reconstruct its missing input features.</p><p>Stacked Bimodal Autoencoder We finally build a stacked bimodal autoencoder (SAE) with all pre-trained layers and fine-tune them with re- spect to a semi-supervised criterion. That is, we unfold the stacked autoencoder and furthermore add a softmax output layer on top of the bimodal layer ˘ y that outputs predictionsˆtpredictionsˆpredictionsˆt with respect to the inputs' object labels (e.g., boat):</p><formula xml:id="formula_6">ˆ t (i) = exp(W (6) ˘ y (i) + b (6) ) ∑ O k=1 exp(W (6) k. ˘ y (i) + b (6) k ) ,<label>(3)</label></formula><p>with weights W (6) ∈ R O×B , b (6) ∈ R O×1 , where O is the number of unique object labels. The over- all objective to be minimized is therefore the weighted sum of the reconstruction error L r and the classification error L c :</p><formula xml:id="formula_7">L = 1 n n ∑ i=1 δ r L r (x (i) , ˆ x (i) )+δ c L c (t (i) , ˆ t (i) ) +λR<label>(4)</label></formula><p>where δ r and δ c are weighting parameters that give different importance to the partial objectives,  <ref type="table">Table 1</ref>: Examples of attribute-based representations provided as input to our autoencoders.</p><note type="other">eats seeds has beak has claws has handlebar has wheels has wings is yellow made of wood canary 0.</note><p>L c and L r are entropic loss functions, and R is a regularization term with R = ∑ 5 j=1 2||W (j) || 2 + ||W (6) || 2 . Finally, ˆ t (i) is the object label vector pre- dicted by the softmax layer for input vector x (i) , and t (i) is the correct object label, represented as a O-dimensional one-hot vector <ref type="bibr">1</ref> .</p><p>The additional supervised criterion drives the learning towards a representation capable of dis- criminating between different objects. Further- more, the semi-supervised setting affords flexibil- ity, allowing to adapt the architecture to specific tasks. For example, by setting the corruption pa- rameter v for the textual modality to one and δ r to zero, a standard object classification model for images can be trained. Setting v close to one for ei- ther modality enables the model to infer the other (missing) modality. As our input consists of nat- ural language attributes, the model would infer textual attributes given visual attributes and vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>In this section we present our experimental setup for assessing the performance of our model. We give details on the tasks and datasets used for eval- uation, we explain how the textual and visual in- puts were constructed, how the SAE model was trained, and describe the approaches used for com- parison with our own work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data</head><p>We learn meaning representations for the nouns contained in <ref type="bibr" target="#b31">McRae et al.'s (2005)</ref> feature norms. These are 541 concrete animate and inanimate ob- jects (e.g., animals, clothing, vehicles, utensils, fruits, and vegetables). The norms were elicited by asking participants to list properties (e.g., barks, an animal, has legs) describing the nouns they were presented with.</p><p>As shown in <ref type="figure">Figure 1</ref>, our model takes as in- put two (real-valued) vectors representing the vi- sual and textual modalities. Vector dimensions correspond to textual and visual attributes, respec- tively. Textual attributes were extracted by run- ning Strudel ( <ref type="bibr" target="#b2">Baroni et al., 2010</ref>) on a 2009 dump of the English Wikipedia. <ref type="bibr">2</ref> Strudel is a fully automatic method for extracting weighted word- attribute pairs (e.g., bat-species:n, bat-bite:v) from a lemmatized and POS-tagged corpus. Weights are log-likelihood ratio scores expressing how strongly an attribute and a word are associated. We only retained the ten highest scored attributes for each target word. This returned a total of 2,362 dimensions for the textual vectors. Association scores were scaled to the [−1, 1] range.</p><p>To obtain visual vectors, we followed the methodology put forward in <ref type="bibr" target="#b43">Silberer et al. (2013)</ref>. Specifically, we used an updated version of their dataset to train SVM-based attribute classifiers that predict visual attributes for images <ref type="bibr" target="#b14">(Farhadi et al., 2009</ref>). The dataset is a taxonomy of 636 vi- sual attributes (e.g., has wings, made of wood) and nearly 700K images from ImageNet ( <ref type="bibr" target="#b13">Deng et al., 2009</ref>) describing more than 500 of <ref type="bibr" target="#b31">McRae et al.'s (2005)</ref> nouns. The classifiers perform reason- ably well with an interpolated average precision of 0.52. We only considered attributes assigned to at least two nouns in the dataset, obtaining a 414 dimensional vector for each noun. Analo- gously to the textual representations, visual vec- tors were scaled to the [−1, 1] range.</p><p>We follow <ref type="bibr">Silberer et al.'s (2013)</ref> partition of the dataset into training, validation, and test set and acquire visual vectors for each of the sets. We use the visual vectors of the training and development set for training the autoencoders, and the vectors for the test set for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model Architecture</head><p>Model parameters were optimized on a subset of the word association norms collected by <ref type="bibr" target="#b35">Nelson et al. (1998)</ref>. <ref type="bibr">3</ref> These were established by present- ing participants with a cue word (e.g., canary) and asking them to name an associate word in response (e.g., bird, sing, yellow). For each cue, the norms provide a set of associates and the frequencies with which they were named. The dataset con- tains a very large number of cue-associate pairs (63,619 in total) some of which luckily are cov- ered in <ref type="bibr" target="#b31">McRae et al. (2005)</ref>. <ref type="bibr">4</ref> During training we used correlation analysis (Spearman's ρ) to monitor the degree of linear relationship between model cue-associate (cosine) similarities and hu- man probabilities.</p><p>The best autoencoder on the word association task obtained a correlation coefficient of 0.33. This performance is superior to the results re- ported in <ref type="bibr" target="#b43">Silberer et al. (2013)</ref> (their correlation coefficients range from 0.16 to <ref type="bibr">0.28)</ref>. This model has the following architecture: the textual autoen- coder (see <ref type="figure">Figure 1</ref>, left-hand side) consists of 700 hidden units which are then mapped to the sec- ond hidden layer with 500 units (the corruption parameter was set to v = 0.1); the visual autoen- coder (see <ref type="figure">Figure 1</ref>, right-hand side) has 170 and 100 hidden units, in the first and second layer, re- spectively. The 500 textual and 100 visual hidden units were fed to a bimodal autoencoder contain- ing 500 latent units, and masking noise was ap- plied to the textual modality with v = 0.2. The weighting parameters for the joint training objec- tive of the stacked autoencoder were set to δ r = 0.8 and δ c = 1 (see Equation <ref type="formula" target="#formula_7">(4)</ref>).</p><p>We used the model described above and the meaning representations obtained from the out- put of the bimodal latent layer for all the eval- uation tasks detailed below. Some performance gains could be expected if parameter optimization took place separately for each task. However, we wanted to avoid overfitting, and show that our pa- rameters are robust across tasks and datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Tasks</head><p>Word Similarity We first evaluated how well our model predicts word similarity ratings. Al- though several relevant datasets exist, such as the widely used WordSim353 ( <ref type="bibr" target="#b17">Finkelstein et al., 2002</ref>) or the more recent Rel-122 norms <ref type="bibr" target="#b50">(Szumlanski et al., 2013)</ref>, they contain many abstract words, (e.g., love-sex or arrest-detention) which are not covered in <ref type="bibr" target="#b31">McRae et al. (2005)</ref>. This is for a good reason, as most abstract words do not have discernible attributes, or at least attributes that par- ticipants would agree upon. We thus created a new dataset consisting exclusively of <ref type="bibr" target="#b31">McRae et al. (2005)</ref> nouns which we hope will be useful for the development and evaluation of grounded semantic space models. <ref type="bibr">5</ref> Initially, we created all possible pairings over <ref type="bibr" target="#b31">McRae et al.'s (2005)</ref> nouns and computed their semantic relatedness using Patwardhan and Peder- sen (2006)'s WordNet-based measure. We opted for this specific measure as it achieves high corre- lation with human ratings and has a high coverage on our nouns. Next, for each word we randomly selected 30 pairs under the assumption that they are representative of the full variation of semantic similarity. This resulted in 7,576 word pairs for which we obtained similarity ratings using Ama- zon Mechanical Turk (AMT). Participants were asked to rate a pair on two dimensions, visual and semantic similarity using a Likert scale of 1 (highly dissimilar) to 5 (highly similar). Each task consisted of 32 pairs covering examples of weak to very strong semantic relatedness. Two con- trol pairs from <ref type="bibr" target="#b34">Miller and Charles (1991)</ref> were in- cluded in each task to potentially help identify and eliminate data from participants who assigned ran- dom scores. Examples of the stimuli and mean ratings are shown in <ref type="table">Table 2</ref>.</p><p>The elicitation study comprised overall 255 tasks, each task was completed by five volun- teers. The similarity data was post-processed so as to identify and remove outliers. We consid- ered an outlier to be any individual whose mean pairwise correlation fell outside two standard de- viations from the mean correlation. 11.5% of the annotations were detected as outliers and re- moved. After outlier removal, we further ex- amined how well the participants agreed in their similarity judgments. We measured inter-subject agreement as the average pairwise correlation co- efficient (Spearman's ρ) between the ratings of all annotators for each task. For semantic similarity, the mean correlation was 0.76 <ref type="bibr">(Min =0.34</ref> <ref type="table">Table 2</ref>: Mean semantic and visual similarity rat- ings for the <ref type="bibr" target="#b31">McRae et al. (2005)</ref> nouns using a scale of 1 (highly dissimilar) to 5 (highly similar).</p><note type="other">, Max Word Pairs Semantic Visual football-pillow 1.0 1.2 dagger-pencil 1.0 2.2 motorcycle-wheel 2.4 1.8 orange-pumpkin 2.5 3.0 cherry-pineapple 3.6 1.2 pickle-zucchini 3.6 4.0 canary-owl 4.0 2.4 jeans-sweater 4.5 2.2 pan-pot 4.7 4.0 hornet-wasp 4.8 4.8 airplane-jet 5.0 5.0</note><p>=0.97, StD =0.11) and for visual similarity 0.63 (Min =0.19, Max =0.90, SD =0.14). These re- sults indicate that the participants found the task relatively straightforward and produced similarity ratings with a reasonable level of consistency. For comparison, <ref type="bibr" target="#b37">Patwardhan and Pedersen's (2006)</ref> measure achieved a coefficient of 0.56 on the dataset for semantic similarity and 0.48 for vi- sual similarity. The correlation between the aver- age ratings of the AMT annotators and the Miller and Charles (1991) dataset was ρ = 0.91. In our experiments (see Section 5), we correlate model- based cosine similarities with mean similarity rat- ings (again using Spearman's ρ).</p><p>Categorization The task of categorization (i.e., grouping objects into meaningful categories) is a classic problem in the field of cognitive science, central to perception, learning, and the use of language. We evaluated model output against a gold standard set of categories created by <ref type="bibr" target="#b18">Fountain and Lapata (2010)</ref>. The dataset contains a classification, produced by human participants, of McRae et al.'s (2005) nouns into (possibly multiple) semantic categories (40 in total). <ref type="bibr">6</ref> To obtain a clustering of nouns, we used Chi- nese Whispers <ref type="bibr" target="#b6">(Biemann, 2006</ref>), a randomized graph-clustering algorithm. In the categorization setting, Chinese Whispers (CW) produces a hard clustering over a weighted graph whose nodes cor- <ref type="bibr">6</ref> The dataset can be downloaded from http: //homepages.inf.ed.ac.uk/s0897549/data/.</p><p>respond to words and edges to cosine similarity scores between vectors representing their mean- ing. CW is a non-parametric model, it induces the number of clusters (i.e., categories) from the data as well as which nouns belong to these clusters. In our experiments, we initialized Chinese Whis- pers with different graphs resulting from different vector-based representations of the <ref type="bibr" target="#b31">McRae et al. (2005)</ref> nouns. We also transformed the dataset into hard categorizations by assigning each noun to its most typical category as extrapolated from human typicality ratings (for details see <ref type="bibr" target="#b18">Fountain and Lapata, 2010)</ref>. CW can optionally ap- ply a minimum weight threshold which we opti- mized using the categorization dataset from <ref type="bibr" target="#b2">Baroni et al. (2010)</ref>. The latter contains a classifica- tion of 82 <ref type="bibr" target="#b31">McRae et al. (2005)</ref> nouns into 10 cate- gories. These nouns were excluded from the gold standard <ref type="bibr" target="#b18">(Fountain and Lapata, 2010</ref>) in our final evaluation.</p><p>We evaluated the clusters produced by CW us- ing the F-score measure introduced in the Se- mEval 2007 task <ref type="bibr" target="#b0">(Agirre and Soroa, 2007)</ref>; it is the harmonic mean of precision and recall defined as the number of correct members of a cluster di- vided by the number of items in the cluster and the number of items in the gold-standard class, re- spectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with Other Models</head><p>Throughout our experiments we compare a bi- modal stacked autoencoder against unimodal au- toencoders based solely on textual and visual in- put (left-and right-hand sides in <ref type="figure">Figure 1</ref>, respec- tively). We also compare our model against two approaches that differ in their fusion mechanisms. The first one is based on kernelized canonical cor- relation (kCCA, <ref type="bibr" target="#b21">Hardoon et al., 2004</ref>) with a lin- ear kernel which was the best performing model in <ref type="bibr" target="#b43">Silberer et al. (2013)</ref>. The second one emulates <ref type="bibr">Bruni et al.'s (2014)</ref> fusion mechanism. Specifi- cally, we concatenate the textual and visual vec- tors and project them onto a lower dimensional la- tent space using SVD <ref type="bibr" target="#b19">(Golub and Reinsch, 1970)</ref>. All these models run on the same datasets/items and are given input identical to our model, namely attribute-based textual and visual representations.</p><p>We furthermore report results obtained with <ref type="bibr">Bruni et al.'s (2014)</ref> bimodal distributional model, which employs SVD to integrate co-occurrence- based textual representations with visual repre- <ref type="table">Semantic   Visual  Models  T  V T+V  T  V T+V  McRae</ref> 0.71 0.49 0.68 0.58 0.52 0.62 Attributes 0.58 0.61 0.68 0.46 0.56 0.58 SAE 0.65 0.60 0.70 0.52 0.60 0.64 SVD -- 0.67 --0.57 kCCA -- 0.57 --0.55 Bruni -- 0.52 --0.46 RNN-640 0.41 - -0.34 - - sentations constructed from low-level image fea- tures. In their model, the textual modality is represented by the 30K-dimensional vectors ex- tracted from UKWaC and WaCkypedia. <ref type="bibr">7</ref> The visual modality is represented by bag-of-visual- words histograms built on the basis of clustered SIFT features <ref type="bibr" target="#b29">(Lowe, 2004)</ref>. We rebuilt their model on the ESP image dataset <ref type="bibr" target="#b53">(von Ahn and Dabbish, 2004</ref>) using Bruni et al.'s (2013) publicly available system. Finally, we also compare to the word embed- dings obtained using <ref type="bibr" target="#b32">Mikolov et al.'s (2011)</ref> re- current neural network based language model. These were pre-trained on Broadcast news data (400M words) using the word2vec tool. <ref type="bibr">8</ref> We re- port results with the 640-dimensional embeddings as they performed best. <ref type="table" target="#tab_1">Table 3</ref> presents our results on the word simi- larity task. We report correlation coefficients of model predictions against similarity ratings. As an indicator to how well automatically extracted at- tributes can approach the performance of clean hu- man generated attributes, we also report results of a distributional model induced from <ref type="bibr" target="#b31">McRae et al.'s (2005)</ref> norms (see the row labeled McRae in the table). Each noun is represented as a vector with dimensions corresponding to attributes elicited by participants of the norming study. Vector compo- nents are set to the (normalized) frequency with which participants generated the corresponding at- tribute. We show results for three models, using all attributes except those classified as visual (T), only <ref type="bibr">7</ref> We thank Elia Bruni for providing us with their data. 8 Available from http://www.rnnlm.org/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># Pair # Pair 1 pliers-tongs</head><p>11 cello-violin 2 cathedral-church 12 cottage-house 3 cathedral-chapel 13 horse-pony 4 pistol-revolver 14 gun-rifle 5 chapel-church 15 cedar-oak 6 airplane-helicopter 16 bull-ox 7 dagger-sword 17 dress-gown 8 pistol-rifle 18 bolts-screws 9 cloak-robe 19 salmon-trout 10 nylons-trousers 20 oven-stove visual attributes (V), and all available attributes (V+T). <ref type="bibr">9</ref> As baselines, we also report the perfor- mance of a model based solely on textual attributes (which we obtain from Strudel), visual attributes (obtained from our classifiers), and their concate- nation (see row Attributes in <ref type="table" target="#tab_1">Table 3</ref>, and columns T, V, and T+V, respectively). The automatically obtained textual and visual attribute vectors serve as input to SVD, kCCA, and our stacked autoen- coder (SAE). The third row in the table presents three variants of our model trained on textual and visual attributes only (T and V, respectively) and on both modalities jointly (T+V).</p><p>Recall that participants were asked to provide ratings on two dimensions, namely semantic and visual similarity. We would expect the textual modality to be more dominant when modeling se- mantic similarity and conversely the perceptual modality to be stronger with respect to visual sim- ilarity. This is borne out in our unimodal SAEs. The textual SAE correlates better with seman- tic similarity judgments (ρ = 0.65) than its vi- sual equivalent (ρ = 0.60). And the visual SAE correlates better with visual similarity judgments (ρ = 0.60) compared to the textual SAE (ρ = 0.52). Interestingly, the bimodal SAE is better than the unimodal variants on both types of similarity judg- ments, semantic and visual. This suggests that both modalities contribute complementary infor- mation and that the SAE model is able to extract a shared representation which improves general- ization performance across tasks by learning them Models  <ref type="table">Table 5</ref>: F-score results on concept categorization.</p><p>jointly. The bimodal autoencoder (SAE, T+V) outperforms all other bimodal models on both sim- ilarity tasks. It yields a correlation coefficient of ρ = 0.70 on semantic similarity and ρ = 0.64 on visual similarity. Human agreement on the former task is 0.76 and 0.63 on the latter. <ref type="table" target="#tab_2">Table 4</ref> shows examples of word pairs with highest semantic and visual similarity according to the SAE model. We also observe that simply concatenating textual and visual attributes (Attributes, T+V) performs competitively with SVD and better than kCCA. This indicates that the attribute-based representation is a powerful predictor on its own. Interestingly, both <ref type="bibr" target="#b9">Bruni et al. (2013) and</ref><ref type="bibr" target="#b32">Mikolov et al. (2011)</ref> which do not make use of attributes are out-performed by all other attribute-based sys- tems (see columns T and T+V in <ref type="table" target="#tab_1">Table 3</ref>).</p><p>Our results on the categorization task are given in <ref type="table">Table 5</ref>. In this task, simple concatenation of vi- sual and textual attributes does not yield improved performance over the individual modalities (see row Attributes in <ref type="table">Table 5</ref>). In contrast, all bimodal models (SVD, kCCA, and SAE) are better than their unimodal equivalents and RNN-640. The SAE outperforms both kCCA and SVD by a large margin delivering clustering performance similar to the <ref type="bibr" target="#b31">McRae et al.'s (2005)</ref> norms. <ref type="table">Table 6</ref> shows examples of clusters produced by Chinese Whis- pers when using vector representations provided by the SAE model.</p><p>In sum, our experiments show that the bi- modal SAE model delivers superior performance across the board when compared against competi- tive baselines and related models. It is interesting to note that the unimodal SAEs are in most cases better than the raw textual or visual attributes. This indicates that higher level embeddings may be beneficial to NLP tasks in general, not only to those requiring multimodal information.  <ref type="table">Table 6</ref>: Examples of clusters produced by CW using the representations obtained from the SAE model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we presented a model that uses stacked autoencoders to learn grounded meaning representations by simultaneously combining tex- tual and visual modalities. The two modalities are encoded as vectors of natural language attributes and are obtained automatically from decoupled text and image data. To the best of our knowl- edge, our model is novel in its use of attribute- based input in a deep neural network. Experimen- tal results in two tasks, namely simulation of word similarity and word categorization, show that our model outperforms competitive baselines and re- lated models trained on the same attribute-based input. Our evaluation also reveals that the bimodal models are superior to their unimodal counterparts and that higher-level unimodal representations are better than the raw input. In the future, we would like to apply our model to other tasks, such as im- age and text retrieval <ref type="bibr" target="#b24">(Hodosh et al., 2013;</ref><ref type="bibr" target="#b46">Socher et al., 2013b</ref>), zero-shot learning <ref type="bibr" target="#b45">(Socher et al., 2013a)</ref>, and word learning ( <ref type="bibr" target="#b56">Yu and Ballard, 2007</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Correlation of model predictions against 
similarity ratings for McRae et al. (2005) noun 
pairs (using Spearman's ρ). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Word pairs with highest semantic and vi-
sual similarity according to SAE model. Pairs are 
ranked from highest to lowest similarity. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>STICK -LIKE UTENSILS baton, ladle, peg, spatula, spoon RELIGIOUS BUILDINGS cathedral, chapel, church WIND INSTRUMENTS clarinet, flute, saxophone, trom- bone, trumpet, tuba AXES axe, hatchet, machete, toma- hawk</head><label>STICK</label><figDesc></figDesc><table>FURNITURE W/ LEGS 

bed, bench, chair, couch, desk, 
rocker, sofa, stool, table 

FURNITURE W/O LEGS 

bookcase, bureau, cabinet, 
closet, cupboard, dishwasher, 
dresser 

LIGHTINGS 

candle, 
chandelier, 
lamp, 
lantern 

ENTRY POINTS 

door, elevator, gate 

UNGULATES 

bison, buffalo, bull, calf, camel, 
cow, donkey, elephant, goat, 
horse, lamb, ox, pig, pony, 
sheep 

BIRDS 

crow, dove, eagle, falcon, hawk, 
ostrich, owl, penguin, pigeon, 
raven, stork, vulture, wood-
pecker 

</table></figure>

			<note place="foot" n="1"> In a one-hot vector, the element corresponding to the object label is one and the others are zero.</note>

			<note place="foot" n="2"> The corpus is downloadable from http://wacky. sslmit.unibo.it/doku.php?id=corpora.</note>

			<note place="foot" n="3"> http://w3.usf.edu/Freeassociation. 4 435 word pairs constitute the overlap between Nelson et al.&apos;s norms (1998) and McRae et al.&apos;s (2005) nouns.</note>

			<note place="foot" n="5"> Available from http://homepages.inf.ed.ac.uk/ mlap/index.php?page=resources.</note>

			<note place="foot" n="9"> Classification of attributes into categories is provided by McRae et al. (2005) in their dataset.</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SemEval2007 Task 02: Evaluating Word Sense Induction and Discrimination Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Soroa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Workshop on Semantic Evaluations</title>
		<meeting>the Fourth International Workshop on Semantic Evaluations<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Integrating Experiential and Distributional Data to Learn Semantic Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vigliocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="463" to="498" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Strudel: A Corpus-Based Semantic Model Based on Properties and Types</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poesio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="222" to="254" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Grounded Cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">W</forename><surname>Barsalou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Psychology</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="617" to="845" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Greedy Layer-Wise Training of Deep Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 19</title>
		<editor>Bernhard Schölkopf, John Platt, and Thomas Hoffman</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning Deep Architectures for AI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="127" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Chinese Whispers-an Efficient Graph Clustering Algorithm and its Application to Natural Language Processing Problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TextGraphs: the 1st Workshop on Graph Based Methods for Natural Language Processing</title>
		<meeting>TextGraphs: the 1st Workshop on Graph Based Methods for Natural Language Processing<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="73" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Latent Dirichlet Allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distributional Semantics in Technicolor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Boleda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="136" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Vsem: An open library for visual semantics representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Bordignon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sergienya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="187" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multimodal distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res. (JAIR)</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distributional Semantics with Eyes: Using Image Analysis to Improve Computational Representations of Word Meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM International Conference on Multimedia</title>
		<meeting>the 20th ACM International Conference on Multimedia<address><addrLine>Nara, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1219" to="1228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Natural Language Processing (almost) from Scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition<address><addrLine>Miami, Florida</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Describing Objects by their Attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition<address><addrLine>Miami Beach, Florida</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1778" to="1785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Constructing Hierarchical Image-tags Bimodal Representations for Word Tags Alternative Choice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangxiang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruifan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICML 2013 Workshop on Challenges in Representation Learning</title>
		<meeting>the ICML 2013 Workshop on Challenges in Representation Learning<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Visual Information in Semantic Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Los Angeles, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Placing Search in Context: The Concept Revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="116" to="131" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Meaning Representation in Natural Language Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Fountain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st Annual Conference of the Cognitive Science Society</title>
		<meeting>the 31st Annual Conference of the Cognitive Science Society<address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1916" to="1921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Singular Value Decomposition and Least Squares Solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gene</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Reinsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numerische Mathematik</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="403" to="420" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Topics in Semantic Representation. Psychological Review</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="211" to="244" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Canonical Correlation Analysis: An Overview with Application to Learning Methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Hardoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Szedmak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Shawetaylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2639" to="2664" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distributional Structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zellig</forename><surname>Harris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Papers in Structural and Transformational Linguistics</title>
		<imprint>
			<date type="published" when="1970" />
			<biblScope unit="page" from="775" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reducing the Dimensionality of Data with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>And Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="853" to="899" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Audiovisual Deep Learning for Noise Robust Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Acoustics, Speech, and Signal Processing</title>
		<meeting>the 38th International Conference on Acoustics, Speech, and Signal Processing<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="7596" to="7599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Generating Query Substititions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Madani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Greiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Conference on the World-Wide Web</title>
		<meeting>the 15th International Conference on the World-Wide Web<address><addrLine>Edinburgh, Scotland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="387" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Object Perception and Object Naming in Early Development</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Landau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="19" to="24" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A Solution to Plato&apos;s Problem: the Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="211" to="240" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Distinctive Image Features from Scale-invariant Keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Introduction to Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Semantic Feature Production Norms for a Large Set of Living and Nonliving Things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcrae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Cree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Seidenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mcnorgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="547" to="559" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Extensions of Recurrent Neural Network Language Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kombrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cernock´ycernock´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<meeting>the 2011 IEEE International Conference on Acoustics, Speech, and Signal Processing<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="5528" to="5531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Linguistic Regularities in Continuous Space Word Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Atlanta</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Atlanta<address><addrLine>Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Contextual Correlates of Semantic Similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><forename type="middle">G</forename><surname>Charles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language and Cognitive Processes</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">The University of South Florida Word Association, Rhyme, and Word Fragment Norms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Mcevoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Schreiber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multimodal Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhan</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning</title>
		<meeting>the 28th International Conference on Machine Learning<address><addrLine>Bellevue, Washington</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Using WordNet-based Context Vectors to Estimate the Semantic Relatedness of Concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Patwardhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EACL 2006 Workshop on Making Sense of Sense: Bringing Computational Linguistics and Psycholinguistics Together</title>
		<meeting>the EACL 2006 Workshop on Making Sense of Sense: Bringing Computational Linguistics and Psycholinguistics Together<address><addrLine>Trento, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Semi-supervised Learning of Compact Document Representations with Deep Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">&amp;apos;</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Aurelio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szummer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning</title>
		<meeting>the 25th International Conference on Machine Learning<address><addrLine>Helsinki, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="792" to="799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">The Human Semantic Potential</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Regier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, Massachusetts</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">A Multimodal LDA Model integrating</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Schulte Im Walde</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Cognitive and Visual Modalities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Textual</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="1146" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Machine Learning in Automated Text Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Models of Semantic Representation with Visual Attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Silberer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="572" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Grounded Models of Semantic Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carina</forename><surname>Silberer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1423" to="1433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Zero-shot learning through crossmodal transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ganjoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="935" to="943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Grounded Compositional Semantics for Finding and Describing Images with Sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NIPS Deep Learning Workshop</title>
		<meeting>the NIPS Deep Learning Workshop</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Edinburgh, Scotland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Multimodal Learning with Deep Boltzmann Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="2231" to="2239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Combining Feature Norms and Text Data with Topic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Psychologica</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="234" to="342" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A New Set of Norms for Semantic Relatedness Measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Szumlanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Sims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="890" to="895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">From Frequency to Meaning: Vector Space Models of Semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">D</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Labeling Images with a Computer Game</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Von Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Dabbish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="319" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Online Multimodal Deep Similarity Learning with Application to Image Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pengcheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dayong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM International Conference on Multimedia</title>
		<meeting>the 21st ACM International Conference on Multimedia<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="153" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Question Answering Using Enhanced Lexical Semantic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrzej</forename><surname>Pastusiak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1744" to="1753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A Unified Model of Early Word Learning Integrating Statistical and Social Cues</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2149" to="2165" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
