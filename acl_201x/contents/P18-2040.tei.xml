<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:55+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Topic Quality by Promoting Named Entities in Topic Modeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsiaryna</forename><surname>Krasnashchok</surname></persName>
							<email>katherine.krasnoschok@euranova.eu</email>
							<affiliation key="aff0">
								<orgName type="institution">EURA NOVA Rue Emile Francqui</orgName>
								<address>
									<postCode>4 1435</postCode>
									<settlement>Mont-Saint-Guibert</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Jouili</surname></persName>
							<email>salim.jouili@euranova.eu</email>
							<affiliation key="aff1">
								<orgName type="institution">EURA NOVA</orgName>
								<address>
									<addrLine>Rue Emile Francqui, 4 1435 Mont-Saint-Guibert</addrLine>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Topic Quality by Promoting Named Entities in Topic Modeling</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="247" to="253"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>247</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>News-related content has been extensively studied in both topic modeling research and named entity recognition. However, expressive power of named entities and their potential for improving the quality of discovered topics has not received much attention. In this paper we use named entities as domain-specific terms for news-centric content and present a new weight-ing model for Latent Dirichlet Allocation. Our experimental results indicate that involving more named entities in topic de-scriptors positively influences the overall quality of topics, improving their inter-pretability, specificity and diversity.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>News-centric content conveys information about events, individuals and other entities. Analysis of news-related documents includes identifying hid- den features for classifying them or summarizing the content. Topic modeling is the standard tech- nique for such purposes, and Latent Dirichlet Al- location (LDA) ( <ref type="bibr" target="#b1">Blei et al., 2003</ref>) is the most used algorithm, which models the documents as distri- bution over topics and topics as distribution over words. A good topic model is characterized by its coherence: any coherent topic should contain related words belonging to the same concept. A good topic must also be distinctive enough to in- clude domain-specific content. For news-related texts domain-specific content can be represented by named entities (NE), describing facts, events and people involved in news and discussions. It explains the need to include named entities in topic modeling process.</p><p>The main contribution of this work is improving topic quality with LDA by increasing the impor- tance of named entities in the model. The idea is to adapt the topic model to include more domain- specific terms (NE) in the topic descriptors. We designed our model to be flexible, in order to be used in different variations of LDA. We ultimately employ a term-weighting approach for the LDA input. Our results show that: i) named entities can serve as favorable candidates for high-quality topic descriptors, and ii) weighting model based on pseudo term frequencies is able to improve overall topic quality without the need to interfere with LDA's generative process, which makes it adaptable to other LDA variations.</p><p>The paper is organized in the following manner: in Section 2 we present the related work; Section 3 describes the proposed solution and is followed by Section 4, where the details of evaluation process and results are outlined. We finish with Section 5, concluding the results and next steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>This section describes the related work in the area of topic modeling, specifically LDA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Topic Modeling and Named Entities</head><p>Several works explored the relation between LDA and named entities in recent years. The most fa- mous model is <ref type="bibr">CorrLDA2 (Newman et al., 2006</ref>). It introduces two types of topics, general and en- tity, and represents word topics as a mixture of entity topics. <ref type="bibr" target="#b6">Hu et al. (2013)</ref> reverses the con- cept, assuming that entities are critical for news- centric content. Their entity-centered topic model (ECTM) designs entity topics as a mixture of word topics and shows better results in entity prediction than CorrLDA2 ( <ref type="bibr" target="#b6">Hu et al., 2013</ref>). Both models, however, introduce significant changes to the LDA algorithm. In this paper we strive to incorporate named entities into LDA in a natural way, with- out affecting the generative algorithm, to keep it flexible and adaptable to any LDA variations. <ref type="bibr" target="#b7">Lau et al. (2013)</ref> study the impact of colloca- tions on topic modeling and work with the in- put of LDA by replacing unigrams with colloca- tions. Adding multiword named entities, as a spe- cial type of collocations, enhanced the topic model for the tested dataset ( <ref type="bibr" target="#b7">Lau et al., 2013)</ref>. Our work follows similar tokenization process, but goes fur- ther in improving the topic model by promoting named entities in it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Topic Modeling and Term Weighting</head><p>Traditionally, the input of LDA is a document- term matrix of term frequencies (TF), according to the bag-of-words model (BoW). However, Wil- son and Chew <ref type="bibr">(2010)</ref> showed that point-wise mu- tual information (PMI) term weighting model can be successfully applied to eliminate stop words from topic descriptors. More weighting schemes were evaluated by <ref type="bibr" target="#b19">Truica et al. (2016)</ref> and showed promising results for clustering accuracy. There- fore, term weighting approach in LDA can be ben- eficial for certain tasks. In this paper we intro- duce unnormalized TF-based weighting scheme using pseudo frequency as a way of increasing the weight of a term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed model</head><p>LDA model has been criticized for favoring highly frequent, general words in topic descriptors <ref type="bibr" target="#b14">(O'Callaghan et al., 2015</ref>). This problem can be partly solved by eliminating domain-specific stop- words from the corpus. On the other hand, instead of narrowing the corpus, it may be more efficient to promote domain-specific important words, es- pecially if such words can be identified automat- ically, like named entities. In this paper we deal with the online Variational Bayes version of the LDA algorithm from <ref type="bibr" target="#b5">Hoffman et al. (2010)</ref>, as alternative to collapsed Gibbs sampling, used by <ref type="bibr" target="#b21">Wilson and Chew (2010)</ref> and <ref type="bibr" target="#b19">Truica et al. (2016)</ref> to incorporate weights into the LDA model. In <ref type="bibr" target="#b5">Hoffman et al. (2010)</ref> the authors demonstrate that the objective of the optimization relies only on the counts of terms in documents, and therefore docu- ments can be summarized by their TF values. Our proposed model takes the TF scores as initial term weights (unnormalized). To increase the weight of a named entity we add a pseudo-frequency to its TF without changing the weights of other terms. This strengthens the chances of NE to appear in a topic descriptor, even if originally it was not mentioned often in the corpus. There are multi- ple ways of increasing the weights, e.g. we can promote all NE in the same proportion, or set their weights separately for each document in the cor- pus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Independent Named Entity Promoting</head><p>NE Independent model assumes that all named en- tities in the corpus are α times more important than their initial weights (TF), i.e. they may not be the most important terms in the corpus, but they should weigh α times more than they do now. Therefore, for each column m w of document-term matrix M , we apply scalar multiplication:</p><formula xml:id="formula_0">m w = � α * m w if w is NE m w otherwise (1)</formula><p>By varying α, we can set the importance of named entities in the corpus and impact the out- come of topic modeling. The value need not be an integer, since typical LDA implementation can deal with any numbers. In Section 4 we provide results for several tested values of α parameter and discuss our findings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Document Dependent Named Entity Promoting</head><p>While we want the topics produced by LDA to include more named entities as domain-specific words, we may assume that NE, in fact, should be the most important, i.e. the most frequent, terms in each document. In order to set the weights ac- cordingly, the maximum term-frequency per doc- ument is calculated and added to each named en- tity's weight in each document:</p><formula xml:id="formula_1">m dw = � m dw + max w m dw if w is NE m dw otherwise (2)</formula><p>This weighting scheme obliges named entities to be the "heaviest" terms in each document. At the same time, we do not change the weight of other frequent terms, so eventually they still have a high probability to make the top terms list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>We designed a series of tests to evaluate our proposed model: a) Baseline Unigram: basic model on the corpus consisting of single tokens (no named entities involved); b) Baseline NE: ba- sic model on the corpus with named entities (the strategy of injecting NE in all tests is replacement instead of supplementation, as suggested by <ref type="bibr" target="#b7">Lau et al., 2013</ref>); c) NE Independent: independent named entity promoting model described in Sec- tion 3.1; and d) NE Document Dependent: doc- ument dependent named entity promoting model described in Section 3.2. We evaluate the tests us- ing the topic quality measures presented below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset And Preprocessing</head><p>Our test corpora consists of news-related publicly- available datasets: 1) 20 Newsgroups 1 : widely studied by NLP research community dataset <ref type="bibr" target="#b0">(Aletras and Stevenson, 2013;</ref><ref type="bibr" target="#b19">Truica et al., 2016;</ref><ref type="bibr" target="#b20">Wallach et al., 2009;</ref><ref type="bibr" target="#b15">Röder et al., 2015;</ref><ref type="bibr" target="#b6">Hu et al., 2013)</ref>. Contains 18846 documents with messages discussing news, people, events and other entities. 2) Reuters-2013: a set of 14595 news articles from Reuters for year 2013, obtained from Financial News Dataset 2 , first compiled and used in ( <ref type="bibr" target="#b4">Ding et al., 2014</ref>). The documents in Reuters-2013 are generally longer than in 20 Newsgroups. For NE recognition we used NeuroNER 3 , a tool designed by <ref type="bibr" target="#b3">Dernoncourt et al. (2016</ref><ref type="bibr" target="#b2">Dernoncourt et al. ( , 2017</ref>, trained on CONLL2003 dataset and recognizing four types of NE: person, location, organization and miscel- laneous. The further preprocessing pipeline con- sists of classic steps used in topic modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Topic Coherence</head><p>The term "topic coherence" covers a set of mea- sures describing the quality of the topics regard- ing interpretability by a human. Most widely used measures are based on PMI (or NPMI, nor- malized) and log conditional probability, both of which rely on the co-occurrence of terms ( <ref type="bibr" target="#b7">Lau et al., 2013</ref><ref type="bibr" target="#b8">Lau et al., , 2014</ref><ref type="bibr" target="#b14">O'Callaghan et al., 2015;</ref><ref type="bibr" target="#b0">Aletras and Stevenson, 2013;</ref><ref type="bibr" target="#b11">Newman et al., 2010;</ref><ref type="bibr" target="#b9">Mimno et al., 2011;</ref><ref type="bibr" target="#b13">Nikolenko, 2016;</ref><ref type="bibr" target="#b12">Nguyen et al., 2015;</ref><ref type="bibr" target="#b17">Syed and Spruit, 2017)</ref>. Recently a study by <ref type="bibr">Röder et al. (2015)</ref> put all known co- herence measures into single framework, assessed their correlation with human ratings and discov- ered the best performing measure -previously un- known C v , based on cosine similarity of word vec- tors over a sliding window. We inferred the defini-tion from <ref type="bibr">Röder et al. (2015)</ref>:</p><formula xml:id="formula_2">C v = 1 N � t=1...N 1 Nt � i=1...Nt s cos (� v N P M I (w i ), � v N P M I (W t )) (3)</formula><p>where N is the number of topics, W t is the set of top N t terms in topic t, the vectors are defined as:</p><formula xml:id="formula_3">� v N P M I (w i ) = � N P M I(w i , w j ) � j∈Wt (4) � v N P M I (W t ) = � � wi∈Wt N P M I(w i , w j ) � wj ∈Wt<label>(5)</label></formula><p>and the underlying measure is NPMI with proba- bility P sw over a sliding window. C v with sliding window of 110 words <ref type="bibr">(Röder et al., 2015</ref>) is the coherence measure we use in this paper. Majority of studies also use a reference cor- pus like Wikipedia for calculating word frequen- cies and co-occurrences ( <ref type="bibr" target="#b0">Aletras and Stevenson, 2013;</ref><ref type="bibr" target="#b14">O'Callaghan et al., 2015;</ref><ref type="bibr" target="#b8">Lau et al., 2014;</ref><ref type="bibr" target="#b15">Röder et al., 2015;</ref><ref type="bibr" target="#b22">Yang et al., 2017</ref>). In our case the need for reference corpus is particularly significant, since we change natural frequencies of named entities in the corpus, therefore coher- ence will definitely decline if calculated on orig- inal data. For the tests we have preprocessed the dump of English Wikipedia from 2014/06/15 with the same pipeline as used for the test corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Generality Measures</head><p>Coherence measures tend to favor topics with gen- eral highly frequent terms. As a result we end up with well understandable but quite generic topics. A good topic should also be specific enough to distinguish documents <ref type="bibr" target="#b14">(O'Callaghan et al., 2015)</ref>. Moreover, averaging the coherences of all topics may produce very good coherence for a model with many repeating words across topics. For cov- ering these aspects of the topic quality we adopt two other measures.</p><p>Exclusivity: Represents the degree of overlap between topics, based on the appearance of terms in multiple descriptors <ref type="bibr" target="#b14">(O'Callaghan et al., 2015</ref>). We define exclusivity as |Wu| |W | , where |W u | is the number of unique terms and |W | is the total num- ber of terms in topic descriptors.</p><p>Lift: Generally used for reranking the terms in descriptors <ref type="bibr" target="#b18">(Taddy, 2012;</ref><ref type="bibr" target="#b16">Sievert and Shirley, 2014)</ref>, lift is employed here as a topic quality met- ric. It is defined as β ti b i</p><p>, where β ti is the weight of word i in topic t and b i is the probability of Topics Test <ref type="bibr">20 Newsgroups Reuters-2013 C v</ref> Lift Excl. C v Lift Excl. 20 Baseline Unigram 0,534 3,390 0,788 0,539 3,891 0,610 Baseline NE 0,503 3,273 0,767 0,559 4,059 0,598 NE Independent (x1,3) 0,494 3,394 0,755 0,551 4,209 0,563 NE Independent (x1,5) 0,527 3,464 0,770 0,552 4,308 0,618 NE Independent (x2) 0,525 3,756 0,797 0,548 4,449 0,640 NE Independent (x2,5) 0,539 3,779 0,765 0,550 4,661 0,635 NE Independent (x5) 0,543 5,071 0,898 0,517 5,701 0,708 NE Independent (x10) 0,486 6,416 0,950 0,511 6,560 0,773 NE Doc. Dependent 0,543 4,600 0,780 0,566 5,749 0,625 50 Baseline Unigram 0,492 2,882 0,511 0,514 3,977 0,427 Baseline NE 0,467 2,704 0,469 0,534 4,064 0,402 NE Independent (x1,3) 0,476 2,825 0,487 0,538 4,291 0,406 NE Independent (x1,5) 0,479 2,987 0,497 0,527 4,370 0,423 NE Independent (x2) 0,471 3,394 0,533 0,510 4,684 0,459 NE Independent (x2,5) 0,467 3,652 0,561 0,499 4,958 0,483 NE Independent (x5) 0,437 5,243 0,702 0,461 5,956 0,564 NE Independent (x10) 0,385 6,693 0,787 0,447 6,943 0,641 NE Doc. Dependent</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0,512 4,951 0,624 0,532 5,452 0,421 100 Baseline Unigram 0,486 2,457 0,325 0,503 3,692 0,286 Baseline NE 0,478 2,248 0,282 0,525 3,775 0,253 NE Independent (x1,3) 0,473 2,391 0,300 0,527 4,041 0,286 NE Independent (x1,5) 0,467 2,499 0,315 0,520 4,126 0,295 NE Independent (x2) 0,463 2,737 0,332 0,508 4,505 0,329 NE Independent (x2,5) 0,453 3,108 0,374 0,491 4,705 0,339 NE Independent (x5)</head><p>0,416 5,079 0,537 0,455 5,840 0,432 NE Independent (x10) 0,394 6,622 0,614 0,444 6,747 0,498 NE Doc. Dependent 0,478 4,310 0,442 0,509 5,030 0,266 <ref type="table" target="#tab_0">Table 1</ref>: Topic quality results on the corpora word i in the reference corpus. The overall model measure is the average of the log-lift of descriptor terms and shows the degree of presence of non- general words in topics.  <ref type="bibr">4</ref> with N = {20, 50, 100} topics and top 10 words used for the measures. Firstly, we can observe one common outcome: NE Independent (x10) model exhibited the best exclusivity and lift values across all tests, which is logical since this model enforced the biggest number of pseudo- frequent words to be in topic descriptors. How- ever, the same model also showed the lowest co- herence in all experiments. This confirms the sec- ondary status of lift and exclusivity: the full per- 4 Tests were run with gensim: https://radimrehurek.com/ gensim/ formance of the model is decided by the combi- nation of all three measures. From the table we can see that for 20 Newsgroups, Baseline Unigram model resulted in better coherence than Baseline NE. Previously <ref type="bibr" target="#b7">Lau et al. (2013)</ref> showed that coherence (NPMI-based) is supposed to improve with NE replacement model. However, the goal of this work goes beyond just including named en- tities into LDA. We want to demonstrate that our weighting model increases the number of NE in topic descriptors, which makes them more under- standable and diverse. For these purposes we use different coherence measure <ref type="bibr">(Röder et al., 2015)</ref>, and include additional NE type -miscellaneous, which was omitted in ( <ref type="bibr" target="#b7">Lau et al., 2013</ref>) though it contains some potentially important named enti- ties. Hence, at the moment we do not compare our results with <ref type="bibr" target="#b7">Lau et al. (2013)</ref>. For each dataset we chose the baseline for comparison depending on Topic Baseline Unigram C v Topic NE Doc. Dependent C v game, good, year, team, player, play, think, get, time, like  <ref type="bibr">Reuters-2013.</ref> In the majority of cases NE Document Depen- dent ended up being the optimal model for both datasets: while it did not perform best in terms of lift or exclusivity, it achieved the best or good enough coherence values, better lift and better or the same exclusivity as baseline models. The ex- ceptions are 20 Newsgropus with 20 topics, where NE Independent (x5) became the optimal model, and Reuters-2013 with 100 topics, where NE In- dependent (x1,3) performed the best for combina- tion of all three measures. The only case where baseline model achieved superior coherence is 20 Newsgroups with N = 100, but we note that NE Document Dependent model came close in terms of coherence while having much better lift and ex- clusivity, therefore it can also be considered opti- mal. In general, NE Independent model showed improvement in coherence up to a certain value of α (different in each case), followed by a de- cline, reaching very low values for NE Indepen- dent (x10). On the other hand, NE Document De- pendent model does not introduce new parame- ters into LDA and manages to achieve best perfor- mance in the majority of settings, thus being more stable and easy to use. <ref type="table" target="#tab_1">Table 2</ref> demonstrates qualitative analysis on the individual topics from 20 Newsgroups, gen- erated by Baseline Unigram, and their semanti- cally closest counterparts from NE Document De- pendent model. As evident from the table, base- line topics describe mostly abstract concepts of "sport", "space" and "gun control". From NE Document Dependent topics we get more specific descriptors, resulting in better coherence (as well as lift/exclusivity). It is worth particularly noting the names of the organizations (in bold), crucial to the corresponding topics, that, despite being uni- grams, only appear in NE Document Dependent model, because they are not met often enough in the test corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Presented results indicate that, firstly, our pro- posed model is capable of improving topic qual- ity by only modifying the TF scores in the in- put of LDA in favor of named entities. This makes it applicable to any LDA-based models relying on the same input. Secondly, we have shown that named entities are well suited to be used as domain-specific terms and produce high- quality topics in news-related texts. Next steps in our research include experimenting with different weights for different categories of named entities, as well as adding new coherence measures, such as word2vec-based one, used by O' <ref type="bibr">Callaghan et al. (2015)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 depicts the results of running the exper- iments</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Comparison of Baseline Unigram and NE Doc. Dependent topics for 20 Newsgroups coherence: Baseline Unigram for 20 Newsgroups, and Baseline NE for</figDesc><table></table></figure>

			<note place="foot" n="1"> http://qwone.com/ ∼ jason/20Newsgroups/ 2 https://github.com/philipperemy/financial-news-dataset 3 https://github.com/Franck-Dernoncourt/NeuroNER</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The elaboration of this scientific paper was sup-ported by the Ministry of Economy, Industry, Re-search, Innovation, IT, Employment and Educa-tion of the Region of Wallonia (Belgium), through the funding of the industrial research project Jeri-cho (convention no. 7717).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Evaluating topic coherence using distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Aletras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Stevenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IWCS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">NeuroNER: an easy-to-use program for named-entity recognition based on neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><forename type="middle">Young</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Szolovits</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. Association for Computational Linguistics</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">De-identification of patient notes with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><forename type="middle">Young</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozlem</forename><surname>Uzuner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Szolovits</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="page">156</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Using structured events to predict stock price movement: An empirical investigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwen</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Online learning for latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David M</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="856" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Incorporating entities in news topic modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linmei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixing</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications in Computer and Information Science</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="139" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On collocations and topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Jey Han Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Machine reading tea leaves: Automatically evaluating topic coherence and topic model quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jey Han</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Optimizing semantic coherence in topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edmund</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Talley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Leenders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on empirical methods in natural language processing</title>
		<meeting>the conference on empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="262" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Statistical entity-topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Chemudugunta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padhraic</forename><surname>Smyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining-KDD06</title>
		<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining-KDD06</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic evaluation of topic coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jey</forename><forename type="middle">Han</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Grieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="100" to="108" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improving topic coherence with latent feature word representations in map estimation for topic modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kairit</forename><surname>Dat Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sirts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Australasian Language Technology Association Workshop</title>
		<meeting>the Australasian Language Technology Association Workshop</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="116" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Topic quality metrics based on distributed word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sergey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nikolenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval-SIGIR16</title>
		<meeting>the 39th International ACM SIGIR conference on Research and Development in Information Retrieval-SIGIR16</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An analysis of the coherence of descriptors in topic modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Derek O&amp;apos;callaghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pádraig</forename><surname>Carthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cunningham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="5645" to="5657" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Exploring the space of topic coherence measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Röder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Both</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hinneburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth ACM International Conference on Web Search and Data Mining-WSDM15</title>
		<meeting>the Eighth ACM International Conference on Web Search and Data Mining-WSDM15</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">LDAvis: A method for visualizing and interpreting topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carson</forename><surname>Sievert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Shirley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces. Association for Computational Linguistics</title>
		<meeting>the Workshop on Interactive Language Learning, Visualization, and Interfaces. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Full-text or abstract? examining topic coherence scores using latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaheen</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Spruit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Data Science and Advanced Analytics (DSAA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On estimation and selection for topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Taddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1184" to="1193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Comparing different term weighting schemas for topic modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florin</forename><surname>Ciprian-Octavian Truica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Radulescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boicea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rethinking lda: Why priors matter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Hanna M Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1973" to="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Term weighting schemes for latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">human language technologies: The 2010 annual conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="465" to="473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adapting topic models using lexical associations with tree priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
