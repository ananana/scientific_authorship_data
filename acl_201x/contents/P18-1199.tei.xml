<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Robust Distant Supervision Relation Extraction via Deep Reinforcement Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengda</forename><surname>Qin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Xu</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
							<email>{william}@cs.ucsb.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Robust Distant Supervision Relation Extraction via Deep Reinforcement Learning</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2137" to="2147"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2137</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Distant supervision has become the standard method for relation extraction. However , even though it is an efficient method, it does not come at no cost-The resulted distantly-supervised training samples are often very noisy. To combat the noise, most of the recent state-of-the-art approaches focus on selecting one-best sentence or calculating soft attention weights over the set of the sentences of one specific entity pair. However, these methods are suboptimal, and the false positive problem is still a key stumbling bottleneck for the performance. We argue that those incorrectly-labeled candidate sentences must be treated with a hard decision, rather than being dealt with soft attention weights. To do this, our paper describes a radical solution-We explore a deep reinforcement learning strategy to generate the false-positive indicator, where we automatically recognize false positives for each relation type without any supervised information. Unlike the removal operation in the previous studies, we redistribute them into the negative examples. The experimental results show that the proposed strategy significantly improves the performance of distant supervision comparing to state-of-the-art systems.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Relation extraction is a core task in informa- tion extraction and natural language understand- ing. The goal of relation extraction is to predict relations for entities in a sentence ( <ref type="bibr">Zelenko et al., 2003;</ref><ref type="bibr" target="#b1">Bunescu and Mooney, 2005;</ref><ref type="bibr" target="#b5">GuoDong et al., 2005</ref>). For example, given a sentence Figure 1: Our deep reinforcement learning frame- work aims at dynamically recognizing false posi- tive samples, and moving them from the positive set to the negative set during distant supervision.</p><p>"Barack Obama is married to Michelle Obama.", a relation classifier aims at predicting the relation of "spouse". In downstream applications, rela- tion extraction is the key module for construct- ing knowledge graphs, and it is a vital compo- nent of many natural language processing applica- tions such as structured search, sentiment analysis, question answering, and summarization.</p><p>A major issue encountered in the early devel- opment of relation extraction algorithms is the data sparsity issue-It is extremely expensive, and almost impossible for human annotators to go through a large corpus of millions of sentences to provide a large amount of labeled training in- stances. Therefore, distant supervision relation ex- traction ( <ref type="bibr" target="#b10">Mintz et al., 2009;</ref><ref type="bibr" target="#b7">Hoffmann et al., 2011;</ref><ref type="bibr">Surdeanu et al., 2012</ref>) becomes popular, because it uses entity pairs from knowledge bases to se- lect a set of noisy instances from unlabeled data. In recent years, neural network approaches ( <ref type="bibr" target="#b12">Zeng et al., 2014</ref><ref type="bibr" target="#b11">Zeng et al., , 2015</ref> have been proposed to train the relation extractor under these noisy conditions. To suppress the noisy( <ref type="bibr">Roth et al., 2013)</ref>, recent stud-ies ( <ref type="bibr" target="#b9">Lin et al., 2016)</ref> have proposed the use of at- tention mechanisms to place soft weights on a set of noisy sentences, and select samples. However, we argue that only selecting one example or based on soft attention weights are not the optimal strat- egy: To improve the robustness, we need a system- atic solution to make use of more instances, while removing false positives and placing them in the right place.</p><p>In this paper, we investigate the possibility of using dynamic selection strategies for robust dis- tant supervision. More specifically, we design a deep reinforcement learning agent, whose goal is to learn to choose whether to remove or remain the distantly supervised candidate instance based on the performance change of the relation classi- fier. Intuitively, our agent would like to remove false positives, and reconstruct a cleaned set of distantly supervised instances to maximize the re- ward based on the classification accuracy. Our proposed method is classifier-independent, and it can be applied to any existing distant supervision model. Empirically, we show that our method has brought consistent performance gains in var- ious deep neural network based models, achieving strong performances on the widely used New York Times dataset ( <ref type="bibr">Riedel et al., 2010)</ref>. Our contribu- tions are three-fold:</p><p>• We propose a novel deep reinforcement learning framework for robust distant super- vision relation extraction.</p><p>• Our method is model-independent, meaning that it could be applied to any state-of-the-art relation extractors.</p><p>• We show that our method can boost the per- formances of recently proposed neural rela- tion extractors.</p><p>In Section 2, we will discuss related works on distant supervision relation extraction. Next, we will describe our robust distant supervision frame- work in Section 3. In Section 4, empirical evalu- ation results are shown. And finally, we conclude in Section 5. <ref type="bibr" target="#b10">Mintz et al. (2009)</ref> is the first study that combines dependency path and feature aggregation for dis- tant supervision. However, this approach would introduce a lot of false positives, as the same en- tity pair might have multiple relations. To alleviate this issue, <ref type="bibr" target="#b7">Hoffmann et al. (2011)</ref> address this is- sue, and propose a model to jointly learn with mul- tiple relations. <ref type="bibr">Surdeanu et al. (2012)</ref> further pro- pose a multi-instance multi-label learning frame- work to improve the performance. Note that these early approaches do not explicitly remove noisy instances, but rather hope that the model would be able to suppress the noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Recently, with the advance of neural network techniques, deep learning methods ( <ref type="bibr" target="#b12">Zeng et al., 2014</ref><ref type="bibr" target="#b11">Zeng et al., , 2015</ref> are introduced, and the hope is to model noisy distant supervision process in the hid- den layers. However, their approach only selects one most plausible instance per entity pair, in- evitably missing out a lot of valuable training in- stances. Recently, <ref type="bibr" target="#b9">Lin et al. (2016)</ref> propose an attention mechanism to select plausible instances from a set of noisy instances. However, we believe that soft attention weight assignment might not be the optimal solution, since the false positives should be completely removed and placed in the negative set. <ref type="bibr" target="#b8">Ji et al. (2017)</ref> combine the external knowledge to rich the representation of entity pair, in which way to improve the accuracy of atten- tion weights. Even though these above-mentioned methods can select high-quality instances, they ig- nore the false positive case: all the sentences of one entity pair belongs to the false positives. In this work, we take a radical approach to solve this problem-We will make use of the distantly la- beled resources as much as possible, while learn- ing a independent false-positive indicator to re- move false positives, and place them in the right place. After our ACL submission, we notice that a contemporaneous study <ref type="bibr" target="#b3">Feng et al. (2018)</ref> also adopts reinforcement learning to learn an instance selector, but their reward is calculated from the prediction probabilities. In contrast, while in our method, the reward is intuitively reflected by the performance change of the relation classifier. Our approach is also complement to most of the ap- proaches above, and can be directly applied on top of any existing relation extraction classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Reinforcement Learning for Distant Supervision</head><p>We introduce a performance-driven, policy-based reinforcement learning method to heuristically recognize false positive samples. Comparing to a prior study that has underutilized the distantly- supervised samples ( <ref type="bibr" target="#b9">Lin et al., 2016)</ref>, we consider an RL agent for robust distant supervision rela- tion extraction. We first describe the definitions of our RL method, including the policy-based agent, external environment, and pre-training strategy. Next, we describe the retraining strategy for our RL agent. The goal of our agent is to deter- mine whether to retain or remove a distantly- supervised sentence, based on the performance change of relation classifier. Finally, we describe the noisy-suppression method, where we teach our policy-based agent to make a redistribution for a cleaner distant supervision training dataset.</p><p>Distant supervision relation extraction is to pre- dict the relation type of entity pair under the automatically-generated training set. However, the issue is that these distantly-supervised sen- tences that mention this entity pair may not ex- press the desired relation type. Therefore, what our RL agent should do is to determine whether the distantly-supervised sentence is a true posi- tive instance for this relation type. For reinforce- ment learning, external environment and RL agent are two necessary components, and a robust agent is trained from the dynamic interaction between these two parts ( <ref type="bibr" target="#b0">Arulkumaran et al., 2017)</ref>. First, the prerequisite of reinforcement learning is that the external environment should be modeled as a Markov decision process (MDP). However, the traditional setting of relation extraction cannot sat- isfy this condition: the input sentences are inde- pendent of each other. In other words, we cannot merely use the information of the sentence being processed as the state. Thus, we add the informa- tion from the early states into the representation of the current state, in which way to model our task as a MDP problem <ref type="bibr" target="#b2">(Fang et al., 2017)</ref>. The other component, RL agent, is parameterized with a pol- icy network π θ (s, a) = p(a|s; θ). The probability distribution of actions A = {a remove , a remain } is calculated by policy network based on state vec- tors. What needs to be noted is that, Deep Q Net- work (DQN) ( <ref type="bibr">Mnih et al., 2013</ref>) is also a widely- used RL method; however, it is not suitable for our case, even if our action space is small. First, we cannot compute the immediate reward for ev- ery operation; In contrast, the accurate reward can only be obtained after finishing processing the whole training dataset. Second, the stochastic pol- icy of the policy network is capable of prevent- ing the agent from getting stuck in an intermedi- ate state. The following subsections detailedly in- troduce the definitions of the fundamental compo- nents in the proposed RL method.</p><p>States In order to satisfy the condition of MDP, the state s includes the information from the cur- rent sentence and the sentences that have been re- moved in early states. The semantic and syntactic information of sentence is represented by a con- tinuous real-valued vector. According to some state-of-the-art supervised relation extraction ap- proaches ( <ref type="bibr" target="#b12">Zeng et al., 2014;</ref><ref type="bibr">Nguyen and Grishman, 2015</ref>), we utilize both word embedding and position embedding to convert sentence into vec- tor. With this sentence vector, the current state is the concatenation of the current sentence vector and the average vector of the removed sentences in early states. We give relatively larger weight for the vector of the current sentence, in which way to magnify the dominating influence of the current sentence information for the decision of action.</p><p>Actions At each step, our agent is required to determine whether the instance is false positive for target relation type. Each relation type has a agent 1 . There are two actions for each agent: whether to remove or retain the current instance from the training set. With the initial distantly- supervised dataset that is blended with incorrectly- labeled instances, we hope that our agent is ca- pable of using the policy network to filter noisy instances; Under this cleaned dataset, distant su- pervision is then expected to achieve better per- formance.</p><p>Rewards As previously mentioned, the intuition of our model is that, when the incorrectly-labeled instances are filtered, the better performance of re- lation classifier will achieve. Therefore, we use the change of performance as the result-driven reward for a series of actions decided by the agent. Com- pared to accuracy, we adopt the F 1 score as the evaluation criterion, since accuracy might not be an indicative metric in a multi-class classification setting where the data distribution could be imbal- anced. Thus, the reward can be formulated as the difference between the adjacent epochs:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RL Agent Train</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation</head><formula xml:id="formula_0">R i = α(F i 1 − F i−1 1 )<label>(1)</label></formula><p>As this equation shows, in step i, our agent is given a positive reward only if F 1 gets improved; oth- erwise, the agent will receive a negative reward. Under this setting, the value of reward is propor- tional to the difference of F 1 , and α is used to con- vert this difference into a rational numeric range. Naturally, the value of the reward is in a contin- uous space, which is more reasonable than a bi- nary reward (−1 and 1), because this setting can reflect the number of wrong-labeled instance that the agent has removed. In order to avoid the ran- domness of F 1 , we use the average F 1 of last five epochs to calculate the reward.</p><p>Policy Network For each input sentence, our policy network is to determine whether it ex- presses the target relation type and then make re- moval action if it is irrelevant to the target rela- tion type. Thus, it is analogous to a binary re- lation classifier. CNN is commonly used to con- struct relation classification system ( <ref type="bibr">Santos et al., 2015;</ref><ref type="bibr">Xu et al., 2015;</ref><ref type="bibr">Shen and Huang, 2016)</ref>, so we adopt a simple CNN with window size c w and kernel size c k , to model policy network π(s; θ). The reason why we do not choice the vari- ants of CNN ( <ref type="bibr" target="#b11">Zeng et al., 2015;</ref><ref type="bibr" target="#b9">Lin et al., 2016)</ref> that are well-designed for distant supervision is that these two models belong to bag-level mod- els (dealing with a bag of sentences simultane- ously) and deal with the multi-classification prob- lem; We just need a model to do binary sentence- level classification. Naturally, the simpler network is adopted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training Policy-based Agent</head><p>Unlike the goal of distant supervision relation ex- traction, our agent is to determine whether an an- notated sentence expresses the target relation type rather than predict the relationship of entity pair, so sentences are treated independently despite be- longing to the same entity pair. In distant su- pervision training dataset, one relation type con- tains several thousands or ten thousands sentences; moreover, reward R can only be calculated after processing the whole positive set of this relation type. If we randomly initialize the parameters of policy network and train this network by trial and errors, it will waste a lot of time and be inclined to poor convergence properties. In order to overcome this problem, we adopt a supervised learning pro- cedure to pre-train our policy network, in which way to provide a general learning direction for our policy-based agent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Pre-training Strategy</head><p>The pre-training strategy, inspired from AlphaGo ( <ref type="bibr">Silver et al., 2016)</ref>, is a common strategy in RL related works to accelerate the training of RL agents. Normally, they utilize a small part of the annotated dataset to train policy networks before reinforcement learning. For example, AlphaGo uses the collected experts moves to do a supervised learning for Go RL agent. However, in distant supervision relation extraction task, there is not any supervised in- formation that can be used unless let linguistic experts to do some manual annotations for part of the entity pairs. However, this is expensive, and it is not the original intention of distant supervision. Under this circumstance, we propose a compromised solution. With well-aligned corpus, the true positive samples should have evident advantage in quantity compared with false positive samples in the distantly-supervised dataset. So, for a specific relation type, we directly treat the distantly-supervised positive set as the positive set, and randomly extract part of distantly-supervised negative set as the negative set. In order to better consider prior information during this pre-training procedure, the amount of negative samples is 10 times of the number of positive samples. It is because, when learning with massive negative samples, the agent is more likely to develop toward a better direction. Cross-entropy cost function is used to train this binary classifier, where the negative label corresponds to the removing action, and the positive label corresponds to the retaining action. </p><formula xml:id="formula_1">+ (1 − y i )log[1 − π(a = y i |s i ; θ)]</formula><p>Due to the noisy nature of the distantly-labeled in- stances, if we let this pre-training process overfit this noisy dataset, the predicted probabilities of most samples tend to be close to 0 or 1, which is difficult to be corrected and unnecessarily in- creases the training cost of reinforcement learning. So, we stop this training process when the accu- racy reaches 85% ∼ 90%. Theoretically, our ap- proach can be explained as increasing the entropy of the policy gradient agent, and preventing the en- tropy of the policy being too low, which means that the lack of exploration may be a concern.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Retraining Agent with Rewards</head><p>As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, in order to discover incorrectly-labeled instances without any super- vised information, we introduce a policy-based RL method. What our agent tries to deal with is the noisy samples from the distantly-supervised positive dataset; Here we call it as the DS pos- itive dataset. We split it into the training posi- tive set P ori t and the validation positive set P ori v ; naturally, both of these two set are noisy. Cor- respondingly, the training negative set N ori t and the validation negative set N ori v are constructed by randomly selected from the DS negative dataset. In every epoch, the agent removes a noisy sam- ple set Ψ i from P ori t according to the stochastic policy π(a|s), and we obtain a new positive set P t = P ori t − Ψ i . Because Ψ i is recognized as the wrong-labeled samples, we redistribute it into the negative set N t = N ori t + Ψ i . Under this set- ting, the scale of training set is constant for each epoch. Now we utilize the cleaned data {P t , N t } to train a relation classifier. The desirable situa- tion is that RL agent has the capacity to increase the performance of relation classifier through relo- cating incorrectly-labeled false positive instances. Therefore, we use the validation set {P ori v , N ori v } to measure the performance of the current agent. First, this validation set is filtered and redistributed by the current agent as {P v , N v }; the F 1 score of the current relation classifier is calculated from it. Finally, the difference of F 1 scores between the current and previous epoch is used to calculate re- ward. Next, we will introduce several strategies to train a more robust RL agent.</p><p>Removing the fixed number of sentences in each epoch In every epoch, we let the RL agent to remove a fixed number of sentences or less (when the number of the removed sentences in one epoch does not reach this fixed number during training), in which way to prevent the case that the agent tries to remove more false positive instances by removing more instances. Under the restriction of fixed number, if the agent decides to remove the current state, it means the chance of removing other states decrease. Therefore, in order to obtain a better reward, the agent should try to remove a instance set that includes more negative instances.</p><p>Loss function The quality of the RL agent is re- flected by the quality of the removed part. After the pre-training process, the agent just possesses Algorithm 1 Retraining agent with rewards for relation k. For a clearer expression, k is omitted in the following algorithm. Require: Positive set {P ori t , P ori v }, Negative set {N ori t , N ori v }, the fixed number of removal γ t , γ v 1: Load parameters θ from pre-trained policy network 2: Initialize s * as the all-zero vector with the same dimension of s j 3: for epoch i = 1 → N do <ref type="bibr">4:</ref> for s j ∈ P ori t do 5:</p><formula xml:id="formula_2">s j = concatenation(s j , s * ) 6:</formula><p>Randomly sample a j ∼ π(a| s j ; θ); compute p j = π(a = 0| s j ; θ)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>if a j == 0 then 8:</p><p>Save tuple t j = ( s j , p j ) in T and recompute the average vector of removed sentences s *</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p>end if 10:</p><p>end for 11:</p><p>Rank T based on p j from high to low, obtain T rank 12:</p><formula xml:id="formula_3">for t i in T rank [: γ t ] do 13: Add t i [0] into Ψ i 14:</formula><p>end for 15: </p><formula xml:id="formula_4">P i t = P ori t − Ψ i , N i t = N ori t + Ψ i ,</formula><formula xml:id="formula_5">R = α(F i 1 − F i−1 1 )</formula><p>19:</p><formula xml:id="formula_6">Ω i−1 = Ψ i−1 − Ψ i ∩ Ψ i−1 ; Ω i = Ψ i − Ψ i ∩ Ψ i−1 20:</formula><p>21:</p><p>Updata θ: g ∝ θ Ω i log π(a|s; θ)R + θ Ω i−1 log π(a|s; θ)(−R) <ref type="bibr">22:</ref> end for the ability to distinguish the obvious false posi- tive instances, which means the discrimination of the indistinguishable wrong-labeled instances are still ambiguous. Particularly, this indistinguish- able part is the criterion to reflect the quality of the agent. Therefore, regardless of these easy- distinguished instances, the different parts of the removed parts in different epochs are the determi- nant of the change of F 1 scores. Therefore, we definite two sets:</p><formula xml:id="formula_7">Ω i−1 = Ψ i−1 − (Ψ i ∩ Ψ i−1 ) (3) Ω i = Ψ i − (Ψ i ∩ Ψ i−1 )<label>(4)</label></formula><p>where Ψ i is the removed part of epoch i. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Redistributing Training Dataset with Policy-based Agents</head><p>Through the above reinforcement learning proce- dure, for each relation type, we obtain a agent as the false-positive indicator. These agents possess the capability of recognizing incorrectly-labeled instances of the corresponding relation types. We adopt these agents as classifiers to recognize false positive samples in the noisy distantly-supervised training dataset. For one entity pair, if all the sen- tence aligned from corpus are classified as false positive, then this entity pair is redistributed into the negative set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We adopt a policy-based RL method to generate a series of relation indicators and use them to re-distribute training dataset by moving false positive samples to negative sample set. Therefore, our ex- periments are intended to demonstrate that our RL agents possess this capability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datast and Evaluation Metrics</head><p>We evaluate the proposed method on a commonly- used dataset 2 , which is first presented in <ref type="bibr">Riedel et al. (2010)</ref>. This dataset is generated by aligning entity pairs from Freebase with New York Times corpus(NYT). Entity mentions of NYT corpus are recognized by the Stanford named entity recog- nizer ( <ref type="bibr" target="#b4">Finkel et al., 2005</ref>). The sentences from the years <ref type="bibr">[2005]</ref><ref type="bibr">[2006]</ref> are used as the training corpus and sentences from 2007 are used as the testing corpus. There are 52 actual relations and a special relation N A which indicates there is no relation between the head and tail entities. The sentences of N A are from the entity pairs that exist in the same sentence of the actual relations but do not appear in the Freebase. Similar to the previous works, we adopt the held-out evaluation to evaluate our model, which can provide an approximate measure of the clas- sification ability without costly human evaluation. Similar to the generation of the training set, the entity pairs in test set are also selected from Free- base, which will be predicted under the sentences discovered from the NYT corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Policy-based Agent</head><p>The action space of our RL agent just includes two actions. Therefore, the agent can be modeled as a binary classifier. We adopt a single-window CNN as this policy network. The detailed hyperparam- eter settings are presented in <ref type="table" target="#tab_2">Table 1</ref>. As for word embeddings, we directly use the word embedding file released by <ref type="bibr" target="#b9">Lin et al. (2016)</ref>  <ref type="bibr">3</ref> , which just keeps the words that appear more than 100 times in NYT. Moreover, we have the same dimension set- ting of the position embedding, and the maximum length of relative distance is −30 and 30 ("-" and "+" represent the left and right side of the enti- ties). The learning rate of reinforcement learning is 2e −5 . For each relation type, the fixed num- ber γ t , γ v are according to the pre-trained agent. When one relation type has too many distant- supervised positive sentences (for example, /lo-  Comparison of F 1 scores among three cases: the relation classifier is trained with the original dataset, the redistributed dataset generated by the pre-trained agent, and the redistributed dataset generated by our RL agent respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyperparameter Value</head><note type="other">Window size c w 3 Kernel size c k 100 Batch size 64 Regulator α 100</note><p>The name of relation types are abbreviated: /peo/per/pob represents /people/person/place of birth cation/location/contains has 75768 sentences), we sample a subset of size 7,500 sentences to train the agent. For the average vector of the removed sentences, in the pre-training process and the first state of the retraining process, it is set as all-zero vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Relation Classifier for Calculating Reward</head><p>In order to evaluate a series of actions by agent, we use a simple CNN model, because the simple net- work is more sensitive to the quality of the training set. The proportion between P ori t and P ori v is 2:1, and they are all derived from the training set of Riedel dataset; the corresponding negative sample sets N ori t and N ori v are randomly selected from the Riedel negative dataset, whose size is twice that of their corresponding positive sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">The Effectiveness of Reinforcement Learning</head><p>In <ref type="table" target="#tab_3">Table 2</ref>, we list the F 1 scores before and after adopting the proposed RL method. Even though there are 52 actual relation types in Riedel dataset, only 10 relation types have more than 1000 pos- itive instances <ref type="bibr">4</ref> . Because of the randomness of deep neural network on the small-scale dataset, we just train policy-based agents for these 10 relation types. First, compared with Original case, most of the Pretrain agents yield obvious improvements: It not only demonstrates the rationality of our pre- training strategy, but also verifies our hypothe- sis that most of the positive samples in Riedel dataset are true positive. More significantly, af- ter retraining with the proposed policy-based RL method, the F 1 scores achieve further improve- ment, even for the case the Pretrain agents per- form bad. These comparable results illustrate that the proposed policy-based RL method is capable of making agents develop towards a good direc- tion. <ref type="bibr">4</ref> The supervised relation classification task <ref type="bibr">Semeval-2010</ref><ref type="bibr">Task 8 (Hendrickx et al., 2009</ref>) annotates nearly 1,000 in- stances for each relation type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>- +RL p-value CNN+ONE 0.177 0.190 1.24e-4 CNN+ATT 0.219 0.229 7.63e-4 PCNN+ONE 0.206 0.220 8.35e-6 PCNN+ATT 0.253 0.261 4.36e-3 <ref type="table">Table 3</ref>: Comparison of AUC values between pre- vious studies and our RL method, and the p-value of t-test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Impact of False Positive Samples</head><p>Zeng et al. (2015) and <ref type="bibr" target="#b9">Lin et al. (2016)</ref> are both the robust models to solve wrong labeling problem of distant supervision relation extraction. <ref type="bibr" target="#b11">Zeng et al. (2015)</ref> combine at-least-one multi-instance learning with deep neural network to extract only one active sentence to predict the relation between entity pair; <ref type="bibr" target="#b9">Lin et al. (2016)</ref> combine all sen- tences of one entity pair and assign soft attention weights to them, in which way to generate a com- positive relation representation for this entity pair. However, the false positive phenomenon also in- cludes the case that all the sentences of one en- tity pair are wrong, which is because the corpus is not completely aligned with the knowledge base. This phenomenon is also common between Riedel dataset and Freebase through our manual inspec- tion. Obviously, there is nothing the above two methods can do in this case.</p><p>The proposed RL method is to tackle this prob- lem. We adopt our RL agents to redistribute Riedel dataset by moving false positive samples into the negative sample set. Then we use <ref type="bibr" target="#b11">Zeng et al. (2015)</ref> and <ref type="bibr" target="#b9">Lin et al. (2016)</ref> to predict relations on this cleaned dataset, and compare the performance with that on the original Riedel dataset. As shown in <ref type="figure">Figure 3</ref> and <ref type="figure" target="#fig_3">Figure 4</ref>, under the assistant of our RL agent, the same model can achieve obvious im- provement with more reasonable training dataset. In order to give the more intuitive comparison, we calculate the AUC value of each PR curve, which reflects the area size under these curves. These comparable results also indicate the effectiveness of our policy-based RL method. Moreover, as can be seen from the result of t-test evaluation, all the p-values are less than 5e-02, so the improvements are significant. <ref type="figure">Figure 5</ref> indicates that, for different relations, the scale of the detected false positive samples is not</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Case Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation</head><p>/people/person/place of birth FP 1. GHETTO SUPERSTAR ( THE MAN THAT I AM) -Ranging from Pittsburgh to Broadway, Billy Porter performs his musical memoir. FP 1. "They are trying to create a united front at home in the face of the pressures Syria is facing," said Sami Moubayed, a political analyst and writer here. 2. "Iran injected Syria with a lot of confidence: stand up, show defiance," said Sami Moubayed, a political analyst and writer in Damascus. Relation /people/deceased person/place of death FP 1. Some New York city mayors -William O'Dwyer, Vincent R. Impellitteri and Abraham Beame -were born abroad. 2. Plenty of local officials have, too, including two New York city mayors, James J. <ref type="bibr">Walker, in 1932, and</ref><ref type="bibr">William O'Dwyer, in 1950</ref>. <ref type="table">Table 4</ref>: Some examples of the false positive samples detected by our policy-based agent. Each row denotes the annotated sentences of one entity pair.</p><p>proportional to the original scale, which is in ac- cordance with the actual accident situation. At the same time, we analyze the correlation between the false positive phenomenon and the number of sentences of entity pairs : With this the number ranging from 1 to 5, the corresponding percent- ages are [55.9%, 32.0%, 3.7%, 4.4%, 0.7%]. This distribution is consistent with our assumption. Be- cause Freebase is, to some extent, not completely aligned with the NYT corpus, entity pairs with fewer sentences are more likely to be false posi- tive, which is the major factor hindering the per- formance of the previous systems. In <ref type="table">Table 4</ref>, we present some false positive examples selected by our agents. Taking entity pair (Sami Moubayed, Syria) as an example, it is obvious that there is not any valuable information reflecting relation /peo- ple/person/place of birth. Both of these sentences talks about the situation analysis of Syria from the political analyst Sami Moubayed. We also found that, for some entity pairs, even though there are multiple sentences, all of them are identical. This phenomenon also increases the probability of the appearance of false positive samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we propose a deep reinforcement learning framework for robust distant supervision. The intuition is that, in contrast to prior works that utilize only one instance per entity pair and use soft attention weights to select plausible distantly supervised examples, we describe a policy-based framework to systematically learn to relocate the false positive samples, and better utilize the un- labeled data. More specifically, our goal is to </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RELATION ID</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Removed Total</head><p>Figure 5: This figure presents the scale of the re- moved part for each relation type, where the hori- zontal axis corresponds to the IDs in <ref type="table" target="#tab_3">Table 2</ref>.</p><p>teach the reinforcement agent to optimize the se- lection/redistribution strategy that maximizes the reward of boosting the performance of relation classification. An important aspect of our work is that our framework does not depend on a spe- cific form of the relation classifier, meaning that it is a plug-and-play technique that could be poten- tially applied to any relation extraction pipeline. In experiments, we show that our framework boosts the performance of distant supervision relation ex- traction of various strong deep learning baselines on the widely used New York Times -Freebase dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>µí±¤í µí±í µí±í µí± í µí°´í µí±í µí±¡í µí±í µí±í µí± Classifier í µí±í µí±í µí±í µí±í µí±</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( 2 )</head><label>2</label><figDesc>J(θ) = i y i log[π(a = y i |s i ; θ)]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Ω i−1 and Ω i are represented with the different colors in Fig- ure 2. If F 1 score increases in the epoch i, it means the actions of the epoch i is more reasonable than that in the epoch i − 1. In other words, Ω i is more negative than Ω i−1 . Thus, we assign the positive reward to Ω i and the negative reward to Ω i−1 , and vice versa. In summary, the ultimate loss function is formulated as follow: (5) J(θ) = Ω i log π(a|s; θ)R + Ω i−1 log π(a|s; θ)(−R)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 3: Aggregate PR curves of CNN˙based model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 1 : Hyperparameter settings.</head><label>1</label><figDesc></figDesc><table>ID 
Relation 
Original Pretrain 
RL 
1 
/peo/per/pob 
55.60 
53.63 
55.74 
2 
/peo/per/n 
78.85 
80.80 
83.63 
3 
/peo/per/pl 
86.65 
89.62 
90.76 
4 
/loc/loc/c 
80.78 
83.79 
85.39 
5 
/loc/cou/ad 
90.9 
88.1 
89.86 
6 
/bus/per/c 
81.03 
82.56 
84.22 
7 
/loc/cou/c 
88.10 
93.78 
95.19 
8 
/loc/adm/c 
86.51 
85.56 
86.63 
9 
/loc/nei/n 
96.51 
97.20 
98.23 
10 
/peo/dec/p 
82.2 
83.0 
84.6 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> We also tried the strategy that just builds a single agent for all relation types: a binary classifier(TP/FP) or a multiclass classifier(rela1/rela2/.../FP). But, it has the limitation in the performance. We found that our one-agent-for-onerelation strategy obtained better performance than the single agent strategy.</note>

			<note place="foot" n="2"> http://iesl.cs.umass.edu/riedel/ecml/ 3 https://github.com/thunlp/NRE</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledge</head><p>This work was supported by National Natural Sci-ence Foundation of China (61702047), Beijing Natural Science Foundation (4174098), the Fun-damental Research Funds for the Central Univer-sities (2017RC02) and National Natural Science Foundation of China <ref type="formula">(61703234)</ref> </p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Vol- ume 2-Volume 2, pages 1003-1011. Association for</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Arulkumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">Peter</forename><surname>Deisenroth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miles</forename><surname>Brundage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anil Anthony</forename><surname>Bharath</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05866</idno>
		<title level="m">A brief survey of deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Subsequence kernels for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="171" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning how to active learn: A deep reinforcement learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02383</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Reinforcement learning for relation classification from noisy data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Incorporating non-local information into information extraction systems by gibbs sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trond</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd annual meeting on association for computational linguistics</title>
		<meeting>the 43rd annual meeting on association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Exploring various knowledge in relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Guodong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Min</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd annual meeting on association for computational linguistics</title>
		<meeting>the 43rd annual meeting on association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="427" to="434" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iris</forename><surname>Hendrickx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><forename type="middle">Nam</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diarmuid´odiarmuid´</forename><forename type="middle">Diarmuid´o</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenza</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Szpakowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions</title>
		<meeting>the Workshop on Semantic Evaluations: Recent Achievements and Future Directions</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="94" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Knowledgebased weak supervision for information extraction of overlapping relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congle</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction with sentence-level attention and entity descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3060" to="3066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural relation extraction with selective attention over instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACL</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Rion Snow, and Dan Jurafsky</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction via piecewise convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="17" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
