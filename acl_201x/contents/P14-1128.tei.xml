<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zeng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidia</forename><forename type="middle">S</forename><surname>Chao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Tian</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">NLP</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">CT Lab / Department of Computer and Information Science</orgName>
								<orgName type="institution">University of Macau ‡ INESC-ID / Instituto Superior Ténico</orgName>
								<address>
									<settlement>Lisboa</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1360" to="1369"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This study investigates on building a better Chinese word segmentation model for statistical machine translation. It aims at leveraging word boundary information , automatically learned by bilingual character-based alignments, to induce a preferable segmentation model. We propose dealing with the induced word boundaries as soft constraints to bias the continuous learning of a supervised CRF-s model, trained by the treebank data (la-beled), on the bilingual data (unlabeled). The induced word boundary information is encoded as a graph propagation constraint. The constrained model induction is accomplished by using posterior reg-ularization algorithm. The experiments on a Chinese-to-English machine translation task reveal that the proposed model can bring positive segmentation effects to translation quality.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Word segmentation is regarded as a critical pro- cedure for high-level Chinese language process- ing tasks, since Chinese scripts are written in con- tinuous characters without explicit word bound- aries (e.g., space in English). The empirical works show that word segmentation can be beneficial to Chinese-to-English statistical machine translation (SMT) ( <ref type="bibr" target="#b23">Xu et al., 2005;</ref><ref type="bibr" target="#b3">Chang et al., 2008;</ref><ref type="bibr" target="#b30">Zhao et al., 2013)</ref>. In fact most current SMT models assume that parallel bilingual sentences should be segmented into sequences of tokens that are meant to be "words" <ref type="bibr" target="#b11">(Ma and Way, 2009</ref>). The practice in state-of-the-art MT systems is that Chinese sen- tences are tokenized by a monolingual supervised word segmentation model trained on the hand- annotated treebank data, e.g., Chinese treebank (CTB) ( <ref type="bibr" target="#b24">Xue et al., 2005</ref>). These models are con- ducive to MT to some extent, since they common- ly have relatively good aggregate performance and segmentation consistency ( <ref type="bibr" target="#b3">Chang et al., 2008)</ref>. But one outstanding problem is that these mod- els may leave out some crucial segmentation fea- tures for SMT, since the output words conform to the treebank segmentation standard designed for monolingually linguistic intuition, rather than spe- cific to the SMT task.</p><p>In recent years, a number of works ( <ref type="bibr" target="#b23">Xu et al., 2005;</ref><ref type="bibr" target="#b3">Chang et al., 2008;</ref><ref type="bibr" target="#b11">Ma and Way, 2009;</ref><ref type="bibr" target="#b21">Xi et al., 2012</ref>) attempted to build segmentation models for SMT based on bilingual unsegment- ed data, instead of monolingual segmented data. They proposed to learn gainful bilingual knowl- edge as golden-standard segmentation supervi- sions for training a bilingual unsupervised mod- el. Frequently, the bilingual knowledge refers to the mappings of an individual English word to one or more consecutive Chinese characters, generat- ed via statistical character-based alignment. They leverage such mappings to either constitute a Chi- nese word dictionary for maximum-matching seg- mentation ( <ref type="bibr" target="#b22">Xu et al., 2004</ref>), or form labeled data for training a sequence labeling model <ref type="bibr" target="#b17">(Paul et al., 2011</ref>). The prior works showed that these models help to find some segmentations tailored for SMT, since the bilingual word occurrence feature can be captured by the character-based alignment <ref type="bibr" target="#b14">(Och and Ney, 2003)</ref>. However, these models tend to miss out other linguistic segmentation patterns as monolingual supervised models, and suffer from the negative effects of erroneously alignments to word segmentation. This paper proposes an alternative Chinese Word Segmentation (CWS) model adapted to the SMT task, which seeks not only to maintain the advantages of a monolingual supervised model, having hand-annotated linguistic knowledge, but also to assimilate the relevant bilingual segmenta-tion nature. We propose leveraging the bilingual knowledge to form learning constraints that guide a supervised segmentation model toward a better solution for SMT. Besides the bilingual motivat- ed models, character-based alignment is also em- ployed to achieve the mappings of the successive Chinese characters and the target language word- s. Instead of directly merging the characters in- to concrete segmentations, this work attempts to extract word boundary distributions for character- level trigrams (types) from the "chars-to-word" mappings. Furthermore, these word boundaries are encoded into a graph propagation (GP) expres- sion, in order to widen the influence of the induced bilingual knowledge among Chinese texts. The G- P expression constrains similar types having ap- proximated word boundary distributions. Crucial- ly, the GP expression with the bilingual knowledge is then used as side information to regularize a CRFs (conditional random fields) model's learn- ing over treebank and bitext data, based on the posterior regularization (PR) framework ( <ref type="bibr" target="#b8">Ganchev et al., 2010)</ref>. This constrained learning amounts to a jointly coupling of GP and CRFs, i.e., integrating GP into the estimation of a parametric structural model. This paper is structured as follows: Section 2 points out the main differences with the related works of this study. Section 3 presents the de- tails of the proposed segmentation model. Section 4 reports the experimental results of the proposed model for a Chinese-to-English MT task. The con- clusion is drawn in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In the literature, many approaches have been pro- posed to learn CWS models for SMT. They can be put into two categories, monolingual-motivated and bilingual-motivated. The former primarily op- timizes monolingual supervised models according to some predefined segmentation properties that are manually summarized from empirical MT e- valuations. <ref type="bibr" target="#b3">Chang et al. (2008)</ref> enhanced a CRF- s segmentation model in MT tasks by tuning the word granularity and improving the segmentation consistence. <ref type="bibr" target="#b28">Zhang et al. (2008)</ref> produced a bet- ter segmentation model for SMT by concatenat- ing various corpora regardless of their differen- t specifications. Distinct from their behaviors, this work uses automatically learned constraints instead of manually defined ones. Most impor- tantly, the constraints have a better learning guid- ance since they originate from the bilingual texts. On the other hand, the bilingual-motivated CWS models typically rely on character-based align- ments to generate segmentation supervisions. <ref type="bibr" target="#b22">Xu et al. (2004)</ref> proposed to employ "chars-to-word" alignments to generate a word dictionary for max- imum matching segmentation in SMT task. The works in <ref type="bibr" target="#b11">(Ma and Way, 2009;</ref><ref type="bibr" target="#b30">Zhao et al., 2013)</ref> extended the dictionary extraction strategy. Ma and Way (2009) adopted co-occurrence frequency metric to iteratively optimize "candidate words" extract from the alignments. <ref type="bibr" target="#b30">Zhao et al. (2013)</ref> at- tempted to find an optimal subset of the dictionary learned by the character-based alignment to maxi- mize the MT performance. <ref type="bibr" target="#b17">Paul et al. (2011)</ref> used the words learned from "chars-to-word" align- ments to train a maximum entropy segmentation model. Rather than playing the "hard" uses of the bilingual segmentation knowledge, i.e., direct- ly merging "char-to-word" alignments to words as supervisions, this study extracts word bound- ary information of characters from the alignments as soft constraints to regularize a CRFs model's learning.</p><p>The graph propagation (GP) technique provides a natural way to represent data in a variety of tar- get domains <ref type="bibr" target="#b0">(Belkin et al., 2006</ref>). In this tech- nique, the constructed graph has vertices consist- ing of labeled and unlabeled examples. Pairs of vertices are connected by weighted edges encod- ing the degree to which they are expected to have the same label ( <ref type="bibr" target="#b31">Zhu et al., 2003</ref>). Many recent works, such as by <ref type="bibr" target="#b19">Subramanya et al. (2010)</ref>, <ref type="bibr" target="#b5">Das and Petrov (2011)</ref>, <ref type="bibr" target="#b25">Zeng et al. (2013;</ref> and <ref type="bibr" target="#b32">Zhu et al. (2014)</ref>, proposed GP for inferring the la- bel information of unlabeled data, and then lever- age these GP outcomes to learn a semi-supervised scalable model (e.g., CRFs). These approaches are referred to as pipelined learning with GP. This s- tudy also works with a similarity graph, encoding the learned bilingual knowledge. But, unlike the prior pipelined approaches, this study performs a joint learning behavior in which GP is used as a learning constraint to interact with the CRFs mod- el estimation.</p><p>One of our main objectives is to bias CRF- s model's learning on unlabeled data, under a non-linear GP constraint encoding the bilingual knowledge. This is accomplished by the poste- rior regularization (PR) framework ( <ref type="bibr" target="#b8">Ganchev et al., 2010)</ref>. PR performs regularization on poste- riors, so that the learned model itself remains sim- ple and tractable, while during learning it is driven to obey the constraints through setting appropriate parameters. The closest prior study is constrained learning, or learning with prior knowledge. <ref type="bibr" target="#b3">Chang et al. (2008)</ref> described constraint driven learning (CODL) that augments model learning on unla- beled data by adding a cost for violating expec- tations of constraint features designed by domain knowledge. <ref type="bibr" target="#b12">McCallum (2008) and</ref><ref type="bibr" target="#b13">McCallum et al. (2007)</ref> proposed to employ gener- alized expectation criteria (GE) to specify prefer- ences about model expectations in the form of lin- ear constraints on some feature expectations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>This work aims at building a CWS model adapted to the SMT task. Here, we are interested on n-to-1 alignment pat- terns, i.e., one target word is aligned to one or more source Chinese characters. The second step aims to collect word boundary distributions for al- l types, i.e., character-level trigrams, according to the n-to-1 mappings (Section 3.1). The third step is to encode the induced word boundary informa- tion into a k-nearest-neighbors (k-NN) similarity graph constructed over the entire set of types from </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Word Boundaries Learned from</head><p>Character-based Alignments</p><p>The gainful supervisions toward a better segmen- tation solution for SMT are naturally extracted from MT training resources, i.e., bilingual parallel data. This study employs an approximated method introduced in ( <ref type="bibr" target="#b22">Xu et al., 2004;</ref><ref type="bibr" target="#b11">Ma and Way, 2009;</ref><ref type="bibr" target="#b4">Chung and Gildea, 2009</ref>) to learn bilingual seg- </p><formula xml:id="formula_0">D c↔f ← char align bitext (D c u , D f u ) 2: r ← learn word bound (D c↔f ) 3: G ← encode graph constraint (D c l , D c u , r) 4: θ ← pr crf graph (D c l , D c u , G)</formula><p>mentation knowledge. This relies on statistical character-based alignment: first, every Chinese character in the bitexts is divided by a white s- pace so that individual characters are regarded as special "words" or alignment targets, and second, they are connected with English words by using a statistical word aligner, e.g., GIZA++ <ref type="bibr" target="#b14">(Och and Ney, 2003)</ref>. Note that the aligner is restricted to use an n-to-1 alignment pattern. The primary idea is that consecutive Chinese characters are grouped to a candidate word, if they are aligned to the same foreign word. It is worth mentioning that prior works presented a straightforward usage for can- didate words, treating them as golden segmenta- tions, either dictionary units or labeled resources. But this study treats the induced candidate word- s in a different way. We propose to extract the word boundary distributions 1 for character-level trigrams (type) 2 , as shown in <ref type="figure" target="#fig_2">Figure 1</ref>, instead of the very specific words. There are two main rea- sons to do so. First, it is a more general expression which can reduce the impact amplification of er- roneous character alignments. Second, boundary distributions can play more flexible roles as con- straints over labelings to bias the model learning.</p><p>The type-level word boundary extraction is for- mally described as follows. Given the ith sen- tence pair</p><formula xml:id="formula_1">x c i , x f i , A c→f i</formula><p>of the aligned bilin- gual corpus D c↔f , the Chinese sentence x c i con- sisting of m characters {x c i,1 , x c i,2 , ..., x c i,m }, and the foreign language sentence x f i , consisting of</p><formula xml:id="formula_2">n words {x f i,1 , x f i,2 , ..., x f i,n }, A c→f i represents a</formula><p>set of alignment pairs a j = C j , x f i,j that de- fines connections between a few Chinese char-</p><formula xml:id="formula_3">acters C j = {x c i,j 1 , x c i,j 2 , ..., x c i,j k } and a sin- gle foreign word x f i,j . For an alignment a j = C j , x f i,j , only the sequence of characters C j = {x c i,j 1 , x c i,j 2 , ..., x c i,j k } ∀d ∈ [1, k − 1], j d+1 − j d = 1</formula><p>constitutes a valid candidate word. For the w- hole bilingual corpus, we assign each character in the candidate words with a word boundary tag T ∈ {B, M, E, S}, and then count across the en- tire corpus to collect the tag distributions r i = {r i,t ; t ∈ T } for each type</p><formula xml:id="formula_4">x c i,j−1 x c i,j x c i,j+1 . 北 京 奥 运 会 Beijing Olympus</formula><p>Character-based alignment  </p><formula xml:id="formula_5">北 京 奥 运 会 B E B M E Beijing</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Constraints Encoded by Graph Propagation Expression</head><p>The previous step contributes to generate bilingual segmentation supervisions, i.e., type-level word boundary distributions. An intuitive manner is to directly leverage the induced boundary distribu- tions as label constraints to regularize segmenta- tion model learning, based on a constrained learn- ing algorithm. This study, however, makes further efforts to elevate the positive effects of the bilin- gual knowledge via the graph propagation tech- nique. We adopt a similarity graph to encode the learned type-level word boundary distribution- s. The GP expression will be defined as a PR con- straint in Section 3.3 that reflects the interactions between the graph and the CRFs model. In other words, GP is integrated with estimation of para- metric structural model. This is greatly different from the prior pipelined approaches ( <ref type="bibr" target="#b19">Subramanya et al., 2010;</ref><ref type="bibr" target="#b5">Das and Petrov, 2011;</ref><ref type="bibr" target="#b25">Zeng et al., 2013)</ref>, where GP is run first and its propagated outcomes are then used to bias the structural mod- el. This work seeks to capture the GP benefits dur- ing the modeling of sequential correlations.</p><p>In what follows, the graph setting and propa- gation expression are introduced. As in conven- tional GP examples ( <ref type="bibr" target="#b6">Das and Smith, 2012</ref>), a sim- ilarity graph G = (V, E) is constructed over N types extracted from Chinese training data, includ- ing treebank D c l and bitexts D c u . Each vertex V i has a |T |-dimensional estimated measure v i = {v <ref type="bibr" target="#b5">Das and Petrov, 2011;</ref><ref type="bibr" target="#b25">Zeng et al., 2013</ref>). The similarities are measured based on co-occurrence statistics over a set of predefined features (intro- duced in Section 4.1). Specifically, the point-wise mutual information (PMI) values, between ver- tices and each feature instantiation that they have in common, are summed to sparse vectors, and their cosine distances are computed as the sim- ilarities. The nature of this similarity graph en- forces that the connected types with high weight- s appearing in different texts should have similar word boundary distributions.</p><note type="other">i,t ; t ∈ T } representing a probability distribu- tion on word boundary tags. The induced type- level word boundary distributions r i = {r i,t ; t ∈ T } are empirical measures for the corresponding M graph vertices. The edges E ∈ V i × V j connect all the vertices. Scores between pairs of graph ver- tices (types), w ij , refer to the similarities of their syntactic environment, which are computed fol- lowing the method in (Subramanya et al., 2010;</note><p>The quality (smoothness) of the similarity graph can be estimated by using a standard propagation function, as shown in Equation 1. The square-loss criterion ( <ref type="bibr" target="#b31">Zhu et al., 2003;</ref><ref type="bibr" target="#b2">Bengio et al., 2006</ref>) is used to formulate this function:</p><formula xml:id="formula_6">P(v) = T t=1 M i=1 (v i,t − r i,t ) 2 +µ N j=1 N i=1 w ij (v i,t − v j,t ) 2 + ρ N i=1 (v i,t ) 2 (1)</formula><p>The first term in this equation refers to seed match- es that compute the distances between the estimat- ed measure v i and the empirical probabilities r i . The second term refers to edge smoothness that measures how vertices v i are smoothed with re- spect to the graph. Two types connected by an edge with high weight should be assigned similar word boundary distributions. The third term, a 2 norm, evaluates the distribution sparsity <ref type="bibr" target="#b6">(Das and Smith, 2012)</ref> per vertex. Typically, the GP process amounts to an optimization process with respect to parameter v such that Equation 1 is minimized. This propagation function can be used to reflect the graph smoothness, where the higher the score, the lower the smoothness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">PR Learning with GP Constraint</head><p>Our learning problem belongs to semi-supervised learning (SSL), as the training is done on treebank labeled data (X L , Y L ) = {(x 1 , y 1 ), ..., (x l , y l )}, and bilingual unlabeled data (X U ) = {x 1 , ..., x u } where x i = {x 1 , ..., x m } is an input word se- quence and y i = {y 1 , ..., y m }, y ∈ T is its corre- sponding label sequence. Supervised linear-chain CRFs can be modeled in a standard conditional log-likelihood objective with a Gaussian prior:</p><formula xml:id="formula_7">L(θ) = p θ (y i |x i ) − θ 2 2σ<label>(2)</label></formula><p>The conditional probabilities p θ are expressed as a log-linear form:</p><formula xml:id="formula_8">p θ (y i |x i ) = exp( m k=1 θ T f (y k−1 i , y k i , x i )) Z θ (x i )<label>(3)</label></formula><p>Where Z θ (x i ) is a partition function that normal- izes the exponential form to be a probability dis- tribution, and f (y k−1 i , y k i , x i ) are arbitrary feature functions.</p><p>In our setting, the CRFs model is required to learn from unlabeled data. This work em- ploys the posterior regularization (PR) frame- work 3 ( <ref type="bibr" target="#b8">Ganchev et al., 2010)</ref> to bias the CRFs model's learning on unlabeled data, under a con- straint encoded by the graph propagation expres- sion. It is expected that similar types in the graph should have approximated expected taggings un- der the CRFs model. We follow the approach in- troduced by <ref type="bibr" target="#b9">(He et al., 2013</ref>) to set up a penalty- based PR objective with GP: the CRFs likelihood is modified by adding a regularization term, as shown in Equation 4, representing the constraints:</p><formula xml:id="formula_9">R U (θ, q) = KL(q||p θ ) + λP(v)<label>(4)</label></formula><p>Rather than regularize CRFs model's posteriors p θ (Y|x i ) directly, our model uses an auxiliary distribution q(Y|x i ) over the possible labelings</p><p>Y for x i , and penalizes the CRFs marginal log- likelihood by a KL-divergence term 4 , represent- ing the distance between the estimated posteriors p and the desired posteriors q, as well as a penal- ty term, formed by the GP function. The hy- perparameter λ is used to control the impacts of the penalty term. Note that the penalty is fired if the graph score computed based on the expect- ed taggings given by the current CRFs model is increased vis-a-vis the previous training iteration. This nature requires that the penalty term P(v) should be formed as a function of posteriors q over CRFs model predictions <ref type="bibr">5</ref> , i.e., P(q). To state this, a mapping M : ({1, ..., u}, {1, ..., m}) → V from words in the corpus to vertices in the graph is de- fined. We can thus decompose v i,t into a function of q as follows:</p><formula xml:id="formula_10">vi,t = u a=1 m b=1; M(a,b)=V i T c=1 y∈Y 1(y b = t, y b−1 = c)q(y|xa) u a=1 m b=1 1(M(a, b) = Vi)<label>(5)</label></formula><p>The final learning objective combines the CRF- s likelihood with the PR regularization term: J (θ, q) = L(θ) + R U (θ, q). This joint objec- tive, over θ and q, can be optimized by an expecta- tion maximization (EM) style algorithm as report- ed in ( <ref type="bibr" target="#b8">Ganchev et al., 2010</ref>). We start from ini- tial parameters θ 0 , estimated by supervised CRFs model training on treebank data. The E-step is to minimize R U (θ, q) over the posteriors q that are constrained to the probability simplex. Since the penalty term P(v) is a non-linear form, the opti- mization method in ( <ref type="bibr" target="#b8">Ganchev et al., 2010</ref>) via pro- jected gradient descent on the dual is inefficient <ref type="bibr">6</ref> . This study follows the optimization method ( <ref type="bibr" target="#b9">He et al., 2013</ref>) that uses exponentiated gradient descent (EGD) algorithm. It allows that the variable up- date expression, as shown in Equation 6, takes a multiplicative rather than an additive form.</p><formula xml:id="formula_11">q (w+1) (y|x i ) = q (w) (y|x i ) exp(−η ∂R ∂q (w) (y|x i ) )<label>(6)</label></formula><p>where the parameter η controls the optimization rate in the E-step. With the contributions from the E-step that further encourage q and p to agree, the M-step aims to optimize the objective J (θ, q) with respect to θ. The M-step is similar to the stan- dard CRFs parameter estimation, where the gradi- ent ascent approach still works. This EM-style ap- proach monotonically increases J (θ, q) and thus is guaranteed to converge to a local optimum.</p><formula xml:id="formula_12">E-step: q (t+1) = arg min q R U (θ (t) , q (t) ) M-step: θ (t+1) = arg max θ L(θ) +δ u i=1 y∈Y q (t+1) (y|x i ) log p θ (y|x i )<label>(7)</label></formula><p>4 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data and Setup</head><p>The experiments in this study evaluated the per- formances of various CWS models in a Chinese- to-English translation task. The influence of the word segmentation on the final translation is our main investigation. We adopted three state-of-the-art metrics, BLEU ( <ref type="bibr" target="#b16">Papineni et al., 2002</ref>), NIST ( <ref type="bibr" target="#b7">Doddington et al., 2000</ref>) and ME- TEOR ( <ref type="bibr" target="#b1">Banerjee and Lavie, 2005</ref>), to evaluate the translation quality.</p><p>The monolingual segmented data, train TB , is extracted from the Penn Chinese Treebank (CTB- 7) ( <ref type="bibr" target="#b24">Xue et al., 2005</ref>), containing 51,447 sentences. The bilingual training data, train MT , is formed by a large in-house Chinese-English parallel cor- pus ( <ref type="bibr" target="#b20">Tian et al., 2014</ref>). There are in total 2,244,319 Chinese-English sentence pairs crawled from on- line resources, concentrated in 5 different domains including laws, novels, spoken, news and miscel- laneous <ref type="bibr">7</ref> . This in-house bilingual corpus is the MT training data as well. The target-side lan- guage model is built on over 35 million mono- lingual English sentences, train LM , crawled from online resources. The NIST evaluation campaign data, MT-03 and MT-05, are selected to comprise the MT development data, dev MT , and testing da- ta, test MT , respectively.</p><p>For the settings of our model, we adopted the standard feature templates introduced by <ref type="bibr" target="#b29">Zhao et al. (2006)</ref> for CRFs. The character-based align- ment for achieving the "chars-to-word" mappings is accomplished by GIZA++ aligner <ref type="bibr" target="#b14">(Och and Ney, 2003)</ref>. For the GP, a 10-NNs similarity graph was constructed 8 . Following <ref type="bibr" target="#b19">(Subramanya et al., 2010;</ref><ref type="bibr" target="#b25">Zeng et al., 2013)</ref>, the features used to compute similarities between vertices were (Sup- pose given a type " w 2 w 3 w 4 " surrounding contexts "w 1 w 2 w 3 w 4 w 5 "): unigram (w 3 ), bigram (w 1 w 2 , w 4 w 5 , w 2 w 4 ), trigram (w 2 w 3 w 4 , w 2 w 4 w 5 , w 1 w 2 w 4 ), trigram+context (w 1 w 2 w 3 w 4 w 5 ) and character classes in number, punctuation, alpha- betic letter and other (t(w 2 )t(w 3 )t(w 4 )). There are four hyperparameters in our model to be tuned by using the development data (dev MT ) among the following settings: for the graph propagation, µ ∈ {0.2, 0.5, 0.8} and ρ ∈ {0.1, 0.3, 0.5, 0.8}; for the PR learning, λ ∈ {0 ≤ λ i ≤ 1} and σ ∈ {0 ≤ σ i ≤ 1} where the step is 0.1. The best per- formed joint settings, µ = 0.5, ρ = 0.5, λ = 0.9 and σ = 0.8, were used to measure the final per- formance.</p><p>The MT experiment was conducted based on a standard log-linear phrase-based SMT model. The GIZA++ aligner was also adopted to obtain word alignments <ref type="bibr" target="#b14">(Och and Ney, 2003</ref>) over the segmented bitexts. The heuristic strategy of grow- diag-final-and ( <ref type="bibr" target="#b10">Koehn et al., 2007</ref>) was used to combine the bidirectional alignments for extract- ing phrase translations and reordering tables. A 5-gram language model with Kneser-Ney smooth- ing was trained with SRILM <ref type="bibr" target="#b18">(Stolcke, 2002</ref>) on monolingual English data. <ref type="bibr">Moses (Koehn et al., 2007</ref>) was used as decoder. The Minimum Error Rate Training (MERT) <ref type="bibr" target="#b15">(Och, 2003)</ref> was used to tune the feature parameters on development data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Various Segmentation Models</head><p>To provide a thorough analysis, the MT experi- ments in this study evaluated three baseline seg- mentation models and two off-the-shelf models, in addition to four variant models that also employ the bilingual constraints. We start from three base- line models:</p><p>• Character Segmenter (CS): this model sim- ply divides Chinese sentences into sequences of characters.</p><p>• Supervised Monolingual Segmenter (SM- S): this model is trained by CRFs on treebank training data (train TB ). The same feature templates ( <ref type="bibr" target="#b29">Zhao et al., 2006</ref>) are used. The standard four-tags (B, M, E and S) were used as the labels. The stochastic gradient descent is adopted to optimize the parameters.</p><p>• Unsupervised Bilingual Segmenter (UBS): this model is trained on the bitexts (trainMT) following the approach introduced in <ref type="bibr" target="#b11">(Ma and Way, 2009)</ref>. The optimal set of the mod- el parameter values was found on dev MT to be k = 3, t AC = 0.0 and t COOC = 15.</p><p>The comparison candidates also involve two pop- ular off-the-shelf segmentation models:</p><p>• Stanford Segmenter: this model, trained by <ref type="bibr" target="#b3">Chang et al. (2008)</ref>, treats CWS as a binary word boundary decision task. It covers sev- eral features specific to the MT task, e.g., ex- ternal lexicons and proper noun features.</p><p>• ICTCLAS Segmenter: this model, trained by <ref type="bibr" target="#b27">Zhang et al. (2003)</ref>, is a hierarchical HMM segmenter that incorporates parts-of- speech (POS) information into the probabili- ty models and generates multiple HMM mod- els for solving segmentation ambiguities.</p><p>This work also evaluated four variant models 9 that perform alternative ways to incorporate the bilingual constraints based on two state-of-the-art graph-based SSL approaches.</p><p>• Self-training Segmenters (STS): two vari- ant models were defined by the approach re- ported in ( <ref type="bibr" target="#b19">Subramanya et al., 2010</ref>) that us- es the supervised CRFs model's decodings, incorporating empirical and constraint infor- mation, for unlabeled examples as additional labeled data to retrain a CRFs model. One variant (STS-NO-GP) skips the GP step, di- rectly decoding with type-level word bound- ary probabilities induced from bitexts, while the other (STS-GP-PL) runs the GP at first and then decodes with GP outcomes. The optimal hyperparameter values were found to be: STS-NO-GP (α = 0.8) and η = 0.6) and STS-GP-PL (µ = 0.5, ρ = 0.3, α = 0.8 and η = 0.6).</p><p>• Virtual Evidences Segmenters (VES): T- wo variant models based on the approach in ( <ref type="bibr" target="#b25">Zeng et al., 2013</ref>) were defined. The type- level word boundary distributions, induced by the character-based alignment (VES-NO- GP), and the graph propagation (VES-GP- PL), are regarded as virtual evidences to bias CRFs model's learning on the unlabeled da- ta. The optimal hyperparameter values were found to be: VES-NO-GP (α = 0.7) and VES-GP-PL (µ = 0.5, ρ = 0.3 and α = 0.7). This outcome validated that the models, trained by either the treebank or the bilin- gual data, performed reasonably well. But they only capture partial segmentation features so that less gains for SMT are achieved when compar- ing to other sophisticated models. Thirdly, we no- tice that the two off-the-shelf models, Stanford and ICTCLAS, just brought minor improvements over the SMS baseline, although they are trained us- ing richer supervisions. This behaviour illustrates that the conventional optimizations to the mono- lingual supervised model, e.g., accumulating more supervised data or predefined segmentation prop- erties, are insufficient to help model for achiev- ing better segmentations for SMT. Finally, high- lighting the five models working with the bilingual constraints, most of them can achieve significant gains over the other ones without using the bilin- gual constraints. This strongly demonstrates that bilingually-learned segmentation knowledge does helps CWS for SMT. The models working with G- P, STS-GP-PL, VES-GP-PL and ours outperform all others. We attribute this to the role of GP in assisting the spread of bilingual knowledge on the Chinese side. Importantly, it can be observed that our model outperforms STS-GP, VES-GP, which greatly supports that joint learning of CRFs and GP can alleviate the error transfer by the pipelined models. This is one of the most crucial findings in this study. Overall, the boldface numbers in the last row illustrate that our model obtains average improvements of 1.89, 1.76 and 1.61 on BLEU, NIST and METEOR over others.  <ref type="table" target="#tab_2">Table 1</ref>: Translation performances (%) on MT-05 testing data by using ten different CWS models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Main Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BLEU NIST METEOR</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis &amp; Discussion</head><p>This section aims to further analyze the three pri- mary observations concluded in Section 4.3: i) word segmentation is useful to SMT; ii) the tree- bank and the bilingual segmentation knowledge are helpful, performing segmentation of differen- t nature; and iii) the bilingual constraints lead to learn segmentations better tailored for SMT. The first observation derives from the compar- isons between the CS baseline and other model- s. Our results, showing the significant CWS ben- efits to SMT, are consistent with the works re- ported in the literature ( <ref type="bibr" target="#b22">Xu et al., 2004;</ref><ref type="bibr" target="#b3">Chang et al., 2008</ref>). In our experiment, two additional evidences found in the translation model are pro- vided to further support that NO tokenization of Chinese (i.e., the CS model's output) could har- m the MT system. First, the SMT phrase extrac- tion, i.e., building "phrases" on top of the char- acter sequences, cannot fully capture all meaning- ful segmentations produced by the CS model. The character based model leads to missing some use- ful longer phrases, and to generate many meaning- less or redundant translations in the phrase table. Moreover, it is affected by translation ambiguities, caused by the cases where a Chinese character has very different meanings in different contextual en- vironments.</p><p>The second observation shifts the emphasis to SMS and UBS, based on the treebank and the bilingual segmentation, respectively. Our result- s show that both segmentation patterns can bring positive effects to MT. Through analyzing both models' segmentations for train MT and test MT , we attempted to get a closer inspection on the seg- mentation preferences and their influence on MT. Our first finding is that the segmentation consen- suses between SMS and UBS are positive to MT. There have about 35% identical segmentations produced by the two models. If these identical segmentations are removed, and the experiments are rerun, the translation scores decrease (on av- erage) by 0.50, 0.85 and 0.70 on BLEU, NIST and METEOR, respectively. Our second finding is that SMS exhibits better segmentation consis- tency than UBS. One representative example is the segmentations for "孤零零 (lonely)". All the out- puts of SMS were "孤零零", while UBS generat- ed three ambiguous segmentations, "孤(alone) 零 零(double zero)", "孤 零(lonely) 零(zero)" and "孤(alone) 零(zero) 零(zero)". The segmentation consistency of SMS rests on the high-quality tree- bank data and the robust CRFs tagging mod- el. On the other hand, the advantage of UB- S is to capture the segmentations matching the aligned target words. For example, UBS grouped "国(country) 际(border) 间(between)" to a word "国 际 间(international)", rather than two word- s "国际(international) 间(between)" (as given by SMS), since these three characters are aligned to a single English word "international". The above analysis shows that SMS and UBS have their own merits and combining the knowledge derived from both segmentations is highly encouraged.</p><p>The third observation concerns the great im- pact of the bilingual constraints to the segmenta- tion models in the MT task. The use of the bilin- gual constraints is the prime objective of this s- tudy. Our first contribution for this purpose is on using the word boundary distributions to cap- ture the bilingual segmentation supervisions. This representation contributes to reduce the negative impacts of erroneous "chars-to-word" alignments. The ambiguous types (having relatively uniform boundary distribution), caused by alignment er- rors, cannot directly bias the model tagging pref- erences. Furthermore, the word boundary distri- butions are convenient to make up the learning constraints over the labelings among various con- strained learning approaches. They have success- fully played in three types of constraints for our experiments: PR penalty (Our model), decoding constraints in self-training (STS) and virtual evi- dences (VES). The second contribution is the use of GP, illustrated by STS-GP-PL, VES-GP-PL and Our model. The major effect is to multiply the im- pacts of the bilingual knowledge through the sim- ilarity graph. The graph vertices (types) <ref type="bibr">10</ref> , with- out any supervisions, can learn the word bound- ary information from their similar types (neigh- borhoods) having the empirical boundary prob- abilities. The segmentations given by the three GP models show about 70% positive segmenta- tion changes, affected by the unlabeled graph ver- tices, with respect to the ones given by the NO- GP models, STS-NO-GP and VES-NO-GP. In our opinion, the learning mechanism of our approach, joint coupling of GP and CRFs, rather than the pipelined one as the other two models, contributes to maximizing the graph smoothness effects to the CRFs estimation so that the error propagation of the pipelined approaches is alleviated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper proposed a novel CWS model for the SMT task. This model aims to maintain the lin- guistic segmentation supervisions from treebank data and simultaneously integrate useful bilingual segmentations induced from the bitexts. This ob- jective is accomplished by three main steps: 1) learn word boundaries from character-based align- ments; 2) encode the learned word boundaries into a GP constraint; and 3) training a CRFs model, un- der the GP constraint, by using the PR framework. The empirical results indicate that the proposed model can yield better segmentations for SMT.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>D c l and D c u (Section 3.2). The final step trains a discriminative sequential labeling model, condi- tional random fields, on D c l and D c u under bilin- gual constraints in a graph propagation expression (Section 3.3). This constrained learning is carried out based on posterior regularization (PR) frame- work (Ganchev et al., 2010).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of similarity graph over character-level trigrams (types).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 summarizes</head><label>1</label><figDesc></figDesc><table>the final MT performance on 
the MT-05 test data, evaluated with ten different 
CWS models. In what follows, we summarized 
four major observations from the results. First-
ly, as expected, having word segmentation does 
help Chinese-to-English MT. All other nine CWS 
models outperforms the CS baseline which does 
not try to identify Chinese words at all. Second-
ly, the other two baselines, SMS and UBS, are on 
a par with each other, showing less than 0.36 av-
erage performance differences on the three eval-
uation metrics. </table></figure>

			<note place="foot" n="1"> The distribution is on four word boundary labels indicating the character positions in a word, i.e., B (begin), M (middle), E (end) and S (single character). 2 A word boundary distribution corresponds to the center character of a type. In fact, it aims at reducing label ambiguities to collect boundary information of character trigrams, rather than individual characters (Altun et al., 2006).</note>

			<note place="foot" n="3"> The readers are refered to the original paper of Ganchev et al. (2010).</note>

			<note place="foot" n="4"> The form of KL term: KL(q||p) = q∈Y q(y) log q(y) p(y) . 5 The original PR setting also requires that the penalty term should be a linear (Ganchev et al., 2010) or non-linear (He et al., 2013) function on q. 6 According to (He et al., 2013), the dual of quadratic program implies an expensive matrix inverse.</note>

			<note place="foot" n="7"> The in-house corpus has been manually validated, in a long process that exceeded 500 hours.</note>

			<note place="foot" n="8"> We evaluated graphs with top k (from 3 to 20) nearest neighbors on development data, and found that the performance converged beyond 10-NNs.</note>

			<note place="foot" n="9"> Note that there are two variant models working with GP. To be fair, the same similarity graph settings introduced in this paper were used.</note>

			<note place="foot" n="10"> This experiment yielded a similarity graph that consists of 11,909,620 types from trainTB and trainMT, where there have 8,593,220 (72.15%) types without any empirical boundary distributions.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors are grateful to the Science and Technology Development Fund of Macau and the Research Committee of the University of Macau (Grant No. MYRG076 (Y1-L2)-FST13-WF and MYRG070 (Y1-L2)-FST12-CS) for the funding support for our research. The work of Isabel Trancoso was supported by national funds through FCT-FundaçFundaç˜Fundação para a Ciêcia e a Tecnolo-gia, under project PEst-OE/EEI/LA0021/2013. The authors also wish to thank the anonymous re-viewers for many helpful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Maximum margin semi-supervised learning for structured variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasemin</forename><surname>Altun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Meteor: An automatic metric for mt evaluation with improved correlation with human judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satanjeev</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization</title>
		<meeting>the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Label propagation and quadratic criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Delalleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Semi-Supervised Learning</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="193" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Optimizing Chinese word segmentation for machine translation performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pi-Chuan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WMT</title>
		<meeting>WMT</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="224" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised tokenization for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tagyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="718" to="726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised part-of-speech tagging with bilingual graph-based projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="600" to="609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Graph-based lexicon expansion with sparsity-inducing penalties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="677" to="687" />
		</imprint>
	</monogr>
	<note>Proceedings of NAACL</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The nist speaker recognition evaluation-overview, methodology, systems, results, perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">R</forename><surname>Doddington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Przybocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><forename type="middle">F</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="225" to="254" />
		</imprint>
	</monogr>
	<note>Speech Communication</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Posterior regularization for structured latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jõao</forename><surname>Graça</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Gillenwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2001" to="2049" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graph-based posterior regularization for semi-supervised structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Gillenwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL, page 38. Association for Computational Linguistics</title>
		<meeting>CoNLL, page 38. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL on Interactive Poster and Demonstration Sessions</title>
		<meeting>ACL on Interactive Poster and Demonstration Sessions</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bilingually motivated domain-adapted word segmentation for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Way</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="549" to="557" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generalized expectation criteria for semi-supervised learning of conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gideon</forename><forename type="middle">S</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="870" to="878" />
		</imprint>
	</monogr>
	<note>Proceedings of ACL</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Generalized expectation criteria</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gideon</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Druck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<pubPlace>Amherst, MA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Computer Science Technical Note, University of Massachusetts</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A systematic comparison of various statistical alignment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="51" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Minimum error rate training in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Integration of multiple bilingually-trained segmentation schemes into statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Finch</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumita</forename><surname>Eiichiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE Transactions on Information and Systems</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="690" to="697" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">SRILM-an extensible language modeling toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient graph-based semisupervised learning of structured tagging models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amarnag</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="167" to="176" />
		</imprint>
	</monogr>
	<note>Proceedings of EMNLP</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">UM-Corpus: A large English-Chinese parallel corpus for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">F</forename><surname>Liang Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidia</forename><forename type="middle">S</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paulo</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Quaresma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC. European Language Resources Association</title>
		<meeting>LREC. European Language Resources Association</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Enhancing statistical machine translation with character alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangchao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="285" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Do we need Chinese word segmentation for statistical machine translation?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third SIGHAN Workshop on Chinese Language Learning</title>
		<meeting>the Third SIGHAN Workshop on Chinese Language Learning</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="122" to="128" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Integrated Chinese word segmentation in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Matusov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="216" to="223" />
		</imprint>
	</monogr>
	<note>Proceedings of IWSLT</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The Penn Chinese TreeBank: Phrase structure annotation of a large corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-Dong</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="238" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Graph-based semi-supervised model for joint Chinese word segmentation and partof-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidia</forename><forename type="middle">S</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="770" to="779" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Isabel Trancoso, Liangye He, and Qiuping Huang. 2014. Lexicon expansion for latent variable grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidia</forename><forename type="middle">S</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="47" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">HHMM-based Chinese lexical analyzer ICTCLAS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua-Ping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Kui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De-Yi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second SIGHAN Workshop on Chinese Language Processing</title>
		<meeting>the Second SIGHAN Workshop on Chinese Language Processing</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="184" to="187" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improved statistical machine translation by multiple Chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keiji</forename><surname>Yasuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="216" to="223" />
		</imprint>
	</monogr>
	<note>Proceedings of WMT</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An improved Chinese word segmentation system with conditional random field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Ning</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing</title>
		<meeting>the Fifth SIGHAN Workshop on Chinese Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An empirical study on word segmentation for Chinese machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoliang</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Linguistics and Intelligent Text Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="248" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised chunking based on graph propagation from bilingual corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidia</forename><forename type="middle">S</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Scientific World Journal</title>
		<imprint>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
