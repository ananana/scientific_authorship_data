<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:56+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Affect-LM: A Neural Language Model for Customizable Affective Text Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sayan</forename><surname>Ghosh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Chollet</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Laksana</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Scherer</surname></persName>
						</author>
						<title level="a" type="main">Affect-LM: A Neural Language Model for Customizable Affective Text Generation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="634" to="642"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1059</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Human verbal communication includes affective messages which are conveyed through use of emotionally colored words. There has been a lot of research in this direction but the problem of integrating state-of-the-art neural language models with affective information remains an area ripe for exploration. In this paper, we propose an extension to an LSTM (Long Short-Term Memory) language model for generating conversational text, conditioned on affect categories. Our proposed model, Affect-LM enables us to customize the degree of emotional content in generated sentences through an additional design parameter. Perception studies conducted using Amazon Mechanical Turk show that Affect-LM generates naturally looking emotional sentences without sacrificing grammatical correctness. Affect-LM also learns affect-discriminative word representations, and perplexity experiments show that additional affective information in conversational text can improve language model prediction.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>morency@cs.cmu.edu</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Human verbal communication includes affective messages which are conveyed through use of emotionally colored words. There has been a lot of research in this direction but the problem of integrat- ing state-of-the-art neural language mod- els with affective information remains an area ripe for exploration. In this paper, we propose an extension to an LSTM (Long Short-Term Memory) lan- guage model for generating conversa- tional text, conditioned on affect cate- gories. Our proposed model, Affect-LM enables us to customize the degree of emotional content in generated sentences through an additional design parameter. Perception studies conducted using Ama- zon Mechanical Turk show that Affect- LM generates naturally looking emotional sentences without sacrificing grammatical correctness. Affect-LM also learns affect- discriminative word representations, and perplexity experiments show that addi- tional affective information in conversa- tional text can improve language model prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Affect is a term that subsumes emotion and longer term constructs such as mood and personality and refers to the experience of feeling or emo- tion ( <ref type="bibr" target="#b22">Scherer et al., 2010)</ref>. <ref type="bibr" target="#b21">Picard (1997)</ref> provides a detailed discussion of the importance of affect analysis in human communication and interaction. Within this context the analysis of human affect from text is an important topic in natural language understanding, examples of which include senti- ment analysis from Twitter ( <ref type="bibr" target="#b18">Nakov et al., 2016)</ref>, affect analysis from poetry (Kao and Jurafsky, e t1 c t1</p><p>Figure 1: Affect-LM is capable of generating emotionally colored conversational text in five specific affect categories (e t−1 ) with varying affect strengths (β). Three generated example sentences for happy affect category are shown in three distinct affect strengths.</p><p>2012) and studies of correlation between function words and social/psychological processes <ref type="bibr" target="#b19">(Pennebaker, 2011</ref>). People exchange verbal messages which not only contain syntactic information, but also information conveying their mental and emo- tional states. Examples include the use of emo- tionally colored words (such as furious and joy) and swear words. The automated processing of affect in human verbal communication is of great importance to understanding spoken language sys- tems, particularly for emerging applications such as dialogue systems and conversational agents.</p><p>Statistical language modeling is an integral component of speech recognition systems, with other applications such as machine translation and information retrieval. There has been a resur- gence of research effort in recurrent neural net- works for language modeling <ref type="bibr" target="#b16">(Mikolov et al., 2010)</ref>, which have yielded performances far supe- rior to baseline language models based on n-gram approaches. However, there has not been much effort in building neural language models of text that leverage affective information. Current liter- ature on deep learning for language understand- ing focuses mainly on representations based on word semantics <ref type="bibr" target="#b17">(Mikolov et al., 2013)</ref>, encoder- decoder models for sentence representations <ref type="bibr" target="#b4">(Cho et al., 2015)</ref>, language modeling integrated with symbolic knowledge <ref type="bibr" target="#b0">(Ahn et al., 2016</ref>) and neural caption generation ( <ref type="bibr" target="#b25">Vinyals et al., 2015</ref>), but to the best of our knowledge there has been no work on augmenting neural language modeling with affec- tive information, or on data-driven approaches to generate emotional text.</p><p>Motivated by these advances in neural language modeling and affective analysis of text, in this pa- per we propose a model for representation and generation of emotional text, which we call the Affect-LM. Our model is trained on conversational speech corpora, common in language modeling for speech recognition applications ( <ref type="bibr" target="#b3">Bulyko et al., 2007)</ref>. <ref type="figure">Figure 1</ref> provides an overview of our Affect-LM and its ability to generate emotionally colored conversational text in a number of affect categories with varying affect strengths. While these parameters can be manually tuned to gener- ate conversational text, the affect category can also be automatically inferred from preceding context words. Specifically for model training, the affect category is derived from features generated using keyword spotting from a dictionary of emotional words, such as the LIWC (Linguistic Inquiry and Word Count) tool <ref type="bibr" target="#b20">(Pennebaker et al., 2001</ref>). Our primary research questions in this paper are: Q1:Can Affect-LM be used to generate affective sentences for a target emotion with varying de- grees of affect strength through a customizable model parameter? Q2:Are these generated sentences rated as emo- tionally expressive as well as grammatically cor- rect in an extensive crowd-sourced perception ex- periment? Q3:Does the automatic inference of affect cate- gory from the context words improve language modeling performance of the proposed Affect-LM over the baseline as measured by perplexity?</p><p>The remainder of this paper is organized as fol- lows. In Section 2, we discuss prior work in the fields of neural language modeling, and generation of affective conversational text. In Section 3 we describe the baseline LSTM model and our pro- posed Affect-LM model. Section 4 details the ex- perimental setup, and in Section 5, we discuss re- sults for customizable emotional text generation, perception studies for each affect category, and perplexity improvements over the baseline model before concluding the paper in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Language modeling is an integral component of spoken language systems, and traditionally n- gram approaches have been used <ref type="bibr" target="#b23">(Stolcke et al., 2002</ref>) with the shortcoming that they are unable to generalize to word sequences which are not in the training set, but are encountered in unseen data. <ref type="bibr" target="#b1">Bengio et al. (2003)</ref> proposed neural language models, which address this shortcoming by gen- eralizing through word representations. <ref type="bibr" target="#b16">Mikolov et al. (2010)</ref> and <ref type="bibr" target="#b24">Sundermeyer et al. (2012)</ref> extend neural language models to a recurrent architecture, where a target word w t is predicted from a con- text of all preceding words w 1 , w 2 , ..., w t−1 with an LSTM (Long Short-Term Memory) neural net- work. There also has been recent effort on build- ing language models conditioned on other modali- ties or attributes of the data. For example, <ref type="bibr" target="#b25">Vinyals et al. (2015)</ref> introduced the neural image caption generator, where representations learnt from an in- put image by a CNN (Convolutional Neural Net- work) are fed to an LSTM language model to gen- erate image captions. <ref type="bibr" target="#b12">Kiros et al. (2014)</ref> used an LBL model (Log-Bilinear language model) for two applications -image retrieval given sentence queries, and image captioning. Lower perplexity was achieved on text conditioned on images rather than language models trained only on text.</p><p>In contrast, previous literature on affective lan- guage generation has not focused sufficiently on customizable state-of-the-art neural network tech- niques to generate emotional text, nor have they quantitatively evaluated their models on multiple emotionally colored corpora. Mahamood and Re- iter (2011) use several NLG (natural language gen- eration) strategies for producing affective medi- cal reports for parents of neonatal infants under- going healthcare. While they study the difference between affective and non-affective reports, their work is limited only to heuristic based systems and do not include conversational text. <ref type="bibr" target="#b14">Mairesse and Walker (2007)</ref> developed PERSONAGE, a sys- tem for dialogue generation conditioned on ex- traversion dimensions. They trained regression models on ground truth judge's selections to au- tomatically determine which of the sentences se- lected by their model exhibit appropriate extrover- sion attributes. In <ref type="bibr" target="#b11">Keshtkar and Inkpen (2011)</ref>, the authors use heuristics and rule-based approaches for emotional sentence generation. Their gener- ation system is not training on large corpora and they use additional syntactic knowledge of parts of speech to create simple affective sentences. In contrast, our proposed approach builds on state-of- the-art approaches for neural language modeling, utilizes no syntactic prior knowledge, and gener- ates expressive emotional text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">LSTM Language Model</head><p>Prior to providing a formulation for our pro- posed model, we briefly describe a LSTM lan- guage model. We have chosen this model as a baseline since it has been reported to achieve state-of-the-art perplexities compared to other ap- proaches, such as n-gram models with Kneser-Ney smoothing ( <ref type="bibr" target="#b9">Jozefowicz et al., 2016)</ref>. Unlike an ordinary recurrent neural network, an LSTM net- work does not suffer from the vanishing gradient problem which is more pronounced for very long sequences <ref type="bibr" target="#b8">(Hochreiter and Schmidhuber, 1997)</ref>. Formally, by the chain rule of probability, for a sequence of M words w 1 , w 2 , ..., w M , the joint probability of all words is given by:</p><formula xml:id="formula_0">P (w 1 , w 2 , ..., w M ) = t=M t=1 P (w t |w 1 , w 2 , ...., w t−1 )</formula><p>(1) If the vocabulary consists of V words, the condi- tional probability of word w t as a function of its context c t−1 = (w 1 , w 2 , ...., w t−1 ) is given by:</p><formula xml:id="formula_1">P (w t = i|c t−1 ) = exp(U i T f (c t−1 ) + b i ) V j=1 exp(U j T f (c t−1 ) + b j ) (2) f (.)</formula><p>is the output of an LSTM network which takes in the context words w 1 , w 2 , ..., w t−1 as in- puts through one-hot representations, U is a ma- trix of word representations which on visualiza- tion we have found to correspond to POS (Part of Speech) information, while b i is a bias term cap- turing the unigram occurrence of word i. Equa- tion 2 expresses the word w t as a function of its context for a LSTM language model which does not utilize any additional affective information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Proposed Model: Affect-LM</head><p>The proposed model Affect-LM has an additional energy term in the word prediction, and can be de- scribed by the following equation:</p><formula xml:id="formula_2">P (w t = i|c t−1 , e t−1 ) = exp (U i T f (c t−1 ) + βV i T g(e t−1 ) + b i ) V j=1 exp(U j T f (c t−1 ) + βV j T g(e t−1 ) + b j )<label>(3)</label></formula><p>e t−1 is an input vector which consists of affect category information obtained from the words in the context during training, and g(.) is the output of a network operating on e t−1 .V i is an embed- ding learnt by the model for the i-th word in the vocabulary and is expected to be discriminative of the affective information conveyed by each word. In <ref type="figure">Figure 4</ref> we present a visualization of these af- fective representations.</p><p>The parameter β defined in Equation 3, which we call the affect strength defines the influence of the affect category information (frequency of emotionally colored words) on the overall predic- tion of the target word w t given its context. We can consider the formulation as an energy based model (EBM), where the additional energy term captures the degree of correlation between the pre- dicted word and the affective input (Bengio et al., 2003).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Descriptors for Affect Category Information</head><p>Our proposed model learns a generative model of the next word w t conditioned not only on the pre- vious words w 1 , w 2 , ..., w t−1 but also on the af- fect category e t−1 which is additional informa- tion about emotional content. During model train- ing, the affect category is inferred from the con- text data itself. Thus we define a suitable feature extractor which can utilize an affective lexicon to infer emotion in the context. presence or absence of a specific emotion, which is obtained by binary thresholding of the features extracted from LIWC. For example, the affective representation of the sentence i will fight in the war is e t−1 ={"sad":0, "angry":1, "anxiety":0, "neg- ative emotion":1, "positive emotion":0}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Affect-LM for Emotional Text Generation</head><p>Affect-LM can be used to generate sentences con- ditioned on the input affect category, the affect strength β, and the context words. For our exper- iments, we have chosen the following affect cate- gories -positive emotion, anger, sad, anxiety, and negative emotion (which is a superclass of anger, sad and anxiety). As described in Section 3.2, the affect strength β defines the degree of dominance of the affect-dependent energy term on the word prediction in the language model, consequently af- ter model training we can change β to control the degree of how "emotionally colored" a generated utterance is, varying from β = 0 (neutral; base- line model) to β = ∞ (the generated sentences only consist of emotionally colored words, with no grammatical structure). When Affect-LM is used for generation, the af- fect categories could be either (1) inferred from the context using LIWC (this occurs when we provide sentence beginnings which are emotion- ally colored themselves), or (2) set to an input emotion descriptor e (this is obtained by setting e to a binary vector encoding the desired emo- tion and works even for neutral sentence begin- nings). Given an initial starting set of M words w 1 , w 2 , ..., w M to complete, affect strength β, and the number of words N to generate each i- th generated word is obtained by sampling from P (w i |w 1 , w 2 , ..., w i−1 , e; β) for i ∈ {M + 1, M + 2, ..., M + N }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>In Section 1, we have introduced three primary research questions related to the ability of the proposed Affect-LM model to generate emotion- ally colored conversational text without sacrific- ing grammatical correctness, and to obtain lower perplexity than a baseline LSTM language model when evaluated on emotionally colored corpora. In this section, we discuss our experimental setup to address these questions, with a description of Affect-LM's architecture and the corpora used for training and evaluating the language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Speech Corpora</head><p>The Fisher English Training Speech Corpus is the main corpus used for training the proposed model, in addition to which we have chosen three emo- tionally colored conversational corpora. A brief description of each corpus is given below, and in <ref type="table" target="#tab_0">Table 1</ref>, we report relevant statistics, such as the total number of words, along with the fraction of emotionally colored words (those belonging to the LIWC affective word categories) in each corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fisher English Training Speech Parts 1 &amp; 2:</head><p>The Fisher dataset ( <ref type="bibr" target="#b5">Cieri et al., 2004</ref>) consists of speech from telephonic conversations of 10 min- utes each, along with their associated transcripts. Each conversation is between two strangers who are requested to speak on a randomly selected topic from a set. Examples of conversation top- ics are Minimum Wage, Time Travel and Comedy. Distress Assessment Interview Corpus (DAIC): The DAIC corpus introduced by <ref type="bibr" target="#b7">Gratch (2014)</ref> consists of 70+ hours of dyadic interviews be- tween a human subject and a virtual human, where the virtual human asks questions designed to di- agnose symptoms of psychological distress in the subject such as depression or PTSD (Post Trau- matic Stress Disorder).  where in each video a speaker expresses his opinion on a commercial product. The cor- pus consist of speech from 93 videos from 89 dis- tinct speakers (41 male and 48 female speakers). This corpus differs from the others since it con- tains monologues rather than conversations.</p><p>While we find that all corpora contain spoken language, they have the following characteristics different from the Fisher corpus: (1) More emo- tional content as observed in <ref type="table" target="#tab_0">Table 1</ref>, since they have been generated through a human subject's spontaneous replies to questions designed to gen- erate an emotional response, or from conversa- tions on emotion-inducing topics (2) Domain mis- match due to recording environment (for example, the DAIC corpus was created in a mental health setting, while the CMU-MOSI corpus consisted of opinion videos uploaded online). (3) Significantly smaller than the Fisher corpus, which is 25 times the size of the other corpora combined. Thus, we perform training in two separate stages -training of the baseline and Affect-LM models on the Fisher corpus, and subsequent adaptation and fine-tuning on each of the emotionally colored corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Affect-LM Neural Architecture</head><p>For our experiments, we have implemented a baseline LSTM language model in Ten- sorflow ( <ref type="bibr" target="#b6">Abadi et al., 2016)</ref>, which follows the non-regularized implementation as described in <ref type="bibr">Zaremba et al. (2014)</ref> and to which we have added a separate energy term for the affect cate- gory in implementing Affect-LM. We have used a vocabulary of 10000 words and an LSTM network with 2 hidden layers and 200 neurons per hidden layer. The network is unrolled for 20 time steps, and the size of each minibatch is 20. The affect category e t−1 is processed by a multi-layer per- ceptron with a single hidden layer of 100 neurons and sigmoid activation function to yield g(e t−1 ). We have set the output layer size to 200 for both f (c t−1 ) and g(e t−1 ). We have kept the network architecture constant throughout for ease of com- parison between the baseline and Affect-LM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Language Modeling Experiments</head><p>Affect-LM can also be used as a language model where the next predicted word is estimated from the words in the context, along with an affect cate- gory extracted from the context words themselves (instead of being encoded externally as in gener- ation). To evaluate whether additional emotional information could improve the prediction perfor- mance, we train the corpora detailed in Section 4.1 in two stages as described below: (1) Training and validation of the language models on Fisher dataset-The Fisher corpus is split in a 75:15:10 ratio corresponding to the train- ing, validation and evaluation subsets respectively, and following the implementation in <ref type="bibr">Zaremba et al. (2014)</ref>, we train the language models (both the baseline and Affect-LM) on the training split for 13 epochs, with a learning rate of 1.0 for the first four epochs, and the rate decreasing by a fac- tor of 2 after every subsequent epoch. The learn- ing rate and neural architecture are the same for all models. We validate the model over the affect strength β ∈ [1.0, 1.5, 1.75, 2.0, 2.25, 2.5, 3.0]. The best performing model on the Fisher valida- tion set is chosen and used as a seed for subsequent adaptation on the emotionally colored corpora. (2) Fine-tuning the seed model on other cor- pora-Each of the three corpora -CMU-MOSI, DAIC and SEMAINE are split in a 75:15:10 ratio to create individual training, validation and eval- uation subsets. For both the baseline and Affect- LM, the best performing model from Stage 1 (the seed model) is fine-tuned on each of the train- ing corpora, with a learning rate of 0.25 which is constant throughout, and a validation grid of β ∈ [1.0, 1.5, 1.75, 2.0]. For each model adapted on a corpus, we compare the perplexities obtained by Affect-LM and the baseline model when evalu- ated on that corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Sentence Generation Perception Study</head><p>We assess Affect-LM's ability to generate emo- tionally colored text of varying degrees without severely deteriorating grammatical correctness, by conducting an extensive perception study on Ama- zon's Mechanical Turk (MTurk) platform. The MTurk platform has been successfully used in the past for a wide range of perception experi- ments and has been shown to be an excellent re- source to collect human ratings for large stud- ies <ref type="bibr" target="#b2">(Buhrmester et al., 2011</ref>). Specifically, we generated more than 200 sentences for four sen- tence beginnings (namely the three sentence be- ginnings listed in <ref type="table">Table 2</ref> as well as an end of sentence token indicating that the model should generate a new sentence) in five affect categories happy(positive emotion), angry, sad, anxiety, and negative emotion. The Affect-LM model trained</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Beginning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Affect Category</head><p>Completed sentence I feel so Happy good because i think that it's important to have a relationship with a friend Angry bad that i hate it and i hate that because they they kill themselves and then they fight Sad sad to miss because i i miss the feelings of family members who i lost feelings with Anxious horrible i mean i think when we're going to you know war and alert alert and we're actually gonna die Neutral bad if i didn't know that the decision was going on I told him to Happy be honest and i said well i hope that i 'm going to be a better person Angry see why he was fighting with my son Sad leave the house because i hurt one and i lost his leg and hurt him Anxious be afraid of him and he he just he just didn't care about the death penalty Neutral do this position i think he is he's got a lot of money he has to pay himself a lot of money Why did you Happy have a best friend Angry say it was only a criminal being killed at a war or something Sad miss your feelings Anxious worry about fear factor Neutral believe in divorce <ref type="table">Table 2</ref>: Example sentences generated by the model conditioned on different affect categories on the Fisher corpus was used for sentence gen- eration. Each sentence was evaluated by two hu- man raters that have a minimum approval rating of 98% and are located in the United States. The human raters were instructed that the sentences should be considered to be taken from a conver- sational rather than a written context: repetitions and pause fillers (e.g., um, uh) are common and no punctuation is provided. The human raters evaluated each sentence on a seven-point Likert scale for the five affect categories, overall affec- tive valence as well as the sentence's grammati- cal correctness and were paid 0.05USD per sen- tence. We measured inter-rater agreement using Krippendorffs α and observed considerable agree- ment between raters across all categories (e.g., for valence α = 0.510 and grammatical correctness α = 0.505). For each target emotion (i.e., intended emo- tion of generated sentences) we conducted an ini- tial MANOVA, with human ratings of affect cat- egories the DVs (dependent variables) and the affect strength parameter β the IV (independent variable). We then conducted follow-up univariate ANOVAs to identify which DV changes signifi- cantly with β. In total we conducted 5 MANOVAs and 30 follow-up ANOVAs, which required us to update the significance level to p&lt;0.001 following a Bonferroni correction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Generation of Emotional Text</head><p>In Section 3.4 we have described the process of sampling text from the model conditioned on in- put affective information (research question Q1). <ref type="table">Table 2</ref> shows three sentences generated by the model for input sentence beginnings I feel so ..., Why did you ... and I told him to ... for each of five affect categories -happy(positive emotion), angry, sad anxiety, and neutral(no emotion). They have been selected from a pool of 20 generated sen- tences for each category and sentence beginning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">MTurk Perception Experiments</head><p>In the following we address research question Q2 by reporting the main statistical findings of our MTurk study, which are visualized in <ref type="figure" target="#fig_4">Figures 2  and 3</ref>.</p><p>Positive Emotion Sentences.</p><p>The multi- variate result was significant for positive emo- tion generated sentences (Pillai's Trace=.327, F(4,437)=6.44, p&lt;.0001). Follow up ANOVAs revealed significant results for all DVs except an- gry with p&lt;.0001, indicating that both affective valence and happy DVs were successfully manip- ulated with β, as seen in <ref type="figure" target="#fig_4">Figure 2(a)</ref>. Grammat- ical correctness was also significantly influenced by the affect strength parameter β and results show that the correctness deteriorates with increasing β (see <ref type="figure" target="#fig_5">Figure 3)</ref>. However, a post-hoc Tukey test revealed that only the highest β value shows a sig- nificant drop in grammatical correctness at p&lt;.05.</p><p>Negative Emotion Sentences. The multi- variate result was significant for negative emo- tion generated sentences (Pillai's Trace=.130, F(4,413)=2.30, p&lt;.0005). Follow up ANOVAs revealed significant results for affective valence and happy DVs with p&lt;.0005, indicating that the affective valence DV was successfully manipu- lated with β, as seen in <ref type="figure" target="#fig_4">Figure 2(b)</ref>. Further, as intended there were no significant differences for DVs angry, sad and anxious, indicating that the negative emotion DV refers to a more gen- eral affect related concept rather than a specific negative emotion. This finding is in concordance with the intended LIWC category of negative af- fect that forms a parent category above the more  specific emotions, such as angry, sad, and anxious ( <ref type="bibr" target="#b20">Pennebaker et al., 2001</ref>). Grammatical correct- ness was also significantly influenced by the affect strength β and results show that the correctness de- teriorates with increasing β (see <ref type="figure" target="#fig_5">Figure 3</ref>). As for positive emotion, a post-hoc Tukey test revealed that only the highest β value shows a significant drop in grammatical correctness at p&lt;.05. Angry Sentences. The multivariate result was significant for angry generated sentences <ref type="bibr">(Pillai's Trace=.199</ref>, F(4,433)=3.76, p&lt;.0001). Follow up ANOVAs revealed significant results for affec- tive valence, happy, and angry DVs with p&lt;.0001, indicating that both affective valence and angry DVs were successfully manipulated with β, as seen in <ref type="figure" target="#fig_4">Figure 2</ref>(c). Grammatical correctness was not significantly influenced by the affect strength parameter β, which indicates that angry sentences are highly stable across a wide range of β (see <ref type="figure">Fig- ure</ref> 3). However, it seems that human raters could not successfully distinguish between angry, sad, and anxious affect categories, indicating that the generated sentences likely follow a general nega- tive affect dimension. Sad Sentences. The multivariate result was significant for sad generated sentences (Pillai's Trace=.377, F(4,425)=7.33, p&lt;.0001). Follow up ANOVAs revealed significant results only for the sad DV with p&lt;.0001, indicating that while the sad DV can be successfully manipulated with β, as seen in <ref type="figure" target="#fig_4">Figure 2(d)</ref>. The grammatical cor- rectness deteriorates significantly with β. Specifi- cally, a post-hoc Tukey test revealed that only the two highest β values show a significant drop in grammatical correctness at p&lt;.05 (see <ref type="figure" target="#fig_5">Figure 3</ref>). A post-hoc Tukey test for sad reveals that β = 3 is optimal for this DV, since it leads to a significant jump in the perceived sadness scores at p&lt;.005 for β ∈ {0, 1, 2}. Anxious Sentences. The multivariate result was significant for anxious generated sentences (Pillai's Trace=.289, F(4,421)=6.44, p&lt;.0001). Follow up ANOVAs revealed significant results for affective valence, happy and anxious DVs with p&lt;.0001, indicating that both affective valence and anxiety DVs were successfully manipulated with β, as seen in <ref type="figure" target="#fig_4">Figure 2</ref>(e). Grammatical correctness was also significantly influenced by the affect strength parameter β and results show that the correctness deteriorates with increasing β. Similarly for sad, a post-hoc Tukey test revealed that only the two highest β values show a signif- icant drop in grammatical correctness at p&lt;.05 (see <ref type="figure" target="#fig_5">Figure 3)</ref>. Again, a post-hoc Tukey test for anxious reveals that β = 3 is optimal for this DV, since it leads to a significant jump in the perceived  <ref type="table">Table 3</ref>: Evaluation perplexity scores obtained by the baseline and Affect-LM models when trained on Fisher and subsequently adapted on DAIC, SEMAINE and CMU-MOSI corpora anxiety scores at p&lt;.005 for β ∈ {0, 1, 2}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a) Positive Emotion (b) Negative Emotion (c) Angry (d) Sad (e) Anxious</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Language Modeling Results</head><p>In <ref type="table">Table 3</ref>, we address research question Q3 by presenting the perplexity scores obtained by the baseline model and Affect-LM, when trained on the Fisher corpus and subsequently adapted on three emotional corpora (each adapted model is indi- vidually trained on CMU-MOSI, DAIC and SE- MAINE). The models trained on Fisher are eval- uated on all corpora while each adapted model is evaluated only on it's respective corpus. For all corpora, we find that Affect-LM achieves lower perplexity on average than the baseline model, im- plying that affect category information obtained from the context words improves language model prediction. The average perplexity improvement is 1.44 (relative improvement 1.94%) for the model trained on Fisher, while it is 0.79 (1.31%) for the adapted models. We note that larger improve- ments in perplexity are observed for corpora with higher content of emotional words. This is sup- ported by the results in <ref type="table">Table 3</ref>, where Affect- LM obtains a larger reduction in perplexity for the CMU-MOSI and SEMAINE corpora, which re- spectively consist of 2.76% and 2.75% more emo- tional words than the Fisher corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Word Representations</head><p>In Equation 3, Affect-LM learns a weight ma- trix V which captures the correlation between the predicted word w t , and the affect category e t−1 . Thus, each row of the matrix V i is an emotion- ally meaningful embedding of the i-th word in the vocabulary. In <ref type="figure">Figure 4</ref>, we present a t-SNE vi- sualization of these embeddings, where each data point is a separate word, and words which ap- pear in the LIWC dictionary are colored based on which affect category they belong to (we have la- beled only words in categories positive emotion, negative emotion, anger, sad and anxiety since <ref type="figure">Figure 4</ref>: Embeddings learnt by Affect-LM these categories contain the most frequent words). Words colored grey are those not in the LIWC dictionary. In <ref type="figure">Figure 4</ref>, we observe that the em- beddings contain affective information, where the positive emotion is highly separated from the neg- ative emotions (sad, angry, anxiety) which are clustered together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>In this paper, we have introduced a novel language model Affect-LM for generating affective conver- sational text conditioned on context words, an af- fective category and an affective strength parame- ter. MTurk perception studies show that the model can generate expressive text at varying degrees of emotional strength without affecting grammatical correctness. We also evaluate Affect-LM as a lan- guage model and show that it achieves lower per- plexity than a baseline LSTM model when the af- fect category is obtained from the words in the context. For future work, we wish to extend this model by investigating language generation con- ditioned on other modalities such as facial images and speech, and to applications such as dialogue generation for virtual agents.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>SEMAINE dataset: SEMAINE (McKeown et al., 2012) is a large audiovisual corpus consisting of interactions between subjects and an operator simulating a SAL (Sensitive Artificial Listener). There are a total of 959 conversations which are approximately 5 minutes each, and are transcribed and annotated with affective dimensions. Multimodal Opinion-level Sentiment Intensity Dataset (CMU-MOSI): (Zadeh et al., 2016) This is a multimodal annotated corpus of opinion</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>videos</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Amazon Mechanical Turk study results for generated sentences in the target affect categories positive emotion, negative emotion, angry, sad, and anxious (a)-(e). The most relevant human rating curve for each generated emotion is highlighted in red, while less relevant rating curves are visualized in black. Affect categories are coded via different line types and listed in legend below figure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Mechanical Turk study results for grammatical correctness for all generated target emotions. Perceived grammatical correctness for each affect categories are color-coded.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : Summary of corpora used in this paper. CMU-MOSI and SEMAINE are observed to have higher emotional content than Fisher and DAIC corpora.</head><label>1</label><figDesc></figDesc><table>For our experiments, 
</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This material is based upon work supported by the U.S. Army Research Laboratory under con-tract number W911NF-14-D-0005. Any opinions, findings, and conclusions or recommendations ex-pressed in this material are those of the author(s) and do not necessarily reflect the views of the Government, and no official endorsement should be inferred. Sayan Ghosh also acknowledges the Viterbi Graduate School Fellowship for funding his graduate studies.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heeyoul</forename><surname>Sungjin Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanel</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Pärnamaa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.00318</idno>
		<title level="m">A neural knowledge language model</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Amazon&apos;s mechanical turk a new source of inexpensive, yet high-quality, data? Perspectives on psychological science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Buhrmester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tracy</forename><surname>Kwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gosling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="3" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Web resources for language modeling in conversational speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Bulyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manhung</forename><surname>Siu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Speech and Language Processing (TSLP)</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Describing multimedia content using attention-based encoder-decoder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1875" to="1886" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The fisher corpus: a resource for the next generations of speech-to-text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Cieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="69" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI). Savannah, Georgia</title>
		<meeting>the 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI). Savannah, Georgia<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The distress analysis interview corpus of human and computer interviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC. Citeseer</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3123" to="3128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Exploring the limits of language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02410</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A computational analysis of style, affect, and imagery in contemporary poetry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justine</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A patternbased model for generating text to express emotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fazel</forename><surname>Keshtkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Affective Computing and Intelligent Interaction</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="11" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard S</forename><surname>Zemel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generating affective natural language for parents of neonatal infants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saad</forename><surname>Mahamood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th European Workshop on Natural Language Generation</title>
		<meeting>the 13th European Workshop on Natural Language Generation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="12" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Personage: Personality generation for dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Mairesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marilyn</forename><surname>Walker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The semaine database: Annotated multimodal records of emotionally colored conversations between a person and a limited agent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roddy</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Schroder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="17" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2010-01" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>Cernock`Cernock`y, and Sanjeev Khudanpur</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Rosenthal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<title level="m">Semeval2016 task 4: Sentiment analysis in twitter. Proceedings of SemEval</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The secret life of pronouns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>James W Pennebaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New Scientist</title>
		<imprint>
			<biblScope unit="volume">211</biblScope>
			<biblScope unit="page" from="42" to="45" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Linguistic inquiry and word count: Liwc</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><forename type="middle">E</forename><surname>James W Pennebaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger J</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Booth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mahway: Lawrence Erlbaum Associates</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Affective computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosalind</forename><surname>Picard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>MIT press Cambridge</publisher>
			<biblScope unit="volume">252</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A Blueprint for Affective Computing: A sourcebook and manual</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanja</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Bänziger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roesch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Srilm-an extensible language modeling toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">2002</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Lstm neural networks for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="194" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Pincus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louisphilippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="88" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<title level="m">Ilya Sutskever, and Oriol Vinyals. 2014. Recurrent neural network regularization</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
