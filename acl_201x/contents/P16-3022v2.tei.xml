<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:04+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Personalized Markov Clustering and Deep Learning Approach for Arabic Text Categorization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasu</forename><surname>Jindal</surname></persName>
							<email>vasu.jindal@utdallas.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Texas at Dallas Richardson</orgName>
								<address>
									<postCode>75080</postCode>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Personalized Markov Clustering and Deep Learning Approach for Arabic Text Categorization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Text categorization has become a key research field in the NLP community. However , most works in this area are focused on Western languages ignoring other Semitic languages like Arabic. These languages are of immense political and social importance necessitating robust cate-gorization techniques. In this paper, we present a novel three-stage technique to efficiently classify Arabic documents into different categories based on the words they contain. We leverage the significance of root-words in Arabic and incorporate a combination of Markov clustering and Deep Belief Networks to classify Arabic words into separate groups (clusters). Our approach is tested on two public datasets giving a F-Measure of 91.02%.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the emerging era of big data technology, there has been a widespread increase in information ob- tained from text documents. Furthermore, with the rapid availability of machine-readable docu- ments, text classification techniques have gained tremendous interest during the recent years. Con- sequently, automatic categorization of numerous new documents to different categories has become critical for political, social and for news research purposes.</p><p>Text categorization techniques have been widely investigated by researchers around the world. However, most of the recent developments in this field are focused on popular Western lan- guages, ignoring Semitic and Middle-Eastern lan- guages like Arabic, Hebrew, Urdu and Persian. As discussed further in related works in Section 2.1, most classification algorithms utilized English and Chinese to validate their methods while works on Arabic are extremely rare. This is primarily due to significant dialect differences between these languages and their complex morphology. Ad- ditionally, the presence of various inflections in Arabic as opposed to English makes it difficult for the NLP community to validate techniques on the popular Middle Eastern languages. According to the US Department of Cultural Affairs, Arabic and Urdu are categorized as critical languages and United Nations heavily emphasizes on the social and political importance of these languages. Ara- bic is listed as one of the six official languages of the United Nations.</p><p>In this paper, we present a novel three stage approach to classify Arabic text documents into different categories combining Markov Cluster- ing, Fuzzy-C-means and Deep Learning. To the best of our knowledge, this is the first work that leverages the heavy influence of root-words in Arabic to extract features for both clustering and deep learning to perform classification of Ara- bic documents. First, we segment each document and extract root-word information. Then we per- form clustering with root-word based features us- ing Fuzzy-C-Means and Markov clustering. This allows us to separate documents into unsupervised groups (clusters). We then train a deep belief network (DBN) for each cluster using Restricted Boltzmann Machines. This personalization which is essentially training DBN for each cluster im- proves the classification accuracy and features ex- traction. Finally, we generate network graphs of these clusters which can be used for similarity re- latedness or summarization in future works. This is the first work to use a modified combination of Markov clustering and personalized deep learning to classify documents into different categories.</p><p>The rest of the paper is organized as follows: Section 2 discusses the literature review and Ara- bic morphology. Section 3 focuses on methodol- ogy for Markov clustering and deep learning. Sec- tion 4 discusses our experimental results and fi- nally, Section 5 summarizes the paper and presents conclusions and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Related Works</head><p>As mentioned previously, even though numer- ous works in text categorization have been pro- posed for Western languages, works on categoriz- ing Semitic languages like Arabic are very rare in the NLP community.</p><p>Most previous works on Arabic text categoriza- tion treats documents as a bag-of-words where the text is represented as a vector of weighted fre- quencies for each of the distinct words or tokens. ( <ref type="bibr">Diederich et al. 2003;</ref><ref type="bibr" target="#b17">Sebastiani et al. 2002)</ref>. We use a similar approach to extract features from documents based on root-word frequency. Early efforts to categorize Arabic documents were per- formed using Naive Bayes algorithm <ref type="bibr" target="#b7">(El-Kourdi et al., 2004</ref>), maximum entropy ( <ref type="bibr" target="#b16">Sawaf et al., 2001)</ref> and support vector machines ( <ref type="bibr" target="#b8">Gharib et al., 2009)</ref>.</p><p>N-gram frequency statistics technique was used with Manhattan distance to categorize documents by <ref type="bibr" target="#b12">Khreisat (Khreisat, 2006</ref>) El-Halees described a method based on association rules to classify Arabic documents <ref type="bibr" target="#b6">(El-Halees, 2007</ref>). Hmeidi I. uses two machine learning methods for Arabic text categorization: K-Nearest Neighbor (KNN) and support vector machines (SVM) ( <ref type="bibr" target="#b10">Hmeidi et al., 2008</ref>). An approach for feature selection in Arabic text categorization was proposed using in- formation gain and Chi-square <ref type="bibr" target="#b19">(Yang and Pedersen, 1997</ref>). Abu-Errub A. proposed a new Arabic text classification algorithm using Tf-Idf and Chi square measurements ( <ref type="bibr" target="#b1">Abu-Errub, 2014</ref>). How- ever, all these methods were not very efficient for large datasets giving an accuracy of less than 70% and were unable to classify documents with dif- ferent diacritics. Diacritics are signs or accents whose pronunciation or presence in a word can re- sult in a different meaning.</p><p>Recently, a new technique using a combina- tion of kNN and Rocchio classifier for text cat- egorization was introduced ( <ref type="bibr" target="#b15">Mohammad et al., 2016)</ref>. This approach specifically solves the Word Sense Disambiguation (WSD) problem in a su- pervised approach using lexical samples of five Arabic words. Although, this method achieved higher accuracy than previous works, usage of only five Arabic words limits usage for larger datasets. Our proposed approach generates mul- tiple roots of each Arabic words addressing the is- sue of different diacritics in Arabic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Arabic Morphology</head><p>Arabic belongs to the family of Semitic languages and has significant morphological, syntactical and semantical differences from other languages. It consists of 28 letters and can be extended to 90 by added shapes, marks, and vowels. Furthermore, Arabic is written from right to left and letters have different styles based on the position of its appear- ance in the word. The base words of Arabic inflect to express eight main features. Verbs inflect for aspect, mood, person and voice. Nouns and ad- jectives inflect for case and state. Verbs, nouns and adjectives inflect for both gender and num- ber. Arabic morphology consists from a bare root verb form that is trilateral, quadrilateral, or pental- iteral. The derivational morphology can be lexeme = Root + Pattern or inflection morphology (word = Lexeme + Features) where features are noun spe- cific, verb specific or single letter conjunctions. In contrast, in most European languages words are formed by concatenating morphemes. For exam- ple in German, 'Zeitgeit'(the spirit of the times) is simply 'Zeit'(time) + 'geist'(spirit) i.e the root and pattern are essentially interleaved.</p><p>Stem pattern are often difficult to parse in Ara- bic as they interlock with root consonants <ref type="bibr" target="#b0">(Abdelali, 2004</ref>). Arabic is also influenced by infixes which may be consonants and vowels and can be misinterpreted as root-words. One of the major problem is usage of a consonant, hamza. Hamza is not always pronounced and can be a vowel. This creates a severe orthographic problem as words may have differently positioned hamzas making them different strings yet having similar meaning.</p><p>Furthermore, diacritics are critical in categoriz- ing Arabic documents. For example, consider the two words "zhb" mean "to go" and "gold" differ- ing by just one diacritic. The two words can only be distinguished using diacritics. The word "go" may appear in a variety of text documents while "gold" may likely appear in documents contain- ing other finance related words. This is where our personalized deep learning approach is extremely efficient for Arabic as discussed in future sections. For the purpose of clarity, we use the term "root- words" throughout this paper to represent the roots of an Arabic word. <ref type="figure">Figure 1</ref> presents an overview of our algorithm. In summary, our approach consists of a pre- processing stage, two stages of clustering and fi- nally a learning stage. In the pre-processing stage documents are tokenized and segmented into dif- ferent words and the Tf-Idf weighted root-word counts are extracted. Subsequently, we cluster the documents by a combination of Fuzzy C-means and Markov clustering in Step I and II. Finally, in</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>Step III, we use deep learning models on each ob- tained cluster to personalize learning for each root word cluster. Personalization essentially means to train a separate deep belief network for each clus- ter. Each of these stages is discussed in subsequent sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pre-Processing</head><p>In the pre-processing stage we first remove the punctuation marks, auxiliary verbs, pronouns, conjunctions etc. As we stated in section 2.2, it is very important for processing Arabic to properly use the semantic information provided by root- words ( <ref type="bibr" target="#b11">Kanaan et al., 2009</ref>). Therefore, repre- senting words presented in a document in the root pattern increases efficiency of classification. Root extraction or stemming for the Arabic dataset is performed using a letter weight and order scheme. For example, the root meaning "write" has the form k-t-b. More words can be formed using vow- els and additional consonants in the root word. Some of the words that can be formed using k-t-b are kitab meaning "book", kutub meaning "books", katib meaning "writer", kuttab represent- ing "writers", kataba meaning "he wrote", yaktubu meaning "he writes", etc.</p><p>In our method we assign weights to letters of Arabic and subsequently rank them based on their frequency in the document. Root-words of the Arabic word are selected by recurring patterns with the maximum weight. Furthermore, we cal- culate the standard Tf-Idf frequency of each root word to use as features in clustering and deep learning. Tf-Idf (term frequency-inverse docu- ment frequency) is one of the widely used fea- ture selection techniques in information retrieval ( <ref type="bibr" target="#b3">Baeza-Yates et al., 1999</ref>). Tf measures the im- portance of a term in a given document while Idf signifies the relative importance of a term in a col- lection of documents. In the next section, we will discuss the clustering step of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Estimation of Initial Number of Clusters</head><p>The words frequencies (Tf-Idf) and root-word fre- quencies are used from the pre-processing stage and grouped into clusters. Once the words are clustered, we consider each document and find which cluster of words it may belong. This is done by rank-matching based approach discussed later. The clustering step is described below.</p><p>Consider each document to be P</p><formula xml:id="formula_0">P =    p 0,0 p 0,1 · · · p 0,W −1 . . . . . . · · · . . . p H−1,0 p H−1,2 · · · p H−1,W −1    (1)</formula><p>where p i,0 indicate extracted words tokenized from the documents. p i,j are the similar root- words of p i,0 . We first estimate the initial number of cate- gories that may be present in a corpus of text doc- uments. The estimation of initial number of clus- ters is critical for text categorization as many Ara- bic documents may contain synonyms, different morphologies of Arabic and yet may have a close meaning. We perform estimation by computing the total number of modes found in all eigenvec- tors of the data. Modes in each eigenvector of the data are detected using kernel density estimation. Then, significance test of the gradient and second derivative of a kernel density estimation is com- puted. A similar approach was used for bioinfor- matics data <ref type="bibr" target="#b15">(Pouyan et al., 2015)</ref>. The number of modes is approximated using the number of times, the gradient changes from positive to negative for each projection of data on the eigenvectors. K rep- resents initial number of clusters approximated by summation of all the modes in eigenvectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Initial Clustering Using Fuzzy-C-Mean</head><p>We leverage the heavy influence of root-words in Arabic for clustering words (tf-idf) from the pre- processing stage. As described in section 2.1, new words in Arabic can be formed by filling vowels or consonants. For example, k-t-b is a triliteral root word as it contains a sequence of three consonants. Accordingly, an improved version of Fuzzy-C- Means clustering is developed to calculate the membership probability of each words to its root</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Estimation of Number of Distinct</head><p>Root-words</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Estimation of Number of Distinct</head><p>Root-words</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modified Fuzzy C- Means Clustering</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modified Fuzzy C- Means Clustering</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modified Markov Clustering</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modified Markov Clustering</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.</head><p>Step I</p><p>Step II</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classification using Deep Belief Networks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classification using Deep Belief Networks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TF-IDF of Arabic Documents</head><p>‫الرحيم‬ ‫الرحمن‬ ‫هللا‬ ‫بسم‬ 9 ‫مرحبا‬ 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graphs of Arabic words</head><p>Step III <ref type="figure">Figure 1</ref>: The general view of our proposed method word. The clusters we obtain are intended to cor- respond to the different root-words in the docu- ment. Concisely, χ = {α 1 , ..., α j , ..., α K } will be centers of K root-words C = {c 1 , ..., c j , ..., c K } which represents potential similarities of root- words. Words are assigned to different root word clusters by minimizing the following optimization model:</p><formula xml:id="formula_1">J m = N i=1 K j=1 u m ij D m (x i , α j )<label>(2)</label></formula><p>where word x i belongs to root word c j with the membership probability of u ij . The fuzzification coefficient is selected as m = 2 similar to set by James C. Bezdek. D m (x i , α j ) implies the Maha- lanobis Distance between word x i and root word c j . Since membership probability depends on the dispersion of cluster c j , we use Mahalanobis Dis- tance instead of Euclidean Distance as a distance metric between x i and population c j . The initial cluster set C will be available after applying the revised Fuzzy-C-Means and used in future steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Merging and Ranking Clusters Using Markov Clustering</head><p>Kernel density may result in duplicate clusters due to the similar diacritics or morphologies in Ara- bic. Furthermore, it is important to rank the clus- ters based on the similar root words. We use Markov clustering to address this issue, a fast, divisive and scalable clustering algorithm based on stochastic modelling of flow of networks ( <ref type="bibr" target="#b18">Van Dongen, 2001</ref>). Markov clustering (MCL) has re- cently emerged as a popular clustering technique for determining cluster networks. The algorithm computes the probability of random walks through a graph by applying two main methods: expansion and inflation. When MCL is applied on centers of initial clusters, the centers corresponding to ini- tial populations will be clustered in the same seg- ments. We extract the final categories of the text documents by merging these clusters. The redun- dant clusters due to similar diacritics are merged in this stage.</p><p>Once the words are clustered, we consider each document and find the most similar cluster of words it may belong to using a rank-matching based approach. The ranking algorithm can be explain by considering a graph of the Arabic root words of interest. The frequency of a random walk visiting a particular root word in sufficiently large steps i.e. in a stationary distribution is the score of the root-word. Let a ij represent the fraction of time root-word j has higher rank over root-word i. Then weighted edges of graph</p><formula xml:id="formula_2">A ij = a ij a ij +a ji .</formula><p>Recall that we denoted the document with matrix P in the previous step. Let p t (i) = P (X t = i) denote the distribution of the random walk at time t. Then, the transition matrix can be formulated as:</p><formula xml:id="formula_3">p T t+1 = p T t · P<label>(3)</label></formula><p>Iteratively, the random walk will converge to a unique stationary distribution and the score can be found using:</p><formula xml:id="formula_4">π(i) = j π(j) · A ij + Aji l (A il + A li )<label>(4)</label></formula><p>We find the maximum match between the 40% most frequent words in the document and every cluster. Cluster of words consisting of the highest number of words from top 40% frequent words in the document is assigned to the document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Deep Learning</head><p>We further use the state-of-the-art deep learning for extracting features from the clusters and use them for future clustering. We create separate deep belief network classifier for each cluster, which allows us to capture differences in between dialects, topics etc. To reiterate, our novel contri- bution is a personalized deep learning model for each root word cluster. Personalized means train- ing of each model for each specific root word, di- Pre-trained hidden layers using RBM <ref type="figure">Figure 2</ref>: Deep Neural Network for C 1 alects and diacritics. The classifiers get more fine- grained by training one classifier for every cluster. This personalization poses several advantages: the features learned using deep learning are per- sonalized for each root word. Consequently, the technique is robust against different dialects, scripts and way of writing. Secondly, personalized deep learning models extract features from diacrit- ics. As previously described in section 2.2, dia- critics makes Arabic word classification very chal- lenging. By extracting features from diacritics in the context of appeared root-words, our personal- ized deep learning approach efficiently solves this problem.</p><p>We separately train deep belief networks (DBN) on each cluster obtained from Stage II. For ex- ample the deep network contains one input layer, three hidden layers and one output layer for Clus- ter 1 with hidden layers consisting of 80, 80 and 40 nodes respectively. The input of these classi- fication models are words in each clusters. The activation function of hidden units is the sigmoid function which is traditionally used in nonlinear neural networks. Furthermore, higher number of parameters in neural networks generally makes pa- rameter estimation much more difficult. There- fore, it is inefficient to start training of deep neu- ral networks from random initial weight and bias values. We incorporate Restricted Boltzmann Ma- chines (RBM) to pre-train the network and find good initial weights for training deep belief net- works <ref type="bibr" target="#b13">(Le Roux and Bengio, 2008)</ref>.</p><p>Let D i be a DBN model for cluster C i . The hidden layers of each D i are first trained as RBMs using unlabeled inputs. We use Contrastive Divergence-1 (CD-1) algorithm to obtain samples after 1-step Gibbs sampling <ref type="bibr" target="#b9">(Hinton, 2002</ref>). CD- 1 allows accurate estimation of gradient's direc- tion and minimize reconstruction error. Due to this pre-training, D i learns an identity function with same desired output as the original input. Further- more, it enhances the robustness of D i by learning feature representations of the Arabic words before the final supervised learning stage.</p><p>Subsequently, the pre-trained DBN network is fine-tuned by vanilla back-propagation with la- beled segments as the input layer. <ref type="figure">Figure 2</ref> gives the abstract structure of final resulting D 1 after tuning and is also used for identification for clus- ter C 1 . If a new document is added for text catego- rization then we find it's nearest neighbor cluster and use the deep learning model specific to that cluster. Finally, we plot the network graphs of the extracted cluster words for visualization of the as- sociation of these words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>We evaluate our three-stage technique using two popular datasets previously used in Arabic text categorization: 10,000 documents from Al- Jazeera news website (http://www.aljazeera.net) and 6,000 Saudi Press Agency ( <ref type="bibr" target="#b2">Al-Harbi et al., 2008)</ref> documents. The results are reported using 10-fold cross validation. Our proposed method achieves a precision of 91.2% and recall of 90.9% giving F-measure of 91.02%.</p><p>Clustering is performed on a set consisting of the total 12,000 documents, randomly sampled from separately from Al-Jazeera and Saudi Press Agency. We further ran deep learning on each of these clusters and extracted network graphs. Two example networks are presented in <ref type="figure">Figure 3</ref> and 4. We compare our results with existing ap- proaches in <ref type="table">Table 1</ref>. We can see that our tech- nique improves substantially on the previous pub- lished works. Furthermore, it is capable to catego- rize different diacritics by using clusters based on root-words. Most misclassified cases in our algo- rithm due to random outliers and/or mix categories in a document. An example of a random outlier are some recent words which are not influenced by root-words. This can be further improved by using a larger dataset and using new discrimina- tive features for clustering and deep learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>This paper presents a novel three-stage technique for Arabic text categorization using a combina- tion of clustering and deep learning. We lever- age the influence of root-words in Arabic to ex- tract the features. Our technique is robust against <ref type="table">Table 1</ref>: Performance of our algorithm on Al-Jazeera Dataset</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Technique</head><p>Precision Recall Root-words based? Robust to Diacritcs? Naive Bayes ( <ref type="bibr" target="#b7">El-Kourdi et al., 2004)</ref> 62.6% 57.4% No No SVM ( <ref type="bibr" target="#b10">Hmeidi et al.,2008)</ref> 71.2% 66.7% No No kNN <ref type="bibr">(Mohammad AH et al., 2016)</ref> 83  different diacritics and complex morphology of Semitic languages. Furthermore, this procedure can be extended to Persian and Semitic languages like Hebrew which heavily depend on root-words. Future work includes extracting more discrimina- tive features of root-words using deep learning and improving training of our models using larger datasets. We also plan to explore other Arabic morphologies like lemmas used in Arabic depen- dency parsing <ref type="bibr">(Marton et al., 2003</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Network of a document's words from Cluster 1</figDesc><graphic url="image-41.png" coords="6,100.78,179.38,160.70,144.01" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgements</head><p>We acknowledge the support and guidance of Quality of Life Technology (QoLT) laboratory in the University of Texas at Dallas. We are thankful to Dr. Maziyar Baran Pouyan and Dr. Mehrdad Nourani for conceiving the original integration techniques for bioinformatics data using Gaussian Estimation and Fuzzy-c-means. We further like to thank University of Texas at Dallas and ACL Don and Betty Walker Scholarship program. We are specially thankful to Dr. Christoph Teichmann for his insightful comments as the mentor through ACL Student Mentorship Program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Localization in modern standard Arabic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Abdelali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science and technology</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="28" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Arabic text classification algorithm using tf-idf and chi square measurements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aymen</forename><surname>Abu-Errub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Applications</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic Arabic text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Al-Harbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Almuhareb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Al-Thubaity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khorsheed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Al-Rajeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journes internationals</title>
		<imprint>
			<biblScope unit="page" from="77" to="83" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Modern information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berthier</forename><surname>Ribeiro-Neto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>ACM press New York</publisher>
			<biblScope unit="volume">463</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Diederich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorg</forename><surname>Kindermann</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Authorship attribution with support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leopold</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Paass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="109" to="123" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Arabic text classification using maximum entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaa</forename><surname>El-Halees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Islamic University Journal(Series of Natural Studies and Engineering)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="157" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatic arabic document categorization based on the Naive bayes algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><forename type="middle">El</forename><surname>Kourdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amine</forename><surname>Bensaid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tajje-Eddine</forename><surname>Rachidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Computational Approaches to Arabic Script-based Languages</title>
		<meeting>the Workshop on Computational Approaches to Arabic Script-based Languages</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="51" to="58" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Arabic text classification using support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tarek F Gharib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Habib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fayed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Applications</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="192" to="199" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Training products of experts by minimizing Contrastive Divergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geoffrey E Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1771" to="1800" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Performance of kNN and SVM classifiers on full word arabic articles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ismail</forename><surname>Hmeidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Hawashin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eyas</forename><surname>Elqawasmeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advanced Engineering Informatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="106" to="111" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A comparison of text-classification techniques applied to arabic text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghassan</forename><surname>Kanaan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riyad</forename><surname>Al-Shalabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameh</forename><surname>Ghwanmeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamda</forename><surname>Al-Maadeed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1836" to="1844" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Arabic text classification using N-gram frequency statistics a comparative study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laila</forename><surname>Khreisat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DMIN</title>
		<imprint>
			<biblScope unit="page" from="78" to="82" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Representational power of restricted boltzmann machines and deep belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1631" to="1649" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dependency parsing of modern standard arabic with lexical and inflectional features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Rambow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="161" to="194" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Arabic text categorization using k-nearest neighbour, decision trees (c4.5) and rocchio classifier: A comparative study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><surname>Adel Hamdan Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tariq</forename><surname>Al-Momani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Alwadan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>M Baran Pouyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jindal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Birjandtalab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nourani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</title>
		<meeting>2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="511" to="516" />
		</imprint>
	</monogr>
	<note>A two-stage clustering technique for automatic biaxial gating of flow cytometry data</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Statistical classification methods for arabic news articles. Natural Language Processing in ACL</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Sawaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorg</forename><surname>Zaplo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<pubPlace>Toulouse, France</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Machine learning in automated text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM computing surveys (CSUR)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Graph clustering by flow simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stijn Marinus Van Dongen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A comparative study on feature selection in text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">O</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="412" to="420" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
