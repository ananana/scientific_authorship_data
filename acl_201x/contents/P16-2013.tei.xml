<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reference Bias in Monolingual Machine Translation Evaluation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>August 7-12, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marina</forename><surname>Fomicheva</surname></persName>
							<email>marina.fomicheva@upf.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute for Applied Linguistics Pompeu</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution">Fabra University</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
							<email>l.specia@sheffield.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Reference Bias in Monolingual Machine Translation Evaluation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="77" to="82"/>
							<date type="published">August 7-12, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In the translation industry, human translations are assessed by comparison with the source texts. In the Machine Translation (MT) research community, however, it is a common practice to perform quality assessment using a reference translation instead of the source text. In this paper we show that this practice has a serious issue-annotators are strongly biased by the reference translation provided, and this can have a negative impact on the assessment of MT quality.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Equivalence to the source text is the defining char- acteristic of translation. One of the fundamental aspects of translation quality is, therefore, its se- mantic adequacy, which reflects to what extent the meaning of the original text is preserved in the translation. In the field of Machine Translation (MT), on the other hand, it has recently become common practice to perform quality assessment using a human reference translation instead of the source text. Reference-based evaluation is an at- tractive practical solution since it does not require bilingual speakers.</p><p>However, we believe this approach has a strong conceptual flaw: the assumption that the task of translation has a single correct solution. In real- ity, except for very short sentences or very specific technical domains, the same source sentence may be correctly translated in many different ways. Depending on a broad textual and real-world con- text, the translation can differ from the source text at any linguistic level -lexical, syntactic, seman- tic or even discourse -and still be considered per- fectly correct. Therefore, using a single translation as a proxy for the original text may be unreliable.</p><p>In the monolingual, reference-based evaluation scenario, human judges are expected to recognize acceptable variations between translation options and assign a high score to a good MT, even if it happens to be different from a particular hu- man reference provided. In this paper we argue that, contrary to this expectation, annotators are strongly biased by the reference. They inadver- tently favor machine translations (MTs) that make similar choices to the ones present in the reference translation. To test this hypothesis, we perform an experiment where the same set of MT outputs is manually assessed using different reference trans- lations and analyze the discrepancies between the resulting quality scores.</p><p>The results confirm that annotators are indeed heavily influenced by the particular human trans- lation that was used for evaluation. We discuss the implications of this finding on the reliability of current practices in manual quality assessment. Our general recommendation is that, in order to avoid reference bias, the assessment should be per- formed by comparing the MT output to the origi- nal text, rather than to a reference.</p><p>The rest of this paper is organized as follows. In Section 2 we present related work. In Section 3 we describe our experimental settings. In Section 4 we focus on the effect of reference bias on MT evaluation. In Section 5 we examine the impact of the fatigue factor on the results of our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>It has become widely acceptable in the MT com- munity to use human translation instead of (or along with) the source segment for MT evalua- tion. In most major evaluation campaigns <ref type="bibr">(ARPA (White et al., 1994)</ref>, 2008 NIST Metrics for Machine Translation Challenge ( <ref type="bibr" target="#b14">Przybocki et al., 2008)</ref>, and annual Workshops on Statistical Ma-chine Translation <ref type="bibr" target="#b3">(Callison-Burch et al., 2007;</ref><ref type="bibr">Bojar et al., 2015)</ref>), manual assessment is ex- pected to consider both MT fluency and adequacy, with a human (reference) translation commonly used as a proxy for the source text to allow for adequacy judgement by monolingual judges.</p><p>The reference bias problem has been exten- sively discussed in the context of automatic MT evaluation. Evaluation systems based on string- level comparison, such as the well known BLEU metric ( <ref type="bibr" target="#b13">Papineni et al., 2002</ref>) heavily penalize po- tentially acceptable variations between MT and human reference. A variety of methods have been proposed to address this issue, from using multiple references <ref type="bibr" target="#b9">(Dreyer and Marcu, 2012</ref>) to reference- free evaluation ( <ref type="bibr" target="#b15">Specia et al., 2010)</ref>.</p><p>Research in manual evaluation has focused on overcoming annotator bias, i.e. the preferences and expectations of individual annotators with re- spect to translation quality that lead to low levels of inter-annotator agreement <ref type="bibr" target="#b6">(Cohn and Specia, 2013;</ref><ref type="bibr" target="#b7">Denkowski and Lavie, 2010;</ref><ref type="bibr" target="#b10">Graham et al., 2013;</ref><ref type="bibr" target="#b11">Guzmán et al., 2015)</ref>. The problem of ref- erence bias, however, has not been examined in previous work. By contrast to automatic MT eval- uation, monolingual quality assessment is consid- ered unproblematic, since human annotators are supposed to recognize meaning-preserving varia- tions between the MT output and a given human reference. However, as will be shown in what fol- lows, manual evaluation is also strongly affected by biases due to specific reference translations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Settings</head><p>To show that monolingual quality assessment de- pends on the human translation used as gold- standard, we devised an evaluation task where annotators were asked to assess the same set of MT outputs using different references. As control groups, we have annotators assessing MT using the same reference, and using the source segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>MT data with multiple references is rare. We used MTC-P4 Chinese-English dataset, produced by Linguistic Data Consortium (LDC2006T04). The dataset contains 919 source sentences from news domain, 4 reference translations and MT outputs generated by 10 translation systems. Human trans- lations were produced by four teams of profes- sional translators and included editor's proofread- ing. All teams used the same translation guide- lines, which emphasize faithfulness to the source sentence as one of the main requirements.</p><p>We note that even in such a scenario, human translations differ from each other. We measured the average similarity between the four references in the dataset using the Meteor evaluation met- ric <ref type="bibr" target="#b8">(Denkowski and Lavie, 2014)</ref>. Meteor scores range between 0 and 1 and reflect the proportion of similar words occurring in similar order. This metric is normally used to compare the MT out- put with a human reference, but it can also be ap- plied to measure similarity between any two trans- lations. We computed Meteor for all possible com- binations between the four available references and took the average score. Even though Me- teor covers certain amount of acceptable linguis- tic variation by allowing for synonym and para- phrase matching, the resulting score is only 0.33, which shows that, not surprisingly, human transla- tions vary substantially.</p><p>To make the annotation process feasible given the resources available, we selected a subset of 100 source sentences for the experiment. To en- sure variable levels of similarity between the MT and each of the references, we computed sentence- level Meteor scores for the MT outputs using each of the references and selected the sentences with the highest standard deviation between the scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Method</head><p>We developed a simple online interface to collect human judgments. Our evaluation task was based on the adequacy criterion. Specifically, judges were asked to estimate how much of the meaning of the human translation was expressed in the MT output (see <ref type="figure" target="#fig_0">Figure 1</ref>). The responses were inter- preted on a five-point scale, with the labels in Fig- ure 1 corresponding to numbers from 1 ("None") to 5 ("All").</p><p>For the main task, judgments were collected us- ing English native speakers who volunteered to participate. They were either professional trans- lators or researchers with a degree in Computa- tional Linguistics, English or Translation Stud- ies. 20 annotators participated in this monolin- gual task. Each of them evaluated the same set of 100 MT outputs. Our estimates showed that the task could be completed in approximately one hour. The annotators were divided into four groups, corresponding to the four available refer- As a control group, five annotators (native speakers of English, fluent in Chinese or bilingual speakers) performed a bilingual evaluation task for the same MT outputs. In the bilingual task, an- notators were presented with an MT output and its corresponding source sentence and asked how much of the meaning of the source sentence was expressed in the MT.</p><p>In total, we collected 2,500 judgments. Both the data and the tool for collecting human judgments are available at https://github. com/mfomicheva/tradopad.git.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Reference Bias</head><p>The goal of the experiment is to show that depend- ing on the reference translation used for evalua- tion, the quality of the same MT output will be per- ceived differently. However, we are aware that MT evaluation is a subjective task. Certain discrepan- cies between evaluation scores produced by dif- ferent raters are expected simply because of their backgrounds, individual perceptions and expecta- tions regarding translation quality.</p><p>To show that some differences are related to reference bias and not to the bias introduced by individual annotators, we compare the agreement between annotators evaluating with the same and with different references. First, we randomly se- lect from the data 20 pairs of annotators who used the same reference translations and 20 pairs of annotators who used different reference transla- tions. The agreement is then computed for each pair. Next, we calculate the average agreement for the same-reference and different-reference groups. We repeat the experiment 100 times and report the corresponding averages and confidence intervals. <ref type="table">Table 1</ref> shows the results in terms of stan- dard <ref type="bibr" target="#b4">(Cohen, 1960)</ref> and linearly weighted <ref type="bibr" target="#b5">(Cohen, 1968</ref>) Kappa coefficient (k). <ref type="bibr">1</ref> We also report one- off version of weighted k, which discards the dis- agreements unless they are larger than one cate- gory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Kappa</head><p>Diff As shown in <ref type="table">Table 1</ref>, the agreement is consis- tently lower for annotators using different refer- ences. In other words, the same MT outputs sys- tematically receive different scores when differ-ent human translations are used for their evalua- tion. Here and in what follows, the differences between the results for the same-reference annota- tor group and different-reference annotator group were found to be statistically significant with p- value &lt; 0.01.</p><p>The agreement between annotators using the source sentences is slightly lower than in the monolingual, same-reference scenario, but it is higher than in the case of the different-reference group. This may be an indication that reference- based evaluation is an easier task for annotators, perhaps because in this case they are not required to shift between languages. Nevertheless, the fact that given a different reference, the same MT out- puts receive different scores, undermines the reli- ability of this type of evaluation.  In <ref type="table" target="#tab_2">Table 2</ref> we computed average evaluation scores for each group of annotators. Average scores vary considerably across groups of anno- tators. This shows that MT quality is perceived differently depending on the human translation used as gold-standard. For the sake of compari- son, we also present the scores from the widely used automatic evaluation metric BLEU. Not sur- prisingly, BLEU scores are also strongly affected by the reference bias. Below we give an example of linguistic variation in professional human translations and its effect on reference-based MT evaluation.</p><p>Src: 不过这一切都由不得你 2 MT: But all this is beyond the control of you. R1: But all this is beyond your control. R2: However, you cannot choose yourself. R3: However, not everything is up to you to decide.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R4: But you can't choose that.</head><p>Although all the references carry the same mes- sage, the linguistic means used by the translators are very different. Most of these references are high-level paraphrases of what we would consider a close version of the source sentence. Annota- tors are expected to recognize meaning-preserving variation between the MT and any of the refer- ences. However, the average score for this sen- tence was 3.4 in case of Reference 1, and 2.0, 2.0 and 2.8 in case of the other three references, re- spectively, which illustrates the bias introduced by the reference translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Time Effect</head><p>It is well known that the reliability and consistency of human annotation tasks is affected by fatigue ( <ref type="bibr" target="#b12">Llorà et al., 2005</ref>). In this section we examine how this factor may gave influenced the evalua- tion on the impact of reference bias and thus the reliability of our experiment.</p><p>We measured inter-annotator agreement for the same-reference and different-reference annotators at different stages of the evaluation process. We divided the dataset in five sets of sentences based on the chronological order in which they were an- notated (0-20, 20-40, ..., 80-100). For each slice of the data we repeated the procedure reported in Section 4. <ref type="figure">Figure 2</ref> shows the results.</p><p>First, we note that the agreement is always higher in the case of same-reference annotators. Second, in the intermediate stages of the task we observe the highest inter-annotator agreement (sentences 20-40) and the smallest difference be- tween the same-reference and different-reference annotators (sentences 40-60). This seems to in- dicate that the effect of reference bias is minimal half-way through the evaluation process. In other words, when the annotators are already acquainted with the task but not yet tired, they are able to better recognize meaning-preserving variation be- tween different translation options.</p><p>To further investigate how fatigue affects the evaluation process, we tested the variability of hu- man scores in different (chronological) slices of the data. We again divided the data in five sets of sentences and calculated standard deviation be- tween the scores in each set. We repeated this pro- cedure for each annotator and averaged the results. As can be seen in <ref type="figure">Figure 3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this work we examined the effect of reference bias on monolingual MT evaluation. We com- pared the agreement between the annotators who used the same human reference translation and those who used different reference translations. We were able to show that in addition to the in- evitable bias introduced by different annotators, monolingual evaluation is systematically affected by the reference provided. Annotators consistently assign different scores to the same MT outputs when a different human translation is used as gold- standard. The MTs that are correct but happen to be different from a particular human translation are inadvertently penalized during evaluation.</p><p>We also analyzed the relation between reference bias and annotation at different times throughout the process. The results suggest that annotators are less influenced by specific translation choices present in the reference in the intermediate stages of the evaluation process, when they have already familiarized themselves with the task but are not yet fatigued by it. To reduce the fatigue effect, the task may be done in smaller batches over time. Re- garding the lack of experience, annotators should receive previous training.</p><p>Quality assessment is instrumental in the devel- opment and deployment of MT systems. If evalua- tion is to be objective and informative, its purpose must be clearly defined. The same sentence can be translated in many different ways. Using a hu- man reference as a proxy for the source sentence, we evaluate the similarity of the MT to a partic- ular reference, which does not necessarily reflect how well the contents of the original is expressed in the MT or how suitable it is for a given pur- pose. Therefore, monolingual evaluation under- mines the reliability of quality assessment. We recommend that unless the evaluation is aimed for a very specific translation task, where the number of possible translations is indeed limited, the as- sessment should be performed by comparing MT to the original text.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Evaluation Interface</figDesc><graphic url="image-1.png" coords="3,106.02,62.81,385.52,207.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: Inter-annotator agreement at different stages of evaluation process</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Average human scores for the groups of 
annotators using different references and BLEU 
scores calculated with the corresponding refer-
ences. Human scores range from 1 to 5, while 
BLEU scores range from 0 to 1. 

</table></figure>

			<note place="foot" n="1"> In MT evaluation, agreement is usually computed using standard k both for ranking different translations and for scoring translations on an interval-level scale. We note, however, that weighted k is more appropriate for scoring, since it allows the use of weights to describe the closeness of the agreement between categories (Artstein and Poesio, 2008).</note>

			<note place="foot" n="2"> Literally: &quot;However these all totally beyond the control of you.&quot;</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Marina Fomicheva was supported by funding from IULA (UPF) and the FI-DGR grant program of the Generalitat de Catalunya. Lucia Specia was sup-ported by the QT21 project (H2020 No. 645452). The authors would also like to thank the three anonymous reviewers for their helpful comments and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Inter-coder Agreement for Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Artstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="555" to="596" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajen</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<imprint>
			<pubPlace>Matthias Huck, Chris Hokamp, Philipp Koehn, Varvara Logacheva, Christof Monz</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolina</forename><surname>Scarton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Turchi</surname></persName>
		</author>
		<title level="m">Proceedings of the Tenth Workshop on Statistical Machine Translation</title>
		<meeting>the Tenth Workshop on Statistical Machine Translation<address><addrLine>Lisboa, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="46" />
		</imprint>
	</monogr>
	<note>Findings of the 2015 Workshop on Statistical Machine Translation</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">meta-) evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cameron</forename><surname>Fordyce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Schroeder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Statistical Machine Translation</title>
		<meeting>the Second Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="136" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Coefficient of Agreement for Nominal Scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Educational and Psychological Measurement</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="37" to="46" />
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Weighted Kappa: Nominal Scale Agreement Provision for Scaled Disagreement or Partial Credit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological bulletin</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="213" to="220" />
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Modelling Annotator Bias with Multi-task Gaussian Processes: An Application to Machine Translation Quality Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="32" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Choosing the Right Evaluation for Machine Translation: an Examination of Annotator and Automatic Metric Performance on Human Judgment Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Biennal Conference of the Association for Machine Translation in the Americas</title>
		<meeting>the Ninth Biennal Conference of the Association for Machine Translation in the Americas</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Meteor Universal: Language Specific Translation Evaluation for Any Target Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EACL 2014 Workshop on Statistical Machine Translation</title>
		<meeting>the EACL 2014 Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="376" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">HyTER: Meaning-equivalent Semantics for Translation Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Dreyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="162" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Continuous Measurement Scales in Human Evaluation of Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alistair</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 7th Linguistic Annotation Workshop and Interoperability with Discourse</title>
		<meeting>7th Linguistic Annotation Workshop and Interoperability with Discourse</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="33" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">How do Humans Evaluate Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Abdelali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Temnikova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Sajjad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Vogel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Workshop on Statistical Machine Translation</title>
		<meeting>the Tenth Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="457" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Combating User Fatigue in iGAs: Partial Ordering, Support Vector Machines, and Synthetic Fitness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Llorà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumara</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">E</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhimanyu</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lalitha</forename><surname>Lakshmi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Annual Conference on Genetic and Evolutionary Computation</title>
		<meeting>the 7th Annual Conference on Genetic and Evolutionary Computation</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1363" to="1370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">BLEU: a Method for Automatic Evaluation of Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the ACL</title>
		<meeting>the 40th Annual Meeting of the ACL</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Metrics for MAchine TRanslation&quot; Challenge (MetricsMATR08)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Przybocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kay</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Bronsart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AMTA-2008 Workshop on Metrics for Machine Translation</title>
		<meeting>the AMTA-2008 Workshop on Metrics for Machine Translation<address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Official Results of the NIST</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Machine Translation Evaluation versus Quality Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhwaj</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Turchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Translation</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="39" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The ARPA MT Evaluation Methodologies: Evolution, Lessons, and Future Approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Theresa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis O&amp;apos;</forename><surname>Connell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Machine Translation in the Americas Conference</title>
		<meeting>the Association for Machine Translation in the Americas Conference<address><addrLine>Columbia, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="193" to="205" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
