<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:56+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Content Importance Models for Scoring Writing From Sources</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beata</forename><forename type="middle">Beigman</forename><surname>Klebanov</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Educational Testing Service</orgName>
								<address>
									<addrLine>660 Rosedale Road Princeton</addrLine>
									<postCode>08541</postCode>
									<region>NJ</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Madnani</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Educational Testing Service</orgName>
								<address>
									<addrLine>660 Rosedale Road Princeton</addrLine>
									<postCode>08541</postCode>
									<region>NJ</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jill</forename><surname>Burstein</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Educational Testing Service</orgName>
								<address>
									<addrLine>660 Rosedale Road Princeton</addrLine>
									<postCode>08541</postCode>
									<region>NJ</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swapna</forename><surname>Somasundaran</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Educational Testing Service</orgName>
								<address>
									<addrLine>660 Rosedale Road Princeton</addrLine>
									<postCode>08541</postCode>
									<region>NJ</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Content Importance Models for Scoring Writing From Sources</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="247" to="252"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Selection of information from external sources is an important skill assessed in educational measurement. We address an integrative summarization task used in an assessment of English proficiency for non-native speakers applying to higher education institutions in the USA. We evaluate a variety of content importance models that help predict which parts of the source material should be selected by the test-taker in order to succeed on this task.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Selection and integration of information from ex- ternal sources is an important academic and life skill, mentioned as a critical competency in the Common Core State Standards for English Lan- guage Arts/Literacy: College-ready students will be able to "gather relevant information from mul- tiple print and digital sources, assess the credibi- lity and accuracy of each source, and integrate the information while avoiding plagiarism." <ref type="bibr">1</ref> Accordingly, large-scale assessments of writing incorporate tasks that test this skill. One such test requires test-takers to read a passage, then to lis- ten to a lecture discussing the same topic from a different point of view, and to summarize the points made in the lecture, explaining how they cast doubt on points made in the reading. The qua- lity of the information selected from the lecture is emphasized in excerpts from the scoring rubric for this test (below); essays are scored on a 1-5 scale:</p><p>Score 5 successfully selects the important infor- mation from the lecture and coherently and accurately presents this information in rela- tion to the relevant information presented in the reading.</p><p>Score 4 is generally good in selecting the impor- tant information from the lecture ..., but it may have a minor omission.</p><p>Score 3 contains some important information from the lecture ..., but it may omit one major key point.</p><p>Score 2 contains some relevant information from the lecture ... The response significantly omits or misrepresents important points.</p><p>Score 1 provides little or no meaningful or rele- vant coherent content from the lecture.</p><p>The ultimate goal of our project is to improve automated scoring of such essays by taking into account the extent to which a response integrates important information from the lecture. This pa- per reports on the first step aimed at automatically assigning importance scores to parts of the lecture. The next step -developing an essay scoring sys- tem using content importance models along with other features of writing quality, will be addressed in future work. A simple essay scoring mechanism will be used for evaluation purposes in this paper, as described in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Design of Experiment</head><p>In evaluations of summarization algorithms, it is common practice to derive the gold standard con- tent importance scores from human summaries, as done, for example, in the pyramid method, where the importance of a content element corresponds to the number of reference human summaries that make use of it ( <ref type="bibr" target="#b12">Nenkova and Passonneau, 2004</ref>). Selection of the appropriate content plays a cru- cial role in attaining a high score for the essays we consider here, as suggested by the quotes from the scoring rubric in §1, as well as by a corpus study by <ref type="bibr" target="#b14">Plakans and Gebril (2013)</ref>. We therefore observe that high-scoring essays can be thought of as high-quality human summaries of the lec- ture, albeit containing, in addition, references to the reading material and language that contrasts the different viewpoints, making them a somewhat noisy gold standard. On the other hand, since low- scoring essays contain deficient summaries of the lecture, our setup allows for a richer evaluation than typical in studies using gold standard human data only, in that a good model should not only agree with the gold standard human summaries but should also disagree with sub-standard human summaries. We therefore use correlation with es- say score to evaluate content importance models.</p><p>The evaluation will proceed as follows. Every essay E is responding to a test prompt that con- tains a lecture L and a reading R. We identify the essay's overlap with the lecture:</p><formula xml:id="formula_0">O(E, L) = {x|x ∈ L, x ∈ E} (1)</formula><p>where the exact definition of x, that is, what is taken to be a single unit of information, will be one of the parameters to be studied. The essay is then assigned the following score by the content importance model M :</p><formula xml:id="formula_1">S M (E) = Σ x∈O(E,L) w M (x) × C(x, E) n E (2)</formula><p>where w M (x) is the importance weight as- signed by model M to item x in the lecture, C(x, E) is the count of tokens in E that realize the information unit x, and n E is the number of tokens in the essay. In this paper, the distinction between x and C is that between type and token count of instances of that type. <ref type="bibr">2</ref> This simple sco- ring mechanism quantifies the rate of usage of im- portant information per token in the essay. Finally, we calculate the correlation of scores assigned to essays by model M with scores assigned to the same essays by human graders.</p><p>This design ensures that once x is fixed, all the content importance models are evaluated within the same scoring scheme, so any differences in the correlations can be attributed to the differences in the weights assigned by the importance models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Content Importance Models</head><p>Our setting can be thought of as a special kind of summarization task. Test-takers are required to summarize the lecture while referencing the reading, making this a hybrid of single-and multi- document summarization, where one source is treated as primary and the other as secondary.</p><p>We therefore consider models of content impor- tance that had been found useful in the summariza- tion literature, as well as additional models that utilize a special feature of our scenario: We have hundreds of essays of varying quality responding to any given prompt, as opposed to a typical news summarization scenario where a small number of high quality human summaries are available for a given article. A sample of these essays can be used when developing a content importance model.</p><p>We define the following importance models. For all definitions, x is a unit of information in the lecture; C(x, t) is the number of tokens in text t that realize x; n L and n R are the number of tokens in the lecture and the reading, respectively. 3</p><p>Na¨ıveNa¨ıve: w(x) = 1. This is a simple overlap model.</p><formula xml:id="formula_2">Prob: w(x) = C(x,L) n L</formula><p>, an MLE estimate of the probability that x appears in the lecture. Those x that appear more are more important.</p><formula xml:id="formula_3">Position: w(x) = F P (x) n L</formula><p>, where F P (x) is the offset of the first occurrence of x in the lec- ture. The offset corresponds to the token's serial number in the text, 1 through n L .</p><formula xml:id="formula_4">LectVsRead: w(x) = C(x,L) n L − C(x,R) n R</formula><p>, that is, the difference in the probabilities of occurrence of x in the lecture and in the reading passage that accompanies the lecture. This model at- tempts to capture the contrastive aspect of importance -the content that is unique to the lecture is more important than the content that is shared by the lecture and the reading.</p><p>The following two models capitalize on evi- dence of use of information in better and worse es- says. For estimating these models, we sample, for each prompt, a development set of 750 essays re- sponding to the prompt (that is, addressing a given pair of lecture and reading stimuli). Out of these, we take, for each prompt, all essays at score points 4 and 5 (EGood) and all essays at score points 1 and 2 (EBad). These data do not overlap with the experimental data described in section 4. In both definitions below, e is an essay.</p><p>Good: w(x) = |{e∈EGood|x∈e}| |EGood| . An x is more im- portant if more good essays use it. <ref type="bibr" target="#b7">Hong and Nenkova (2014)</ref> showed that a variant of this measure used on pairs of articles and their ab- stracts from the New York Times effectively identified words that typically go into sum- maries, across topics. In contrast, our mea- surements are prompt-specific.</p><formula xml:id="formula_5">GoodVsBad: w(x) = |{e∈EGood|x∈e}| |EGood| − |{e∈EBad|x∈e}| |EBad|</formula><p>. An x is more important if good essays use it more than bad essays. To our knowledge, this measure has not been used in the summarization literature, probably because a large sample of human summaries of varying quality is typically not available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Data</head><p>We use 116 prompts drawn from an assessment of English proficiency for non-native speakers. Each prompt contains a lecture and a reading passage. For each prompt, we sample about 750 essays. Each essay has an operational score provided by a human grader. <ref type="table">Table 1</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>Independently from the content importance models, we address the effect of the granularity of the unit of information. Intuitively, since all the materials for a given prompt deal with the same topic, we expect large unigram overlaps between lecture and reading, and between good and bad essays, whereas n-grams with larger n can be more distinctive. On the other hand, larger n lead to misses, where an information unit would fail to be identified in an essay due to a paraphrase, thus impairing the ability of the scoring function to use the content importance model effectively.</p><p>We therefore evaluate each content importance model for different granularities of the content unit x: n-grams for n = 1, 2, 3, 4.  <ref type="table" target="#tab_1">Table 2</ref>: Correlations with essay scores attained by content models, for various definitions of informa- tion unit (n-grams with n = 1, 2, 3, 4). Five top scores are boldfaced. The baseline performance is shown in underlined italics. Correlations that are significantly better (p &lt; 0.05) than the na¨ıvena¨ıve n = 1 model are marked with an asterisk. We use McNemar (1955, p. 148) test for significance of difference between same-sample correlations. N = 85, 252 for all correlations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>The Na¨ıveNa¨ıve model with n = 1 can be considered a baseline, corresponding to unweighted word over- lap between the lecture and the essay. This model attains a significant positive correlation with essay score (r = 0.24), suggesting that, in general, bet- ter writers use more material from the lecture.</p><p>Our next observation is that the Prob and Good models do not improve over the baseline, that is, their weighting schemes generally assign higher weights to the wrong units. We believe the rea- son for this is that the most highly used n-grams, in the lecture and in the essays, correspond to ge- neral topical and functional elements. The impor- tance of these elements is discounted in the more effective Position, LectVsRead, and GoodVsBad models, highlighting subtler aspects of the lecture.</p><p>Next, let us consider the granularity of the units of information. We observe that 4-grams are in- ferior to trigrams for all models, suggesting that data sparsity is becoming a problem for matching 4-word sequences. For models that assign weight based on one or two sources (lecture, or lecture and reading) -Na¨ıveNa¨ıve, Position, LectVsRead -un- igram models are generally ineffective, while bi-gram and trigram models significantly outperform the baseline. We interpret this as suggesting that it is certain particular, detailed aspects of the top- ical concepts that constitute the important nuggets in the lecture; these are usually realized by multi- word sequences.</p><p>The GoodVsBad models show a different pat- tern, obtaining the best performance with a uni- gram version. These models are sensitive to data sparsity not only when matching essays to the lecture (this problem is common to all models) but also during model building. Recall that the weights in a GoodVsBad model are estimated based on differential use in samples of good and bad essays. The estimation of use-in-a-corpus is more accurate for smaller n, because longer n- grams are more susceptible to paraphrasing, which leads to under-estimation of use. Assuming that paraphrasing behavior of good and bad writers is not the same -in fact, there is corpus evidence that better writers paraphrase more ( <ref type="bibr" target="#b3">Burstein et al., 2012</ref>) -the resulting inaccuracies might im- pact the estimation of differential use in a sys- tematic manner, making the n &gt; 1 models less effective than the unigrams. Given that (a) the GoodVsBad bigram model is the second best over- all in spite of the shortcomings of the estimation process, and (b) that the bigram models worked better than unigram models for all the other con- tent importance models, the GoodVsBad bigram model could probably be improved significantly by using a more flexible information realization mechanism.</p><p>To illustrate the information assigned high im- portance by different models, consider a lec- ture discussing advantages of fish farming. The top-scoring Good bigrams are topical expressions (fish farming), functional bigrams around fish and farming, 4 aspects of content dealt with at length in the lecture (wild fish, commercial fishing), bi- grams referencing some of the claims -fish con- taining less fat and being used for fish meal. In addition, this model picks out some sequences of function words and punctuation (of the, are not, ", and", ", the") that suggest that better essays tend to give more detail (hence have more com- plex noun phrases and coordinated constructions) and to draw contrast.</p><p>For the bigram GoodVsBad model, the topi- cal bigram fish farming is not in the top 20 bi-grams. Although some bigrams are shared with the Good model, the GoodVsBad model selects additional details about the claims, such as the contrast between inedible fish and edible fish that is eaten by humans, as well as reference to chemi- cals used in farming and to the claim that wild fish are already endangered by other practices.</p><p>The most important bigrams according to the LectVsRead model include functional bigrams around fish and farming, functional sequences (that the, is a), as well as commercial fishing and edible fish. Also selected are functional bigrams around consumption and species, hinting, indi- rectly, at the edibility differences between species. Finally, this model selects almost all bigrams in the reading passage makes, the reading makes claims that and the reading says. While distin- guishing the lecture from the reading, these do not capture topic-relevant content of the lecture.</p><p>The GoodVsBad unigram model selects poul- try, endangered, edible, chemicals among its top 6 unigrams, 5 effectively touching upon the connec- tion with other farm-raised foods (poultry, chemi- cals), with wild fish (endangered) and with human benefit (edible) that are made in the lecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related work</head><p>Modern essay scoring systems are complex and cover various aspects of the writing construct, such as grammar, organization, vocabulary <ref type="bibr">(Shermis and Burstein, 2013</ref>). The quality of content is often addressed by features that quantify the similarity between the vocabulary used in an es- say and reference essays from given score points ( <ref type="bibr" target="#b0">Attali and Burstein, 2006;</ref><ref type="bibr" target="#b5">Foltz et al., 2013;</ref><ref type="bibr" target="#b1">Attali, 2011</ref>). For example, Attali (2011) proposed a measure of differential use of words in higher and lower scoring essays defined similarly to Good- VsBad, without, however, considering the source text at all. Such features can be thought of as con- tent quality features, as they implicitly assume that writers of better essays use better content. How- ever, there are various kinds of better content, only one of them being selection of important informa- tion from the source; other elements of content originate with the writer, such as examples, dis- course markers, evaluations, introduction and con- clusion, etc. Our approach allows focusing on a particular aspect of content quality, namely, selec- tion of appropriate materials from the source.</p><p>Our results are related to the findings of <ref type="bibr" target="#b6">Gurevich and Deane (2007)</ref> who studied the difference between the reading and the lecture in their im- pact on essay scores for this test. Using data from a single prompt, they showed that the difference between the essay's average cosine similarity to the reading and its average cosine similarity to the lecture is predictive of the score for non-native speakers of English, thus using a model similar to LectVsRead, although they took all lecture, reading, and essay words into account, in contrast to our model that looks only at n-grams that ap- pear in the lecture. Our study shows that the ef- fectiveness of lecture-reading contrast models for essay scoring generalizes to a large set of prompts. Similarly, <ref type="bibr" target="#b4">Evanini et al. (2013)</ref> found that over- lap with material that is unique to the lecture (not shared with the reading) was predictive of scores in a spoken source-based question answering task.</p><p>In the vast literature on summarization, our work is closest to <ref type="bibr" target="#b7">Hong and Nenkova (2014)</ref> who studied models of word importance for multi- document summarization of news. The Prob, Po- sition, and Good models are inspired by their findings of the effectiveness of similar models in their setting. We found that, in our setting, Prob and Good models performed worse than assigning a uniform weight to all words. We note, however, that models from <ref type="bibr" target="#b7">Hong and Nenkova (2014)</ref> are not strictly comparable, since their word proba- bility models were calculated after stopword ex- clusion, and their model that inspired our Good model was defined somewhat differently and val- idated using content words only. The defini- tion of our Position model and its use in the es- say scoring function S (equation 2) correspond to <ref type="bibr" target="#b7">Hong and Nenkova (2014)</ref> average first location model for scoring summaries. Differently from their findings, this model is not effective for sin- gle words in our setting. Position models over n- grams with n &gt; 1 are effective, but their predic- tion is in the opposite direction of that found for the news data -the more important materials tend to appear later in the lecture, as indicated by the positive r between average first position and essay score. These findings underscore the importance of paying attention to the genre of the source ma- terial when developing summarization systems.</p><p>Our summarization task incorporates elements of contrastive opinion summarization ( <ref type="bibr" target="#b13">Paul et al., 2010;</ref><ref type="bibr" target="#b8">Kim and Zhai, 2009)</ref>, since the lecture and the reading sometimes interpret the same facts in a positive or negative light (for example, the fact that chemicals are used in fish farms is negative if compared to wild fish, but not so if compared to other farm-raised foods like poultry). Relation- ships between aspect and sentiment <ref type="bibr" target="#b2">(Brody and Elhadad, 2010;</ref><ref type="bibr" target="#b9">Lazaridou et al., 2013</ref>) are also relevant, since aspects of the same fact are em- phasized with different evaluations (the quantity vs the variety of species that go into fish meal for farmed fish). We hypothesize that units participat- ing in sentiment and aspect contrasts are of higher importance; this is a direction for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this paper, we addressed the task of automati- cally assigning importance scores to parts of a lec- ture that is to be summarized as part of an English language proficiency test. We investigated the op- timal units of information to which importance should be assigned, as well as a variety of impor- tance scoring models, drawing on the news sum- marization and essay scoring literature.</p><p>We found that bigrams and trigrams were ge- nerally more effective than unigrams and 4-grams across importance models, with some exceptions.</p><p>We also found that the most effective impor- tance models are those that equate importance of an n-gram with its preferential use in higher- scoring essays than in lower-scoring ones, above and beyond merely looking at the n-grams used in good essays. This demonstrates the utility of using not only gold, high-quality human summaries, but also sub-standard ones when developing content importance models.</p><p>Additional importance criteria that are intrinsic to the lecture, as well as those that capture contrast with a different source discussing the same topic, were also found to be reasonably effective. Since different importance models often select different items as most important, we intend to investigate complementarity of the different models.</p><p>Finally, our results highlight that the effective- ness of an importance model depends on the genre of the source text. Thus, while a first sentence baseline is very competitive in news summariza- tion, we found that important information tends not to be located in the opening sentences in our data (these tend to provide general, introductory information), but appears later on, when more de- tailed, specific claims are put forward.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 shows the correlations with essay scores.</head><label>2</label><figDesc></figDesc><table>Content 
Pearson's r 
Importance 
Model 
n=1 
n=2 
n=3 
n=4 
Na¨ıveNa¨ıve 
0.24 0.27* 0.24 
0.20 
Prob 
0.04 
0.14 
0.17 
0.14 
Position 
0.22 0.30* 0.26* 0.20 
LectVsRead 0.09 0.25* 0.31* 0.26* 
Good 
0.07 
0.15 
0.10 
0.07 
GoodVsBad 0.54* 0.42* 0.32* 0.21 

</table></figure>

			<note place="foot" n="1"> http://www.corestandards.org/ ELA-Literacy/CCRA/W.</note>

			<note place="foot" n="2"> In the future, we intend to explore more complex realization functions, allowing paraphrase, skip n-grams (as in ROUGE (Lin, 2004)), and other approximate matches, such as misspellings and inflectional variants.</note>

			<note place="foot" n="3"> Prob, Position, and LectVsRead models normalize by nR and nL to enable comparison of essays responding to different lecture + reading stimuli (prompts).</note>

			<note place="foot" n="4"> such as that fish, of fish, farming is, &quot;, fish&quot;</note>

			<note place="foot" n="5"> the other two being fishing and used.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automated Essay Scoring With e-rater R V.2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yigal</forename><surname>Attali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jill</forename><surname>Burstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Learning, and Assessment</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>Journal of Technology</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Differential Word Use Measure for Content Analysis in Automated Essay Scoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yigal</forename><surname>Attali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ETS Research Report</title>
		<imprint>
			<biblScope unit="page" from="11" to="36" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An unsupervised aspect-sentiment model for online reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Brody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noemie</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT &apos;10</title>
		<meeting><address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="804" to="812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Examining Linguistic Characteristics of Paraphrase in Test-Taker Summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jill</forename><surname>Burstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Flor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Madnani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Holtzman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ETS Research Report</title>
		<imprint>
			<biblScope unit="page" from="12" to="18" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Prompt-based content scoring for automated spoken language assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keelan</forename><surname>Evanini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shasha</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Zechner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the Eighth Workshop on Innovative Use of NLP for Building Educational Applications<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="157" to="162" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Implementation and Application of the Intelligent Essay Assessor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Foltz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynn</forename><surname>Streeter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Lochbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Landauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of automated essay evaluation: Current applications and new directions</title>
		<editor>Mark Shermis and Jill Burstein</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="68" to="88" />
		</imprint>
	</monogr>
	<note>Routhledge</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Document similarity measures to distinguish native vs. nonnative essay writers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Gurevich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Deane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers</title>
		<meeting><address><addrLine>Rochester, New York</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007-04" />
			<biblScope unit="page" from="49" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving the estimation of word importance for news multidocument summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Gottenberg, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generating comparative summaries of contradictory opinions in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><forename type="middle">Duk</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM Conference on Information and Knowledge Management, CIKM &apos;09</title>
		<meeting>the 18th ACM Conference on Information and Knowledge Management, CIKM &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="385" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A bayesian model for joint unsupervised induction of sentiment, aspect and discourse representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Sporleder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1630" to="1639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL workshop: Text summarization branches out</title>
		<meeting>ACL workshop: Text summarization branches out<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004-07" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quinn</forename><surname>Mcnemar</surname></persName>
		</author>
		<title level="m">Psychological Statistics. New York: J. Wiley and Sons</title>
		<imprint>
			<date type="published" when="1955" />
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Evaluating content selection in summarization: The pyramid method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Passonneau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Boston, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-05-02" />
			<biblScope unit="page" from="145" to="152" />
		</imprint>
	</monogr>
	<note>Human Language Technologies. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Summarizing contrastive viewpoints in opinionated text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roxana</forename><surname>Girju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;10</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;10<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="66" to="76" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Using multiple texts in an integrated writing assessment: Source text use as a predictor of score</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lia</forename><surname>Plakans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atta</forename><surname>Gebril</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Second Language Writing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="217" to="230" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Handbook of Automated Essay Evaluation: Current Applications and Future Directions</title>
		<editor>Mark Shermis and Jill Burstein</editor>
		<imprint>
			<date type="published" when="2013" />
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>Routledge</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
