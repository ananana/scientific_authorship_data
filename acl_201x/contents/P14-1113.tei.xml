<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Semantic Hierarchies via Word Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiji</forename><surname>Fu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Guo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Research Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Semantic Hierarchies via Word Embeddings</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1199" to="1209"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Semantic hierarchy construction aims to build structures of concepts linked by hypernym-hyponym (&quot;is-a&quot;) relations. A major challenge for this task is the automatic discovery of such relations. This paper proposes a novel and effective method for the construction of semantic hierarchies based on word em-beddings, which can be used to measure the semantic relationship between words. We identify whether a candidate word pair has hypernym-hyponym relation by using the word-embedding-based semantic projections between words and their hypernyms. Our result, an F-score of 73.74%, outperforms the state-of-the-art methods on a manually labeled test dataset. Moreover, combining our method with a previous manually-built hierarchy extension method can further improve F-score to 80.29%.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic hierarchies are natural ways to orga- nize knowledge. They are the main components of ontologies or semantic thesauri <ref type="bibr" target="#b15">(Miller, 1995;</ref><ref type="bibr" target="#b26">Suchanek et al., 2008</ref>). In the WordNet hierar- chy, senses are organized according to the "is-a" relations. For example, "dog" and "canine" are connected by a directed edge. Here, "canine" is called a hypernym of "dog." Conversely, "dog" is a hyponym of "canine." As key sources of knowledge, semantic thesauri and ontologies can support many natural language processing applications. However, these semantic resources are limited in its scope and domain, and their manual construction is knowledge intensive and time consuming. Therefore, many researchers * Email correspondence. have attempted to automatically extract semantic relations or to construct taxonomies.</p><p>A major challenge for this task is the auto- matic discovery of hypernym-hyponym relations. <ref type="bibr" target="#b7">Fu et al. (2013)</ref> propose a distant supervision method to extract hypernyms for entities from multiple sources. The output of their model is a list of hypernyms for a given enity (left pan- el, <ref type="figure" target="#fig_0">Figure 1</ref>). However, there usually also exists hypernym-hyponym relations among these hy- pernyms. For instance, "植 物 (plant)" and "毛 茛 科 (Ranunculaceae)" are both hyper- nyms of the entity "乌 头 (aconit)," and "植 物 (plant)" is also a hypernym of "毛 茛 科 (Ranunculaceae)." Given a list of hypernyms of an entity, our goal in the present work is to construct a semantic hierarchy of these hypernyms (right panel, <ref type="figure" target="#fig_0">Figure 1</ref>). <ref type="bibr">1</ref> Some previous works extend and refine manually-built semantic hierarchies by using other resources (e.g., Wikipedia) ( <ref type="bibr" target="#b26">Suchanek et al., 2008)</ref>. However, the coverage is limited by the scope of the resources. Several other works relied heavily on lexical patterns, which would suffer from deficiency because such patterns can only cover a small proportion of complex linguistic cir- cumstances <ref type="bibr" target="#b9">(Hearst, 1992;</ref><ref type="bibr" target="#b21">Snow et al., 2005</ref>).</p><p>Besides, distributional similarity methods <ref type="bibr" target="#b10">(Kotlerman et al., 2010;</ref><ref type="bibr" target="#b11">Lenci and Benotto, 2012</ref>) are based on the assumption that a term can only be used in contexts where its hypernyms can be used and that a term might be used in any contexts where its hyponyms are used. However, it is not always rational. Our previous method based on web mining ( <ref type="bibr" target="#b7">Fu et al., 2013</ref>) works well for hy- pernym extraction of entity names, but it is unsuit- able for semantic hierarchy construction which in- volves many words with broad semantics. More- over, all of these methods do not use the word semantics effectively. This paper proposes a novel approach for se- mantic hierarchy construction based on word em- beddings. Word embeddings, also known as dis- tributed word representations, typically represent words with dense, low-dimensional and real- valued vectors. Word embeddings have been empirically shown to preserve linguistic regular- ities, such as the semantic relationship between words ( <ref type="bibr" target="#b14">Mikolov et al., 2013b</ref>). For example,</p><formula xml:id="formula_0">v(king) − v(queen) ≈ v(man) − v(woman),</formula><p>where v(w) is the embedding of the word w. We observe that a similar property also applies to the hypernym-hyponym relationship (Section 3.3), which is the main inspiration of the present study.</p><p>However, we further observe that hypernym- hyponym relations are more complicated than a single offset can represent. To address this chal- lenge, we propose a more sophisticated and gen- eral method -learning a linear projection which maps words to their hypernyms (Section 3.3.1). Furthermore, we propose a piecewise linear pro- jection method based on relation clustering to better model hypernym-hyponym relations (Sec- tion 3.3.2). Subsequently, we identify whether an unknown word pair is a hypernym-hyponym re- lation using the projections (Section 3.4). To the best of our knowledge, we are the first to apply word embeddings to this task.</p><p>For evaluation, we manually annotate a dataset containing 418 Chinese entities and their hyper- nym hierarchies, which is the first dataset for this task as far as we know. The experimental results show that our method achieves an F-score of 73.74% which significantly outperforms the pre- vious state-of-the-art methods. Moreover, com- bining our method with the manually-built hier- archy extension method proposed by <ref type="bibr" target="#b26">Suchanek et al. (2008)</ref> can further improve F-score to 80.29%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>As main components of ontologies, semantic hi- erarchies have been studied by many researchers. Some have established concept hierarchies based on manually-built semantic resources such as WordNet <ref type="bibr" target="#b15">(Miller, 1995)</ref>. Such hierarchies have good structures and high accuracy, but their cov- erage is limited to fine-grained concepts (e.g., "Ranunculaceae" is not included in Word- Net.). We have made similar obsevation that about a half of hypernym-hyponym relations are absent in a Chinese semantic thesaurus. Therefore, a broader range of resources is needed to supple- ment the manually built resources. In the construc- tion of the famous ontology YAGO, <ref type="bibr" target="#b26">Suchanek et al. (2008)</ref> link the categories in Wikipedia onto WordNet. However, the coverage is still limited by the scope of Wikipedia.</p><p>Several other methods are based on lexical patterns. They use manually or automatically constructed lexical patterns to mine hypernym- hyponym relations from text corpora. A hierarchy can then be built based on these pairwise relations. The pioneer work by <ref type="bibr" target="#b9">Hearst (1992)</ref> has found out that linking two noun phrases (NPs) via cer- tain lexical constructions often implies hypernym relations. For example, NP 1 is a hypernym of NP 2 in the lexical pattern "such NP 1 as NP 2 ." <ref type="bibr" target="#b21">Snow et al. (2005)</ref> propose to automatically extract large numbers of lexico-syntactic patterns and subse- quently detect hypernym relations from a large newswire corpus. Their method relies on accurate syntactic parsers, and the quality of the automat- ically extracted patterns is difficult to guarantee. Generally speaking, these pattern-based methods often suffer from low recall or precision because of the coverage or the quality of the patterns.</p><p>The distributional methods assume that the con- texts of hypernyms are broader than the ones of their hyponyms. For distributional similarity com- puting, each word is represented as a semantic vector composed of the pointwise mutual infor- mation (PMI) with its contexts. <ref type="bibr" target="#b10">Kotlerman et al. (2010)</ref> design a directional distributional measure to infer hypernym-hyponym relations based on the standard IR Average Precision evaluation mea- sure. <ref type="bibr" target="#b11">Lenci and Benotto (2012)</ref> propose anoth- er measure focusing on the contexts that hyper- nyms do not share with their hyponyms. However, broader semantics may not always infer broader contexts. For example, for terms "Obama' and "American people", it is hard to say whose contexts are broader.</p><p>Our previous work ( <ref type="bibr" target="#b7">Fu et al., 2013</ref>) applies a web mining method to discover the hypernyms of Chinese entities from multiple sources. We as- sume that the hypernyms of an entity co-occur with it frequently. It works well for named enti- ties. But for class names (e.g., singers in Hong Kong, tropical fruits) with wider range of mean- ings, this assumption may fail.</p><p>In this paper, we aim to identify hypernym- hyponym relations using word embeddings, which have been shown to preserve good properties for capturing semantic relationship between words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we first define the task formally. Then we elaborate on our proposed method com- posed of three major steps, namely, word embed- ding training, projection learning, and hypernym- hyponym relation identification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task Definition</head><p>Given a list of hypernyms of an entity, our goal is to construct a semantic hierarchy on it <ref type="figure" target="#fig_0">(Figure 1)</ref>. We represent the hierarchy as a directed graph G, in which the nodes denote the words, and the edges denote the hypernym-hyponym relations. Hypernym-hyponym relations are asymmetric and transitive when words are unambiguous:</p><formula xml:id="formula_1">• ∀x, y ∈ L : x H − →y ⇒ ¬(y H − →x) • ∀x, y, z ∈ L : (x H − →z ∧ z H − →y) ⇒ x H − →y</formula><p>Here, L denotes the list of hypernyms. x, y and z denote the hypernyms in L. We use H − → to represent a hypernym-hyponym relation in this paper. Actually, x, y and z are unambiguous as the hypernyms of a certain entity. Therefore, G should be a directed acyclic graph (DAG).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Word Embedding Training</head><p>Various models for learning word embeddings have been proposed, including neural net lan- guage models ( <ref type="bibr" target="#b0">Bengio et al., 2003;</ref><ref type="bibr" target="#b16">Mnih and Hinton, 2008;</ref><ref type="bibr" target="#b14">Mikolov et al., 2013b</ref>) and spec- tral models <ref type="bibr" target="#b5">(Dhillon et al., 2011</ref>). More recent- ly, <ref type="bibr" target="#b13">Mikolov et al. (2013a)</ref> propose two log-linear models, namely the Skip-gram and CBOW model, to efficiently induce word embeddings. These two models can be trained very efficiently on a large- scale corpus because of their low time complexity. <ref type="table">Table 1</ref>: Embedding offsets on a sample of hypernym-hyponym word pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>No. Examples</head><formula xml:id="formula_2">1 v(虾) − v(对虾) ≈ v(鱼) − v(金鱼) v(shrimp) − v(prawn) ≈ v(fish) − v(gold fish) 2 v(工人) − v(木匠) ≈ v(演员) − v(小丑) v(laborer) − v(carpenter) ≈ v(actor) − v(clown) 3 v(工人) − v(木匠) ≈ v(鱼) − v(金鱼) v(laborer) − v(carpenter) ≈ v(fish) − v(gold fish)</formula><p>Additionally, their experiment results have shown that the Skip-gram model performs best in identi- fying semantic relationship among words. There- fore, we employ the Skip-gram model for estimat- ing word embeddings in this study.</p><p>The Skip-gram model adopts log-linear classi- fiers to predict context words given the current word w(t) as input. First, w(t) is projected to its embedding. Then, log-linear classifiers are em- ployed, taking the embedding as input and pre- dict w(t)'s context words within a certain range, e.g. k words in the left and k words in the right. After maximizing the log-likelihood over the entire dataset using stochastic gradient descent (SGD), the embeddings are learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Projection Learning</head><p>Mikolov et al. (2013b) observe that word em- beddings preserve interesting linguistic regulari- ties, capturing a considerable amount of syntac- tic/semantic relations. Looking at the well-known example: v(king) − v(queen) ≈ v(man) − v(woman), it indicates that the embedding offsets indeed represent the shared semantic relation be- tween the two word pairs.</p><p>We observe that the same property also ap- plies to some hypernym-hyponym relations. As a preliminary experiment, we compute the em- bedding offsets between some randomly sampled hypernym-hyponym word pairs and measure their similarities. The results are shown in <ref type="table">Table 1</ref>.</p><p>The first two examples imply that a word can also be mapped to its hypernym by utilizing word embedding offsets. However, the offset from "carpenter" to "laborer" is distant from the one from "gold fish" to "fish," indicat- ing that hypernym-hyponym relations should be more complicated than a single vector offset can represent. To verify this hypothesis, we com- pute the embedding offsets over all hypernym- hyponym word pairs in our training data and vi- sualize them. 2 <ref type="figure" target="#fig_1">Figure 2</ref> shows that the relations are adequately distributed in the clusters, which implies that hypernym-hyponym relations in- deed can be decomposed into more fine-grained relations. Moreover, the relations about animals are spatially close, but separate from the relations about people's occupations.</p><formula xml:id="formula_3">运动员-足球球员 sportsman -footballer 职员-公务员 staff -civil servant 工人-园丁 laborer -gardener 海员-领航员 seaman -navigator 演员-歌手 actor -singer 演员-主角 actor -protagonist 演员-小丑 actor -clown 职位-校长 position -headmaster 演员-斗牛士 actor -matador 工人-临时工 laborer -temporary worker 工人-木匠 laborer -carpenter 职位-总领事 position -consul general 职员-空姐 staff -airline hostess 职员-售货员 staff -salesclerk 职员-售票员 staff -conductor 鸡-公鸡 chicken -cock 羊-小尾寒羊 sheep -small-tail Han sheep 羊-公羊 sheep -ram 马-斑马 equus -zebra 虾-对虾 shrimp -prawn 狗-警犬 dog -police dog 兔-长毛兔 rabbit -wool rabbit 海豚-白鳍豚 dolphin -white-flag dolphin 鱼-鲨鱼 fish -shark 鱼-热带鱼 fish -tropical fish 鱼-金鱼 fish -gold fish 蟹-海蟹 crab -sea crab 驴-野驴</formula><p>To address this challenge, we propose to learn the hypernym-hyponym relations using projection matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">A Uniform Linear Projection</head><p>Intuitively, we assume that all words can be pro- jected to their hypernyms based on a uniform tran- sition matrix. That is, given a word x and its hy- pernym y, there exists a matrix Φ so that y = Φx. For simplicity, we use the same symbols as the words to represent the embedding vectors. Ob- taining a consistent exact Φ for the projection of all hypernym-hyponym pairs is difficult. Instead, we can learn an approximate Φ using Equation 1 on the training data, which minimizes the mean- squared error:</p><formula xml:id="formula_4">Φ * = arg min Φ 1 N (x,y) Φx − y 2 (1)</formula><p>where N is the number of (x, y) word pairs in the training data. This is a typical linear regres- sion problem. The only difference is that our pre- dictions are multi-dimensional vectors instead of scalar values. We use SGD for optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Piecewise Linear Projections</head><p>A uniform linear projection may still be under- representative for fitting all of the hypernym- hyponym word pairs, because the relations are rather diverse, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. To better model the various kinds of hypernym-hyponym relations, we apply the idea of piecewise linear re- gression ( <ref type="bibr">Ritzema, 1994)</ref> in this study. Specifically, the input space is first segmented into several regions. That is, all word pairs (x, y) in the training data are first clustered into sever- al groups, where word pairs in each group are expected to exhibit similar hypernym-hyponym relations. Each word pair (x, y) is represented with their vector offsets: y − x for clustering. The reasons are twofold: (1) Mikolov's work has shown that the vector offsets imply a certain lev- el of semantic relationship. <ref type="formula">(2)</ref> The vector off- sets distribute in clusters well, and the word pairs which are close indeed represent similar relations, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>Then we learn a separate projection for each cluster, respectively (Equation 2).</p><formula xml:id="formula_5">Φ * k = arg min Φ k 1 N k (x,y)∈C k Φ k x − y 2 (2)</formula><p>where N k is the amount of word pairs in the k th cluster C k . We use the k-means algorithm for clustering, where k is tuned on a development dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Training Data</head><p>To learn the projection matrices, we extract train- ing data from a Chinese semantic thesaurus, Tongyi Cilin (Extended) (CilinE for short) which The senses of words in the first level, such as "物 (object)" and "时间 (time)," are very gen- eral. The fourth level only has sense codes without real words. Therefore, we extract words in the sec- ond, third and fifth levels to constitute hypernym- hyponym pairs (left panel, <ref type="figure" target="#fig_2">Figure 3</ref>).</p><p>Note that mapping one hyponym to multi- ple hypernyms with the same projection (Φx is unique) is difficult. Therefore, the pairs with the same hyponym but different hypernyms are ex- pected to be clustered into separate groups. <ref type="figure" target="#fig_2">Fig- ure 3</ref> shows that the word "dragonfly" in the fifth level has two hypernyms: "insect" in the third level and "animal" in the second level. Hence the relations dragonfly H − → insect and dragonfly H − → animal should fall into differ- ent clusters.</p><p>In our implementation, we apply this constraint by simply dividing the training data into two cat- egories, namely, direct and indirect. Hypernym- hyponym word pair (x, y) is classified into the di- rect category, only if there doesn't exist another word z in the training data, which is a hypernym of x and a hyponym of y. Otherwise, (x, y) is classi- fied into the indirect category. Then, data in these two categories are clustered separately.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Hypernym-hyponym Relation Identification</head><p>Upon obtaining the clusters of training data and the corresponding projections, we can identify whether two words have a hypernym-hyponym re- lation. Given two words x and y, we find cluster C k whose center is closest to the offset y − x, and obtain the corresponding projection Φ k . For y to be considered a hypernym of x, one of the two conditions below must hold. Condition 1: The projection Φ k puts Φ k x close enough to y <ref type="figure">(Figure 4)</ref>. Formally, the euclidean distance between Φ k x and y: d(Φ k x, y) must be less than a threshold δ.</p><formula xml:id="formula_6">d(Φ k x, y) = Φ k x − y 2 &lt; δ<label>(3)</label></formula><p>Condition 2: There exists another word z sat- isfying x H − →z and z H − →y. In this case, we use the transitivity of hypernym-hyponym relations.</p><p>Besides, the final hierarchy should be a DAG as discussed in Section 3.1. However, the pro- jection method cannot guarantee that theoretical- ly, because the projections are learned from pair- wise hypernym-hyponym relations without the w- hole hierarchy structure. All pairwise hypernym- hyponym relation identification methods would suffer from this problem actually. It is an inter- esting problem how to construct a globally opti-mal semantic hierarchy conforming to the form of a DAG. But this is not the focus of this paper. So if some conflicts occur, that is, a relation cir- cle exists, we remove or reverse the weakest path heuristically ( <ref type="figure">Figure 5</ref>). If a circle has only two nodes, we remove the weakest path. If a circle has more than two nodes, we reverse the weakest path to form an indirect hypernym-hyponym relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Data</head><p>In this work, we learn word embeddings from a Chinese encyclopedia corpus named Baidubaike 4 , which contains about 30 million sentences (about 780 million words). The Chinese segmentation is provided by the open-source Chinese language processing platform LTP <ref type="bibr">5 (Che et al., 2010)</ref>. Then, we employ the Skip-gram method (Section 3.2) to train word embeddings. Finally we obtain the embedding vectors of 0.56 million words.</p><p>The training data for projection learning is collected from CilinE (Section 3.3.3). We ob- tain 15,247 word pairs of hypernym-hyponym relations (9,288 for direct relations and 5,959 for indirect relations).</p><p>For evaluation, we collect the hypernyms for 418 entities, which are selected randomly from Baidubaike, following <ref type="bibr" target="#b7">Fu et al. (2013)</ref>. We then ask two annotators to manually label the seman- tic hierarchies of the correct hypernyms. The final data set contains 655 unique hypernyms and 1,391 hypernym-hyponym relations among them. We randomly split the labeled data into 1/5 for de- velopment and 4/5 for testing ( <ref type="table">Table 2</ref>). The hi- erarchies are represented as relations of pairwise words. We measure the inter-annotator agreement using the kappa coefficient <ref type="bibr" target="#b20">(Siegel and Castellan Jr, 1988)</ref>. The kappa value is 0.96, which indi- cates a good strength of agreement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Metrics</head><p>We use precision, recall, and F-score as our met- rics to evaluate the performances of the methods.</p><p>Since hypernym-hyponym relations and its re- verse (hyponym-hypernym) have one-to-one cor- respondence, their performances are equal. For 4 Baidubaike (baike.baidu.com) is one of the largest Chinese encyclopedias containing more than 7.05 million en- tries as of <ref type="bibr">September, 2013.</ref> 5 www.ltp-cloud.com/demo/ Relation # of word pairs Dev. <ref type="table" target="#tab_1">Test  hypernym-hyponym  312  1,079  hyponym-hypernym  *   312  1,079  unrelated  1,044  3,250  Total  1,668  5,408   Table 2</ref>: The evaluation data. * Since hypernym- hyponym relations and hyponym-hypernym relations have one-to-one correspondence, their numbers are the same. simplicity, we only report the performance of the former in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Varying the Amount of Clusters</head><p>We first evaluate the effect of different number of clusters based on the development data. We vary the numbers of the clusters both for the direct and indirect training word pairs. As shown in <ref type="figure" target="#fig_5">Figure 6</ref>, the performance of clus- tering is better than non-clustering (when the clus- ter number is 1), thus providing evidences that learning piecewise projections based on clustering is reasonable. We finally set the numbers of the clusters of direct and indirect to 20 and 5, respec- tively, where the best performances are achieved on the development data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison with Previous Work</head><p>In this section, we compare the proposed method with previous methods, including manually-built hierarchy extension, pairwise relation extraction   based on patterns, word distributions, and web mining (Section 2). Results are shown in <ref type="table" target="#tab_1">Table 3</ref>.</p><formula xml:id="formula_7">P(%) R(%) F(%) M W</formula><formula xml:id="formula_8">Pattern Translation w 是[一个|一种] h w is a [a kind of] h w [、] 等 h w[,] and other h h [，] 叫[做] w h[,] called w h [，] [像]如 w h[,] such as w h [，] 特别是 w h[,] especially w</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Overall Comparison</head><p>M W iki+CilinE refers to the manually-built hierar- chy extension method of <ref type="bibr" target="#b26">Suchanek et al. (2008)</ref>.</p><p>In our experiment, we use the category taxonomy of Chinese Wikipedia 6 to extend CilinE. <ref type="table" target="#tab_1">Table 3</ref> shows that this method achieves a high precision but also a low recall, mainly because of the limit- ed scope of Wikipedia. M P attern refers to the pattern-based method of Hearst (1992). We extract hypernym-hyponym relations in the Baidubaike corpus, which is al- so used to train word embeddings (Section 4.1). We use the Chinese Hearst-style patterns <ref type="table" target="#tab_2">(Table  4)</ref> proposed by <ref type="bibr" target="#b7">Fu et al. (2013)</ref>, in which w rep- resents a word, and h represents one of its hy- pernyms. The result shows that only a small part of the hypernyms can be extracted based on these patterns because only a few hypernym relations are expressed in these fixed patterns, and many are expressed in highly flexible manners.</p><p>In the same corpus, we apply the method M Snow originally proposed by <ref type="bibr" target="#b21">Snow et al. (2005)</ref>. The same training data for projections learn-ing from CilinE (Section 3.3.3) is used as seed hypernym-hyponym pairs. Lexico-syntactic patterns are extracted from the Baidubaike corpus by using the seeds. We then develop a logistic re- gression classifier based on the patterns to recog- nize hypernym-hyponym relations. This method relies on an accurate syntactic parser, and the qual- ity of the automatically extracted patterns is diffi- cult to guarantee.</p><p>We re-implement two previous distribution- al methods M balApinc ( <ref type="bibr" target="#b10">Kotlerman et al., 2010)</ref> and M invCL ( <ref type="bibr" target="#b11">Lenci and Benotto, 2012</ref>) in the Baidubaike corpus. Each word is represented as a feature vector in which each dimension is the PMI value of the word and its context words. We com- pute a score for each word pair and apply a thresh- old to identify whether it is a hypernym-hyponym relation.</p><p>M F u refers to our previous web mining method ( <ref type="bibr" target="#b7">Fu et al., 2013)</ref>. This method mines hy- pernyms of a given word w from multiple sources and returns a ranked list of the hypernyms. We select the hypernyms with scores over a threshold of each word in the test set for evaluation. This method assumes that frequent co-occurrence of a noun or noun phrase n in multiple sources with w indicate possibility of n being a hypernym of w. The results presented in <ref type="bibr" target="#b7">Fu et al. (2013)</ref> show that the method works well when w is an entity, but not when w is a word with a common semantic concept. The main reason may be that there are relatively more introductory pages about entities than about common words in the Web.</p><p>M Emb is the proposed method based on word embeddings. <ref type="table" target="#tab_1">Table 3</ref> shows that the proposed method achieves a better recall and F-score than all of the previous methods do. It can significantly (p &lt; 0.01) improve the F-score over the state-of- the-art method M W iki+CilinE .</p><p>M Emb and M CilinE can also be combined. The combination strategy is to simply merge all pos- itive results from the two methods together, and then to infer new relations based on the transitiv- ity of hypernym-hyponym relations. The F-score is further improved from 73.74% to 76.29%. Note that, the combined method achieves a 4.43% re- call improvement over M Emb , but the precision is almost unchanged. The reason is that the infer- ence based on the relations identified automatical- ly may lead to error propagation. For example, the relation x H − →y is incorrectly identified by M Emb .   Combining M Emb with M W iki+CilinE achieves a 7% F-score improvement over the best baseline M W iki+CilinE . Therefore, the proposed method is complementary to the manually-built hierarchy extension method <ref type="bibr" target="#b26">(Suchanek et al., 2008</ref>).</p><formula xml:id="formula_9">P(%) R(%) F(%) M W</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Comparison on the Out-of-CilinE Data</head><p>We are greatly interested in the practical perfor- mance of the proposed method on the hypernym- hyponym relations outside of CilinE. We say a word pair is outside of CilinE, as long as there is one word in the pair not existing in CilinE. In our test data, about 62% word pairs are outside of CilinE. <ref type="table" target="#tab_4">Table 5</ref> shows the performances of the best baseline method and our method on the out- of-CilinE data. The method exploiting the tax- onomy in Wikipedia, M W iki+CilinE , achieves the highest precision but has a low recall. By con- trast, our method can discover more hypernym- hyponym relations with some loss of precision, thereby achieving a more than 29% F-score im- provement. The combination of these two meth- ods achieves a further 4.5% F-score improvement over M Emb+CilinE . Generally speaking, the pro- posed method greatly improves the recall but dam- ages the precision. Actually, we can get different precisions and re-  calls by adjusting the threshold δ (Equation 3). <ref type="figure" target="#fig_6">Figure 7</ref> shows that M Emb+CilinE achieves a high- er precision than M W iki+CilinE when their recalls are the same. When they achieve the same preci- sion, the recall of M Emb+CilinE is higher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Error Analysis and Discussion</head><p>We analyze error cases after experiments. Some cases are shown in <ref type="figure" target="#fig_8">Figure 8</ref>. We can see that there is only one general relation "植物 (plant)" H − → "生物 (organism)" existing in CilinE. Some fine-grained relations exist in Wikipedia, but the coverage is limited.</p><p>Our method based on word embeddings can discover more hypernym- hyponym relations than the previous methods can. When we combine the methods together, we get the correct hierarchy. <ref type="figure" target="#fig_8">Figure 8</ref> shows that our method loses the relation "乌 头 属 (Aconitum)" H − → "毛 茛 科 (Ranunculaceae)." It is because they are very semantically similar (their cosine similarity is 0.9038). Their representations are so close to each other in the embedding space that we have not find projections suitable for these pairs. The error statistics show that when the cosine similari- ties of word pairs are greater than 0.8, the recall is only 9.5%. This kind of error accounted for about 10.9% among all the errors in our test set. One possible solution may be adding more data of this kind to the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>In addition to the works mentioned in Section 2, we introduce another set of related studies in this section.</p><p>Evans <ref type="formula">(2004)</ref>, <ref type="bibr" target="#b17">Ortega-Mendoza et al. (2007), and</ref><ref type="bibr" target="#b19">Sang (2007)</ref> consider web data as a large cor- pus and use search engines to identify hypernyms based on the lexical patterns of <ref type="bibr" target="#b9">Hearst (1992)</ref>. However, the low quality of the sentences in the search results negatively influence the precision of hypernym extraction.</p><p>Following the method for discovering patterns automatically ( <ref type="bibr" target="#b21">Snow et al., 2005</ref>), <ref type="bibr" target="#b12">McNamee et al. (2008)</ref> apply the same method to extract hy- pernyms of entities in order to improve the perfor- mance of a question answering system. <ref type="bibr" target="#b18">Ritter et al. (2009)</ref> propose a method based on patterns to find hypernyms on arbitrary noun phrases. They use a support vector machine classifier to identify the correct hypernyms from the candidates that match the patterns. As our experiments show, pattern- based methods suffer from low recall because of the low coverage of patterns.</p><p>Besides <ref type="bibr" target="#b10">Kotlerman et al. (2010)</ref> and <ref type="bibr" target="#b11">Lenci and Benotto (2012)</ref>, other researchers also propose di- rectional distributional similarity methods ( <ref type="bibr" target="#b29">Weeds et al., 2004;</ref><ref type="bibr" target="#b8">Geffet and Dagan, 2005;</ref><ref type="bibr" target="#b1">Bhagat et al., 2007;</ref><ref type="bibr" target="#b27">Szpektor et al., 2007;</ref><ref type="bibr" target="#b3">Clarke, 2009)</ref>. How- ever, their basic assumption that a hyponym can only be used in contexts where its hypernyms can be used and that a hypernym might be used in all of the contexts where its hyponyms are used may not always rational. <ref type="bibr" target="#b22">Snow et al. (2006)</ref> provides a global optimiza- tion scheme for extending WordNet, which is d- ifferent from the above-mentioned pairwise rela- tionships identification methods.</p><p>Word embeddings have been successfully ap- plied in many applications, such as in sentiment analysis <ref type="bibr" target="#b25">(Socher et al., 2011b</ref>), paraphrase detec- tion ( <ref type="bibr" target="#b24">Socher et al., 2011a</ref>), chunking, and named entity recognition ( <ref type="bibr" target="#b28">Turian et al., 2010;</ref><ref type="bibr" target="#b4">Collobert et al., 2011</ref>). These applications mainly utilize the representing power of word embeddings to al- leviate the problem of data sparsity. <ref type="bibr" target="#b13">Mikolov et al. (2013a)</ref> and <ref type="bibr" target="#b14">Mikolov et al. (2013b)</ref> further ob- serve that the semantic relationship of words can be induced by performing simple algebraic oper- ations with word vectors. Their work indicates that word embeddings preserve some interesting linguistic regularities, which might provide sup- port for many applications. In this paper, we improve on their work by learning multiple lin- ear projections in the embedding space, to model hypernym-hyponym relationships within different clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>This paper proposes a novel method for seman- tic hierarchy construction based on word em- beddings, which are trained using a large-scale corpus. Using the word embeddings, we learn the hypernym-hyponym relationship by estimat- ing projection matrices which map words to their hypernyms. Further improvements are made us- ing a cluster-based approach in order to model the more fine-grained relations. Then we propose a few simple criteria to identity whether a new word pair is a hypernym-hyponym relation. Based on the pairwise hypernym-hyponym relations, we build semantic hierarchies automatically.</p><p>In our experiments, the proposed method signif- icantly outperforms state-of-the-art methods and achieves the best F1-score of 73.74% on a manual- ly labeled test dataset. Further experiments show that our method is complementary to the previous manually-built hierarchy extension methods.</p><p>For future work, we aim to improve word embedding learning under the guidance of hypernym-hyponym relations. By including the hypernym-hyponym relation constraints while training word embeddings, we expect to improve the embeddings such that they become more suit- able for this task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of semantic hierarchy construction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Clusters of the vector offsets in training data. The figure shows that the vector offsets distribute in some clusters. The left cluster shows some hypernym-hyponym relations about animals. The right one shows some relations about people's occupations.</figDesc><graphic url="image-1.png" coords="4,212.72,135.98,132.00,63.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Hierarchy of CilinE and an Example of Training Data Generation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>3</head><label></label><figDesc>www.ltp-cloud.com/download/</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: In this example, Φ k x is located in the circle with center y and radius δ. So y is considered as a hypernym of x. Conversely, y is not a hypernym of x .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Performance on development data w.r.t. cluster size.</figDesc><graphic url="image-2109.png" coords="6,306.10,212.57,230.79,200.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Precision-Recall curves on the out-ofCilinE data in the test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: An example for error analysis. The red paths refer to the relations between the named entity and its hypernyms extracted using the web mining method (Fu et al., 2013). The black paths with hollow arrows denote the relations identified by the different methods. The boxes with dotted borders refer to the concepts which are not linked to correct positions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Comparison of the proposed method with existing methods in the test set.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Chinese Hearst-style lexical patterns. The contents in square brackets are omissible.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 : Performance on the out-of-CilinE data+W i k i +C i l i nE M E mb +C i l i nE M W i k i +C i l i nE</head><label>5</label><figDesc></figDesc><table>in 
the test set. 

0.0 
0.2 
0.4 
0.6 
0.8 
1.0 

0.0 

0.2 

0.4 

0.6 

0.8 

1.0 

Recall 

Precision 

q 
q q 

q 
q 
q 
q 
q 
q 

q 

q 

q 
q 
q 

q 

M E mb </table></figure>

			<note place="foot" n="1"> In this study, we focus on Chinese semantic hierarchy construction. The proposed method can be easily adapted to other languages.</note>

			<note place="foot" n="2"> Principal Component Analysis (PCA) is applied for dimensionality reduction.</note>

			<note place="foot" n="6"> dumps.wikimedia.org/zhwiki/20131205/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by National Natu-ral Science Foundation of China (NSFC) via grant 61133012, 61273321 and the National 863 Leading Technology Research Project via grant 2012AA011102. Special thanks to Shiqi Zhao, Zhenghua Li, Wei Song and the anonymous re-viewers for insightful comments and suggestions. We also thank Xinwei Geng and Hongbo Cai for their help in the experiments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ledir: An unsupervised algorithm for learning directionality of inference rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Bhagat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eduard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marina</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="161" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ltp: A chinese language technology platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Coling 2010: Demonstrations</title>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-08" />
			<biblScope unit="page" from="13" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Context-theoretic semantics for natural language: an overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daoud</forename><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Geometrical Models of Natural Language Semantics</title>
		<meeting>the Workshop on Geometrical Models of Natural Language Semantics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="112" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-view learning of word embeddings via cca</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paramveer</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyle H</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="199" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A framework for named entity recognition in the open domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recent Advances in Natural Language Processing III: Selected Papers from RANLP 2003</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">260</biblScope>
			<biblScope unit="page" from="267" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exploiting multiple sources for open-domain hypernym discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiji</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1224" to="1234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The distributional inclusion hypotheses and lexical entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maayan</forename><surname>Geffet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="107" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automatic acquisition of hyponyms from large text corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marti A Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th conference on Computational linguistics</title>
		<meeting>the 14th conference on Computational linguistics</meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="539" to="545" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Directional distributional similarity for lexical inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Kotlerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Szpektor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maayan</forename><surname>Zhitomirsky-Geffet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="359" to="389" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Identifying hypernyms in distributional semantic spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulia</forename><surname>Benotto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Workshop on Semantic Evaluation</title>
		<meeting>the Sixth International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="75" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning named entity hyponyms for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Schone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Mayfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Joint Conference on Natural Language Processing</title>
		<meeting>the Third International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="799" to="804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>arX- iv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACLHLT</title>
		<meeting>NAACLHLT</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A scalable hierarchical distributed language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1081" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Using lexical patterns for extracting hyponyms from the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Ortega-Mendoza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Villaseñor-Pineda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Montes-Y Gómez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICAI 2007: Advances in Artificial Intelligence</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="904" to="911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">What is this, anyway: Automatic hypernym discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 AAAI Spring Symposium on Learning by Reading and Learning to Read</title>
		<meeting>the 2009 AAAI Spring Symposium on Learning by Reading and Learning to Read</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="88" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Extracting hypernym pairs from the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik Tjong Kim</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions</title>
		<meeting>the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="165" to="168" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Nonparametric statistics for the behavioral sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sidney</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>John Castellan</surname><genName>Jr</genName></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<publisher>McGraw-Hill</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning syntactic patterns for automatic hypernym discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Lawrence K. Saul, Yair Weiss, and Léon Bottou</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1297" to="1304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semantic taxonomy induction from heterogenous evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and 44th</title>
		<meeting>the 21st International Conference on Computational Linguistics and 44th</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="801" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pennin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="801" to="809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Yago: A large ontology from wikipedia and wordnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gjergji</forename><surname>Fabian M Suchanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Web Semantics: Science, Services and Agents on the World Wide Web</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="203" to="217" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Instance-based evaluation of entailment rule acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Szpektor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eyal</forename><surname>Shnarch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="456" to="463" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Characterising measures of lexical distributional similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Weeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Mccarthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on Computational Linguistics</title>
		<meeting>the 20th international conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page">1015</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
