<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:56+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Progressive Learning Approach to Chinese SRL Using Heterogeneous Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaolin</forename><surname>Xia</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sha</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of EECS</orgName>
								<orgName type="department" key="dep2">Beijing Advanced Innovation Center for Imaging Technology</orgName>
								<orgName type="laboratory">Key Laboratory of Computational Linguistics (Ministry of Education)</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Capital Normal University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Progressive Learning Approach to Chinese SRL Using Heterogeneous Data</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2069" to="2077"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1189</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Previous studies on Chinese semantic role labeling (SRL) have concentrated on a single semantically annotated corpus. But the training data of single corpus is often limited. Whereas the other existing semantically annotated corpora for Chi-nese SRL are scattered across different annotation frameworks. But still, Data sparsity remains a bottleneck. This situation calls for larger training datasets, or effective approaches which can take advantage of highly heterogeneous data. In this paper, we focus mainly on the latter, that is, to improve Chinese SRL by using heterogeneous corpora together. We propose a novel progressive learning model which augments the Progressive Neural Network with Gated Recurrent Adapters. The model can accommodate heterogeneous inputs and effectively transfer knowledge between them. We also release a new corpus, Chinese Sem-Bank, for Chinese SRL 1. Experiments on CPB 1.0 show that our model outperforms state-of-the-art methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic role labeling (SRL) is one of the fun- damental tasks in natural language processing be- cause of its important role in information extrac- tion ( <ref type="bibr" target="#b1">Bastianelli et al., 2013)</ref>, statistical machine translation ( <ref type="bibr" target="#b0">Aziz et al., 2016;</ref><ref type="bibr" target="#b15">Xiong et al., 2012)</ref>, and so on.</p><p>However, state-of-the-art performance of Chi- nese SRL is still far from satisfactory. And data sparsity has been a bottleneck which can not be <ref type="bibr">1</ref>  ] 。 <ref type="figure">Figure 1</ref>: Sentences from (a) CPB and (b) our het- erogeneous dataset. In CPB, each predicate (e.g., 修改) has a specific set of core roles given with numbers (e.g., Arg0). While our dataset uses a different semantic role set, and all roles are non- predicate-specific.</p><p>ignored. For English, the most commonly used benchmark dataset PropBank ( <ref type="bibr" target="#b18">Xue and Palmer, 2003)</ref> has about 54,900 sentences. But for Chi- nese, there are only 10,364 sentences in Chinese PropBank 1.0 (CPB) (with about 35,700 proposi- tions) <ref type="bibr" target="#b17">(Xue, 2008)</ref>.</p><p>To mitigate the data sparsity, models incor- porating heterogeneous resources have been in- troduced to improve Chinese SRL performance ( <ref type="bibr" target="#b14">Wang et al., 2015;</ref><ref type="bibr" target="#b8">Guo et al., 2016;</ref>. The heterogeneous resources introduced by these models include other semantically an- notated corpora with annotation schema different to that used in PropBank, and even of a differ- ent language. The challenge here lies in the fact that those newly introduced resources are hetero- geneous in nature, without sharing the same tag- ging schema, semantic role set, syntactic tag set and domain. For example, <ref type="bibr" target="#b14">Wang et al. (2015)</ref> introduced a heterogeneous dataset, Chinese Net- Bank, by pretraining word embeddings. Specifi- cally, they learn an LSTM RNN model based on NetBank first, then initialize a new model with the pretrained embeddings obtained from NetBank, and then train it on CPB. Chinese NetBank <ref type="bibr" target="#b21">(Yulin, 2007)</ref> is also a corpus annotated with seman- tic roles, but using a very different role set and annotation schema. Wang's method can inherit knowledge acquired from other resources con- veniently, but only at word representation level, missing more generalized semantic meanings in higher hidden layers.  proposed a two- pass training approach to use corpora of two lan- guages, but a few non-common roles are ignored in the first pass. <ref type="bibr" target="#b8">Guo et al. (2016)</ref> proposed a uni- fied neural network model for SRL and relation classification (RC). It can learn two tasks at the same time, but cannot filter out harmful features learned in incompatible tasks.</p><p>Recently, Progressive Neural Networks (PNN) model was proposed by <ref type="bibr" target="#b10">Rusu et al. (2016)</ref> to trans- fer learned reinforcement learning policies from one game to another, or from simulation to the real robot. PNN "freezes" learned parameters once starting to learn a new task, and it uses lateral connections, namely adapter, to access previously learned features.</p><p>Inspired by the PNN model, we propose a pro- gressive learning model to Chinese semantic role labeling in this paper. Especially, we extend the model with Gated Recurrent Adapters (GRA). Since the standard PNN takes pixels as input, poli- cies as output, it is not suitable for SRL task we focus in this context. Moreover, to handle long sentences in the corpus, we enhance adapters with internal memories, and gates to keep the gradient stable. The contributions of this paper are three- fold:</p><p>1. We reconstruct PNN columns with bidirec- tional LSTMs to introduce heterogeneous corpora to improve Chinese SRL. The archi- tecture can also be applied to a wider range of NLP tasks, like event extraction and relation classification, etc.</p><p>2. We further extend the model with GRA to re- member and take advantage of what has been transferred, thus improve the performance on long sentences.</p><p>3. We also release a new corpus, Chinese Sem- Bank, which was annotated with the schema different to that used in CPB. We hope that it will be helpful for future work on SRL tasks.</p><p>Subjective roles: agent(施事), co-agent(同事), experiencer(当事) , indirect experiencer(接事)</p><p>Objective roles: patient(受事), relative(系事), dative(与事) , result(结果), content(内容), target(对象)</p><p>Space roles: a point of departure(起点) , a point of arrival(终点) , path(路径), direction(方向), location(处所)</p><p>Time roles: start time(起始), end time(结束), time point(时点) , duration(时段)</p><p>Comparison roles: comparison subject(比较 主体), comparison object(比较对象) , comparison range(比较范围), comparison thing(比较项目) , comparison result(比较结 果)</p><p>Others: instrument(工具) , material(材料) , manner(方式) , quantity(物量) , range(范围) , reason(原因) , purpose(目的) We use our new corpus as a heterogeneous resource, and evaluate the proposed model on the benchmark dataset CPB 1.0. The experi- ment shows that our approach achieves 79.67% F1 score, significantly outperforms existing state-of- the-art systems by a large margin (Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Heterogeneous Corpora for Chinese SRL</head><p>In this paper, we provide a new SRL corpus Chi- nese SemBank (CSB) and use it as an example of heterogeneous data in our experiments. In this section, we first briefly introduce the corpus, then compare it to existing corpora. Sentences in CSB are from various sources in- cluding online articles and news. The vision of this project is to build a very large and complete Chinese semantic corpus in the future. Currently, it only focuses on the predicate-argument struc- tures in a sentence without annotation of the tem- poral relations and coreference. CBS is different with respect to commonly used dataset CPB in the following aspects:</p><p>• In terms of predicate, CSB takes wider range of predicates into account. We not only anno- tated common verbs, but also nominal verbs, as NomBank does, and state words. Whereas CPB only annotate common verbs as predi- cates.</p><p>• In terms of semantic roles, CSB has a more fine-grained semantic role set. There are 31 roles defined in five types (as <ref type="table">Table.</ref> 1 shows). Whereas in CPB, there are totally 23 roles, including core roles and non-core roles.</p><p>• CSB does not have any pre-defined frames for predicates because all roles are set to be non-predicate-specific. The reason for not defining frames is that frames may lead in- consistencies in labels. For example, accord- ing to Chinese verb formation theory ( <ref type="bibr" target="#b13">Sun et al., 2009)</ref>, in CPB, an agent of a verb is often marked as its Arg0, but not all Arg0 are agents. Therefore, roles are defined for predi- cates with similar syntactic and semantic reg- ularities, rather than single predicate.</p><p>Two direct benefits of using stand-alone non- predicate-specific roles are: First, meanings of all semantic roles can be directly inferred from their labels. For instance, roles of things that people are telling (谈 ) or looking (看) are labeled as 内 容/content, because verbs like 谈 and 看 are often followed by an object. Second, we can easily annotate sentences with new predicates without defining new frame files.</p><p>Other Corpora for Chinese SRL Other popular semantic role labeling corpora include Chinese NomBank <ref type="bibr" target="#b16">(Xue, 2006</ref>), Peking University Chi- nese NetBank <ref type="bibr" target="#b21">(Yulin, 2007)</ref>. NomBank, often used as a complement to PropBank, annotates nominal predicates and semantic roles according to the similar semantic schema as PropBank does. Peking University Chinese NetBank was created by adding a semantic layer to Peking University Chinese TreeBank ( <ref type="bibr" target="#b22">Zhou et al., 1997)</ref>. It only uses non-predicate-specific roles as we do. And its role set is smaller, which has 20 roles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Challenges in Inheriting Knowledge from Heterogeneous Corpora</head><p>Although there are a lot of annotated corpora for Chinese SRL as we mentioned in the previous sec- tion, most of them are quite small as compared to that in English. Data sparsity remains a bot- tleneck. This situation calls for larger training dataset, or effective approaches which can take ad- vantage of very heterogeneous datasets. In this pa- per, we focus on the second problem, that is, to improve Chinese SRL by using heterogeneous cor- pora together within one model.</p><p>We will consider the combination of the stan- dard benchmark, CPB 1.0 dataset <ref type="bibr" target="#b18">(Xue and Palmer, 2003)</ref>, with the new corpus, CSB, because there are a lot of differences between them, as we discussed in Section 2. Consequently, a number of challenges arise for this task. Now we describe them as below.</p><p>Inheriting from Different Schema and Role Sets. CPB was annotated with PropBank-style frames and roles, whereas Chinese FrameNet uses its own frames and roles. And our dataset has no frame files and use different role set. Therefore, it is hard to find explicit mapping or hierarchical re- lationships among their role sets, or decide which system is better, especially when there are more than two resources. Inheriting from Different Domain/Genre. The datasets mentioned above are composed of sen- tences from various sources, including news and stories, etc. However, it is well known that adding data in very different genre to training data may hurt parser performance <ref type="bibr" target="#b2">(Bikel, 2004)</ref>. There- fore, we also need to deal with domain adapta- tion problem when using heterogeneous data. In other words, the proposed approach should be ro- bust to harmful features learned on incompatible datasets. It can also accommodate potentially dif- ferent model structures and inputs in the procedure of knowledge fusion. Inheriting from Different Syntactic Annota- tion. Unlikes English, previous works <ref type="bibr" target="#b5">(Ding and Chang, 2009;</ref><ref type="bibr" target="#b13">Sun et al., 2009)</ref> on Chinese SRL task often use both correct segmentation and part-of-speech tagging, and even treebank gold- standard parses (Xue, 2008) as their features. But some corpora like CPB and NetBank do not share the same PoS tag set, or do not have correct PoS tagging and gold treebank parses at all, like CSB. And in real application scenarios, it is more conve- nient to use automatic PoS tagging instead of gold- standard tagging on large datasets, as they can be obtained quickly. So to deal with the absence of syntactic features, we adopt automatic PoS tag- ging when training on CSB in this work.</p><p>Some previous techniques, such as finetuning after pretraining ( <ref type="bibr" target="#b14">Wang et al., 2015;</ref> and multi-task learning ( <ref type="bibr" target="#b8">Guo et al., 2016)</ref>, have</p><formula xml:id="formula_0">h (1) 1 h (1) 2 h (2) 1 h (2) 2 input output 1 output 2 σ σ (a) h (1) 1 h (1) 2 h (2) 1 h (2) 2 input output 1 output 2 GRA GRA Heter. 2 Target h (1) 1 h (1) 2 output 1 GRA GRA Heter. 1 (b)</formula><p>Figure 2: Depiction of the standard Progressive Neural Network architecture (a) and ours PNN GRA model (b). Our model uses Gated Recur- rent Adapters (GRA), instead of sigmoid adapters to access previous knowledge in previous columns learned on heterogeneous data. If there are more than one heterogeneous resources available, more columns can be added on the left.</p><p>been used to deal with these challenges. Though they can also leverage knowledge from different domains, they have following drawbacks: finetun- ing cannot avoid catastrophic forgetting because learned parameters, whether embeddings or other hidden weights, will be tuned after the model has been initialized; And multi-task learning can- not ignore previously learned harmful features be- cause some features are learned in shared layers, although it avoids forgetting by randomly select- ing a task to learn at each iteration. Therefore, to solve the above-mentioned challenges, we further introduce progressive learning which we believe is more suitable for the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Progressive Learning Approach</head><p>We propose a progressive learning approach which is ideal for combining heterogeneous SRL data for multiple reasons. First, it can accom- modate dissimilar inputs with different schema, syntactic information and domain, because it al- low models for heterogeneous resources to be ex- tremely different, such as different network struc- tures, different width, and different learning rates, etc. Second, it is immune to forgetting by freezing learned weights and can leverage prior knowledge via lateral connections. Third, the lateral connec- tions can be extended with recurrent structure and gate mechanism to handle with forgetting problem over long distance.</p><p>Our model is mainly inspired by <ref type="bibr" target="#b10">Rusu et al. (2016)</ref>. They proposed progressive neural net- works for a wide variety of reinforcement learning tasks (e.g. Atari games and robot simulation). In their cases, inputs are pixels, outputs are learned policies. And each column, consisting of simple layers and convolutional layers, is trained to solve a particular Markov Decision Process. But in our case, inputs are sentences annotated using differ- ent syntactic tagsets and outputs are semantic role sequences. So we change the structure of columns to recurrent neural networks with LSTM, similar to the model proposed by <ref type="bibr" target="#b14">Wang et al. (2015)</ref>. Be- low we first introduce basic progressive neural net- work architecture, then describe our model, PNN with gated recurrent adapters. with n i units is h 1 i ∈ R n i . Θ 1 denotes the parameters to be learned in the first column. When switching to a second corpus, it "freezes" the parameter Θ 1 and randomly initialize a new column with parameters Θ 2 and several lateral connections between two columns so that layer h 2 i can receive input from both h 2 i−1 and h 1 i−1 . In this straightforward man- ner, progressive neural networks can make use of columns with any structures or to compile lateral connections in an ensemble setting. To be more general, we calculate the output of ith layer in kth column h k i by:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Progressive Neural Networks</head><formula xml:id="formula_1">h k i = f (W k i h k i−1 + j&lt;k U (k:j) i h j i−1 )<label>(1)</label></formula><p>where</p><formula xml:id="formula_2">W k i ∈ R n k i ×n k i−1</formula><note type="other">is the weight matrix of layer i of column k, U (k:j) i ∈ R n k i ×n j i−1 are the lat- eral connections to transfer information from layer i − 1 of column j to layer i of column k, h 0</note><p>is the input of the network. f can be any activation function, such as element-wise non-linearity. Bias term was omitted in the equation. Adapters. With implicit assumption that there is some "overlap" between the first task and the sec- ond task, pretrain-and-finetune learning paradigm is effective, as only slight adjustment to param- eters is needed to learn new features. Progres- sive networks also have ability to transfer knowl- edge from previous tasks to improve convergence speed. On the one hand, the model reuse previ- ously learned features from left columns via lat- eral connections (i.e., adapters). On the other hand, new features can be learned by adding more columns incrementally. Moreover, when the "overlap" between two tasks is small, lateral con- nections can filter out harmful features by sigmoid functions. So in practice, the output of adapters can also be calculated by</p><formula xml:id="formula_3">a (k:j) i = σ(A (k:j) i α j i−1 h j i−1 )<label>(2)</label></formula><p>where A</p><formula xml:id="formula_4">(k:j) i</formula><p>is a matrix to be learned. We treat Equation 2 as one of baseline settings in experi- ments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">PNN with Gated Recurrent Adapter for Chinese SRL</head><p>We reconstruct PNN with bidirectional LSTM to solve SRL problems. Our model is illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>. First, each column in the PNN architecture is a stacked bidirectional LSTM RNN, rather than convolutional neural networks, because inputs are sentences not pixels, and bi-LSTM RNN has proved powerful for Chinese SRL ( <ref type="bibr" target="#b14">Wang et al., 2015)</ref>.</p><p>Second, we enhance the adapter with recurrent structure and gate mechanism, because the sim- ple Multi-Layer Perceptron (MLP) adapters have a limitation: their weights are learned word after word independently. For tasks like transferring re- inforcement learning policies, this is enough be- cause there are little dependencies among actions. But in NLP domain, things are different. There- fore, we add internal memory to adapters to help them remember what has been inherited from het- erogeneous resource.</p><p>Third, to keep gradient stable and balance be- tween long-term and short-term memory, we in- troduce gate mechanism which has been widely used in RNN models. Intuitively, we call the new adapter Gated Recurrent Adapter (GRA). . The output vector is multiplied by a learned matrix W a initialized by random small values before going to GRAs. Its role is to adjust for the different scales of the dif- ferent inputs and reduce the dimensionality. For- mally, the candidate outputs is</p><formula xml:id="formula_5">â t = f (W j a h j t + U j a a j t−1 )<label>(3)</label></formula><p>where a t−1 is the output of the adapter at the pre- vious time-step. U a is a weight matrix to learn. The output of an adapter a j t of layer i at time t can be formalized as follows,</p><formula xml:id="formula_6">g i =σ(W j i h j t + U j i a j t−1 ) (4) g f =σ(W j f h j t + U j f a j t−1 ) (5) g o =σ(W j o h j t + U j o a j t−1 )<label>(6)</label></formula><formula xml:id="formula_7">ã t =g i â t + g f ã j t−1 (7) a t =g o f (ã t−1 )<label>(8)</label></formula><p>where</p><formula xml:id="formula_8">h j ∈ R m j i−1 ×n j i−1 is the outputs of previous layers, W f , W o , W a ∈ R m i−1 ×n i−1 , U f , U o , U a ∈ R m i−1 ×d i−1 are parameters to learn. d i−1</formula><p>is the dimension of the inner memory in adapters. ã t represents the inner state of the adapter. f is an activation function, like tanh. The input gate and the forget gate can be coupled as a uniform gate, that is g i = 1 − g f to alleviate the problem of in- formation redundancy and reduce the possibility of overfitting ( <ref type="bibr" target="#b7">Greff et al., 2015)</ref>.</p><p>Finally, we calculate the output of the next layer i of column k by</p><formula xml:id="formula_9">h k i = f (W k i concat[a (&lt;k) , h k i−1 ])<label>(9)</label></formula><p>2073 where</p><formula xml:id="formula_10">W i ∈ R n (k) i × m (&lt;k)</formula><p>i−1 is the parameters in ith layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training Criteria</head><p>We adopt the sentence tagging approach as <ref type="bibr" target="#b14">Wang et al. (2015)</ref> did, because words in a sentence may closely be related with each other, independently labeling each word is inappropriate. Sentence tag- ging approach only consider valid transition paths of tags when calculating the cost. For example, when using IOBES tagging schema, tag transi- tion from I-Arg0 to B-Arg0 is invalid, and transi- tion from I-Arg0 to I-Arg1 is also invalid because the type of the role changed inside the semantic chunk. For each task (column), the log likelihood of sentence x and its correct path y is</p><formula xml:id="formula_11">log p(y|x, Θ) = log exp N t o t,yt z exp N i t o t,zt<label>(10)</label></formula><p>where N is the number of words, o t ∈ R M is the output of the last layer at time t. y t = k means the tth word has the kth semantic role label. z ranges from all the valid paths of tags. The negative log likelihood of the whole train- ing set D is</p><formula xml:id="formula_12">J(Θ) = (x,y)∈D log p(y|x, Θ)<label>(11)</label></formula><p>We minimize J(Θ) using stochastic gradient descent to learn network parameters Θ. When test- ing, the best prediction of a sentence can be found using Viterbi algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiment Settings</head><p>To compare our approach with others, we designed four experimental setups:</p><p>(1) A simple LSTM setup on CSB and CPB with automatic PoS tagging. Since CPB is about two times as large as the new corpus, we need to know whether CSB can be used for training good semantic parsers and how much information can be learned from CSB by machine. So we conduct this experiment to provide two baselines for CSB and CPB respectively. In this setup we train and evaluate a one-column LSTM model on CSB.</p><p>(2) A simple LSTM setup on CPB with pre- trained word embedding on CSB (marked as bi- LSTM+CSB embedding). Previous work found that using pretrained word embeddings can im- prove performance ( <ref type="bibr" target="#b14">Wang et al., 2015</ref>) on Chi- nese SRL. So we conduct this experiment to com- pare with the method using embeddings trained on large-scale unlabeled data like Gigaword 2 , and NetBank.</p><p>(3) A two-column finetuning setup where we pretrain the first column on CSB and finetune both two columns on CPB. Clearly, finetuning is a traditional method for continual learning scenar- ios. But the disadvantage of it is that learned fea- tures will be gradually forgotten when the model is adapting new tasks. To assess this empirically, we design this experiment. The model uses the same network structure as PNN does, but it does not "freeze" parameters in the first column when tuning two columns.</p><p>(4) A progressive network setup where we train column 1 on CSB, then train column 2 and adapters on CPB. We conduct this experiment to evaluate the proposed model and compare it to all previous methods. To further analyze effective- ness of the new adapter structure, we also conduct an experiment for progressive nets with GRA.</p><p>We apply grid-search technique to explore hyper-parameters including learning rates and width of layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprocessing.</head><p>We follow the same data setting as previous work <ref type="bibr" target="#b17">(Xue, 2008;</ref><ref type="bibr" target="#b13">Sun et al., 2009)</ref>, which divided CPB dataset 3 into three parts: 648 files, from chtb_081.fid to chtb_899.fid, are the training set; 40 files, from chtb_041.fid to chtb_080.fid, are the development set; 72 files, from chtb_001.fid to chtb_040.fid, and chtb_900.fid to chtb_931.fid, are used as the test set.</p><p>We also divide shuffled CSB corpus into three sets with similar partition ratios. Currently, there are 10634 sentences in CSB. So 8900 samples are used as training set, 500 samples as development set and the rest 965 samples as test set. We use Stanford Parser 4 for PoS tagging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>Performance on Chinese SemBank <ref type="table">Table 2</ref> gives the results of Experiment 1. We see that precision on CPB with automatic PoS tagging is  <ref type="table">Table 2</ref>: Results of Chinese SRL tested on CPB and CSB with automatic PoS tagging, using stan- dard LSTM RNN model (Experiment 1). about 0.9 percentage point higher than that on CSB, while recall is about 0.4 percentage point lower, and the gap between F1 scores on CPB and CSB is not significant, which is only about 0.3 percentage point, although the size of CSB is smaller. We can explain this by two reasons. First, CSB does not have predicate-specific roles which may lead to inconsistency, as we explained in Sec- tion 3. Thus, it might be easier to learn by ma- chine. Second, there are underlying similarities between them: both of them annotate predicate- argument structures. So when there is sufficient training data, difference between scores on testing sets is not very likely to be huge. Overall, the results indicated that the new annotated corpus CSB is not a bad choice for training semantic parser even when this does not involve larger training sets.</p><formula xml:id="formula_13">Corpus Pr.(%) Rec.(%) F1(%) 1.</formula><p>Compare to Methods without Using Het- erogeneous Data <ref type="table" target="#tab_4">Table 3</ref> summarizes the SRL performance of previous benchmark methods and our experiments described above. Collobert and Weston only conducted their experiments on English corpus, but we notice that their approach has been implemented and tested on CPB by <ref type="bibr" target="#b14">Wang et al. (2015)</ref>, so we also put their result here for comparison. We can make several observations from these results. Our approach significantly outperforms <ref type="bibr" target="#b11">Sha et al. (2016)</ref> by a large margin (Wilcoxon Signed Rank Test, p &lt; 0.05), even without using GRA. This result can prove the ability of our model to capture underlying similarities between heterogeneous SRL resources.</p><p>Compare to Methods Using Heterogeneous Resources The results of methods using external language resources are also presented in <ref type="table" target="#tab_4">Table 3</ref>. Not surprisingly, we see that the overall best F1 score, 79.67%, is achieved by the progressive nets with the GRAs. Furthermore, as shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, PNN with GRA performs better on longer sentences, which is consistent with our expectation. Without GRA, the F1 drops 0.37% percentage point to 79.30, confirming that gated recurrent adapter structure is more suitable for our task because it can remember what has been transferred in previous time steps.</p><p>Compared to progressive learning methods, finetuning method does not perform well even with the same network structure (Two-column finetuning), but it is still better than simply pre- training word embeddings (bi-LSTM+CSB em- bedding). This confirms the effectiveness of multi- column learning structure which add capacity to the model by adding new columns. Therefore, as can be seen, our PNN model achieves 79.30% F1 score, outperforming finetuning by 0.88% per- centage point, and pretraining embeddings by even larger margin.</p><p>To sum up, not only network struc- tures but also learning methods (finetun- ing/multitask/progressive) can influence the performance of knowledge transfer. According to the results, our PNN approach is more effective than others because it is immune to forgetting and robust to harmful features, and GRA is more suitable for our task than simple adapters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Chinese Semantic Role Labeling</head><p>The concept of Semantic Role Labeling is first proposed by <ref type="bibr" target="#b6">Gildea and Jurafsky(2002)</ref>. Previ- ous work on Chinese SRL mainly focused on how to improve SRL on single corpus. Approaches falls into two categories: feature-based machine learning approaches and neural-network-based ap- proaches.</p><p>Using feature-based method, Sun and Jurafsky (2004) did the preliminary work and achieved promising results without using any large</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F1(%)</head><p>Xue <ref type="formula" target="#formula_3">(2008)</ref>    <ref type="bibr" target="#b18">and Palmer (2003)</ref>, more complete and system- atic research on Chinese SRL were done <ref type="bibr" target="#b19">(Xue and Palmer, 2005;</ref><ref type="bibr" target="#b3">Chen et al., 2006;</ref><ref type="bibr" target="#b5">Ding and Chang, 2009;</ref><ref type="bibr" target="#b20">Yang et al., 2014</ref>). Neural network methods do not rely on hand- crafted features.</p><p>For Chinese SRL, <ref type="bibr" target="#b14">Wang et al. (2015)</ref> proposed bidirectional a LSTM RNN model. And based on their work, Sha (2016) pro- posed quadratic optimization method as a post- processing module and further improved the re- sult.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Learning with Heterogeneous Data</head><p>In this paper, we mainly focus on learning with heterogeneous semantic resource for Chinese SRL. <ref type="bibr" target="#b14">Wang et al. (2015)</ref> introduced heteroge- neous data by using pretrained embeddings at ini- tialization and achieved promising results. <ref type="bibr" target="#b8">Guo et al. (2016)</ref> proposed a multitask learning method with a unified neural network model to learn SRL and relation classification task together and also achieved improvement.</p><p>Different from previous work, we proposed a progressive neural network model with gated re- current adapters to leverage knowledge from het- erogeneous semantic data. Compared with pre- vious methods, this approach is more construc- tive, rather than destructive, because it uses lat- eral connections to access previously learned fea- tures which are fixed when learning new tasks. And by introducing gated recurrent adapters, we further enhance our model to deal with long sen- tences and achieve state-of-the-art performance on Chinese PropBank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>In this paper, we proposed a progressive neural network model with gated recurrent adapters to leverage heterogeneous corpus for Chinese SRL. Unlike previous methods like finetuning, ours leverage prior knowledge via lateral connections. Experiments have shown that our model yields better performance on CPB than all baseline mod- els. Moreover, we proposed novel gated recurrent adapter to handle transfer on long sentences, The experiment has proved the effectiveness of the new adapter structure.</p><p>We believe that progressive learning with het- erogeneous data is a promising avenue to pursue. So in the future, we might try to combine more heterogeneous semantic data for other tasks like event extraction and relation classification, etc.</p><p>We also release the new corpus Chinese Sem- Bank for Chinese SRL. We hope that it will be helpful in providing common benchmarks for fu- ture work on Chinese SRL tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2076</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig.</head><label></label><figDesc>Fig. 2a is an illustration of the basic progressive neural network model. It starts with single column (a neural network), in which there are L hidden layers and the output for ith layer (i ≤ L) with n i units is h 1 i ∈ R n i. Θ 1 denotes the parameters to be learned in the first column. When switching to a second corpus, it "freezes" the parameter Θ 1 and randomly initialize a new column with parameters Θ 2 and several lateral connections between two columns so that layer h 2 i can receive input from both h 2 i−1 and h 1 i−1. In this straightforward manner, progressive neural networks can make use of columns with any structures or to compile lateral connections in an ensemble setting. To be more general, we calculate the output of ith layer in kth column h k i by:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Each column is a stacked bidirectional LSTM RNN model. Two columns are connected by GRAs. There are three gates in each GRA: g i , g f , and g o. The input gate g i and the forget gate g f can also be coupled as one uniform gate, that is g i = 1 − g f .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>be the outputs of i − 1 layers from the first col- umn to the (k − 1)th column. The dimension- ality of them is n (&lt;k) i−1 = [n 1 i−1 , ..., n k−1 i−1 ]. a (&lt;k) is the outputs of k − 1 adapters with dimension m (&lt;k) = [m 1 , ..., m k−1 ]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance of PNN models with and without GRAs over sentence length. For sentences shorter than 40 words, there is no big difference. But for longer sentences (≥40 words), PNN with GRA model performs significantly better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 : Semantic roles in Chinese SemBank</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Result comparison on CPB dataset. Compared to learning with single corpus using bi-LSTM 
model (77.09%), learning with CSB can improve the performance by at list 0.59%. Also the best score 
(79.67%) was achieved by the PNN GRA model. 

annotated corpus. After CPB was built by Xue 
</table></figure>

			<note place="foot" n="2"> https://code.google.com/p/word2vec/ 3 https://catalog.ldc.upenn.edu/LDC2005T23 4 http://nlp.stanford.edu/software/lex-parser.shtml</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This paper is supported by NSFC project 61375074, National Key Basic Research Pro-gram of China 2014CB340504 and Beijing Ad-vanced Innovation Center for Imaging Technology BAICIT-2016016. The contact authors of this pa-per are Baobao Chang and Zhifang Sui.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Shallow semantic trees for smt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilker</forename><surname>Aziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Rios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 6th Workshop on Statistical Machine Translation. Edinburgh</title>
		<meeting>of the 6th Workshop on Statistical Machine Translation. Edinburgh<address><addrLine>Scotland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="316" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Textual inference and meaning representation in human robot interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Bastianelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Castellucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Croce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Basili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Symposium on Semantic Processing. Textual Inference and Structures in Corpora</title>
		<meeting>the Joint Symposium on Semantic Processing. Textual Inference and Structures in Corpora</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="65" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">On the parameter space of generative lexicalized statistical parsing models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bikel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An empirical study of chinese chunking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hitoshi</forename><surname>Isahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the COLING/ACL on Main conference poster sessions</title>
		<meeting>the COLING/ACL on Main conference poster sessions</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Word based chinese semantic role labeling with semantic chunking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Processing Of Languages</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">02n03</biblScope>
			<biblScope unit="page" from="133" to="154" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic labeling of semantic roles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="245" to="288" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh</forename><forename type="middle">Kumar</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Koutník</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Steunebrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.04069</idno>
		<title level="m">Lstm: A search space odyssey</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A unified architecture for semantic role labeling and relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 26th International Conference on Computational Linguistics (COLING)</title>
		<meeting>of the 26th International Conference on Computational Linguistics (COLING)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improving chinese semantic role labeling with english proposition bank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">China National Conference on Chinese Computational Linguistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Razvan Pascanu, and Raia Hadsell</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><forename type="middle">C</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>CoRR abs/1606.04671</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Capturing argument relationships for chinese semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingsong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Shallow semantic parsing of chinese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL 2004</title>
		<meeting>NAACL 2004</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Chinese semantic role labeling with shallow parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 EMNLP. Association for Computational Linguistics</title>
		<meeting>the 2009 EMNLP. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1475" to="1483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Chinese semantic role labeling with bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingsong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>of the 2015 Conference on Empirical Methods in Natural Language essing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1626" to="1631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Modeling the translation of predicate-argument structure for smt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haizhou</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>of the 50th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="902" to="911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Annotating the predicateargument structure of chinese nominalizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth international conference on Language Resources and Evaluation</title>
		<meeting>the fifth international conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1382" to="1387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Labeling chinese predicates with semantic roles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="225" to="255" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Annotating the propositions in the penn chinese treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the second SIGHAN workshop on Chinese language processing</title>
		<meeting>the second SIGHAN workshop on Chinese language processing</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="47" to="54" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic semantic role labeling for chinese verbs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 19th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1160" to="1165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multipredicate semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="363" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The fineness hierarchy of semantic roles and its application in nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuan Yulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chinese Information Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="10" to="20" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Building a chinese treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwen</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chinese Information Processing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
