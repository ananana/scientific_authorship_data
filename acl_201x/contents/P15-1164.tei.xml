<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Text Categorization as a Graph Classification Problem</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Rousseau</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIX</orgName>
								<orgName type="institution">École Polytechnique</orgName>
								<address>
									<country>France Michalis Vazirgiannis</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanouil</forename><surname>Kiagias</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIX</orgName>
								<orgName type="institution">École Polytechnique</orgName>
								<address>
									<country>France Michalis Vazirgiannis</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Text Categorization as a Graph Classification Problem</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1702" to="1712"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we consider the task of text categorization as a graph classification problem. By representing textual documents as graph-of-words instead of historical n-gram bag-of-words, we extract more discriminative features that correspond to long-distance n-grams through frequent subgraph mining. Moreover, by capitalizing on the concept of k-core, we reduce the graph representation to its densest part-its main core-speeding up the feature extraction step for little to no cost in prediction performances. Experiments on four standard text classification datasets show statistically significant higher accuracy and macro-averaged F1-score compared to baseline approaches.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The task of text categorization finds applications in a wide variety of domains, from news filter- ing and document organization to opinion mining and spam detection. With the ever-growing quan- tity of information available online nowadays, it is crucial to provide effective systems capable of classifying text in a timely fashion. Compared to other application domains of classification, its specificity lies in its high number of features, its sparse feature vectors and its skewed multiclass scenario. For instance, when dealing with thou- sands of news articles, it is not uncommon to have millions of n-gram features, only a few hundreds actually present in each document and tens of class labels -some of them with thousands of articles and some others will only a few hundreds. These particularities have to be taken into account when envisaging a different representation for a docu- ment and in our case when considering the task as a graph classification problem.</p><p>Graphs are powerful data structures that are used to represent complex information about en- tities and interaction between them and we think text makes no exception. Historically, following the traditional bag-of-words representation, uni- grams have been considered as the natural features and later extended to n-grams to capture some word dependency and word order. However, n- grams correspond to sequences of words and thus fail to capture word inversion and subset match- ing (e. g., "article about news" vs. "news article"). We believe graphs can help solve these issues like they did for instance with chemical compounds where repeating substructure patterns are good in- dicators of belonging to one particular class, e. g., predicting carcinogenicity in molecules ( <ref type="bibr" target="#b18">Helma et al., 2001</ref>). Graph classification has received a lot of attention this past decade and various tech- niques have been developed to deal with the task but rarely applied on textual data and at its scale.</p><p>In our work, we explored a graph representation of text, namely graph-of-words, to challenge the traditional bag-of-words representation and help better classify textual documents into categories. We first trained a classifier using frequent sub- graphs as features for increased effectiveness. We then reduced each graph-of-words to its main core before mining the features for increased efficiency. Finally, we also used this technique to reduce the total number of n-gram features considered in the baselines for little to no loss in prediction perfor- mances.</p><p>The rest of the paper is organized as follows. Section 2 provides a review of the related work. Section 3 defines the preliminary concepts upon which our work is built. Section 4 introduces the proposed approaches. Section 5 describes the ex- perimental settings and presents the results we ob- tained on four standard datasets. Finally, Section 6 concludes our paper and mentions future work directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1702</head><p>In this section, we present the related work in text categorization, graph classification and the com- bination of the two fields like in our case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Text categorization</head><p>Text categorization, a.k.a. text classification, cor- responds to the task of automatically predicting the class label of a given textual document. We refer to <ref type="bibr" target="#b41">(Sebastiani, 2002</ref>) for an in-depth re- view of the earliest works in the field and <ref type="bibr" target="#b0">(Aggarwal and Zhai, 2012</ref>) for a survey of the more recent works that capitalize on additional meta- information. We note in particular the seminal work of Joachims (1998) who was the first to pro- pose the use of a linear SVM with TF×IDF term features for the task. This approach is one of the standard baselines because of its simplicity yet ef- fectiveness (unsupervised n-gram feature mining followed by standard supervised learning). An- other popular approach is the use of Naive Bayes and its multiple variants <ref type="bibr" target="#b32">(McCallum and Nigam, 1998)</ref>, in particular for the subtask of spam de- tection ( <ref type="bibr" target="#b1">Androutsopoulos et al., 2000</ref>). Finally, there are a couple of works such as ( <ref type="bibr" target="#b17">Hassan et al., 2007</ref>) that used the graph-of-words representa- tion to propose alternative weights for the n-gram features but still without considering the task as a graph classification problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph classification</head><p>Graph classification corresponds to the task of au- tomatically predicting the class label of a given graph. The learning part in itself does not differ from other supervised learning problems and most proposed methods deal with the feature extrac- tion part. They fall into two main categories: ap- proaches that consider subgraphs as features and graph kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Subgraphs as features</head><p>The main idea is to mine frequent subgraphs and use them as features for classification, be it with Adaboost ( ) or a linear SVM ( <ref type="bibr" target="#b12">Deshpande et al., 2005</ref>). Indeed, most datasets that were used in the associated experiments cor- respond to chemical compounds where repeating substructure patterns are good indicators of be- longing to one particular class. Some popular graph pattern mining algorithms are gSpan <ref type="bibr" target="#b47">(Yan and Han, 2002</ref>), FFSM ( <ref type="bibr" target="#b19">Huan et al., 2003)</ref> and <ref type="bibr">Gaston (Nijssen and Kok, 2004</ref>). The number of frequent subgraphs can be enormous, especially for large graph collections, and handling such a feature set can be very expensive. To overcome this issue, recent works have proposed to retain or even only mine the discriminative subgraphs, i. e. features that contribute to the classification decision, in particular gBoost ( <ref type="bibr" target="#b40">Saigo et al., 2009</ref>), CORK ( <ref type="bibr" target="#b44">Thoma et al., 2009</ref>) and GAIA ( <ref type="bibr" target="#b21">Jin et al., 2010)</ref>. However, when experimenting, gBoost did not converge on our larger datasets while GAIA and CORK consider subgraphs of node size at least 2, which exclude unigrams, result- ing in poorer performances. Moreover, all these approaches have been developed for binary clas- sification, which meant mining features as many times as the number of classes instead of just once (one-vs-all learning strategy). In this paper, we tackle the scalability issue differently through an unsupervised feature selection approach to reduce the size of the graphs and a fortiori the number of frequent subgraphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Graph kernels</head><p>Gärtner et al. <ref type="formula">(2003)</ref> proposed the first kernels be- tween graphs (as opposed to previous kernels on graphs, i. e. between nodes) based on either ran- dom walks or cycles to tackle the problem of clas- sification between graphs. In parallel, the idea of marginalized kernels was extended to graphs by <ref type="bibr" target="#b24">Kashima et al. (2003)</ref> and by <ref type="bibr" target="#b28">Mahé et al. (2004)</ref>. We refer to ( <ref type="bibr" target="#b46">Vishwanathan et al., 2010</ref>) for an in- depth review of the topic and in particular its lim- itations in terms of number of unique node labels, which make them unsuitable for our problem as tested in practice (limited to a few tens of unique labels compared to hundreds of thousands for us).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Similar works</head><p>The work of <ref type="bibr" target="#b30">Markov et al. (2007)</ref> is perhaps the closest to ours since they also perform subgraph feature mining on graph-of-words representations but with non-standard datasets and baselines. The works of <ref type="bibr" target="#b20">Jiang et al. (2010)</ref> and <ref type="bibr" target="#b2">Arora et al. (2010)</ref> are also related but their representations are dif- ferent and closer to parse and dependency trees used as base features for text categorization by  and <ref type="bibr" target="#b31">Matsumoto et al. (2005)</ref>. Moreover, they do not discuss the choice of the support value, which controls the total num- ber of features and can potentially lead to millions of subgraphs on standard datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminary concepts</head><p>In this section, we introduce the preliminary con- cepts upon which our work is built.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Graph-of-words</head><p>We model a textual document as a graph-of-words, which corresponds to a graph whose vertices rep- resent unique terms of the document and whose edges represent co-occurrences between the terms within a fixed-size sliding window. The under- lying assumption is that all the words present in a document have some undirected relationships with the others, modulo a window size outside of which the relationship is not considered. This rep- resentation was first used in keyword extraction and summarization <ref type="bibr" target="#b35">(Ohsawa et al., 1998;</ref><ref type="bibr" target="#b33">Mihalcea and Tarau, 2004</ref>) and more recently in ad hoc IR ( <ref type="bibr" target="#b6">Blanco and Lioma, 2012;</ref><ref type="bibr" target="#b38">Rousseau and Vazirgiannis, 2013)</ref>. We refer to <ref type="bibr" target="#b6">(Blanco and Lioma, 2012</ref>) for an in-depth review of the graph repre- sentations of text in NLP. As a discipline, computer science spans a range of topics from theoretical studies of algorithms and the limits of computation to the practical issues of implementing computing systems in hardware and software. hide me <ref type="figure">Figure 1</ref>: Graph-of-words representation of a tex- tual document -in bold font, its main core. <ref type="figure">Figure 1</ref> illustrates the graph-of-words repre- sentation of a textual document. The vertices cor- respond to the remaining terms after standard pre- processing steps have been applied (tokenization, stop word removal and stemming). The undirected edges were drawn between terms co-occurring within a sliding window over the processed text of size 4, value consistently reported as working well in the references aforementioned and validated in our experiments. Edge direction was used by Fil- ippova (2010) so as to extract valid sentences but not here in order to capture some word inversion.</p><p>Note that for small-enough window sizes (which is typically the case in practice), we can consider that two terms linked represent a long- distance bigram ( <ref type="bibr" target="#b3">Bassiou and Kotropoulos, 2010)</ref>, if not a bigram. Furthermore, by extending the denomination, we can consider that a subgraph of size n is a long-distance n-gram, if not an n- gram. Indeed, the nodes belonging to a subgraph do not necessarily appear in a sequence in the doc- ument like for a n-gram. Moreover, this enables us to "merge" together n-grams that share the same terms but maybe not in the same order. In the ex- periments, by abusing the terminology, we will re- fer to them as n-grams to adopt a common termi- nology with the baseline approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Node/edge labels and subgraph matching</head><p>In graph classification, it is common to introduce a node labeling function µ to map a node id to its label. For instance, consider the case of chemi- cal compounds (e. g., the benzene C 6 H 6 ). Then in its graph representation (its "structural formula"), it is crucial to differentiate between the multiple nodes labeled the same (e. g., C or H). In the case of graph-of-words, node labels are unique inside a graph since they represent unique terms of the document and we can therefore omit these func- tions since they are injective in our case and we can substitute node ids for node labels. In partic- ular, the general problem of subgraph matching, which defines an isomorphism between a graph and a subgraph and is NP-complete ( <ref type="bibr" target="#b15">Garey and Johnson, 1990)</ref>, can be reduced to a polynomial problem when node labels are unique. In our ex- periments, we used the standard algorithm VF2 developed by <ref type="bibr" target="#b9">Cordella et al. (2001)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">K-core and main core</head><p>Seidman (1983) defined the k-core of a graph as the maximal connected subgraph whose vertices are at least of degree k within the subgraph. The non-empty k-core of largest k is called the main core and corresponds to the most cohesive set(s) of vertices. The corresponding value of k may dif- fer from one graph to another. <ref type="bibr">Batagelj and Zaveršnik (2003)</ref> proposed an algorithm to extract the main core of an unweighted graph in time lin- ear in the number of edges, complexity similar in our case to the other NLP preprocessing steps. Bold font on <ref type="figure">Figure 1</ref> indicates that a vertex be- longs to the main core of the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Graph-of-words classification</head><p>In this section, we present our work and the sev- eral approaches we explored, from unsupervised feature mining using gSpan to propose more dis- criminative features than standard n-grams to un- supervised feature selection using k-core to reduce the total number of subgraph and n-gram features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Unsupervised feature mining using gSpan</head><p>We considered the task of text categorization as a graph classification problem by representing tex- tual documents as graph-of-words and then ex- tracting subgraph features to train a graph classi- fier. Each document is a separate graph-of-words and the collection of documents thus corresponds to a set of graphs. Therefore, for larger datasets, the total number of graphs increases but not the average graph size (the average number of unique terms in a text), assuming homogeneous datasets.</p><p>Because the total number of unique node la- bels corresponds to the number of unique terms in the collection in our case, graph kernels are not suitable for us as verified in practice using the MATLAB code made available by <ref type="bibr">Shervashidze (2009)</ref>. We therefore only explored the meth- ods that consider subgraphs as features. Repeat- ing substructure patterns between graphs are intu- itively good candidates for classification since, at least for chemical compounds, shared subparts of molecules are good indicators of belonging to one particular class. We assumed it would the same for text. Indeed, subgraphs of graph-of-words corre- spond to sets of words co-occurring together, just not necessarily always as the same sequence like for n-grams -it can be seen as a relaxed definition of a n-gram to capture additional variants.</p><p>We used gSpan (graph-based Substructure pattern ( <ref type="bibr" target="#b47">Yan and Han, 2002</ref>)) as frequent sub- graph miner like ( <ref type="bibr" target="#b20">Jiang et al., 2010;</ref><ref type="bibr" target="#b2">Arora et al., 2010</ref>) mostly because of its fast available C++ implementation from gBoost ( <ref type="bibr" target="#b40">Saigo et al., 2009</ref>). Briefly, the key idea behind gSpan is that in- stead of enumerating all the subgraphs and test- ing for isomorphism throughout the collection, it first builds for each graph a lexicographic order of all the edges using depth-first-search (DFS) traversal and assigns to it a unique minimum DFS code. Based on all these DFS codes, a hierarchical search tree is constructed at the collection-level. By pre-order traversal of this tree, gSpan discov- ers all frequent subgraphs with required support.</p><p>Consider the set of all subgraphs in the collec- tion of graphs, which corresponds to the set of all potential features. Note that there may be overlap- ping (subgraphs sharing nodes/edges) and redun- dant (subgraphs included in others) features. Be- cause its size is exponential in the number of edges (just like the number of n-grams is exponential in n), it is common to only retain/mine the most fre- quent subgraphs (again just like for n-grams with a minimum document frequency <ref type="bibr" target="#b14">(Fürnkranz, 1998;</ref><ref type="bibr" target="#b22">Joachims, 1998)</ref>). This is controlled via a param- eter known as the support, which sets the mini- mum number of graphs in which a given subgraph has to appear to be considered as a feature, i. e. the number of subgraph matches in the collection. Here, since node labels are unique inside a graph, we do not have to consider multiple occurrences of the same subgraph in a given graph. The lower the support, the more features selected/considered but the more expensive the mining and the training (not only in time spent for the learning but also for the feature vector generation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Unsupervised support selection</head><p>The optimal value for the support can be learned through cross-validation so as to maximize the prediction accuracy of the subsequent classifier, making the whole feature mining process super- vised. But if we consider that the classifier can only improve its goodness of fit with more fea- tures (the sets of features being nested as the sup- port varies), it is likely that the lowest support will lead to the best test accuracy; assuming subse- quent regularization to prevent overfitting. How- ever, this will come at the cost of an exponential number of features as observed in practice. In- deed, as the support decreases, the number of fea- tures increases slightly up until a point where it increases exponentially, which makes both the fea- ture vector generation and the learning expensive, especially with multiple classes. Moreover, we observed that the prediction performances did not benefit that much from using all the possible fea- tures (support of 1) as opposed to a more manage- able number of features corresponding to a higher support. Therefore, we propose to select the sup- port using the so-called elbow method. This is an unsupervised empirical method initially developed for selecting the number of clusters in k-means <ref type="bibr" target="#b45">(Thorndike, 1953)</ref>. <ref type="figure" target="#fig_3">Figure 3</ref> (upper plots) in Sec- tion 5 illustrates this process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Considered classifiers</head><p>In text categorization, standard baseline classifiers include k-nearest neighbors (kNN) <ref type="bibr" target="#b27">(Larkey and Croft, 1996)</ref>, Naive Bayes (NB) <ref type="bibr" target="#b32">(McCallum and Nigam, 1998</ref>) and linear Support Vector Machines (SVM) <ref type="bibr" target="#b22">(Joachims, 1998</ref>) with the latter perform- ing the best on n-gram features as verified in our experiments. Since our subgraph features corre- spond to "long-distance n-grams", we used linear SVMs as our classifiers in all our experiments - the goal of our work being to explore and propose better features rather than a different classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Multiclass scenario</head><p>In standard binary graph classification (e. g., pre- dicting chemical compounds' carcinogenicity as either positive or negative ( <ref type="bibr" target="#b18">Helma et al., 2001)</ref>), feature mining is performed on the whole graph collection as we expect the mined features to be able to discriminate between the two classes (thus producing a good classifier). However, for the task of text categorization, there are usually more than two classes (e. g., 118 categories of news ar- ticles for the Reuters-21578 dataset) and with a skewed class distribution (e. g., a lot more news related to "acquisition" than to "grain"). There- fore, a single support value might lead to some classes generating a tremendous number of fea- tures (e. g., hundreds of thousands of frequent sub- graphs) and some others only a few (e. g., a few hundreds subgraphs) resulting in a skewed and non-discriminative feature set. To include dis- criminative features for these minority classes, we would need an extremely low support resulting in an exponential number of features because of the majority classes. For these reasons, we de- cided to mine frequent subgraphs per class using the same relative support (%) and then aggregat- ing each feature set into a global one at the cost of a supervised process (but which still avoids cross- validated parameter tuning). This was not needed for the tasks of spam detection and opinion mining since the corresponding datasets consist of only two balanced classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Main core mining using gSpan</head><p>Since the main drawback of mining frequent sub- graphs for text categorization rather than chemical compound classification is the very high number of possible subgraphs because of the size of the graphs and the total number of graphs (more than 10x in both cases), we thought of ways to reduce the graphs' sizes while retaining as much classifi- cation information as possible.</p><p>The graph-of-words representation is designed to capture dependency between words, i. e. de- pendency between features in the context of ma- chine learning but at the document-level. Ini- tially, we wanted to capture recurring sets of words (i. e. take into account word inversion and sub- set matching) and not just sequences of words like with n-grams. In terms of subgraphs, this means words that co-occur with each other and form a dense subgraph as opposed to a path like for a n- gram. Therefore, when reducing the graphs, we need to keep their densest part(s) and that is why we considered extracting their main cores. Com- pared to other density-based algorithms, retaining the main core of a graph has the advantage of be- ing linear in the number of edges, i. e. in the num- ber of unique terms in a document in our case (the number of edges is at most the number of nodes times the fixed size of the sliding window, a small constant in practice).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Unsupervised n-gram feature selection</head><p>Similarly to <ref type="bibr" target="#b17">(Hassan et al., 2007</ref>) that used graph- of-words to propose alternative weights for the n- gram features, we can capitalize on main core re- tention to still extract binary n-gram features for classification but considering only the terms be- longing to the main core of each document. Be- cause some terms never belong to any main core of any document, the dimension of the overall fea- ture space decreases. Additionally, since a docu- ment is only represented by a subset of its original terms, the number of non-zero feature values per document also decreases, which matters for SVM, even for the linear kernel, when considering the dual formulation or in the primal with more recent optimization techniques <ref type="bibr" target="#b23">(Joachims, 2006</ref>).</p><p>Compared to most existing feature selection techniques in the field <ref type="bibr" target="#b49">(Yang and Pedersen, 1997)</ref>, it is unsupervised and corpus-independent as it does not rely on any labeled data like IG, MI or χ 2 nor any collection-wide statistics like IDF, which can be of interest for large-scale text cate- gorization in order to process documents in paral- lel, independently of each other. In some sense, it is similar to what <ref type="bibr">Özgür et al. (2005)</ref> proposed with corpus-based and class-based keyword selec- tion for text classification except that we use here document-based keyword selection following the approach from <ref type="bibr" target="#b39">Rousseau and Vazirgiannis (2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section we present the experiments we con- ducted to validate our approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We used four standard text datasets: two for multi- class document categorization (WebKB and R8), one for spam detection (LingSpam) and one for opinion mining (Amazon) so as to cover all the main subtasks of text categorization:</p><p>• WebKB: 4 most frequent categories among labeled webpages from various CS depart- ments -split into 2,803 for training and 1,396 for test <ref type="bibr">(Cardoso-Cachopo, 2007, p. 39-41</ref>).</p><p>• R8: 8 most frequent categories of Reuters- 21578, a set of labeled news articles from the 1987 Reuters newswire -split into 5,485 for training and 2,189 for test <ref type="bibr" target="#b11">(Debole and Sebastiani, 2005</ref>).</p><p>• LingSpam: 2,893 emails classified as spam or legitimate messages -split into 10 sets for 10-fold cross validation ( <ref type="bibr" target="#b1">Androutsopoulos et al., 2000</ref>).</p><p>• Amazon: 8,000 product reviews over four different sub-collections (books, DVDs, elec- tronics and kitchen appliances) classified as positive or negative -split into 1,600 for training and 400 for test each <ref type="bibr" target="#b7">(Blitzer et al., 2007</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Implementation</head><p>We developed our approaches mostly in Python using the igraph library ( <ref type="bibr" target="#b10">Csardi and Nepusz, 2006</ref>) for the graph representation and main core extraction. For unsupervised subgraph feature mining, we used the C++ implementation of gSpan from gBoost ( <ref type="bibr" target="#b40">Saigo et al., 2009</ref>). Finally for classification and standard n-gram text catego- rization we used scikit (Pedregosa et al., 2011), a standard Python machine learning library.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation metrics</head><p>To evaluate the performance of our proposed ap- proaches over standard baselines, we computed on the test set both the micro-and macro-average F1- score. Because we are dealing with single-label classification, the micro-average F1-score corre- sponds to the accuracy and is a measure of the overall prediction effectiveness (Manning et al.,  <ref type="table">Table 1</ref>: Total number of features (n-grams or sub- graphs) vs. number of features present only in main cores along with the reduction of the dimen- sion of the feature space on all four datasets.</p><p>2008, p. 281). Conversely, the macro-average F1- score takes into account the skewed class label dis- tributions by weighting each class uniformly. The statistical significance of improvement in accuracy over the n-gram SVM baseline was assessed us- ing the micro sign test (p &lt; 0.05) <ref type="bibr" target="#b48">(Yang and Liu, 1999</ref>). For the Amazon dataset, we report the av- erage of each metric over the four sub-collections. <ref type="table">Table 2</ref> shows the results on the four considered datasets. The first three rows correspond to the baselines: unsupervised n-gram feature extrac- tion and then supervised learning using kNN, NB (Multinomial but Bernoulli yields similar results) and linear SVM. The last three rows correspond to our approaches. In our first approach, denoted as "gSpan + SVM", we mine frequent subgraphs (gSpan) as features and then train a linear SVM. These fea- tures correspond to long-distance n-grams. This leads to the best results in text categorization on almost all datasets (all if we compare to baseline methods), in particular on multiclass document categorization (R8 and WebKB).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results</head><p>In our second approach, denoted as "MC + gSpan + SVM", we repeat the same procedure except that we mine frequent subgraphs (gSpan) from the main core (MC) of each graph-of-words and then train an SVM on the resulting features. Main cores can vary from 1-core to 12-core de- pending on the graph structure, 5-core and 6-core being the most frequent (more than 60%). This yields results similar to the SVM baseline for a faster mining and training compared to gSpan + SVM. <ref type="table">Table 1 (upper table)</ref> shows the reduction in the dimension of the feature space and we see <ref type="table">Table 2</ref>: Test accuracy and macro-average F1-score on four standard datasets. Bold font marks the best performance in a column. * indicates statistical significance at p &lt; 0.05 using micro sign test with regards to the SVM baseline of the same column. MC corresponds to unsupervised feature selection using the main core of each graph-of-words to extract n-gram and subgraph features. gSpan mining support values are 1.6% (WebKB), 7% (R8), 4% (LingSpam) and 0.5% (Amazon).   that on average less than 60% of the subgraphs are kept for little to no cost in prediction effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>In our final approach, denoted as "MC + SVM", we performed unsupervised feature selection by keeping the terms appearing in the main core (MC) of each document's graph-of-words representation and then extracted standard n-gram features. <ref type="table">Ta- ble 1 (lower table)</ref> shows the reduction in the di- mension of the feature space and we see that on av- erage less than half the n-grams remain. <ref type="figure" target="#fig_2">Figure 2</ref> shows the distribution of non-zero features before and after the feature selection on the R8 dataset. Similar changes in distribution can be observed on the other datasets, from a right-tail Gaussian to a power law distribution as expected from the main core retention. <ref type="table">Table 2</ref> shows that the main core retention has little to no cost in accuracy and F1- score but can reduce drastically the feature space and the number of non-zero values per document. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Unsupervised support selection</head><p>Figure 3 above illustrates the unsupervised heuris- tic (elbow method) we used to select the support value, which corresponds to the minimum number of graphs in which a subgraph has to appear to be considered frequent. We noticed that as the sup- port decreases, the number of features increases slightly up until a point where it increases expo- nentially. This support value, highlighted in black on the figure and chosen before taking into ac- count the class label, is the value we used in our experiments and for which we report the results in <ref type="table">Table 1</ref> and 2. The lower plots provide evidence that the elbow method helps selecting in an unsu- pervised manner a support that leads to the best or close to the best accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Distribution of mined n-grams</head><p>In order to gain more insights on why the long- distance n-grams mined with gSpan result in bet- ter classification performances than the baseline n- grams, we computed the distribution of the num- ber of unigrams, bigrams, etc. up to 6-grams in the traditional feature set and ours <ref type="figure" target="#fig_4">(Figure 4</ref>) as well as in the top 5% features that contribute the most to the classification decision of the trained SVM ( <ref type="figure">Figure 5</ref>). Again, a long-distance n-gram corre- sponds to a subgraph of size n in a graph-of-words and can be seen as a relaxed definition of the tra- ditional n-gram, one that takes into account word inversion for instance. To obtain comparable re- sults, we considered for the baseline n-grams with a minimum document frequency equal to the sup- port. Otherwise, by definition, there are at least as many bigrams as there are unigrams and so forth. <ref type="figure" target="#fig_4">Figure 4</ref> shows that our approaches mine way more n-grams than unigrams compared to the baseline. This happens because with graph-of- words a subgraph of size n corresponds to a set of n terms while with bag-of-words a n-gram cor- responds to a sequence of n terms. Note that even when restricting the subgraphs to the main cores, there are still more higher order n-grams mined. <ref type="figure">Figure 5</ref> shows that the higher order n-grams still contribute indeed to the classification deci- sion and in higher proportion than with the base- line, even when restricting to the main cores. For baseline SVM gSpan + SVM MC + gSpan + SVM <ref type="figure">Figure 5</ref>: Distribution of n-grams (standard and long-distance ones) among the top 5% most dis- criminative features for SVM on WebKB dataset. instance, on the R8 dataset, {bank, base, rate} was a discriminative (top 5% SVM features) long- distance 3-gram for the category "interest" and occurred in documents in the form of "barclays bank cut its base lending rate", "midland bank matches its base rate" and "base rate of natwest bank dropped", pattern that would be hard to cap- ture with traditional n-gram bag-of-words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Timing</head><p>With an Intel Core i5-3317U clocking at 2.6GHz and 8GB of RAM, mining the subgraph features with gSpan takes on average 30s for the selected support. It can take several hours with lower sup- port and goes down to 5s using the main cores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we tackled the task of text cate- gorization by representing documents as graph- of-words and then considering the problem as a graph classification one. We were able to extract more discriminative features that correspond to long-distance n-grams through frequent subgraph mining. Experiments on four standard datasets show statistically significant higher accuracy and macro-averaged F1-score compared to baselines.</p><p>To the best of our knowledge, graph classifi- cation has never been tested at that scale -thou- sands of graphs and tens of thousands of unique node labels -and also in the multiclass scenario. For these reasons, we could not capitalize on all standard methods. In particular, we believe new kernels that support a very high number of unique node labels could yield even better performances.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Distribution of non-zero n-gram feature values before and after unsupervised feature selection (main core retention) on R8 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Number of subgraph features/accuracy in test per support (%) on WebKB (left) and R8 (right) datasets: in black, the selected support value chosen via the elbow method and in red, the accuracy in test for the SVM baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Distribution of n-grams (standard and long-distance ones) among all the features on WebKB dataset.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A Survey of Text Classification Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mining Text Data</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="163" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An Evaluation of Naive Bayesian Anti-Spam Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Koutsias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><forename type="middle">V</forename><surname>Chandrinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Paliouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Constantine</forename><forename type="middle">D</forename><surname>Spyropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Machine Learning in the New Information Age, 11th European Conference on Machine Learning</title>
		<meeting>the Workshop on Machine Learning in the New Information Age, 11th European Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="9" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sentiment Classification Using Automatically Extracted Subgraph Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilpa</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elijah</forename><surname>Mayfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolyn</forename><surname>Penstein-Rosé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, CAAGET &apos;10</title>
		<meeting>the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, CAAGET &apos;10</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="131" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Word Clustering Using PLSA Enhanced with Long Distance Bigrams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikoletta</forename><surname>Bassiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Constantine</forename><surname>Kotropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Pattern Recognition, ICPR &apos;10</title>
		<meeting>the 20th International Conference on Pattern Recognition, ICPR &apos;10</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="4226" to="4229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Batagelj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matjaž</forename><surname>Zaversnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">An O(m) Algorithm for Cores Decomposition of Networks. The Computing Research Repository (CoRR)</title>
		<idno>cs.DS/0310049</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Graph-based term weighting for information retrieval. Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Blanco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christina</forename><surname>Lioma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="54" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Biographies, bollywood, boomboxes and blenders: Domain adaptation for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, ACL &apos;07</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics, ACL &apos;07</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="440" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Improving Methods for Single-label Text Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><surname>Cardoso-Cachopo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<pubPlace>Lisbon, Portugal</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Universidade de Lisboa</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis, Instituto Superior Técnico</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An improved algorithm for matching large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><forename type="middle">Pietro</forename><surname>Cordella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Foggia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Sansone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Vento</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd IAPR-TC15 Workshop on Graph-based Representations in Pattern Recognition</title>
		<meeting>the 3rd IAPR-TC15 Workshop on Graph-based Representations in Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="149" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The igraph software package for complex network research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Csardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamas</forename><surname>Nepusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Complex Systems</title>
		<imprint>
			<biblScope unit="volume">1695</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>InterJournal</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An Analysis of the Relative Hardness of Reuters-21578 Subsets: Research Articles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franca</forename><surname>Debole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="584" to="596" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Frequent Substructure-Based Approaches for Classifying Chemical Compounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mukund</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michihiro</forename><surname>Kuramochi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikil</forename><surname>Wale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1036" to="1050" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-sentence Compression: Finding Shortest Paths in Word Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Filippova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics, COLING &apos;10</title>
		<meeting>the 23rd International Conference on Computational Linguistics, COLING &apos;10</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="322" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A study using n-gram features for text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Fürnkranz</surname></persName>
		</author>
		<idno>OEFAI-TR-98-30</idno>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
		<respStmt>
			<orgName>Austrian Research Institute for Artificial Intelligence</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Computers and Intractability; A Guide to the Theory of NP-Completeness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">S</forename><surname>Garey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>W. H. Freeman &amp; Co</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On graph kernels: Hardness results and efficient alternatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Gärtner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Flach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Wrobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference on Computational Learning Theory, COLT &apos;03</title>
		<meeting>the Annual Conference on Computational Learning Theory, COLT &apos;03</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="129" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Random-Walk Term Weighting for Improved Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samer</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Semantic Computing, ICSC &apos;07</title>
		<meeting>the International Conference on Semantic Computing, ICSC &apos;07</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="242" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The predictive toxicology challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Helma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">D</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashwin</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="107" to="108" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient Mining of Frequent Subgraphs in the Presence of Isomorphism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Huan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd IEEE International Conference on Data Mining, ICDM &apos;03</title>
		<meeting>the 3rd IEEE International Conference on Data Mining, ICDM &apos;03</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="549" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Text classification using graph mining-based feature extraction. Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuntao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frans</forename><surname>Coenen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Zito</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="302" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">GAIA: graph classification using evolutionary computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Calvin</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 ACM SIGMOD international conference on Management of data, SIGMOD &apos;10</title>
		<meeting>the 2010 ACM SIGMOD international conference on Management of data, SIGMOD &apos;10</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="879" to="890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Text categorization with Support Vector Machines: Learning with many relevant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><forename type="middle">Joachims</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th European Conference on Machine Learning, ECML &apos;98</title>
		<meeting>the 10th European Conference on Machine Learning, ECML &apos;98</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="137" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Training Linear SVMs in Linear Time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><forename type="middle">Joachims</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge Discovery and Data mining, KDD &apos;06</title>
		<meeting>the 12th ACM SIGKDD international conference on Knowledge Discovery and Data mining, KDD &apos;06</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="217" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Marginalized kernels between labeled graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisashi</forename><surname>Kashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koji</forename><surname>Tsuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiro</forename><surname>Inokuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Machine Learning</title>
		<meeting>the 20th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
	<note>of ICML &apos;03</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A Boosting Algorithm for Classification of Semi-Structured Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 9th Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="301" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An application of boosting to graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eisaku</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 17, NIPS &apos;04</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="729" to="736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Combining Classifiers in Text Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leah</forename><forename type="middle">S</forename><surname>Larkey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR &apos;96</title>
		<meeting>the 19th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR &apos;96</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="289" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Extensions of marginalized graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Mahé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nobuhisa</forename><surname>Ueda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Akutsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeanluc</forename><surname>Perret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Philippe</forename><surname>Vert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Machine Learning, ICML &apos;04</title>
		<meeting>the 21st International Conference on Machine Learning, ICML &apos;04</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="70" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Introduction to Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prabhakar</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fast Categorization of Web Documents Represented by Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Markov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Last</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><surname>Kandel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Web Mining and Web Usage Analysis, number 4811 in Lecture Notes in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="56" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sentiment Classification Using Word Sub-sequences and Dependency Sub-trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shotaro</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroya</forename><surname>Takamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining, PAKDD &apos;05</title>
		<meeting>the 9th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining, PAKDD &apos;05</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="301" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A comparison of event models for Naive Bayes text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamal</forename><surname>Nigam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI workshop on learning for text categorization, AAAI &apos;98</title>
		<meeting>the AAAI workshop on learning for text categorization, AAAI &apos;98</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">TextRank: Bringing Order into Texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tarau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;04</title>
		<meeting>the 9th Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;04</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="404" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A Quickstart in Frequent Structure Mining Can Make a Difference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siegfried</forename><surname>Nijssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><forename type="middle">N</forename><surname>Kok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM SIGKDD international conference on Knowledge Discovery and Data mining, KDD &apos;04</title>
		<meeting>the 10th ACM SIGKDD international conference on Knowledge Discovery and Data mining, KDD &apos;04</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="647" to="652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">KeyGraph: Automatic Indexing by Co-occurrence Graph Based on Building Construction Metaphor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukio</forename><surname>Ohsawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nels</forename><forename type="middle">E</forename><surname>Benson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masahiko</forename><surname>Yachida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Digital Libraries Conference, ADL &apos;98</title>
		<meeting>the Advances in Digital Libraries Conference, ADL &apos;98</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="12" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Text Categorization with Class-based and Corpus-based Keyword Selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arzucan</forename><surname>Özgür</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Özgür</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tunga</forename><surname>Güngör</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Computer and Information Sciences, ISCIS &apos;05</title>
		<meeting>the 20th International Conference on Computer and Information Sciences, ISCIS &apos;05</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="606" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaël</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Graph-of-word and TW-IDF: New Approach to Ad Hoc IR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Rousseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michalis</forename><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Information and knowledge management, CIKM &apos;13</title>
		<meeting>the 22nd ACM international conference on Information and knowledge management, CIKM &apos;13</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="59" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Main Core Retention on Graph-of-words for SingleDocument Keyword Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Rousseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michalis</forename><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th European Conference on Information Retrieval, ECIR &apos;15</title>
		<meeting>the 37th European Conference on Information Retrieval, ECIR &apos;15</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="382" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">gBoost: a mathematical programming approach to graph classification and regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroto</forename><surname>Saigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadashi</forename><surname>Kadowaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koji</forename><surname>Tsuda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="69" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning in Automated Text Categorization. ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Network structure and minimum degree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seidman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="269" to="287" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<ptr target="http://www.di.ens.fr/~shervashidze/code.html" />
		<title level="m">Visited on 30/05/2015. Graph kernels</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Near-optimal Supervised Feature Selection among Frequent Subgraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marisa</forename><surname>Thoma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIAM International Conference on Data Mining, SDM &apos;09</title>
		<meeting>the SIAM International Conference on Data Mining, SDM &apos;09</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1076" to="1087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Who belongs in the family?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Thorndike</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="267" to="276" />
			<date type="published" when="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicol</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risi</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1201" to="1242" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">gspan: Graphbased substructure pattern mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd IEEE International Conference on Data Mining, ICDM &apos;02</title>
		<meeting>the 2nd IEEE International Conference on Data Mining, ICDM &apos;02</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="721" to="724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A Re-examination of Text Categorization Methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR &apos;99</title>
		<meeting>the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR &apos;99</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="42" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A Comparative Study on Feature Selection in Text Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">O</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Conference on Machine Learning, ICML &apos;97</title>
		<meeting>the 14th International Conference on Machine Learning, ICML &apos;97</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="412" to="420" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
