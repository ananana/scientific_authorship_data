<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:19+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rumor Detection on Twitter with Tree-structured Recursive Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong SAR</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Gao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Victoria University of Wellington</orgName>
								<address>
									<settlement>New Zealand</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong SAR</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">MoE Key Laboratory of High Confidence Software Technologies</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Rumor Detection on Twitter with Tree-structured Recursive Neural Networks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1980" to="1989"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1980</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Automatic rumor detection is technically very challenging. In this work, we try to learn discriminative features from tweets content by following their non-sequential propagation structure and generate more powerful representations for identifying different type of rumors. We propose two recursive neural models based on a bottom-up and a top-down tree-structured neural networks for rumor representation learning and classification, which naturally conform to the propagation layout of tweets. Results on two public Twit-ter datasets demonstrate that our recursive neural models 1) achieve much better performance than state-of-the-art approaches; 2) demonstrate superior capacity on detecting rumors at very early stage.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Rumors have always been a social disease. In re- cent years, it has become unprecedentedly conve- nient for the "evil-doers" to create and disseminate rumors in massive scale with low cost thanks to the popularity of social media outlets on Twitter, Facebook, etc. The worst effect of false rumors could be devastating to individual and/or society.</p><p>Research pertaining rumors spans multiple dis- ciplines, such as philosophy and humanities <ref type="bibr" target="#b6">(DiFonzo and Bordia, 2007;</ref><ref type="bibr" target="#b8">Donovan, 2007)</ref>, social psychology ( <ref type="bibr" target="#b1">Allport and Postman, 1965;</ref><ref type="bibr" target="#b15">Jaeger et al., 1980;</ref><ref type="bibr" target="#b27">Rosnow and Foster, 2005)</ref>, politi- cal studies <ref type="bibr" target="#b0">(Allport and Postman, 1946;</ref><ref type="bibr" target="#b2">Berinsky, 2017</ref>), management science ( <ref type="bibr" target="#b7">DiFonzo et al., 1994;</ref><ref type="bibr" target="#b16">Kimmel, 2004)</ref> and recently computer sci- ence and artificial intelligence <ref type="bibr" target="#b25">(Qazvinian et al., 2011;</ref><ref type="bibr" target="#b26">Ratkiewicz et al., 2011;</ref><ref type="bibr" target="#b3">Castillo et al., 2011;</ref><ref type="bibr" target="#b12">Hannak et al., 2014;</ref><ref type="bibr" target="#b35">Zhao et al., 2015;</ref><ref type="bibr" target="#b21">Ma et al., 2015)</ref>. Rumor is commonly defined as informa- tion that emerge and spread among people whose truth value is unverified or intentionally false <ref type="bibr" target="#b6">(DiFonzo and Bordia, 2007;</ref><ref type="bibr" target="#b25">Qazvinian et al., 2011</ref>). Analysis shows that people tend to stop spread- ing a rumor if it is known as false ( <ref type="bibr" target="#b39">Zubiaga et al., 2016b</ref>). However, identifying such misinforma- tion is non-trivial and needs investigative jour- nalism to fact check the suspected claim, which is labor-intensive and time-consuming. The pro- liferation of social media makes it worse due to the ever-increasing information load and dynam- ics. Therefore, it is necessary to develop automatic and assistant approaches to facilitate real-time ru- mor tracking and debunking.</p><p>For automating rumor detection, most of the previous studies focused on text mining from se- quential microblog streams using supervised mod- els based on feature engineering <ref type="bibr" target="#b3">(Castillo et al., 2011;</ref><ref type="bibr" target="#b17">Kwon et al., 2013;</ref><ref type="bibr" target="#b18">Liu et al., 2015;</ref><ref type="bibr" target="#b21">Ma et al., 2015)</ref>, and more recently deep neural mod- els ( <ref type="bibr" target="#b20">Ma et al., 2016;</ref><ref type="bibr" target="#b4">Chen et al., 2017;</ref><ref type="bibr" target="#b28">Ruchansky et al., 2017</ref>). These methods largely ignore or oversimplify the structural information asso- ciated with message propagation which however has been shown conducive to provide useful clues for identifying rumors. Kernel-based method ( <ref type="bibr" target="#b33">Wu et al., 2015;</ref><ref type="bibr" target="#b22">Ma et al., 2017)</ref> was thus proposed to model the structure as propagation trees in or- der to differentiate rumorous and non-rumorous claims by comparing their tree-based similarities. But such kind of approach cannot directly classify a tree without pairwise comparison with all other trees imposing unnecessary overhead, and it also cannot automatically learn any high-level feature representations out of the noisy surface features.</p><p>In this paper, we present a neural rumor de- tection approach based on recursive neural net- works (RvNN) to bridge the content semantics and propagation clues. RvNN and its variants were originally used to compose phrase or sen- tence representation for syntactic and semantic parsing <ref type="bibr" target="#b30">(Socher et al., 2011</ref><ref type="bibr" target="#b29">(Socher et al., , 2012</ref>. Unlike pars- ing, the input into our model is a propagation tree rooted from a source post rather than the parse tree of an individual sentence, and each tree node is a responsive post instead of an individual words. The content semantics of posts and the responsive relationship among them can be jointly captured via the recursive feature learning process along the tree structure.</p><p>So, why can such neural model do better for the task? Analysis has generally found that Twit- ter could "self-correct" some inaccurate informa- tion as users share opinions, conjectures and evi- dences ( <ref type="bibr" target="#b37">Zubiaga et al., 2017)</ref>. To illustrate our in- tuition, <ref type="figure">Figure 1</ref> exemplifies the propagation trees of two rumors in our dataset, one being false and the other being true 1 . Structure-insensitive meth- ods basically relying on the relative ratio of differ- ent stances in the text cannot do well when such clue is unclear like this example. However, it can be seen that when a post denies the false rumor, it tends to spark supportive or affirmative replies confirming the denial; in contrast, denial to a true rumor tends to trigger question or denial in its replies. This observation may suggest a more gen- eral hypothesis that the repliers tend to disagree with (or question) who support a false rumor or deny a true rumor, and also they tend to agree with who deny a false rumor or support a true rumor. Meanwhile, a reply, rather than directly respond- ing to the source tweet (i.e., the root), is usually re- sponsive to its immediate ancestor <ref type="bibr" target="#b19">(Lukasik et al., 2016;</ref><ref type="bibr" target="#b38">Zubiaga et al., 2016a)</ref>, suggesting obvious local characteristic of the interaction. The recur- sive network naturally models such structures for learning to capture the rumor indicative signals and enhance the representation by recursively ag- gregating the signals from different branches.</p><p>To this end, we extend the standard RvNN into two variants, i.e., a bottom-up (BU) model and a top-down (TD) model, which represent the propa- gation tree structure from different angles, in order to visit the nodes and combine their representa- tions following distinct directions. The important merit of such architecture is that the node features can be selectively refined by the recursion given the connection and direction of all paths of the (a) False rumor (b) True rumor <ref type="figure">Figure 1</ref>: Propagation trees of two rumorous source tweets. Nodes may express stances on their parent as commenting, supporting, questioning or denying. The edge arrow indicates the direction from a response to its responded node, and the po- larity is marked as '+' ('-') for support (denial). The same node color indicates the same stance on the veracity of root node (i.e., source tweet).</p><p>tree. As a result, it can be expected that the dis- criminative signals are better embedded into the learned representations. We evaluate our proposed approach based on two public Twitter datasets. The results show that our method outperforms strong rumor detection baselines with large margin and also demonstrate much higher effectiveness for detection at early stage of propagation, which is promising for real- time intervention and debunking. Our contribu- tions are summarized as follows in three folds:</p><p>• This is the first study that deeply integrates both structure and content semantics based on tree-structured recursive neural networks for detecting rumors from microblog posts.</p><p>• We propose two variants of RvNN models based on bottom-up and top-down tree struc- tures to generate better integrated representa- tions for a claim by capturing both structural and textural properties signaling rumors.</p><p>• Our experiments based on real-world Twitter datasets achieve superior improvements over state-of-the-art baselines on both rumor clas- sification and early detection tasks. We make the source codes in our experiments publicly accessible 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Most previous automatic approaches for rumor de- tection <ref type="bibr" target="#b3">(Castillo et al., 2011;</ref><ref type="bibr" target="#b34">Yang et al., 2012;</ref><ref type="bibr" target="#b18">Liu et al., 2015</ref>) intended to learn a supervised classi- fier by utilizing a wide range of features crafted from post contents, user profiles and propagation patterns. Subsequent studies were then conducted to engineer new features such as those represent- ing rumor diffusion and cascades <ref type="bibr" target="#b10">(Friggeri et al., 2014;</ref><ref type="bibr" target="#b12">Hannak et al., 2014</ref>) characterized by com- ments with links to debunking websites. <ref type="bibr" target="#b17">Kwon et al. (2013)</ref> introduced a time-series-fitting model based on the volume of tweets over time. <ref type="bibr" target="#b21">Ma et al. (2015)</ref> extended their model with more chronolog- ical social context features. These approaches typ- ically require heavy preprocessing and feature en- gineering. <ref type="bibr" target="#b35">Zhao et al. (2015)</ref> alleviated the engineering ef- fort by using a set of regular expressions (such as "really?", "not true", etc) to find questing and denying tweets, but the approach was oversimpli- fied and suffered from very low recall. <ref type="bibr" target="#b20">Ma et al. (2016)</ref> used recurrent neural networks (RNN) to learn automatically the representations from tweets content based on time series. Recently, they studied to mutually reinforce stance detection and rumor classification in a neural multi-task learn- ing framework ( <ref type="bibr" target="#b23">Ma et al., 2018)</ref>. However, the approaches cannot embed features reflecting how the posts are propagated and requires careful data segmentation to prepare for time sequence.</p><p>Some kernel-based methods were exploited to model the propagation structure. <ref type="bibr" target="#b33">Wu et al. (2015)</ref> proposed a hybrid SVM classifier which combines a RBF kernel and a random-walk-based graph ker- nel to capture both flat and propagation patterns for detecting rumors on Sina Weibo. <ref type="bibr" target="#b22">Ma et al. (2017)</ref> used tree kernel to capture the similarity of propagation trees by counting their similar sub- structures in order to identify different types of ru- mors on Twitter. Compared to their studies, our model can learn the useful features via a more nat- ural and general approach, i.e., the tree-structured neural network, to jointly generate representations from both structure and content.</p><p>RvNN has demonstrated state-of-the-art perfor- mances in a variety of tasks, e.g., images seg- mentation <ref type="bibr" target="#b30">(Socher et al., 2011</ref>), phrase represen- tation from word vectors ( <ref type="bibr" target="#b29">Socher et al., 2012)</ref>, and sentiment classification in sentences <ref type="bibr" target="#b31">(Socher et al., 2013)</ref>. More recently, a deep RvNN was proposed to model the compositionality in natu- ral language for fine-grained sentiment classifica- tion by stacking multiple recursive layers <ref type="bibr" target="#b14">(Irsoy and Cardie, 2014)</ref>. In order to avoid gradient van- ishing, some studies integrated Long Short Term Memory (LSTM) <ref type="bibr" target="#b13">(Hochreiter and Schmidhuber, 1997</ref>) to <ref type="bibr">RvNN (Zhu et al., 2015;</ref><ref type="bibr" target="#b32">Tai et al., 2015)</ref>. <ref type="bibr" target="#b24">Mou et al. (2015)</ref> used a convolutional network over tree structures for syntactic tree parsing of natural language sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Statement</head><p>We define a Twitter rumor detection dataset as a set of claims C = {C 1 , C 2 , · · · , C |C| }, where each claim C i corresponds to a source tweet r i which consists of ideally all its relevant respon- sive tweets in chronological order, i.e., C i = {r i , x i1 , x i2 , · · · , x im } where each x i * is a respon- sive tweet of the root r i . Note that although the tweets are notated sequentially, there are connec- tions among them based on their reply or repost relationships, which can form a propagation tree structure ( <ref type="bibr" target="#b33">Wu et al., 2015;</ref><ref type="bibr" target="#b22">Ma et al., 2017</ref>) with r i being the root node.</p><p>We formulate this task as a supervised classifi- cation problem, which learns a classifier f from labeled claims, that is f : C i → Y i , where Y i takes one of the four finer-grained classes: non-rumor, false rumor, true rumor, and unverified rumor that are introduced in the literature ( <ref type="bibr" target="#b22">Ma et al., 2017;</ref><ref type="bibr" target="#b39">Zubiaga et al., 2016b</ref>).</p><p>An important issue of the tree structure is con- cerned about the direction of edges, which can re- sult in two different architectures of the model: 1) a bottom-up tree; 2) a top-down tree, which are defined as follows:</p><p>• Bottom-up tree takes the similar shape as shown in <ref type="figure">Figure 1</ref>, where responsive nodes always point to their responded nodes and leaf nodes not having any response are laid out at the furthest level. We represent a tree as T i = V i , E i , where V i = C i which con- sists of all relevant posts as nodes, and E i de- notes a set of all directed links, where for any u, v ∈ V i , u ← v exists if v responses to u. This structure is similar to a citation network where a response mimics a reference.</p><p>• Top-down tree naturally conforms to the di- rection of information propagation, in which a link u → v means the information flows from u to v and v sees it and provides a re- sponse to u. This structure reverses bottom- up tree and simulates how information cas- cades from a source tweet, i.e., the root, to all its receivers, i.e., the decedents, which is similar as ( <ref type="bibr" target="#b33">Wu et al., 2015;</ref><ref type="bibr" target="#b22">Ma et al., 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RvNN-based Rumor Detection</head><p>The core idea of our method is to strengthen the high-level representation of tree nodes by the re- cursion following the propagation structure over different branches in the tree. For instance, the re- sponsive nodes confirming or supporting a node (e.g., "I agree", "be right", etc) can further rein- force the stance of that node while denial or ques- tioning responses (e.g., "disagree, "really?!) oth- erwise weaken its stance. Compared to the kernel- based method using propagation tree ( <ref type="bibr" target="#b33">Wu et al., 2015;</ref><ref type="bibr" target="#b22">Ma et al., 2017)</ref>, our method does not need pairwise comparison among large number of sub- trees, and can learn much stronger representation of content following the response structure.</p><p>In this section, we will describe our extension to the standard RvNN for modeling rumor detec- tion based on the bottom-up and top-down archi- tectures presented in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Standard Recursive Neural Networks</head><p>RvNN is a type of tree-structured neural networks. The original version of RvNN utilized binarized sentence parse trees <ref type="bibr" target="#b29">(Socher et al., 2012)</ref>, in which the representation associated with each node of a parse tree is computed from its direct children. The overall structure of the standard RvNN is il- lustrated as the right side of <ref type="figure" target="#fig_0">Figure 2</ref>, correspond- ing to the input parse tree at the left side.</p><p>Leaf nodes are the words in an input sen- tence, each represented by a low-dimensional word embedding. Non-leaf nodes are sentence constituents, computed by recursion based on the presentations of child nodes. Let p be the feature vector of a parent node whose children are c 1 and c 2 , the representation of the parent is computed by p = f (W ·[c 1 ; c 2 ]+b), where f (·) is the activation function with W and b as parameters. This compu- tation is done recursively over all tree nodes; the learned hidden vectors of the nodes can then be used for various classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Bottom-up RvNN</head><p>The core idea of bottom-up model is to generate a feature vector for each subtree by recursively visit- ing every node from the leaves at the bottom to the root at the top. In this way, the subtrees with sim- ilar contexts, such as those subtrees having a de- nial parent and a set of supportive children, will be projected into the proximity in the representation space. And thus such local rumor indicative fea- tures are aggregated along different branches into some global representation of the whole tree.</p><p>For this purpose, we make a natural extension to the original RvNN. The overall structure of our proposed bottom-up model is illustrated in <ref type="figure" target="#fig_1">Fig- ure 3(b)</ref>, taking a bottom-up tree (see <ref type="figure" target="#fig_1">Figure 3</ref>(a)) as input. Different from the standard RvNN, the input of each node in the bottom-up model is a post represented as a vector of words in the vocab- ulary in terms of tf idf values. Here, every node has an input vector, and the number of children of nodes varies significantly 3 .</p><p>In rumor detection, long short-term memory (LSTM) <ref type="bibr" target="#b13">(Hochreiter and Schmidhuber, 1997</ref>) and gated recurrent units (GRU) ( <ref type="bibr" target="#b5">Cho et al., 2014)</ref> were used to learn textual representation, which adopts memory units to store information over long time steps <ref type="bibr" target="#b20">(Ma et al., 2016)</ref>. In this paper, we choose to extend GRU as hidden unit to model long-distance interactions over the tree nodes be- cause it is more efficient due to fewer parameters. Let S(j) denote the set of direct children of the node j. The transition equations of node j in the bottom-up model are formulated as follows: where x j is the original input vector of node j, E denotes the parameter matrix for transforming this input post, ˜ x j is the transformed representa- tion of j, [W * , U * ] are the weight connections in- side GRU, and h j and h s refer to the hidden state of j and its s-th child. Thus h S denotes the sum of the hidden state of all the children of j assum- ing that all children are equally important to j. As with the standard GRU, denotes element-wise multiplication; a reset gate r j determines how to combine the current input˜xinput˜ input˜x j with the memory of children, and an update gate z j defines how much memory from the children is cascaded into the cur- rent node; and˜hand˜ and˜h j denotes the candidate activation of the hidden state of the current node. Different from the standard GRU unit, the gating vectors in our variant of GRU are dependent on the states of many child units, allowing our model to incorpo- rate representations from different children.</p><formula xml:id="formula_0">˜ x j = x j E h S = s∈S(j) h s r j = σ (W r ˜ x j + U r h S ) z j = σ (W z ˜ x j + U z h S ) ˜ h j = tanh (W h ˜ x j + U h (h S r j )) h j = (1 − z j ) h S + z j ˜ h j<label>(1)</label></formula><p>After recursive aggregation from bottom to up, the state of root node (i.e., source tweet) can be re- gard as the representation of the whole tree which is used for supervised classification. So, an output layer is connected to the root node for predicting the class of the tree using a softmax function:</p><formula xml:id="formula_1">ˆ y = Sof tmax(Vh 0 + b)<label>(2)</label></formula><p>where h 0 is the learned hidden vector of root node; V and b are the weights and bias in output layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Top-down RvNN</head><p>This model is designed to leverage the structure of top-down tree to capture complex propagation patterns for classifying rumorous claims, which is shown in <ref type="figure" target="#fig_1">Figure 3(c)</ref>. It models how the informa- tion flows from source post to the current node. The idea of this top-down approach is to generate a strengthened feature vector for each post consid- ering its propagation path, where rumor-indicative features are aggregated along the propagation his- tory in the path. For example, if current post agree with its parent's stance which denies the source post, the denial stance from the root node down to the current node on this path should be reinforced.</p><p>Due to different branches of any non-leaf node, the top-down visit to its subtree nodes is also recur- sive. However, the nature of top-down tree lends this model different from the bottom-up one. The representation of each node is computed by com- bining its own input and its parent node instead of its children nodes. This process proceeds recur- sively from the root node to its children until all leaf nodes are reached. Suppose that the hidden state of a non-leaf node can be passed synchronously to all its child nodes without loss. Then the hidden state h j of a node j can be computed by combining the hidden state h P(j) of its parent node P(j) and its own input vector x j . Therefore, the transition equations of node j can be formulated as a standard GRU:</p><formula xml:id="formula_2">˜ x j = x j E r j = σ W r ˜ x j + U r h P(j) z j = σ W z ˜ x j + U z h P(j) ˜ h j = tanh W h ˜ x j + U h (h P(j) r j ) h j = (1 − z j ) h P(j) + z j ˜ h j<label>(3)</label></formula><p>Through the top-down recursion, the learned representations are eventually embedded into the hidden vector of all the leaf nodes. Since the num-ber of leaf nodes varies, the resulting vectors can- not be directly fed into a fixed-size neural layer for output. Therefore, we add a max-pooling layer to take the maximum value of each dimension of the vectors over all the leaf nodes. This can also help capture the most appealing indicative features from all the propagation paths.</p><p>Based on the pooling result, we finally use a softmax function in the output layer to predict the label of the tree:</p><formula xml:id="formula_3">ˆ y = Sof tmax(Vh ∞ + b)<label>(4)</label></formula><p>where h ∞ is the pooling vector over all leaf nodes, V and b are parameters in the output layer.</p><p>Although both of the two RvNN models aim to capture the structural properties by recursively visiting all nodes, we can conjecture that the top- down model would be better. The hypothesis is that in the bottom-up case the final output relies on the representation of single root, and its informa- tion loss can be larger than the top-down one since in the top-down case the representations embed- ded into all leaf nodes along different propagation paths can be incorporated via pooling holistically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Model Training</head><p>The model is trained to minimize the squared error between the probability distributions of the predic- tions and the ground truth:</p><formula xml:id="formula_4">L(y, ˆ y) = N n=1 C c=1 (y c − ˆ y c ) 2 + λ||θ|| 2 2 (5)</formula><p>where y c is the ground truth andˆyandˆ andˆy c is the pre- diction probability of a class, N is the number of training claims, C is the number of classes, ||.|| 2 is the L 2 regularization term over all model parame- ters θ, and λ is the trade-off coefficient. During training, all the model parameters are updated using efficient back-propagation through structure <ref type="bibr" target="#b11">(Goller and Kuchler, 1996;</ref><ref type="bibr" target="#b31">Socher et al., 2013)</ref>, and the optimization is gradient-based fol- lowing the Ada-grad update rule <ref type="bibr" target="#b9">(Duchi et al., 2011</ref>) to speed up the convergence. We empiri- cally initialize the model parameters with uniform distribution and set the vocabulary size as 5,000, the size of embedding and hidden units as 100. We iterate over all the training examples in each epoch and continue until the loss value converges or the maximum epoch number is met.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>For experimental evaluation, we use two publicly available Twitter datasets released by <ref type="bibr" target="#b22">Ma et al. (2017)</ref>, namely Twitter15 and Twitter16 4 , which respectively contains 1,381 and 1,181 propagation trees (see <ref type="bibr" target="#b22">(Ma et al., 2017</ref>) for detailed statistics). In each dataset, a group of wide spread source tweets along with their propagation threads, i.e., replies and retweets, are provided in the form of tree structure. Each tree is annotated with one of the four class labels, i.e., non-rumor, false ru- mor, true rumor and unverified rumor. We remove the retweets from the trees since they do not pro- vide any extra information or evidence content- wise. We build two versions for each tree, one for the bottom-up tree and the other for the top-down tree, by flipping the edges' direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Setup</head><p>We make comprehensive comparisons between our models and some state-of-the-art baselines on rumor classification and early detection tasks.</p><p>-DTR: Zhao et al. <ref type="formula" target="#formula_0">(2015)</ref> proposed a Decision- Tree-based Ranking model to identify trending ru- mors by searching for inquiry phrases.</p><p>-DTC: The information credibility model using a Decision-Tree Classifier <ref type="bibr" target="#b3">(Castillo et al., 2011</ref>) based on manually engineering various statistical features of the tweets.</p><p>-RFC: The Random Forest Classier using 3 fit- ting parameters as temporal properties and a set of handcrafted features on user, linguistic and struc- tural properties ( <ref type="bibr" target="#b17">Kwon et al., 2013)</ref>.</p><p>-SVM-TS: A linear SVM classifier that uses time-series to model the variation of handcrafted social context features ( <ref type="bibr" target="#b21">Ma et al., 2015)</ref>.</p><p>-SVM-BOW: A naive baseline we built by rep- resenting text content using bag-of-words and us- ing linear SVM for rumor classification.</p><p>-SVM-TK and SVM-HK: SVM classifier uses a Tree Kernel ( <ref type="bibr" target="#b22">Ma et al., 2017)</ref> and that uses a Hy- brid Kernel ( <ref type="bibr" target="#b33">Wu et al., 2015</ref>), respectively, both of which model propagation structures with kernels.</p><p>-GRU-RNN: A detection model based on re- current neural networks ( <ref type="bibr" target="#b20">Ma et al., 2016</ref>) with GRU units for learning rumor representations by modeling sequential structure of relevant posts.    -BU-RvNN and TD-RvNN: Our bottom-up and top-down RvNN models, respectively.</p><p>We implement DTC and RFC using Weka 5 , SVM-based models using LibSVM <ref type="bibr">6</ref> and all neural-network-based models with Theano <ref type="bibr">7</ref> . We conduct 5-fold cross-validation on the datasets and use accuracy over all the four categories and F1 measure on each class to evaluate the performance of models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Rumor Classification Performance</head><p>As shown in <ref type="table" target="#tab_1">Table 1</ref>, our proposed models ba- sically yield much better performance than other methods on both datasets via the modeling of in- teraction structures of posts in the propagation.</p><p>It is observed that the performance of the 4 baselines in the first group based on handcrafted features is obviously poor, varying between 0.409 and 0.585 in accuracy, indicating that they fail to generalize due to the lack of capacity capturing helpful features. Among these baselines, SVM- TS and RFC perform relatively better because they <ref type="bibr">5</ref> www.cs.waikato.ac.nz/ml/weka 6 www.csie.ntu.edu.tw/ ˜ cjlin/libsvm 7 deeplearning.net/software/theano use additional temporal traits, but they are still clearly worse than the models not relying on fea- ture engineering. DTR uses a set of regular ex- pressions indicative of stances. However, only 19.6% and 22.2% tweets in the two datasets con- tain strings covered by these regular expressions, rendering unsatisfactory result.</p><p>Among the two kernel methods that are based on comparing propagation structures, we observe that SVM-TK is much more effective than SVM- HK. There are two reasons: 1) SVM-HK was originally proposed and experimented on Sina Weibo ( <ref type="bibr" target="#b33">Wu et al., 2015</ref>), which may not be gener- alize well on Twitter. 2) SVM-HK loosely couples two separate kernels: a RBF kernel based on hand- crafted features, plus a random walk-based ker- nel which relies on a set of pre-defined keywords for jumping over the nodes probabilistically. This under utilizes the propagation information due to such oversimplified treatment of tree structure. In contrast, SVM-TK is an integrated kernel and can fully utilize the structure by comparing the trees based on both textual and structural similarities.</p><p>It appears that using bag-of-words is already a decent model evidenced as the fairly good perfor- mance of SVM-BOW which is even better than SVM-HK. This is because the features of SVM- HK are handcrafted for binary classification (i.e., non-rumor vs rumor), ignoring the importance of indicative words or units that benefit finer-grained classification which can be captured more effec- tively by SVM-BOW.</p><p>The sequential neural model GRU-RNN per- forms slightly worse than SVM-TK, but much worse than our recursive models. This is because it is a special case of the recursive model where each non-leaf node has only one child. It has to rely on a linear chain as input, which missed out valuable structural information. However, it does learn high-level features from the post content via hidden units of the neural model while SVM-TK cannot which can only evaluates similarities based on the overlapping words among subtrees. Our re- cursive models are inherently tree-structured and take advantages of representation learning follow- ing the propagation structure, thus beats SVM-TK.</p><p>In the two recursive models, TD-RvNN outper- forms BU-RvNN, which indicates that the bottom- up model may suffer from larger information loss than the top-down one. This verifies the hypothe- sis we made in Section 4.3 that the pooling layer  in the top-down model can effectively select im- portant features embedded into the leaf nodes.</p><p>For only the non-rumor class, it seems that our method does not perform so well as some feature- engineering baselines. This can be explained by the fact that these baselines are trained with ad- ditional features such as user information (e.g., profile, verification status, etc) which may contain clues for differentiating non-rumors from rumors. Also, the responses to non-rumors are usually much more diverse with little informative indi- cation, making identification of non-rumors more difficult based on content even with the structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Early Rumor Detection Performance</head><p>Detecting rumors at early state of propagation is important so that interventions can be made in a timely manner. We compared different methods in term of different time delays measured by ei- ther tweet count received or time elapsed since the source tweet is posted. The performance is evalu- ated by the accuracy obtained when we incremen- tally add test data up to the check point given the targeted time delay or tweets volume. <ref type="figure" target="#fig_4">Figure 4</ref> shows that the performance of our re- cursive models climbs more rapidly and starts to supersede the other models at the early stage. Al- though all the methods are getting to their best per- formance in the end, TD-RvNN and BU-RvNN only need around 8 hours or about 90 tweets to achieve the comparable performance of the best baseline model, i.e., SVM-TK, which needs about 36 hours or around 300 posts, indicating superior early detection performance of our method. <ref type="figure" target="#fig_5">Figure 5</ref> shows a sample tree at the early stage of propagation that has been correctly classified as a false rumor by both recursive models. We can see that this false rumor demonstrates typical pat- terns in subtrees and propagation paths indicative of the falsehood, where a set of responses sup- porting the parent posts that deny or question the source post are captured by our bottom-up model. Similarly, some patterns of propagation from the root to leaf nodes like "support→deny→support" are also seized by our top-down model. In com- parison, sequential models may be confused be- cause the supportive key terms such as "be right", "yeah", "exactly!" dominate the responses, and the SVM-TK may miss similar subtrees by just comparing the surface words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>We propose a bottom-up and a top-down tree- structured model based on recursive neural net- works for rumor detection on Twitter. The inher-ent nature of recursive models allows them using propagation tree to guide the learning of represen- tations from tweets content, such as embedding various indicative signals hidden in the structure, for better identifying rumors. Results on two pub- lic Twitter datasets show that our method improves rumor detection performance in very large mar- gins as compared to state-of-the-art baselines.</p><p>In our future work, we plan to integrate other types of information such as user properties into the structured neural models to further enhance representation learning and detect rumor spread- ers at the same time. We also plan to use unsuper- vised models for the task by exploiting structural information.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A binarized sentence parse tree (left) and its corresponding RvNN architecture (right).</figDesc><graphic url="image-3.png" coords="4,73.13,62.81,216.01,71.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A bottom-up/top-down propagation tree and the corresponding RvNN-based models. The black-color and red-color edges differentiate the bottom-up and top-down tree in Figure 3(a).</figDesc><graphic url="image-4.png" coords="5,74.73,73.77,144.00,134.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>(</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>(Figure 4 :</head><label>4</label><figDesc>Figure 4: Early rumor detection accuracy at different checkpoints in terms of elapsed time (tweets count).</figDesc><graphic url="image-11.png" coords="8,75.57,204.82,446.38,120.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: A correctly detected false rumor at early stage by both of our models, where propagation paths are marked with relevant stances. Note that edge direction is not shown as it applies to either case.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>a) Twitter15 dataset</head><label></label><figDesc></figDesc><table>Method 
NR 
FR 
TR 
UR 
Acc. 
F1 
F1 
F1 
F1 
DTR 
0.409 0.501 0.311 0.364 0.473 
DTC 
0.454 0.733 0.355 0.317 0.415 
RFC 
0.565 0.810 0.422 0.401 0.543 
SVM-TS 
0.544 0.796 0.472 0.404 0.483 
SVM-BOW 0.548 0.564 0.524 0.582 0.512 
SVM-HK 
0.493 0.650 0.439 0.342 0.336 
SVM-TK 
0.667 0.619 0.669 0.772 0.645 
GRU-RNN 
0.641 0.684 0.634 0.688 0.571 
BU-RvNN 
0.708 0.695 0.728 0.759 0.653 
TD-RvNN 
0.723 0.682 0.758 0.821 0.654 

(b) Twitter16 dataset 

Method 
NR 
FR 
TR 
UR 
Acc. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Results of rumor detection. (NR: non-
rumor; FR: false rumor; TR: true rumor; UR: un-
verified rumor) 

</table></figure>

			<note place="foot" n="1"> False (true) rumor means the veracity of the rumorous claim is false (true).</note>

			<note place="foot" n="2"> https://github.com/majingCUHK/Rumor_ RvNN</note>

			<note place="foot" n="3"> In standard RvNN, since an input instance is the parse tree of a sentence, only leaf nodes have input vector, each node representing a word of the input sentence, and the nonleaf nodes are constituents of the sentence, and thus the number of children of a node is limited.</note>

			<note place="foot" n="4"> https://www.dropbox.com/s/ 7ewzdrbelpmrnxu/rumdetect2017.zip?dl=0</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work is partly supported by Innovation and Technology Fund (ITF) Project No. 6904333, and General Research Fund (GRF) Project <ref type="bibr">No. 14232816 (12183516)</ref>. We would like to thank anonymous reviewers for the insightful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An analysis of rumor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Allport</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Postman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Public Opinion Quarterly</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="501" to="517" />
			<date type="published" when="1946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The psychology of rumor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Allport</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Postman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1965" />
			<pubPlace>Russell &amp; Russell</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rumors and health care reform: Experiments in political misinformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">J</forename><surname>Berinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British Journal of Political Science</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">241262</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Information credibility on twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcelo</forename><surname>Mendoza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Poblete</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW</title>
		<meeting>WWW</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="675" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Call attention to rumors: Deep attention based recurrent neural networks for early rumor detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05973</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">Encoder-decoder approaches. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rumor, gossip and urban legends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Difonzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prashant</forename><surname>Bordia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Diogenes</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="35" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Reining in rumors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Difonzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prashant</forename><surname>Bordia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph L</forename><surname>Rosnow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Organizational Dynamics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="62" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">How idle is idle talk? one hundred years of rumor research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Donovan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Diogenes</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="82" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rumor cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Friggeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><surname>Adamic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Eckles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICWSM</title>
		<meeting>ICWSM</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning task-dependent distributed representations by backpropagation through structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Goller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kuchler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="347" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Get back! you don&apos;t know me like that: The social mediation of fact checking interventions in twitter conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniko</forename><surname>Hannak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Drew</forename><surname>Margolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Keegan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingmar</forename><surname>Weber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICWSM</title>
		<meeting>ICWSM</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep recursive neural networks for compositionality in language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2096" to="2104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Who hears what from whom and with what effect: A study of rumor. Personality and Social</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marianne</forename><forename type="middle">E</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph L</forename><surname>Rosnow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychology Bulletin</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="473" to="478" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Rumors and rumor control: A manager&apos;s guide to understanding and combatting rumors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kimmel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>Routledge</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Prominent features of rumor propagation in online social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meeyoung</forename><surname>Sejeong Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyomin</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICDM</title>
		<meeting>ICDM</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1103" to="1108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Real-time rumor debunking on twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaomo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armineh</forename><surname>Nourbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameena</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International on Conference on Information and Knowledge Management. CIKM &apos;15</title>
		<meeting>the 24th ACM International on Conference on Information and Knowledge Management. CIKM &apos;15</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1867" to="1870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hawkes processes for continuous time sequence classification: an application to rumour stance classification in twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duy</forename><surname>Srijith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalina</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arkaitz</forename><surname>Bontcheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Zubiaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="393" to="398" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Detecting rumors from microblogs with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasenjit</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sejeong</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kam-Fai</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meeyoung</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence. IJCAI&apos;16</title>
		<meeting>the Twenty-Fifth International Joint Conference on Artificial Intelligence. IJCAI&apos;16</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3818" to="3824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Detect rumors using time series of social context information on microblogging websites</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueming</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International on Conference on Information and Knowledge Management. CIKM &apos;15</title>
		<meeting>the 24th ACM International on Conference on Information and Knowledge Management. CIKM &apos;15</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1751" to="1754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Detect rumors in microblog posts using propagation structure via kernel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="708" to="717" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Detect rumor and stance jointly by neural multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion Proceedings of the The Web Conference 2018. WWW &apos;18</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="585" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Discriminative neural sentence modeling by tree-based convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.01106</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rumor has it: Identifying misinformation in microblogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Vahed Qazvinian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rosengren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Dragomir R Radev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing. EMNLP &apos;11</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing. EMNLP &apos;11</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1589" to="1599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Truthy: mapping the spread of astroturf in microblog streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Ratkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Conover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Meiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Gonçalves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Snehal</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Flammini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filippo</forename><surname>Menczer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference Companion on World Wide Web. WWW &apos;11</title>
		<meeting>the 20th International Conference Companion on World Wide Web. WWW &apos;11</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="249" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rumor and gossip research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ralph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric K</forename><surname>Rosnow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Foster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science Agenda</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Csi: A hybrid deep model for fake news detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natali</forename><surname>Ruchansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungyong</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management. CIKM &apos;17</title>
		<meeting>the 2017 ACM on Conference on Information and Knowledge Management. CIKM &apos;17</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="797" to="806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
		<title level="m">Parsing natural scenes and natural language with recursive neural networks. In Proceedings of the 28th international conference on machine learning (ICML-11)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 conference on empirical methods in natural language processing</title>
		<meeting>the 2013 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00075</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">False rumors detection on sina weibo by propagation structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenny Q</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 31st International Conference on. IEEE</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="651" to="662" />
		</imprint>
	</monogr>
	<note>Data Engineering (ICDE)</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Automatic detection of rumor on sina weibo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD Workshop on Mining Data Semantics. MDS &apos;12</title>
		<meeting>the ACM SIGKDD Workshop on Mining Data Semantics. MDS &apos;12</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Enquiring minds: Early detection of rumors in social media from enquiry posts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Resnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web. WWW &apos;15</title>
		<meeting>the 24th International Conference on World Wide Web. WWW &apos;15</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1395" to="1405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Long short-term memory over recursive structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parinaz</forename><surname>Sobihani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1604" to="1612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Detection and resolution of rumours in social media: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arkaitz</forename><surname>Zubiaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmet</forename><surname>Aker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalina</forename><surname>Bontcheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Liakata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Procter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00656</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Stance classification in rumours as a sequential task exploiting the tree structure of social media conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arkaitz</forename><surname>Zubiaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Kochkina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Liakata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Procter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Lukasik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2438" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Analysing how people orient to and spread rumours in social media by looking at conversational threads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arkaitz</forename><surname>Zubiaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Liakata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Procter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">150989</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Geraldine Wong Sak Hoi, and Peter Tolmie</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
