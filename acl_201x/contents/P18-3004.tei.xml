<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Recursive Neural Network Based Preordering for English-to-Japanese Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Kawara</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Graduate School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Institute for Datability Science</orgName>
								<orgName type="institution" key="instit1">Osaka University</orgName>
								<orgName type="institution" key="instit2">Osaka University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhui</forename><surname>Chu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Graduate School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Institute for Datability Science</orgName>
								<orgName type="institution" key="instit1">Osaka University</orgName>
								<orgName type="institution" key="instit2">Osaka University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Arase</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Graduate School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Institute for Datability Science</orgName>
								<orgName type="institution" key="instit1">Osaka University</orgName>
								<orgName type="institution" key="instit2">Osaka University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Recursive Neural Network Based Preordering for English-to-Japanese Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of ACL 2018, Student Research Workshop</title>
						<meeting>ACL 2018, Student Research Workshop <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="21" to="27"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>21</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The word order between source and target languages significantly influences the translation quality in machine translation. Preordering can effectively address this problem. Previous preordering methods require a manual feature design, making language dependent design costly. In this paper, we propose a preordering method with a recursive neural network that learns features from raw inputs. Experiments show that the proposed method achieves comparable gain in translation quality to the state-of-the-art method but without a manual feature design.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The word order between source and target languages significantly influences the transla- tion quality in statistical machine translation (SMT) <ref type="bibr" target="#b6">(Tillmann, 2004;</ref><ref type="bibr">Hayashi et al., 2013;</ref><ref type="bibr">Nakagawa, 2015)</ref>. Models that adjust orders of translated phrases in decoding have been proposed to solve this problem <ref type="bibr" target="#b6">(Tillmann, 2004;</ref><ref type="bibr">Koehn et al., 2005;</ref><ref type="bibr">Nagata et al., 2006</ref>). However, such reordering models do not perform well for long-distance reordering. In addition, their com- putational costs are expensive. To address these problems, preordering ( <ref type="bibr" target="#b9">Xia and McCord, 2004;</ref><ref type="bibr" target="#b8">Wang et al., 2007;</ref><ref type="bibr" target="#b10">Xu et al., 2009;</ref><ref type="bibr">Isozaki et al., 2010b;</ref><ref type="bibr">Gojun and Fraser, 2012;</ref><ref type="bibr">Nakagawa, 2015)</ref> and post-ordering ( <ref type="bibr">Goto et al., 2012</ref><ref type="bibr">Goto et al., , 2013</ref><ref type="bibr">Hayashi et al., 2013</ref>) models have been proposed. Preordering reorders source sentences before translation, while post-ordering reorders sentences translated without considering the word order after translation. In particular, preorder- ing effectively improves the translation quality because it solves long-distance reordering and computational complexity issues ( <ref type="bibr">Jehl et al., 2014;</ref><ref type="bibr">Nakagawa, 2015)</ref>.</p><p>Rule-based preordering methods either man- ually create reordering rules ( <ref type="bibr" target="#b8">Wang et al., 2007;</ref><ref type="bibr" target="#b10">Xu et al., 2009;</ref><ref type="bibr">Isozaki et al., 2010b;</ref><ref type="bibr">Gojun and Fraser, 2012)</ref> or extract reordering rules from a corpus ( <ref type="bibr" target="#b9">Xia and McCord, 2004;</ref><ref type="bibr">Genzel, 2010)</ref>. On the other hand, studies in ( <ref type="bibr" target="#b2">Neubig et al., 2012;</ref><ref type="bibr">Lerner and Petrov, 2013;</ref><ref type="bibr">Hoshino et al., 2015;</ref><ref type="bibr">Nakagawa, 2015)</ref> apply machine learning to the preordering problem. <ref type="bibr">Hoshino et al. (2015)</ref> proposed a method that learns whether child nodes should be swapped at each node of a syntax tree. <ref type="bibr" target="#b2">Neubig et al. (2012)</ref> and <ref type="bibr">Nakagawa (2015)</ref> proposed methods that con- struct a binary tree and reordering simultaneously from a source sentence. These methods require a manual feature design for every language pair, which makes language dependent design costly. To overcome this challenge, methods based on feed forward neural networks that do not require a manual feature design have been proposed (de <ref type="bibr">Gispert et al., 2015;</ref><ref type="bibr">Botha et al., 2017)</ref>. However, these methods decide whether to reorder child nodes without considering the sub-trees, which contains important information for reordering.</p><p>As a preordering method that is free of man- ual feature design and makes use of information in sub-trees, we propose a preordering method with a recursive neural network (RvNN). RvNN cal- culates reordering in a bottom-up manner (from the leaf nodes to the root) on a source syntax tree. Thus, preordering is performed consid- ering the entire sub-trees. Specifically, RvNN learns whether to reorder nodes of a syntax tree 1 with a vector representation of sub-trees and syntactic categories. We evaluate the proposed method for English-to-Japanese translations us- ing both phrase-based SMT (PBSMT) and neu- ral MT (NMT). The results confirm that the pro- posed method achieves comparable translation quality to the state-of-the-art preordering method <ref type="bibr">(Nakagawa, 2015</ref>) that requires a manual feature design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preordering with a Recursive Neural Network</head><p>We explain our design of the RvNN to conduct preordering after describing how to obtain gold- standard labels for preordering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Gold-Standard Labels for Preordering</head><p>We created training data for preordering by label- ing whether each node of the source-side syntax tree has reordered child nodes against a target- side sentence. The label is determined based on <ref type="bibr">Kendall's τ (Kendall, 1938)</ref> as in <ref type="bibr">(Nakagawa, 2015)</ref>, which is calculated by Equation <ref type="formula">(1)</ref>.</p><formula xml:id="formula_0">τ = 4 ∑ |y|−1 i=1 ∑ |y| j=i+1 δ(y i &lt; y j ) |y|(|y| − 1) − 1,(1) δ(x) = { 1 (x is true), 0 (otherwise),</formula><p>where y is a vector of target word indexes that are aligned with source words. The value of Kendall's τ is in <ref type="bibr">[−1, 1]</ref>. When it is 1, it means the se- quence of y is in a complete ascending order, i.e., target sentence has the same word order with the source in terms of word alignment. At each node, if Kendall's τ increases by reordering child nodes, an "Inverted" label is assigned; otherwise, a "Straight" label, which means the child nodes do not need to be reordered, is assigned. When a source word of a child node does not have an alignment, a "Straight" label is assigned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Preordering Model</head><p>RvNN is constructed given a binary syntax tree. It predicts the label determined in Section 2.1 at each node. RvNN decides whether to reorder the child nodes by considering the sub-tree. The vector of the sub-tree is calculated in a bottom-up manner from the leaf nodes. <ref type="figure" target="#fig_0">Figure 1</ref> shows an example of preordering of an English sentence "My parents live in London." At the VP node corresponding to "live in London," the vector of the node is calcu- lated by Equation (2), considering its child nodes correspond to "live" and "in London."</p><formula xml:id="formula_1">p = f ([p l ; p r ]W + b),<label>(2)</label></formula><formula xml:id="formula_2">s = pW s + b s ,<label>(3)</label></formula><p>where f is a rectifier, W ∈ R 2λ×λ is a weight ma- trix, p l and p r are vector representations of the left and right child nodes, respectively. [·; ·] denotes the concatenation of two vectors. W s ∈ R λ×2 is a weight matrix for the output layer, and b, b s ∈ R λ are the biases. s ∈ R 2 calculated by Equation <ref type="formula" target="#formula_2">(3)</ref> is a weight vector for each label, which is fed into a softmax function to calculate the probabilities of the "Straight" and "Inverted" labels. At a leaf node, a word embedding calculated by Equations (4) and (5) is fed into Equation (2).</p><formula xml:id="formula_3">e = xW E ,<label>(4)</label></formula><formula xml:id="formula_4">p e = f (eW l + b l ),<label>(5)</label></formula><p>where x ∈ R N is a one-hot vector of an input word with a vocabulary size of N , W E ∈ R N ×λ is an embedding matrix, and b l ∈ R λ is the bias. The loss function is the cross entropy defined by Equation (6).</p><formula xml:id="formula_5">L(θ) = − 1 K K ∑ k=1 ∑ n∈T l n k log p(l n k ; θ), (6)</formula><p>where θ is the parameters of the model, n is the node of a syntax tree T , K is a mini batch size, and l n k is the label of the n-th node in the k-th syntax tree in the mini batch.</p><p>With the model using POS tags and syntactic categories, we use Equation <ref type="formula" target="#formula_6">(7)</ref> instead of Equa- tion (2).</p><formula xml:id="formula_6">p = f ([p l ; p r ; e t ]W t + b t ),<label>(7)</label></formula><p>where e t represents a vector of POS tags or syn- tactic categories, W t ∈ R 3λ×λ is a weight matrix, and b t ∈ R λ is the bias. e t is calculated in the same manner as Equations <ref type="formula" target="#formula_3">(4)</ref> and <ref type="formula" target="#formula_4">(5)</ref>, but the in- put is a one-hot vector of the POS tags or syntactic categories at each node. λ is tuned on a develop- ment set, whose effects are investigated in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Settings</head><p>We conducted English-to-Japanese transla- tion experiments using the ASPEC corpus ( <ref type="bibr" target="#b1">Nakazawa et al., 2016)</ref>. This corpus provides 3M sentence pairs as training data, 1, 790 sentence pairs as development data, and 1, 812 sentence pairs as test data. We used Stanford CoreNLP 2 for tokenization and POS tagging, Enju 3 for parsing of English, and MeCab 4 for tokenization of Japanese. For word alignment, we used MGIZA. 5 Source-to-target and target-to-source word alignments were calculated using IBM model 1 and hidden Markov model, and they were combined with the intersection heuristic following <ref type="bibr">(Nakagawa, 2015)</ref>. We implemented our RvNN preordering model with Chainer. <ref type="bibr">6</ref> The ASPEC corpus was created using the sentence alignment method proposed in <ref type="bibr" target="#b7">(Utiyama and Isahara, 2007)</ref> and was sorted based on the alignment confidence scores. In this pa- per, we used 100k sentences sampled from the top 500k sentences as training data for preordering. The vocabulary size N was set to 50k. We used Adam ( <ref type="bibr">Kingma and Ba, 2015</ref>) with a weight de- cay and gradient clipping for optimization. The mini batch size K was set to 500.</p><p>We compared our model with the state-of-the- art preordering method proposed in <ref type="bibr">(Nakagawa, 2015)</ref>, which is hereafter referred to as BTG. We used its publicly available implementation, <ref type="bibr">7</ref> and trained it on the same 100k sentences as our model.</p><p>We used the 1.8M source and target sentences as training data for MT. We excluded part of the sentence pairs whose lengths were longer than <ref type="bibr">2</ref>  50 words or the source to target length ratio ex- ceeded 9. For SMT, we used Moses. <ref type="bibr">8</ref> We trained the 5-gram language model on the target side of the training corpus with KenLM. <ref type="bibr">9</ref> Tuning was performed by minimum error rate training <ref type="bibr" target="#b3">(Och, 2003)</ref>. We repeated tuning and testing of each model 3 times and reported the average of scores. For NMT, we used the attention-based encoder- decoder model of ( <ref type="bibr">Luong et al., 2015</ref>) with 2-layer LSTM implemented in OpenNMT. <ref type="bibr">10</ref> The sizes of the vocabulary, word embedding, and hidden layer were set to 50k, 500, and 500, respectively. The batch size was set to 64, and the number of epochs was set to 13. The translation quality was evaluated using BLEU ( <ref type="bibr" target="#b4">Papineni et al., 2002</ref>) and RIBES ( <ref type="bibr">Isozaki et al., 2010a</ref>) using the bootstrap resampling method <ref type="bibr">(Koehn, 2004</ref>) for the signifi- cance test. <ref type="figure">Figure 2</ref> shows the learning curve of our preorder- ing model with λ = 200. 11 Both the training and Avogadro 's hypothesis ( 1811 ) contributed to the development in since then <ref type="figure">Figure 4</ref>: Example of a syntax tree with a parse-error (the phrase "(1811)" was divided in two phrases by mistake). Our preordering result was affected by such parse-errors. (Nodes with a horizontal line means "Inverted".)   the development losses decreased until 2 epochs. However, the development loss started to increase after 3 epochs. Therefore, the number of epochs was set up to 5, and we chose the model with the lowest development loss. The source sentences in the translation evaluation were preordered with this model. Next, we investigated the effect of λ. <ref type="table">Table  1</ref> shows the BLEU scores with different λ val- ues, as well as the BLEU score without preorder- ing. In this experiment, PBSMT was trained with a 500k subset of training data, and the distortion limit was set to 6. Our RvNNs consistently out- performed the plain PBSMT without preordering. The BLEU score improved as λ increased when only word embedding was considered. In addi- tion, RvNNs involving POS tags and syntactic cat- egories achieved even higher BLEU scores. This result shows the effectiveness of POS tags and syntactic categories in reordering. For these mod- els, setting λ larger than 200 did not contribute to the translation quality. Based on these, we further evaluated the RvNN with POS tags and syntactic categories where λ = 200. <ref type="table" target="#tab_2">Table 2</ref> shows BLEU and RIBES scores of the test set on PBSMT and NMT trained on the en- tire training data of 1.8M sentence pairs. The dis- tortion limit of SMT systems trained using pre- ordered sentences by RvNN and BTG was set to 0, while that without preordering was set to 6. Com- pared to the plain PBSMT without preordering, both BLEU and RIBES increased significantly with preordering by RvNN and BTG. These scores were comparable (statistically insignificant at p &lt; 0.05) between RvNN and BTG, 12 indicating that the proposed method achieves a translation quality comparable to BTG. In contrast to the case of PB- SMT, NMT without preordering achieved a signif- icantly higher BLEU score than NMT models with preordering by RvNN and BTG. This is the same phenomenon in the Chinese-to-Japanese transla- tion experiment reported in <ref type="bibr" target="#b5">(Sudoh and Nagata, 2016)</ref>. We assume that one reason is the isola- tion between preordering and NMT models, where both models are trained using independent opti- mization functions. In the future, we will investi- gate this problem and consider a model that unifies Preordered examples Source sentence because of the embedding heterostructure, current leakage around the threshold was minimal. BTG of the embedding heterostructure because, the threshold around current leakage minimal was. RvNN embedding heterostructure the of because, around threshold the current leakage minimal was. Translation examples by PBSMT Reference  (embedding heterostructure of because, threshold around leakage very minimal.) w/o preordering  (embedding heterostructure of because, leakage threshold around minimal.) BTG  (of embedding heterostructure of because, the threshold around leakage minimal.) RvNN  (embedding heterostructure of because, around threshold leakage recognized not.)  <ref type="table">Table 4</ref>: Example of a parse-error disturbed preordering in our method. (Literal translations are given in the parenthesis under the Japanese sentences.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>preordering and translation in a single model. <ref type="figure" target="#fig_1">Figure 3</ref> shows the distribution of Kendall's τ in the original training data as well as the dis- tributions after preordering by RvNN and BTG. The ratio of high Kendall's τ largely increased in the case of RvNN, suggesting that the proposed method learns preordering properly. Furthermore, the ratio of high Kendall's τ by RvNN is more than that of BTG, implying that preordering by RvNN is better than that by BTG.</p><p>We also manually investigated the preordering and translation results. We found that our model improved both. <ref type="table" target="#tab_3">Table 3</ref> shows a successful pre- ordering and translation example on PBSMT. The word order is notably different between source and reference sentences. After preordering, the word order between the source and reference sentences became the same. Because RvNN depends on parsing, sentences with a parse-error tended to fail in preordering. For example, the phrase "(1811)" in <ref type="figure">Figure 4</ref> was divided in two phrases by mistake. Consequently, preordering failed. <ref type="table">Table 4</ref> shows preordering and translation examples for the sen- tence in <ref type="figure">Figure 4</ref>. Compared to the translation without preordering, the translation quality after preordering was improved to deliver correct mean- ing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we proposed a preordering method without a manual feature design for MT. The ex- periments confirmed that the proposed method achieved a translation quality comparable to the state-of-the-art preordering method that requires a manual feature design. As a future work, we plan to develop a model that jointly parses and pre- orders a source sentence. In addition, we plan to integrate preordering into the NMT model. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Preordering an English sentence "My parents live in London" with RvNN (Nodes with a horizontal line mean "Inverted").</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Distribution of Kendall's τ in the training data without preordering, preordering by BTG, and preordering by our RvNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>BLEU and RIBES scores on the test set. 
(All models are trained on the entire training cor-
pus of 1.8M sentence pairs.) Numbers in bold in-
dicate the best systems and the systems that are 
statistically insignificant at p &lt; 0.05 from the best 
systems. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Example where preordering improves translation. (Literal translations are given in the paren-
thesis under the Japanese sentences.) 

Preordered examples 
Source sentence avogadro's hypothesis (1811) contributed to the development in since then. 
BTG 
avogadro's hypothesis (1811) the then since in development to contributed . 
RvNN 
avogadro's hypothesis (1811 then since in to development the contributed). 
Translation examples by PBSMT 
Reference 
Avogadro (1811) 
(Avogadro's hypothesis (1811), since then development to contributed.) 
w/o preordering Avogadro (1811) 
(Avogadro's hypothesis (1811) development to contributed since then.) 
BTG 
Avogadro (1811) 
(Avogadro's hypothesis (1811) since then development to contributed.) 
RvNN 
Avogadro (1811 @@@@@@@@@@@@@@@@ 
(Avogadro's hypothesis (1811 since then these development to contributed.) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Slav Petrov. 2017. Natural language processing with small feed-forward networks. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2879-2885, Copenhagen, Denmark.</head><label>Slav</label><figDesc></figDesc><table>Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine trans-
lation. In Proceedings of the International Con-
ference on Computational Linguistics (COLING), 
pages 376-384, Beijing, China. 

Adrì a de Gispert, Gonzalo Iglesias, and Bill Byrne. 
2015. Fast and accurate preordering for SMT us-
ing neural networks. In Proceedings of the Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies (NAACL-HLT), pages 1012-
1017, Denver, Colorado. 

Anita Gojun and Alexander Fraser. 2012. Determin-
ing the placement of German verbs in English-to-
German SMT. In Proceedings of the Conference of 
the European Chapter of the Association for Com-
putational Linguistics (EACL), pages 726-735, Avi-
gnon, France. 

Isao Goto, Masao Utiyama, and Eiichiro Sumita. 2012. 
Post-ordering by parsing for Japanese-English sta-
tistical machine translation. In Proceedings of the 
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 311-316, Jeju Is-
land, Korea. 

Isao Goto, Masao Utiyama, and Eiichiro Sumita. 2013. 
Post-ordering by parsing with ITG for Japanese-
English statistical machine translation. ACM Trans-
actions on Asian Language Information Processing, 
12(4):17:1-17:22. 

Katsuhiko Hayashi, Katsuhito Sudoh, Hajime Tsukada, 
Jun Suzuki, and Masaaki Nagata. 2013. Shift-
reduce word reordering for machine translation. In 
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP), 
pages 1382-1386, Seattle, Washington, USA. 

Sho Hoshino, Yusuke Miyao, Katsuhito Sudoh, Kat-
suhiko Hayashi, and Masaaki Nagata. 2015. Dis-
criminative preordering meets Kendall's τ maxi-
mization. In Proceedings of the Annual Meeting 
of the Association for Computational Linguistics 
and International Joint Conference on Natural Lan-
guage Processing (ACL-IJCNLP), pages 139-144, 
Beijing, China. 

Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito 
Sudoh, and Hajime Tsukada. 2010a. Automatic 
evaluation of translation quality for distant language 
pairs. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing 
(EMNLP), pages 944-952, Cambridge, USA. 

Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and 
Kevin Duh. 2010b. Head finalization: A simple re-
ordering rule for SOV languages. In Proceedings 

of the Workshop on Statistical Machine Translation 
and MetricsMATR, pages 244-251, Uppsala, Swe-
den. 

Laura Jehl,Adrì a de Gispert, Mark Hopkins, and Bill 
Byrne. 2014. Source-side preordering for transla-
tion using logistic regression and depth-first branch-
and-bound search. In Proceedings of the Confer-
ence of the European Chapter of the Association for 
Computational Linguistics (EACL), pages 239-248, 
Gothenburg, Sweden. 

M. G. Kendall. 1938. A new measure of rank correla-
tion. Biometrika, 30(1/2):81-93. 

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A 
method for stochastic optimization. In Proceedings 
of the International Conference for Learning Repre-
sentations (ICLR), San Diego, USA. 

Philipp Koehn. 2004. Statistical significance tests 
for machine translation evaluation. In Proceedings 
of the Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), pages 388-395, 
Barcelona, Spain. 

Philipp Koehn, Amittai Axelrod, Alexandra Birch-
Mayne, Chris Callison-Burch, Miles Osborne, and 
David Talbot. 2005. Edinburgh system description 
for the 2005 IWSLT speech translation evaluation. 
In Proceedings of the International Workshop on 
Spoken Language Translation (IWSLT), pages 68-
75, Pittsburgh, USA. 

Uri Lerner and Slav Petrov. 2013. Source-side clas-
sifier preordering for machine translation. In Pro-
ceedings of the Conference on Empirical Methods 
in Natural Language Processing (EMNLP), pages 
513-523, Seattle, USA. 

Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Effective approaches to attention-based 
neural machine translation. In Proceedings of the 
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 1412-1421, Lis-
bon, Portugal. 

Masaaki Nagata, Kuniko Saito, Kazuhide Yamamoto, 
and Kazuteru Ohashi. 2006. A clustered global 
phrase reordering model for statistical machine 
translation. In Proceedings of the International 
Conference on Computational Linguistics and An-
nual Meeting of the Association for Computational 
Linguistics (COLING-ACL), pages 713-720, Syd-
ney, Australia. 

Tetsuji Nakagawa. 2015. Efficient top-down BTG 
parsing for machine translation preordering. In 
Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics and Interna-
tional Joint Conference on Natural Language Pro-
cessing (ACL-IJCNLP), pages 208-218, Beijing, 
China. </table></figure>

			<note place="foot" n="1"> In this paper, we used binary syntax trees.</note>

			<note place="foot" n="8"> http://www.statmt.org/moses/ 9 http://github.com/kpu/kenlm 10 http://opennmt.net/ 11 The learning curve behaves similarly for different λ values.</note>

			<note place="foot" n="12"> The p-value for BLEU and RIBES were 0.068 and 0.226, respectively.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was supported by NTT communica-tion science laboratories and Grant-in-Aid for Re-search Activity Start-up #17H06822, JSPS.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">A</forename><surname>Botha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<pubPlace>Anton Bakalov, Alex Salcianu, David Weiss, Ryan McDonald, and</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">ASPEC: Asian scientific paper excerpt corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiaki</forename><surname>Nakazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Yaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyotaka</forename><surname>Uchimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hitoshi</forename><surname>Isahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Language Resources and Evaluation (LREC)</title>
		<meeting>the International Conference on Language Resources and Evaluation (LREC)<address><addrLine>Portorož, Slovenia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2204" to="2208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Inducing a discriminative parser to optimize machine translation reordering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taro</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinsuke</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="843" to="853" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Minimum error rate training in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Sapporo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
	<note>Philadelphia</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Chineseto-Japanese patent machine translation based on syntactic pre-ordering for WAT 2016</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhito</forename><surname>Sudoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Asian Translation (WAT)</title>
		<meeting>the Workshop on Asian Translation (WAT)<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="211" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A unigram orientation model for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Tillmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLTNAACL)</title>
		<meeting>the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLTNAACL)<address><addrLine>Boston, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="101" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Japanese-English patent parallel corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hitoshi</forename><surname>Isahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Machine Translation Summit XI</title>
		<meeting>the Machine Translation Summit XI<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="475" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Chinese syntactic reordering for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="737" to="745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improving a statistical MT system with automatically learned rewrite patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mccord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Linguistics (COLING)</title>
		<meeting>the International Conference on Computational Linguistics (COLING)<address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="508" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Using a dependency parser to improve SMT for subject-object-verb languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeho</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ringgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technologies: Annual Conference of the North American Chapter of the Association for Computational Linguistics (HLTNAACL)</title>
		<meeting>the Human Language Technologies: Annual Conference of the North American Chapter of the Association for Computational Linguistics (HLTNAACL)<address><addrLine>Boulder, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="245" to="253" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
