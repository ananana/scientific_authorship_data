<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Accelerating Neural Transformer via an Average Attention Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Beijing Advanced Innovation Center for Language Resources</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Soochow University</orgName>
								<address>
									<postCode>215006 3</postCode>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Beijing Advanced Innovation Center for Language Resources</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Soochow University</orgName>
								<address>
									<postCode>215006 3</postCode>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<postCode>361005</postCode>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Accelerating Neural Transformer via an Average Attention Network</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1789" to="1798"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1789</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>With parallelizable attention networks, the neural Transformer is very fast to train. However, due to the auto-regressive architecture and self-attention in the decoder, the decoding procedure becomes slow. To alleviate this issue, we propose an average attention network as an alternative to the self-attention network in the decoder of the neural Transformer. The average attention network consists of two layers, with an average layer that models dependencies on previous positions and a gating layer that is stacked over the average layer to enhance the expressiveness of the proposed attention network. We apply this network on the decoder part of the neural Transformer to replace the original target-side self-attention model. With masking tricks and dynamic programming, our model enables the neural Transformer to decode sentences over four times faster than its original version with almost no loss in training time and translation performance. We conduct a series of experiments on WMT17 translation tasks, where on 6 different language pairs, we obtain robust and consistent speed-ups in decoding. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The past few years have witnessed the rapid de- velopment of neural machine translation (NMT), which translates a source sentence into the tar- get language with an encoder-attention-decoder framework <ref type="bibr" target="#b16">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2015</ref>  <ref type="figure">Figure 1</ref>: Illustration of the decoding procedure under different neural architectures. We show which previous target words are required to pre- dict the current target word y j in different NMT architectures. k indicates the filter size of the con- volution layer.</p><p>as the backbone network for translation, ranging from recurrent neural networks (RNN) <ref type="bibr" target="#b16">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b10">Luong et al., 2015)</ref>, convolutional neural networks (CNN) <ref type="bibr">(Gehring et al., 2017a,b)</ref> to full attention networks without recurrence and convolution <ref type="bibr" target="#b17">(Vaswani et al., 2017)</ref>. Particularly, the neural Transformer, relying solely on attention networks, has refreshed state-of-the-art perfor- mance on several language pairs <ref type="bibr" target="#b17">(Vaswani et al., 2017)</ref>.</p><p>Most interestingly, the neural Transformer is ca- pable of being fully parallelized at the training phase and modeling intra-/inter-dependencies of source and target sentences within a short path. The parallelization property enables training NMT very quickly, while the dependency modeling property endows the Transformer with strong abil- ity in inducing sentence semantics as well as trans- lation correspondences. However, the decoding of the Transformer cannot enjoy the speed strength of parallelization due to the auto-regressive genera- tion schema in the decoder. And the self-attention network in the decoder even further slows it.</p><p>We explain this using <ref type="figure">Figure 1</ref>, where we pro- vide a comparison to RNN-and CNN-based NMT systems. To capture dependencies from previ- ously predicted target words, the self-attention in the neural Transformer requires to calculate adap- tive attention weights on all these words ( <ref type="figure">Figure  1 (3)</ref>). By contrast, CNN only requires previous k target words <ref type="figure" target="#fig_0">(Figure 1 (2)</ref>), while RNN merely 1 ( <ref type="figure">Figure 1 (1)</ref>). Due to the auto-regressive gen- eration schema, decoding inevitably follows a se- quential manner in the Transformer. Therefore the decoding procedure cannot be parallelized. Fur- thermore, the more target words are generated, the more time the self-attention in the decoder will take to model dependencies. Therefore, preserv- ing the training efficiency of the Transformer on the one hand and accelerating its decoding on the other hand becomes a new and serious challenge.</p><p>In this paper, we propose an average attention network (AAN) to handle this challenge. We show the architecture of AAN in <ref type="figure" target="#fig_0">Figure 2</ref>, which con- sists of two layers: an average layer and gating layer. The average layer summarizes history in- formation via a cumulative average operation over previous positions. This is equivalent to a simple attention network where original adaptively com- puted attention weights are replaced with averaged weights. Upon this layer, we stack a feed forward gating layer to improve the model's expressiveness in describing its inputs.</p><p>We use AAN to replace the self-attention part of the neural Transformer's decoder. Considering the characteristic of the cumulative average op- eration, we develop a masking method to enable parallel computation just like the original self- attention network in the training. In this way, the whole AAN model can be trained totally in par- allel so that the training efficiency is ensured. As for the decoding, we can substantially accelerate it by feeding only the previous hidden state to the Transformer decoder just like RNN does. This is achieved with a dynamic programming method.</p><p>In spite of its simplicity, our model is capable of modeling complex dependencies. This is because AAN regards each previous word as an equal con- tributor to current word representation. Therefore, no matter how long the input is, our model can always build up connection signals with previous inputs, which we argue is very crucial for inducing long-range dependencies for machine translation.</p><p>We examine our model on WMT17 translation tasks. On 6 different language pairs, our model achieves a speed-up of over 4 times with almost no loss in both translation quality and training speed. In-depth analyses further demonstrate the convergency and advantages of translating long sentences of the proposed AAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>GRU ( <ref type="bibr" target="#b1">Chung et al., 2014</ref>) or LSTM (Hochreiter and Schmidhuber, 1997) RNNs are widely used for neural machine translation to deal with long- range dependencies as well as the gradient van- ishing issue. A major weakness of RNNs lies at its sequential architecture that completely disables parallel computation. To cope with this problem, <ref type="bibr" target="#b2">Gehring et al. (2017a)</ref> propose to use CNN-based encoder as an alternative to <ref type="bibr">RNN, and Gehring et al. (2017b)</ref> further develop a completely CNN- based NMT system. However, shallow CNN can only capture local dependencies. Hence, CNN- based NMT normally develops deep archictures to model long-distance dependencies. Different from these studies, <ref type="bibr" target="#b17">Vaswani et al. (2017)</ref> propose the Transformer, a neural architecture that abandons recurrence and convolution. It fully relies on at- tention networks to model translation. The prop- erties of parallelization and short dependency path significantly improve the training speed as well as model performance for the Transformer. Unfortu- nately, as we have mentioned in Section 1, it suf- fers from decoding inefficiency.</p><p>The attention mechanism is originally proposed to induce translation-relevant source information for predicting next target word in NMT. It con- tributes a lot to make NMT outperform SMT. Re- cently, a variety of efforts are made to further im- prove its accuracy and capability. <ref type="bibr" target="#b10">Luong et al. (2015)</ref> explore several attention formulations and distinguish local attention from global attention. <ref type="bibr" target="#b19">Zhang et al. (2016)</ref> treat RNN as an alternative to the attention to improve model's capability in dealing with long-range dependencies. <ref type="bibr" target="#b18">Yang et al. (2017)</ref> introduce a recurrent cycle on the atten- tion layer to enhance the model's memorization of previous translated source words. <ref type="bibr" target="#b20">Zhang et al. (2017a)</ref> observe the weak discrimination ability of the attention-generated context vectors and pro- pose a GRU-gated attention network. <ref type="bibr" target="#b7">Kim et al. (2017)</ref> further model intrinsic structures inside at- tention through graphical models. <ref type="bibr" target="#b14">Shen et al. (2017)</ref> introduce a direction structure into a self- attention network to integrate both long-range de- pendencies and temporal order information. <ref type="bibr" target="#b11">Mi et al. (2016)</ref> and <ref type="bibr" target="#b9">Liu et al. (2016)</ref> employ stan- dard word alignment to supervise the automati- cally generated attention weights. Our work also focus on the evolution of attention network, but unlike previous work, we seek to simplify the self- attention network so as to accelerate the decoding procedure. The design of our model is partially in- spired by the highway network ( <ref type="bibr" target="#b15">Srivastava et al., 2015</ref>) and the residual network ( <ref type="bibr" target="#b5">He et al., 2015)</ref>.</p><p>In the respect of speeding up the decoding of the neural Transformer, <ref type="bibr" target="#b4">Gu et al. (2018)</ref> change the auto-regressive architecture to speed up trans- lation by directly generating target words with- out relying on any previous predictions. However, compared with our work, their model achieves the improvement in decoding speed at the cost of the drop in translation quality. Our model, instead, not only achieves a remarkable gain in terms of decoding speed, but also preserves the translation performance. Developing fast and efficient atten- tion module for the Transformer, to the best of our knowledge, has never been investigated before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Average Attention Network</head><p>Given an input layer y = {y 1 , y 2 , . . . , y m }, AAN first employs a cumulative-average operation to generate context-sensitive representation for each input embedding as follows <ref type="figure" target="#fig_0">(Figure 2</ref> Average Layer):</p><formula xml:id="formula_0">g j = FFN 1 j j k=1 y k (1)</formula><p>where FFN (·) denotes the position-wise feed- forward network proposed by <ref type="bibr" target="#b17">Vaswani et al. (2017)</ref>, and both y k and g j have a dimension- ality of d. Intuitively, AAN replaces the orig- inal dynamically computed attention weights by the self-attention network in the decoder of the neural Transformer with simple and fixed aver- age weights ( 1 j ). In spite of its simplicity, the cumulative-average operation is very crucial for AAN because it builds up dependencies with pre- vious input embeddings so that the generated rep- resentations are not independent of each other. Another benefit from the cumulative-average op- eration is that no matter how long the input is, the connection strength with each previous input em- bedding is invariant, which ensures the capability of AAN in modeling long-range dependencies.</p><p>We treat g j as a contextual representation for the j-th input, and apply a feed-forward gating layer upon it as well as y j to enrich the non-linear expressiveness of AAN:</p><formula xml:id="formula_1">i j , f j = σ (W [y j ; g j ]) ˜ h j = i j y j + f j g j (2)</formula><p>where [·; ·] denotes concatenation operation, and indicates element-wise multiplication. i j and f j are the input and forget gate respectively. Via this gating layer, AAN can control how much past in- formation can be preserved from previous context g j and how much new information can be captured from current input y j . This helps our model to de- tect correlations inside input embeddings.</p><p>Following the architecture design in the neural Transformer ( <ref type="bibr" target="#b17">Vaswani et al., 2017)</ref>, we employ a residual connection between the input layer and gating layer, followed by layer normalization to stabilize the scale of both output and gradient:</p><formula xml:id="formula_2">h j = LayerNorm y j + ˜ h j (3)</formula><p>We refer to the whole procedure formulated in Eq. (1∼3) as original AAN (·) in following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Parallelization in Training</head><p>A computation bottleneck of the original AAN de- scribed above is that the cumulative-average oper- ation in Eq. <ref type="formula">(1)</ref> can only be performed sequen- tially. That is, this operation can not be paral- lelized. Fortunately, as the average is not a com- plex computation, we can use a masking trick to enable full parallelization of this operation. We show the masking trick in <ref type="figure">Figure 3</ref>, where input embeddings are directly converted into their corresponding cumulative-averaged outputs</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Complexity Sequential Operations Maximum Path Length <ref type="table">Table 1</ref>: Maximum path lengths, model complexity and minimum number of sequential operations for different models. n is the sentence length and d is the representation dimension.</p><formula xml:id="formula_3">Self-attention O n 2 · d + n · d 2 O (1) O (1) Original AAN O n · d 2 O (n) O (1) Masked AAN O n 2 · d + n · d 2 O (1) O (1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mask Matrix</head><p>Figure 3: Visualization of parallel implementa- tion for the cumulative-average operation enabled by a mask matrix. {y 1 , y 2 , y 3 , y 4 } are the input embeddings.</p><p>through a masking matrix. In this way, all the components inside AAN (·) can enjoy full par- allelization, assuring its computational efficiency. We refer to this AAN as masked AAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Analysis</head><p>In this section, we provide a thorough analysis for AAN in comparison to the original self-attention model used by <ref type="bibr" target="#b17">Vaswani et al. (2017)</ref>. Unlike our AAN, the self-attention model leverages a scaled dot-product function rather than the average oper- ation to compute attention weights:</p><formula xml:id="formula_4">Q, K, V = f (Y) Self-Attention (Q, K, V) = softmax QK T √ d V<label>(4)</label></formula><p>where Y ∈ R n×d is the input matrix, f (·) is a mapping function and Q, K, V ∈ R n×d are the corresponding queries, keys and values. Follow- ing <ref type="bibr" target="#b17">Vaswani et al. (2017)</ref>, we compare both mod- els in terms of computational complexity, min- imum number of sequential operations required and maximum path length that a dependency sig- nal between any two positions has to traverse in the network. <ref type="table">Table 1</ref> summarizes the comparison results.</p><p>Our AAN has a maximum path length of O (1), because it can directly capture dependencies be- tween any two input embeddings. For the original AAN, the nature of its sequential computation en- larges its minimum number sequential operations to O (n). However, due to its lack of position- wise masked projection, it only consumes a com- putational complexity of O n · d 2 . By contrast, both self-attention and masked AAN have a com- putational complexity of O n 2 · d + n · d 2 , and require only O (1) sequential operation. Theoreti- cally, our masked AAN performs very similarly to the self-attention according to <ref type="table">Table 1</ref>. We there- fore use the masked version of AAN during train- ing throughout all our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Decoding Acceleration</head><p>Differing noticeably from the self-attention in the Transformer, our AAN can be accelerated in the decoding phase via dynamic programming thanks to the simple average calculation. Particularly, we can decompose Eq. (1) into the following two steps:</p><formula xml:id="formula_5">˜ g j = ˜ g j−1 + y j<label>(5)</label></formula><formula xml:id="formula_6">g j = FFN˜g FFN˜ FFN˜g j j<label>(6)</label></formula><p>where˜gwhere˜ where˜g 0 = 0. In doing so, our model can com- pute the j-th input representation based on only one previous state˜gstate˜ state˜g j−1 , instead of relying on all previous states as the self-attention does. In this way, our model can be substantially accelerated during the decoding phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Neural Transformer with AAN</head><p>The neural Transformer models translation through an encoder-decoder framework, with each layer involving an attention network fol- lowed by a feed forward network ( <ref type="bibr" target="#b17">Vaswani et al., 2017)</ref>. We apply our masked AAN to replace the self-attention network in its decoder part, and illustrate the overall architecture in <ref type="figure" target="#fig_1">Figure 4</ref>. Given a source sentence x = {x 1 , x 2 , . . . , x n }, the Transformer leverages its encoder to induce source-side semantics and dependencies so as to enable its decoder to recover the encoded informa- tion in a target language. The encoder is composed of a stack of N = 6 identical layers, each of which has two sub-layers:</p><formula xml:id="formula_7">˜ h l = LayerNorm h l−1 + MHAtt h l−1 , h l−1 h l = LayerNorm˜h LayerNorm˜ LayerNorm˜h l + FFN˜h FFN˜ FFN˜h l<label>(7)</label></formula><p>where the superscript l indicates layer depth, and MHAtt denotes the multi-head attention mecha- nism proposed by <ref type="bibr" target="#b17">Vaswani et al. (2017)</ref>. Based on the encoded source representation h N , the Transformer relies on its decoder to generate corresponding target translation y = {y 1 , y 2 , . . . , y m }. Similar to the encoder, the de- coder also consists of a stack of N = 6 identical layers. For each layer in our architecture, the first sub-layer is our proposed average attention net- work, aiming at capturing target-side dependen- cies with previous predicted words:</p><formula xml:id="formula_8">˜ s l = AAN s l−1<label>(8)</label></formula><p>Carrying these dependencies, the decoder stacks another two sub-layers to seek translation-relevant source semantics for bridging the gap between the source and target language:</p><formula xml:id="formula_9">s l c = LayerNorm˜s LayerNorm˜LayerNorm˜s l + MHAtt˜s MHAtt˜MHAtt˜s l , h N s l = LayerNorm s l c + FFN s l c<label>(9)</label></formula><p>We use subscript c to denote the source-informed target representation. Upon the top layer of this decoder, translation is performed where a linear transformation and softmax activation are applied to compute the probability of the next token based on s N To memorize position information, the Trans- former augments its input layer h 0 = x, s 0 = y with frequency-based positional encodings. The whole model is a large, single neural network, and can be trained on a large-scale bilingual corpus with a maximum likelihood objective. We refer readers to ( <ref type="bibr" target="#b17">Vaswani et al., 2017</ref>) for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">WMT14 English-German Translation</head><p>We examine various aspects of our AAN on this translation task. The training data consist of 4.5M sentence pairs, involving about 116M En- glish words and 110M German words. We used newstest2013 as the development set for model se- lection, and newstest2014 as the test set. We eval- uated translation quality via case-sensitive BLEU metric ( <ref type="bibr" target="#b12">Papineni et al., 2002</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Model Settings</head><p>We applied byte pair encoding algorithm <ref type="bibr" target="#b13">(Sennrich et al., 2016)</ref> to encode all sentences and limited the vocabulary size to 32K. All out-of- vocabulary words were mapped to an unique to- ken "unk". We set the dimensionality d of all in- put and output layers to 512, and that of inner- FFN layer to 2048. We employed 8 parallel at- tention heads in both encoder and decoder lay- ers. We batched sentence pairs together so that they were approximately of the same length, and each batch had roughly 25000 source and target tokens. During training, we used label smoothing with value ls = 0.1, attention dropout and resid- ual dropout with a rate of p = 0.1. During decod- ing, we employed beam search algorithm and set the beam size to 4. Adam optimizer ( <ref type="bibr" target="#b8">Kingma and Ba, 2015</ref>) with β 1 = 0.9, β 2 = 0.98 and = 10 −9 was used to tune model parameters, and the learn- ing rate was varied under a warm-up strategy with warmup steps = 4000 ( <ref type="bibr" target="#b17">Vaswani et al., 2017</ref>  <ref type="table" target="#tab_2">Table 2</ref>: Case-sensitive tokenized BLEU score on WMT14 English-German translation. BLEU scores are calculated using multi-bleu.perl.</p><p>The maximum number of training steps was set to 100K. Weights of target-side embedding and out- put weight matrix were tied for all models. We implemented our model with masking tricks based on the open-sourced thumt (Zhang et al., 2017b) 2 , and trained and evaluated all models on a single NVIDIA GeForce GTX 1080 GPU. For evalua- tion, we averaged last five models saved with an interval of 1500 training steps. We also show an ablation study in terms of the FFN(·) network in Eq. (1) and the gating layer in Eq. (2). <ref type="table" target="#tab_2">Table 2</ref> shows that without the FFN net- work, the performance of our model drops 0.26 BLEU points. This degeneration is enlarged to 0.40 BLEU points when the gating layer is not available. In order to reach comparable perfor- mance with the original Transformer, integrating both components is desired.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Translation Performance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Analysis on Convergency</head><p>Different neural architectures might require differ- ent number of training steps to converge. In this section, we testify whether our AAN would re- veal different characteristics with respect to con- vergency. We show the loss curve of both the Transformer and our model in <ref type="figure" target="#fig_2">Figure 5</ref>. Surprisingly, both model show highly similar tendency, and successfully converge in the end. To train a high-quality translation system, our model consumes almost the same number of training steps as the Transformer. This strongly suggests   <ref type="table">Table 3</ref>: Time required for training and decod- ing. Training denotes the number of global train- ing steps processed per second; Decoding indi- cates the amount of time in seconds required for translating one sentence, which is averaged over the whole newstest2014 dataset. r shows the ra- tio between the Transformer and our model. that replacing the self-attention network with our AAN does not have negative impact on the con- vergency of the entire model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">Analysis on Speed</head><p>In Section 3, we demonstrate in theory that our AAN is as efficient as the self-attention during training, but can be substantially accelerated dur- ing decoding. In this section, we provide quantita- tive evidences to examine this point. We show the training and decoding speed of both the Transformer and our model in <ref type="table">Table  3</ref>. During training, our model performs approx- imately 0.2464 training steps per second, while the Transformer processes around 0.2474. This indicates that our model shares similar computa- tional strengths with the Transformer during train- ing, which resonates with the computational anal- ysis in Section 3.</p><p>When it comes to decoding procedure, the time of our model required to translate one sentence Sentence Length is only a quarter of that of the Transformer, with beam size ranging from 4 to 20. Another notice- able feature is that as the beam size increases, the ratio of required decoding time between the Trans- former and our model is consistently enlarged. This demonstrates empirically that our model, en- hanced with the dynamic decoding acceleration al- gorithm (Section 3.3), can significantly improve the decoding speed of the Transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.5">Effects on Sentence Length</head><p>A serious common challenge for NMT is to translate long source sentences as handling long- distance dependencies and under-translation is- sues becomes more difficult for longer sentences. Our proposed AAN uses simple cumulative- average operations to deal with long-range depen- dencies. We want to examine the effectiveness of these operations on long sentence translation. For this, we provide the translation results along sen- tence length in <ref type="figure" target="#fig_4">Figure 6</ref>.</p><p>We find that both the Transformer and our model generate very similar translations in terms of BLEU score and translation length, and obtain rather promising performance on long source sen- tences. More specifically, our model yields rel- atively shorter translation length on the longest source sentences but significantly better transla- tion quality. This suggests that in spite of the sim- plicity of the cumulative-average operations, our AAN can indeed capture long-range dependences desired for translating long source sentences.</p><p>Generally, the decoder takes more time for translating longer sentences. When it comes to the Transformer, this time issue of translating long sentences becomes notably severe as all previous predicted words must be included for estimating both self-attention weights and word prediction. We show the average time required for translat- ing a source sentence with respect to its sentence length in <ref type="figure" target="#fig_5">Figure 7</ref>. Obviously, the decoding time of the Transformer grows dramatically with the in- crease of sentence length, while that of our model rises rather slowly. We contribute this great decod- ing advantage of our model over the Transformer to the average attention architecture which enables  our model to perform next-word prediction by cal- culating information just from the previous hid- den state, rather than considering all previous in- puts like the self-attention in the Transformer's de- coder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">WMT17 Translation Tasks</head><p>We further demonstrate the effectiveness of our model on six WMT17 translation tasks in both di- rections (12 translation directions in total). These tasks contain the following language pairs:</p><p>• En-De: The English-German language pair. This training corpus consists of 5.85M sen- tence pairs, with 141M English words and 135M German words. We used the concate- nation of newstest2014, newstest2015 and newstest2016 as the development set, and the newstest2017 as the test set.</p><p>• En-Fi: The English-Finnish language pair. This training corpus consists of 2.63M sen- tence pairs, with 63M English words and 45M Finnish words. We used the concate- nation of newstest2015, newsdev2015, new- stest2016 and newstestB2016 as the develop- ment set, and the newstest2017 as the test set.</p><p>• En-Lv: The English-Latvian language pair. This training corpus consists of 4.46M sen- tence pairs, with 63M English words and 52M Latvian words. We used the news- dev2017 as the development set, and the new- stest2017 as the test set.</p><p>• En-Ru: The English-Russian language pair. This training corpus consists of 25M sen- tence pairs, with 601M English words and 567M Russian words. We used the concate- nation of newstest2014, newstest2015 and newstest2016 as the development set, and the newstest2017 as the test set.</p><p>• En-Tr: The English-Turkish language pair. This training corpus consists of 0.21M sen- tence pairs, with 5.2M English words and 4.6M Turkish words. We used the concate- nation of newsdev2016 and newstest2016 as the development set, and newstest2017 as the test set.</p><p>• En-Cs: The English-Czech language pair. This training corpus consists of 52M sen- tence pairs, with 674M English words and 571M Czech words. We used the concatena- tion of newstest2014, newstest2015 and new- stest2016 as the development set, and the newstest2017 as the test set.</p><p>Interestingly, these translation tasks involves train- ing corpora with different scales (ranging from 0.21M to 52M sentence pairs). This help us thor- oughly examine the ability of our model on differ- ent sizes of training data. All these preprocessed datasets are publicly available, and can be down- loaded from WMT17 official website. <ref type="bibr">3</ref> We used the same modeling settings as in the WMT14 English-German translation task except for the number of training steps for En-Fi and En- Tr, which we set to 60K and 10K respectively. In addition, to compare with official results, we reported both case-sensitive and case-insensitive detokenized BLEU scores.  <ref type="table">Table 5</ref>: Average seconds required for decoding one source sentence on WMT17 translation tasks. <ref type="table" target="#tab_5">Table 4</ref> shows the overall results on 12 transla- tion directions. We also provide the results from WMT17 winning systems 4 . Notice that unlike the Transformer and our model, these winner systems typically use model ensemble, system combina- tion and large-scale monolingual corpus. Although different languages have different lin- guistic and syntactic structures, our model con- sistently yields rather competitive results against the Transformer on all language pairs in both di- rections. Particularly, on the De→En translation task, our model achieves a slight improvement of 0.10/0.07 case-sensitive/case-insensitive BLEU points over the Transformer. The largest perfor- mance gap between our model and the Trans- former occurs on the En→Tr translation task, where our model is lower than the Transformer by 0.52/0.53 case-sensitive/case-insensitive BLEU points. We conjecture that this difference may be due to the small training corpus of the En-Tr task. In all, these results suggest that our AAN is able to perform comparably to Transformer on differ- ent language pairs with different scales of training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Translation Results</head><p>We also show the decoding speed of both the Transformer and our model in <ref type="table">Table 5</ref>. On all lan- guages in both directions, our model yields signif- icant and consistent improvements over the Trans- former in terms of decoding speed. Our model decodes more than 4 times faster than the Trans- former. Surprisingly, our model just consumes 0.02968 seconds to translate one source sentence on the En→Tr language pair, only a seventh of the decoding time of the Transformer. These re- sults show that the benefit of decoding accelera-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this paper, we have described the average at- tention network that considerably alleviates the decoding bottleneck of the neural Transformer. Our model employs a cumulative average oper- ation to capture important contextual clues from previous target words, and a feed forward gat- ing layer to enrich the expressiveness of learned hidden representations. The model is further en- hanced with a masking trick and a dynamic pro- gramming method to accelerate the Transformer's decoder. Extensive experiments on one WMT14 and six WMT17 language pairs demonstrate that the proposed average attention network is able to speed up the Transformer's decoder by over 4 times.</p><p>In the future, we plan to apply our model on other sequence to sequence learning tasks. We will also attempt to improve our model to enhance its modeling ability so as to consistently outperform the original neural Transformer.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Visualization of the proposed model. For clarity, we show an example with only four words.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The new Transformer architecture with the proposed average attention network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Convergence visualization. The horizontal axis denotes training steps scaled by 10 2 , and the vertical axis indicates training loss. Roughly, our model converges similarly to the Transformer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Translation statistics on WMT14 English-German test set (newstest14) with respect to the length of source sentences. The top figure shows tokenized BLEU score, and the bottom one shows the average length of translations, both visa-vis sentence length</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Average time required for translating one source sentence vs. the length of the source sentence. With the increase of sentence length, our model shows more clear and significant advantage over the Transformer in terms of the decoding speed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>).</figDesc><table>Model 
BLEU 
Transformer 
26.37 
Our Model 
26.31 
Our Model w/o FFN 
26.05 
Our Model w/o Gate 
25.91 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 reports</head><label>2</label><figDesc>the translation results. On the same dataset, the Transformer yields a BLEU score of 26.37, while our model achieves 26.31. Both re- sults are almost the same with no significant dif- ference. Clearly, our model is capable of capturing complex translation correspondences so as to gen- erate high-quality translations as effective as the Transformer.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Detokenized BLEU scores for WMT17 translation tasks. Results are reported with multi-bleu-
detok.perl. "winner" denotes the translation results generated by the WMT17 winning systems. d 
indicates the difference between our model and the Transformer. 

</table></figure>

			<note place="foot" n="3"> http://data.statmt.org/wmt17/translationtask/preprocessed/</note>

			<note place="foot" n="4"> http://matrix.statmt.org/matrix tion from the proposed average attention structure is language-invariant, and can be easily adapted to other translation tasks.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A convolutional encoder model for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="123" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Non-autoregressive neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luong</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno>abs/1702.00887</idno>
		<title level="m">Structured attention networks. Proc. of ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural machine translation with supervised attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lemao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Finch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The COLING 2016 Organizing Committee</title>
		<meeting><address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3093" to="3102" />
		</imprint>
	</monogr>
	<note>Proc. of COLING 2016</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Supervised attentions for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Ittycheriah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2283" to="2288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Disan: Directional self-attention network for rnn/cnn-free language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/1709.04696</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh Kumar</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno>abs/1505.00387</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural machine translation with recurrent attention modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EACL</title>
		<meeting>of EACL<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="383" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Recurrent neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<idno>abs/1607.08725</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A gru-gated attention model for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<idno>abs/1704.08430</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">THUMT: an open source toolkit for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhuo</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan-Bo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/1706.06415</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
