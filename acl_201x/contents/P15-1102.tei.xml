<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Co-training for Semi-supervised Sentiment Classification Based on Dual-view Bags-of-words Representation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Xia</surname></persName>
							<email>rxia@njust.edu.cn, wangcheng1022@gmail.com, daixinyu@nju.edu.cn, taoli@cs.fiu.edu</email>
							<affiliation key="aff1">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Dai</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Florida International University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Nanjing University of Posts &amp; Telecommunications</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Nanjing University of Science &amp; Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Co-training for Semi-supervised Sentiment Classification Based on Dual-view Bags-of-words Representation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1054" to="1063"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>A review text is normally represented as a bag-of-words (BOW) in sentiment classification. Such a simplified BOW model has fundamental deficiencies in modeling some complex linguistic phenomena such as negation. In this work, we propose a dual-view co-training algorithm based on dual-view BOW representation for semi-supervised sentiment classification. In dual-view BOW, we automatically construct antonymous reviews and model a review text by a pair of bags-of-words with opposite views. We make use of the original and antonymous views in pairs, in the training, bootstrapping and testing process, all based on a joint observation of two views. The experimental results demonstrate the advantages of our approach , in meeting the two co-training requirements , addressing the negation problem , and enhancing the semi-supervised sentiment classification efficiency.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the past decade, there has been an explosion of user-generated subjective texts on the Internet in forms of online reviews, blogs and microblogs. With the need of automatically identifying senti- ments and opinions from those online texts, senti- ment classification has attracted much attention in the field of natural language processing.</p><p>Lots of previous research focused on the task of supervised sentiment classification. However, in some domains, it is hard to obtain a sufficient amount of labeled training data. Manual annota- tion is also very expensive and time-consuming. To address this problem, semi-supervised learning approaches were employed in sentiment classifica- tion, to reduce the need for labeled reviews by tak- ing advantage of unlabeled reviews.</p><p>The dominating text representation method in both supervised and semi-supervised sentiment classification is known as the bag-of-words (BOW) model, which is difficult to meet the requirements for understanding the review text and dealing with complex linguistic structures such as negation. For example, the BOW representations of two opposite reviews "It works well" and "It doesn't work well" are considered to be very similar by most statistical learning algorithms.</p><p>In supervised sentiment classification, many ap- proaches have been proposed in addressing the negation problem ( <ref type="bibr" target="#b20">Pang et al., 2002;</ref><ref type="bibr" target="#b18">Na et al., 2004;</ref><ref type="bibr" target="#b21">Polanyi and Zaenen , 2004;</ref><ref type="bibr" target="#b9">Kennedy and Inkpen, 2006;</ref><ref type="bibr" target="#b7">Ikeda et al., 2008;</ref><ref type="bibr" target="#b13">Li et al., 2010b;</ref><ref type="bibr" target="#b19">Orimaye et al., 2012;</ref><ref type="bibr" target="#b27">Xia et al., 2013)</ref>. Nev- ertheless, in semi-supervised sentiment classifica- tion, most of the current approaches directly ap- ply standard semi-supervised learning algorithms, without paying attention to appropriate representa- tion for review texts. For example, Aue and Ga- mon (2005) applied the na¨ıvena¨ıve Bayes EM algorithm ( <ref type="bibr" target="#b11">Nigam et al., 2000</ref>). <ref type="bibr" target="#b5">Goldberg and Zhu (2006)</ref> ap- plied a graph-based semi-supervised learning algo- rithm by ( <ref type="bibr" target="#b31">Zhu et al., 2003)</ref>. <ref type="bibr" target="#b26">Wan (2009)</ref> employed a co-training approach for cross-language senti- ment classification. <ref type="bibr" target="#b12">Li et al. (2010a)</ref> employed co- training with personal and impersonal views. <ref type="bibr" target="#b22">Ren et al. (2011)</ref> explored the use of label propagation ( <ref type="bibr" target="#b30">Zhu and Ghahramani, 2002)</ref>.</p><p>As pointed by <ref type="bibr" target="#b5">(Goldberg and Zhu, 2006</ref>): it is necessary to investigate better review text represen- tations and similarity measures based on linguis- tic knowledge, as well as reviews' sentiment pat- terns. However, to the best knowledge, such inves- tigations are very scarce in the research of semi-supervised sentiment classification.</p><p>In <ref type="table">(Xia et al., 2013)</ref>, we have developed a dual sentiment analysis approach, which creates antonymous reviews and makes use of original and antonymous reviews together for supervised sen- timent classification. In this work, we propose a dual-view co-training approach based on dual- view BOW representation for semi-supervised sen- timent classification. Specifically, we model both the original and antonymous reviews by a pair of bags-of-words with opposite views. Based on such a dual-view representation, we design a dual-view co-training approach. The training, bootstrapping and testing processes are all performed by observ- ing two opposite sides of one review. That is, we consider not only how positive/negative the orig- inal review is, but also how negative/positive the antonymous review is.</p><p>In comparison with traditional methods, our dual-view co-training approach has the following advantages:</p><p>• Effectively address the negation problem;</p><p>• Automatically learn the associations among antonyms; • Better meet the two co-training requirements in <ref type="bibr" target="#b2">(Blum and Mitchell, 1998</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The mainstream of the research in sentiment clas- sification focused on supervised and unsupervised learning tasks. In comparison, semi-supervised sentiment classification has much less related stud- ies. In this section, we focus on reviewing the work of semi-supervised sentiment classification. Aue and <ref type="bibr" target="#b0">Gamon (2005)</ref> combined a small amount of labeled data with a large amount of unlabeled data in target domain for cross-domain sentiment classification based on the EM algo- rithm. <ref type="bibr" target="#b5">Goldberg and Zhu (2006)</ref> presented a graph- based semi-supervised learning algorithm ( <ref type="bibr" target="#b31">Zhu et al., 2003</ref>) for the sentiment analysis task of rat- ing inference. <ref type="bibr" target="#b4">Dasgupta and Ng (2009)</ref> proposed a semi-supervised approach to mine the unambigu- ous reviews at first and then exploiting them to classify the ambiguous reviews, via a combination of active learning, transductive learning and en- semble learning. <ref type="bibr" target="#b22">Ren et al. (2011)</ref> explored the use of label propagation (LP) ( <ref type="bibr" target="#b30">Zhu and Ghahramani, 2002</ref>) in building a semi-supervised senti- ment classifier, and compared their results with Transductive SVMs(T-SVM). LP and T-SVM are transductive learning methods where the test data should participate in the training process. <ref type="bibr" target="#b29">Zhou et al. (2010)</ref> proposed a deep learning approach called active deep networks to address semi-supervised sentiment classification with ac- tive learning. <ref type="bibr" target="#b24">Socher et al. (2012)</ref> introduced a deep learning framework called semi-supervised recursive autoencoders for predicting sentence- level sentiment distributions. The limitation of deep learning approaches might be their depen- dence on a considerable amount of unlabeled data to learn the representations and the inability to ex- plicitly model the negation problem.</p><p>One line of semi-supervised learning research is to bootstrap class labels using techniques like self-training, co-training and their variations. <ref type="bibr" target="#b26">Wan (2009)</ref> proposed a co-training approach to address the cross-lingual sentiment classification problem. They made use of the machine translation service to produce two views (a English view and a Chi- nese view) for co-training a Chinese review senti- ment classifier, based on English corpus and unla- beled Chinese corpus. <ref type="bibr" target="#b12">Li et al. (2010a)</ref> proposed an unsupervised method at first to automatically separate the review text into a personal view and an impersonal view, based on which the standard co- training algorithm is then applied to build a semi- supervised sentiment classifier. <ref type="bibr" target="#b14">Li et al. (2011)</ref> further studied semi-supervised learning for imbal- anced sentiment classification by using a dynamic co-training approach. <ref type="bibr" target="#b25">Su et al. (2012)</ref> proposed a multi-view learning approach to semi-supervised sentiment classification with both feature partition and language translation strategies <ref type="bibr" target="#b26">(Wan , 2009)</ref>. Following ( <ref type="bibr" target="#b12">Li et al., 2010a)</ref>, <ref type="bibr" target="#b15">Li (2013)</ref> proposed a co-training approach which exploits subjective and objective views for semi-supervised sentiment classification. Our approach can also be viewed as a variation of co-training. The innovation of our approach is the dual-view construction technique by incorporating antonymous reviews and the boot- strapping mechanism by observing two opposite sides of one review. Original Review: "The app doesn't work well on my phone. Disappointing. Don't recommend it."</p><p>Antonymous Review: "The app works well on my phone. Satisfactory. Recom- mend it."</p><p>Given an original review, its antonymous review is automatically created as follows 1 : 1) We first de- tect the negations in each subsentence of the review text; 2) If there is a negation, we remove negators in that subsentence; 3) Otherwise, we reverse all the sentiment words in the subsentence into their antonyms, according to a pre-defined antonym dic- tionary 2 .</p><p>We subsequently use a dual-view BOW model to represent such a pair of reviews, as shown in Fig- ure 1. The original and antonymous reviews will be used in pairs in our dual-view semi-supervised learning approach. As we determine the sentiment of one review, we could observe not only the orig- inal view, but also the antonymous view.  <ref type="figure">Figure 2</ref>: The process of dual-view co-training. Again, the white font color and black background are used to denote the antonymous view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Antonymous View Antonymous View Original View</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Antonymous</head><p>It is important to notice that the antony- mous view removes all negations and incorporates antonymous features. On this basis, we design a dual-view co-training approach. We will introduce our approach in detail in Section 3.2, and analyze its potential advantages in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Dual-view Co-training Approach</head><p>Since the original and antonymous views form two different views of one review text, it is natural to employ the co-training algorithm, which requires two views for semi-supervised classification.</p><p>Co-training is a typical bootstrapping algorithm that first learns a separate classifier for each view using the labeled data. The most confident predic- tions of each classifier on the unlabeled data are then used to construct additional labeled training data iteratively. Co-training has been extensively used in NLP, including statistical parsing <ref type="bibr" target="#b23">(Sarkar , 2001)</ref>, reference resolution ( <ref type="bibr">Ng and Cardie, 2003)</ref>, part-of-speech tagging ( <ref type="bibr" target="#b3">Clark et al., 2003)</ref>, word sense disambiguation <ref type="bibr" target="#b17">(Mihalcea, 2004)</ref>, and senti- ment classification <ref type="bibr" target="#b26">(Wan , 2009;</ref><ref type="bibr" target="#b12">Li et al., 2010a)</ref>.</p><p>But it should be noted that the dual views in our approach are different from traditional views. One important property of our approach is that two views are opposite and therefore associated with opposite class labels. <ref type="figure">Figure 2</ref> illustrates the process of dual-view co-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(1) Dual-view training</head><p>For each instance in the initial labeled set, we con- struct the dual-view representations. Let x l o and x l a denote the bags of words in the original view and the antonymous view, respectively. Note that the class labels in two views are kept opposite:</p><formula xml:id="formula_0">y l a = 1 − y l o (y ∈ {0, 1})</formula><p>. That is, we reverse the class label in the original view (i.e., positive to negative, or vice versa), as the class label of the created antonymous view.</p><p>Suppose L is the labeled set, with L o and L a denoting the original-view and antonymous-view labeled sets, respectively. We train two distinct classifiers: the original-view classifier h o and the antonymous-view classifier h a , based on L o and L a , respectively. We further train a joint classifier by using L o and L a together as the training data, and refer to it as h d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(2) Dual-view bootstrapping</head><p>In standard co-training, we allow each classifier to examine the unlabeled set U and select the most confidently predicted examples in each category. The selected examples are then added into L , along with the predicted class labels.</p><p>In this work, we design a dual-view co-training algorithm to bootstrap the class labels by a joint ob- servation of two sides of one review. Specifically, we propose a new bootstrapping mechanism, based on a principle called dual-view sentiment consen- sus. Given an unlabeled instance {x u o , x u a }, dual view sentiment consensus requires that, the orig- inal prediction y u o and the antonymous prediction should be opposite: y u a = 1 − y u o . In other words, we only select the instances of which the original prediction is positive/negative, and the same time the antonymous prediction is negative/positive. To increase the degree of sentiment consensus, we fur- ther require that the predition y u d of h d should be the same as y u o . We sort all unlabeled instances according to the dual-view predictions in each class, filter the list according to the dual-view sentiment consensus principle, and add the top-ranked s instances in each class to the labeled set. For each selected un- labeled instance, its original view x u o is added into L o with class label y u o ; and the antonymous view x u a is added into L a , with an opposite class label</p><formula xml:id="formula_1">y u a = 1 − y u o .</formula><p>When L o and L a receive the supple- mental labeled instances, we update h o and h a .</p><p>Our bootstrapping mechanism differs from the traditional methods in two major aspects: First, in traditional co-training, given the same instance, the class labels in two views are the same. But in our approach, the class labels in two views need to be opposite. Second, in traditional co-training, the most confidently predicted examples in each view are selected to extend the amount of labeled data. It is dangerous to believe the confident but incorrect predictions. While in our approach, the candidates are further filtered by the principle of dual-view sentiment consensus. In this way, the labeling accuracy and learning efficiency can be improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(3) Dual-view testing</head><p>Finally, in the testing stage, standard co-training uses a joint set of features in two views to train the classifier. In dual-view testing, we use h o and h a to predict the test example in two views, and make the final prediction by considering both sizes of the review.</p><p>Given a test example x te with its original view denoted by x te o and antonymous view denoted by x te a , let p o (·|x te o ) be the posterior probability predicted by the original-view classifier h o , and p a (·|x te a ) be the posterior probability predicted by h a . The dual-view testing process can be formu- lated as follows:</p><formula xml:id="formula_2">p(+|x te ) = p(+|x te o , x te a ) = p o (+|x te o ) + p a (−|x te a ) 2 ; p(−|x te ) = p(−|x te o , x te a ) = p o (−|x te o ) + p a (+|x te a ) 2 .</formula><p>That is, the final positive score is assigned by measuring not only how positive the original re- view is, but also how negative the antonymous one is; the negative score is assigned by measuring not only how positive the original review is, but also how negative the antonymous one is.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Advantages of Dual-view Co-training</head><p>Our proposed dual-view co-training approach has the following three advantages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(1) Effectively address the negation issue</head><p>We use the antonymous review as a view to effec- tively address the negation issue. Let us revisit the example in Section 3.1 and assume that the orig- inal review (i.e., "The app doesn't work well on my phone. Disappointing. Do not recommend it.") is an unlabeled sample. Because the traditional BOW model cannot well represent negative struc- tures, the review is likely to be incorrectly labeled as positive and then added into the labeled set.</p><p>In our proposed approach, the antonymous re- view (i.e., "The app works well on my phone. Sat- isfactory. Recommend it.") removed all the neg- ative structures, and is thus more suited for the BOW representation. In this example, the antony- mous review is also likely to be marked as positive. Hence, in this case, both the original review and its antonymous review will be labeled as positive, which violates the principle of dual-view sentiment consensus as mentioned in Section 3.2. As a result, the unlabeled instance will not be added into the labeled set.</p><p>Therefore, our approach can overcome the limi- tations of the conventional methods in addressing the negation issue and reduce the labeling error rate (caused by the negative structures) during the bootstrapping process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(2) Automatically learn the associations among antonyms</head><p>In semi-supervised sentiment classification, only limited association information between the words and categories can be obtained from a small num- ber of initial labeled data.</p><p>For instance, in the above example "disappoint- ing" and "satisfactory" are a pair of antonyms. From the initial labeled data, we may only learn that "disappointing" is derogatory, but we cannot infer that "satisfactory" is commendatory.</p><p>During the bootstrapping process in our approach, when constructing the dual view rep- resentation, the original view and its antonymous view are required to have opposite class labels. Hence we can automatically infer the relationship between "satisfactory" and "disappointing" (e.g., one is positive and one is negative), thereby improving the learning efficiency of the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(3) Better meet two co-training requirements</head><p>Compared with traditional methods, our dual-view co-training can better meet the two co-training re- quirements: 1) sufficient condition (i.e., each view is sufficient for classification); 2) complementary condition (i.e., the two views are conditionally in- dependent).</p><p>First, for the sufficient condition, we use a dif- ferent view construction method. Most traditional methods construct the two views by feature parti- tioning (i.e., dividing the original feature set into two subsets), while we use data expansion by gen- erating antonymous reviews. We will demonstrate in the experimental section (Section 4.6), that our data expansion method can construct better views than the feature partition method in terms of pre- dicting the class labels from individual views.</p><p>Second, as we know, every coin has two sides and the two sides are often complementary. In our proposed approach, the original review and its antonymous review (i.e., two sides of one review) are used as two views for co-training and they can better meet the complementary condition. We will illustrate this point in Section 4.6 by calculating the KL divergence between the two views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Experimental Settings</head><p>We conduct the experiments on the multi-domain sentiment datasets, which were introduced in ( <ref type="bibr" target="#b1">Blitzer et al., 2007)</ref> and have been widely used in sentiment classification. It consists of four domains (Book, DVD, Electronics, and Kitchen) of reviews extracted from Amazon.com. Each of the four datasets contains 1,000 positive and 1,000 negative reviews. Following the experimental settings used in ( <ref type="bibr" target="#b12">Li et al., 2010a</ref>), we randomly separate all the reviews in each class into a labeled data set, a un- labeled data set, and a test set, with a proportion of 10%, 70% and 20%, respectively. We report the av- eraged results of 10-fold cross-validation in terms of classification accuracy.</p><p>Note that our approach is a general framework that allows different classification algorithms. Due to the space limitation, we only report the results by using logistic regression 3 . Note the similar conclu- sions can be obtained by using the other algorithms such as SVMs and na¨ıvena¨ıve Bayes. The LibLinear toolkit 4 is utilized, with a dual L2-regularized fac- tor, and a default tradeoff parameter c. Similar to <ref type="bibr" target="#b26">(Wan , 2009;</ref><ref type="bibr" target="#b12">Li et al., 2010a</ref>), we carry out the ex- periments with the unigram features without fea- ture selection. Presence is used as the term weight- ing scheme as it was reported in ( <ref type="bibr" target="#b20">Pang et al., 2002</ref>) that it performed better than TF and TF-IDF. Fi- nally, the paired t-test <ref type="bibr" target="#b28">(Yang and Liu , 1999</ref>) is per- formed to test the significance of the difference be-BOOK DVD ELEC KITC Avg. Baseline 0.680 0.691 0.726 0.740 0.709 LP 0.681 0.676 0.697 0.722 0.694 T-SVM 0.671 0.677 0.716 0.729 0.698 EM 0.702 0.706 0.758 0.744 0.728 Self-Training 0.689 0.705 0.736 0.751 0.720 Self-Reserved 0.690 0.708 0.735 0.754 0.722</p><p>Co-Static 0.696 0.714 0.745 0.762 0.729 Co-Dynamic 0.701 0.725 0.756 0.767 0.737</p><p>Co-PI 0.702 0.716 0.746 0.769 0.733 Our approach 0.721 0.738 0.769 0.780 0.752 <ref type="table">Table 1</ref>: The semi-supervised classification accu- racy of ten systems.</p><p>tween two systems, with a default significant level of 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Compared Systems</head><p>We implement the following nine systems and compare them with our approach:</p><p>• Baseline, the supervised baseline trained with the initial labeled data only; • Expectation Maximization (EM), with the na¨ıvena¨ıve Bayes model proposed by <ref type="bibr" target="#b11">Nigam et al. (2000)</ref>; • Label Propagation (LP), a graph-based semi-supervised learning method proposed by <ref type="bibr" target="#b30">Zhu and Ghahramani (2002)</ref>; • Transductive SVM (T-SVM), an extension of SVM so that it can exploit unlabeled data in semi-supervised learning ( Joachims, 1999); • Self-Training, a bootstrapping model that first trains a classifier, uses it to classify the unlabeled data, and adds the most confident data to the labeled set; • Self-Reserved, a variation of self-training proposed in ( <ref type="bibr" target="#b16">Liu et al., 2013)</ref>,with a reserved procedure to incorporate some less confident examples; • Co-Static, the co-training algorithm by using two static partitions of feature set as two views <ref type="bibr" target="#b2">(Blum and Mitchell, 1998</ref>); and impersonal views for co-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Performance Comparison</head><p>In table 1, we report the semi-supervised classifica- tion accuracy of ten evaluated systems. We report the results with 200 labeled, 1400 unlabeled and 400 test reviews. Note that the similar conclusions can be obtained when the size of the initial labeled data changes. We will discuss its influence later. As can be seen, trained with only 200 labeled data, the supervised baseline yields an average ac- curacy of 0.709. Self-training gains an improve- ment of 1.1%. Self-reserved does not show sig- nificant priority against Self-training. Three co- training systems (Co-static, Co-dynamic and Co- PI) get significant improvements. They increase the supervised baseline by 2.0%, 2.8% and 2.4%, respectively.</p><p>It is somehow surprising that T-SVM and LP do not outperform the supervised baseline, probably because the supervised baseline is obtained by lo- gistic regression, which was reported to be more ef- fective than SVMs in sentiment classification (the supervised result of SVMs is 0.695).</p><p>Our proposed approach significantly outper- forms all the other methods. It gains the improve- ment over the supervised baseline, Self-training, Co-static, Co-dynamic and Co-PI by 4.3%, 3.2%, 2.3%, 1.5% and 1.9%, respectively. All of the im- provements are significant according to the paired t-test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison of Bootstrapping Methods</head><p>In <ref type="figure" target="#fig_1">Figure 3</ref>, we further compare five bootstrap- ping methods by drawing the accuracy curve dur- ing the bootstrapping process. The x-axis denotes the number of new labeled data bootstrapped from the unlabeled data. We can roughly rank five bootstrapping methods as follows: Our approach Co-dynamic &gt; Co- PI &gt; Co-static Self-training. Self-training gives the worst performance. Co-static works better but the effect is limited. Co-PI and Co-dynamic are significantly better. Our proposed approach outper- forms the other systems robustly, along with the in- creased number of the new labeled data. It suggests that our approach is very efficient in bootstrapping the class labels from the unlabeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Influence of the Size of the Initial Labeled Set</head><p>The above results are obtained with 200 labeled, 1400 unlabeled and 400 test reviews. We now tune the size of the initial labeled set (from 20 to 400), and report its influence in <ref type="figure" target="#fig_2">Figure 4</ref>. For all the set- tings, we fix the size of test set as 400. The x-axis denotes the number of initial labeled set. For ex- ample, "20" denotes the setting of 20 labeled and 1580 unlabeled data. We can observe that our all methods improve as the initial size increases. But the improvements be- come limited when the size becomes larger. When the initial size is 400, the semi-supervised perfor- mance is close to the golden result obtained by the supervised classifier trained with all 1600 labeled data.</p><p>Our approach performs consistently the best across different sizes of the initial sizes. The smaller the initial size is, the more improvements our approach can gain, in comparison with the other methods. This confirms our analysis in Sec- tion 3.3 that the technique of dual-view construc- tion is very effective to boost the semi-supervised  classification performance, especially when the size of the initial labeled set is small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Discussion on the Two Co-training Requirements</head><p>Ideally, co-training requires that each view is sufficient for classification (sufficient condition) and two views provide complementary informa- tion of the instance,(complementary condition).In this section, we answer the following question empirically: whether our approach could meet the two requirements?</p><p>(1) Sufficient condition</p><p>In <ref type="figure" target="#fig_4">Figure 5</ref>, we report the classification perfor- mance obtained by the classifiers trained with dis- tinct views and compared them with the two views in Co-PI, on the DVD and Electronics datasets. The observation in Book is similar to that in Elec- tronics; the observation in DVD is similar to that in Kitchen.</p><p>Seen from <ref type="figure" target="#fig_4">Figure 5</ref>, the classification perfor- mance of both the original-view and antonymous- view classifiers are satisfactory. It shows that in our approach, each individual view is sufficient to predict the sentiment. In comparison with the two views in Co-PI (i.e., the personal and impersonal views), two views in our approach perform signifi- cantly better.</p><p>As has been mentioned in Section 3.3, in tradi- tional methods, such as Co-PI and Co-dynamic, two views are created by data partition (or feature partition). In comparison, the two views in our approach are constructed in a manner of data ex- pansion. By creating a new antonymous view, our approach can provide more sufficient information of the reviews than traditional methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(2) Complementary condition</head><p>Since we have not found a direct measure of the complementarity of two views, we instead calcu- late the Kullback-Leibler (KL) divergence between them, based on an assumption that two views with higher KL divergence can provide more comple- mentary information of the instance.</p><p>KL divergence is a widely used metric of statis- tical distance. We assume that distribution of the review text is multinomial, and calculate the K-L divergence between two views as follows:</p><formula xml:id="formula_3">D KL (p||q) = V i=1 p i log p i q i</formula><p>where p i and q i are the probabilities of word ap- pearing in two views, respectively. In our ex- periments, we use information gain (IG) to select a set of discriminative words with the dimension V = 2000.</p><p>In <ref type="table">Table 2</ref>, we report the results of three differ- ent methods: 1) dataset random partition; 2) per- sonal and impersonal views in Co-PI; 3) original and antonymous views in our approach. We can observe from <ref type="table">Table 2</ref> that, random partition has the lowest KL divergence. It shows that the dis- tributional distance between two randomly parti- tioned views is very small. Co-PI is a higher value, but it still does not have significant difference in two views. By contrast, the KL divergence be- tween the original view and the antonymous view is much higher than both random partition and Co- PI. It demonstrates that the distributions of two views in our approach are significantly different. We thereby infer that the two views constructed in our approach can provide more complementary in- formation than traditional methods. It is reason- able since the antonymous view incorporates the KL divergence Random Partition 2.43 Co-PI 4.59 Our approach 12.33 <ref type="table">Table 2</ref>: The average KL divergence between two views across four datasets.</p><p>antonyms that might have not appeared in the origi- nal view (e.g., "satisfactory" in the example in Sec- tion 3.2). These features might provide new infor- mation about the instance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">The Effect of Dual-view Testing</head><p>In <ref type="figure" target="#fig_4">Figure 5</ref>, we can further observe the effect of dual-view testing. On the Electronics dataset, the antonymous view performs better than the orig- inal view. This suggests the advantage of the antonymous view, as it removes the negations and thus is more suitable for the BOW representa- tion. On the DVD dataset, the original view is slightly better. This is also reasonablel, because the antonymous review is automatically created and its quality might be limited in some cases. By tak- ing two opposite views into a joint consideration, our dual-view testing technique guarantees a satis- factory classification performance across different datasets.</p><p>Note that in the current version, the original- view and antonymous-view classifiers have the same predicting weight. We believe that by learn- ing the tradeoff between two views in different set- tings may further improve our approach's perfor- mance. For example, if the original view on the Electronics dataset gets a relatively larger weight, dual-view testing might gain more improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this work, a review text is represented by a pair of bags-of-words with opposite views (i.e., the original and antonymous views). By making use of two views in pairs, a dual-view co-training al- gorithm is proposed for semi-supervised sentiment classification. The dual-view representation is in a good accordance with the two co-training require- ments (i.e., sufficient condition and complemen- tary condition). The experimental results demon- strate the effect of our approach, in addressing the negation problem and enhancing the bootstrapping efficiency for semi-supervised sentiment classifica- tion.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An illustration of the dual-view BOW representation. The feature vector with black font color and grey background denotes the original view; while the one with white font color and black background denotes the reversed antonymous view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>•Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparsion of different boostrapping methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Influence of the size of initial labeled data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Comparison of different views on the DVD and Electronics datasets.</figDesc></figure>

			<note place="foot" n="3"> The Proposed Approach 3.1 Dual-view BOW Representation for Review Texts Every coin has two sizes. In this work, we are motivated to automatically construct the antonymous reviews, consider the original and antonymous reviews as two opposite sides of one review, and rep</note>

			<note place="foot" n="1"> It is worth noting that our emphasis here is not to generate natural-language-like review texts. Since either the original or the created antonymous review will be represented as a vector of independent words in the BOW model, the grammatical requirement is not as strict as that in human languages. 2 In our experiments, we extract the antonym dictionary from the WordNet lexicon http://wordnet. princeton.edu/.</note>

			<note place="foot" n="3"> Logistic regression is quite similar to Maximum Entropy, and has been proved to be more efficient in sentiment classification than some other classification algorithms including na¨ıvena¨ıve Bayes and SVMs (Pang et al., 2002). 4 http://www.csie.ntu.edu.tw/ ˜ cjlin/ liblinear/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>The work is supported by the Natural Science Foundation of China (61305090), and the Jiangsu Provincial Natural Science Foundation of China (BK2012396).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Customizing Sentiment Classifiers to New Domains: A Case Study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Recent Advances in Natural Language Processing</title>
		<meeting>Recent Advances in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLT</title>
		<meeting>COLT</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bootstrapping POS taggers using unlabelled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Osborne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mine the Easy and Classify the Hard: Experiments with Automatic Sentiment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-IJCNLP</title>
		<meeting>ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Seeing stars when there aren&apos;t many stars: graph-based semi-supervised learning for sentiment categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on TextGraphs at HLT-NAACL</title>
		<meeting>the Workshop on TextGraphs at HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mining opinion features in customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the National Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to Shift the Polarity of Words for Sentiment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ikeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Takamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okumura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCNLP</title>
		<meeting>IJCNLP</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Transductive Inference for Text Classification using Support Vector Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sentiment classification of movie reviews using contextual valence shifters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Inkpen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="110" to="125" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Weakly supervised natural language learning without redundant views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL</title>
		<meeting>HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>LaTeX Error: File &apos;url.sty&apos; not found</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Text classification from labeled and unlabeled documents using EM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="103" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Employing personal/impersonal views in supervised and semi-supervised sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sentiment Classification and Polarity Shifting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the International Conference on Computational Linguistics (COLING)</title>
		<meeting>eeding of the International Conference on Computational Linguistics (COLING)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SemiSupervised Learning for Imbalanced Sentiment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sentiment classification using subjective and objective views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Applications</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="30" to="34" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Reserved Self-training: A Semi-supervised Sentiment Classification Method for Chinese Microblogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCNLP</title>
		<meeting>IJCNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Co-training and self-training for word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Effectiveness of simple linguistic processing in automatic sentiment classification of product reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Khoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the Conference of the International Society for Knowledge Organization</title>
		<meeting>eeding of the Conference of the International Society for Knowledge Organization</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Buy itdon&apos;t buy it: sentiment classification on Amazon reviews using sentence polarity shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Orimaye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Alhashmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Siew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Pacific Rim International Conferences on Artificial Intelligence (PRICAI)</title>
		<meeting>the Pacific Rim International Conferences on Artificial Intelligence (PRICAI)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Thumbs up?: sentiment classification using machine learning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vaithyanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Contextual lexical valence shifters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Polanyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zaenen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Spring Symposium on Exploring Attitude and Affect in Text</title>
		<meeting>the AAAI Spring Symposium on Exploring Attitude and Affect in Text</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sentiment Classification in Resource-Scarce Languages by using Label Propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yoshinaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Toyoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kitsuregawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Pacific Asia Conference on Language, Information and Computation</title>
		<meeting>the Pacific Asia Conference on Language, Information and Computation</meeting>
		<imprint>
			<publisher>PACLIC</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Applying cotraining methods to statistical parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-view Learning for Semi-supervised Sentiment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Asian Language Processing</title>
		<meeting>the International Conference on Asian Language Processing</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Co-Training for Cross-Lingual Sentiment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-IJCNLP</title>
		<meeting>ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dual Training and Dual Prediction for Polarity Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A re-examination of text categorization methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Active Deep Networks for Semi-Supervised Sentiment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning from labeled and unlabeled data with label propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno>CMU-CALD-02-107</idno>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Semisupervised learning using Gaussian fields and harmonic functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceddings of the International Conference on Machine Learning (ICML)</title>
		<meeting>eddings of the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
