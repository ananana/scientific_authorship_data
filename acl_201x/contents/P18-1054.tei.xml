<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Entity-Centric Joint Modeling of Japanese Coreference Resolution and Predicate Argument Structure Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomohide</forename><surname>Shibata</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">CREST</orgName>
								<address>
									<addrLine>JST 4-1-8, Kawaguchi-shi</addrLine>
									<postCode>332-0012</postCode>
									<settlement>Honcho, Saitama</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">CREST</orgName>
								<address>
									<addrLine>JST 4-1-8, Kawaguchi-shi</addrLine>
									<postCode>332-0012</postCode>
									<settlement>Honcho, Saitama</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Informatics</orgName>
								<orgName type="institution">Kyoto University Yoshida-honmachi</orgName>
								<address>
									<addrLine>Sakyo-ku</addrLine>
									<postCode>606-8501</postCode>
									<settlement>Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Entity-Centric Joint Modeling of Japanese Coreference Resolution and Predicate Argument Structure Analysis</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="579" to="589"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>579</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Predicate argument structure analysis is a task of identifying structured events. To improve this field, we need to identify a salient entity, which cannot be identified without performing coreference resolution and predicate argument structure analysis simultaneously. This paper presents an entity-centric joint model for Japanese coreference resolution and predicate argument structure analysis. Each entity is assigned an embedding, and when the result of both analyses refers to an entity, the entity embedding is updated. The analyses take the entity embedding into consideration to access the global information of entities. Our experimental results demonstrate the proposed method can improve the performance of the inter-sentential zero anaphora resolution drastically , which is a notoriously difficult task in predicate argument structure analysis.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Natural language often conveys a sequence of events like "who did what to whom", and extract- ing structured events from the raw text is a kind of touchstone for machine reading. This is realized by a combination of coreference resolution (called CR, hereafter) and predicate argument structure analysis (called PA, hereafter).</p><p>The characteristics and difficulties in the anal- yses vary among languages. In English, there are few omissions of arguments, and thus PA is rela- tively easy, around 83% accuracy ( , while CR is relatively difficult, around 70% accu- racy ( .</p><p>On the other hand, in Japanese and Chinese, where arguments are often omitted, PA is a dif- ficult task, and even state-of-the-art systems only achieve around 50% accuracy. Zero anaphora res- olution (ZAR) is a difficult subtask of PA, de- tecting a zero pronoun and identifying a referent of the zero pronoun. As the following example shows, CR in English (identifying the antecedent of it) and ZAR in Japanese (identifying the omit- ted nominative argument) are similar problems.</p><p>(1) a. John bought a car last month.</p><p>It was made by Toyota.</p><p>b.</p><p>John-TOP last month a car-ACC bought. () (ϕ-NOM) Toyota made-COPULA.</p><p>Note that CR such as the relation between "the company" and "Toyota" is also difficult in Japanese.</p><p>According to the argument position relative to the predicate, ZAR is classified into the following three types:</p><p>• intra-sentential (intra in short): an argument is located in the same sentence with the pred- icate • inter-sentential (inter in short): an argument is located in the preceding sentences, such as "" for "" (Toyota made- COPULA) in sentence (1b)</p><p>• exophora: an argument does not appear in a document, such as author and reader Among these three types, the analysis of inter is extremely difficult because there are many candi- dates in preceding sentences, and clues such as a dependency path between a predicate and an argu- ment cannot be used.</p><p>This paper presents a joint model of CR and PA in Japanese. It is necessary to perform them together because PA (especially inter-sentential B#CDE&amp;&amp;&amp;FGH&amp;I&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;9J&amp;K&amp;&amp;LM&amp;&lt;N&amp;/?&amp;@A C)2&lt;0@2=:'DE8A &lt;0@2'744A &lt;9CC"):'-7A+A ZAR) needs to identify salient entities, which can- not be identified without performing CR and PA simultaneously. Our results support this claim, and suggest that the status quo of PA-exclusive re- search in Japanese is an insufficient approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>!"#$"!! !"#$%'? ,-? 234A 2=F:&amp;?39G2)A</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OOOA</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>#%&amp;'(!A H!"#$%&amp;"#'()*I? H12132)I? H;9&lt;&lt;0$=I</head><p>Our work is inspired by <ref type="bibr" target="#b25">(Wiseman et al., 2016)</ref>, which described an English CR system, where entities are represented by embeddings, and they are updated by CR results dynamically. We per- form Japanese CR and PA by extending this idea. Our experimental results demonstrate the pro- posed method can improve the performance of the inter-sentential zero anaphora resolution dras- tically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Predicate Argument Structure Analysis. Early studies have handled both intra-and inter- sentential anaphora ( <ref type="bibr" target="#b24">Taira et al., 2008;</ref><ref type="bibr" target="#b22">Sasano and Kurohashi, 2011)</ref>, and <ref type="bibr" target="#b7">Hangyo et al. (2013)</ref> present a method for handling exophora. Recent studies, however, focus on only intra-sentential anaphora ( <ref type="bibr" target="#b18">Ouchi et al., 2015;</ref><ref type="bibr" target="#b23">Shibata et al., 2016;</ref><ref type="bibr" target="#b11">Iida et al., 2016;</ref><ref type="bibr" target="#b19">Ouchi et al., 2017;</ref><ref type="bibr" target="#b16">Matsubayashi and Inui, 2017)</ref>, because the analysis of inter- sentential anaphora is extremely difficult. Neural network-based approaches ( <ref type="bibr" target="#b23">Shibata et al., 2016;</ref><ref type="bibr" target="#b11">Iida et al., 2016;</ref><ref type="bibr" target="#b19">Ouchi et al., 2017;</ref><ref type="bibr" target="#b16">Matsubayashi and Inui, 2017</ref>) have improved its performance.</p><p>Although most of studies did not consider the notion entity, Sasano and Kurohashi (2011) con- sider an entity, and its salience score is calcu- lated based on simple rules. However, they used gold coreference links to form the entities, and reported the salience score did not improve the performance. In contrast, we perform CR auto- matically, and capture the entity salience by using RNNs.</p><p>For Chinese, where zero anaphors are often used, neural network-based approaches <ref type="bibr" target="#b1">(Chen and Ng, 2016;</ref><ref type="bibr" target="#b26">Yin et al., 2017</ref>) outperformed conven- tional machine learning approaches ( <ref type="bibr" target="#b27">Zhao and Ng, 2007)</ref>.</p><p>Coreference Resolution. CR has been actively studied in English and Chinese. Neural network- based approaches <ref type="bibr" target="#b25">(Wiseman et al., 2016;</ref><ref type="bibr">Clark and Manning, 2016b,a;</ref> outper- formed conventional machine learning approaches <ref type="bibr" target="#b3">(Clark and Manning, 2015)</ref>. <ref type="bibr" target="#b25">Wiseman et al. (2016)</ref> and <ref type="bibr" target="#b5">Clark and Manning (2016b)</ref> learn an entity representation and integrate this into a mention- based model. Our work is inspired by <ref type="bibr" target="#b25">Wiseman et al. (2016)</ref>, which learn the entity representa- tion by using Recurrent Neural Networks (RNNs). Clark and Manning (2016b) adopt a clustering ap- proach for the entity representation. The reason why we do not use this is that if we take a cluster- ing approach in our setting, zero pronouns need to be first identified before clustering, and thus, it is hard to perform CR and PA jointly.  take an end-to-end approach, aiming at not relying on hand-engineering mention detec- tor (consider all spans as potential mentions). In used Japanese evaluation corpora, since the basic unit for the annotations and our analyses (CR and PA) is fixed, we do not need consider all spans.</p><p>In Japanese, CR has not been actively studied other than <ref type="bibr" target="#b10">Iida et al. (2003)</ref>; <ref type="bibr" target="#b21">Sasano et al. (2007)</ref> since the use of zero pronouns is more common and problematic. Semantic Role Labeling. Japanese PA is simi- lar to Semantic Role Labeling (SRL) in English. Neural network-based approaches have improved the performance ( <ref type="bibr" target="#b29">Zhou and Xu, 2015;</ref>. In these approaches, an appropriate argu- ment for a predicate is searched among mentions in a text. The notion entity is not considered. Other Entity-Centric Study. There are several studies that consider the notion entity in other ar- eas: text comprehension ( <ref type="bibr" target="#b14">Kobayashi et al., 2016;</ref><ref type="bibr" target="#b9">Henaff et al., 2016</ref>) and language modeling ( <ref type="bibr" target="#b12">Ji et al., 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Japanese Preliminaries</head><p>Before presenting our proposed method, we de- scribe the basics of Japanese predicate argument structure and its analysis.</p><p>Since the word order is relatively free among arguments in Japanese, an argument is followed by a case marking postposition. The postpositions (ga), (wo), and (ni) indicate nominative (NOM), accusative (ACC) and dative (DAT), respec- tively. In the double nominative construction such as "" (My English is good), "" (English) is regarded as NOM, and "" (I), the outer nominative is regarded as NOM2. This paper targets these four cases.</p><p>PA is tightly related to a dependency structure of a sentence. Considering the relation between a predicate and its argument, and a necessary analy- sis can be classified into the following three cate- gories (see example sentence (2) below).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(2)</head><p>John-TOP bought bread-ACC ate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D D D</head><p>Overt case: When an argument with a case marking postposition has a dependency relation with a predicate, PA is not necessary. In example (2), since " " (bread-ACC) has a dependency relation with "" (ate), it is obvious that " " takes "" as its ACC argument.</p><p>Case analysis: When a topic marker (wa) is attached to an argument, the case marking postpo- sition disappears, and the analysis of identifying the case role becomes necessary. The analysis is called case analysis. In the example, although " " (John-TOP) has a dependency relation with "" (ate), the analysis of identifying NOM is necessary. The same phenomenon happens when a relative clause is used. When an argument is mod- ified by a relative clause, we do not know its case role to the predicate in the relative clause. In the example, although "" has a dependency rela- tion with "" (bought), the analysis of iden- tifying ACC is necessary.</p><p>Zero anaphora resolution (ZAR): Some argu- ments are not included in the phrases with which a predicate has a dependency relation. While pro- nouns are mostly used in English, they are rarely used in Japanese. This phenomenon is called zero anaphora, and the analysis of identifying an argu- ment (referent of the zero pronoun) is called zero anaphora resolution (ZAR). In the example, al- though "" takes "" as its NOM argu- ment, they do not have a dependency relation, and thus zero anaphora resolution is necessary.</p><p>When dependency relations are identified by parsing, what Japanese PA has to do is case analy- sis and zero anaphora resolution.</p><p>Each predicate has a set of required cases, but not all the four cases. For example, "" (buy) takes NOM and ACC, but neither DAT nor NOM2. PA for "" in sentence (2) has to find John as NOM, but also has to judge that it does not take DAT and NOM2 arguments.</p><p>Another difficulty lies in that a predicate takes a case, but in a sentence it does not take a spe- cific argument. For example, in the sentence "it is difficult to bake a bread", NOM of "bake" is not a specific person, but means "anyone" or "in gen- eral". In such cases, PA has to regard arguments as unspecified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Overview of Our Proposed Method</head><p>An overview of our proposed model is described with a motivated example ( <ref type="figure" target="#fig_1">Figure 1</ref>). Our model equips an entity buffer for entity management. At first, it contains only special entities, author and reader.</p><p>In Japanese CR and PA, a basic phrase, which consists of one content word and zero or more function words, is adopted as a basic unit. When an input text is given, the contextual represen- tations of basic phrases are obtained by using Convolutional Neural Network (CNN) and Bi- directional LSTM. Then, from the beginning of the text, CR is performed if a target phrase is a noun phrase, and PA is performed if a target phrase is a predicate phrase. Both of these analyses take into consideration not only the mentions in the text but also the entities in the entity buffer.</p><p>In CR, when a mention refers to an existing en- tity, the entity embedding in the entity buffer is updated. In <ref type="figure" target="#fig_1">Figure 1</ref>, "" (said person) is ana- lyzed to refer to "" (Mr.Kovalyov), and the entity embedding of "" is updated. When a mention is analyzed to have no antecedent, it is registered to the entity buffer as a new entity.</p><p>In PA, when a predicate has no argument for any case, its argument is searched among any mentions in the text, author and reader. In the same way as CR, PA takes into consideration not only the mentions but also entities in the entity buffer, and updates the entity embedding.</p><p>In <ref type="figure" target="#fig_1">Figure 1</ref>, the predicate "" (run for) has no NOM argument. Our method finds " " as its NOM argument, and then updates its entity embedding. As mentioned before, the entity embedding of "" is updated by the coreference relation with "" in the sec- ond sentence. In the third sentence, the predicate "" (support) has also no NOM argu- ment, and "" is identified as its NOM argument, because the frequent reference implies its salience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Base Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Input Encoding</head><p>Conventional machine learning techniques have extracted features from a basic phrase, which re- quire much effort on feature engineering. Our method obtains an embedding of each basic phrase using CNN and bi-LSTM as shown in <ref type="figure" target="#fig_2">Figure 2</ref>.</p><p>Suppose the i-th basic phrase bp i consists of |bp i | words. First, the embedding of each word is represented as a concatenation of word (lemma), part of speech (POS), sub-POS and conjugation embeddings. We append start-of-phrase and end- of-phrase special words to each phrase in order to better represent prefixes and suffixes. Let W i ∈ R d×(|bp i |+2) be an embedding matrix for bp i where d denotes the dimension of word representation.</p><p>The embedding of the basic phrase is obtained by applying CNN to the sequence of words. A fea- ture map f i is obtained by applying a convolution between W i and a filter H of width n. The m-th element of f i is obtained as follows: is the Frobenius inner product. Then, to capture the most important feature for a given filter in bp i , the max pooling is applied as follows:</p><formula xml:id="formula_0">f i [m] = tanh(⟨W i [ * , m : m + n − 1], H⟩),<label>(1)</label></formula><formula xml:id="formula_1">!"#$%&amp;'&amp; !"#$ %&amp;&amp;'()*&amp; +&amp;,-&amp; ./0&amp; x i h i 12&amp;3"'4&amp;35&amp; 16/.5&amp; H W i f i [1] !"#$"! %&amp;'! (&amp; 17,</formula><formula xml:id="formula_2">x i = max m f i [m].<label>(2)</label></formula><p>The process described so far is for one filter. The multiple filters of varying widths are applied to obtain the representation of bp i . When we set h filters, x i , the embedding of the i-th basic phrase, is represented as</p><formula xml:id="formula_3">[x i 1 , · · · , x i h ].</formula><p>The embeddings of basic phrases are read by bi- LSTM to capture their context as follows:</p><formula xml:id="formula_4">− → h i = −−−−→ LST M (x i , − → h i−1 ), ← − h i = ←−−−− LST M (x i , ← − h i+1 ),<label>(3)</label></formula><p>and the contextualized embedding of the i-th ba- sic phrase is represented as a concatenation of the hidden layers of forward and backward LSTM.</p><formula xml:id="formula_5">h i = [ − → h i ; ← − h i ]<label>(4)</label></formula><p>This process is performed for each sentence. Since CR and PA are performed for a whole doc- ument D, the indices of basic phrases are reas- signed from the beginning to the end of D in a consecutive order:</p><formula xml:id="formula_6">D = {h 1 , h 2 , · · · , h i , · · · }.</formula><p>To handle exophora, author and reader are as- signed a unique trainable embedding, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Coreference Resolution</head><p>We adopt a mention-ranking model that as- signs each mention its highest scoring candi- date antecedent. This model assigns a score s m CR (ant, m i ) to a target mention m i and its candidate antecedent ant <ref type="bibr">1</ref> </p><note type="other">. The candidate an- tecedents include i) mentions preceding m i , ii) au- thor and reader, and iii) NA CR (no antecedent). s m CR (ant, m i ) is calculated as follows:</note><formula xml:id="formula_7">s m CR (ant, m i ) = W CR 2 ReLU (W CR 1 v CR input ),<label>(5)</label></formula><p>where <ref type="formula">W</ref>  • whether a pair of m i and ant has an entry in a synonym dictionary. When a candidate antecedent is NA CR , the input vector is just the embedding of a target mention m i , and the same neural network with different weight matrices calculates a score.</p><p>The following margin objective is trained:</p><formula xml:id="formula_8">LCR = Nm ∑ i max ant∈AN T (m i ) (1+s m CR (ant, mi)−s m CR ( ˆ ti, mi)),<label>(6)</label></formula><p>where N m denotes the number of mentions in a document, AN T (m i ) denotes the set of candidate antecedents of m i , andˆtandˆ andˆt i denotes the highest scor- ing true antecedent of m i defined as follows:</p><formula xml:id="formula_9">ˆ t i = argmax ant∈T (m i ) s m CR (ant, m i ),<label>(7)</label></formula><p>where T (m i ) denotes the set of true antecedents of m i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Predicate Argument Structure Analysis</head><p>When a target phrase is a predicate phrase, PA is performed. For each case of a predicate, PA searches an appropriate argument among candi- date arguments: i) basic phrases located in the sen- tence including the predicate and preceding sen- tences, ii) author and reader, iii) unspecified, and <ref type="bibr">1</ref> The superscript m of s m CR (ant, mi) represents a men- tion-based score, which contrasts with an entity-based score introduced in Section 6. iv) NA PA which means the predicate takes no argu- ment of for the case. The probability that the predicate m i takes an argument arg for case c is defined as follows:</p><formula xml:id="formula_10">P (c = arg|m i ) = exp(s m PA (arg, m i , c)) ∑ carg∈ ARG(m i ) exp(s m PA (carg, m i , c)) ,<label>(8)</label></formula><p>where ARG(m i ) denotes the set of candidate ar- guments of m i , and a score s m PA (arg, m i , c) is cal- culated by a neural network as follows <ref type="figure" target="#fig_4">(Figure 3)</ref>:</p><formula xml:id="formula_11">s m PA (arg, m i , c) = W PA 2 tanh(W PA 1,c v PA input ),<label>(9)</label></formula><p>where W PA 1,c , W PA 2 are weight matrices, and v PA input is an input vector, a concatenation of the following vectors:</p><p>• embeddings of m i and arg 2</p><p>• path embedding: the dependency path be- tween a predicate and an argument is an im- portant clue. Roth and Lapata (2016) learn a representation of a lexicalized dependency path for SRL. An LSTM reads words 3 from an argument to a predicate along with a de- pendency path, and the final hidden state is adopted as the embedding of the dependency path. <ref type="bibr">4</ref> For case analysis, the direct depen- dency relation between a predicate and its ar- gument can be represented as the path em- bedding.</p><p>• selectional preference: selectional preference is another important clue for PA. A selec- tional preference score is learned in an un- supervised manner from automatic parses of a raw corpus ( <ref type="bibr" target="#b23">Shibata et al., 2016)</ref>.</p><p>• sentence distance between m i and arg. The distance is binned in the same way as CR.</p><p>The objective is to minimize the cross entropy between predicted and true distributions:</p><formula xml:id="formula_12">L PA = − Np ∑ i ∑ c log P (c = arg|p i ),<label>(10)</label></formula><p>where N p denotes the number of predicates in a document, and arg denotes a true argument.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Entity-Centric Model</head><p>While the base model performs mention-based CR and PA, our proposed model performs entity-based analyses as shown in <ref type="figure" target="#fig_1">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Entity Embedding Update</head><p>The entity embeddings are managed in an entity buffer. First, let us introduce time stamp i for the entity embedding update. Time i corresponds to the analysis for the i-th basic phrase in a docu- ment. If an entity is referred to by the analysis, its embedding is updated. Let e <ref type="bibr">(k)</ref> i be the embedding of an entity k at time i (after the entity embedding is updated).</p><p>In CR, following <ref type="bibr" target="#b25">Wiseman et al. (2016)</ref>, when a target phrase m i refers to the entity k, e (k) i is updated as follows:</p><formula xml:id="formula_13">e (k) i ← LST M e (h i , e (k) i−1 )<label>(11)</label></formula><p>where LST M e denotes an LSTM for the entity embedding update. When an antecedent is NA CR , a new entity embedding is set up, initialized by a zero vector. The entity buffer maintains K LSTMs (K is the number of entities in a document), and their parameters are shared. The proposed method updates the entity embed- ding not only in CR but also in PA. When the ref- erent of a zero pronoun of case c of predicate p i is entity k, the entity embedding is updated by using the predicate embedding h i multiplied by a weight matrix W c for case c as follows:</p><formula xml:id="formula_14">e (k) i ← LST M e (W c h i , e (k) i−1 ).<label>(12)</label></formula><p>In both CR and PA, the embeddings of entities other than the referred entity k are not updated (e</p><formula xml:id="formula_15">(l) i ← e (l) i−1 (l ̸ = k)).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Use of Entity Embedding in CR and PA</head><p>Both CR and PA are allowed to take the entity em- beddings into consideration. In CR, let z ant de- note the id of an entity to which the candidate an- tecedent ant belongs. The entity-based score s e CR is calculated as follows:</p><formula xml:id="formula_16">s e CR (ant, m i ) = { h T i e (zant) i−1 (ant ̸ =NA CR ) g N A (m i ) (ant =NA CR ).<label>(13)</label></formula><p>The intuition behind the first case is that the dot- product of h i , the embedding of the target men- tion, and e <ref type="bibr">(zant)</ref> i−1 , the embedding of the entity that ant belongs to indicates the plausibility of their coreference. g N A (m i ) is defined as follows:</p><formula xml:id="formula_17">g N A (m i ) = q T tanh(W N A [ h i ∑ k e i−1 (k) ] ), (14)</formula><p>where q is a weight vector, and W N A is a weight matrix. The intuition is that whether a target phrase is NA CR can be judged from h i , the embed- ding of the target mention itself, and the sum of all the current entity embeddings. s e CR is added to s m CR , and the training objective is the same as the one described in Section 5.2.</p><p>In PA, the entity embedding corresponding to a candidate argument arg 5 is just added to the input vector v PA input described in Section 5.3, and mention-and entity-based score s m+e PA (arg, m i , c) is calculated in the same way as s m PA (arg, m i , c). The training objective is again the same as the one in Section 5.3.</p><p>In <ref type="bibr" target="#b25">Wiseman et al. (2016)</ref>, the oracle entity as- signment is used for the entity embedding update in training, and the system output is used in a greedy manner in testing. Since the performance of PA is lower than that of English CR, there might be a more significant gap between training and testing. Therefore, scheduled sampling ( <ref type="bibr" target="#b0">Bengio et al., 2015</ref>) is adopted to bridge the gap: in train- ing, the oracle entity assignment is used with prob- ability ϵ t (at the t-th iteration) and the system out- put otherwise. Exponential decay is used: ϵ t = k t (we set k = 0.75 for our experiments).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Experimental Setting</head><p>The two kinds of evaluation sets were used for our experiments. One is the KWDLC (Kyoto Uni-versity Web Document Leads Corpus) evaluation set ( <ref type="bibr" target="#b6">Hangyo et al., 2012)</ref>, and the other is Ky- oto Corpus. KWDLC consists of the first three sentences of 5,000 Web documents (15,000 sen- tences) and Kyoto Corpus consists of 550 News documents (5,000 sentences). Word segmenta- tions, POSs, dependencies, PASs, and corefer- ences were manually annotated (the closest ref- erents and antecedents were annotated for zero anaphora and coreferences, respectively). Since we want to focus on the accuracy of CR and PA, gold segmentations, POSs, and dependen- cies were used. KWDLC (Web) was divided into 3,694 documents (11,558 sents.) for training, 512 documents (1,585 sents.) for development, and 700 documents (2,195 sents.) for testing; Kyoto Corpus (News) was divided into 360 documents (3,210 sents.) for training, 98 documents (971 sents.) for development, and 100 documents (967 sents.) for testing.</p><p>The evaluation measure is an F-measure, and the evaluation of both CR and PA was relaxed using a gold coreference chain, which leads to an entity-based evaluation. We did not use the conventional CR evaluation measures (MUC, B 3 , CEAF and CoNLL) because our F-measure is al- most the same as MUC, which is a link-based measure, and the other measures considering sin- gletons get excessively high values 6 , and thus they do not accord with the actual performance in our setting. <ref type="bibr">7</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Implementation Detail</head><p>The dimension of word embeddings was set to 100, and the word embeddings were initialized with pre-trained embeddings by Skip-gram with a negative sampling ( <ref type="bibr" target="#b17">Mikolov et al., 2013</ref>) on a Japanese Web corpus consisting of 100M sen- tences. The dimension of POS, sub-POS and con- jugation were set to 10, respectively, and these em- beddings were initialized randomly. The dimen- sions of the hidden layer in all the neural networks were set to 100. We used filter windows of 1,2,3 with 33 feature maps each for basic phrase CNN.</p><p>Adam <ref type="bibr" target="#b13">(Kingma and Ba, 2014</ref>) was adopted as the optimizer. F measures were averaged over four runs.</p><p>Checkpoint ensemble <ref type="bibr" target="#b2">(Chen et al., 2017</ref>) was adopted, where the k best models were taken in terms of validation score, and then the parame- ters from the k models were averaged for testing. This method requires only one training process. In our experiments, k was set to 5, and the maximum number of epochs was set to 10.</p><p>We used a single-layer bi-LSTM for the input encoding (Section 5.1); preliminary experiments with stacked stacked bi-directional LSTM with residual connections were not favorable. Although we tried to use the character-level embedding of each word obtained with CNN, as the same way in the basic phrase embedding from the word se- quences, the performance was not improved. The synonym dictionary used for CR (Section 5.2) was constructed from an ordinary dictionary and Web corpus, and has about 7,300 entries ( <ref type="bibr" target="#b21">Sasano et al., 2007</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Experimental Result</head><p>The following three methods were compared:</p><p>• Baseline: the method described in Section 5.</p><p>• "+entity (CR)": this method corresponds to <ref type="bibr" target="#b25">(Wiseman et al., 2016)</ref>. Entity embedding is updated based on the CR result, and CR takes the entity embedding into consideration.</p><p>• "+entity (CR,PA)" (proposed method): en- tity embedding is updated based on PA as well as CR result, and the CR and PA take the entity embedding into consideration.</p><p>The performance of CR and PA (case analysis and zero anaphora resolution (ZAR)) is shown in <ref type="table" target="#tab_1">Table 1</ref>. The performance of CR and case anal- ysis was almost the same for all the methods. For ZAR, "+entity (CR,PA)" improved the perfor- mance drastically.</p><p>CR surely benefits from the entity salience. Since entity embeddings are updated based on system outputs, its performance matters. The performance of Japanese CR is lower than that of English CR. Therefore, we assume there are improved/worsen examples, and our CR perfor- mance did not improve significantly. The perfor- mance of ZAR also matters. However, the perfor- mance of ZAR in our baseline model is extremely low, and thus there are few worsen examples and   a number of improved examples. Therefore, ZAR can benefit from the entity representation obtained by both CR and PA. <ref type="table" target="#tab_2">Table 2</ref> shows performance of case analysis and zero anaphora resolution for each case, and each argument position. Unspecified was counted for exophora. Both for the News and Web evaluation sets, the performance for inter arguments of zero anaphora resolution, which was extremely difficult in the baseline method, was improved by a large margin by our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Ablation Study</head><p>To reveal the importance of each clue for CR and PA, each clue was ablated. <ref type="table">Table 3</ref> shows the result on the development set. We found that, the path embedding was effective for PA, and the string match was effective for CR. The sentence distance for both CR and PA was effective for News, but not for Web since the Web evaluation corpus consists of three-sentence documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Comparison with Other Work</head><p>It is difficult to compare the performance of our method with other studies directly because there are no studies handling both CR and PA. The com- parisons with other studies are summarized as fol- lows:</p><p>• Shibata et al. <ref type="formula" target="#formula_0">(2016)</ref> proposed a neural- network based PA. Their target was intra and exophora for three major cases (NOM, ACC and DAT), and the performance was 0.534 on the same Web corpus as ours. The perfor- mance of our proposed method for the same three cases was 0.626. Furthermore, since their model assumes a static PA graph, their model is difficult to be extended to handle CR.</p><p>•  <ref type="table">Table 3</ref>: Ablation study on the development set. The cells shaded gray represent they are not directly affected from the ablation, but from the counterpart analysis result.</p><p>corpus contains a lot of annotation errors as pointed out in <ref type="bibr" target="#b11">Iida et al. (2016)</ref>, we did not conduct our experiments on the NAIST text corpus.</p><p>• <ref type="bibr" target="#b10">Iida et al. (2003)</ref> reported an F-measure of about 0.7 on News domain. The possible rea- son why our performance on News (0.541) is lower than theirs is that their basic unit is a compound noun while our basic unit is a noun, and thus our setting is difficult in com- parison with theirs.</p><p>Since we handle inter as well as intra and ex- ophora arguments in PA, together with CR, we can say that our experimental setting is more practical in comparison with other studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.6">Error Analysis</head><p>In example (3), although the NOM argument of the predicate "" (go to hospital) is author, our method wrongly classified it as unspecified.</p><p>every day go to hospital! I myself-TOP very healthy.</p><p>((I) go to hospital every day! (I am) very healthy, though.)</p><p>In the second sentence, our method correctly iden- tified the antecedent of "" (I) as author, and the NOM of "" (healthy) as "" (I). Our method adopts the greedy search so that it cannot exploit this handy information in the anal- ysis of the first sentence. The global modeling us- ing reinforcement learning <ref type="bibr" target="#b4">(Clark and Manning, 2016a</ref>) for a whole document is our future work.</p><p>In example (4), although the NOM argument of "" (be decorated) in the second sentence is "" (dress) in the first sentence, our method wrongly classified it as NA PA .</p><p>very impressive dress-COPULA. organdie-GEN top-DAT line-ACC draw-as small bead-INS decorated ((This is) a very impressive dress.</p><p>(The dress) is decorated by small beads as they draw a line on its organdy.)</p><p>"" (organdie) has a bridging relation to "", which might help capture the salience of "". The bridging reference resolution is our next target and must be easily incorporated into our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>This paper has presented an entity-centric neu- ral network-based joint model of coreference res- olution and predicate argument structure analy- sis. Each entity has its embedding, and the em- beddings are updated according to the result of both of these analyses dynamically. Both of these analyses took the entity embedding into consid- eration to access the global information of enti- ties. The experimental results demonstrated that the proposed method could improve the perfor- mance of the inter-sentential zero anaphora res- olution drastically, which has been regarded as a notoriously difficult task. We believe that our pro- posed method is also effective for other pro-drop languages such as Chinese and Korean.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An overview of our proposed method. The phrases with red represent a predicate.</figDesc><graphic url="image-1.png" coords="2,75.20,67.00,455.20,184.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Basic phrase embedding obtained with CNN and Bi-LSTM.</figDesc><graphic url="image-2.png" coords="4,329.85,64.78,170.36,232.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>CR 1 and W CR 2 are weight matrices, and v CR input is an input vector, a concatenation of the following vectors: • embeddings of m i and ant • exact match or partial match between strings of m i and ant • sentence distance between m i and ant. The distance is binned into one of the buckets [0, 1, 2, 3+].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A neural network for PA.</figDesc><graphic url="image-3.png" coords="5,309.20,65.72,238.56,157.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 : Performance (F-measure) of coreference resolution, case analysis and zero anaphora resolution.</head><label>1</label><figDesc></figDesc><table>Web 
News 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Performance of case analysis and zero anaphora resolution for each case, and each argument 
position for zero anaphora resolution. The underlined values indicate the proposed method outperforms 
the baseline by a large margin. 

</table></figure>

			<note place="foot" n="2"> An embedding for NAPA is assigned a trainable one. 3 We add special words {Parent, Child}, which indicate a dependency direction between basic phrases. 4 When an argument is an inter or exophora, the path embedding is set to be a zero vector.</note>

			<note place="foot" n="5"> When arg is NAPA, the entity embedding is set to a zero vector.</note>

			<note place="foot" n="6"> In Japanese, since zero pronouns are often used, there are many singletons. In example sentences (1) of the Introduction section, while &quot;a car&quot; and &quot;It&quot; form one cluster in English sentences (1-a), &quot;a car&quot; is a singleton in Japanese sentences (1-b) because a zero pronoun is used in the second sentence. 7 For the Web evaluation set, the F-measure of our proposed method is 0.685, and the conventional evaluation measures are as follows; MUC: 69.1, B 3 : 97.2, CEAF: 95.7, and CoNLL: 87.3.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work was supported by JST CREST Grant Number JPMJCR1301, Japan.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno>CoRR abs/1506.03099</idno>
		<ptr target="http://arxiv.org/abs/1506.03099" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Chinese zero pronoun resolution with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P16-1074" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="778" to="788" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Checkpoint ensembles: Ensemble methods from a single training process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su-In</forename><surname>Lee</surname></persName>
		</author>
		<idno>CoRR abs/1710.03282</idno>
		<ptr target="http://arxiv.org/abs/1710.03282" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Entity-centric coreference resolution with model stacking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for mention-ranking coreference models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods on Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving coreference resolution by learning entitylevel distributed representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/P16-1061</idno>
		<ptr target="https://doi.org/10.18653/v1/P16-1061" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="643" to="653" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Building a diverse document leads corpus annotated with semantic relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masatsugu</forename><surname>Hangyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/Y12-1058" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Pacific Asia Conference on Language, Information, and Computation. Faculty of Computer Science</title>
		<meeting>the 26th Pacific Asia Conference on Language, Information, and Computation. Faculty of Computer Science<address><addrLine>Bali,Indonesia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="535" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Japanese zero reference resolution considering exophora and author/reader mentions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masatsugu</forename><surname>Hangyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D13-1095" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="924" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep semantic role labeling: What works and what&apos;s next</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Tracking the world state with recurrent entity networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<idno>CoRR abs/1612.03969</idno>
		<ptr target="http://arxiv.org/abs/1612.03969" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Antoine Bordes, and Yann LeCun</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Incorporating contextual cues in trainable models for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryu</forename><surname>Iida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroya</forename><surname>Takamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EACL Workshop on The Computational Treatment of Anaphora</title>
		<meeting>the EACL Workshop on The Computational Treatment of Anaphora</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="23" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Intrasentential subject zero anaphora resolution using multi-column convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryu</forename><surname>Iida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Torisawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong-Hoon</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canasai</forename><surname>Kruengkrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Kloetzer</surname></persName>
		</author>
		<ptr target="https://aclweb.org/anthology/D16-1132" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1244" to="1254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dynamic entity representations in neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Martschat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D17-1195" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1831" to="1840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>CoRR abs/1412.6980</idno>
		<ptr target="http://arxiv.org/abs/" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dynamic entity representation with max-pooling improves machine reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sosuke</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoaki</forename><surname>Okazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL HLT</title>
		<meeting>the NAACL HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">End-to-end neural coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D17-1018" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="188" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Revisiting the design issues of local models for japanese predicate-argument structure analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichiroh</forename><surname>Matsubayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/I17-2022" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="128" to="133" />
		</imprint>
	</monogr>
	<note>Short Papers). Asian Federation of Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Joint case argument identification for Japanese predicate argument structure analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroki</forename><surname>Ouchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P15-1093" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural modeling of multi-predicate interactions for Japanese predicate argument structure analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroki</forename><surname>Ouchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P17-1146" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1591" to="1600" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural semantic role labeling with dependency path embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P16-1113" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1192" to="1202" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improving coreference resolution using bridging reference resolution and automatically acquired synonyms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryohei</forename><surname>Sasano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DAARC</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A discriminative approach to Japanese zero anaphora resolution with large-scale lexicalized case frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryohei</forename><surname>Sasano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/I11-1085" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of 5th International Joint Conference on Natural Language Processing. Asian Federation of Natural Language Processing</title>
		<editor>Chiang Mai, Thailand</editor>
		<meeting>5th International Joint Conference on Natural Language Processing. Asian Federation of Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="758" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural network-based model for Japanese predicate argument structure analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomohide</forename><surname>Shibata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P16-1117" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1235" to="1244" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A Japanese predicate argument structure analysis using decision lists</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirotoshi</forename><surname>Taira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanae</forename><surname>Fujita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D08-1055" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2008 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Honolulu, Hawaii</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="523" to="532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning global features for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><forename type="middle">M</forename><surname>Shieber</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N16-1114" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="994" to="1004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Chinese zero pronoun resolution with deep memory network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://www.aclweb" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1320" to="1329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Identification and resolution of Chinese zero pronouns: A machine learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Association for Computational Linguistics</title>
		<ptr target="http://www.aclweb.org/anthology/D/D07/D07-1057" />
	</analytic>
	<monogr>
		<title level="m">Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</title>
		<meeting><address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">End-to-end learning of semantic role labeling using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P15-1109" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1127" to="1137" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
