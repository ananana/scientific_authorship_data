<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:56+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Adversarial Training for Semi-supervised Japanese Predicate-argument Structure Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhei</forename><surname>Kurita</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Informatics</orgName>
								<orgName type="institution">Kyoto University ‡ CREST</orgName>
								<address>
									<region>JST</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Informatics</orgName>
								<orgName type="institution">Kyoto University ‡ CREST</orgName>
								<address>
									<region>JST</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Informatics</orgName>
								<orgName type="institution">Kyoto University ‡ CREST</orgName>
								<address>
									<region>JST</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Adversarial Training for Semi-supervised Japanese Predicate-argument Structure Analysis</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="474" to="484"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>474</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Japanese predicate-argument structure (PAS) analysis involves zero anaphora resolution, which is notoriously difficult. To improve the performance of Japanese PAS analysis, it is straightforward to increase the size of corpora annotated with PAS. However, since it is prohibitively expensive , it is promising to take advantage of a large amount of raw corpora. In this paper, we propose a novel Japanese PAS analysis model based on semi-supervised adversarial training with a raw corpus. In our experiments, our model outper-forms existing state-of-the-art models for Japanese PAS analysis.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In pro-drop languages, such as Japanese and Chi- nese, pronouns are frequently omitted when they are inferable from their contexts and background knowledge. The natural language processing (NLP) task for detecting such omitted pronouns and searching for their antecedents is called zero anaphora resolution. This task is essential for downstream NLP tasks, such as information ex- traction and summarization.</p><p>For Japanese, zero anaphora resolution is usu- ally conducted within predicate-argument struc- ture (PAS) analysis as a task of finding an omitted argument for a predicate. PAS analysis is a task to find an argument for each case of a predicate. For Japanese PAS analysis, the ga (nominative, NOM), wo (accusative, ACC) and ni (dative, DAT) cases are generally handled. To develop mod- els for Japanese PAS analysis, supervised learn- ing methods using annotated corpora have been applied on the basis of morpho-syntactic clues.</p><p>However, omitted pronouns have few clues and thus these models try to learn relations between a predicate and its (omitted) argument from the an- notated corpora. The annotated corpora consist of several tens of thousands sentences, and it is diffi- cult to learn predicate-argument relations or selec- tional preferences from such small-scale corpora. The state-of-the-art models for Japanese PAS anal- ysis achieve an accuracy of around 50% for zero pronouns ( <ref type="bibr" target="#b15">Ouchi et al., 2015;</ref><ref type="bibr" target="#b20">Shibata et al., 2016;</ref><ref type="bibr" target="#b7">Iida et al., 2016;</ref><ref type="bibr" target="#b16">Ouchi et al., 2017;</ref><ref type="bibr" target="#b12">Matsubayashi and Inui, 2017)</ref>.</p><p>A promising way to solve this data scarcity problem is enhancing models with a large amount of raw corpora. There are two major approaches to using raw corpora: extracting knowledge from raw corpora beforehand ( <ref type="bibr" target="#b19">Sasano and Kurohashi, 2011;</ref><ref type="bibr" target="#b20">Shibata et al., 2016</ref>) and using raw corpora for data augmentation ( <ref type="bibr" target="#b11">Liu et al., 2017b</ref>).</p><p>In traditional studies on Japanese PAS analy- sis, selectional preferences are extracted from raw corpora beforehand and are used in PAS analy- sis models. For example, <ref type="bibr" target="#b19">Sasano and Kurohashi (2011)</ref> propose a supervised model for Japanese PAS analysis based on case frames, which are au- tomatically acquired from a raw corpus by cluster- ing predicate-argument structures. However, case frames are not based on distributed representations of words and have a data sparseness problem even if a large raw corpus is employed. Some recent approaches to Japanese PAS analysis combines neural network models with knowledge extraction from raw corpora. <ref type="bibr" target="#b20">Shibata et al. (2016)</ref> extract se- lectional preferences by an unsupervised method that is similar to negative sampling ( <ref type="bibr" target="#b13">Mikolov et al., 2013</ref>). They then use the pre-extracted selectional preferences as one of the features to their PAS analysis model. The PAS analysis model is trained by a supervised method and the selectional prefer- ence representations are fixed during training. Us- <ref type="table" target="#tab_3">k  e  u  k  a  y  k  i  h  s  u  k  a  t  a  t  t  u  k  o  .  a  t  t  u  k  o  i  n  - i  k  e  o  w  - u  k  a  y  k  a  g  - i  h  s  u  k  a  t  n  o  i  t  a  t  s  r  e  g  n  e  s  s  a  p  i  x  a  t  d  e  i  r  r  a  c  /  t  n  e  s  .  n  o  i  t  a  t  s  e  h  t  o  t  s  r  e  g  n  e  s  s  a  p  d  e  i  r  r  a  c  i  x  a  t  A   (2) ACC     u  s  t  o  m  i  n  a  h  s  s  e  r  a  d  n  o  k  a  h  a.  d  n  o  k  a  h  o  w  - u  s  t  o  m  i  n  a  w  - a  h  s  s  e  r  o  n  o</ref>   <ref type="table">Table 1</ref>: Examples of Japanese sentences and their PAS analysis. In sentence (1), case markers ( が(ga), を(wo), and に(ni) ) correspond to NOM, ACC, and DAT. In example (2), the correct case marker is hidden by the topic marker は (wa). In sentence (3), the NOM argument of the second predicate 巻き込まれた (was involved), is dropped. NULL indicates that the predicate does not have the corresponding case argument or that the case argument is not written in the sentence.</p><formula xml:id="formula_0">Predicate NOM ACC DAT (1) NOM ACC DAT i</formula><p>ing pre-trained external knowledge in the form of word embeddings has also been ubiquitous. How- ever, such external knowledge is overwritten in the task-specific training.</p><p>The other approach to using raw corpora for PAS analysis is data augmentation. <ref type="bibr" target="#b11">Liu et al. (2017b)</ref> generate pseudo training data from a raw corpus and use them for their zero pronoun resolu- tion model. They generate the pseudo training data by dropping certain words or pronouns in a raw corpus and assuming them as correct antecedents. After generating the pseudo training data, they rely on ordinary supervised training based on neu- ral networks.</p><p>In this paper, we propose a neural semi- supervised model for Japanese PAS analysis. We adopt neural adversarial training to directly ex- ploit the advantage of using a raw corpus. Our model consists of two neural network models: a generator model of Japanese PAS analysis and a so-called "validator" model of the generator pre- diction. The generator neural network is a model that predicts probabilities of candidate arguments of each predicate using RNN-based features and a head-selection model ( <ref type="bibr" target="#b23">Zhang et al., 2017</ref>). The validator neural network gets inputs from the gen- erator and scores them. This validator can score the generator prediction even when PAS gold la- bels are not available. We apply supervised learn- ing to the generator and unsupervised learning to the entire network using a raw corpus.</p><p>Our contributions are summarized as follows:</p><p>(1) a novel adversarial training model for PAS analysis; (2) learning from a raw corpus as a source of external knowledge; and (3) as a re- sult, we achieve state-of-the-art performance on Japanese PAS analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Description</head><p>Japanese PAS analysis determines essential case roles of words for each predicate: who did what to whom. In many languages, such as English, case roles are mainly determined by word order. However, in Japanese, word order is highly flexi- ble. In Japanese, major case roles are the nomina- tive case (NOM), the accusative case (ACC) and the dative case (DAT), which roughly correspond to Japanese surface case markers: が(ga), を(wo), and に(ni). These case markers are often hidden by topic markers, and case arguments are also often omitted.</p><p>We explain two detailed tasks of PAS analysis: case analysis and zero anaphora resolution. In Ta- ble 1, we show four example Japanese sentences and their PAS labels. PAS labels are attached to nominative, accusative and dative cases of each predicate. Sentence (1) has surface case markers that correspond to argument cases.</p><p>Sentence <ref type="formula" target="#formula_4">(2)</ref> is an example sentence for case analysis. Case analysis is a task to find hidden case markers of arguments that have direct depen- </p><formula xml:id="formula_1">DAT: v (arg 1 ) v (arg 2 ) v (arg 3 ) . . . v (arg 1 ) v (arg 2 ) v (arg 3 ) . . . v (arg 1 ) v (arg 2 ) v (arg 3 ) . . . Attention mechanism to h DAT pred j h ACC pred j h NOM pred j h pred j FNN of 1 (x l , y l ) x ul Generator PAS Generator Training using x l x ul validator embeddings v ( ) * s DAT pred j s ACC pred j s NOM pred j Error Raw Corpus q(G(x l ), y l ) G(x) V (x)</formula><p>Corpus Corpus Validator <ref type="figure">Figure 1</ref>: The overall model of adversarial training with a raw corpus. The PAS generator G(x) and validator V (x). The validator takes inputs from the generator as a form of the attention mechanism. The validator itself is a simple feed-forward network with inputs of j-th predicate and its argument representations: {h</p><formula xml:id="formula_2">pred j , h case k pred j }.</formula><p>The validator returns scores for three cases and they are used for both the supervised training of the validator and the unsupervised training of the generator. The supervised training of the generator is not included in this <ref type="figure">figure.</ref> dencies to their predicates. Sentence (2) does not have the nominative case marker が(ga). It is hid- den by the topic case marker は(wa). Therefore, a case analysis model has to find the correct NOM case argument 列車(train).</p><p>Sentence <ref type="formula" target="#formula_5">(3)</ref> is an example sentence for zero anaphora resolution. Zero anaphora resolution is a task to find arguments that do not have direct de- pendencies to their predicates. At the second pred- icate "巻き込まれた"(was involved), the correct nomi- native argument is "タクシー"(taxi), while this does not have direct dependencies to the second predi- cate. A zero anaphora resolution model has to find "タクシー"(taxi) from the sentence, and assign it to the NOM case of the second predicate.</p><p>In the zero anaphora resolution task, some cor- rect arguments are not specified in the article. This is called as exophora. We consider "author" and "reader" arguments as exophora ( <ref type="bibr" target="#b5">Hangyo et al., 2013</ref>). They are frequently dropped from Japanese natural sentences. Sentence (4) is an example of dropped nominative arguments. In this sentence, the nominative argument is "あなた" (you), but "あ なた" (you) does not appear in the sentence. This is also included in zero anaphora resolution. Ex- cept these special arguments of exophora, we fo- cus on intra-sentential anaphora resolution in the same way as ( <ref type="bibr" target="#b20">Shibata et al., 2016;</ref><ref type="bibr" target="#b7">Iida et al., 2016;</ref><ref type="bibr" target="#b16">Ouchi et al., 2017;</ref><ref type="bibr" target="#b12">Matsubayashi and Inui, 2017)</ref>. We also attach NULL labels to cases that predicates do not have.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Generative Adversarial Networks</head><p>Generative adversarial networks are originally proposed in image generation tasks <ref type="bibr" target="#b2">(Goodfellow et al., 2014;</ref><ref type="bibr" target="#b18">Salimans et al., 2016;</ref><ref type="bibr" target="#b21">Springenberg, 2015)</ref>. In the original model in <ref type="bibr" target="#b2">Goodfellow et al. (2014)</ref>, they propose a generator G and a discrim- inator D. The discriminator D is trained to dev- ide the real data distribution p data (x) and images generated from the noise samples z (i) ∈ D z from noise prior p(z). The discriminator loss is</p><formula xml:id="formula_3">L D = − E x∼p data (x) [log D(x)] +E z∼pz(z) [log(1 − D(G(z)))] ,<label>(1)</label></formula><p>and they train the discriminator by minimizing this loss while fixing the generator G. Similarly, the generator G is trained through minimizing</p><formula xml:id="formula_4">L G = 1 |D z | i log 1 − D(G(z (i) )) ,<label>(2)</label></formula><p>while fixing the discriminator D. By doing this, the discriminator tries to descriminate the gener- ated images from real images, while the genera- tor tries to generate images that can deceive the adversarial discriminator. This training scheme is applied for many generative tasks including sen- tence generation <ref type="bibr" target="#b22">(Subramanian et al., 2017)</ref>, ma- chine translation ( <ref type="bibr" target="#b0">Britz et al., 2017)</ref>, dialog gener- ation ( <ref type="bibr" target="#b9">Li et al., 2017)</ref>, and text classification (Liu et al., 2017a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Proposed Adversarial Training Using Raw Corpus</head><p>Japanese PAS analysis and many other syntactic analyses in NLP are not purely generative, and we can make use of a raw corpus instead of the numer- ical noise distribution p(z). In this work, we use an adversarial training method using a raw corpus, combined with ordinary supervised learning using an annotated corpus. Let x l ∈ D l indicate labeled data and p(x l ) indicate their label distribution. We also use unlabeled data x ul ∈ D ul later. Our gen- erator G can be trained by the cross entropy loss with labeled data:</p><formula xml:id="formula_5">L G/SL = −E x l ,y∼p(x l ) log G(x l ) .<label>(3)</label></formula><p>Supervised training of the generator works by minimizing this loss. Note that we follow the no- tations of <ref type="bibr" target="#b22">Subramanian et al. (2017)</ref> in this subsec- tion.</p><p>In addition, we train a so-called validator against the generator errors. We use the term "val- idator" instead of "discriminator" for our adversar- ial training. Unlike the discriminator that is used for dividing generated images and real images, our validator is used to score the generator results. As- sume that y l is the true labels and G(x l ) is the pre- dicted label distribution of data x l from the gener- ator. We define the labels of the generator errors as:</p><formula xml:id="formula_6">q(G(x l ), y l ) = δ arg max[G(x l )], y l ,<label>(4)</label></formula><p>where δ i,j = 1 only if i = j, otherwise δ i,j = 0. This means that q is equal to 1 if the argument that the generator predicts is correct, otherwise 0. We use this generator error for training labels of the following validator. The inputs of the validator are both the generator outputs G(x) and data x ∈ D. The validator can be written as V (G(x)). The validator V is trained with labeled data x l by</p><formula xml:id="formula_7">L V /SL = −E x l ,y∼q(G(x l ),y l ) log V (G(x l )) ,<label>(5)</label></formula><p>while fixing the generator G. This equation means that the validator is trained with labels of the gen- erator error q(G(x l ), y l ).</p><p>Once the validator is trained, we train the gen- erator with an unsupervised method. The genera- tor G is trained with unlabeled data x ul ∈ D ul by minimizing the loss</p><formula xml:id="formula_8">L G/U L = − 1 |D ul | i log V (G(x (i) ul )) ,<label>(6)</label></formula><p>while fixing the validator V . This generator train- ing loss using the validator can be explained as fol- lows. The generator tries to increase the validator scores to 1, while the validator is fixed. If the val- idator is well-trained, it returns scores close to 1 for correct PAS labels that the generator outputs, and 0 for wrong labels. Therefore, in Equation (6), the generator tries to predict correct labels in order to increase the scores of fixed validator. Note that the validator has a sigmoid function for the output of scores. Therefore output scores of the validator are in <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>.</p><p>We first conduct the supervised training of gen- erator network with Equation (3). After this, fol- lowing <ref type="bibr" target="#b2">Goodfellow et al. (2014)</ref>, we use k-steps of the validator training and one-step of the gener- ator training. We also alternately conduct l-steps of supervised training of the generator. The entire loss function of this adversarial training is</p><formula xml:id="formula_9">L = L G/SL + L V /SL + L G/U L .<label>(7)</label></formula><p>Our contribution is that we propose the validator and train it against the generator errors, instead of discriminating generated data from real data. <ref type="bibr" target="#b18">Salimans et al. (2016)</ref> explore the semi-supervised learning using adversarial training for K-classes image classification tasks. They add a new class of images that are generated by the generator and classify them. <ref type="bibr" target="#b14">Miyato et al. (2016)</ref> propose virtual adversar- ial training for semi-supervised learning. They ex- ploit unlabeled data for continuous smoothing of data distributions based on the adversarial pertur- bation of <ref type="bibr" target="#b3">Goodfellow et al. (2015)</ref>. These stud- ies, however, do not use the counterpart neural net- works for learning structures of unlabeled data.</p><p>In our Japanese PAS analysis model, the gener- ator corresponds to the head-selection-based neu- ral network for Japanese anaphora resolution. <ref type="figure">Fig- ure 1</ref> shows the entire model. The labeled data correspond to the annotated corpora and the labels correspond to the PAS argument labels. The unla- beled data correspond to raw corpora. We explain the details of the generator and the validator neural networks in Sec.3.3 and Sec.3.4 in turn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Generator of PAS Analysis</head><p>The generator predicts the probabilities of argu- ments for each of the NOM, ACC and DAT cases of a predicate. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, the gener- ator consists of a sentence encoder and an argu- ment selection model. In the sentence encoder, we  For the sentence encoder, inputs are given as a sequence of embeddings v(x), each of which con- sist of word x, its inflection from, POS and de- tailed POS. They are concatenated and fed into the bi-LSTM layers. The bi-LSTM layers read these embeddings in forward and backward order and outputs the distributed representations of a predi- cate and a candidate argument: h pred j and h arg i . Note that we also use the exophora entities, i.e., an author and a reader, as argument candidates. Therefore, we use specific embeddings for them. These embeddings are not generated by the bi- LSTM layers but are directly used in the argument selection model. We also use path embeddings to capture a de- pendency relation between a predicate and its candidate argument as used in <ref type="bibr" target="#b17">Roth and Lapata (2016)</ref>. Although <ref type="bibr" target="#b17">Roth and Lapata (2016)</ref> use a one-way LSTM layer to represent the depen- dency path from a predicate to its potential argu- ment, we use a bi-LSTM layer for this purpose. We feed the embeddings of words and POS tags to the bi-LSTM layer. In this way, the result- ing path embedding represents both predicate-to- argument and argument-to-predicate paths. We concatenate the bidirectional path embeddings to generate h path ij , which represents the dependency relation between the predicate j and its candidate argument i.</p><p>For the argument selection model, we apply the argument selection model ( <ref type="bibr" target="#b23">Zhang et al., 2017)</ref> to evaluate the relation between a predicate and its potential argument for each argument case. In the argument selection model, a single FNN is repeat- edly used to calculate scores for a child word and its head candidate word, and then a softmax func- tion calculates normalized probabilities of candi- date heads. We use three different FNNs that cor- respond to the NOM, ACC and DAT cases. These three FNNs have the same inputs of the distributed representations of j-th predicate h pred j , i-th can- didate argument h arg i and path embedding h path ij between the predicate j and candidate argument i. The FNNs for NOM, ACC and DAT compute the argument scores s case k arg i ,pred j , where case k ∈ {NOM, ACC, DAT}. Finally, the softmax func- tion computes the probability p(arg i |pred j ,case k ) of candidate argument i for case k of j-th predicate as:</p><formula xml:id="formula_10">p(arg i |pred j ,case k ) = exp s case k arg i ,pred j arg i exp s case k arg i ,pred j .<label>(8)</label></formula><p>Our argument selection model is similar to the neural network structure of <ref type="bibr" target="#b12">Matsubayashi and Inui (2017)</ref>. However, <ref type="bibr" target="#b12">Matsubayashi and Inui (2017)</ref> does not use RNNs to read the whole sentence. Their model is also designed to choose a case la- bel for a pair of a predicate and its argument can- didate. In other words, their model can assign the same case label to multiple arguments by itself, while our model does not. Since case arguments are almost unique for each case of a predicate in Japanese, <ref type="bibr" target="#b12">Matsubayashi and Inui (2017)</ref> select the argument that has the highest probability for each case, even though probabilities of case arguments are not normalized over argument candidates. The model of <ref type="bibr" target="#b16">Ouchi et al. (2017)</ref> has the same prob- lem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Validator</head><p>We exploit a validator to train the generator us- ing a raw corpus. It consists of a two-layer FNN to which embeddings of a predicate and its argu- ments are fed. For predicate j, the input of <ref type="figure">the  FNN</ref>   <ref type="bibr">x)</ref>. We use dropout of 0.5 at the FNN input and hidden layer.</p><p>The generator and validator networks are cou- pled by the attention mechanism, or the weighted sum of the validator embeddings. As shown in Equation <ref type="formula" target="#formula_10">(8)</ref>, we compute a probability distribu- tion of candidate arguments. We use the weighted sum of embeddings v (x) of candidate arguments to compute the input representations of the valida- tor:</p><formula xml:id="formula_11">h case k pred j = E x∼p(arg i ) [v (x)] = arg i p(arg i |pred j ,case k )v (arg i ).</formula><p>This summation is taken over candidate arguments in the sentence and the exophora entities. Note that we use embeddings v (x) for the validator that are different from the embeddings v(x) for the generator, in order to separate the computa- tion graphs of the generator and the validator neu- ral networks except the joint part. We use this weighted sum by the softmax outputs instead of the argmax function. This allows the backpropa- gation through this joint. We also feed the embed- ding of a predicate to the validator:</p><formula xml:id="formula_12">h pred j = v (pred j ).<label>(9)</label></formula><p>Note that the validator is a simple neural net- work compared with the generator. The validator has limited inputs of predicates and arguments and no inputs of other words in sentences. This allows the generator to overwhelm the validator during the adversarial training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Type Value</head><p>Size of hidden layers of FNNs 1,000 Size of Bi-LSTMs 256 Dim. of word embedding 100 Dim. of POS, detailed POS, inflection form tags 10, 10, 9 Minibatch size for the generator and validator 16, 1  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Implementation Details</head><p>The neural networks are trained using backprop- agation. The backpropagation has been done to the word and POS tags. We use Adam ( <ref type="bibr" target="#b8">Kingma and Ba, 2015)</ref> at the initial training of the genera- tor network for the gradient learning rule. In ad- versarial learning, Adagrad ( <ref type="bibr" target="#b1">Duchi et al., 2010</ref>) is suitable because of the stability of learning. We use pre-trained word embeddings from 100M sen- tences from Japanese web corpus by word2vec ( <ref type="bibr" target="#b13">Mikolov et al., 2013</ref>). Other embeddings and hid- den weights of neural networks are randomly ini- tialized.</p><p>For adversarial training, we first train the gen- erator for two epochs by the supervised method, and train the validator while fixing the generator for another epoch. This is because the validator training preceding the generator training makes the validator result worse. After this, we alter- nately do the unsupervised training of the genera- tor (L G/U L ), k-times of supervised training of the validator (L V /SL ) and l-times of supervised train- ing of the generator (L G/SL ).</p><formula xml:id="formula_13">We use the N (L G/U L )/N (L G/SL ) = 1/4 and N (L V /SL )/N (L G/SL ) = 1/4</formula><p>, where N (·) indi- cates the number of sentences used for training. Also we use minibatch of 16 sentences for both supervised and unsupervised training of the gen- erator, while we do not use minibatch for validator training. Therefore, we use k = 16 and l = 4. Other parameters are summarized in <ref type="table" target="#tab_3">Table 2</ref>.  The results of case analysis (Case) and zero anaphora resolution (Zero). We use F- measure as an evaluation measure. ‡ denotes that the improvement is statistically significant at p &lt; 0.05, compared with Gen using paired t-test. In KWDLC, lead three sen- tences of each document are annotated with PAS structures including zero pronouns. For a raw cor- pus, we use a Japanese web corpus created by <ref type="bibr" target="#b4">Hangyo et al. (2012)</ref>, which has no duplicated sen- tences with KWDLC. This raw corpus is automat- ically parsed by the Japanese dependency parser KNP. We focus on intra-sentential anaphora resolu- tion, and so we apply a preprocess to KWDLC. We regard the anaphors whose antecedents are in the preceding sentences as NULL in the same way as <ref type="bibr" target="#b15">Ouchi et al. (2015)</ref>; <ref type="bibr" target="#b20">Shibata et al. (2016)</ref>. <ref type="table" target="#tab_4">Tables  3 and 4</ref> list the statistics of KWDLC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KWDLC NOM ACC DAT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># of dep 7,224 1,555 448 # of zero 6,453 515 1,248</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>We use the exophora entities, i.e., an author and a reader, following the annotations in KWDLC. We also assign author/reader labels to the follow- ing expressions in the same way as <ref type="bibr" target="#b5">Hangyo et al. (2013)</ref>; <ref type="bibr" target="#b20">Shibata et al. (2016)</ref>:</p><p>author "私" (I), "僕" (I), "我々" (we), "弊社" (our company) <ref type="bibr">1</ref> The KWDLC corpus is available at http://nlp. ist.i.kyoto-u.ac.jp/EN/index.php?KWDLC reader "あなた" (you), "君" (you), "客" (customer), "皆様" (you all)</p><p>Following <ref type="bibr" target="#b15">Ouchi et al. (2015)</ref> and <ref type="bibr" target="#b20">Shibata et al. (2016)</ref>, we conduct two kinds of analysis: (1) case analysis and (2) zero anaphora resolution. Case analysis is the task to determine the correct case labels when predicates and their arguments have direct dependencies but their case markers are hid- den by surface markers, such as topic markers. Zero anaphora resolution is a task to find certain case arguments that do not have direct dependen- cies to their predicates in the sentence.</p><p>Following <ref type="bibr" target="#b20">Shibata et al. (2016)</ref>, we exclude predicates that the same arguments are filled in multiple cases of a predicate. This is relatively uncommon and 1.5 % of the whole corpus are ex- cluded. Predicates are marked in the gold depen- dency parses. Candidate arguments are just other tokens than predicates. This setting is also the same as <ref type="bibr" target="#b20">Shibata et al. (2016)</ref>.</p><p>All performances are evaluated with micro- averaged F-measure ( <ref type="bibr" target="#b20">Shibata et al., 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Results</head><p>We compare two models: the supervised genera- tor model (Gen) and the proposed semi-supervised model with adversarial training (Gen+Adv). We also compare our models with two previous mod- els: <ref type="bibr" target="#b15">Ouchi et al. (2015)</ref> and <ref type="bibr" target="#b20">Shibata et al. (2016)</ref>, whose performance on the KWDLC corpus is re- ported. <ref type="table" target="#tab_6">Table 5</ref> lists the experimental results. Our mod- els (Gen and Gen+Adv) outperformed the previ- ous models. Furthermore, the proposed model with adversarial training (Gen+Adv) was signifi- cantly better than the supervised model (Gen).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with Data Augmentation Model</head><p>We also compare our GAN-based approach with data augmentation techniques. A data augmenta- tion approach is used in <ref type="bibr" target="#b11">Liu et al. (2017b)</ref>. They automatically process raw corpora and make drops of words with some rules. However, it is difficult to directly apply their approach to Japanese PAS analysis because Japanese zero-pronoun depends on dependency trees. If we make some drops of ar- guments of predicates in sentences, this can cause lacks of nodes in dependency trees. If we prune some branches of dependency trees of the sen- tence, this cause the data bias problem.   <ref type="table">Table 7</ref>: The comparisons of Gen+Adv with Gen and the data augmentation model (Gen+Aug). ‡ denotes that the improvement is statistically sig- nificant at p &lt; 0.05, compared with Gen+Aug.</p><p>Therefore we use existing training corpora and word embeddings for the data augmentation. First we randomly choose an argument word w in the training corpus and then swap it with another word w with the probability of p(w, w ). We choose top-20 nearest words to the original word w in the pre-trained word embedding as candidates of swapped words. The probability is defined as p(w, w ) ∝ [v(w) v(w )] r , where r = 10. This probability is normalized by top-20 nearest words. We then merge this pseudo data and the original training corpus and train the model in the same way with the Gen model. We conducted several experiments and found that the model trained with the same amount of the pseudo data as the training corpus achieved the best result. <ref type="table">Table 7</ref> shows the results of the data augmen- tation model and the GAN-based model. Our Gen+Adv model performs better than the data augmented model. Note that our data augmenta- tion model does not use raw corpora directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Result Analysis</head><p>We report the detailed performance for each case in <ref type="table" target="#tab_8">Table 6</ref>. Among the three cases, zero anaphora resolution of the ACC and DAT cases is notori- ously difficult. This is attributed to the fact that these ACC and DAT cases are fewer than the NOM case in the corpus as shown in <ref type="table" target="#tab_5">Table 4</ref>. However, we can see that our proposed model, Gen+Adv, performs much better than the previous models es- pecially for the ACC and DAT cases. Although the number of training instances of ACC and DAT is much smaller than that of NOM, our semi- supervised model can learn PAS for all three cases using a raw corpus. This indicates that our model can work well in resource-poor cases.</p><p>We analyzed the results of Gen+Adv by com- paring with Gen and the model of <ref type="bibr" target="#b20">Shibata et al. (2016)</ref>. Here, we focus on the ACC and DAT cases because their improvements are notable.</p><formula xml:id="formula_14">• "パックは 洗って、 分別して リサイクルに 出 さなきゃいけないので 手間がかかる。"</formula><p>It is bothersome to wash, classify and recycle spent packs.</p><p>In this sentence, the predicates "洗って" (wash), "分 別して" (classify), "(リサイクルに) 出す" (recycle) takes the same ACC argument, "パック" (pack). This is not so easy for Japanese PAS analysis be- cause the actual ACC case marker "を" (wo) of "パック" (pack) is hidden by the topic marker "は" (wa). The Gen+Adv model can detect the cor- rect argument while the model of <ref type="bibr" target="#b20">Shibata et al. (2016)</ref> fails. In the Gen+Adv model, each pred- icate gives a high probability to "パック" (pack) as an ACC argument and finally chooses this. We found many examples similar to this and speculate that our model captures a kind of selectional pref- erences.</p><p>The next example is an error of the DAT case by the Gen+Adv model.</p><p>• "各専門分野も お任せ下さい。" please leave every professional field (to φ)</p><p>The gold label of this DAT case (to φ) is NULL be- cause this argument is not written in the sentence. However, the Gen+Adv model judged the DAT ar- gument as "author". Although we cannot specify φ as "author" only from this sentence, "author" is a possible argument depending on the context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="0">2 4 6 8 112 14 16 18 Epoch</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Validator Analysis</head><p>We also evaluate the performance of the valida- tor during the adversarial training with raw cor- pora. <ref type="figure" target="#fig_3">Figure 3</ref> shows the validator performance and the generator performance of Zero on the de- velopment set. The validator score is evaluated with the outputs of generator. We notice that the NOM case and the other two cases have different curves in both graphs. This can be explained by the speciality of the NOM case. The NOM case has much more author/reader expressions than the other cases. The prediction of author/reader expressions depends not only on se- lectional preferences of predicates and arguments but on the whole of sentences. Therefore the val- idator that relies only on predicate and argument representations cannot predict author/reader ex- pressions well.</p><p>In the ACC and DAT cases, the scores of the generator and validator increase in the first epochs. This suggests that the validator learns the weak- ness of the generator and vice versa. However, in later epochs, the scores of the generator increase with fluctuation, while the scores of the validator saturates. This suggests that the generator gradu- ally becomes stronger than the validator. <ref type="bibr" target="#b20">Shibata et al. (2016)</ref> proposed a neural network- based PAS analysis model using local and global features. This model is based on the non-neural model of <ref type="bibr" target="#b15">Ouchi et al. (2015)</ref>. They achieved state-of-the-art results on case analysis and zero anaphora resolution using the KWDLC corpus. They use an external resource to extract selectional preferences. Since our model uses an external re- source, we compare our model with the models of <ref type="bibr" target="#b20">Shibata et al. (2016)</ref> and <ref type="bibr" target="#b15">Ouchi et al. (2015)</ref>. <ref type="bibr" target="#b16">Ouchi et al. (2017)</ref> proposed a semantic role labeling-based PAS analysis model using Grid- RNNs. <ref type="bibr" target="#b12">Matsubayashi and Inui (2017)</ref> proposed a case label selection model with feature-based neu- ral networks. They conducted their experiments on NAIST Text Corpus (NTC) ( <ref type="bibr" target="#b6">Iida et al., 2007</ref><ref type="bibr" target="#b7">Iida et al., , 2016</ref>. NTC consists of newspaper articles, and does not include the annotations of author/reader expressions that are common in Japanese natural sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>(</head><label></label><figDesc>j-th predicate ) NOM: Raw Labeled ACC:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The generator of PAS. The sentence encoder is a three-layer bi-LSTM to compute the distributed representations of a predicate and its arguments: h pred i and h arg i. The argument selection model is two-layer feedforward neural networks to compute the scores, s case k arg i ,pred j , of candidate arguments for each case of a predicate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Following Shibata et al.</head><label></label><figDesc>(2016), we use the KWDLC (Kyoto University Web Document Leads Corpus) corpus (Hangyo et al., 2012) for our ex- periments. 1 This corpus contains various Web documents, such as news articles, personal blogs, and commerce sites.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Left: validator scores with the development set during adversarial training epochs. Right: generator scores for Zero with the development set during adversarial training epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Parameters for neural network structure 
and training. 

KWDLC 
# snt 
# of dep # of zero 

Train 
11,558 
9,227 
8,216 
Dev. 
1,585 
1,253 
821 
Test 
2,195 
1,780 
1,669 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 : KWDLC data statistics.</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 4 : KWDLC training data statistics for each case.</head><label>4</label><figDesc></figDesc><table>Case Zero 

Ouchi+ 2015 
76.5 42.1 
Shibata+ 2016 89.3 53.4 

Gen 
91.5 56.2 
Gen+Adv 
92.0  ‡ 58.4  ‡ 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>The detailed results of case analysis and zero anaphora resolution for the NOM, ACC and DAT 
cases. Our models outperform the existing models in all cases. All values are evaluated with F-measure. 

Case Zero 

Gen 
91.5 56.2 
Gen+Aug 91.2 57.0 

Gen+Adv 92.0  ‡ 58.4  ‡ 

</table></figure>

			<note place="foot">Bi-LSTM Bi-LSTM Bi-LSTM Bi-LSTM Bi-LSTM Bi-LSTM Bi-LSTM Bi-LSTM embedding embedding embedding embedding</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We proposed a novel Japanese PAS analysis model that exploits a semi-supervised adversarial train-ing. The generator neural network learns Japanese PAS and selectional preferences, while the valida-tor is trained against the generator errors. This val-idator enables the generator to be trained from raw corpora and enhance it with external knowledge. In the future, we will apply this semi-supervised training method to other NLP tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work was supported by JST CREST Grant Number JPMJCR1301, Japan and JST ACT-I Grant Number JPMJPR17U8, Japan.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Effective domain mixing for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Britz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reid</forename><surname>Pryzant</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W17-4712" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="118" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<idno>UCB/EECS-2010-24</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-08" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Building a diverse document leads corpus annotated with semantic relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masatsugu</forename><surname>Hangyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Pacific Asia Conference on Language, Information, and Computation. Faculty of Computer Science</title>
		<meeting>the 26th Pacific Asia Conference on Language, Information, and Computation. Faculty of Computer Science<address><addrLine>Bali,Indonesia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="535" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Japanese zero reference resolution considering exophora and author/reader mentions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masatsugu</forename><surname>Hangyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="924" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Annotating a japanese text corpus with predicate-argument and coreference relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryu</forename><surname>Iida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mamoru</forename><surname>Komachi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Linguistic Annotation Workshop</title>
		<meeting>the Linguistic Annotation Workshop<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06-28" />
			<biblScope unit="page" from="132" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Intrasentential subject zero anaphora resolution using multi-column convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryu</forename><surname>Iida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Torisawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong-Hoon</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canasai</forename><surname>Kruengkrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Kloetzer</surname></persName>
		</author>
		<ptr target="https://aclweb.org/anthology/D16-" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1244" to="1254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adversarial learning for neural dialogue generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2157" to="2169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adversarial multi-task learning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generating and exploiting large-scale pseudo training data for zero pronoun resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Nan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="102" to="111" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Revisiting the design issues of local models for japanese predicate-argument structure analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichiroh</forename><surname>Matsubayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="128" to="133" />
		</imprint>
	</monogr>
	<note>Short Papers). Asian Federation of Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1301.3781</idno>
		<ptr target="http://arxiv.org/abs/1301.3781" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributional smoothing by virtual adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Shin-Ichi Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin</forename><surname>Nakae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Joint case argument identification for japanese predicate argument structure analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroki</forename><surname>Ouchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural modeling of multi-predicate interactions for japanese predicate argument structure analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroki</forename><surname>Ouchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1591" to="1600" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural semantic role labeling with dependency path embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1192" to="1202" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A discriminative approach to japanese zero anaphora resolution with large-scale lexicalized case frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryohei</forename><surname>Sasano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 5th International Joint Conference on Natural Language Processing. Asian Federation of Natural Language Processing</title>
		<editor>Chiang Mai, Thailand</editor>
		<meeting>5th International Joint Conference on Natural Language Processing. Asian Federation of Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="758" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural network-based model for japanese predicate argument structure analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomohide</forename><surname>Shibata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1235" to="1244" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Unsupervised and semi-supervised learning with categorical generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adversarial generation of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai</forename><surname>Rajeswar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Dutil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Representation Learning for NLP</title>
		<meeting>the 2nd Workshop on Representation Learning for NLP<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="241" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dependency parsing as head selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers. Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="665" to="676" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
