<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:26+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Jointly Extracting Relations with Class Ties via Effective Deep Ranking</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Ye</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Chao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhunchen</forename><surname>Luo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
						</author>
						<title level="a" type="main">Jointly Extracting Relations with Class Ties via Effective Deep Ranking</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1810" to="1820"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1166</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Connections between relations in relation extraction, which we call class ties, are common. In distantly supervised scenario, one entity tuple may have multiple relation facts. Exploiting class ties between relations of one entity tuple will be promising for distantly supervised relation extraction. However, previous models are not effective or ignore to model this property. In this work, to effectively leverage class ties, we propose to make joint relation extraction with a unified model that integrates convolutional neural network (CNN) with a general pairwise ranking framework, in which three novel ranking loss functions are introduced. Additionally, an effective method is presented to relieve the severe class imbalance problem from NR (not relation) for model training. Experiments on a widely used dataset show that leverag-ing class ties will enhance extraction and demonstrate the effectiveness of our model to learn class ties. Our model outperforms the baselines significantly, achieving state-of-the-art performance.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Relation extraction (RE) aims to classify the relations between two given named entities from natural-language text. Supervised machine learn- ing methods require numerous labeled data to work well. With the rapid growth of volume of relation types, traditional methods can not keep up with the step for the limitation of labeled data. In order to narrow down the gap of data spar- sity, <ref type="bibr" target="#b12">Mintz et al. (2009)</ref> propose distant supervi- sion (DS) for relation extraction, which automati- cally generates training data by aligning a knowl- edge facts database (ie. <ref type="bibr">Freebase (Bollacker et al., 2008)</ref>) with texts.</p><p>Class ties mean the connections between rela- tions in relation extraction. In general, we con- clude that class ties can have two types: weak class ties and strong class ties. Weak class ties mainly involve the co-occurrence of relations such as place of birth and place lived, CEO of and founder of. On the contrary, strong class ties mean that relations have latent logical entailments. Take the two relations of capital of and city of for example, if one entity tuple has the rela- tion of capital of, it must express the relation fact of city of, because the two relations have the entailment of capital of ⇒ city of. Obviously the opposite induction is not correct. Further take the sentence of "Jonbenet told me that her mother [Patsy Ramsey] e 1 never left <ref type="bibr">[Atlanta]</ref> e 2 since she was born." in DS scenario for exam- ple. This sentence expresses two relation facts which are place of birth and place lived. How- ever, the word "born" is a strong bios to extract place of birth, so it may not be easy to predict the relation of place lived, but if we can incorporate the weak ties between the two relations, extracting place of birth will provide evidence for prediction of place lived.</p><p>Exploiting class ties is necessary for DS based relation extraction. In DS scenario, there is a chal- lenge that one entity tuple can have multiple rela-tion facts as shown in <ref type="table" target="#tab_0">Table 1</ref>, which is called rela- tion overlapping <ref type="bibr" target="#b6">(Hoffmann et al., 2011;</ref><ref type="bibr" target="#b17">Surdeanu et al., 2012)</ref>. However, the relations of one entity tuple can have class ties mentioned above which can be leveraged to enhance relation extraction for it narrowing down potential searching spaces and reducing uncertainties between relations when predicting unknown relations. If one pair entities has CEO of, it will contain founder of with high possibility.</p><p>To exploit class ties between relations, we pro- pose to make joint extraction for all positive labels of one entity tuple with considering pairwise con- nections between positive and negative labels in- spired by <ref type="bibr" target="#b3">(Fürnkranz et al., 2008;</ref><ref type="bibr" target="#b23">Zhang and Zhou, 2006</ref>). As the two relations with class ties shown in <ref type="table" target="#tab_0">Table 1</ref>, by joint extraction of two relations, we can maintain the class ties (co-occurrence) of them from training samples to be learned by potential model, and then leverage this learned information to extract instances with unknown relations, which can not be achieved by separated extraction for it dividing labels apart losing information of co- occurrence. To classify positive labels from nega- tive ones, we adopt pairwise ranking to rank pos- itive ones higher than negative ones, exploiting pairwise connections between them. In a word, joint extraction exploits class ties between rela- tions and pairwise ranking classify positive labels from negative ones. Furthermore, combining in- formation across sentences will be more appropri- ate for joint extraction which provides more infor- mation from other sentences to extract each rela- tion ( <ref type="bibr" target="#b26">Zheng et al., 2016;</ref><ref type="bibr" target="#b9">Lin et al., 2016)</ref>. In <ref type="table" target="#tab_0">Table  1</ref>, sentence #1 is the evidence for place of birth, but it also expresses the meaning of "living in someplace", so it can be aggregated with sentence #2 to extract place lived. Meanwhile, the word of "hometown" in sentence #2 can provide evidence for place of birth which should be combined with sentence #1 to extract place of birth.</p><p>In this work, we propose a unified model that integrates pairwise ranking with CNN to exploit class ties. Inspired by the effectiveness of deep learning for modeling sentences ( <ref type="bibr" target="#b8">LeCun et al., 2015)</ref>, we use CNN to encode sentences. Simi- lar to <ref type="bibr" target="#b15">(Santos et al., 2015;</ref><ref type="bibr" target="#b9">Lin et al., 2016</ref>  <ref type="figure">Figure 1</ref>: The main architecture of our model. embedded sentences into one bag representation vector aiming to aggregate information across sen- tences, after that we measure the similarity be- tween bag representation and relation class in real- valued space. With two variants for combining sentences, three novel pairwise ranking loss func- tions are proposed to make joint extraction. Be- sides, to relieve the bad impact of class imbalance from NR (not relation) <ref type="bibr" target="#b7">(Japkowicz and Stephen, 2002</ref>) for training our model, we cut down loss propagation from NR class during training.</p><p>Our experimental results on dataset of <ref type="bibr" target="#b13">Riedel et al. (2010)</ref> are evident that: (1) Our model is much more effective than the baselines; (2) Lever- aging class ties will enhance relation extraction and our model is efficient to learn class ties by joint extraction; (3) A much better model can be trained after relieving class imbalance from NR.</p><p>Our contributions in this paper can be encapsu- lated as follows:</p><p>• We propose to leverage class ties to enhance relation extraction. An effective deep ranking model which integrates CNN and pairwise rank- ing framework is introduced to exploit class ties.</p><p>• We propose an effective method to relieve the impact of data imbalance from NR for model training.</p><p>• Our method achieves state-of-the-art perfor- mance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>We summarize related works on two main as- pects:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Distant Supervision Relation Extraction</head><p>Previous works on DS based RE ignore or are not effective to leverage class ties between rela-tions. <ref type="bibr" target="#b13">Riedel et al. (2010)</ref> introduce multi-instance learning to relieve the wrong labelling problem, ignoring class ties. Afterwards, <ref type="bibr" target="#b6">Hoffmann et al. (2011)</ref> and <ref type="bibr" target="#b17">Surdeanu et al. (2012)</ref> model this prob- lem by multi-instance multi-label learning to ex- tract overlapping relations. Though they also pro- pose to make joint extraction of relations, they only use information from single sentence losing information from other sentences. <ref type="bibr" target="#b4">Han and Sun (2016)</ref> try to use Markov logic model to capture consistency between relation labels, on the con- trary, our model leverages deep ranking to learn class ties automatically.</p><p>With the remarkable success of deep learning in CV and NLP ( <ref type="bibr" target="#b8">LeCun et al., 2015)</ref>, deep learning has been applied to relation extraction ( <ref type="bibr" target="#b21">Zeng et al., 2014</ref><ref type="bibr" target="#b20">Zeng et al., , 2015</ref><ref type="bibr" target="#b15">Santos et al., 2015;</ref><ref type="bibr" target="#b9">Lin et al., 2016)</ref>, the specific deep learning architecture can be CNN ( <ref type="bibr" target="#b21">Zeng et al., 2014</ref>), RNN ( ), etc. <ref type="bibr" target="#b20">Zeng et al. (2015)</ref> propose a piecewise convolu- tional neural network with multi-instance learning for DS based relation extraction, which improves the precision and recall significantly. Afterwards, <ref type="bibr" target="#b9">Lin et al. (2016)</ref> introduce the mechanism of at- tention ( <ref type="bibr" target="#b11">Luong et al., 2015;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2014)</ref> to select the sentences to relieve the wrong la- belling problem and use all the information across sentences. However, the two deep learning based models only make separated extraction thus can not model class ties between relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deep Learning to Rank</head><p>Deep learning to rank has been widely used in many problems to serve as a classification model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Model</head><p>In this section, we first conclude the notations used in this paper, then we introduce the used CNN for sentence embedding, afterwards, we present our algorithm of how to learn class ties be- tween relations of one entity tuple.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notation</head><p>We define the relation classes as L = {1, 2, · · · , C}, entity tuples as</p><formula xml:id="formula_0">T = {t i } M i=1 and mentions 1 as X = {x i } N i=1</formula><p>. Dataset is constructed as follows: for entity tuple t i ∈ T and its rela- tion class set L i ⊆ L, we collect all the men- tions X i that contain t i , the dataset we use is</p><formula xml:id="formula_1">D = {(t i , L i , X i )} H i=1 . Given a data (t k , L k , X k ) ∈ {(t i , L i , X i )} H i=1</formula><p>, the sentence embeddings of X k encoded by CNN are defined as</p><formula xml:id="formula_2">S k = {s i } |X k | i=1</formula><p>and we use class embeddings W ∈ R |L|×d to represent the relation classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">CNN for Sentence Embedding</head><p>We take the effective CNN architecture adopted from ( <ref type="bibr" target="#b20">Zeng et al., 2015;</ref><ref type="bibr" target="#b9">Lin et al., 2016</ref>) to encode sentence and we briefly introduce CNN in this sec- tion. More details of our CNN can be obtained from previous work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Words Representations</head><p>• Word Embedding Given a word embedding matrix V ∈ R l w ×d 1 where l w is the size of word dictionary and d 1 is the dimension of word embedding, the words of a mention x = {w 1 , w 2 , · · · , w n } will be represented by real- valued vectors from V .</p><p>• Position Embedding The position embedding of a word measures the distance from the word to entities in a mention. We add position em- beddings into words representations by append- ing position embedding to word embedding for every word. Given a position embedding matrix P ∈ R l p ×d 2 where l p is the number of distances and d 2 is the dimension of position embeddings, the dimension of words representations becomes</p><formula xml:id="formula_3">d w = d 1 + d 2 × 2.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Convolution, Piecewise max-pooling</head><p>After transforming words in x to real-valued vectors, we get the sentence q ∈ R n×d w . The set of kernels K is</p><formula xml:id="formula_4">{K i } d s i=1</formula><p>where d s is the number of kernels. Define the window size as d win and given one kernel K k ∈ R d win ×d w , the convolution operation is defined as follows:</p><formula xml:id="formula_5">m [i] = q [i:i+d win −1] K k + b [k] (1)</formula><p>where m is the vector after conducting convolu- tion along q for n − d win + 1 times and b ∈ R d s is the bias vector. For these vectors whose indexes out of range of <ref type="bibr">[1, n]</ref>, we replace them with zero vectors. By piecewise max-pooling, when pooling, the sentence is divided into three parts: m [p 0 :p 1 ] , m [p 1 :p 2 ] and m [p 2 :p 3 ] (p 1 and p 2 are the positions of entities, p 0 is the beginning of sentence and p 3 is the end of sentence). This piecewise max-pooling is defined as follows:</p><formula xml:id="formula_6">z [j] = max(m [p j−1 :p j ] )<label>(2)</label></formula><p>where z ∈ R 3 is the result of mention x processed by kernel K k ; 1 ≤ j ≤ 3. Given the set of kernels K, following the above steps, the mention x can be embedded to o where o ∈ R d s * 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Non-Linear Layer, Regularization</head><p>To learn high-level features of mentions, we ap- ply a non-linear layer after pooling layer. After that, a dropout layer is applied to prevent over- fitting. We define the final fixed sentence repre- sentation as s</p><formula xml:id="formula_7">∈ R d f (d f = d s * 3). s = g(o) • h<label>(3)</label></formula><p>where g(·) is a non-linear function and we use tanh(·) in this paper; h is a Bernoulli random vec- tor with probability p to be 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning Class Ties by Joint Extraction with Pairwise Ranking</head><p>As mentioned above, to learn class ties, we propose to make joint extraction with consider- ing pairwise connections between positive labels and negative ones. Pairwise ranking is applied to achieve this goal. Besides, combining informa- tion across sentences is necessary for joint extrac- tion. More specifically, as shown in <ref type="figure">Figure 2</ref>, from down to top, all information from sentences is pre-propagated to provide enough information for joint extraction. From top to down, pairwise rank- ing jointly extracting positive relations by combin- ing losses, which are back-propagated to CNN to learn class ties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Combining Information across Sentences</head><p>We propose two options to combine sentences to provide enough information for joint extraction.  <ref type="figure">Figure 2</ref>: Illustration of mechanism of our model to model class ties between relations.</p><p>• AVE The first option is average method. This method regards all the sentences equally and di- rectly average the values in all dimensions of sen- tence embedding. This AVE function is defined as follows:</p><formula xml:id="formula_8">s = 1 n s i ∈S k s i (4)</formula><p>where n is the number of sentences and s is the representation vector combining all sentence em- beddings. Because it weights the importance of sentences equally, this method may bring much noise data from two aspects: (1) the wrong la- belling data; (2) irrelated mentions for one relation class, for all sentences containing the same entity tuple being combined together to construct the bag representation.</p><p>• ATT The second one is a sentence-level atten- tion algorithm used by <ref type="bibr" target="#b9">Lin et al. (2016)</ref> to mea- sure the importance of sentences aiming to relieve the wrong labelling problem. For every sentence, ATT will calculate a weight by comparing the sen- tence to one relation. We first calculate the similar- ity between one sentence embedding and relation class as follows:</p><formula xml:id="formula_9">e j = a · W [c] · s j<label>(5)</label></formula><p>where e j is the similarity between sentence em- bedding s j and relation class c and a is a bias fac- tor. In this paper, we set a as 0.5. Then we apply</p><formula xml:id="formula_10">Softmax to rescale e (e = {e i } |X k | i=1 ) to [0, 1].</formula><p>We get the weight α j for s j as follows:</p><formula xml:id="formula_11">α j = exp(e j ) e i ∈e exp(e i )<label>(6)</label></formula><p>so the function to merge s with ATT is as follows:</p><formula xml:id="formula_12">s = |X k | i=1 α i · s i<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Joint Extraction by Combining Losses to Learn Class Ties</head><p>Firstly, we have to present the score function to measure the similarity between s and relation c.</p><p>• Score Function We use dot function to produce score for s to be predicted as relation c. The score function is as follows:</p><formula xml:id="formula_13">F(s, c) = W [c] · s<label>(8)</label></formula><p>There are other options for score function. In , they propose a margin based loss function that measures the similarity between s and W <ref type="bibr">[c]</ref> by distance. Because score function is not an important issue in our model, we adopt dot function, also used by <ref type="bibr" target="#b15">Santos et al. (2015)</ref> and <ref type="bibr" target="#b9">Lin et al. (2016)</ref>, as our score function. Now we start to introduce the ranking loss func- tion.</p><p>Pairwise ranking aims to learn the score func- tion F(s, c) that ranks positive classes higher than negative ones. This goal can be summarized as follows:</p><formula xml:id="formula_14">∀c + ∈ L k , ∀c − ∈ L−L k : F(s, c + ) &gt; F(s, c − )+β<label>(9)</label></formula><p>where β is a margin factor which controls the min- imum margin between the positive scores and neg- ative scores.</p><p>To learn class ties between relations, we extend the formula (9) to make joint extraction and we propose three ranking loss functions with variants of combining sentences. Followings are the pro- posed loss functions:</p><p>• with AVE (Variant-1) We define the margin- based loss function with option of AVE to aggre- gate sentences as follows:</p><formula xml:id="formula_15">G [ave] = c + ∈L k ρ[0, σ + − F(s, c + )] + +ρ|L k |[0, σ − + F(s, c − )] +<label>(10)</label></formula><p>where [0, · ] + = max(0, · ); ρ is the rescale fac- tor, σ + is positive margin and σ − is negative mar- gin. Similar to <ref type="bibr" target="#b15">Santos et al. (2015)</ref> and , this loss function is designed to rank pos- itive classes higher than negative ones controlled by the margin of σ + − σ − . In reality, F(s, c + ) will be higher than σ + and F(s, c − ) will be lower than σ − . In our work, we set ρ as 2, σ + as 2.5 and σ − as 0.5 adopted from <ref type="bibr" target="#b15">Santos et al. (2015)</ref>. Similar to <ref type="bibr" target="#b19">Weston et al. (2011)</ref> and <ref type="bibr" target="#b15">Santos et al. (2015)</ref>, we update one negative class at every training round but to balance the loss between positive classes and negative ones, we multiply |L k | before the right term in function (10) to ex- pand the negative loss. We apply mini-bach based stochastic gradient descent (SGD) to minimize the loss function. The negative class is chosen as the one with highest score among all negative classes ( <ref type="bibr" target="#b15">Santos et al., 2015)</ref>, i.e.:</p><formula xml:id="formula_16">c − = argmax c∈L−L k F(s, c)<label>(11)</label></formula><p>• with ATT (Variant-2) Now we define the loss function for the option of ATT to combine sen- tences as follows:</p><formula xml:id="formula_17">G [att] = c + ∈L k (ρ[0, σ + − F(s c + , c + )] + +ρ[0, σ − + F(s c + , c − )] + )<label>(12)</label></formula><p>where s c means the attention weights of represen- tation s are merged by comparing sentence embed- dings with relation class c and c − is chosen by the following function:</p><formula xml:id="formula_18">c − = argmax c∈L−L k F(s c + , c)<label>(13)</label></formula><p>which means we update one negative class in ev- ery training round. We keep the values of ρ, σ + and σ − same as values in function (10). According to this loss function, we can see that: for each class c + ∈ L k , it will capture the most related information from sentences to merge s c + , then rank F(s c + , c + ) higher than all negative scores which each is</p><formula xml:id="formula_19">F(s c + , c − ) (c − ∈ L − L k ).</formula><p>We use the same update algorithm to minimize this loss.</p><p>• Extended with ATT (Variant-3) According to function (12), for each c + , we only select one neg- ative class to update the parameters, which only considers the connections between positive classes and negative ones, ignoring connections between positive classes, so we extend function (12) to bet- ter exploit class ties by considering the connec- tions between positive classes. We give out the extended loss function as follows:</p><formula xml:id="formula_20">G [Exatt] = c * ∈L k ( c + ∈L k ρ[0, σ + − F(s c * , c + )] + +ρ[0, σ − + F(s c * , c − )] + )<label>(14)</label></formula><p>Pro.</p><p>Training Test SemE. 17.63% 16.71% Riedel 72.52% 96.26%  <ref type="formula" target="#formula_7">(13)</ref>, we select c − as follows:</p><formula xml:id="formula_21">c − = argmax c∈L−L k F(s c * , c)<label>(15)</label></formula><p>and we use the same method to update this loss function as discussed above. From the function (14), we can see that: for c * ∈ L k , after merging the bag representation s with c * , we share s with all the other positive classes and update the class embeddings of other positive classes with s, in this way, the connections between positive classes can be captured and learned by our model. In loss function <ref type="formula" target="#formula_15">(10)</ref>, <ref type="formula" target="#formula_6">(12)</ref> and <ref type="formula" target="#formula_20">(14)</ref>, we com- bine losses from all positive labels to make joint extraction to capture the class ties among rela- tions. Suppose we make separated extraction, the losses from positive labels will be divided apart and will not get enough information of connec- tions between positive labels, comparing to joint extraction. Connections between positive labels and negative ones are exploited by controlling margins: σ + and σ − .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Relieving Impact of NR</head><p>In relation extraction, the dataset will always contain certain negative samples which do not ex- press relations classified as NR (not relation). Ta- ble 2 presents the proportion of NR samples in SemEval-2010 Task 8 dataset 2 <ref type="bibr">(Erk and Strapparava, 2010</ref>) and dataset from <ref type="bibr" target="#b13">Riedel et al. (2010)</ref>, which shows almost data is about NR in the latter dataset. Data imbalance will severely affect the model training and cause the model only sensitive to classes with high proportion <ref type="bibr" target="#b5">(He and Garcia, 2009)</ref>.</p><p>In order to relieve the impact of NR in DS based relation extraction, we cut the propagation of loss from NR, which means if relation c is NR, we set its loss as 0. Our method is similar to <ref type="bibr" target="#b15">Santos et al. (2015)</ref> with slight variance. <ref type="bibr" target="#b15">Santos et al. (2015)</ref> directly omit the NR class embedding, but we keep it. If we use ATT method to combine informa- tion across sentences, we can not omit NR class</p><formula xml:id="formula_22">Algorithm 1: Merging loss function of Variant-3 input : L, (t k , L k , X k ) and S k ; output: G [Exatt] ; 1 G [Exatt] ← 0; 2 for c * ∈ L k do 3</formula><p>Merge representation s c * by function (5), (6), <ref type="formula" target="#formula_12">(7)</ref>;</p><formula xml:id="formula_23">4 for c + ∈ L k do 5 if c + is not NR then 6 G [Exatt] ← G [Exatt] + ρ[0, σ + − F(s c * , c + )] + ; 7 c − ← argmax c∈L−L k F(s c * , c); 8 G [Exatt] ← G [Exatt] + ρ[0, σ − + F(s c * , c − )] + ; 9 return G [Exatt] ;</formula><p>embedding according to function (6) and <ref type="formula" target="#formula_12">(7)</ref>, on the contrary, it will be updated from the negative classes' loss. In Algorithm 1, we give out the pseudocodes of merging loss with Variant-3 and considering to relieve the impact of NR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Evaluation Criteria</head><p>We conduct our experiments on a widely used dataset, developed by <ref type="bibr" target="#b13">Riedel et al. (2010)</ref>  Following <ref type="bibr" target="#b12">Mintz et al. (2009)</ref>, we adopt held- out evaluation framework in all experiments. Ag- gregated precision/recall curves are drawn and precision@N (P@N) is reported to illustrate the model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Settings</head><p>Word Embeddings. We use a word2vec tool that is gensim 3 to train word embeddings on NYT corpus. Similar to <ref type="bibr" target="#b9">Lin et al. (2016)</ref>, we keep the words that appear more than 100 times to construct word dictionary and use "UNK" to represent the other ones. Hyper-parameter Settings. Three-fold valida- tion on the training dataset is adopted to tune the parameters following <ref type="bibr" target="#b17">Surdeanu et al. (2012)</ref>. We use grid search to determine the optimal hyper- parameters. We select word embedding size from {50, 100, 150, 200, 250, 300}. Batch size is tuned from {80, 160, 320, 640}. We determine learning rate among {0.01, 0.02, 0.03, 0.04}. The window size of convolution is tuned from {1, 3, 5}. We keep other hyper-parameters same as <ref type="bibr" target="#b20">Zeng et al. (2015)</ref>: the number of kernels is 230, position em- bedding size is 5 and dropout rate is 0.5. <ref type="table" target="#tab_4">Table 3</ref> shows the detailed parameter settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter</head><note type="other">Name Symbol Value Window size d win 3 Sentence. emb. dim. d f 690 Word. emb. dim. d 1 50 Position. emb. dim. d 2 5 Batch size B 160 Learning rate λ 0.03 Dropout pos. p 0.5</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparisons with Baselines</head><p>Baseline. We compare our model with the fol- lowing baselines:</p><p>• Mintz ( <ref type="bibr" target="#b12">Mintz et al., 2009</ref>) the original dis- tantly supervised model.</p><p>• MultiR (Hoffmann et al., 2011) a multi- instance learning based graphical model which aims to address overlapping relation problem.</p><p>• MIML (Surdeanu et al., 2012) also solv- ing overlapping relations in a multi-instance multi- label framework.</p><p>• PCNN+ATT ( <ref type="bibr" target="#b9">Lin et al., 2016</ref>) the state-of- the-art model in dataset of <ref type="bibr" target="#b13">Riedel et al. (2010)</ref> which applies ATT to combine the sentences. Results and Discussion. We compare our three variants of loss functions with the baselines and the results are shown in <ref type="figure" target="#fig_2">Figure 3</ref>. From the re- sults we can see that: (1) Rank + AVE (Variant- 1) achieves comparable results with PCNN+ATT; (2) Rank + ATT (Variant-2) and Rank + ExATT (Variant-3) significantly outperform PCNN + ATT with much higher precision and slightly higher re- call in whole view; (3) Rank + ExATT (Variant-3) exhibits the best performances comparing with all the other methods including PCNN + ATT, Rank + AVE and Rank + ATT.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Impact of Joint Extraction and Class Ties</head><p>In this section, we conduct experiments to re- veal the effectiveness of our model to learn class ties with three variant loss functions mentioned above, and the impact of class ties for relation ex- traction. As mentioned above, we make joint ex- traction to learn class ties, so to achieve the goal of this set of experiments, we compare joint ex- traction with separated extraction. To make sep- arated extraction, we divide the labels of entity tuple into single label and for one relation label we only select the sentences expressing this rela- tion, then we use this dataset to train our model with the three variant loss functions. We conduct experiments with Rank + AVE (Variant-1), Rank + ATT (Variant-2) and Rank + ExATT (Variant- 3) relieving impact of NR. Aggregated P/R curves are drawn and precisions@N (100, 200, · · · , 500) are reported to show the model performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P@N(%)</head><p>100 <ref type="formula" target="#formula_6">200 300 400 500</ref>   <ref type="table">Table 4</ref>: Precisions for top 100, 200, 300, 400, 500 and average of them for impact of joint extraction and class ties. Experimental results are shown in <ref type="figure" target="#fig_4">Figure 4</ref> and <ref type="table">Table 4</ref>. From the results we can see that: (1) For Rank + ATT and Rank + ExATT, joint extraction exhibits better performance than separated extrac- tion, which demonstrates class ties will improve relation extraction and the two methods are effec- tive to learn class ties; (2) For Rank + AVE, sur- prisingly joint extraction does not keep up with separated extraction. For the second phenomenon, the explanation may lie in the AVE method to ag- gregate sentences will incorporate noise data con- sistent with the finding in <ref type="bibr" target="#b9">Lin et al. (2016)</ref>. When make joint extraction, we will combine all sen- tences containing the same entity tuple no matter which class type is expressed, so it will engender much noise if we only combine them equally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Comparisons of Variant Joint Extractions</head><p>To make joint extraction, we have proposed three variant loss functions including Rank + AVE, Rank + ATT and Rank + ExATT in the above dis- cussion and <ref type="figure" target="#fig_2">Figure 3</ref> shows that the three vari- ants achieve different performances. In this ex- periment, we aim to compare the three variants in detail. We conduct the experiments with the three variants under the setting of relieving im- P@N(%) 100 200 300 400 500 Ave. R.+AVE 81.3 76.4 74.6 69.6 66.0 73.6 R.+ATT 87.9 84.3 78.0 74.9 70.3 79.1 R.+ExATT 83.5 82.2 78.7 77.2 73.1 79.0  pact of NR and joint extraction. We draw the P/R curves and report the top N (100, 200, · · · , 500) precisions to compare model performance with the three variants.</p><p>From the results as shown in <ref type="figure" target="#fig_5">Figure 5</ref> and Ta- ble 5 we can see that: (1) Comparing Rank + AVE with Rank + ATT, from the whole view, they can achieve the similar maximal recall point, but Rank + ATT exhibits higher precision in all range of recall; (2) Comparing Rank + ATT with Rank + ExATT, Rank + ExATT achieves much better per- formance with broader range of recall and higher precision in almost range of recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Impact of NR Relation</head><p>The goal of this experiment is to inspect how much relation of NR can affect the model perfor- mance. We use Rank + AVE, Rank + ATT, Rank + ExATT under the setting of relieving impact of NR or not to conduct experiments. We draw the aggregated P/R curves as shown in <ref type="figure" target="#fig_6">Figure 6</ref>, from which we can see that after relieving the impact of NR, the model performance can be improved significantly.</p><p>Then we further evaluate the impact of NR for convergence behavior of our model in model train-  <ref type="figure">Figure 7</ref>: Impact of NR for model convergence. "+NR" means not relieving NR impact; "-NR" is opposite.</p><p>ing. Also with the three variant loss functions, in each iteration, we record the maximal value of F- measure 4 to represent the model performance at current epoch. Model parameters are tuned for 15 times and the convergence curves are shown in <ref type="figure">Figure 7</ref>. From the result, we can find out: "+NR" converges quicker than "-NR" and arrives to the fi- nal score at the around 11 or 12 epoch. In general, "-NR" converges more smoothly and will achieve better performance than "+NR" in the end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Case Study</head><p>Joint vs. Sep. Extraction (Class Ties). We randomly select an entity tuple (Cuyahoga County, Cleveland) from test set to see its scores for every relation class with the method of Rank + ATT un- der the setting of relieving impact of NR with joint extraction and separated extraction. This entity tu- ple have two relations: /location/./county seat and /location/./contains, which derive from the same root class and they have weak class ties for they all relating to topic of "location". We rescale the scores by adding value 10. The results are shown in <ref type="figure" target="#fig_7">Figure 8</ref>, from which we can see that: un- der joint extraction setting, the two gold relations have the highest scores among the other relations but under separated extraction setting, only /loca- tion/./contains can be distinguished from the neg- ative relations, which demonstrates that joint ex- traction is better than separated extraction by cap- turing the class ties between relations.</p><p>4 F = 2 * P * R/(P + R)  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Works</head><p>In this paper, we leverage class ties to enhance relation extraction by joint extraction using pair- wise ranking combined with CNN. An effective method is proposed to relieve the impact of NR for model training. Experimental results on a widely used dataset show that leveraging class ties will enhance relation extraction and our model is ef- fective to learn class ties. Our method significantly outperforms the baselines.</p><p>In the future, we will focus on two aspects: (1) Our method in this paper considers pairwise inter- sections between labels, so to better exploit class ties, we will extend our method to exploit all other labels' influences on each relation for relation ex- traction, transferring second-order to high-order (Zhang and Zhou, 2014); (2) We will focus on other problems by leveraging class ties between labels, specially on multi-label learning problems ( <ref type="bibr" target="#b28">Zhou et al., 2012</ref>) such as multi-category text cat- egorization ( <ref type="bibr" target="#b14">Rousu et al., 2005</ref>) and multi-label image categorization ( <ref type="bibr" target="#b22">Zha et al., 2008</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>In image retrieval, Zhao et al. (2015) apply deep semantic ranking for multi-label image retrieval. In text matching, Severyn and Moschitti (2015) adopt learning to rank combined with deep CNN for short text pairs matching. In traditional super- vised relation extraction, Santos et al. (2015) de- sign a pairwise loss function based on CNN for single label relation extraction. Based on the ad- vantage of deep learning to rank, we propose pair- wise learning to rank (LTR) (Liu, 2009) combined with CNN in our model aiming to jointly extract multiple relations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>and has been used by Hoffmann et al. (2011), Surdeanu et al. (2012), Zeng et al. (2015) and Lin et al. (2016). The dataset aligns Freebase relation facts with the New York Times corpus, in which train- ing mentions are from 2005-2006 corpus and test mentions from 2007.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Performance comparison of our model and the baselines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Results for impact of joint extraction and class ties with methods of Rank + AVE, Rank + ATT and Rank + ExATT under the setting of relieving impact of NR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Results for comparisons of variant joint extractions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Results for impact of relation NR with methods of Rank + AVE, Rank + ATT and Rank + ExATT. "+NR" means not relieving impact of NR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: The output scores for every relation with method of Rank + ATT. The top is under joint extraction setting; the bottom is under separated extraction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Training instances generated by freebase.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>), we use class embeddings to represent relation classes. The whole model architecture is presented in Fig- ure 1. We first use CNN to embed sentences, then we introduce two variant methods to combine the</figDesc><table>x2 
x1 
xn 

s1 
s2 
sn 

s 

c1 
c2 
cm 

í µí± [# $ ] &amp; í µí± 

class 
embedding 

encoded by CNN 

sentence 
embedding 

bag representation 
vector 
combine 
sentences 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 : The proportions of NR samples from SemEval-2010 Task 8 dataset and Riedel dataset.</head><label>2</label><figDesc></figDesc><table>Similar to function </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Hyper-parameter settings. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Precisions for top 100, 200, 300, 400, 500 
and average of them for Rank + AVE, Rank + ATT 
and Rank + ExATT. 

</table></figure>

			<note place="foot" n="1"> The sentence containing one certain entity is called mention.</note>

			<note place="foot" n="2"> This is a dataset for relation extraction in traditional supervision framework.</note>

			<note place="foot" n="3"> http://radimrehurek.com/gensim/models/word2vec.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Firstly, we would like to thank Xianpei Han and Kang Liu for their valuable suggestions on the ini-tial version of this paper, which have helped a lot to improve the paper. Secondly, we also want to express gratitudes to the anonymous review-ers for their hard work and kind comments, which will further improve our work in the future. This work was supported by the National High-tech Re-search and Development Program (863 Program) (No. 2014AA015105) and National Natural Sci-ence Foundation of China (No. 61602490).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><forename type="middle">D</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD</title>
		<meeting>KDD</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Proceedings of SemEval. The Association for Computer Linguistics</title>
		<editor>Katrin Erk and Carlo Strapparava</editor>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multilabel classification via calibrated label ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Fürnkranz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eyke</forename><surname>Hüllermeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneldo</forename><surname>Loza Mencía</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Brinker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="133" to="153" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Global distant supervision for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2950" to="2956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning from imbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edwardo</forename><forename type="middle">A</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1263" to="1284" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Knowledgebased weak supervision for information extraction of overlapping relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congle</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACLHLT</title>
		<meeting>ACLHLT</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The class imbalance problem: A systematic study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathalie</forename><surname>Japkowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaju</forename><surname>Stephen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intelligent data analysis</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="429" to="449" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural relation extraction with selective attention over instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2124" to="2133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to rank for information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="225" to="331" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-IJCNLP. Association for Computational Linguistics</title>
		<meeting>ACL-IJCNLP. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECML-PKDD</title>
		<meeting>ECML-PKDD</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning hierarchical multi-category text classification models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Rousu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandor</forename><surname>Szedmak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of ICML</title>
		<meeting>eeding of ICML</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="744" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Classifying relations by ranking with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cicero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of ACL</title>
		<meeting>eeding of ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="626" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to rank short text pairs with convolutional deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="373" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-instance multi-label learning for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Relation classification via multi-level attention cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">WSABIE: scaling up to large vocabulary image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2764" to="2770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction via piecewise convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="17" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of COLING</title>
		<meeting>eeding of COLING</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Joint multi-label multi-instance learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Sheng</forename><surname>Zheng-Jun Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengfu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. IEEE</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multilabel neural networks with applications to functional genomics and text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Ling</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1338" to="1351" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A review on multi-label learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Ling</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1819" to="1837" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep semantic ranking based hashing for multi-label image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongzhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1556" to="1564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Aggregating inter-sentence information to enhance relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senzhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshe</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attentionbased bidirectional long short-term memory networks for relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingchen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of ACL</title>
		<meeting>eeding of ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">207</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-instance multi-label learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Ling</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng-Jun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Feng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">176</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2291" to="2320" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
