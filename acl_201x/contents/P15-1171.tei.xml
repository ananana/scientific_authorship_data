<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Inducing Word and Part-of-Speech with Pitman-Yor Hidden Semi-Markov Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 26-31, 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kei</forename><surname>Uchiumi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Denso IT Laboratory</orgName>
								<orgName type="department" key="dep2">The Institute of Statistical Mathematics</orgName>
								<address>
									<addrLine>Inc. Shibuya Cross Tower 28F 2-15-1 Shibuya, 10-3 Midori-cho</addrLine>
									<settlement>Tokyo, Tachikawa city Tokyo</settlement>
									<country>Japan, Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Tsukahara</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Denso IT Laboratory</orgName>
								<orgName type="department" key="dep2">The Institute of Statistical Mathematics</orgName>
								<address>
									<addrLine>Inc. Shibuya Cross Tower 28F 2-15-1 Shibuya, 10-3 Midori-cho</addrLine>
									<settlement>Tokyo, Tachikawa city Tokyo</settlement>
									<country>Japan, Japan</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daichi</forename><surname>Mochihashi</surname></persName>
							<email>daichi@ism.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Denso IT Laboratory</orgName>
								<orgName type="department" key="dep2">The Institute of Statistical Mathematics</orgName>
								<address>
									<addrLine>Inc. Shibuya Cross Tower 28F 2-15-1 Shibuya, 10-3 Midori-cho</addrLine>
									<settlement>Tokyo, Tachikawa city Tokyo</settlement>
									<country>Japan, Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Inducing Word and Part-of-Speech with Pitman-Yor Hidden Semi-Markov Models</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1774" to="1782"/>
							<date type="published">July 26-31, 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a nonparametric Bayesian model for joint unsupervised word seg-mentation and part-of-speech tagging from raw strings. Extending a previous model for word segmentation, our model is called a Pitman-Yor Hidden Semi-Markov Model (PYHSMM) and considered as a method to build a class n-gram language model directly from strings, while integrating character and word level information. Experimental results on standard datasets on Japanese, Chinese and Thai revealed it outperforms previous results to yield the state-of-the-art accuracies. This model will also serve to analyze a structure of a language whose words are not identified a priori.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Morphological analysis is a staple of natural lan- guage processing for broad languages. Especially for some East Asian languages such as Japanese, Chinese or Thai, word boundaries are not explic- itly written, thus morphological analysis is a cru- cial first step for further processing. Note that also in Latin and old English, scripts were orig- inally written with no word indications (scripta continua), but people felt no difficulty reading them. Here, morphological analysis means word segmentation and part-of-speech (POS) tagging.</p><p>For this purpose, supervised methods have of- ten been employed for training. However, to train such supervised classifiers, we have to pre- pare a large amount of training data with cor- rect annotations, in this case, word segmentation and POS tags. Creating and maintaining these data is not only costly but also very difficult, be- cause generally there are no clear criteria for ei- ther "correct" segmentation or POS tags. In fact, since there are different standards for Chinese word segmentation, widely used SIGHAN Bake- off dataset <ref type="bibr" target="#b3">(Emerson, 2005</ref>) consists of multiple parts employing different annotation schemes.</p><p>Lately, this situation has become increasingly important because there are strong demands for processing huge amounts of text in consumer gen- erated media such as Twitter, Weibo or Facebook ( <ref type="figure" target="#fig_0">Figure 1</ref>). They contain a plethora of colloquial expressions and newly coined words, including sentiment expressions such as emoticons that can- not be covered by fixed supervised data.</p><p>To automatically recognize such linguistic phe- nomena beyond small "correct" supervised data, we have to extract linguistic knowledge from the statistics of strings themselves in an unsupervised fashion. Needless to say, such methods will also contribute to analyzing speech transcripts, classic texts, or even unknown languages. From a scien- tific point of view, it is worth while to find "words" and their part-of-speech purely from a collection of strings without any preconceived assumptions.</p><p>To achieve that goal, there have been two kinds of approaches: heuristic methods and statisti- cal generative models. Heuristic methods are based on basic observations such that word bound- aries will often occur at the place where predic- tive entropy of characters is large (i.e. the next character cannot be predicted without assuming the next word). By formulating such ideas as search or MDL problems of given coding length 1 , word boundaries are found in an algorithmic fash- ion ( <ref type="bibr" target="#b27">Zhikov et al., 2010;</ref><ref type="bibr" target="#b14">Magistry and Sagot, 2013)</ref>. However, such methods have difficulty in- corporating higher-order statistics beyond simple heuristics, such as word transitions, word spelling formation, or word length distribution. Moreover, they usually depends on tuning parameters like thresholds that cannot be learned without human intervention. In contrast, statistical models are ready to in- corporate all such phenomena within a consistent statistical generative model of a string, and often prove to work better than heuristic methods <ref type="bibr" target="#b5">(Goldwater et al., 2006;</ref><ref type="bibr" target="#b16">Mochihashi et al., 2009</ref>). In fact, the statistical methods often include the cri- teria of heuristic methods at least in a conceptual level, which is noted in ( <ref type="bibr" target="#b16">Mochihashi et al., 2009)</ref> and also explained later in this paper. In a statisti- cal model, each word segmentation w of a string s is regarded as a hidden stochastic variable, and the unsupervised learning of word segmentation is formulated as a maximization of a probability of w given s: argmax w p(w|s) .</p><p>This means that we want the most "natural" seg- mentation w that have a high probability in a lan- guage model p(w|s). Lately, <ref type="bibr" target="#b1">Chen et al. (2014)</ref> proposed an interme- diate model between heuristic and statistical mod- els as a product of character and word HMMs. However, these two models do not have informa- tion shared between the models, which is not the case with generative models.</p><p>So far, these approaches only find word seg- mentation, leaving part-of-speech information be- hind. These two problems are not actually in- dependent but interrelated, because knowing the part-of-speech of some infrequent or unknown word will give contextual clues to word segmen- tation, and vice versa. For example, in Japanese can be segmented into not only /// (plum/too/peach/too), but also into // (plum/peach/peach), which is ungrammati- cal. However, we could exclude the latter case if we leverage knowledge that a state sequence N/P/N/P is much more plausible in Japanese than N/N/N from the part-of-speech information. <ref type="bibr" target="#b24">Sirts and Alumäe (2012)</ref> treats a similar problem of POS induction with unsupervised morphological segmentation, but they know the words in advance and only consider segmentation within a word.</p><p>For this objective, we attempt to maximize the joint probability of words and tags:</p><formula xml:id="formula_1">argmax w,z p(w, z|s) ∝ p(w, z, s)<label>(2)</label></formula><p>From the expression above, this amounts to building a generative model of a string s with words w and tags z along with an associated infer- ence procedure. We solve this problem by extend- ing previous generative model of word segmenta- tion. Note that heuristic methods are never able to model the hidden tags, and only statistical genera- tive models can accommodate this objective. This paper is organized as follows. In Sec- tion 2, we briefly introduce NPYLM ( <ref type="bibr" target="#b16">Mochihashi et al., 2009</ref>) on which our extension is based. Sec- tion 3 extends it to include hidden states to yield a hidden semi-Markov models <ref type="bibr" target="#b17">(Murphy, 2002</ref>), and we describe its inference procedure in Section 4. We conduct experiments on some East Asian lan- guages in Section 5. Section 6 discusses implica- tions of our model and related work, and Section 7 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Nested Pitman-Yor Language Model</head><p>Our joint model of words and states is an extension of the Nested Pitman-Yor Language Model ( <ref type="bibr" target="#b16">Mochihashi et al., 2009</ref>) of a string, which in turn is an extension of a Bayesian n-gram lan- guage model called Hierarchical Pitman-Yor Lan- guage Model (HPYLM) <ref type="bibr" target="#b25">(Teh, 2006</ref>).</p><p>HPYLM is a nonparametric Bayesian model of n-gram distribution based on the Pitman-Yor pro- cess <ref type="bibr" target="#b21">(Pitman and Yor, 1997</ref>) that generates a dis- crete distribution G as G ∼ PY(G 0 , d, θ). Here, d is a discount factor, "parent" distribution G 0 is called a base measure and θ controls how similar G is to G 0 in expectation. In HPYLM, n-gram distribution G n = {p(w t |w t−1 · · · w t−(n−1) )} is assumed to be generated from the Pitman-Yor pro- cess</p><formula xml:id="formula_2">G n ∼ PY(G n−1 , d n , θ n ) ,<label>(3)</label></formula><p>where the base measure G n−1 is an (n−1)-gram distribution generated recursively in accordance with (3). Note that there are different G n for each n-gram history h = w t−1 · · · w t−(n−1) . When we reach the unigram G 1 and need to use a base mea- sure G 0 , i.e. prior probabilities of words, HPYLM usually uses a uniform distribution over the lexi- con.</p><p>However, in the case of unsupervised word seg- mentation, every sequence of characters could be a word, thus the size of the lexicon is unbounded. Moreover, prior probability of forming a word should not be uniform over all sequences of char- acters: for example, English words rarely begin with 'gme' but tend to end with '-ent' like in seg- ment. To model this property, NPYLM assumes that word prior G 0 is generated from character HPYLM to model a well-formedness of w. In practice, to avoid dependency on n in the charac- ter model, we used an ∞-gram VPYLM <ref type="bibr" target="#b15">(Mochihashi and Sumita, 2008</ref>) in this research. Finally, NPYLM gives an n-gram probability of word w given a history h recursively by integrating out G n ,</p><formula xml:id="formula_3">p(w|h) = c(w|h)−d·t hw θ+c(h) + θ+d·t h · θ+c(h) p(w|h ′ ) ,<label>(4)</label></formula><p>where h ′ is the shorter history of (n − 1)-grams. c(w|h), c(h) = ∑ w c(w|h) are n-gram counts of w appearing after h, and t hw , t h · = ∑ w t hw are associated latent variables explained below. In case the history h is already empty at the unigram, p(w|h ′ ) = p 0 (w) is computed from the character ∞-grams for the word w = c 1 · · · c k :</p><formula xml:id="formula_4">p 0 (w) = p(c 1 · · · c k ) (5) = ∏ k i=1 p(c i |c i−1 · · · c 1 ) .<label>(6)</label></formula><p>In practice, we further corrected (6) so that a word length follows a mixture of Poisson distributions. For details, see <ref type="bibr" target="#b16">(Mochihashi et al., 2009</ref>).</p><p>When we know word segmentation w of the data, the probability above can be computed by adding each n-gram count of w given h to the model, i.e. increment c(w|h) in accordance with a hierarchical Chinese restaurant process associ- ated with HPYLM ( <ref type="figure" target="#fig_1">Figure 2</ref>). When each n-gram count called a customer is inferred to be actually generated from (n − 1)-grams, we send its proxy customer for smoothing to the parent restaurant and increment t hw , and this process will recurse. Notice that if a word w is never seen in w, its proxy customer is eventually sent to the parent restaurant of unigrams. In that case 2 , w is decom- posed to its character sequence c 1 · · · c k and this is added to the character HPYLM in the same way, making it a little "clever" about possible word spellings. Inference Because we do not know word seg- mentation w beforehand, we begin with a trivial segmentation in which every sentence is a single word 3 . Then, we iteratively refine it by sampling a new word segmentation w(s) of a sentence s in a Markov Chain Monte Carlo (MCMC) frame- work using a dynamic programming, as is done with PCFG by <ref type="bibr" target="#b9">(Johnson et al., 2007)</ref> shown in <ref type="figure">Fig- ure 3</ref> where we omit MH steps for computational reasons. Further note that every hyperparameter d n , θ n of NPYLM can be sampled from the poste- rior in a Bayesian fashion, as opposed to heuristic methods that rely on a development set for tuning. For details, see Teh (2006).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Pitman-Yor Hidden Semi-Markov Models</head><p>NPYLM is a complete generative model of a string, that is, a hierarchical Bayesian n-gram lan-   guage model combining words and characters. It can also be viewed as a way to build a Bayesian word n-gram language model directly from a se- quence of characters, without knowing "words" a priori.</p><formula xml:id="formula_5">Input: a collection of strings S Add initial segmentation w(s) to Θ for j = 1 · · · J do for s in randperm (S) do</formula><formula xml:id="formula_6">z t z t+1 w t−1 w t w t+1 Observation s · · · · · · · · · · · ·</formula><p>One possible drawback of it is a lack of part-of- speech: as described in the introduction, grammat- ical states will contribute much to word segmenta- tion. Also, from a computational linguistics point of view, it is desirable to induce not only words from strings but also their part-of-speech purely from the usage statistics (imagine applying it to an unknown language or colloquial expressions). In classical terms, it amounts to building a class n- gram language model where both class and words are unknown to us. Is this really possible?</p><p>Yes, we can say it is possible. The idea is sim- ple: we augment the latent states to include a hid- den part-of-speech z t for each word w t , which is again unknown as displayed in <ref type="figure" target="#fig_4">Figure 4</ref>. As- suming w t is generated from z t '-th NPYLM, we can draw a generative model of a string s as fol- lows: z 0 = BOS; s = ϵ (an empty string).</p><formula xml:id="formula_7">for t = 1 · · · T do Draw z t ∼ p(z t |z t−1 ) , Draw w t ∼ p(w t |w 1 · · · w t−1 , z t ) , Append w t to s . end for</formula><p>Here, z 0 = BOS and z T +1 = EOS are distin- guished states for beginning and end of a sentence, respectively. For the transition probability of hid- den states, we put a HPY process prior as (Blun- som and Cohn, 2011):</p><formula xml:id="formula_8">p(z t |z t−1 ) ∼ HPY(d, θ)<label>(7)</label></formula><p>with the final base measure being a uniform dis- tribution over the states. The word boundaries are known in (Blunsom and Cohn, 2011), but in our case it is also learned from data at the same time.</p><p>Note that because w t depends on already gener- ated words w 1 · · · w t−1 , our model is considered as an autoregressive HMM rather than a vanilla HMM, as shown in <ref type="figure" target="#fig_4">Figure 4</ref> (w t−1 → w t depen- dency).</p><p>Since segment models like NPYLM have seg- ment lengths as hidden states, they are called semi- Markov models <ref type="bibr" target="#b17">(Murphy, 2002</ref>). In contrast, our model also has hidden part-of-speech, thus we call it a Pitman-Yor Hidden Semi-Markov model (PYHSMM). <ref type="bibr">4</ref> Note that this is considered as a generative counterpart of a discriminative model known as a hidden semi-Markov CRF <ref type="bibr" target="#b22">(Sarawagi and Cohen, 2005</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Inference</head><p>Inference of PYHSMM proceeds in almost the same way as NPYLM in <ref type="figure">Figure 3</ref>: For each sen- tence, first remove the customers associated with the old segmentation similarly to adding them. Af- ter sampling a new segmentation and states, the model is updated by adding new customers in ac- cordance with the new segmentation and hidden states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Sampling words and states</head><p>To sample words and states (part-of-speech) jointly, we first compute inside probabilities for- ward from BOS to EOS and sample backwards from EOS according to the Forward filtering- Backward sampling algorithm <ref type="bibr" target="#b23">(Scott, 2002</ref>). This can be regarded as a "stochastic Viterbi" algorithm that has the advantage of not being trapped in local minima, since it is a valid move of a Gibbs sampler in a Bayesian model. For a word bigram case for simplicity, inside variable α[t] <ref type="bibr">[k]</ref>[z] is a probability that a substring c 1 · · · c t of a string s = c 1 · · · c N is generated with its last k characters being a word, generated from state z as shown in <ref type="figure" target="#fig_5">Figure 5</ref>. From the definition of PYHSMM, this can be computed recursively as follows:</p><formula xml:id="formula_9">α[t][k][z] = L ∑ j=1 K ∑ y=1 p(c t t−k |c t−k t−k−j+1 , z) p(z|y)α[t−k][j][y] . (8)</formula><p>Here, c t s is a substring c s · · · c t and L (≤ t) is the maximum length of a word, and K is the number of hidden states. <ref type="bibr">5</ref> In <ref type="figure" target="#fig_5">Figure 5</ref>, each cell represents α[t] <ref type="bibr">[k]</ref>[z] and a single path connecting from EOS to BOS cor- responds to a word sequence w and its state se- quence z. Note that each cell is not always con- nected to adjacent cells (we omit the arrows), be- cause the length-k substring associated with each cell already subsumes that of neighborhood cells.</p><p>Once w and z are sampled, each w t is added to z t '-th NPYLM to update its statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Efficient computation by the Negative Binomial generalized linear model</head><p>Inference algorithm of PYHSMM has a computa- tional complexity of O(K 2 L 2 N ), where N is a length of the string to analyze. To reduce com- putations it is effective to put a small L of maxi- mum word length, but it might also ignore occa- sionally long words. Since these long words are often predictable from some character level infor- mation including suffixes or character types, in a Type Feature</p><formula xml:id="formula_10">c i Character at time t−i (0 ≤ i ≤ 1) t i</formula><p>Character type at time t−i (0 ≤ i ≤ 4) cont # of the same character types before t ch # of times character types changed within 8 characters before t semi-supervised setting we employ a Negative Bi- nomial generalized linear model (GLM) for set- ting L t adaptively for each character position t in the corpus. Specifically, we model the word length ℓ by a Negative Binomial distribution <ref type="bibr" target="#b2">(Cook, 2009)</ref>:</p><formula xml:id="formula_11">ℓ ∼ NB(ℓ|r, p) = Γ(r+ℓ) Γ(r) ℓ! p ℓ (1 − p) r .<label>(9)</label></formula><p>This counts the number of failures of Bernoulli draws with probability (1−p) before r'th success. For our model, note that Negative Binomial is ob- tained from a Poisson distribution Po(λ) whose parameter λ again follows a Gamma distribution Ga(r, b) and integrated out:</p><formula xml:id="formula_12">p(ℓ|r, b) = ∫ Po(ℓ|λ)Ga(λ|r, b)dλ (10) = Γ(r+ℓ) Γ(r) ℓ! ( b 1+b ) ℓ ( 1 1+b ) r .<label>(11)</label></formula><p>This construction exactly mirrors the Poisson- Gamma word length distribution in ( <ref type="bibr" target="#b16">Mochihashi et al., 2009</ref>) with sampled λ. Therefore, our Neg- ative Binomial is basically a continuous analogue of the word length distribution in NPYLM. <ref type="bibr">6</ref> Since r &gt; 0 and 0 ≤ p ≤ 1, we employ an expo- nential and sigmoidal linear regression</p><formula xml:id="formula_13">r = exp(w T r f ), p = σ(w T p f )<label>(12)</label></formula><p>where σ(x) is a sigmoid function and w r , w p are weight vectors to learn. f is a feature vector com- puted from the substring c 1 · · · c t , including f 0 ≡ 1 for a bias term. <ref type="table" target="#tab_0">Table 1</ref> shows the features we used for this Negative Binomial GLM. Since Neg- ative Binomial GLM is not convex in w r and w p , we endow a Normal prior N(0, σ 2 I) for them and used a random walk MCMC for inference.</p><p>Predicting L t Once the model is obtained, we can set L t adaptively as the time where the cu- mulative probability of ℓ exceeds some threshold θ (we used θ = 0.99). <ref type="table">Table 2</ref> shows the preci- sion of predicting maximum word length learned from 10,000 sentences from each set: it measures whether the correct word boundary in test data is included in the predicted L t . Overall it performs very well with high preci- sion, and works better for longer words that cannot be accommodated with a fixed maximum length. <ref type="table" target="#tab_0">Training Test   Ja  Kyoto corpus  37,400  1,000  BCCWJ OC  20,000  1,000   Zh  SIGHAN MSR  86,924  3,985  SIGHAN CITYU 53,019  1,492  SIGHAN PKU  19,056  1,945  Th</ref> InterBEST Novel 1,000 1,000  <ref type="figure" target="#fig_7">Figure 6</ref> shows the distribution of predicted max- imum lengths for Japanese. Although we used θ = 0.99, it is rather parsimonious but accurate that makes the computation faster. Because this cumulative Negative Binomial prediction is language independent, we believe it might be beneficial for other natural language pro- cessing tasks that require some maximum lengths within which to process the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lang Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>To validate our model, we conducted experiments on several corpora of East Asian languages with no word boundaries.</p><p>Datasets For East Asian languages, we used standard datasets in Japanese, Chinese and Thai as shown in <ref type="table" target="#tab_1">Table 3</ref>. The Kyoto corpus is a collection of sentences from Japanese newspaper ( <ref type="bibr" target="#b12">Kurohashi and Nagao, 1998</ref>) with both word seg- mentation and part-of-speech annotations. BC- CWJ (Balanced Corpus of Contemporary Writ- ten Japanese) is a balanced corpus of written Japanese <ref type="bibr" target="#b13">(Maekawa, 2007)</ref> from the National Institute of Japanese Language and Linguistics, also with both word segmentation and part-of- speech annotations from slightly different crite- ria. For experiments on colloquial texts, we used a random subset of "OC" register from this cor- pus that is comprised of Yahoo!Japan Answers from users. For Chinese, experiments are con- ducted on standard datasets of <ref type="bibr">SIGHAN Bakeoff 2005</ref><ref type="bibr" target="#b3">(Emerson, 2005</ref>; for comparison we used MSR and PKU datasets for simplified Chinese, and the CITYU dataset for traditional Chinese. SIGHAN datasets have word boundaries only, and we conformed to original training/test splits pro- vided with the data. InterBEST is a dataset in Thai used in the InterBEST 2009 word segmen- tation contest <ref type="bibr" target="#b10">(Kosawat, 2009)</ref>. For contrastive purposes, we used a "Novel" subset of it with a random sampling without replacement for training and test data. Accuracies are measured in token F -measures computed as follows:</p><formula xml:id="formula_14">F = 2P R P +R ,<label>(13)</label></formula><formula xml:id="formula_15">P = # of correct words # of words in output ,<label>(14)</label></formula><p>R = # of correct words # of words in gold standard .</p><p>Unsupervised word segmentation In <ref type="table">Table 4</ref>, we show the accuracies of unsupervised word seg- mentation with previous figures. We used bi- gram PYHSMM and set L = 4 for Chinese, L = 5, 8, 10, 21 for Japanese with different types of contiguous characters, and L = 6 for Thai. The number of hidden states are K = 10 (Chinese and Thai), K = 20 (Kyoto) and K = 30 (BCCWJ). We can see that our PYHSMM outperforms on all the datasets. <ref type="bibr" target="#b7">Huang and Zhao (2007)</ref> reports that the maximum possible accuracy in unsuper- vised Chinese word segmentation is 84.8%, de- rived through the inconsistency between different segmentation standards of the SIGHAN dataset. Our PYHSMM performs nearer to this best possi- ble accuracy, leveraging both word and character knowledge in a consistent Bayesian fashion. Fur- ther note that in Thai, quite high performance is achieved with a very small data compared to pre- vious work.    <ref type="table">Table 4</ref>: Accuracies of unsupervised word seg- mentation. BE is a Branching Entropy method of <ref type="bibr" target="#b27">Zhikov et al. (2010)</ref>, and HMM 2 is a product of word and character HMMs of <ref type="bibr" target="#b1">Chen et al. (2014)</ref>. * is the accuracy decoded with L = 3: it becomes 81.7 with L = 4 as MSR and PKU.</p><p>have part-of-speech annotations as well. For these data, we also evaluated the precision of part-of- speech induction on the output of unsupervised word segmentation above. Note that the precision is measured only over correct word segmentation that the system has output. <ref type="table" target="#tab_4">Table 5</ref> shows the precisions; to the best of our knowledge, there are no previous work on joint unsupervised learn- ing of words and tags, thus we only compared with Bayesian HMM (Goldwater and Griffiths, 2007) on both NPYLM segmentation and gold segmentation. In this evaluation, we associated each tag of supervised data with a latent state that cooccurred most frequently with that tag. We can see that the precision of joint POS tagging is better than NPYLM+HMM, and even better than HMM that is run over the gold segmentation. For colloquial Chinese, we also conducted an experiment on the Leiden Weibo Corpus (LWC), a corpus of Chinese equivalent of Twitter <ref type="bibr">7</ref> . We used random 20,000 sentences from this corpus, and re- sults are shown in <ref type="figure">Figure 7</ref>. In many cases plausi- ble words are found, and assigned to syntactically consistent states. States that are not shown here are either just not used or consists of a mixture of different syntactic categories. Guiding our model to induce more accurate latent states is a common problem to all unsupervised part-of-speech induc- tion, but we show some semi-supervised results next.  Semi-supervised experiments Because our PYHSMM is a generative model, it is easily amenable to semi-supervised segmentation and tagging. We used random 10,000 sentences from supervised data on Kyoto, BCCWJ, and LWC datasets along with unsupervised datasets in <ref type="table" target="#tab_1">Table 3</ref>. Results are shown in <ref type="table" target="#tab_6">Table 6</ref>: segmentation ac- curacies came close to 90% but do not go be- yond. By inspecting the segmentation and POS that PYHSMM has output, we found that this is not necessarily a fault of our model, but it came from the often inconsistet or incorrect tagging of the dataset. In many cases PYHSMM found more "natural" segmentations, but it does not always conform to the gold annotations. On the other hand, it often oversegments emotional expressions (sequence of the same character, for example) and this is one of the major sources of errors.</p><p>Finally, we note that our proposed model for un- supervised learning is most effective for the lan- guage which we do not know its syntactic behavior but only know raw strings as its data. In <ref type="figure">Figure 8</ref>, we show an excerpt of results to model a Japanese local dialect (Mikawa-ben around Nagoya district) collected from a specific Twitter. Even from the surface appearance of characters, we can see that similar words are assigned to the same state in- cluding some emoticons <ref type="bibr">(states 9,29,32)</ref>, and in fact we can identify a state of postpositions spe- cific to that dialect (state 3). Notice that the words themselves are not trivial before this anal- ysis. There are also some name of local places (state 41) and general Japanese postpositions (2) or nouns <ref type="bibr">(11,</ref><ref type="bibr">18,</ref><ref type="bibr">25,</ref><ref type="bibr">27,</ref><ref type="bibr">31)</ref>. Because of the spar- sity promoting prior (7) over the hidden states, ac- tually used states are sparse and the results can be considered quite satisfactory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>The characteristics of NPYLM is a Baysian inte- gration of character and word level information, which is related to ( <ref type="bibr" target="#b0">Blunsom and Cohn, 2011</ref>) and the adaptor idea of <ref type="bibr" target="#b6">(Goldwater et al., 2011</ref>   <ref type="table" target="#tab_0">18   227   182   86   65   62   53   44   41   31   30    3309   1901   482   226   110   93   69   56   47   43   13440   5989   5224    3237   1504   1206   1190   900   861   742   207   201    199  192   192   177   167   165   154   146   68  60  59  55  53  51  49  49  45</ref>  is different from (and misunderstood in) a joint model of <ref type="bibr" target="#b1">Chen et al. (2014)</ref>, where word and char- acter HMMs are just multiplied. There are no in- formation shared from the model structure, and in fact it depends on a BIO-like heuristic tagging scheme in the character HMM.</p><formula xml:id="formula_17">z = 1 z = 3 z = 10 z = 11 z =</formula><p>In the present paper, we extended it to include a hidden state for each word. Therefore, it might be interesting to introduce a hidden state also for each character. Unlike western languages, there are many kinds of Chinese characters that work quite differently, and Japanese uses several distinct kinds of characters, such as a Chinese character, Hiragana, Katakana, whose mixture would consti- tute a single word. Therefore, statistical modeling of different types of characters is an important re- search venue for the future.</p><p>NPYLM has already applied and extended to speech recognition ( <ref type="bibr" target="#b19">Neubig et al., 2010)</ref>, statisti- cal machine translation <ref type="bibr" target="#b20">(Nguyen et al., 2010)</ref>, or even robotics ( <ref type="bibr" target="#b18">Nakamura et al., 2014</ref>). For all these research area, we believe PYHSMM would be beneficial for their extension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we proposed a Pitman-Yor Hidden Semi-Markov model for joint unsupervised word segmentation and part-of-speech tagging on a raw sequence of characters. It can also be viewed as a way to build a class n-gram language model di- rectly on strings, without any "word" information a priori.</p><p>We applied our PYHSMM on several standard datasets on Japanese, Chinese and Thai, and it out- performed previous figures to yield the state-of- the-art results, as well as automatically induced word categories. It is especially beneficial for col- loquial text, local languages or speech transcripts, where not only words themselves are unknown but their syntactic behavior is a focus of interest.</p><p>In order to adapt to human standards given in supervised data, it is important to conduct a semi- supervised learning with discriminative classifiers. Since semi-supervised learning requires genera- tive models in advance, our proposed Bayesian generative model will also lay foundations to such an extension.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Sample of Japanese Twitter text that is difficult to analyze by ordinary supervised segmentation. It contains a lot of novel words, emoticons, and colloquial expressions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: NPYLM represented in a hierarchical Chinese restaurant process. Here, a character ∞gram HPYLM is embedded in a word n-gram HPYLM and learned jointly during inference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 3: MCMC inference of NPYLM Θ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Graphical model of PYHSMM in a bigram case. White nodes are latent variables, and the shaded node is the observation. We only observe a string s that is a concatenation of hidden words w 1 · · · w T .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Graphical representation of sampling words and POSs. Each cell corresponds to an inside probability α[t][k][z]. Note each cell is not always connected to adjacent cells, because of an overlap of substrings associated with each cell.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Table 2 :</head><label>2</label><figDesc>Precision of maximum word length prediction with a Negative Binomial generalized linear model (in percent). ≥ 5 are figures for word length ≥ 5. Final row is the maxi- mum length of a word found in each dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Distribution of predicted maximum word lengths on the Kyoto corpus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>39 1 Figure 7 :Figure 8 :</head><label>39178</label><figDesc>Figure 7: Some interesting words and states induced from Weibo corpus (K = 20). Numbers represent frequencies that each word is generated from that class. Although not perfect, emphatic (z = 1), endof-sentence expressions (z = 3), and locative words (z = 18) are learned from tweets. Distinction is far more clear in the semi-supervised experiments (not shown here). z Induced words 2 3 9 ( * ˆˆ*ˆˆ* ) (ˆ-ˆ; (ˆ_ˆ;) (ˆˆ;; (ˆˆ;; 10 11 13 18 19 20 24 25 26 27 &amp; 29 ( (; ( ( * ( * ˆ_ˆ*ˆ_ˆ* ) 30 31 32 ( * \(ˆ (ˆ (ˆ * \(ˆ (ˆ_ˆ(ˆ_ˆ( * ˆ 34 35 36 41 Figure 8: Unsupervised analysis of a Japanese local dialect by PYHSMM. (K = 50)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Features used for the Negative Binomial generalized linear model for maximum word length prediction.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Datasets used for evaluation. Abbrevi-
ations: Ja=Japanese, Zh=Chinese, Th=Thai lan-
guage. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Precision of POS tagging on correctly 
segmented words. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>) . This</head><label>.</label><figDesc></figDesc><table>Dataset Seg POS 
Kyoto 
92.1 87.1 
BCCWJ 89.4 83.1 
LWC 
88.5 86.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Semi-supervised segmentation and POS 
tagging accuracies. POS is measured by precision. </table></figure>

			<note place="foot" n="1"> For example, Zhikov et al. (2010) defined a coding length using character n-grams plus MDL penalty. Since this can be interpreted as a crude &quot;likelihood&quot; and a prior, its essence is similar but driven by a quite simplistic model.</note>

			<note place="foot" n="2"> To be precise, this occurs whenever t hw is incremented in the unigram restaurant. 3 Note that a child first memorizes what his mother says as a single word and gradually learns the lexicon.</note>

			<note place="foot" n="4"> Lately, Johnson et al. (2013) proposed a nonparametric Bayesian hidden semi-Markov models for general state spaces. However, it depends on a separate distribution for a state duration, thus is clealy different from ours for a natural language.</note>

			<note place="foot" n="5"> For computational reasons, we do not pursue using a Dirichlet process to yield an infinite HMM (Van Gael et al., 2009), but it is straightforward to extend our PYHSMM to iHMM.</note>

			<note place="foot" n="6"> Because NPYLM employs a mixture of Poisson distributions for each character type of a substring, this correspondence is not exact.</note>

			<note place="foot" n="7"> http://lwc.daanvanesch.nl/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2011</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="865" to="874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Joint Model for Unsupervised Chinese Word Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaohong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Pei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2014</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="854" to="863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cook</surname></persName>
		</author>
		<ptr target="http://www.johndcook.com/negativebinomial.pdf" />
		<title level="m">Notes on the Negative Binomial Distribution</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The Second International Chinese Word Segmentation Bakeoff</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Emerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing</title>
		<meeting>the Fourth SIGHAN Workshop on Chinese Language Processing</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Fully Bayesian Approach to Unsupervised Part-of-Speech Tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2007</title>
		<meeting>ACL 2007</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="744" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Contextual Dependencies in Unsupervised Word Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL/COLING 2006</title>
		<meeting>ACL/COLING 2006</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="673" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Producing Power-Law Distributions and Damping Word Frequencies with TwoStage Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2335" to="2382" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Chinese word segmentation: A decade review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Ning</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chinese Information Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="8" to="20" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bayesian Nonparametric Hidden Semi-Markov Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Willsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="673" to="701" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bayesian Inference for PCFGs via Markov Chain Monte Carlo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT/NAACL 2007</title>
		<meeting>HLT/NAACL 2007</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="139" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">InterBEST 2009: Thai Word Segmentation Workshop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krit</forename><surname>Kosawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<title level="m">Eighth International Symposium on Natural Language Processing (SNLP2009)</title>
		<meeting><address><addrLine>Thailand</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Building a Japanese Parsed Corpus while Improving the Parsing System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Nagao</surname></persName>
		</author>
		<ptr target="http://nlp.kuee.kyoto-u.ac.jp/nl-resource/corpus.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC 1998</title>
		<meeting>LREC 1998</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="719" to="724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Kotonoha and BCCWJ: Development of a Balanced Corpus of Contemporary Written Japanese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kikuo</forename><surname>Maekawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Corpora and Language Research: Proceedings of the First International Conference on Korean Language, Literature, and Culture</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="158" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Can MDL Improve Unsupervised Chinese Word Segmentation?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Magistry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoˆıtbenoˆıt</forename><surname>Sagot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh SIGHAN Workshop on Chinese Language Processing</title>
		<meeting>the Seventh SIGHAN Workshop on Chinese Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The Infinite Markov Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daichi</forename><surname>Mochihashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1017" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bayesian Unsupervised Word Segmentation with Nested Pitman-Yor Language Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daichi</forename><surname>Mochihashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naonori</forename><surname>Ueda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-IJCNLP 2009</title>
		<meeting>ACL-IJCNLP 2009</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="100" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Hidden semi-Markov models (segment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<ptr target="http://www.cs.ubc.ca/˜murphyk/Papers/segment.pdf" />
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mutual Learning of an Object Concept and Language Model Based on MLDA and NPYLM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoaki</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takayuki</forename><surname>Nagai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kotaro</forename><surname>Funakoshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shogo</forename><surname>Nagasaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadahiro</forename><surname>Taniguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoto</forename><surname>Iwahashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS&apos;14)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="600" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning a Language Model from Continuous Speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masato</forename><surname>Mimura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinsuke</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Kawahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of INTERSPEECH</title>
		<meeting>of INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Nonparametric Word Segmentation for Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thuylinh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2010</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The Two-Parameter Poisson-Dirichlet Distribution Derived from a Stable Subordinator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Pitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Yor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Probability</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="855" to="900" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">SemiMarkov Conditional Random Fields for Information Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 17 (NIPS 2004)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1185" to="1192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bayesian Methods for Hidden Markov Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="337" to="351" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A Hierarchical Dirichlet Process Model for Joint Part-of-Speech and Morphology Induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kairit</forename><surname>Sirts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanel</forename><surname>Alumäe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL 2012</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="407" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A Bayesian Interpretation of Interpolated Kneser-Ney</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
		<idno>TRA2/06</idno>
		<imprint>
			<date type="published" when="2006" />
			<pubPlace>School of Computing, NUS</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The infinite HMM for unsupervised PoS tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Van Gael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2009</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="678" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An Efficient Algorithm for Unsupervised Word Segmentation with Branching Entropy and MDL</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Zhikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroya</forename><surname>Takamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2010</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="832" to="842" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
