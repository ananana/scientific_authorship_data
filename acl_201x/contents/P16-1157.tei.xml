<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:16+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cross-lingual Models of Word Embeddings: An Empirical Comparison</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyam</forename><surname>Upadhyay</surname></persName>
							<email>upadhya3@illinois.edu,</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Illinois</orgName>
								<orgName type="institution" key="instit2">Urbana-Champaign</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
							<email>mfaruqui@cs.cmu.edu</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
							<email>cdyer@cs.cmu.edu,</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
							<email>danr@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Illinois</orgName>
								<orgName type="institution" key="instit2">Urbana-Champaign</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cross-lingual Models of Word Embeddings: An Empirical Comparison</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1661" to="1670"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Despite interest in using cross-lingual knowledge to learn word embeddings for various tasks, a systematic comparison of the possible approaches is lacking in the literature. We perform an extensive evaluation of four popular approaches of inducing cross-lingual embeddings, each requiring a different form of supervision, on four typologically different language pairs. Our evaluation setup spans four different tasks, including intrinsic evaluation on mono-lingual and cross-lingual similarity , and extrinsic evaluation on downstream semantic and syntactic applications. We show that models which require expensive cross-lingual knowledge almost always perform better, but cheaply supervised models often prove competitive on certain tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning word vector representations using mono- lingual distributional information is now a ubiqui- tous technique in NLP. The quality of these word vectors can be significantly improved by incor- porating cross-lingual distributional information ( <ref type="bibr" target="#b18">Klementiev et al., 2012;</ref><ref type="bibr" target="#b38">Zou et al., 2013;</ref><ref type="bibr" target="#b36">Vuli´cVuli´c and Moens, 2013b;</ref><ref type="bibr" target="#b25">Mikolov et al., 2013b;</ref><ref type="bibr" target="#b7">Faruqui and Dyer, 2014;</ref><ref type="bibr">Hermann and Blunsom, 2014;</ref><ref type="bibr" target="#b4">Chandar et al., 2014</ref>, inter alia), with improve- ments observed both on monolingual <ref type="bibr" target="#b7">(Faruqui and Dyer, 2014;</ref><ref type="bibr" target="#b27">Rastogi et al., 2015</ref>) and cross-lingual tasks ( <ref type="bibr" target="#b12">Guo et al., 2015;</ref><ref type="bibr" target="#b13">Guo et al., 2016)</ref>.</p><p>Several models for inducing cross-lingual em- beddings have been proposed, each requiring a dif- ferent form of cross-lingual supervision -some can use document-level alignments <ref type="bibr" target="#b37">(Vuli´cVuli´c and Moens, 2015)</ref>, others need alignments at the sen- tence ( <ref type="bibr">Hermann and Blunsom, 2014;</ref> or word level <ref type="bibr" target="#b7">(Faruqui and Dyer, 2014;</ref><ref type="bibr" target="#b10">Gouws and Søgaard, 2015)</ref>, while some require both sentence and word alignments ( <ref type="bibr" target="#b21">Luong et al., 2015)</ref>. However, a systematic comparison of these models is missing from the literature, making it difficult to analyze which approach is suitable for a particular NLP task. In this paper, we fill this void by empirically comparing four cross-lingual word embedding models each of which require different form of alignment(s) as supervision, across several dimensions. To this end, we train these models on four different language pairs, and evaluate them on both monolingual and cross-lingual tasks. <ref type="bibr">1</ref> First, we show that different models can be viewed as instances of a more general frame- work for inducing cross-lingual word embeddings. Then, we evaluate these models on both extrin- sic and intrinsic tasks. Our intrinsic evaluation assesses the quality of the vectors on monolin- gual ( §4.2) and cross-lingual ( §4.3) word simi- larity tasks, while our extrinsic evaluation spans semantic (cross-lingual document classification §4.4) and syntactic tasks (cross-lingual depen- dency parsing §4.5).</p><p>Our experiments show that word vectors trained using expensive cross-lingual supervision (word alignments or sentence alignments) perform the best on semantic tasks. On the other hand, for syn- tactic tasks like cross-lingual dependency parsing, models requiring weaker form of cross-lingual su- pervision (such as context agnostic translation dic- tionary) are competitive to models requiring ex- pensive supervision. We also show qualitatively how the nature of cross-lingual supervision used to train word vectors affects the proximity of translation pairs across languages, and of words with similar meaning in the same language in the vector-space. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 General Algorithm</head><formula xml:id="formula_0">1: Initialize W ← W 0 ,V ← V 0 2: (W * , V * ) ← arg min αA(W) + βB(V) + C(W, V)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Bilingual Embeddings</head><p>A general schema for inducing bilingual embed- dings is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Our comparison fo- cuses on dense, fixed-length distributed embed- dings which are obtained using some form of cross-lingual supervision. We briefly describe the embedding induction procedure for each of the se- lected bilingual word vector models, with the aim to provide a unified algorithmic perspective for all methods, and to facilitate better understanding and comparison. Our choice of models spans across different forms of supervision required for induc- ing the embeddings, illustrated in <ref type="figure">Figure 2</ref>.</p><p>Notation. Let W = {w 1 , w 2 , . . . , w |W | } be the vocabulary of a language l 1 with |W | words, and W ∈ R |W |×l be the corresponding word embed- dings of length l. Let V = {v 1 , v 2 , . . . , v |V | } be the vocabulary of another language l 2 with |V | words, and V ∈ R |V |×m the corresponding word embeddings of length m. We denote the word vec- tor for a word w by w. <ref type="bibr" target="#b21">Luong et al. (2015)</ref> proposed Bilingual Skip- Gram, a simple extension of the monolingual skip- gram model, which learns bilingual embeddings by using a parallel corpus along with word align- ments (both sentence and word level alignments).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Bilingual Skip-Gram Model (BiSkip)</head><p>The learning objective is a simple extension of the skip-gram model, where the context of a word is expanded to include bilingual links ob- tained from word alignments, so that the model is trained to predict words cross-lingually. In par- ticular, given a word alignment link from word v ∈ V in language l 2 to w ∈ W in language l 1 , the model predicts the context words of w using v and vice-versa. Formally, the cross lingual part of the objective is,</p><formula xml:id="formula_1">D 12 (W, V) = − (v,w)∈Q wc∈NBR 1 (w) log P (w c | v)</formula><p>(1) where NBR 1 (w) is the context of w in language l 1 , Q is the set of word alignments, and P (w c | v) ∝ exp(w T c v). Another similar term D 21 models the objective for v and NBR 2 (v). The objective can be cast into Algorithm 1 as,</p><formula xml:id="formula_2">C(W, V) = D 12 (W, V) + D 21 (W, V) (2) A(W) = − w∈W wc∈NBR 1 (w) log P (w c | w) (3) B(V) = − v∈V vc∈NBR 2 (v) log P (v c | v) (4)</formula><p>where A(W) and B(V) are the familiar skip- gram formulation of the monolingual part of the objective. α and β are chosen hyper-parameters which set the relative importance of the monolin- gual terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Bilingual Compositional Model (BiCVM)</head><p>Hermann and Blunsom (2014) present a method that learns bilingual word vectors from a sentence aligned corpus. Their model leverages the fact that aligned sentences have equivalent meaning, thus their sentence representations should be similar.</p><p>We denote two aligned sentences, v = x 1 , . . . , and w = y 1 , . . . , where x i ∈ V, y i ∈ W, are vectors corresponding to the words in the sentences. Let functions f : v → R n and g : w → R n , map sentences to their seman- tic representations in R n . BiCVM generates word vectors by minimizing the squared 2 norm be- tween the sentence representations of aligned sen- tences. In order to prevent the degeneracy aris- ing from directly minimizing the 2 norm, they use a noise-contrastive large-margin update, with randomly drawn sentence pairs ( v, w n ) as negative samples. The loss for the sentence pairs ( v, w) and ( v, w n ) can be written as,</p><formula xml:id="formula_3">E( v, w, w n ) = max (δ + ∆E( v, w, w n ), 0) (5)</formula><p>where, Bonjour! Je t' aime.</p><formula xml:id="formula_4">E( v, w) = f ( v) − g( w) 2<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(d) BiVCD</head><p>Figure 2: Forms of supervision required by the four models compared in this paper. From left to right, the cost of the supervision required varies from expensive (BiSkip) to cheap (BiVCD). BiSkip requires a parallel corpus annotated with word alignments <ref type="figure">(Fig. 2a)</ref>, BiCVM requires a sentence-aligned corpus <ref type="figure">(Fig. 2b)</ref>, BiCCA only requires a bilingual lexicon <ref type="figure">(Fig. 2c)</ref> and BiVCD requires comparable documents <ref type="figure">(Fig. 2d)</ref>. and,</p><formula xml:id="formula_5">∆E( v, w, w n ) = E( v, w) − E( v, w n )<label>(7)</label></formula><p>This can be cast into Algorithm 1 by,</p><formula xml:id="formula_6">C(W, V) = aligned ( v, w) random w n E( v, w, w n ) (8) A(W) = W 2 B(V) = V 2<label>(9)</label></formula><p>with A(W) and B(V) being regularizers, with α = β.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Bilingual Correlation Based Embeddings (BiCCA)</head><p>The BiCCA model, proposed by <ref type="bibr" target="#b7">Faruqui and Dyer (2014)</ref>, showed that when (independently trained) monolingual vector matrices W, V are projected using CCA <ref type="bibr" target="#b17">(Hotelling, 1936)</ref> to respect a transla- tion lexicon, their performance improves on word similarity and word analogy tasks. They first con- struct W ⊆ W, V ⊆ V such that |W |= |V | and the corresponding words (w i , v i ) in the matri- ces are translations of each other. The projection is then computed as:</p><formula xml:id="formula_7">P W , P V = CCA(W , V )<label>(10)</label></formula><formula xml:id="formula_8">W * = WP W V * = VP V (11)</formula><p>where, P V ∈ R l×d , P W ∈ R m×d are the projec- tion matrices with d ≤ min(l, m) and the V * ∈ R |V |×d , W * ∈ R |W |×d are the word vectors that have been "enriched" using bilingual knowledge. The BiCCA objective can be viewed 2 as the fol- lowing instantiation of Algorithm 1:</p><formula xml:id="formula_9">W 0 = W , V 0 = V (12) C(W, V) = W − V 2 +γ V T W (13) A(W) = W 2 −1 B(V) = V 2 −1 (14)</formula><p>where W = W 0 P W and V = V 0 P V , where we set α = β = γ = ∞ to set hard constraints.</p><p>2 described in Section 6.5 of (Hardoon et al., 2004)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Bilingual Vectors from Comparable Data (BiVCD)</head><p>Another approach of inducing bilingual word vec- tors, which we refer to as BiVCD, was proposed by Vuli´c <ref type="bibr" target="#b37">Vuli´c and Moens (2015)</ref>. Their approach is designed to use comparable corpus between the source and target language pair to induce cross- lingual vectors.</p><p>Let d e and d f denote a pair of comparable documents with length in words p and q respec- tively (assume p &gt; q). BiVCD first merges these two comparable documents into a single pseudo- bilingual document using a deterministic strategy based on length ratio of two documents R = p q . Every R th word of the merged pseudo-bilingual document is picked sequentially from d f . Finally, a skip-gram model is trained on the corpus of pseudo-bilingual documents, to generate vectors for all words in W * ∪ V * . The vectors consti- tuting W * and V * can then be easily identified.</p><p>Instantiating BiVCD in the general algorithm is obvious: C(W, V) assumes the familiar word2vec skip-gram objective over the pseudo- bilingual document,</p><formula xml:id="formula_10">C(W, V) = − s∈W ∪V t∈NBR(s) log P (t | s) (15)</formula><p>where NBR(s) is defined by the pseudo-bilingual document and</p><formula xml:id="formula_11">P (t | s) ∝ exp(t T s). Note that t, s ∈ W ∪ V .</formula><p>Although BiVCD is designed to use comparable corpus, we provide it with parallel data in our ex- periments (to ensure comparability), and treat two aligned sentences as comparable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data</head><p>We train cross-lingual embeddings for 4 language pairs: English-German (en-de), English-French (en-fr), English-Swedish (en-sv) and English- Chinese (en-zh). For en-de and en-sv we use the l 1 l 2 #sent #l 1 -words #l 2 -words en de</p><p>1 <ref type="table" target="#tab_1">.9  53  51  fr  2.0  55  61  sv  1.7  46  42  zh</ref> 2.0 58 50 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>We measure the quality of the induced cross- lingual word embeddings in terms of their per- formance, when used as features in the following tasks:</p><p>• monolingual word similarity for English</p><p>• Cross-lingual dictionary induction</p><p>• Cross-lingual document classification</p><p>• Cross-lingual syntactic dependency parsing</p><p>The first two tasks intrinsically measure how much can monolingual and cross-lingual similar- ity benefit from cross-lingual training. The last two tasks measure the ability of cross-lingually trained vectors to extrinsically facilitate model transfer across languages, for semantic and syn- tactic applications respectively. These tasks have been used in previous works ( <ref type="bibr" target="#b18">Klementiev et al., 2012;</ref><ref type="bibr" target="#b21">Luong et al., 2015;</ref><ref type="bibr" target="#b35">Vuli´cVuli´c and Moens, 2013a;</ref><ref type="bibr" target="#b12">Guo et al., 2015</ref>) for evaluating cross-lingual em- beddings, but no comparison exists which uses them in conjunction.</p><p>To ensure fair comparison, all models are trained with embeddings of size 200. We provide all models with parallel corpora, irrespective of their requirements. Whenever possible, we also report statistical significance of our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Parameter Selection</head><p>We follow the BestAvg parameter selection strat- egy from <ref type="bibr" target="#b20">Lu et al. (2015)</ref>: we selected the param- eters for all models by tuning on a set of values (described below) and picking the parameter set- ting which did best on an average across all tasks.</p><p>BiSkip. All models were trained using a win- dow size of 10 (tuned over {5, 10, 20}), and 30 negative samples (tuned over {10, 20, 30}). The cross-lingual weight was set to 4 (tuned over {1, 2, 4, 8}).</p><p>The word alignments for training the model (available at github. com/lmthang/bivec) were generated using fast_align ( <ref type="bibr" target="#b6">Dyer et al., 2013)</ref>. The number of training iterations was set to 5 (no tuning) and we set α = 1 and β = 1 (no tuning).</p><p>BiCVM. We use the tool (available at github. com/karlmoritz/bicvm) released by <ref type="bibr">Hermann and Blunsom (2014)</ref> to train all embed- dings. We train an additive model (that is, f ( x) = g( x) = i x i ) with hinge loss margin set to 200 (no tuning), batch size of 50 (tuned over 50, 100, 1000) and noise parameter of 10 (tuned over {10, 20, 30}). All models are trained for 100 iterations (no tuning).</p><p>BiCCA. First, monolingual word vectors are trained using the skip-gram model <ref type="bibr">5</ref> with negative sampling ( <ref type="bibr" target="#b24">Mikolov et al., 2013a</ref>) with window of size 5 (tuned over {5, 10, 20}). To generate a cross-lingual dictionary, word alignments are gen- erated using cdec from the parallel corpus. Then, word pairs (a, b), a ∈ l 1 , b ∈ l 2 are selected such that a is aligned to b the most number of times and vice versa. This way, we obtained dictionaries of approximately 36k, 35k, 30k and 28k word pairs for en-de, en-fr, en-sv and en-zh respectively.</p><p>The monolingual vectors are aligned using the above dictionaries with the tool (available at github.com/mfaruqui/eacl14-cca) re- leased by <ref type="bibr" target="#b7">Faruqui and Dyer (2014)</ref> to generate the cross-lingual word embeddings. We use k = 0.5 as the number of canonical components (tuned over {0.2, 0.3, 0.5, 1.0}). Note that this results in a embedding of size 100 after performing CCA.</p><p>BiVCD. We use word2vec's skip gram model for training our embeddings, with a window size of 5 (tuned on {5, 10, 20, 30}) and negative sam- pling parameter set to 5 (tuned on {5, 10, 25}). Every pair of parallel sentences is treated as a <ref type="bibr">5</ref> code.google.com/p/word2vec pair of comparable documents, and merging is per- formed using the sentence length ratio strategy de- scribed earlier. <ref type="bibr">6</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Monolingual Evaluation</head><p>We first evaluate if the inclusion of cross-lingual knowledge improves the quality of English em- beddings.</p><p>Word Similarity. Word similarity datasets con- tain word pairs which are assigned similarity rat- ings by humans. The task evaluates how well the notion of word similarity according to humans is emulated in the vector space. Evaluation is based on the Spearman's rank correlation coef- ficient ( <ref type="bibr" target="#b26">Myers and Well, 1995)</ref> between human rankings and rankings produced by computing co- sine similarity between the vectors of two words.</p><p>We use the SimLex dataset for English ( <ref type="bibr" target="#b16">Hill et al., 2014</ref>) which contains 999 pairs of En- glish words, with a balanced set of noun, adjec- tive and verb pairs. SimLex is claimed to capture word similarity exclusively instead of WordSim- 353 ( <ref type="bibr" target="#b8">Finkelstein et al., 2001</ref>) which captures both word similarity and relatedness. We declare sig- nificant improvement if p &lt; 0.1 according to Steiger's method <ref type="bibr" target="#b31">(Steiger, 1980)</ref> for calculating the statistical significant differences between two dependent correlation coefficients. <ref type="table" target="#tab_3">Table 2</ref> shows the performance of English em- beddings induced by all the models by training on different language pairs on the SimLex word sim- ilarity task. The score obtained by monolingual English embeddings trained on the respective En- glish side of each language is shown in column marked Mono. In all cases (except BiCCA on en- sv), the bilingually trained vectors achieve better scores than the mono-lingually trained vectors.</p><p>Overall, across all language pairs, BiCVM is the best performing model in terms of Spearman's correlation, but its improvement over BiSkip and BiVCD is often insignificant. It is notable that 2 of the 3 top performing models, BiCVM and BiVCD, need sentence aligned and document-aligned cor- pus only, which are easier to obtain than parallel data with word alignments required by BiSkip.</p><p>QVEC. <ref type="bibr" target="#b34">Tsvetkov et al. (2015)</ref> proposed an in- trinsic evaluation metric for estimating the qual- ity of English word vectors. The score produced by QVEC measures how well a given set of word vectors is able to quantify linguistic properties <ref type="bibr">6</ref> We implemented the code for performing the merging as we could not find a tool provided by the authors.   of words, with higher being better. The metric is shown to have strong correlation with perfor- mance on downstream semantic applications. As it can be currently only used for English, we use it to evaluate the English vectors obtained using cross-lingual training of different models. <ref type="table" target="#tab_4">Ta- ble 3</ref> shows that on average across language pairs, BiSkip achieves the best score, followed by Mono (mono-lingually trained English vectors), BiVCD and BiCCA. A possible explanation for why Mono scores are better than those obtained by some of the cross-lingual models is that QVEC measures monolingual semantic content based on a linguis- tic oracle made for English. Cross-lingual training might affect these semantic properties arbitrarily.</p><p>Interestingly, BiCVM which was the best model according to SimLex, ranks last according to QVEC. The fact that the best models according to QVEC and word similarities are different re- inforces observations made in previous work that performance on word similarity tasks alone does not reflect quantification of linguistic properties of words ( <ref type="bibr" target="#b34">Tsvetkov et al., 2015;</ref><ref type="bibr" target="#b29">Schnabel et al., 2015</ref>).   <ref type="table">Table 4</ref>: Cross-lingual dictionary induction results (top-10 accuracy). The same trend was also observed across models when computing MRR (mean reciprocal rank).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Cross-lingual Dictionary Induction</head><p>The task of cross-lingual dictionary induc- tion <ref type="bibr" target="#b35">(Vuli´cVuli´c and Moens, 2013a;</ref><ref type="bibr" target="#b25">Mikolov et al., 2013b</ref>) judges how good cross- lingual embeddings are at detecting word pairs that are semantically similar across languages. We follow the setup of Vuli´cVuli´c and Moens (2013a), but instead of manually creating a gold cross-lingual dictionary, we derived our gold dictionaries using the Open Multilingual WordNet data released by <ref type="bibr" target="#b2">Bond and Foster (2013)</ref>. The data includes synset alignments across 26 languages with over 90% ac- curacy. First, we prune out words from each synset whose frequency count is less than 1000 in the vo- cabulary of the training data from §3. Then, for each pair of aligned synsets s 1 = {k 1 , k 2 , · · ·} s 2 = {g 1 , g 2 , · · ·}, we include all elements from the set {(k, g) | k ∈ s 1 , g ∈ s 2 } into the gold dic- tionary, where k and g are the lemmas. Using this approach we generated dictionaries of sizes 1.5k, 1.4k, 1.0k and 1.6k pairs for en-fr, en-de, en-sv and en-zh respectively.</p><p>We report top-10 accuracy, which is the frac- tion of the entries (e, f ) in the gold dictionary, for which f belongs to the list of top-10 neighbors of the word vector of e, according to the induced cross-lingual embeddings. From the results (Ta- ble 4), it can be seen that for dictionary induction, the performance improves with the quality of su- pervision. As we move from cheaply supervised methods (eg. BiVCD) to more expensive supervi- sion (eg. BiSkip), the accuracy improves. This suggests that for cross lingual similarity tasks, the more expensive the cross-lingual knowledge available, the better. Models using weak super- vision like BiVCD perform poorly in comparison to models like BiSkip and BiCVM, with perfor- mance gaps upwards of 10 pts on an average.   <ref type="table">Table 5</ref>: Cross-lingual document classification accuracy when trained on language l1, and evaluated on language l2. The best score for each language is shown in bold. Scores which are significantly better (per McNemar's Test with p &lt; 0.05) than the next lower score are underlined. For example, for sv→en, BiVCD is significantly better than BiSkip, which in turn is significantly better than BiCVM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Cross-lingual Document Classification</head><p>We follow the cross-lingual document classifica- tion (CLDC) setup of <ref type="bibr" target="#b18">Klementiev et al. (2012)</ref>, but extend it to cover all of our language pairs. We use the RCV2 Reuters multilingual corpus 7 for our ex- periments. In this task, for a language pair (l 1 , l 2 ), a document classifier is trained using the docu- ment representations derived from word embed- dings in language l 1 , and then the trained model is tested on documents from language l 2 (and vice-versa). By using supervised training data in one language and evaluating without further su- pervision in another, CLDC assesses whether the learned cross-lingual representations are semanti- cally coherent across multiple languages. All embeddings are learned on the data de- scribed in §3, and we only use the RCV2 data to learn document classification models. Following previous work, we compute document representa- tion by taking the tf-idf weighted average of vec- tors of the words present in it. 8 A multi-class clas- sifier is trained using an averaged perceptron <ref type="bibr" target="#b9">(Freund and Schapire, 1999</ref>) for 10 iterations, using the document vectors of language l 1 as features <ref type="bibr">9</ref> . Majority baselines for en → l 2 and l 1 → en are 49.7% and 46.7% respectively, for all languages. <ref type="table">Table 5</ref> shows the performance of different mod- els across different language pairs. We computed confidence values using the McNemar test <ref type="bibr">(McNe-mar, 1947</ref>) and declare significant improvement if p &lt; 0.05. <ref type="table">Table 5</ref> shows that in almost all cases, BiSkip performs significantly better than the remaining models. For transferring semantic knowledge across languages via embeddings, sentence and word level alignment proves superior to sentence or word level alignment alone. This observation is consistent with the trend in cross-lingual dictio- nary induction, where too the most expensive form of supervision performed the best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Cross-lingual Dependency Parsing</head><p>Using cross lingual similarity for direct-transfer of dependency parsers was first shown in <ref type="bibr">Täckström et al. (2012)</ref>. The idea behind direct-transfer is to train a dependency parsing model using em- beddings for language l 1 and then test the trained model on language l 2 , replacing embeddings for language l 1 with those of l 2 . The transfer relies on coherence of the embeddings across languages arising from the cross lingual training. For our ex- periments, we use the cross lingual transfer setup of <ref type="bibr" target="#b12">Guo et al. (2015)</ref>. <ref type="bibr">10</ref> Their framework trains a transition-based dependency parser using nonlin- ear activation function, with the source-side em- beddings as lexical features. These embeddings can be replaced by target-side embeddings at test time.</p><p>All models are trained for 5000 iterations with fixed word embeddings during training. Since our goal is to determine the utility of word embed- dings in dependency parsing, we turn off other features that can capture distributional information like brown clusters, which were originally used in <ref type="bibr" target="#b12">Guo et al. (2015)</ref>. We use the universal depen- dency treebank <ref type="bibr" target="#b22">(McDonald et al., 2013</ref>) version- 2.0 for our evaluation. For Chinese, we use the treebank released as part of the CoNLL-X shared task ( <ref type="bibr" target="#b3">Buchholz and Marsi, 2006</ref>).</p><p>We first evaluate how useful the word embed- dings are in cross-lingual model transfer of depen- dency parsers (  <ref type="table" target="#tab_7">Table 6</ref>: Labeled attachment score (LAS) for cross-lingual dependency parsing when trained on language l1, and eval- uated on language l2. The best score for each language is shown in bold.</p><p>two languages. Surprisingly, unlike the seman- tic tasks considered earlier, the models with ex- pensive supervision requirements like BiSkip and BiCVM could not outperform a cheaply super- vised BiCCA.</p><p>We also evaluate whether using cross-lingually trained vectors for learning dependency parsers is better than using mono-lingually trained vectors in <ref type="table" target="#tab_9">Table 7</ref>. We compare against parsing models trained using mono-lingually trained word vectors (column marked Mono in <ref type="table" target="#tab_9">Table 7</ref>). These vectors are the same used as input to the BiCCA model. All other settings remain the same. On an aver- age across language pairs, improvement over the monolingual embeddings was obtained with the BiSkip and BiCCA models, while BiCVM and BiVCD consistently performed worse. A possible reason for this is that BiCVM and BiVCD oper- ate on sentence level contexts to learn the embed- dings, which only captures the semantic meaning of the sentences and ignores the internal syntac- tic structure. As a result, embedding trained us- ing BiCVM and BiVCD are not informative for syntactic tasks. On the other hand, BiSkip and BiCCA both utilize the word alignment informa- tion to train their embeddings and thus do better in capturing some notion of syntax.   the translated words are more distant than BiSkip and BiCVM. This is not surprising because BiSkip and BiCVM require more expensive supervision at the sentence level in contrast to the other two models. An interesting observation is that BiCCA and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Qualitative Analysis</head><p>BiVCD are better at separating antonyms. The words peace and war, (and their French trans- lations paix and guerre) are well separated in BiCCA and BiVCD. However, in BiSkip and BiCVM these pairs are very close together. This can be attributed to the fact that BiSkip and BiCVM are trained on parallel sentences, and if two antonyms are present in the same sentence in English, they will also be present together in its French translation. However, BiCCA uses bilin- gual dictionary and BiVCD use comparable sen- tence context, which helps in pulling apart the syn- onyms and antonyms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>The goal of this paper was to formulate the task of learning cross-lingual word vector representa- tions in a unified framework, and conduct exper- iments to compare the performance of existing models in a unbiased manner. We chose exist- ing cross-lingual word vector models that can be trained on two languages at a given time. In re- cent work, <ref type="bibr" target="#b1">Ammar et al. (2016)</ref> train multilingual word vectors using more than two languages; our comparison does not cover this setting. It is also worth noting that we compare here different cross- lingual word embeddings, which are not to be con- fused with a collection of monolingual word em- beddings trained for different languages individu- ally (Al- <ref type="bibr" target="#b0">Rfou et al., 2013</ref>). The paper does not cover all approaches that generate cross-lingual word embeddings. Some methods do not have publicly available code ( <ref type="bibr" target="#b5">Coulmance et al., 2015;</ref><ref type="bibr" target="#b38">Zou et al., 2013)</ref>; for others, like BilBOWA ( , we identified problems in the available code, which caused it to consistently produced results that are inferior even to mono-lingually trained vectors. <ref type="bibr">11</ref> However, the models that we included for com- parison in our survey are representative of other cross-lingual models in terms of the form of cross- lingual supervision required by them. For exam- ple, <ref type="bibr">BilBOWA (Gouws et al., 2015)</ref> and cross- lingual Auto-encoder ( <ref type="bibr" target="#b4">Chandar et al., 2014</ref>) are similar to BiCVM in this respect. <ref type="bibr">Multi-view CCA (Rastogi et al., 2015</ref>) and deep CCA ( <ref type="bibr" target="#b20">Lu et al., 2015)</ref> can be viewed as extensions of BiCCA. Our choice of models was motivated to com- pare different forms of supervision, and therefore, adding these models, would not provide additional insight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We presented the first systematic comparative evaluation of cross-lingual embedding methods on several downstream NLP tasks, both intrinsic and extrinsic. We provided a unified representation for all approaches, showing them as instances of a general algorithm. Our choice of methods spans a diverse range of approaches, in that each requires a different form of supervision.</p><p>Our experiments reveal interesting trends. When evaluating on intrinsic tasks such as mono- lingual word similarity, models relying on cheaper forms of supervision (such as BiVCD) perform al- most on par with models requiring expensive su- pervision. On the other hand, for cross-lingual se- mantic tasks, like cross-lingual document classi- fication and dictionary induction, the model with the most informative supervision performs best <ref type="bibr">11</ref> We contacted the authors of the papers and were unable to resolve the issues in the toolkit. overall. In contrast, for the syntactic task of de- pendency parsing, models that are supervised at a word alignment level perform slightly better. Overall this suggests that semantic tasks can ben- efit more from richer cross-lingual supervision, as compared to syntactic tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (Above) A general schema for induction of crosslingual word vector representations. The word vector model generates embeddings which incorporates distributional information cross-lingually. (Below) A general algorithm for inducing bilingual word embeddings, where α, β, W 0 , V 0 are parameters and A, B, C are suitably defined losses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 showsFigure 3 :</head><label>33</label><figDesc>Figure 3 shows the PCA projection of some of the most frequent words in the English-French corpus. It is clear that BiSkip and BiCVM produce crosslingual vectors which are the most comparable, the English and French words which are translations of each other are represented by almost the same point in the vector-space. In BiCCA and BiVCD</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>The size of parallel corpora (in millions) of different 
language pairs used for training cross-lingual word vectors. 

Europarl v7 parallel corpus 3 (Koehn, 2005). For 
en-fr, we use Europarl combined with the news-
commentary and UN-corpus dataset from WMT 
2015. 4 For en-zh, we use the FBIS parallel cor-
pus from the news domain (LDC2003E14). We 
use the Stanford Chinese Segmenter (Tseng et al., 
2005) to preprocess the en-zh parallel corpus. Cor-
pus statistics for all languages is shown in Table 1. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Word similarity score measured in Spearman's cor-
relation ratio for English on SimLex-999. The best score for 
each language pair is shown in bold. Scores which are sig-
nificantly better (per Steiger's Method with p &lt; 0.1) than 
the next lower score are underlined. For example, for en-zh, 
BiCVM is significantly better than BiSkip, which in turn is 
significantly better than BiVCD. 

pair Mono BiSkip BiCVM BiCCA BiVCD 

en-de 0.39 
0.40 
0.31 
0.33 
0.37 
en-fr 0.39 
0.40 
0.31 
0.33 
0.38 
en-sv 0.39 
0.39 
0.31 
0.32 
0.37 
en-zh 0.40 
0.40 
0.32 
0.33 
0.38 

avg. 0.39 
0.40 
0.31 
0.33 
0.38 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Intrinsic evaluation of English word vectors mea- sured in terms of QVEC score across models. Best scores for each language pair is shown in bold.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>1 l 2 BiSkip BiCVM BiCCA BiVCD</head><label>1</label><figDesc></figDesc><table>en 

de 
79.7 
74.5 
72.4 
62.5 
fr 
78.9 
72.9 
70.1 
68.8 
sv 
77.1 
76.7 
74.2 
56.9 
zh 
69.4 
66.0 
59.6 
53.2 

avg. 
76.3 
72.5 
69.1 
60.4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>1 l 2 BiSkip BiCVM BiCCA BiVCD</head><label>1</label><figDesc></figDesc><table>en 

de 
85.2 
85.0 
79.1 
79.9 
fr 
77.7 
71.7 
70.7 
72.0 
sv 
72.3 
69.1 
65.3 
59.9 
zh 
75.5 
73.6 
69.4 
73.0 

de 

en 

74.9 
71.1 
64.9 
74.1 
fr 
80.4 
73.7 
75.5 
77.6 
sv 
73.4 
67.7 
67.0 
78.2 
zh 
81.1 
76.4 
77.3 
80.9 

avg. 
77.6 
73.5 
71.2 
74.5 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 6 )</head><label>6</label><figDesc></figDesc><table>. On an average, BiCCA 
does better than other models. BiSkip is a close 
second, with an average performance gap of less 
than 1 point. BiSkip outperforms BiCVM on Ger-
man and French (over 2 point improvement), ow-
ing to word alignment information BiSkip's model 
uses during training. It is not surprising that 
English-Chinese transfer scores are low, due to the 
significant difference in syntactic structure of the 

10 github.com/jiangfeng1124/ 
acl15-clnndep 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Labeled attachment score (LAS) for dependency 
parsing when trained and tested on language l. Mono refers 
to parser trained with mono-lingually induced embeddings. 
Scores in bold are better than the Mono scores for each lan-
guage, showing improvement from cross-lingual training. 

</table></figure>

			<note place="foot" n="1"> Instructions and code to reproduce the experiments available at http://cogcomp.cs.illinois.edu/ page/publication_view/794</note>

			<note place="foot" n="3"> www.statmt.org/europarl/v7/{de, sv}-en.tgz 4 www.statmt.org/wmt15/ translation-task.html</note>

			<note place="foot" n="7"> http://trec.nist.gov/data/reuters/ reuters.html 8 tf-idf (Salton and Buckley, 1988) was computed using all documents for that language in RCV2. 9 We use the implementation of Klementiev et al. (2012).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This material is based on research sponsored by DARPA under agreement number FA8750-13-2-0008 and Contract HR0011-15-2-0025. Approved for Public Release, Distribu-tion Unlimited. The views expressed are those of the authors and do not reflect the official policy or position of the Depart-ment of Defense or the U.S. Government.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Polyglot: Distributed word representations for multilingual nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL</title>
		<meeting>of CoNLL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Mulcaire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.01925</idno>
		<title level="m">Massively multilingual word embeddings</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Linking and extending an open multilingual wordnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Foster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Conll-X shared task on multilingual dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Buchholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erwin</forename><surname>Marsi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL</title>
		<meeting>of CoNLL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An autoencoder approach to learning bilingual word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarath</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislas</forename><surname>Lauly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitesh</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaraman</forename><surname>Ravindran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vikas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amrita</forename><surname>Raykar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Transgram, fast cross-lingual word-embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jocelyn</forename><surname>Coulmance</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Marc</forename><surname>Marty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amine</forename><surname>Benhalloum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A simple, fast, and effective reparameterization of ibm model 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Chahuneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving vector space word representations using multilingual correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EACL</title>
		<meeting>of EACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Placing search in context: the concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WWW</title>
		<meeting>of WWW</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Large margin classification using the perceptron algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="277" to="296" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Simple task-specific bilingual word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bilbowa: Fast bilingual distributed representations without word alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cross-lingual dependency parsing based on distributed representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A representation learning framework for multi-source transfer parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Canonical correlation analysis: An overview with application to learning methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandor</forename><surname>David R Hardoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Szedmak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shawetaylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2639" to="2664" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multilingual Models for Compositional Distributional Semantics</title>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<editor>Karl Moritz Hermann and Phil Blunsom</editor>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Simlex-999: Evaluating semantic models with (genuine) similarity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.3456</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Relations between two sets of variates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harold</forename><surname>Hotelling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3/4</biblScope>
			<biblScope unit="page" from="321" to="377" />
			<date type="published" when="1936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Inducing crosslingual distributed representations of words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Klementiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binod</forename><surname>Bhattarai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
		<meeting>of COLING</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Europarl: A parallel corpus for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of MT Summit</title>
		<meeting>of MT Summit</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep multilingual correlation for improved word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bilingual word representations with monolingual quality in mind</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Workshop on Vector Space Modeling for NLP</title>
		<meeting>of the Workshop on Vector Space Modeling for NLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Universal dependency annotation for multilingual parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvonne</forename><surname>Quirmbachbrundage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Bedini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Núria Bertomeu Castelló, and Jungmee Lee</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Note on the sampling error of the difference between correlated proportions or percentages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quinn</forename><surname>Mcnemar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="153" to="157" />
			<date type="published" when="1947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Exploiting similarities among languages for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.4168</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><forename type="middle">L</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold</forename><forename type="middle">D</forename><surname>Well</surname></persName>
		</author>
		<title level="m">Research Design &amp; Statistical Analysis</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
	<note>Routledge</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multiview LSA: Representation learning via generalized CCA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpendre</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raman</forename><surname>Arora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Termweighting approaches in automatic text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">formation Processing and Management</title>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Evaluation methods for unsupervised word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Labutov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Inverted indexing for cross-lingual nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Héctor Martínez</forename><surname>Zeljko Agi´cagi´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johannsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Tests for comparing elements of a correlation matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological bulletin</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">245</biblScope>
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cross-lingual word clusters for direct transfer of linguistic structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A conditional random field word segmenter for sighan bakeoff</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huihsin</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichuan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galen</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGHAN</title>
		<meeting>of SIGHAN</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Evaluation of word vector representations by subspace alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Crosslingual semantic similarity of words as the similarity of their semantic word responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A study on bootstrapping bilingual vector spaces from nonparallel data (and nothing else)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Bilingual word embeddings from non-parallel documentaligned data applied to bilingual lexicon induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Bilingual word embeddings for phrase-based machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
