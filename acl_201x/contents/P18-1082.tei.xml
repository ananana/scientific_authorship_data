<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:13+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Neural Story Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Menlo Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Menlo Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Menlo Park</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchical Neural Story Generation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="889" to="898"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>889</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Story-telling is on the frontier of current text gen- eration technology: stories must remain themati- cally consistent across the complete document, re- quiring modeling very long range dependencies; stories require creativity; and stories need a high level plot, necessitating planning ahead rather than word-by-word generation ( <ref type="bibr" target="#b27">Wiseman et al., 2017)</ref>.</p><p>We tackle the challenges of story-telling with a hierarchical model, which first generates a sen- tence called the prompt describing the topic for the story, and then conditions on this prompt when generating the story. Conditioning on the prompt or premise makes it easier to generate consistent stories because they provide grounding for the overall plot. It also reduces the tendency of stan- dard sequence models to drift off topic.</p><p>Prompt: The Mage, the Warrior, and the Priest Story: A light breeze swept the ground, and carried with it still the distant scents of dust and time-worn stone. The Warrior led the way, heaving her mass of armour and mus- cle over the uneven terrain. She soon crested the last of the low embankments, which still bore the unmistakable fin- gerprints of haste and fear. She lifted herself up onto the top the rise, and looked out at the scene before her. <ref type="bibr">[...]</ref> Figure 1: Example prompt and beginning of a story from our dataset. We train a hierarchical model that first generates a prompt, and then con- ditions on the prompt when generating a story.</p><p>We find that standard sequence-to-sequence (seq2seq) models <ref type="bibr" target="#b24">(Sutskever et al., 2014</ref>) applied to hierarchical story generation are prone to de- generating into language models that pay little at- tention to the writing prompt (a problem that has been noted in other domains, such as dialogue re- sponse generation ( <ref type="bibr" target="#b12">Li et al., 2015a)</ref>). This failure is due to the complex and underspecified depen- dencies between the prompt and the story, which are much harder to model than the closer depen- dencies required for language modeling (for exam- ple, consider the subtle relationship between the first sentence and prompt in <ref type="figure">Figure 1)</ref>.</p><p>To improve the relevance of the generated story to its prompt, we introduce a fusion mechanism ( <ref type="bibr" target="#b21">Sriram et al., 2017</ref>) where our model is trained on top of an pre-trained seq2seq model. To improve over the pre-trained model, the second model must focus on the link between the prompt and the story. For the first time, we show that fusion mechanisms can help seq2seq models build dependencies be- tween their input and output.</p><p>Another major challenge in story generation is the inefficiency of modeling long documents with standard recurrent architectures-stories contain 734 words on average in our dataset. We improve efficiency using a convolutional architecture, al- Experiments show that our fusion and self- attention mechanisms improve over existing tech- niques on both automated and human evaluation measures. Our new dataset and neural architec- tures allow for models which can creatively gen- erate longer, more consistent and more fluent pas- sages of text. Human judges prefer our hierarchi- cal model's stories twice as often as those of a non- hierarchical baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Writing Prompts Dataset</head><p>We collect a hierarchical story generation dataset 1 from Reddit's WRITINGPROMPTS forum. <ref type="bibr">2</ref> WRITINGPROMPTS is a community where online users inspire each other to write by submitting story premises, or prompts, and other users freely respond. Each prompt can have multiple story responses. The prompts have a large diversity of topic, length, and detail. The stories must be at least 30 words, avoid general profanity and inappropriate content, and should be inspired by the prompt (but do not necessarily have to fulfill every requirement). <ref type="figure">Figure 1</ref> shows an example.</p><p>We scraped three years of prompts and their associated stories using the official Reddit API. We clean the dataset by removing automated bot posts, deleted posts, special announcements, com-ments from moderators, and stories shorter than 30 words. We use NLTK for tokenization. The dataset models full text to generate immediately human-readable stories. We reserve 5% of the prompts for a validation set and 5% for a test set, and present additional statistics about the dataset in <ref type="table">Table 1</ref>.</p><p>For our experiments, we limit the length of the stories to 1000 words maximum and limit the vo- cabulary size for the prompts and the stories to words appearing more than 10 times each. We model an unknown word token and an end of doc- ument token. This leads to a vocabulary size of 19,025 for the prompts and 104,960 for the sto- ries. As the dataset is scraped from an online fo- rum, the number of rare words and misspellings is quite large, so modeling the full vocabulary is challenging and computationally intensive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>The challenges of WRITINGPROMPTS are primar- ily in modeling long-range dependencies and con- ditioning on an abstract, high-level prompt. Re- current and convolutional networks have success- fully modeled sentences ( <ref type="bibr" target="#b10">Jozefowicz et al., 2016;</ref>), but accurately modeling several paragraphs is an open problem. While seq2seq networks have strong performance on a variety of problems, we find that they are unable to build stories that accurately reflect the prompts. We will evaluate strategies to address these chal- lenges in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Hierarchical Story Generation</head><p>High-level structure is integral to good stories, but language models generate on a strictly-word-by- word basis and so cannot explicitly make high- level plans. We introduce the ability to plan by decomposing the generation process into two lev- els. First, we generate the premise or prompt of the story using the convolutional language model from . The prompt gives a sketch of the structure of the story. Second, we use a seq2seq model to generate a story that follows the premise. Conditioning on the prompt makes it easier for the story to remain consistent and also have structure at a level beyond single phrases. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Efficient Learning with Convolutional</head><p>Sequence-to-Sequence Model</p><p>The length of stories in our dataset is a challenge for RNNs, which process tokens sequentially. To transform prompts into stories, we instead build on the convolutional seq2seq model of <ref type="bibr" target="#b5">Gehring et al. (2017)</ref>, which uses deep convolutional net- works as the encoder and decoder. Convolutional models are ideally suited to modeling long se- quences, because they allow parallelism of com- putation within the sequence. In the Conv seq2seq model, the encoder and decoder are connected with attention modules ( <ref type="bibr" target="#b1">Bahdanau et al., 2015</ref>) that perform a weighted sum of encoder outputs, using attention at each layer of the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Modeling Unbounded Context with Gated Multi-Scale Self-attention</head><p>CNNs can only model a bounded context win- dow, preventing the modeling of long-range de- pendencies within the output story.</p><p>To en- able modeling of unbounded context, we supple- ment the decoder with a self-attention mechanism <ref type="bibr" target="#b22">(Sukhbaatar et al., 2015;</ref><ref type="bibr" target="#b25">Vaswani et al., 2017</ref>), which allows the model to refer to any previously generated words. The self-attention mechanism improves the model's ability to extract long-range context with limited computational impact due to parallelism.</p><p>Gated Attention: Similar to <ref type="bibr" target="#b25">Vaswani et al. (2017)</ref>, we use multi-head attention to allow each head to attend to information at different posi- tions. However, the queries, keys and values are not given by linear projections but by more expres- sive gated deep neural nets with Gated Linear Unit (  activations. We show that gating lends the self-attention mechanism crucial capacity to make fine-grained selections.</p><p>Multi-Scale Attention: Further, we propose to have each head operating at a different time scale, depicted in <ref type="figure" target="#fig_0">Figure 2</ref>. Thus the input to each head is downsampled a different amount-the first head sees the full input, the second every other input timestep, the third every third input timestep, etc. The different scales encourage the heads to attend to different information. The downsampling oper- ation limits the number of tokens in the attention maps, making them sharper.</p><p>The output of a single attention head is given by</p><formula xml:id="formula_0">h L+1 0:t = Linear v(h L 0:t−1 ) (1) softmax(q(h L 0:t )k(h L 0:t ) )</formula><p>where h L 0:t contains the hidden states up to time t at layer L, and q, k, v are gated downsampling net- works as shown in <ref type="figure" target="#fig_0">Figure 2</ref>. Unlike <ref type="bibr" target="#b25">Vaswani et al. (2017)</ref>, we allow the model to optionally attend to a 0 vector at each timestep, if it chooses to ignore the information of past timesteps (see <ref type="figure" target="#fig_1">Figure 3</ref>). This mechanism allows the model to recover the non-self-attention architecture and avoid attending to the past if it provides only noise. Additionally, we do not allow the self-attention mechanism to attend to the current timestep, only the past.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Improving Relevance to Input Prompt with Model Fusion</head><p>Unlike tasks such as translation, where the seman- tics of the target are fully specified by the source, the generation of stories from prompts is far more open-ended. We find that seq2seq models ignore the prompt and focus solely on modeling the sto- ries, because the local dependencies required for language modeling are easier to model than the subtle dependencies between prompt and story. We propose a fusion-based approach to en- courage conditioning on the prompt. We train a seq2seq model that has access to the hidden states of a pretrained seq2seq model. Doing so can be seen as a type of boosting or residual learning that allows the second model to focus on what the first model failed to learn-such as conditioning on the prompt. To our knowledge, this paper is the first to show that fusion reduces the problem of seq2seq models degenerating into language mod- els that capture primarily syntactic and grammati- cal information.</p><p>The cold fusion mechanism of <ref type="bibr" target="#b21">Sriram et al. (2017)</ref> pretrains a language model and subse- quently trains a seq2seq model with a gating mechanism that learns to leverage the final hidden layer of the language model during seq2seq train- ing. We modify this approach by combining two seq2seq models as follows (see <ref type="figure" target="#fig_2">Figure 4</ref>):</p><formula xml:id="formula_1">g t = σ(W [h Training t ; h Pretrained t ] + b) h t = g t • [h Training t ; h Pretrained t ]</formula><p>where the hidden state of the pretrained seq2seq model and training seq2seq model (represented by h t ) are concatenated to learn gates g t . The gates are computed using a linear projection with the weight matrix W . The gated hidden layers are combined by concatenation and followed by more fully connected layers with GLU activations (see Appendix). We use layer normalization ( <ref type="bibr">Ba et al., 2016)</ref> after each fully connected layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Story Generation</head><p>Sequence-to-sequence neural networks <ref type="bibr" target="#b24">(Sutskever et al., 2014</ref>) have achieved state of the art perfor- mance on a variety of text generation tasks, such as machine translation <ref type="bibr" target="#b24">(Sutskever et al., 2014</ref>) and summarization ( <ref type="bibr">Rush et al., 2015)</ref>. Recent work has applied these models to more open-ended gen- eration tasks, including writing Wikipedia articles ( <ref type="bibr" target="#b14">Liu et al., 2018</ref>) and poetry <ref type="bibr" target="#b29">(Zhang and Lapata, 2014)</ref>.</p><p>Previous work on story generation has explored seq2seq RNN architectures <ref type="bibr" target="#b18">(Roemmele, 2016)</ref>, but has focused largely on using various content to inspire the stories. For instance, <ref type="bibr" target="#b11">Kiros et al. (2015)</ref> uses photos to inspire short paragraphs trained on romance novels, and Jain et al. (2017) chain a se- ries of independent descriptions together into a short story. <ref type="bibr" target="#b15">Martin et al. (2017)</ref> decompose story generation into two steps, first converting text into event representations, then modeling stories as se- quences of events before translating back to natu- ral language. Similarly,  gen- erate summaries of movies as sequences of events using an RNN, then sample event representations using MCMC. They find this technique can gener- ate text of the desired genre, but the movie plots are not interpretable (as the model outputs events, not raw text). However, we are not aware of pre- vious work that has used hierarchical generation from a textual premise to improve the coherence and structure of stories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Hierarchical Text Generation</head><p>Previous work has proposed decomposing the challenge of generating long sequences of text into a hierarchical generation task. For instance, <ref type="bibr">Li et al. (2015b)</ref> use an LSTM to hierarchically learn word, then sentence, then paragraph embed- dings, then transform the paragraph embeddings into text. <ref type="bibr" target="#b28">Yarats and Lewis (2017)</ref> generate a dis- crete latent variable based on the context, then generates text conditioned upon it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Fusion Models</head><p>Previous work has investigated the integration of language models with seq2seq models. The two models can be leveraged together without ar- chitectural modifications: <ref type="bibr" target="#b17">Ramachandran et al. (2016)</ref> use language models to initialize the en- coder and decoder side of the seq2seq model inde- pendently, and Chorowski and Jaitly (2016) com- bine the predictions of the language model and seq2seq model solely at inference time. Recent work has also proposed deeper integration. <ref type="bibr" target="#b6">Gulcehre et al. (2015)</ref> combined a trained language model with a trained seq2seq model to learn a gat- ing function that joins them. <ref type="bibr" target="#b21">Sriram et al. (2017)</ref> propose training the seq2seq model given the fixed language model then learning a gate to filter the in- formation from the language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Baselines</head><p>We evaluate a number of baselines:</p><p>(1) Language Models: Non-hierarchical models for story generation, which do not condition on the prompt. We use both the gated convolutional lan- guage (GCNN) model of  and our additional self-attention mechanism.</p><p>(2) seq2seq: using LSTMs and convolutional seq2seq architectures, and Conv seq2seq with de- coder self-attention.</p><p>(3) Ensemble: an ensemble of two Conv seq2seq with self-attention models.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Fusion Training</head><p>To train the fusion model, we first pretrain a Conv seq2seq with self-attention model on the WRIT- INGPROMPTS dataset. This pretrained model is fixed and provided to the second Conv seq2seq with self-attention model during training time.</p><p>The two models are integrated with the fusion mechanism described in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Training</head><p>We implement models with the fairseq-py library in PyTorch. Similar to <ref type="bibr" target="#b5">Gehring et al. (2017)</ref>, we train using the Nesterov accelerated gradient method (Sutskever et al., 2013) using gradient clipping ( <ref type="bibr" target="#b16">Pascanu et al., 2013</ref>). We perform hy- perparameter optimization on each of our models by cross-validating with random search on a vali- dation set. We provide model architectures in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Generation</head><p>We generate stories from our models using a top-k random sampling scheme. At each timestep, the model generates the probability of each word in the vocabulary being the likely next word. We randomly sample from the k = 10 most likely candidates from this distribution. Then, subse- quent timesteps generate words based on the pre- viously selected words. We find this sampling strategy substantially more effective than beam search, which tends to produce common phrases and repetitive text from the training set ( <ref type="bibr" target="#b26">Vijayakumar et al., 2016;</ref><ref type="bibr" target="#b20">Shao et al., 2017</ref>  <ref type="table">Table 3</ref>: Perplexity on WRITINGPROMPTS. We dramatically improve over standard seq2seq models. <ref type="figure">Figure 5</ref>: Human accuracy at pairing stories with the prompts used to generate them. People find that our fusion model significantly improves the link between the prompt and generated stories.</p><p>duced by beam search tend to be short and generic. Completely random sampling can introduce very unlikely words, which can damage generation as the model has not seen such mistakes at training time. The restriction of sampling from the 10 most likely candidates reduces the risk of these low- probability samples. For each model, we tune a temperature parame- ter for the softmax at generation time. To ease hu- man evaluation, we generate stories of 150 words and do not generate unknown word tokens.</p><p>For prompt generation, we use a self- attentive GCNN language model trained with the same prompt-side vocabulary as the sequence-to- sequence story generation models. The language model to generate prompts has a validation per- plexity of 63.06. Prompt generation is conducted using the top-k random sampling from the 10 most likely candidates, and the prompt is completed when the language model generates the end of prompt token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Evaluation</head><p>We propose a number of evaluation metrics to quantify the performance of our models. Many commonly used metrics, such as BLEU for ma-  : Accuracy on the prompt/story pairing task vs. number of generated stories. Our genera- tive fusion model can produce many stories with- out degraded performance, while the KNN can only produce a limited number relevant stories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Human Preference</head><p>Language model 32.68% Hierarchical Model 67.32% <ref type="table">Table 4</ref>: Effect of Hierarchical Generation. Hu- man judges prefer stories that were generated hier- archically by first creating a premise and creating a full story based on it with a seq2seq model. chine translation or ROUGE for summarization, compute an n-gram overlap between the generated text and the human text-however, in our open- ended generation setting, these are not useful. We do not aim to generate a specific story; we want to generate viable and novel stories. We focus on measuring both the fluency of our models and their ability to adhere to the prompt.</p><p>For automatic evaluation, we measure model perplexity on the test set and prompt ranking accu- racy. Perplexity is commonly used to evaluate the quality of language models, and it reflects how flu- ently the model can produce the correct next word given the preceding words. We use prompt rank- ing to assess how strongly a model's output de- pends on its input. Stories are decoded under 10 different prompts-9 randomly sampled prompts and 1 true corresponding prompt-and the like- lihood of the story given the various prompts is recorded. We measure the percentage of cases where the true prompt is the most likely to gen- erate the story. In our evaluation, we examined 1000 stories from the test set for each model.</p><p>For human evaluation, we use Amazon Me- chanical Turk to conduct a triple pairing task. We use each model to generate stories based on held- out prompts from the test set. Then, groups of three stories are presented to the human judges. The stories and their corresponding prompts are shuffled, and human evaluators are asked to se- lect the correct pairing for all three prompts. 105 stories per model are grouped into questions, and each question is evaluated by 15 judges.</p><p>Lastly, we conduct human evaluation to evalu- ate the importance of hierarchical generation for story writing. We use Amazon Mechanical Turk to compare the stories from hierarchical generation from a prompt with generation without a prompt. 400 pairs of stories were evaluated by 5 judges each in a blind test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>We analyze the effect of our modeling improve- ments on the WRITINGPROMPTS dataset.</p><p>Effect of Hierarchical Generation: We ex- plore leveraging our dataset to perform hierarchi- cal story generation by first using a self-attentive GCNN language model to generate a prompt, and then using a fusion model to write a story given the generated prompt. We evaluate the effect of hierarchical generation using a human study in Ta- ble 4. 400 stories were generated from a self- attentive GCNN language model, and another 400 were generated from our hierarchical fusion model given generated prompts from a language model. In a blind comparison where raters were asked to choose the story they preferred reading, human raters preferred the hierarchical model 67% of the time.</p><p>Effect of new attention mechanism: <ref type="table" target="#tab_2">Table 2</ref> shows the effect of the proposed additions to the self-attention mechanism proposed by <ref type="bibr" target="#b25">Vaswani et al. (2017)</ref>. <ref type="table">Table 3</ref> shows that deep multi-scale self-attention and fusion each significantly im- prove the perplexity compared to the baselines. In combination these additions to the Conv seq2seq baseline reduce the perplexity by 9 points. <ref type="table">Table 3</ref> show that adding our fusion mechanism substantially improves the likelihood of human-generated sto- ries, and even outperforms an ensemble despite having fewer parameters. We observe in <ref type="figure">Figure  5</ref> that fusion has a much more significant impact on the topicality of the stories. In comparison, en- sembling has no effect on people's ability to as- sociate stories with a prompt, but adding model fusion leads improves the pairing accuracy of the human judges by 7%. These results suggest that by training a second model on top of the first, we have encouraged that model to learn the challeng-ing additional dependencies to relate to the source sequence. To our knowledge, these are the first results to show that fusion has such capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of model fusion: Results in</head><p>Comparison with Nearest Neighbours: Near- est Neighbour Search (KNN) provides a strong baseline for text generation. <ref type="figure">Figure 5</ref> shows that the fusion model can match the performance of nearest neighbour search in terms of the connec- tion between the story and prompt. The real value in our generative approach is that it can produce an unlimited number of stories, whereas KNN can never generalize from its training data. To quan- tify this improvement, <ref type="figure" target="#fig_4">Figure 7</ref> plots the relevance of the kth best story to a given prompt; the perfor- mance of KNN degrades much more rapidly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Generation Quality</head><p>Our proposed fusion model is capable of gener- ating unique text without copying directly from the training set. When analyzing 500 150-word generated stories from test-set prompts, the aver- age longest common subsequence is 8.9. In con- trast, the baseline Conv seq2seq model copies 10.2 words on average and the KNN baseline copies all 150 words from a story in the training set. <ref type="figure" target="#fig_5">Figure 8</ref> shows the values of the fusion gates for an example story, averaged at each timestep. The pretrained seq2seq model acts similarly to a lan- guage model producing common words and punc- tuation. The second seq2seq model learns to focus on rare words, such as horned and robe.</p><p>However, the fusion model has limitations. Us- ing random sampling to generate can produce er- rors. For example, can't is tokenized to ca n't, and the model occasionally produces the first to- ken but misses the second. A similar error is after one line of dialogue, the model may move to an- other line of dialogue without generating a new- line token. A further obstacle is repetition. The model focuses frequently on what it has recently produced, which leads to the generation of similar text multiple times.</p><p>In the generation of prompts using the GCNN language model, we find that prompts are fairly generic compared to human prompts. Language models often struggle to model rare words accu- rately, as the probability distribution over the next word is dominated by more common words. This tends to produce similar prompts, particularly at the start -we see many prompts that start with the man. In contrast, many of the human prompts are very unique (e.g. prompting stories in fantasy worlds such as Harry Potter and Game of Thrones) and the language model rarely produces the spe- cific vocabulary required by these settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Use of Attention</head><p>We analyze the encoder-decoder attention in the fusion model and find that unlike attention maps in machine translation, where each decoder timestep tends to attend to a different word on the encoder- side, the attention map for each decoder timestep looks similar and focuses mainly on salient words in the prompt. We further look at the usage of the self-attention layers within the decoder. While they could be leveraged to look at words generated very far in the past, at many timesteps the self- attention focuses on the recent past.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We have collected the first dataset for creative text generation based on short writing prompts. This new dataset pushes the boundaries of text gen- eration by requiring longer range dependencies and conditioning on an abstract premise. Build- ing on this dataset, we show through automatic and human evaluation that novel hierarchical mod- els, self-attention mechanisms and model fusion significantly improves the fluency, topicality, and overall quality of the generated stories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generation with Fusion of Conv seq2seq + Self-Attention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example Prompt 1: Aliens start abducting humans</head><p>It has been two weeks, and the last of my kind has gone. It is only a matter of time until there will be nothing left. I'm not sure what the hell is going on... I can't think. I can hear a distant scream. I think of a strange, alien sound. I try to speak, but am interrupted by something, something that feels like a drum, I ca not tell. I mean I'm just a little bit older than an average human. But I can, and I can feel the vibrations . I hear the sound of a ship approaching. The ground quakes at the force of the impact, and a loud explosion shatters the silence.</p><p>Example Prompt 2: The scientists have discovered something terrible .</p><p>The scientist stood there, a little dazed as he stared. "What is it?" He asked. "This...this...Thing...This is a virus. A chemical that can destroy entire planet and it is a very small, complex, chemical that could destroy any planet." The scientist replied. His lab assistant looked down at the tablet. "I've just discovered it. I can't believe it. It looks like it's made of some sort of chemical that's very dangerous." " Well, there's a virus on the ground. It's very effective..." " I can't believe what it is." he said, pointing to the scientist . " We don't know what this thing is. We haven't seen anything like it . We can't even see anything like this. " Dr. Jones stared at the scientist for a moment. "What do you mean what does it do ?" "It...It 's a monster."</p><p>Example Prompt 3: The silence before the storm comes "I'm sorry," she said "I can't do this anymore." She sat in the grass, the grass blowing behind her as she sat staring at the sky. Her eyes were still closed but she was still staring at it. Her hand was still shaking, her eyes closed as she watched the rain roll in and out of view. She could see the world through her eyelids now. Her hands were still shaking like she hadn't eaten in days. Her fingers were still clenched tight and her fingers were now shaking. She wasn't even sure if she was going to cry or if she just wanted to cry. It wasn't until she was close enough to hear the words. "I ...I just...I need to...I ..." She was crying now and then, the rain was picking up. Her lips were shaking now and she could see that the storm was getting closer and closer. Her heart stopped when she heard the thunder. She knew it was coming. The storm was The man was an accountant. He had to be. He had to be the next president. I looked back over the top and saw that his wife was crying in the kitchen. I looked at the clock. It seemed to be coming slower, but I knew if I did it would not be long before I was in my own home. I wasn't sure. I had a hard time finding the right words to say. I was about to leave when he suddenly became angry and began talking to me. "Hello, sir, I'm John. What is your name?" "My name is Manuel and I'm a journalist." I said <ref type="table">Table 5</ref>: Example stories generated by the proposed hierarchical fusion approach compared to stories generated by a language model. Stories generated by the fusion model relate to the desired prompt and show increased coherence between sentences and ability to stay on one topic compared to the language modeling baseline.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Self-Attention Mechanism of a single head, with GLU gating and downsampling. Multiple heads are concatenated, with each head using a separate downsampling function.</figDesc><graphic url="image-1.png" coords="3,101.76,62.80,158.74,282.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Multihead self-attention mechanism. The decoder layer depicted attends with itself to gate the input of the subsequent decoder layer.</figDesc><graphic url="image-2.png" coords="3,325.70,62.81,181.40,223.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Diagram of our fusion model, which learns a second seq2seq model to improve a pretrained model. The separate hidden states are combined after gating through concatenation.</figDesc><graphic url="image-3.png" coords="4,325.70,62.81,181.41,203.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Accuracy of prompt ranking. The fusion model most accurately pairs prompt and stories.</figDesc><graphic url="image-5.png" coords="6,307.28,215.88,226.75,118.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7</head><label>7</label><figDesc>Figure 7: Accuracy on the prompt/story pairing task vs. number of generated stories. Our generative fusion model can produce many stories without degraded performance, while the KNN can only produce a limited number relevant stories.</figDesc><graphic url="image-6.png" coords="6,307.28,387.32,226.77,129.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Average weighting of each model in our Fusion model for the beginning of the generated story for the prompt Gates of Hell. The fused model (orange) is primarily used for words which are closely related to the prompt, whereas generic words are generated by the pre-trained model (green).</figDesc><graphic url="image-7.png" coords="7,83.34,62.81,430.86,63.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Generation with Language Model + Self-Attention Baseline Example 1: I've always been the curious type. Growing up in the ghetto, I've been around for more than a year now. I still haven't heard it on my own. I'm not sure if I'm a good person. But I'm not the only one. I've been to the hospital, but it's not my first time. I've been to my psychiatrist's journals before. "Hey, are you ... are you okay?" I asked. "Yes" I said. "What's wrong?" she replied, "I was just ... Doing something." She said, "I've never been a bad person." Example 2:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>303,358 human generated stories paired with writing prompts from an online forum. Evaluating free form text is challenging, so we also introduce new evaluation metrics which isolate different as- pects of story generation.</figDesc><table># Train Stories 
272,600 
# Test Stories 
15,138 
# Validation Stories 
15,620 
# Prompt Words 
7.7M 
# Story Words 
200M 
Average Length of Prompts 
28.4 
Average Length of Stories 
734.5 

Table 1: Statistics of WRITINGPROMPTS dataset 

lowing whole stories to be encoded in parallel. 
Existing convolutional architectures only encode a 
bounded amount of context (Dauphin et al., 2017), 
so we introduce a novel gated self-attention mech-
anism that allows the model to condition on its 
previous outputs at different time-scales. 
To train our models, we gathered a large dataset 
of </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>KNN: we also compare with a KNN model to find the closest prompt in the training set for each prompt in the test set. A TF-IDF vector for</figDesc><table>Model 
Valid 
Perplexity 

Test 
Perplexity 

Conv seq2seq 
45.27 
45.54 
+ self-attention 42.01 
42.32 
+ multihead 
40.12 
40.39 
+ multiscale 
38.76 
38.91 
+ gating 
37.37 
37.94 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Effect of new attention mechanism. 
Gated multi-scale attention significantly improves 
the perplexity on the WRITINGPROMPTS dataset. 

each prompt was created using FASTTEXT (Bo-
janowski et al., 2016) and FAISS (Johnson et al., 
2017) was used for KNN search. The retrieved 
story from the training set is limited to 150 words 
to match the length of generated stories. 

</table></figure>

			<note place="foot" n="1"> www.github.com/pytorch/fairseq 2 www.reddit.com/r/WritingPrompts/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint/>
	</monogr>
<note type="report_type">ton. 2016. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>International Conference on Learning Representation (ICLR</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.04606</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Towards better decoding and language model integration in sequence to sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02695</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grangier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huei-Chi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.03535</idno>
		<title level="m">On using monolingual corpora in neural machine translation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Toward automated story generation with markov chain monte carlo methods and deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brent</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Purdy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Story generation from sequence of independent short descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parag</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyanka</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohak</forename><surname>Sukhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Laha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05501</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Billion-scale similarity search with gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08734</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Exploring the limits of language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02410</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Raquel Urtasun, and Sanja Fidler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.06726</idno>
	</analytic>
	<monogr>
		<title level="m">Skip-thought vectors</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.03055</idno>
		<title level="m">A diversity-promoting objective function for neural conversation models</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.01057</idno>
		<title level="m">2015b. A hierarchical neural autoencoder for paragraphs and documents</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Pot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Goodrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.10198</idno>
		<title level="m">Generating wikipedia by summarizing long sequences</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithviraj</forename><surname>Lara J Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Ammanabrolu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shruti</forename><surname>Hancock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brent</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harrison</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.01331</idno>
		<title level="m">Event representations for automated story generation with deep neural nets</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>In ICML</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Unsupervised pretraining for sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02683</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Writing stories with help from recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melissa</forename><surname>Roemmele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>In AAAI</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alexander M Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.00685</idno>
		<title level="m">Sumit Chopra, and Jason Weston. 2015. A neural attention model for abstractive sentence summarization</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Britz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Goldie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ray</forename><surname>Kurzweil</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.03185</idno>
		<title level="m">Generating long and diverse responses with neural conversation models</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Cold fusion: Training seq2seq models together with language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuroop</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.06426</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Diverse beam search: Decoding diverse solutions from neural sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Vijayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02424</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Challenges in data-to-document generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.08052</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Hierarchical text generation and planning for strategic dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.05846</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Chinese poetry generation with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="670" to="680" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
