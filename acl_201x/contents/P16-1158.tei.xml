<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Evaluating the Utility of Vector Differences for Lexical Relation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Vylomova</surname></persName>
							<email>evylomova@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing and Information Systems</orgName>
								<orgName type="institution">University of Melbourne</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Rimell</surname></persName>
							<email>laura.rimell@cl.cam.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Computer Laboratory</orgName>
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing and Information Systems</orgName>
								<orgName type="institution">University of Melbourne</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing and Information Systems</orgName>
								<orgName type="institution">University of Melbourne</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Evaluating the Utility of Vector Differences for Lexical Relation Learning</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1671" to="1682"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
					<note>Take and Took, Gaggle and Goose, Book and Read:</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recent work has shown that simple vector subtraction over word embeddings is surprisingly effective at capturing different lexical relations, despite lacking explicit supervision. Prior work has evaluated this intriguing result using a word analogy prediction formulation and hand-selected relations, but the generality of the finding over a broader range of lexical relation types and different learning settings has not been evaluated. In this paper, we carry out such an evaluation in two learning settings: (1) spectral clustering to induce word relations , and (2) supervised learning to classify vector differences into relation types. We find that word embeddings capture a surprising amount of information, and that, under suitable supervised training, vector subtraction generalises well to a broad range of relations, including over unseen lexical items.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning to identify lexical relations is a fundamen- tal task in natural language processing ("NLP"), and can contribute to many NLP applications including paraphrasing and generation, machine translation, and ontology building ( <ref type="bibr">Banko et al., 2007;</ref><ref type="bibr" target="#b2">Hendrickx et al., 2010)</ref>.</p><p>Recently, attention has been focused on iden- tifying lexical relations using word embeddings, which are dense, low-dimensional vectors ob- tained either from a "predict-based" neural net- work trained to predict word contexts, or a "count- based" traditional distributional similarity method combined with dimensionality reduction. The skip- gram model of <ref type="bibr" target="#b14">Mikolov et al. (2013a)</ref> and other similar language models have been shown to per- form well on an analogy completion task ( <ref type="bibr" target="#b15">Mikolov et al., 2013b;</ref><ref type="bibr" target="#b16">Mikolov et al., 2013c;</ref><ref type="bibr" target="#b8">Levy and Goldberg, 2014a)</ref>, in the space of relational sim- ilarity prediction <ref type="bibr" target="#b31">(Turney, 2006)</ref>, where the task is to predict the missing word in analogies such as A:B :: C: -?-. A well-known example involves predicting the vector queen from the vector com- bination king − man + woman, where linear operations on word vectors appear to capture the lexical relation governing the analogy, in this case OPPOSITE-GENDER. The results extend to several semantic relations such as CAPITAL-OF (paris−france+poland ≈ warsaw) and mor- phosyntactic relations such as PLURALISATION (cars − car + apple ≈ apples). Remarkably, since the model is not trained for this task, the re- lational structure of the vector space appears to be an emergent property.</p><p>The key operation in these models is vector dif- ference, or vector offset. For example, the paris − france vector appears to encode CAPITAL-OF, pre- sumably by cancelling out the features of paris that are France-specific, and retaining the features that distinguish a capital city ( <ref type="bibr" target="#b8">Levy and Goldberg, 2014a)</ref>. The success of the simple offset method on analogy completion suggests that the difference vectors ("DIFFVEC" hereafter) must themselves be meaningful: their direction and/or magnitude encodes a lexical relation.</p><p>Previous analogy completion tasks used with word embeddings have limited coverage of lexical relation types. Moreover, the task does not explore the full implications of DIFFVECs as meaningful vector space objects in their own right, because it only looks for a one-best answer to the particu- lar lexical analogies in the test set. In this paper, we introduce a new, larger dataset covering many well-known lexical relation types from the linguis- tics and cognitive science literature. We then apply DIFFVECs to two new tasks: unsupervised and su- pervised relation extraction. First, we cluster the DIFFVECs to test whether the clusters map onto true lexical relations. We find that the clustering works remarkably well, although syntactic relations are captured better than semantic ones.</p><p>Second, we perform classification over the DIFF- VECs and obtain remarkably high accuracy in a closed-world setting (over a predefined set of word pairs, each of which corresponds to a lexical re- lation in the training data). When we move to an open-world setting including random word pairs -many of which do not correspond to any lexical relation in the training data -the results are poor. We then investigate methods for better attuning the learned class representation to the lexical relations, focusing on methods for automatically synthesis- ing negative instances. We find that this improves the model performance substantially.</p><p>We also find that hyper-parameter optimised count-based methods are competitive with predict- based methods under both clustering and super- vised relation classification, in line with the find- ings of <ref type="bibr" target="#b10">Levy et al. (2015a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Related Work</head><p>A lexical relation is a binary relation r holding be- tween a word pair (w i , w j ); for example, the pair (cart, wheel) stands in the WHOLE-PART relation. Relation learning in NLP includes relation extrac- tion, relation classification, and relational similarity prediction. In relation extraction, related word pairs in a corpus and the relevant relation are identified. Given a word pair, the relation classification task in- volves assigning a word pair to the correct relation from a pre-defined set. In the Open Information Ex- traction paradigm ( <ref type="bibr">Banko et al., 2007;</ref><ref type="bibr" target="#b35">Weikum and Theobald, 2010)</ref>, also known as unsupervised re- lation extraction, the relations themselves are also learned from the text (e.g. in the form of text labels). On the other hand, relational similarity prediction involves assessing the degree to which a word pair (A, B) stands in the same relation as another pair <ref type="bibr">(C, D)</ref>, or to complete an analogy A:B :: C: -?-. Re- lation learning is an important and long-standing task in NLP and has been the focus of a number of shared tasks ( <ref type="bibr" target="#b1">Girju et al., 2007;</ref><ref type="bibr" target="#b2">Hendrickx et al., 2010;</ref><ref type="bibr" target="#b3">Jurgens et al., 2012)</ref>.</p><p>Recently, attention has turned to using vector space models of words for relation classification and relational similarity prediction. Distributional word vectors have been used for detection of rela- tions such as hypernymy <ref type="bibr">(Geffet and Dagan, 2005;</ref><ref type="bibr" target="#b6">Kotlerman et al., 2010;</ref><ref type="bibr" target="#b7">Lenci and Benotto, 2012;</ref><ref type="bibr" target="#b34">Weeds et al., 2014;</ref><ref type="bibr" target="#b22">Rimell, 2014;</ref><ref type="bibr" target="#b26">Santus et al., 2014</ref>) and qualia structure ( <ref type="bibr" target="#b37">Yamada et al., 2009</ref>).</p><p>An exciting development, and the inspiration for this paper, has been the demonstration that vec- tor difference over word embeddings ( <ref type="bibr" target="#b16">Mikolov et al., 2013c)</ref> can be used to model word anal- ogy tasks. This has given rise to a series of pa- pers exploring the DIFFVEC idea in different con- texts. The original analogy dataset has been used to evaluate predict-based language models by <ref type="bibr" target="#b18">Mnih and Kavukcuoglu (2013)</ref> and also <ref type="bibr" target="#b39">Zhila et al. (2013)</ref>, who combine a neural language model with a pattern-based classifier. <ref type="bibr" target="#b4">Kim and de Marneffe (2013)</ref> use word embeddings to derive representa- tions of adjective scales, e.g. hot-warm-cool- cold. <ref type="bibr">Fu et al. (2014)</ref> similarly use embeddings to predict hypernym relations, in this case clustering words by topic to show that hypernym DIFFVECs can be broken down into more fine-grained rela- tions. Neural networks have also been developed for joint learning of lexical and relational similar- ity, making use of the WordNet relation hierarchy ( <ref type="bibr">Bordes et al., 2013;</ref><ref type="bibr" target="#b27">Socher et al., 2013;</ref><ref type="bibr" target="#b36">Xu et al., 2014;</ref><ref type="bibr" target="#b38">Yu and Dredze, 2014;</ref><ref type="bibr">Faruqui et al., 2015;</ref><ref type="bibr">Fried and Duh, 2015)</ref>.</p><p>Another strand of work responding to the vector difference approach has analysed the structure of predict-based embedding models in order to help explain their success on the analogy and other tasks ( <ref type="bibr" target="#b8">Levy and Goldberg, 2014a;</ref><ref type="bibr" target="#b9">Levy and Goldberg, 2014b;</ref><ref type="bibr" target="#b0">Arora et al., 2015)</ref>. However, there has been no systematic investigation of the range of relations for which the vector difference method is most effective, although there have been some smaller- scale investigations in this direction. <ref type="bibr">Makrai et al. (2013)</ref> divide antonym pairs into semantic classes such as quality, time, gender, and distance, find- ing that for about two-thirds of antonym classes, DIFFVECs are significantly more correlated than random. Necs¸ulescu <ref type="bibr" target="#b19">Necs¸ulescu et al. (2015)</ref> train a classifier on word pairs, using word embeddings to predict coordinates, hypernyms, and meronyms. <ref type="bibr" target="#b23">Roller and Erk (2016)</ref> analyse the performance of vector con- catenation and difference on the task of predicting lexical entailment and show that vector concatena- tion overwhelmingly learns to detect Hearst pat- terns (e.g., including, such as). <ref type="bibr">Köper et al. (2015)</ref> undertake a systematic study of morphosyntac- tic and semantic relations on word embeddings produced with word2vec ("w2v" hereafter; see §3.1) for English and German. They test a variety of relations including word similarity, antonyms, synonyms, hypernyms, and meronyms, in a novel analogy task. Although the set of relations tested by <ref type="bibr">Köper et al. (2015)</ref> is somewhat more constrained than the set we use, there is a good deal of overlap. However, their evaluation is performed in the con- text of relational similarity, and they do not perform clustering or classification on the DIFFVECs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">General Approach and Resources</head><p>We define the task of lexical relation learning to take a set of (ordered) word pairs {(w i , w j )} and a set of binary lexical relations R = {r k }, and map each word pair (w i , w j ) as follows: (a) (w i , w j ) → r k ∈ R, i.e. the "closed-world" set- ting, where we assume that all word pairs can be uniquely classified according to a relation in R; or (b) (w i , w j ) → r k ∈ R ∪ {φ} where φ signifies the fact that none of the relations in R apply to the word pair in question, i.e. the "open-world" setting. Our starting point for lexical relation learning is the assumption that important information about various types of relations is implicitly embedded in the offset vectors. While a range of methods have been proposed for composing word vectors ( <ref type="bibr">Baroni et al., 2012;</ref><ref type="bibr" target="#b34">Weeds et al., 2014;</ref><ref type="bibr" target="#b24">Roller et al., 2014)</ref>, in this research we focus exclusively on DIFFVEC (i.e. w 2 − w 1 ). A second assumption is that there exist dimensions, or directions, in the embedding vector spaces responsible for a particular lexical relation. Such dimensions could be identified and exploited as part of a clustering or classification method, in the context of identifying relations be- tween word pairs or classes of DIFFVECs.</p><p>In order to test the generalisability of the DIFF- VEC method, we require: (1) word embeddings, and (2) a set of lexical relations to evaluate against. As the focus of this paper is not the word embed- ding pre-training approaches so much as the utility of the DIFFVECs for lexical relation learning, we take a selection of four pre-trained word embed- dings with strong currency in the literature, as de- tailed in §3.1. We also include the state-of-the-art count-based approach of <ref type="bibr" target="#b10">Levy et al. (2015a)</ref>, to test the generalisability of DIFFVECs to count-based word embeddings.</p><p>For the lexical relations, we want a range of rela- tions that is representative of the types of relational learning tasks targeted in the literature, and where there is availability of annotated data. To this end, we construct a dataset from a variety of sources, fo- cusing on lexical semantic relations (which are less well represented in the analogy dataset of <ref type="bibr" target="#b16">Mikolov et al. (2013c)</ref>), but also including morphosyntactic and morphosemantic relations (see §3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Name</head><p>Dimensions Training data <ref type="bibr">w2v</ref> 300 100 × 10 9 GloVe 200 6 × 10 9 SENNA 100 37 × 10 6 HLBL 200 37 × 10 6 w2v wiki 300 50 × 10 6 GloVe wiki 300 50 × 10 6 SVD wiki 300 50 × 10 6 <ref type="table">Table 1</ref>: The pre-trained word embeddings used in our experiments, with the number of dimensions and size of the training data (in word tokens). The models trained on English Wikipedia ("wiki") are in the lower half of the table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Word Embeddings</head><p>We consider four highly successful word embed- ding models in our experiments: <ref type="bibr">w2v</ref>   <ref type="bibr">w2v</ref> and GloVe (which we call w2v wiki and GloVe wiki , respectively) on the English Wikipedia corpus (comparable in size to the training data of SENNA and HLBL), and apply the preprocessing of <ref type="bibr" target="#b10">Levy et al. (2015a)</ref>. We additionally normalise the w2v wiki and SVD wiki vectors to unit length; GloVe wiki is natively normalised by column. 1 w2v CBOW (Continuous Bag-Of-Words; <ref type="bibr" target="#b14">Mikolov et al. (2013a)</ref>) predicts a word from its context using a model with the objective:</p><formula xml:id="formula_0">J = 1 T T i=1 log exp w i j∈[−c,+c],j =0˜w =0˜ =0˜w i+j V k=1 exp w k j∈[−c,+c],j =0˜w =0˜ =0˜w i+j</formula><p>where w i and˜wand˜ and˜w i are the vector representations for the ith word (as a focus or context word, re- spectively), V is the vocabulary size, T is the number of tokens in the corpus, and c is the con- text window size. <ref type="bibr">2</ref> Google News data was used  to train the model. We use the focus word vec-</p><formula xml:id="formula_1">tors, W = {w k } V k=1 , normalised such that each w k = 1.</formula><p>The GloVe model ( <ref type="bibr" target="#b21">Pennington et al., 2014</ref>) is based on a similar bilinear formulation, framed as a low-rank decomposition of the matrix of corpus co-occurrence frequencies:</p><formula xml:id="formula_2">J = 1 2 V i,j=1 f (P ij )(w i ˜ w j − log P ij ) 2 ,</formula><p>where w i is a vector for the left context, w j is a vector for the right context, P ij is the relative fre- quency of word j in the context of word i, and f is a heuristic weighting function to balance the in- fluence of high versus low term frequencies. The model was trained on English Wikipedia and the English Gigaword corpus version 5. The SVD model ( <ref type="bibr" target="#b10">Levy et al., 2015a</ref>) uses pos- itive pointwise mutual information (PMI) matrix defined as: HLBL ( <ref type="bibr" target="#b17">Mnih and Hinton, 2009</ref>) is a log-bilinear formulation of an n-gram language model, which predicts the ith word based on context words (i − n, . . . , i − 2, i − 1). This leads to the following training objective:</p><formula xml:id="formula_3">PPMI(w, c) = max(logˆP logˆ logˆP (w, c) ˆ P (w) ˆ P (c) , 0) , wherê P (w, c</formula><formula xml:id="formula_4">J = 1 T T i=1 exp( ˜ w i w i + b i ) V k=1 exp( ˜ w i w k + b k ) ,</formula><p>duty, denoting either the embedding for the ith token, wi, or kth word type, w k .</p><p>where˜wwhere˜ where˜w i = n−1 j=1 C j w i−j is the context embed- ding, C j is a scaling matrix, and b * is a bias term.</p><p>The final model, SENNA (Collobert and Weston, 2008), was initially proposed for multi-task train- ing of several language processing tasks, from lan- guage modelling through to semantic role labelling. Here we focus on the statistical language modelling component, which has a pairwise ranking objective to maximise the relative score of each word in its local context:</p><formula xml:id="formula_5">J = 1 T T i=1 V k=1 max 0, 1 − f (w i−c , . . . , w i−1 , w i ) + f (w i−c , . . . , w i−1 , w k ) ,</formula><p>where the last c − 1 words are used as context, and f (x) is a non-linear function of the input, defined as a multi-layer perceptron.</p><p>For HLBL and SENNA, we use the pre-trained embeddings from <ref type="bibr" target="#b30">Turian et al. (2010)</ref>, trained on the Reuters English newswire corpus. In both cases, the embeddings were scaled by the global stan- dard deviation over the word-embedding matrix,</p><formula xml:id="formula_6">W scaled = 0.1 × W σ(W )</formula><p>. For w2v wiki , GloVe wiki and SVD wiki we used English Wikipedia. We followed the same prepro- cessing procedure described in <ref type="bibr" target="#b10">Levy et al. (2015a)</ref>, <ref type="bibr">3</ref> i.e., lower-cased all words and removed non-textual elements. During the training phase, for each model we set a word frequency threshold of 5. For the SVD model, we followed the recommendations of <ref type="bibr" target="#b10">Levy et al. (2015a)</ref> in setting the context window size to 2, negative sampling parameter to 1, eigen- value weighting to 0.5, and context distribution smoothing to 0.75; other parameters were assigned their default values. For the other models we used the following parameter values: for w2v, context window = 8, negative samples = 25, hs = 0, sample = 1e-4, and iterations = 15; and for GloVe, context window = 15, x max = 10, and iterations = 15.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Lexical Relations</head><p>In order to evaluate the applicability of the DIFF- VEC approach to relations of different types, we assembled a set of lexical relations in three broad categories: lexical semantic relations, morphosyn- tactic paradigm relations, and morphosemantic re- lations. We constrained the relations to be binary and to have fixed directionality. <ref type="bibr">4</ref> Consequently we excluded symmetric lexical relations such as syn- onymy. We additionally constrained the dataset to the words occurring in all embedding sets. There is some overlap between our relations and those in- cluded in the analogy task of <ref type="bibr" target="#b16">Mikolov et al. (2013c)</ref>, but we include a much wider range of lexical se- mantic relations, especially those standardly evalu- ated in the relation classification literature. We man- ually filtered the data to remove duplicates (e.g., as part of merging the two sources of LEXSEM Hyper intances), and normalise directionality.</p><p>The final dataset consists of 12,458 triples relation, word 1 , word 2 , comprising 15 relation types, extracted from SemEval'12 ( <ref type="bibr" target="#b3">Jurgens et al., 2012</ref>), BLESS (Baroni and Lenci, 2011), the MSR analogy dataset ( <ref type="bibr" target="#b16">Mikolov et al., 2013c</ref>), the light verb dataset of <ref type="bibr" target="#b28">Tan et al. (2006a)</ref>, Princeton Word- Net <ref type="bibr">(Fellbaum, 1998)</ref>, Wiktionary, 5 and a web lex- icon of collective nouns, <ref type="bibr">6</ref> as listed in <ref type="table" target="#tab_1">Table 2</ref>. <ref type="bibr">7</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Clustering</head><p>Assuming DIFFVECs are capable of capturing all lexical relations equally, we would expect cluster- ing to be able to identify sets of word pairs with high relational similarity, or equivalently clusters of similar offset vectors. Under the additional as- sumption that a given word pair corresponds to a unique lexical relation (in line with our defini- tion of the lexical relation learning task in §3), a hard clustering approach is appropriate. In order to <ref type="bibr">4</ref> Word similarity is not included; it is not easily captured by DIFFVEC since there is no homogeneous "content" to the lexical relation which could be captured by the direction and magnitude of a difference vector (other than that it should be small). test these assumptions, we cluster our 15-relation closed-world dataset in the first instance, and eval- uate against the lexical resources in §3.2.</p><p>As further motivation, we projected the DIFF- VEC space for a small number of samples of each class using t-SNE (Van der Maaten and Hinton, 2008), and found that many of the morphosyntactic relations (VERB 3 , VERB Past , VERB 3Past , NOUN SP ) form tight clusters <ref type="figure" target="#fig_2">(Figure 1)</ref>.</p><p>We cluster the DIFFVECs between all word pairs in our dataset using spectral clustering <ref type="bibr" target="#b33">(Von Luxburg, 2007)</ref>. Spectral clustering has two hyperparameters: the number of clusters, and the pairwise similarity measure for comparing DIFF- VECs. We tune the hyperparameters over devel- opment data, in the form of 15% of the data ob- tained by random sampling, selecting the configura- tion that maximises the V-Measure ( <ref type="bibr" target="#b25">Rosenberg and Hirschberg, 2007)</ref>. <ref type="figure">Figure 2</ref> presents V-Measure values over the test data for each of the four word embedding models. We show results for different numbers of clusters, from N = 10 in steps of 10, up to N = 80 (beyond which the clustering quality diminishes). <ref type="bibr">8</ref> Observe that w2v achieves the best results, with a V-Measure value of around 0.36, 9 which is relatively constant over varying numbers of clusters. GloVe and SVD mirror this result, but are consistently below <ref type="bibr">w2v</ref>    <ref type="bibr">w2v and GloVe, respectively)</ref> indicates that the volume of training data plays a role in the clustering results. However, both methods still perform well above SENNA and HLBL, and w2v has a clear empirical advantage over GloVe. We note that SVD wiki performs almost as well as w2v wiki , consistent with the results of <ref type="bibr" target="#b10">Levy et al. (2015a)</ref>.</p><p>We additionally calculated the entropy for each lexical relation, based on the distribution of in- stances belonging to a given relation across the different clusters (and simple MLE). For each em- bedding method, we present the entropy for the cluster size where V-measure was maximised over the development data. Since the samples are dis- tributed nonuniformly, we normalise entropy re- sults for each method by log(n) where n is the number of samples in a particular relation. The re- sults are in <ref type="table" target="#tab_3">Table 3</ref>, with the lowest entropy (purest clustering) for each relation indicated in bold.</p><p>Looking across the different lexical relation types, the morphosyntactic paradigm relations (NOUN SP and the three VERB relations) are by far the easiest to capture. The lexical semantic rela- tions, on the other hand, are the hardest to capture for all embeddings.</p><p>Considering w2v embeddings, for VERB 3 there was a single cluster consisting of around 90% of VERB 3 word pairs. Most errors resulted from POS ambiguity, leading to confusion with VERB- NOUN in particular. Example VERB 3 pairs incor- rectly clustered are: (study, studies), (run, runs), and (like, likes). This polysemy results in the dis- tance represented in the DIFFVEC for such pairs being above average for VERB <ref type="bibr">3</ref> , and consequently clustered with other cross-POS relations.</p><p>For VERB Past , a single relatively pure cluster was generated, with minor contamination due to pairs such as (hurt, saw), (utensil, saw), and (wipe, saw). Here, the noun saw is ambiguous with a high-frequency past-tense verb; hurt and wipe also have ambigous POS.</p><p>A related phenomenon was observed for NOUN Coll , where the instances were assigned to a large mixed cluster containing word pairs where the second word referred to an animal, reflect- ing the fact that most of the collective nouns in our dataset relate to animals, e.g. (stand, horse), (ambush, tigers), (antibiotics, bacteria). This is in- teresting from a DIFFVEC point of view, since it shows that the lexical semantics of one word in the pair can overwhelm the semantic content of the DIFFVEC (something that we return to investigate in §5.4). LEXSEM Mero was also split into multiple clusters along topical lines, with separate clusters for weapons, dwellings, vehicles, etc.</p><p>Given the encouraging results from our cluster- ing experiment, we next evaluate DIFFVECs in a supervised relation classification setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Classification</head><p>A natural question is whether we can accurately characterise lexical relations through supervised learning over the DIFFVECs. For these experi- ments we use the w2v, w2v wiki , and SVD wiki em- beddings exclusively (based on their superior per- formance in the clustering experiment), and a sub- set of the relations which is both representative of the breadth of the full relation set, and for which we have sufficient data for supervised training and evaluation, namely: NOUN Coll , LEXSEM Event , LEXSEM Hyper , LEXSEM Mero , NOUN SP , PREFIX, VERB 3 , VERB 3Past , and VERB Past (see <ref type="table" target="#tab_1">Table 2</ref>).</p><p>We consider two applications: (1) a CLOSED- WORLD setting similar to the unsupervised evalua- tion, in which the classifier only encounters word pairs which correspond to one of the nine relations; and (2) a more challenging OPEN-WORLD setting where random word pairs -which may or may not correspond to one of our relations -are included in the evaluation. For both settings, we further in- vestigate whether there is a lexical memorisation effect for a broad range of relation types of the sort identified by <ref type="bibr" target="#b34">Weeds et al. (2014)</ref> and <ref type="bibr" target="#b11">Levy et al. (2015b)</ref> for hypernyms, by experimenting with disjoint training and test vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">CLOSED-WORLD Classification</head><p>For the CLOSED-WORLD setting, we train and test a multiclass classifier on datasets comprising DIFFVEC, r pairs, where r is one of our nine relation types, and DIFFVEC is based on one of w2v, w2v wiki and SVD. As a baseline, we cluster the data as described in §4, running the clusterer several times over the 9-relation data to select the optimal V-Measure value based on the develop- ment data, resulting in 50 clusters. We label each cluster with the majority class based on the training instances, and evaluate the resultant labelling for the test instances.</p><p>We use an SVM with a linear kernel, and report results from 10-fold cross-validation in <ref type="table">Table 4</ref>.</p><p>The SVM achieves a higher F-score than the baseline on almost every relation, particularly on LEXSEM Hyper , and the lower-frequency NOUN SP , NOUN Coll , and PREFIX. Most of the relations - even the most difficult ones from our clustering experiment -are classified with very high F- score. That is, with a simple linear transforma- tion of the embedding dimensions, we are able to achieve near-perfect results. The PREFIX relation achieved markedly lower recall, resulting in a lower</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation</head><p>Baseline w2v <ref type="bibr">w2v</ref>   <ref type="table">Table 4</ref>: F-scores (F) for CLOSED-WORLD classi- fication, for a baseline method based on clustering + majority-class labelling, a multiclass linear SVM trained on w2v, w2v wiki and SVD wiki DIFFVEC inputs.</p><p>F-score, due to large differences in the predomi- nant usages associated with the respective words (e.g., (union, reunion), where the vector for union is heavily biased by contexts associated with trade unions, but reunion is heavily biased by contexts re- lating to social get-togethers; and (entry, reentry), where entry is associated with competitions and en- trance to schools, while reentry is associated with space travel). Somewhat surprisingly, given the small dimensionality of the input (vectors of size 300 for all three methods), we found that the lin- ear SVM slightly outperformed a non-linear SVM using an RBF kernel. We observe no real differ- ence between w2v wiki and SVD wiki , supporting the hypothesis of <ref type="bibr" target="#b10">Levy et al. (2015a)</ref> that under ap- propriate parameter settings, count-based methods achieve high results. The impact of the training data volume for pre-training of the embeddings is also less pronounced than in the case of our clustering experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">OPEN-WORLD Classification</head><p>We now turn to a more challenging evaluation set- ting: a test set including word pairs drawn at ran- dom. This setting aims to illustrate whether a DIFF- VEC-based classifier is capable of differentiating related word pairs from noise, and can be applied to open data to learn new related word pairs. <ref type="bibr">10</ref> For these experiments, we train a binary classi- fier for each relation type, using 2 3 of our relation data for training and 1 3 for testing. The test data is augmented with an equal quantity of random pairs, generated as follows:</p><p>(1) sample a seed lexicon by drawing words pro- portional to their frequency in Wikipedia; 11 <ref type="bibr">10</ref> Hereafter we provide results for w2v only, as we found that SVD achieved similar results.</p><p>11 Filtered to consist of words for which we have embed-  <ref type="table">Table 5</ref>: Precision (P) and recall (R) for OPEN- WORLD classification, using the binary classifier without ("Orig") and with ("+neg") negative sam- ples .</p><p>(2) take the Cartesian product over pairs of words from the seed lexicon; (3) sample word pairs uniformly from this set. This procedure generates word pairs that are repre- sentative of the frequency profile of our corpus.</p><p>We train 9 binary RBF-kernel SVM classifiers on the training partition, and evaluate on our ran- domly augmented test set. Fully annotating our random word pairs is prohibitively expensive, so instead, we manually annotated only the word pairs which were positively classified by one of our mod- els. The results of our experiments are presented in the left half of <ref type="table">Table 5</ref>, in which we report on results over the combination of the original test data from §5.1 and the random word pairs, noting that recall (R) for OPEN-WORLD takes the form of relative recall ( <ref type="bibr" target="#b20">Pantel et al., 2004</ref>) over the positively-classified word pairs. The re- sults are much lower than for the closed-word set- ting <ref type="table">(Table 4)</ref>, most notably in terms of precision (P). For instance, the random pairs (have, works), (turn, took), and (works, started) were incorrectly classified as VERB 3 , VERB Past and VERB 3Past , re- spectively. That is, the model captures syntax, but lacks the ability to capture lexical paradigms, and tends to overgenerate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">OPEN-WORLD Training with Negative Sampling</head><p>To address the problem of incorrectly classifying random word pairs as valid relations, we retrain the classifier on a dataset comprising both valid and automatically-generated negative distractor sam- ples. The basic intuition behind this approach is to construct samples which will force the model to learn decision boundaries that more tightly cap- ture the true scope of a given relation. To this end, we automatically generated two types of negative dings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>distractors:</head><p>opposite pairs: generated by switching the or- der of word pairs, Oppos w1 ,w2 = word 1 − word 2 . This ensures the classifier adequately captures the asymmetry in the relations. shuffled pairs: generated by replacing w 2 with a random word w 2 from the same relation, Shuff w1 , <ref type="bibr">w2</ref> = word 2 − word 1 . This is tar- geted at relations that take specific word classes in particular positions, e.g., (VB, VBD) word pairs, so that the model learns to encode the relation rather than simply learning the properties of the word classes. Both types of distractors are added to the train- ing set, such that there are equal numbers of valid relations, opposite pairs and shuffled pairs.</p><p>After training our classifier, we evaluate its pre- dictions in the same way as in §5.2, using the same test set combining related and random word pairs. <ref type="bibr">12</ref> The results are shown in the right half of <ref type="table">Table 5</ref> (as "+neg"). Observe that the precision is much higher and recall somewhat lower compared to the classi- fier trained with only positive samples. This follows from the adversarial training scenario: using nega- tive distractors results in a more conservative classi- fier, that correctly classifies the vast majority of the random word pairs as not corresponding to a given relation, resulting in higher precision at the expense of a small drop in recall. Overall this leads to higher F-scores, as shown in <ref type="figure">Figure 3</ref>, other than for hy- pernyms (LEXSEM Hyper ) and prefixes (PREFIX). For example, the standard classifier for NOUN Coll learned to match word pairs including an animal name (e.g., (plague, rats)), while training with neg- ative samples resulted in much more conservative predictions and consequently much lower recall. The classifier was able to capture (herd, horses) but not (run, salmon), (party, jays) or (singular, boar) as instances of NOUN Coll , possibly because of poly- semy. The most striking difference in performance was for LEXSEM Mero , where the standard classi- fier generated many false positive noun pairs (e.g. (series, radio)), but the false positive rate was con- siderably reduced with negative sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Lexical Memorisation</head><p>Weeds et al. (2014) and <ref type="bibr" target="#b11">Levy et al. (2015b)</ref> re- cently showed that supervised methods using DIFF- VECs achieve artificially high results as a result of "lexical memorisation" over frequent words asso-  ciated with the hypernym relation. For example, (animal, cat), (animal, dog), and (animal, pig) all share the superclass animal, and the model thus learns to classify as positive any word pair with animal as the first word.</p><p>To address this effect, we follow <ref type="bibr" target="#b11">Levy et al. (2015b)</ref> in splitting our vocabulary into training and test partitions, to ensure there is no overlap between training and test vocabulary. We then train classi- fiers with and without negative sampling ( §5.3), incrementally adding the random word pairs from §5.2 to the test data (from no random word pairs to five times the original size of the test data) to in- vestigate the interaction of negative sampling with greater diversity in the test set when there is a split vocabulary. The results are shown in <ref type="figure" target="#fig_3">Figure 4</ref>.</p><p>Observe that the precision for the standard clas- sifier decreases rapidly as more random word pairs are added to the test data. In comparison, the pre- cision when negative sampling is used shows only a small drop-off, indicating that negative sampling is effective at maintaining precision in an OPEN- WORLD setting even when the training and test vocabulary are disjoint. This benefit comes at the expense of recall, which is much lower when neg- ative sampling is used (note that recall stays rela- tively constant as random word pairs are added, as the vast majority of them do not correspond to any relation). At the maximum level of random word pairs in the test data, the F-score for the negative sampling classifier is higher than for the standard classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>This paper is the first to test the generalisability of the vector difference approach across a broad range of lexical relations (in raw number and also variety). Using clustering we showed that many types of morphosyntactic and morphosemantic dif- ferences are captured by DIFFVECs, but that lexical semantic relations are captured less well, a find- ing which is consistent with previous work <ref type="bibr">(Köper et al., 2015)</ref>. In contrast, classification over the DIFFVECs works extremely well in a closed-world setting, showing that dimensions of DIFFVECs en- code lexical relations. Classification performs less well over open data, although with the introduction of automatically-generated negative samples, the results improve substantially. Negative sampling also improves classification when the training and test vocabulary are split to minimise lexical mem- orisation. Overall, we conclude that the DIFFVEC approach has impressive utility over a broad range of lexical relations, especially under supervised classification.</p><p>Michele Banko, Michael J. <ref type="bibr">Cafarella, Stephen Soderland, Matthew Broadhead, and Oren Etzioni. 2007</ref>. Open information extraction for the web. In <ref type="table" target="#tab_1">Pro- ceedings of the 20th International Joint Conference  on Artificial Intelligence (IJCAI-2007</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(Mikolov et al., 2013a; Mikolov et al., 2013b), GloVe (Pen- nington et al., 2014), SENNA (Collobert and We- ston, 2008), and HLBL (Mnih and Hinton, 2009), as detailed below. We also include SVD (Levy et al., 2015a), a count-based model which factorises a positive PMI (PPMI) matrix. For consistency of comparison, we train SVD as well as a version of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>)</head><label></label><figDesc>is the joint probability of word w and context c, andˆPandˆ andˆP (w) andˆPandˆ andˆP (c) are their marginal probabilities. The matrix is factorised by singular value decomposition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>5Figure 1 :</head><label>1</label><figDesc>Figure 1: t-SNE projection (Van der Maaten and Hinton, 2008) of DIFFVECs for 10 sample word pairs of each relation type, based on w2v. The intersection of the two axes identify the projection of the zero vector. Best viewed in colour.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Evaluation of the OPEN-WORLD model when trained on split vocabulary, for varying numbers of random word pairs in the test dataset (expressed as a multiplier relative to the number of CLOSED-WORLD test instances).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Description of the 15 lexical relations.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>The entropy for each lexical relation over 
the clustering output for each set of pre-trained 
word embeddings. 

similarly, at a substantially lower V-Measure than 
w2v or GloVe, closer to 0.21. As a crude calibra-
tion for these results, over the related clustering 
task of word sense induction, the best-performing 
systems in SemEval-2010 Task 4 (Manandhar et 
al., 2010) achieved a V-Measure of under 0.2. 
The lower V-measure for w2v wiki and 
GloVe wiki (as compared to </table></figure>

			<note place="foot" n="1"> We ran a series of experiments on normalised and unnormalised w2v models, and found that normalisation tends to boost results over most of our relations (with the exception of LEXSEMEvent and NOUNColl). We leave a more detailed investigation of normalisation to future work. 2 In a slight abuse of notation, the subscripts of w do double</note>

			<note place="foot" n="3"> Although the w2v model trained without preprocessing performed marginally better, we used preprocessing throughout for consistency.</note>

			<note place="foot" n="8"> Although 80 clusters our 15 relation types, the SemEval&apos;12 classes each contain numerous subclasses, so the larger number may be more realistic. 9 V-Measure returns a value in the range [0, 1], with 1 indicating perfect homogeneity and completeness.</note>

			<note place="foot" n="12"> But noting that relative recall for the random word pairs is based on the pool of positive predictions from both models.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>LR was supported by EPSRC grant EP/I037512/1 and ERC Starting Grant DisCoTex (306920). TC and TB were supported by the Australian Research Council.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Random walks on context spaces: Towards an explanation of the mysteries of semantic word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Risteski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03520</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>cs.LG</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SemEval-2007 Task 4: Classification of semantic relations between nominals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roxana</forename><surname>Girju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivi</forename><surname>Nastase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Szpakowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Yuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Workshop on Semantic Evaluation</title>
		<meeting>the 4th International Workshop on Semantic Evaluation<address><addrLine>SemEval; Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">SemEval-2010 Task 8: Multi-way classification of semantic relations between pairs of nominals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iris</forename><surname>Hendrickx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><forename type="middle">Nam</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diarmuid´odiarmuid´</forename><forename type="middle">Diarmuid´o</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenza</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Szpakowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Semantic Evaluation (SemEval 2010)</title>
		<meeting>the 5th International Workshop on Semantic Evaluation (SemEval 2010)<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="33" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SemEval-2012 Task 2: Measuring degrees of relational similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saif</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Holyoak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Workshop on Semantic Evaluation</title>
		<meeting>the 6th International Workshop on Semantic Evaluation<address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="356" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deriving adjectival scales from continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joo-Kyung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP 2013)</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP 2013)<address><addrLine>Seattle, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1625" to="1630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multilingual reliability and &quot;semantic&quot; structure of continuous word spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Köper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Scheible</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Schulte Im Walde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Workshop on Computational Semantics (IWCS-11)</title>
		<meeting>the Eleventh International Workshop on Computational Semantics (IWCS-11)<address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="40" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Directional distributional similarity for lexical inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Kotlerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Szpektor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maayan</forename><surname>Zhitomirsky-Geffet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="359" to="389" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Identifying hypernyms in distributional semantic spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulia</forename><surname>Benotto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Joint Conference on Lexical and Computational Semantics (*SEM 2012)</title>
		<meeting>the First Joint Conference on Lexical and Computational Semantics (*SEM 2012)<address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="75" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Linguistic regularities in sparse and explicit word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Conference on Natural Language Learning (CoNLL-2014)</title>
		<meeting>the 18th Conference on Natural Language Learning (CoNLL-2014)<address><addrLine>Baltimore, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="171" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural word embeddings as implicit matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improving distributional similarity with lessons learned from word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="211" to="225" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Do supervised distributional methods really learn lexical inference relations?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Remus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Israel</forename><surname>Ramat-Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics-Human Language Technologies (NAACL HLT 2015)</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics-Human Language Technologies (NAACL HLT 2015)<address><addrLine>Denver, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="970" to="976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Applicative structure in vector space models</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality (CVSC)</title>
		<editor>Márton Makrai, Dávid Nemeskey, and András Kornai</editor>
		<meeting>the Workshop on Continuous Vector Space Models and their Compositionality (CVSC)<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="59" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">SemEval-2010 Task 14: Word sense induction &amp; disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suresh</forename><surname>Manandhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Klapaftis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Dligach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Semantic Evaluation</title>
		<meeting>the 5th International Workshop on Semantic Evaluation<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="63" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop of the First International Conference on Learning Representations (ICLR 2013)</title>
		<meeting>the Workshop of the First International Conference on Learning Representations (ICLR 2013)<address><addrLine>Scottsdale, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 25 (NIPS-13)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT 2013)</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT 2013)<address><addrLine>Atlanta, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A scalable hierarchical distributed language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 21 (NIPS-09)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1081" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning word embeddings efficiently with noise-contrastive estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 25 (NIPS-13)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Reading between the lines: Overcoming data sparsity for accurate classification of lexical relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Necs¸ulescunecs¸ulescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Núria</forename><surname>Bel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015)</title>
		<meeting>the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015)<address><addrLine>Denver, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="182" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Towards terascale semantic acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Computational Linguistics (COLING 2004)</title>
		<meeting>the 20th International Conference on Computational Linguistics (COLING 2004)<address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="771" to="777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distributional lexical entailment by topic coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Rimell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2014)</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2014)<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="511" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Relations such as hypernymy: Identifying and exploiting Hearst patterns in distributional vectors for lexical entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.05433</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Inclusive yet selective: Supervised distributional hypernymy detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Boleda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Computational Linguistics (COLING 2014)</title>
		<meeting>the 25th International Conference on Computational Linguistics (COLING 2014)<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1025" to="1036" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">VMeasure: A conditional entropy-based external cluster evaluation measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hirschberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="410" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Chasing hypernyms in vector spaces with entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Santus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Schulte Im Walde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2014)</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2014)<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="38" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 25 (NIPS-13)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Introduction to Data Mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pang-Ning</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Steinbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vipin</forename><surname>Kumar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Addison Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Extending corpus-based identification of light verb constructions using a supervised learning framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Yee Fan Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EACL 2006 Workshop on Multiword-expressions in a Multilingual Context</title>
		<meeting>the EACL 2006 Workshop on Multiword-expressions in a Multilingual Context<address><addrLine>Trento, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Word representations: A simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev-Arie</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the ACL (ACL 2010)</title>
		<meeting>the 48th Annual Meeting of the ACL (ACL 2010)<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Similarity of semantic relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="379" to="416" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">85</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrike</forename><forename type="middle">Von</forename><surname>Luxburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="395" to="416" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning to distinguish hypernyms and co-hyponyms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Weeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daoud</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Reffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Computational Linguistics (COLING 2014)</title>
		<meeting>the 25th International Conference on Computational Linguistics (COLING 2014)<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2249" to="2259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">From information to knowledge: harvesting entities and relationships from web sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Theobald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty Ninth ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems</title>
		<meeting>the Twenty Ninth ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems<address><addrLine>Indianapolis, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="65" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">RCNET: A general framework for incorporating knowledge into word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanlong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM Conference on Information and Knowledge Management (CIKM 2014)</title>
		<meeting>the 23rd ACM Conference on Information and Knowledge Management (CIKM 2014)<address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1219" to="1228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hypernym discovery based on distributional similarity and hierarchical structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ichiro</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Torisawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kow</forename><surname>Jun&amp;apos;ichi Kazama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaki</forename><surname>Kuroda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Murata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stijn De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Saeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asuka</forename><surname>Bond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sumida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="929" to="937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Improving lexical embeddings with semantic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014)</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014)<address><addrLine>Baltimore, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="545" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Combining heterogeneous models for measuring relational similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
