<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Lexical Comparison Between Wikipedia and Twitter Corpora by Using Word Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luchen</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">David R. Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">David R. Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">David R. Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
							<email>mark.smucker@uwaterloo.ca</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Management Sciences</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Lexical Comparison Between Wikipedia and Twitter Corpora by Using Word Embeddings</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="657" to="661"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Compared with carefully edited prose, the language of social media is informal in the extreme. The application of NLP techniques in this context may require a better understanding of word usage within social media. In this paper, we compute a word embedding for a corpus of tweets, comparing it to a word embedding for Wikipedia. After learning a transformation of one vector space to the other, and adjusting similarity values according to term frequency, we identify words whose usage differs greatly between the two corpora. For any given word, the set of words closest to it in a particular embedding provides a characterization for that word&apos;s usage within the corresponding corpora.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Users of social media typically employ highly informal language, including slang, acronyms, typos, deliberate misspellings, and interjec- tions ( <ref type="bibr" target="#b3">Han and Baldwin, 2011</ref>). This heavy use of nonstandard language, as well as the overall level of noise on social media, creates substantial problems when applying standard NLP tools and techniques <ref type="bibr" target="#b1">(Eisenstein, 2013)</ref>. For example, <ref type="bibr" target="#b5">Kaufmann and Kalita (2010)</ref> apply machine translation methods to convert tweets to standard English in an attempt to ameliorate this problem. Similarly, <ref type="bibr" target="#b0">Baldwin et al. (2013)</ref> and <ref type="bibr" target="#b4">Han et al. (2012)</ref> address this problem by generating corrections for irregu- larly spelled words in social media.</p><p>In this short paper, we continue this line of re- search, applying word embedding to the problem of translating between the informal English of so- cial media, specifically Twitter, and the formal En- glish of carefully edited texts, such as those found in Wikipedia. Starting with a large collection of tweets and a copy of Wikipedia, we construct word embeddings for both corpora. We then gener- ate a transformation matrix, mapping one vector space into another. After applying a normalization based on term frequency, we use distances in the transformed space as an indicator of differences in word usage between the two corpora. The method identifies differences in usage due to jargon, con- tractions, abbreviations, hashtags, and the influ- ence of popular culture, as well as other factors. As a method of validation, we examine the over- lap in closely related words, showing that distance after transformation and normalization correlates with the degree of overlap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Mikolov et al. (2013b) proposed a novel neural network model to train continuous vector repre- sentation for words. The high-quality word vec- tors obtained from large data sets achieve high accuracy in both semantic and syntactic relation- ships ( <ref type="bibr" target="#b2">Goldberg and Levy, 2014)</ref>.</p><p>Some probabilistic similarity measures, based on Kullback-Leibler (KL) divergence (or relative entropy), give an inspection of relative divergence between two probability distributions of corpus ( <ref type="bibr" target="#b6">Kullback and Leibler, 1951;</ref><ref type="bibr" target="#b12">Tan and Clarke, 2014</ref>). For a given token, KL divergence measures the distribution divergence of this word in different corpora according to its corresponding probability. Intuitively, the value for KL divergence increases as two distributions become more different. <ref type="bibr" target="#b14">Verspoor et al. (2009)</ref> found that KL divergence could be applied to analyze text in terms of two charac- teristics: the magnitude of the differences, and the semantic nature of the characteristic words.</p><p>Subaši´Subaši´c and Berendt (2011) applied a sym- metrical variant of KL divergence, the Jensen- Shannon (JS) divergence <ref type="bibr" target="#b7">(Lin, 1991)</ref>, to compare various aspects of the corpora such as language divergence, headline divergence, named-entity di- vergence and sentiment divergence. As for the ap- plications derived from above methods, <ref type="bibr" target="#b13">Tang et al. (2011)</ref> studied the lexical semantics and sentiment tendency of high frequency terms in each corpus by comparing microblog texts with general arti- cles. <ref type="bibr" target="#b0">Baldwin et al. (2013)</ref> analyzed non-standard language on social media in the aspects of lexi- cal variants, acronyms, grammaticality and corpus similarity. Their results revealed that social media text is less grammatical than edited text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods of Lexical Comparison</head><p>Mikolov et al. (2013a) construct vector spaces for various languages, including English and Spanish, finding that the relative positions of semantically related words are preserved across languages. We adapt this result to explore differences between corpora written in a single language, specifically to explore the contrast between the highly in- formal language used in English-language social media with the more formal language used in Wikipedia. We assume that there exists a lin- ear transformation relationship between the vec- tors for the most frequent words from each cor- pus. Working with these frequent terms, we learn a linear projection matrix that maps source to tar- get spaces. We hypothesize that usage of those words appearing far apart after this transformation differs substantially between the two corpora.</p><p>Let a ∈ R 1×d and b ∈ R 1×d be the corre- sponding source and target word vector represen- tation with dimension d. We construct a source</p><formula xml:id="formula_0">matrix A = [a T 1 , a T 2 , ..., a T c ] T and a target matrix B = [b T 1 , b T 2 , ..., b T c ] T , composed of vector pairs {a i , b i } c i=1</formula><p>, where c is the size of the vocabulary common between the source and target corpora. We order these vectors according to frequency in the target corpus, so that a i and b i correspond to the i-th most common word in the target corpus.</p><p>These vectors are used to learn a linear transfor- mation matrix M ∈ R d×d . Once this transforma- tion matrix M is obtained, we can transform any a i to a i = a i M in order to approximate b i . The linear transformation can be depicted as:</p><formula xml:id="formula_1">AM = B<label>(1)</label></formula><p>Following the solution provided by <ref type="bibr" target="#b8">(Mikolov et al., 2013a</ref>), M can be approximately computed by using stochastic gradient descent:</p><formula xml:id="formula_2">min M n i=1 a i M − b i 2 (2)</formula><p>where we limit the training process to the top n terms.</p><p>After the generation of M , we calculate a i = a i M for each word. For each a i where i &gt; n, we determine the distance between a i and b i :</p><formula xml:id="formula_3">Sim(a i , b i ), n ≤ i ≤ c.<label>(3)</label></formula><p>Let Z be the set of these words ordered by dis- tance, so that z j is the word with the j-th greatest distance between the corresponding a and b vec- tors. For the experiments reported in this paper, we used cosine distance to calculate this Sim metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we describe the results of applying our method to Twitter and Wikipedia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>The Wikipedia dataset for our experiments con- sists of all English Wikipedia articles downloaded from MediaWiki data dumps 1 . The Twitter dataset was collected through the Twitter Streaming API from November 2013 to March 2015. We re- stricted the dataset to English-language tweets on the basis of the language field contained in each tweet. To obtain distributed word representation for both corpora, we trained word vectors sep- arately by applying the word2vec 2 tool, a well- known implementation of word embedding. Before applying the tool, we cleaned Wikipedia and Twitter corpora.</p><p>The clean version of Wikipedia retains only normally visible article text on Wikipedia web pages. The Twitter clean version removes HTML code, URLs, user men- tions(@), the # symbol of hashtags, and all the retweeted tweets. The sizes of document and vo- cabulary in both corpora are listed in <ref type="table">Table 1</ref>. There are two major parameters that affect word2vec training quality: the dimensionality of word vectors, and the size of the surrounding words window. We choose 300 for our word vec- tor dimensionality, which is typical for training large dataset with word2vec. We choose 10 words for the window, since tweet sentence length is 9.2 ± 6.4 <ref type="figure" target="#fig_1">(Baldwin et al., 2013</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corpora # Documents # Vocabulary</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Visualization</head><p>In <ref type="figure">Figure 1</ref>, we visualize the vectors of some most common English words by applying prin- cipal component analysis (PCA) to the vector spaces. The words "and", "is", "was" and "by" have similar geometric arrangements in Wikipedia and in Twitter, since these common words are not key differentiators for these corpora. On the other hand, the pronouns "I" and "you", are heavily used in Twitter but rarely used in Wikipedia. Despite this difference in term frequency, after transfor- mation, the vectors for these terms appear close together.</p><p>Figure 1: Word representations in Wikipedia, Twitter and transformed vectors after mapping from Wikipedia to Twitter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>As our primary goal, we hope to demonstrate that our transformation method reflects meaning- ful lexical usage differences between Wikipedia and Twitter. To train our space transformation ma- trix, we used the top n = 1, 000 most frequent words from the 505,121 words that appear in both corpora. The transformation can be either from Twitter to Wikipedia (T2W) or the opposite direc- tion W2T. We observed that the two transforma- tion matrices are not exactly the same, but they produce similar results. <ref type="bibr" target="#b10">Mikolov et al. (2013c)</ref> suggest that a simple vector offset method based on cosine distance was remarkably effective to search both syntactic and semantic similar words. They also report that cosine similarity preformed well, given that the embedding vectors are all nor- malized to unit norm. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates how T2W word vectors are similar to their original word vectors. For the purpose of explaining <ref type="figure" target="#fig_1">Figure 2</ref>, we define new notation as follows: Let T and W be the word sets of Twitter and Wikipedia respectively, and let C = T ∩ W. Denote the document frequency of a word t in the Twitter corpus as df (t). Sorting the whole set C by df (t) in an ascending order, we obtain a sequence ¯ S = {c 0 , · · · , c m−1 }, where c i ∈ C; m = 505, 121; and df (c i ) ≤ df (c j ), ∀i &lt; j. We partition the sequence ¯ S into 506 buckets, with a bucket size b = 1000. B i = {c i * b , · · · , c (i+1) * b−1 } represents the i-th bucket. We number the curves in <ref type="figure" target="#fig_1">Figure 2</ref> from the top to the bottom. The points on the i-th curve demon- strates the cosine similarity of the (i − 1) * 100-th word in each bucket. From this figure, it is appar- ent that words with higher frequencies have higher average cosine similarity than those words with lower frequencies. Since our goal is to find words with lower than average similar, we apply the me- dian curve of <ref type="figure" target="#fig_1">Figure 2</ref> to adjust word distances.   Defining adjusted distance as D adjusted (t) of a given word t, we calculate the cosine distance between t and the median point c median from its corresponding bucket B i .</p><formula xml:id="formula_4">D adjusted (t) = Sim(c median ) − Sim(t) (4)</formula><p>where the index of median point should be i * b + b/2. A negative adjusted distance value means the word is more similar than at least half of <ref type="table">bc  because bcus bcuz cuz cos  bce macedon hellenistic euthydemus ptolemaic  ill  ll imma ima will youll  unwell sick frail fated bedridden  cameron  cam nash followmecam camerons callmecam  gillies duncan mckay mitchell bryce  mentions unfollow reply respond strangerswelcomed offend  mentions mentioned mentioning reference attested  miss  misss love missss missssss imiss  pageant pageants titlehoder titlehoders pageantopolis  yup  yep yupp yeah yea yepp  chevak yupik gwaii tlingit nunivak  taurus  capricorn sagittarius pisces gemini scorpio  poniatovii scorpio subcompact sagittarius chevette   Table 2</ref>: Characteristic Words in Twitter Corpora words in its bucket. On the other hand, the words that are less similar than at least half of words in their buckets have positive adjusted distance val- ues. The larger an adjusted distance, the less sim- ilar the word is between the corpora. <ref type="table">Table 2</ref> provides some examples of common words with large adjusted distance, suggesting that their usage in the two corpora are quite differ- ent. For each of these words, the example shows the closest terms to that word in the two corpora. In Twitter, "bc" is frequently an abbreviation for "because", while in Wikipedia "bc" is more com- monly used as part of dates, e.g. 900 BC. Simi- larly, in Twitter "ill" is often a misspelling of the contraction "I'll", rather than a synonym for sick- ness, as in Wikipedia. In Twitter, the most similar words to "cameron" relate to a YouTube person- ality, whereas in Wikipedia they relate to notable Scotish persons. In Wikipedia, "miss" is related to beauty pageants, while in Twitter it is related to expressions of affection ("I misssss you"). The other examples also have explanations related to popular culture, jargon, slang, and other factors.</p><note type="other">Word Twitter Most Similar Wikipedia Most Similar</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Examples</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Validation</head><p>To validate our method of comparing lexical dis- tinctions in the two corpora, we employ a ranking similarity measurement. Within a single corpus, the most similar words to a word t can be gen- erated by ranking cosine distance to t. We then determine the overlap between the most similar words to t from Twitter and Wikipedia. The more the two lists overlap, the greater the similarity be- tween the words in the two corpora. Our hypoth- esis is that larger rank similarity correlates with smaller adjusted distance. Rank biased overlap (RBO) provides a rank similarity measure designed for comparisons be- tween top-weighted, incomplete and indefinite rankings. Given two ranked lists, A and B, let A 1:k and B 1:k denote the top k items in A and B <ref type="bibr" target="#b15">(Webber et al., 2010)</ref>. RBO defines the overlap between A and B at depth k as the size of the inter- section between these lists at depth k and defines the agreement between A and B at depth k as the overlap divided by the depth. <ref type="bibr" target="#b15">Webber et al. (2010)</ref> define RBO as a weighted average of agreement across depths, where the weights decay geometri- cally with depth, reflecting the requirement for top weighting:</p><formula xml:id="formula_5">RBO = (1 − ϕ) ∞ k=1 ϕ k−1 |A 1:k ∩ B 1:k | k (5)</formula><p>Here, ϕ is a persistence parameter. As suggested by Webber et al., we set ϕ = 0.9. In practice, RBO is computed down to some fixed depth K. We se- lect K = 50 for our experiments. For a word t, we compute RBO value between its top 50 simi- lar words in Wikipedia and top 50 similar words in Twitter.</p><p>In <ref type="figure" target="#fig_2">Figure 3</ref>, we validate consistency between results of our space transformation method and RBO. For the top 5,000 terms in the Twitter cor- pus, we sort them by their adjusted distance value. Due to properties of RBO, there are many zero RBO values. To illustrate the density of these zero overlaps, we smooth our plot by sliding a 100- word window with a step of 10 words. As shown sharply in the figure, RBO and adjusted distance is negatively correlated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper analyzed the lexical usage difference between Twitter microblog corpus and Wikipedia corpus. A word-level comparison method based on word embedding is employed to find the char- acterisic words that particularly discriminating corpora. In future work, we plan to introduce this method to normalize the nonstandard language used in Twitter, applying the methods to problems in search and other areas. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: T2W transformated similarity curves.</figDesc><graphic url="image-1.png" coords="3,72.29,370.95,217.69,141.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: T2W and W2T negative correlation between adjusted distance and RBO.</figDesc></figure>

			<note place="foot" n="1"> https://dumps.wikimedia.org/enwiki/20150304/ 2 https://code.google.com/p/word2vec/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">How noisy social media text, how diffrnt social media sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mackinlay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Joint Conference on Natural Language Processing</title>
		<meeting>the Sixth International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="356" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">What to do about bad language on the internet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="359" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">word2vec explained: deriving mikolov et al.&apos;s negativesampling word-embedding method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1402.3722</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Lexical normalisation of short text messages: Makn sens a# twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="368" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatically constructing a normalisation dictionary for microblogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning</title>
		<meeting>the 2012 joint conference on empirical methods in natural language processing and computational natural language learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="421" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Syntactic normalization of twitter messages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jugal</forename><surname>Kalita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on natural language processing</title>
		<meeting><address><addrLine>Kharagpur, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">On information and sufficiency. The annals of mathematical statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Solomon</forename><surname>Kullback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leibler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1951" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Divergence measures based on the shannon entropy. Information Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="145" to="151" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Exploiting similarities among languages for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.4168</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Peddling or creating? investigating the role of twitter in news reporting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Subaši´subaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bettina</forename><surname>Berendt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="207" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Succinct queries for linking and tracking news in social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luchen</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, CIKM &apos;14</title>
		<meeting>the 23rd ACM International Conference on Conference on Information and Knowledge Management, CIKM &apos;14<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1883" to="1886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A comparison between microblog corpus and balanced corpus from linguistic and sentimental perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Ye</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Hsi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Analyzing Microtext</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The textual characteristics of traditional and open access scientific journals are similar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karin</forename><surname>Verspoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hunter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">183</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A similarity measure for indefinite rankings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Webber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alistair</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
