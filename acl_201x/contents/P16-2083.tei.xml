<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:11+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Word Embedding Calculus in Meaningful Ultradense Subspaces</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>August 7-12, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sascha</forename><surname>Rothe</surname></persName>
							<email>sascha@cis.lmu.de</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing LMU Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing LMU Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sch¨</forename><surname>Schütze</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing LMU Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Word Embedding Calculus in Meaningful Ultradense Subspaces</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="512" to="517"/>
							<date type="published">August 7-12, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We decompose a standard embedding space into interpretable orthogonal sub-spaces and a &quot;remainder&quot; subspace. We consider four interpretable subspaces in this paper: polarity, concreteness, frequency and part-of-speech (POS) sub-spaces. We introduce a new calculus for subspaces that supports operations like &quot;−1 × hate = love&quot; and &quot;give me a neutral word for greasy&quot; (i.e., oleaginous). This calculus extends analogy computations like &quot;king−man+woman = queen&quot;. For the tasks of Antonym Classification and POS Tagging our method outperforms the state of the art. We create test sets for Morphological Analogies and for the new task of Polarity Spectrum Creation.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Word embeddings are usually trained on an ob- jective that ensures that words occurring in simi- lar contexts have similar embeddings. This makes them useful for many tasks, but has drawbacks for others; e.g., antonyms are often interchangeable in context and thus have similar word embeddings even though they denote opposites. If we think of word embeddings as members of a (commuta- tive or Abelian) group, then antonyms should be inverses of (as opposed to similar to) each other. In this paper, we use DENSIFIER ( <ref type="bibr" target="#b9">Rothe et al., 2016)</ref> to decompose a standard embedding space into interpretable orthogonal subspaces, including a one-dimensional polarity subspace as well as concreteness, frequency and POS subspaces. We introduce a new calculus for subspaces in which antonyms are inverses, e.g., "−1 × hate = love".</p><p>The formula shows what happens in the polarity subspace; the orthogonal complement (all the re- maining subspaces) is kept fixed. We show be- low that we can predict an entire polarity spec- trum based on the subspace, e.g., the four-word spectrum hate, dislike, like, love. Similar to polar- ity, we explore other interpretable subspaces and do operations such as: given a concrete word like friend find the abstract word friendship (concrete- ness); given the frequent word friend find a less frequent synonym like comrade (frequency); and given the noun friend find the verb befriend (POS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Word Embedding Transformation</head><p>We now give an overview of DENSIFIER; see <ref type="bibr" target="#b9">Rothe et al. (2016)</ref> for details. Let Q ∈ R d×d be an orthogonal matrix that transforms the orig- inal word embedding space into a space in which certain types of information are represented by a small number of dimensions. The orthogonality can be seen as a hard regularization of the trans- formation. We choose this because we do not want to add or remove any information from the origi- nal embeddings space. This ensures that the trans- formed word embeddings behave differently only when looking at subspaces, but behave identically when looking at the entire space. By choosing an orthogonal and thus linear transformation we also assume that the information is already encoded linearly in the original word embedding. This is a frequent assumption, as we generally use the vec- tor addition for word embeddings.</p><p>Concretely, we learn Q such that the dimen- sions D p ⊂ {1, . . . , d} of the resulting space cor- respond to a word's polarity information and the {1, . . . , d} \ D p remaining dimensions correspond to non-polarity information. Analogously, the sets of dimensions D c , D f and D m correspond to a word's concreteness, frequency and POS (or mor- phological) information, respectively. In this pa- per, we assume that these properties do not corre- <ref type="figure">Figure 1</ref>: Illustration of the transformed embeddings. The horizontal axis is the polarity subspace. All non-polarity information, including concreteness, frequency and POS, is projected into a two di- mensional subspace for visualization (gray plane). A query word (bold) specifies a line parallel to the horizontal axis. We then construct a cylinder around this line. Words in this cylinder are considered to be part of the word spectrum.</p><p>late and therefore the ultradense subspaces do not overlap. E.g., D p ∩D c = ∅. This might not be true for other settings, e.g., sentiment and semantic in- formation. As we are using only four properties there is also a subspace which is in the orthogonal complement of all trained subspaces. This sub- space includes the not classified information, e.g., genre information in our case (e.g., "clunker" is a colloquial word for "automobile").</p><p>If e v ∈ R d is the original embedding of word v, the transformed representation is u v = Qe v . We use * as a placeholder for polarity (p), concrete- ness (c), frequency (f ) and POS/morphology (m) and call d * = |D * | the dimensionality of the ultra- dense subspace of property * . For each ultradense subspace, we create P * ∈ R d * ×d , an identity ma- trix for the dimensions in D * . Thus, the ultradense</p><formula xml:id="formula_0">(UD) representation u * v ∈ R d * of word v is defined as: u * v := P * Qe v<label>(1)</label></formula><p>For notational simplicity, u * v will either refer to a vector in R d * or to a vector in R d where all dimen- sions / ∈ D * are set to zero. For training, the orthogonal transformation Q we assume we have a lexicon resource. Let L * ∼ be a set of word index pairs (v, w) with different labels, e.g., positive/negative, concrete/abstract or noun/verb. We want to maximize the distance for pairs in this group. Thus, our objective is:</p><formula xml:id="formula_1">argmin Q * ∈{p,c,f,m} (v,w)∈L * ∼ −−P * Q(e v − e w )</formula><p>(2) subject to Q being an orthogonal matrix. Another goal is to minimize the distance of two words with identical labels. Let L * ∼ be a set of word index pairs (v, w) with identical labels. In contrast to Eq. 2, we now want to minimize each distance. Thus, the objective is given by:</p><formula xml:id="formula_2">argmin Q * ∈{p,c,f,m} (v,w)∈L * ∼ P * Q(e v −e w ) (3)</formula><p>subject to Q being an orthogonal matrix. For train- ing Eq. 2 is weighted with α * and Eq. 3 with 1 − α * . We do a batch gradient descent where each batch contains the same number of positive and negative examples. This means the number of examples in the lexica -which give rise to more negative than positive examples -does not influ- ence the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Setup and Method</head><p>Eqs. 2/3 can be combined to train an orthogonal transformation matrix. We use pretrained 300- dimensional English word embeddings ( <ref type="bibr" target="#b7">Mikolov et al., 2013</ref>) (W2V). To train the transformation matrix, we use a combination of MPQA <ref type="bibr" target="#b14">(Wilson et al., 2005</ref>), Opinion Lexicon ( <ref type="bibr" target="#b6">Hu and Liu, 2004</ref>) and NRC Emotion lexicons (Mohammad and Turney, 2013) for polarity; BWK, a lexicon of 40,000 English words ( <ref type="bibr" target="#b2">Brysbaert et al., 2014)</ref>, for concreteness; the order in the word embed- ding file for frequency; and the training set of the FLORS tagger ( <ref type="bibr" target="#b10">Schnabel and Schütze, 2014</ref>) for POS. The application of the transformation ma-trix to the word embeddings gives us four sub- spaces for polarity, concreteness, frequency and POS. These subspaces and their orthogonal com- plements are the basis for an embedding calculus that supports certain operations. Here, we investi- gate four such operations. The first operation com- putes the antonym of word v:</p><formula xml:id="formula_3">antonym(v) = nn(u v − 2u p v )<label>(4)</label></formula><p>where nn : R d → V returns the word whose em- bedding is the nearest neighbor to the input. Thus, our hypothesis is that antonyms are usually very similar in semantics except that they differ on a single "semantic axis," the polarity axis. <ref type="bibr">1</ref> The sec- ond operation is "neutral version of word v":</p><formula xml:id="formula_4">neutral(v) = nn(u v − u p v )<label>(5)</label></formula><p>Thus, our hypothesis is that neutral words are words with a value close to zero in the polarity subspace. The third operation produces the polar- ity spectrum of v:</p><formula xml:id="formula_5">spectrum(v) = {nn(u v + xu p v ) | ∀x ∈ R} (6)</formula><p>This means that we keep the semantics of the query word fixed, while walking along the polar- ity axis, thus retrieving different shades of polarity. <ref type="figure">Figure 1</ref> shows two example spectra. The fourth operation is "word v with POS of word w":</p><formula xml:id="formula_6">POS w (v) = nn(u v − u m v + u m w )<label>(7)</label></formula><p>This is similar to analogies like king − man + woman, except that the analogy is inferred by the subspace relevant for the analogy. We create word spectra for some manually cho- sen words using the Google News corpus (W2V) and a Twitter corpus. As the transformation was orthogonal and therefore did not change the length of a dimension, we multiply the polarity dimen- sion with 30 to give it a high weight, i.e., paying more attention to it. We then use Eq. 6 with a suf- ficiently small step size for x, i.e., further reduc- ing the step size does not increase the spectrum. We also discard words that have a cosine distance of more than .6 in the non-polarity space. Ta- ble 1 shows examples. The results are highly do- main dependent, with Twitter's spectrum indicat- ing more negative views of politicians than news. While fall has negative associations, autumn's are positive -probably because autumn is of a higher register in American English. <ref type="bibr">1</ref> See discussion/experiments below for exceptions  We evaluate on Adel and Schütze (2014)'s data; the task is to decide for a pair of words whether they are antonyms or synonyms. The set has 2,337 positive and negative pairs each and is split into 80% training, 10% dev and 10% test. <ref type="bibr" target="#b0">Adel and Schütze (2014)</ref> collected positive/negative exam- ples from the nearest neighbors of the word em- beddings to make it hard to solve the task using word embeddings. We train an SVM (RBF kernel) on three features that are based on the intuition de- picted in <ref type="figure">Figure 1</ref>: the three cosine distances in: the polarity subspace; the orthogonal complement; and the entire space.  <ref type="table">Table 3</ref>: Results for POS tagging. LSJU = Stanford. SVM = SVMTool. F=FLORS. We show three state- of-the-art taggers (lines 1-3), FLORS extended with 300-dimensional embeddings (4) and extended with UD embeddings (5). †: significantly better than the best result in the same column (α = .05, one-tailed Z-test).</p><note type="other">Corpus, Type Spectrum News, Polarity hypocrite, politician, legislator, busi- nessman, reformer, statesman, thinker fall, winter, summer, spring, autumn drunks, booze, liquor, lager, beer, beers, wine, beverages, wines, tastings Twitter, Polarity corrupt, coward, politician, journalist, citizen, musician, representative stalker, neighbour, gf, bf, cousin, frnd, friend, mentor #stupid, #problems, #homework, #mylife, #reality, #life, #happiness News, Concreteness imperialist, conflict, war, Iraq,</note><p>this dictionary returns a list of up to 80 words of shades of meaning between two polar opposites. We look for words that are also present in <ref type="bibr" target="#b0">Adel and Schütze (2014)</ref>'s Antonym Classification data and retrieve 35 spectra. Each word in a spectrum can be used as a query word; after intersecting the spectra with our vocabulary, we end up with 1301 test cases.</p><p>To evaluate PSC-SET, we calculate the 10 near- est neighbors of the m words in the spectrum and rank the 10m neighbors by the distance to our spectrum, i.e., the cosine distance in the orthog- onal complement of the polarity subspace. We re- port mean average precision (MAP) and weighted MAP where each MAP is weighted by the num- ber of words in the spectrum. As shown in <ref type="table" target="#tab_4">Table 4</ref> there is no big difference between both numbers, meaning that our algorithm does not work better or worse on smaller or larger spectra.</p><p>To evaluate PSC-ORD, we calculate Spear- man's ρ of the ranks in OAWT and the values on the polarity dimension. Again, there is no signifi- cant difference between average and weighted av- erage of ρ. Table 4 also shows that the variance of the measures is low for PSC-SET and high for PSC-ORD; thus, we do well on certain spectra and worse on others. The best one, beautiful ↔ ugly, is given as an example. The worst performing spectrum is fat ↔ skinny (ρ = .13) -presumably because both extremes are negative, contradicting our modeling assumption that spectra go from pos- itive to negative. We test this hypothesis by sepa- rating the spectrum into two subspectra. We then report the weighted average correlation of the op- timal separation. For fat ↔ skinny, this improves ρ to .67.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Morphological Analogy.</head><p>The previous two subspaces were one- dimensional. Now we consider a POS subspace, because POS is not one-dimensional and cannot be modeled as a single scalar quantity. We create a word analogy benchmark by extracting derivational forms from WordNet <ref type="bibr" target="#b5">(Fellbaum, 1998)</ref>. We discard words with ≥2 derivational forms of the same POS and words not in the most frequent 30,000. We then randomly se- lect 26 pairs for every POS combination for the dev set and 26 pairs for the test set. <ref type="bibr">2</ref> An example of the type of equation we solve here is prediction − predict + symbolize = symbol (from the dev set). W2V embeddings are our baseline. We can also rewrite the left side of the equation as POS(prediction) + Semantics(symbolize); note that this cannot be done using standard word em- beddings. In contrast, our method can use mean- ingful UD embeddings and Eq. 7 with POS(v) be- ing u m v and Semantics(v) being u v − u m v . The dev set indicates that a 8-dimensional POS subspace is optimal and <ref type="table">Table 5</ref> shows that this method out-  <ref type="table">Table 5</ref>: Accuracy @1 on test for Morphological Analogy. †: significantly better than the corre- sponding result in the same row (α = .05, one- tailed Z-test).</p><p>performs the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">POS Tagging</head><p>Our final evaluation is extrinsic. We use FLORS ( <ref type="bibr" target="#b10">Schnabel and Schütze, 2014</ref>), a state-of-the-art POS tagger which was extended by <ref type="bibr" target="#b16">Yin et al. (2015)</ref> with word embeddings as additional fea- tures. W2V gives us a consistent improvement on OOVs <ref type="table">(Table 3</ref>, line 4). However, training this model requires about 500GB of RAM. When we use the 8-dimensional UD embeddings (the same as for Morphological Analogy), we outperform W2V except for a virtual tie on news <ref type="table">(Table 3</ref>, line 5). So we perform better even though we only use 8 of 300 dimensions! However, the greatest advan- tage of UD is that we only need 100GB of RAM, 80% less than W2V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Yih et al. (2012) also tackled the problem of antonyms having similar embeddings. In their model, the antonym is the inverse of the en- tire vector whereas in our work the antonym is only the inverse in an ultradense subspace. Our model is more intuitive since antonyms invert only part of the meaning, not the entire mean- ing. <ref type="bibr" target="#b11">Schwartz et al. (2015)</ref> present a method that switches an antonym parameter on or off (depend- ing on whether a high antonym-synonym similar- ity is useful for an application) and learn multiple embedding spaces. We only need a single space, but consider different subspaces of this space. An unsupervised approach using linguistic pat- terns that ranks adjectives according to their inten- sity was presented by <ref type="bibr" target="#b4">de Melo and Bansal (2013)</ref>. <ref type="bibr" target="#b12">Sharma et al. (2015)</ref> present a corpus-independent approach for the same problem. Our results (Ta- ble 1) suggest that polarity should not be consid- ered to be corpus-independent.</p><p>There is also much work on incorporating the additional information into the original word embedding training. Examples include <ref type="bibr" target="#b1">(Botha and Blunsom, 2014)</ref> and <ref type="bibr" target="#b3">(Cotterell and Schütze, 2015)</ref>. However, postprocessing has several ad- vantages. DENSIFIER can be trained on a normal work station without access to the original train- ing corpus. This makes the method more flexible, e.g., when new training data or desired properties are available.</p><p>On a general level, our method bears some re- semblance with <ref type="bibr" target="#b13">(Weinberger and Saul, 2009</ref>) in that we perform supervised learning on a set of de- sired (dis)similarities and that we can think of our method as learning specialized metrics for particu- lar subtypes of linguistic information or particular tasks. Using the method of <ref type="bibr" target="#b13">Weinberger and Saul (2009)</ref>, one could learn k metrics for k subtypes of information and then simply represent a word w as the concatenation of (i) the original embedding and (ii) k representations corresponding to the k metrics. <ref type="bibr">3</ref> In case of a simple one-dimensional type of information, the corresponding representation could simply be a scalar. We would expect this approach to have similar advantages for practical applications, but we view our orthogonal transfor- mation of the original space as more elegant and it gives rise to a more compact representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We presented a new word embedding calculus based on meaningful ultradense subspaces. We applied the operations of the calculus to Antonym Classification, Polarity Spectrum Creation, Mor- phological Analogy and POS Tagging. Our eval- uation shows that our method outperforms pre- vious work and is applicable to different types of information. We have published test sets and word embeddings at http://www.cis.lmu. de/ ˜ sascha/Ultradense/.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Results for Antonym Classification 

4 Evaluation 

4.1 Antonym Classification. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 showsALL OOV ALL OOV ALL OOV ALL OOV ALL OOV 1 LSJU 89.11 † 56.02 † 91.43 † 58.66 † 94.15 † 77.13 † 88.92 † 49.30 † 88.68 † 58.42 † 96.83 90.25 2 SVM 89.14 † 53.82 † 91.30 † 54.20 † 94.21 † 76.44 † 88.96 † 47.25 † 88.64 † 56.37 † 96.63 87.96</head><label>2</label><figDesc>† 3 F 90.86 66.42 † 92.95 75.29 † 94.71 83.64 † 90.30 62.15 † 89.44 62.61 † 96.59 90.37 4 F+W2V 90.51 72.26 92.46 † 78.03 94.70 86.05 90.34 65.16 89.26 63.70 † 96.44 91.36 5 F+UD 90.79 72.20 92.84 78.80 94.84 86.47 90.60 65.48 89.68 66.24 96.61 92.36</figDesc><table>that improve-
ment of precision is minor (.76 vs. .75), but recall 
and F 1 improve by a lot (+.30 and +.16). 

4.2 Polarity Spectrum Creation 

consists of two subtasks. PSC-SET: Given a query 
word how well can we predict a spectrum? PSC-
ORD: How good is the order in the spectrum? 
Our gold standard is Word Spectrum, included in 
the Oxford American Writer's Thesaurus (OAWT) 
and therefore also in MacOS. For each query word </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Results for Polarity Spectrum Creation: 
MAP, Spearman's ρ (one spectrum) and average ρ 
(two subspectra) 

</table></figure>

			<note place="foot" n="2"> This results in an even number of 25 * 26 = 650 questions per POS combination, 4 * 2 * 650 = 5200 in total (4 POS combinations, where each POS can be used as query POS).</note>

			<note place="foot" n="3"> We would like to thank an anonymous reviewer for suggesting this alternative approach.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported by Deutsche Forschungsgemeinschaft (DFG, grant 2246/10-1).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Using mined coreference chains as a resource for a semantic task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heike</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Botha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.4273</idno>
		<title level="m">Compositional morphology for word representations and language modelling</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Concreteness ratings for 40 thousand generally known english word lemmas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brysbaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><forename type="middle">Beth</forename><surname>Warriner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Kuperman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior research methods</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="904" to="911" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Morphological word-embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05" />
			<biblScope unit="page" from="1287" to="1292" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Good, great, excellent: Global inference of semantic intensities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Gerard De Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="279" to="290" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">WordNet: An Electronic Lexical Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Bradford Books</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mining and Summarizing Customer Reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Crowdsourcing a Word-Emotion Association Lexicon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">D</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sascha</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07572</idno>
		<title level="m">Ultradense word embeddings by orthogonal transformation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Flors: Fast and simple domain adaptation for part-ofspeech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="15" to="26" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Symmetric pattern based word embeddings for improved word similarity prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adjective intensity and sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raksha</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Astha</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpak</forename><surname>Bhattacharyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">K</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="207" to="244" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recognizing contextual polarity in phrase-level sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT/EMNLP</title>
		<meeting>HLT/EMNLP</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Polarity inducing latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John C</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Online updating of word representations for part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
