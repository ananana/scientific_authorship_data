<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:34+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Keyphrase Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Meng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanqiang</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuguang</forename><surname>Han</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daqing</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Brusilovsky</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Chi</surname></persName>
						</author>
						<title level="a" type="main">Deep Keyphrase Generation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="582" to="592"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1054</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Keyphrase provides highly-summative information that can be effectively used for understanding, organizing and retrieving text content. Though previous studies have provided many workable solutions for automated keyphrase extraction, they commonly divided the to-be-summarized content into multiple text chunks, then ranked and selected the most meaningful ones. These approaches could neither identify keyphrases that do not appear in the text, nor capture the real semantic meaning behind the text. We propose a generative model for keyphrase prediction with an encoder-decoder framework, which can effectively overcome the above drawbacks. We name it as deep keyphrase generation since it attempts to capture the deep semantic meaning of the content with a deep learning method. Empirical analysis on six datasets demonstrates that our proposed model not only achieves a significant performance boost on extracting keyphrases that appear in the source text, but also can generate absent keyphrases based on the semantic meaning of the text. Code and dataset are available at https://github.com/memray/seq2seq-keyphrase.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A keyphrase or keyword is a piece of short, sum- mative content that expresses the main semantic meaning of a longer text. The typical use of a keyphrase or keyword is in scientific publications to provide the core information of a paper. We use * Corresponding author the term "keyphrase" interchangeably with "key- word" in the rest of this paper, as both terms have an implication that they may contain mul- tiple words. High-quality keyphrases can facili- tate the understanding, organizing, and accessing of document content. As a result, many studies have focused on ways of automatically extracting keyphrases from textual content ( <ref type="bibr" target="#b23">Liu et al., 2009;</ref><ref type="bibr" target="#b27">Medelyan et al., 2009a;</ref>). Due to public accessibility, many scientific publication datasets are often used as test beds for keyphrase extraction algorithms. Therefore, this study also focuses on extracting keyphrases from scientific publications.</p><p>Automatically extracting keyphrases from a document is called keypharase extraction, and it has been widely used in many applications, such as information retrieval <ref type="bibr" target="#b15">(Jones and Staveley, 1999</ref>), text summarization ( <ref type="bibr" target="#b47">Zhang et al., 2004</ref>), text categorization ( <ref type="bibr" target="#b14">Hulth and Megyesi, 2006</ref>), and opinion mining <ref type="bibr" target="#b2">(Berend, 2011</ref>). Most of the existing keyphrase extraction algorithms have addressed this problem through two steps ( <ref type="bibr" target="#b23">Liu et al., 2009;</ref><ref type="bibr" target="#b39">Tomokiyo and Hurst, 2003)</ref>. The first step is to acquire a list of keyphrase candi- dates. Researchers have tried to use n-grams or noun phrases with certain part-of-speech patterns for identifying potential candidates <ref type="bibr" target="#b13">(Hulth, 2003;</ref><ref type="bibr" target="#b20">Le et al., 2016;</ref><ref type="bibr" target="#b22">Liu et al., 2010;</ref>. The second step is to rank candidates on their importance to the document, either through su- pervised or unsupervised machine learning meth- ods with a set of manually-defined features <ref type="bibr" target="#b23">Liu et al., 2009</ref><ref type="bibr" target="#b22">Liu et al., , 2010</ref><ref type="bibr" target="#b16">Kelleher and Luz, 2005;</ref><ref type="bibr" target="#b26">Matsuo and Ishizuka, 2004;</ref><ref type="bibr" target="#b30">Mihalcea and Tarau, 2004;</ref><ref type="bibr" target="#b37">Song et al., 2003;</ref>).</p><p>There are two major drawbacks in the above keyphrase extraction approaches. First, these methods can only extract the keyphrases that ap-pear in the source text; they fail at predicting meaningful keyphrases with a slightly different se- quential order or those that use synonyms. How- ever, authors of scientific publications commonly assign keyphrases based on their semantic mean- ing, instead of following the written content in the publication. In this paper, we denote phrases that do not match any contiguous subsequence of source text as absent keyphrases, and the ones that fully match a part of the text as present keyphrases. <ref type="table" target="#tab_0">Table 1</ref> shows the proportion of present and absent keyphrases from the docu- ment abstract in four commonly-used datasets, from which we can observe large portions of ab- sent keyphrases in all the datasets. The absent keyphrases cannot be extracted through previous approaches, which further prompts the develop- ment of a more powerful keyphrase prediction model.</p><p>Second, when ranking phrase candidates, pre- vious approaches often adopted machine learning features such as TF-IDF and PageRank. However, these features only target to detect the importance of each word in the document based on the statis- tics of word occurrence and co-occurrence, and are unable to reveal the full semantics that underlie the document content. To overcome the limitations of previous stud- ies, we re-examine the process of keyphrase pre- diction with a focus on how real human annotators would assign keyphrases. Given a document, hu- man annotators will first read the text to get a ba- sic understanding of the content, then they try to digest its essential content and summarize it into keyphrases. Their generation of keyphrases relies on an understanding of the content, which may not necessarily use the exact words that occur in the source text. For example, when human annota- tors see "Latent Dirichlet Allocation" in the text, they might write down "topic modeling" and/or "text mining" as possible keyphrases. In addition to the semantic understanding, human annotators might also go back and pick up the most impor- tant parts, based on syntactic features. For exam- ple, the phrases following "we propose/apply/use" could be important in the text. As a result, a better keyphrase prediction model should understand the semantic meaning of the content, as well as cap- ture the contextual features.</p><p>To effectively capture both the semantic and syntactic features, we use recurrent neural net- works (RNN) ( <ref type="bibr" target="#b5">Gers and Schmidhuber, 2001</ref>) to compress the semantic informa- tion in the given text into a dense vector (i.e., se- mantic understanding). Furthermore, we incorpo- rate a copying mechanism ( <ref type="bibr" target="#b9">Gu et al., 2016</ref>) to al- low our model to find important parts based on positional information. Thus, our model can gen- erate keyphrases based on an understanding of the text, regardless of the presence or absence of keyphrases in the text; at the same time, it does not lose important in-text information.</p><p>The contribution of this paper is three-fold. First, we propose to apply an RNN-based gen- erative model to keyphrase prediction, as well as incorporate a copying mechanism in RNN, which enables the model to successfully pre- dict phrases that rarely occur. Second, this is the first work that concerns the problem of ab- sent keyphrase prediction for scientific publica- tions, and our model recalls up to 20% of absent keyphrases. Third, we conducted a comprehen- sive comparison against six important baselines on a broad range of datasets, and the results show that our proposed model significantly outperforms existing supervised and unsupervised extraction methods.</p><p>In the remainder of this paper, we first review the related work in Section 2. Then, we elaborate upon the proposed model in Section 3. After that, we present the experiment setting in Section 4 and results in Section 5, followed by our discussion in Section 6. Section 7 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Automatic Keyphrase Extraction</head><p>A keyphrase provides a succinct and accurate way of describing a subject or a subtopic in a document. A number of extraction algorithms have been proposed, and the process of extracting keyphrases can typically be broken down into two steps.</p><p>The first step is to generate a list of phrase can-didates with heuristic methods. As these candi- dates are prepared for further filtering, a consid- erable number of candidates are produced in this step to increase the possibility that most of the correct keyphrases are kept. The primary ways of extracting candidates include retaining word se- quences that match certain part-of-speech tag pat- terns (e.g., nouns, adjectives) ( <ref type="bibr" target="#b21">Liu et al., 2011;</ref><ref type="bibr" target="#b20">Le et al., 2016)</ref>, and extracting important n-grams or noun phrases <ref type="bibr" target="#b13">(Hulth, 2003;</ref><ref type="bibr" target="#b29">Medelyan et al., 2008)</ref>. The second step is to score each candidate phrase for its likelihood of being a keyphrase in the given document. The top-ranked candidates are returned as keyphrases. Both supervised and un- supervised machine learning methods are widely employed here. For supervised methods, this task is solved as a binary classification problem, and various types of learning methods and features have been explored <ref type="bibr" target="#b13">Hulth, 2003;</ref><ref type="bibr" target="#b28">Medelyan et al., 2009b;</ref><ref type="bibr" target="#b24">Lopez and Romary, 2010;</ref><ref type="bibr" target="#b6">Gollapalli and Caragea, 2014)</ref>. As for unsupervised approaches, primary ideas include finding the central nodes in text graph ( <ref type="bibr" target="#b30">Mihalcea and Tarau, 2004;</ref><ref type="bibr" target="#b8">Grineva et al., 2009)</ref>, detecting representative phrases from topi- cal clusters ( <ref type="bibr" target="#b23">Liu et al., 2009</ref><ref type="bibr" target="#b22">Liu et al., , 2010</ref>, and so on.</p><p>Aside from the commonly adopted two-step process, another two previous studies realized the keyphrase extraction in entirely different ways. <ref type="bibr" target="#b39">Tomokiyo and Hurst (2003)</ref> applied two language models to measure the phraseness and informa- tiveness of phrases. <ref type="bibr" target="#b21">Liu et al. (2011)</ref> share the most similar ideas to our work. They used a word alignment model, which learns a translation from the documents to the keyphrases. This approach alleviates the problem of vocabulary gaps between source and target to a certain degree. However, this translation model is unable to handle seman- tic meaning. Additionally, this model was trained with the target of title/summary to enlarge the number of training samples, which may diverge from the real objective of generating keyphrases. <ref type="bibr" target="#b46">Zhang et al. (2016)</ref> proposed a joint-layer recur- rent neural network model to extract keyphrases from tweets, which is another application of deep neural networks in the context of keyphrase ex- traction. However, their work focused on se- quence labeling, and is therefore not able to pre- dict absent keyphrases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Encoder-Decoder Model</head><p>The RNN Encoder-Decoder model (which is also referred as sequence-to-sequence Learning) is an end-to-end approach. It was first introduced by  and <ref type="bibr" target="#b38">Sutskever et al. (2014)</ref> to solve translation problems. As it provides a pow- erful tool for modeling variable-length sequences in an end-to-end fashion, it fits many natural lan- guage processing tasks and can rapidly achieve great successes ( <ref type="bibr" target="#b34">Rush et al., 2015;</ref><ref type="bibr" target="#b41">Vinyals et al., 2015;</ref><ref type="bibr" target="#b35">Serban et al., 2016)</ref>.</p><p>Different strategies have been explored to im- prove the performance of the Encoder-Decoder model. The attention mechanism ( ) is a soft alignment approach that allows the model to automatically locate the relevant input components. In order to make use of the impor- tant information in the source text, some stud- ies sought ways to copy certain parts of content from the source text and paste them into the target text ( <ref type="bibr" target="#b0">Allamanis et al., 2016;</ref><ref type="bibr" target="#b9">Gu et al., 2016;</ref><ref type="bibr" target="#b45">Zeng et al., 2016)</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. A discrepancy exists between the optimizing objective during training and the met- rics during evaluation. A few studies attempted to eliminate this discrepancy by incorporating new training algorithms (Marc'Aurelio Ranzato et al., 2016) or by modifying the optimizing ob- jectives (Shen et al., 2016).</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>This section will introduce our proposed deep keyphrase generation method in detail. First, the task of keyphrase generation is defined, fol- lowed by an overview of how we apply the RNN Encoder-Decoder model. Details of the frame- work as well as the copying mechanism will be introduced in Sections 3.3 and 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition</head><p>Given a keyphrase dataset that consists of N data samples, the i-th data sample (x (i) , p (i) ) contains one source text x (i) , and</p><formula xml:id="formula_0">M i tar- get keyphrases p (i) = (p (i,1) , p (i,2) , . . . , p (i,M i ) ).</formula><p>Both the source text x (i) and keyphrase p (i,j) are sequences of words:</p><formula xml:id="formula_1">x (i) = x (i) 1 , x (i) 2 , . . . , x (i) L x i p (i,j) = y (i,j) 1 , y (i,j) 2 , . . . , y (i,j) L p (i,j)</formula><p>L x (i) and L p (i,j) denotes the length of word se- quence of x (i) and p (i,j) respectively.</p><p>Each data sample contains one source text sequence and multiple target phrase sequences. To apply the RNN Encoder-Decoder model, the data need to be converted into text-keyphrase pairs that contain only one source sequence and one target sequence. We adopt a simple way, which splits the data sample (</p><formula xml:id="formula_2">x (i) , p (i) ) into M i pairs: (x (i) , p (i,1) ), (x (i) , p (i,2) ), . . . , (x (i) , p (i,M i ) ).</formula><p>Then the Encoder-Decoder model is ready to be applied to learn the mapping from the source sequence to target sequence. For the purpose of simplicity, (x, y) is used to denote each data pair in the rest of this section, where x is the word sequence of a source text and y is the word sequence of its keyphrase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Encoder-Decoder Model</head><p>The basic idea of our keyphrase generation model is to compress the content of source text into a hid- den representation with an encoder and to generate corresponding keyphrases with the decoder, based on the representation . Both the encoder and de- coder are implemented with recurrent neural net- works (RNN).</p><p>The encoder RNN converts the variable-length input sequence x = (x 1 , x 2 , ..., x T ) into a set of hidden representation h = (h 1 , h 2 , . . . , h T ), by iterating the following equations along time t:</p><formula xml:id="formula_3">h t = f (x t , h t−1 ) (1)</formula><p>where f is a non-linear function. We get the con- text vector c acting as the representation of the whole input x through a non-linear function q.</p><formula xml:id="formula_4">c = q(h 1 , h 2 , ..., h T )<label>(2)</label></formula><p>The decoder is another RNN; it decompresses the context vector and generates a variable-length sequence y = (y 1 , y 2 , ..., y T ) word by word, through a conditional language model:</p><formula xml:id="formula_5">s t = f (y t−1 , s t−1 , c) p(y t |y 1,...,t−1 , x) = g(y t−1 , s t , c)<label>(3)</label></formula><p>where s t is the hidden state of the decoder RNN at time t. The non-linear function g is a softmax classifier, which outputs the probabilities of all the words in the vocabulary. y t is the predicted word at time t, by taking the word with largest probabil- ity after g(·). The encoder and decoder networks are trained jointly to maximize the conditional probability of the target sequence, given a source sequence. Af- ter training, we use the beam search to generate phrases and a max heap is maintained to get the predicted word sequences with the highest proba- bilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Details of the Encoder and Decoder</head><p>A bidirectional gated recurrent unit (GRU) is ap- plied as our encoder to replace the simple recur- rent neural network. Previous studies ( ) indicate that it can generally provide better performance of language modeling than a simple RNN and a simpler struc- ture than other Long Short-Term Memory net- works <ref type="bibr" target="#b12">(Hochreiter and Schmidhuber, 1997)</ref>. As a result, the above non-linear function f is replaced by the GRU function (see in <ref type="figure" target="#fig_0">(Cho et al., 2014)</ref>).</p><p>Another forward GRU is used as the decoder. In addition, an attention mechanism is adopted to improve performance. The attention mechanism was firstly introduced by  to make the model dynamically focus on the impor- tant parts in input. The context vector c is com- puted as a weighted sum of hidden representation h = (h 1 , . . . , h T ):</p><formula xml:id="formula_6">c i = T j=1 α ij h j α ij = exp(a(s i−1 , h j )) T k=1 exp(a(s i−1 , h k ))<label>(4)</label></formula><p>where a(s i−1 , h j ) is a soft alignment function that measures the similarity between s i−1 and h j ; namely, to which degree the inputs around posi- tion j and the output at position i match.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Copying Mechanism</head><p>To ensure the quality of learned representation and reduce the size of the vocabulary, typically the RNN model considers a certain number of fre- quent words (e.g. 30,000 words in ( ), but a large amount of long-tail words are simply ignored. Therefore, the RNN is not able to recall any keyphrase that contains out-of- vocabulary words. Actually, important phrases can also be identified by positional and syntactic information in their contexts, even though their ex- act meanings are not known. The copying mecha- nism ( <ref type="bibr" target="#b9">Gu et al., 2016</ref>) is one feasible solution that enables RNN to predict out-of-vocabulary words by selecting appropriate words from the source text.</p><p>By incorporating the copying mechanism, the probability of predicting each new word y t con- sists of two parts. The first term is the probability of generating the term (see Equation 3) and the second one is the probability of copying it from the source text: p(y t |y 1,...,t−1 , x) = p g (y t |y 1,...,t−1 , x) + p c (y t |y 1,...,t−1 , x)</p><p>Similar to attention mechanism, the copying mechanism weights the importance of each word in source text with a measure of positional atten- tion. But unlike the generative RNN which pre- dicts the next word from all the words in vocabu- lary, the copying part p c (y t |y 1,...,t−1 , x) only con- siders the words in source text. Consequently, on the one hand, the RNN with copying mechanism is able to predict the words that are out of vocab- ulary but in the source text; on the other hand, the model would potentially give preference to the ap- pearing words, which caters to the fact that most keyphrases tend to appear in the source text.</p><formula xml:id="formula_8">p c (y t |y 1,...,t−1 , x) = 1 Z j:x j =yt exp(ψ c (x j )), y ∈ χ ψ c (x j ) = σ(h T j W c )s t<label>(6)</label></formula><p>where χ is the set of all of the unique words in the source text x, σ is a non-linear function and W c ∈ R is a learned parameter matrix. Z is the sum of all the scores and is used for normalization. Please see ( <ref type="bibr" target="#b9">Gu et al., 2016</ref>) for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment Settings</head><p>This section begins by discussing how we de- signed our evaluation experiments, followed by the description of training and testing datasets. Then, we introduce our evaluation metrics and baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training Dataset</head><p>There are several publicly-available datasets for evaluating keyphrase generation. The largest one came from <ref type="bibr" target="#b19">Krapivin et al. (2008)</ref>, which con- tains 2,304 scientific publications. However, this amount of data is unable to train a robust recur- rent neural network model. In fact, there are mil- lions of scientific papers available online, each of which contains the keyphrases that were assigned by their authors. Therefore, we collected a large amount of high-quality scientific metadata in the computer science domain from various online dig- ital libraries, including ACM Digital Library, Sci- enceDirect, Wiley, and Web of Science etc. ( <ref type="bibr" target="#b10">Han et al., 2013;</ref><ref type="bibr" target="#b32">Rui et al., 2016)</ref>. In total, we ob- tained a dataset of 567,830 articles, after remov- ing duplicates and overlaps with testing datasets, which is 200 times larger than the one of <ref type="bibr" target="#b19">Krapivin et al. (2008)</ref>. Note that our model is only trained on 527,830 articles, since 40,000 publications are randomly held out, among which 20,000 articles were used for building a new test dataset KP20k. Another 20,000 articles served as the validation dataset to check the convergence of our model, as well as the training dataset for supervised base- lines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Testing Datasets</head><p>For evaluating the proposed model more compre- hensively, four widely-adopted scientific publica- tion datasets were used. In addition, since these datasets only contain a few hundred or a few thou- sand publications, we contribute a new testing dataset KP20k with a much larger number of sci- entific articles. We take the title and abstract as the source text. Each dataset is described in detail below.</p><p>-Inspec (Hulth, 2003): This dataset provides 2,000 paper abstracts. We adopt the 500 test- ing papers and their corresponding uncon- trolled keyphrases for evaluation, and the re- maining 1,500 papers are used for training the supervised baseline models.</p><p>-Krapivin ( <ref type="bibr" target="#b19">Krapivin et al., 2008</ref>): This dataset provides 2,304 papers with full-text and author-assigned keyphrases. However, the author did not mention how to split test- ing data, so we selected the first 400 papers in alphabetical order as the testing data, and the remaining papers are used to train the su- pervised baselines.</p><p>-NUS (Nguyen and Kan, 2007): We use the author-assigned keyphrases and treat all 211 papers as the testing data. Since the NUS dataset did not specifically mention the ways of splitting training and testing data, the re- sults of the supervised baseline models are obtained through a five-fold cross-validation.</p><p>- <ref type="bibr">SemEval-2010</ref><ref type="bibr" target="#b17">(Kim et al., 2010</ref>: 288 ar- ticles were collected from the ACM Digital Library. 100 articles were used for testing and the rest were used for training supervised baselines.</p><p>-KP20k: We built a new testing dataset that contains the titles, abstracts, and keyphrases of 20,000 scientific articles in computer sci- ence. They were randomly selected from our obtained 567,830 articles. Due to the mem- ory limits of implementation, we were not able to train the supervised baselines on the whole training set. Thus we take the 20,000 articles in the validation set to train the su- pervised baselines. It is worth noting that we also examined their performance by enlarg- ing the training dataset to 50,000 articles, but no significant improvement was observed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details</head><p>In total, there are 2,780,316 text, keyphrase pairs for training, in which text refers to the concate- nation of the title and abstract of a publication, and keyphrase indicates an author-assigned key- word. The text pre-processing steps including to- kenization, lowercasing and replacing all digits with symbol digit are applied. Two encoder- decoder models are trained, one with only at- tention mechanism (RNN) and one with both at- tention and copying mechanism enabled (Copy- RNN). For both models, we choose the top 50,000 frequently-occurred words as our vocabulary, the dimension of embedding is set to 150, the di- mension of hidden layers is set to 300, and the word embeddings are randomly initialized with uniform distribution in [-0.1,0.1]. Models are op- timized using Adam ( <ref type="bibr" target="#b18">Kingma and Ba, 2014</ref>) with initial learning rate = 10 −4 , gradient clipping = 0.1 and dropout rate = 0.5. The max depth of beam search is set to 6, and the beam size is set to 200. The training is stopped once convergence is de- termined on the validation dataset (namely early- stopping, the cross-entropy loss stops dropping for several iterations).</p><p>In the generation of keyphrases, we find that the model tends to assign higher probabilities for shorter keyphrases, whereas most keyphrases con- tain more than two words. To resolve this problem, we apply a simple heuristic by preserving only the first single-word phrase (with the highest generat- ing probability) and removing the rest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Baseline Models</head><p>Four unsupervised algorithms <ref type="bibr">(Tf-Idf, TextRank (Mihalcea and Tarau, 2004</ref>), SingleR- ank ( <ref type="bibr" target="#b42">Wan and Xiao, 2008)</ref>, and ExpandRank ( <ref type="bibr" target="#b42">Wan and Xiao, 2008)</ref>) and two supervised algorithms (KEA ( ) and <ref type="bibr">Maui (Medelyan et al., 2009a)</ref>) are adopted as baselines. We set up the four unsupervised methods following the opti- mal settings in <ref type="bibr" target="#b11">(Hasan and Ng, 2010)</ref>, and the two supervised methods following the default setting as specified in their papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Evaluation Metric</head><p>Three evaluation metrics, the macro-averaged pre- cision, recall and F-measure (F 1 ) are employed for measuring the algorithm's performance. Fol- lowing the standard definition, precision is defined as the number of correctly-predicted keyphrases over the number of all predicted keyphrases, and recall is computed by the number of correctly- predicted keyphrases over the total number of data records. Note that, when determining the match of two keyphrases, we use Porter Stemmer for pre- processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Analysis</head><p>We conduct an empirical study on three different tasks to evaluate our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Predicting Present Keyphrases</head><p>This is the same as the keyphrase extraction task in prior studies, in which we analyze how well our proposed model performs on a commonly-defined task. To make a fair comparison, we only con- sider the present keyphrases for evaluation in this task. <ref type="table">Table 2</ref> provides the performances of the six baseline models, as well as our proposed models (i.e., RNN and CopyRNN). For each method, the table lists its F-measure at top 5 and top 10 pre- dictions on the five datasets. The best scores are highlighted in bold and the underlines indicate the second best performances.</p><p>The results show that the four unsupervised models (Tf-idf, TextTank, SingleRank and Ex- pandRank) have a robust performance across dif- ferent datasets. The ExpandRank fails to return any result on the KP20k dataset, due to its high time complexity. The measures on NUS and Se- mEval here are higher than the ones reported in <ref type="bibr" target="#b11">(Hasan and Ng, 2010)</ref> and <ref type="bibr" target="#b17">(Kim et al., 2010)</ref>, probably because we utilized the paper abstract instead of the full text for training, which may   <ref type="table">Table 2</ref>: The performance of predicting present keyphrases of various models on five benchmark datasets filter out some noisy information. The perfor- mance of the two supervised models (i.e., Maui and KEA) were unstable on some datasets, but Maui achieved the best performances on three datasets among all the baseline models.</p><formula xml:id="formula_9">Method Inspec Krapivin NUS SemEval KP20k F 1 @5 F 1 @10 F 1 @5 F 1 @10 F 1 @5 F 1 @10 F 1 @5 F 1 @10 F 1 @5 F 1 @10 Tf-Idf</formula><note type="other">ExpandRank 0.210 0.304 0.081 0.126 0.132 0.164 0.139 0.170 N/A N/A Maui 0.040 0.042 0.249 0.216 0.249 0.268 0.044 0.039 0.270 0.230 KEA 0.098 0.126 0.110 0.152 0.069 0.084 0.025 0.026 0.171 0.</note><p>As for our proposed keyphrase prediction ap- proaches, the RNN model with the attention mech- anism did not perform as well as we expected. It might be because the RNN model is only con- cerned with finding the hidden semantics behind the text, which may tend to generate keyphrases or words that are too general and may not neces- sarily refer to the source text. In addition, we ob- serve that 2.5% (70,891/2,780,316) of keyphrases in our dataset contain out-of-vocabulary words, which the RNN model is not able to recall, since the RNN model can only generate results with the 50,000 words in vocabulary. This indicates that a pure generative model may not fit the ex- traction task, and we need to further link back to the language usage within the source text. The CopyRNN model, by considering more contextual information, significantly outperforms not only the RNN model but also all baselines, exceed- ing the best baselines by more than 20% on av- erage. This result demonstrates the importance of source text to the extraction task. Besides, nearly 2% of all correct predictions contained out- of-vocabulary words.</p><p>The example in <ref type="figure" target="#fig_0">Figure 1(a)</ref> shows the result of predicted present keyphrases by RNN and Copy- RNN for an article about video search. We see that both models can generate phrases that relate to the topic of information retrieval and video. How- ever most of RNN predictions are high-level ter- minologies, which are too general to be selected as keyphrases. CopyRNN, on the other hand, predicts more detailed phrases like "video meta- data" and "integrated ranking". An interesting bad case, "rich content" coordinates with a keyphrase "video metadata", and the CopyRNN mistakenly puts it into prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Predicting Absent Keyphrases</head><p>As stated, one important motivation for this work is that we are interested in the proposed model's capability for predicting absent keyphrases based on the "understanding" of content. It is worth noting that such prediction is a very challenging task, and, to the best of our knowledge, no existing methods can handle this task. Therefore, we only provide the RNN and CopyRNN performances in the discussion of the results of this task. Here, we evaluate the performance within the recall of the top 10 and top 50 results, to see how many absent keyphrases can be correctly predicted. We use the absent keyphrases in the testing datasets for eval- uation.   <ref type="table" target="#tab_4">Table 3</ref> presents the recall results of the top 10/50 predicted keyphrases for our RNN and CopyRNN models, in which we observe that the CopyRNN can, on average, recall around 8% (15%) of keyphrases at top 10 (50) predictions. This indicates that, to some extent, both models can capture the hidden semantics behind the tex- tual content and make reasonable predictions. In addition, with the advantage of features from the source text, the CopyRNN model also outperforms the RNN model in this condition, though it does not show as much improvement as the present keyphrase extraction task. An example is shown in <ref type="figure" target="#fig_0">Figure 1(b)</ref>, in which we see that two absent keyphrases, "video retrieval" and "video index- ing", are correctly recalled by both models. Note that the term "indexing" does not appear in the text, but the models may detect the information "index videos" in the first sentence and paraphrase it to the target phrase. And the CopyRNN success- fully predicts another two keyphrases by capturing the detailed information from the text (highlighted text segments).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Transferring the Model to the News Domain</head><p>RNN and CopyRNN are supervised models, and they are trained on data in a specific domain and writing style. However, with sufficient training on a large-scale dataset, we expect the models to be able to learn universal language features that are also effective in other corpora. Thus in this task, we will test our model on another type of text, to see whether the model would work when being transferred to a different environment. We use the popular news article dataset <ref type="bibr">DUC2001 (Wan and</ref><ref type="bibr" target="#b42">Xiao, 2008</ref>) for analysis. The dataset consists of 308 news articles and 2,488 manually annotated keyphrases. The result of this analysis is shown in <ref type="table" target="#tab_6">Table 4</ref>, from which we could see that the CopyRNN can extract a portion of cor- rect keyphrases from a unfamiliar text. Compared to the results reported in <ref type="bibr" target="#b11">(Hasan and Ng, 2010)</ref>, the performance of CopyRNN is better than Tex- tRank ( <ref type="bibr" target="#b30">Mihalcea and Tarau, 2004</ref>) and KeyClus- ter ( <ref type="bibr" target="#b23">Liu et al., 2009)</ref>, but lags behind the other three baselines.</p><p>As it is transferred to a corpus in a completely different type and domain, the model encounters more unknown words and has to rely more on the positional and syntactic features within the text. In this experiment, the CopyRNN recalls 766 keyphrases. 14.3% of them contain out-of- vocabulary words, and many names of persons and places are correctly predicted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>Our experimental results demonstrate that the CopyRNN model not only performs well on pre- dicting present keyphrases, but also has the abil- ity to generate topically relevant keyphrases that are absent in the text. In a broader sense, this model attempts to map a long text (i.e., paper ab- stract) with representative short text chunks (i.e., keyphrases), which can potentially be applied to improve information retrieval performance by generating high-quality index terms, as well as as- sisting user browsing by summarizing long docu- ments into short, readable phrases.</p><p>Thus far, we have tested our model with sci- entific publications and news articles, and have demonstrated that our model has the ability to cap- ture universal language patterns and extract key in- formation from unfamiliar texts. We believe that our model has a greater potential to be general- ized to other domains and types, like books, online reviews, etc., if it is trained on a larger data cor- pus. Also, we directly applied our model, which was trained on a publication dataset, into generat- ing keyphrases for news articles without any adap- tive training. We believe that with proper training on news data, the model would make further im- provement.</p><p>Additionally, this work mainly studies the prob- lem of discovering core content from textual mate- rials. Here, the encoder-decoder framework is ap- plied to model language; however, such a frame- work can also be extended to locate the core infor- mation on other data resources, such as summariz- ing content from images and videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions and Future Work</head><p>In this paper, we proposed an RNN-based gen- erative model for predicting keyphrases in scien- tific text. To the best of our knowledge, this is the first application of the encoder-decoder model to a keyphrase prediction task. Our model sum- marizes phrases based the deep semantic meaning of the text, and is able to handle rarely-occurred phrases by incorporating a copying mechanism. Comprehensive empirical studies demonstrate the effectiveness of our proposed model for generat- ing both present and absent keyphrases for differ- ent types of text. Our future work may include the following two directions.</p><p>-In this work, we only evaluated the perfor- mance of the proposed model by conducting off-line experiments. In the future, we are in- terested in comparing the model to human an- notators and using human judges to evaluate the quality of predicted phrases.</p><p>-Our current model does not fully consider correlation among target keyphrases. It would also be interesting to explore the multiple-output optimization aspects of our model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of predicted keyphrase by RNN and CopyRNN. Phrases shown in bold are correct predictions.</figDesc><graphic url="image-1.png" coords="9,72.00,62.81,453.52,164.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Proportion of the present keyphrases and 
absent keyphrases in four public datasets 

Dataset 
# Keyphrase % Present % Absent 
Inspec 
19,275 
55.69 
44.31 
Krapivin 
2,461 
44.74 
52.26 
NUS 
2,834 
67.75 
32.25 
SemEval 
12,296 
42.01 
57.99 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Absent keyphrases prediction perfor-
mance of RNN and CopyRNN on five datasets 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Keyphrase prediction performance of 
CopyRNN on DUC-2001. The model is trained 
on scientific publication and evaluated on news. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Jiatao Gu and Miltiadis Allamanis for sharing the source code and giv-ing helpful advice. We also thank Wei Lu, Yong Huang, Qikai Cheng and other IRLAB members at Wuhan University for the assistance of dataset development. This work is partially supported by the National Science Foundation under Grant No.1525186.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A Convolutional Attention Network for Extreme Summarization of Source Code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Opinion expression mining by exploiting keyphrase extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gábor</forename><surname>Berend</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNLP. Citeseer</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1162" to="1170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Domain-specific keyphrase extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eibe</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paynter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig G Nevill-Manning</forename><surname>Gutwin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Lstm recurrent networks learn simple context-free and contextsensitive languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1333" to="1340" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Extracting keyphrases from research papers using citation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Das</forename><surname>Sujatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cornelia</forename><surname>Gollapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caragea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<title level="m">AAAI Conference on Artificial Intelligence. AAAI Press, AAAI&apos;14</title>
		<imprint>
			<biblScope unit="page" from="1629" to="1635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Extracting key terms from noisy and multitheme documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Grineva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Grinev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Lizorkin</surname></persName>
		</author>
		<idno type="doi">10.1145/1526709.1526798</idno>
		<ptr target="https://doi.org/10.1145/1526709.1526798" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on World Wide Web</title>
		<meeting>the 18th International Conference on World Wide Web<address><addrLine>New York, NY, USA, WWW &apos;09</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="661" to="670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06393</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Supporting exploratory people search: a study of factor transparency and user control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuguang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daqing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiepu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Yue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Information &amp; Knowledge Management</title>
		<meeting>the 22nd ACM international conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="449" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Conundrums in unsupervised keyphrase extraction: making sense of the state-of-the-art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saidul</forename><surname>Kazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics: Posters. Association for Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics: Posters. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="365" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improved automatic keyword extraction given more linguistic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anette</forename><surname>Hulth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 conference on Empirical methods in natural language processing</title>
		<meeting>the 2003 conference on Empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="216" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A study on automatically extracted keywords in text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anette</forename><surname>Hulth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beáta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Megyesi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="537" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Phrasier: a system for interactive document retrieval using keyphrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Staveley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 22nd annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic hypertext keyphrase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kelleher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saturnino</forename><surname>Luz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 19th International Joint Conference on Artificial Intelligence<address><addrLine>San Francisco, CA, USA,</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1608" to="1609" />
		</imprint>
	</monogr>
	<note>IJCAI&apos;05</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semeval-2010 task 5: Automatic keyphrase extraction from scientific articles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olena</forename><surname>Su Nam Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Medelyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Semantic Evaluation. Association for Computational Linguistics</title>
		<meeting>the 5th International Workshop on Semantic Evaluation. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="21" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Large dataset for keyphrases extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikalai</forename><surname>Krapivin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksandr</forename><surname>Autayeu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maurizio</forename><surname>Marchese</surname></persName>
		</author>
		<idno>DISI-09-055</idno>
		<imprint>
			<date type="published" when="2008" />
			<pubPlace>DISI, Trento, Italy</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Unsupervised Keyphrase Extraction: Introducing New Kinds of Words to Keyphrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tho Thi Ngoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><forename type="middle">Le</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Shimazu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="665" to="671" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automatic keyphrase extraction by bridging vocabulary gap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Fifteenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="135" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Automatic keyphrase extraction via topic decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 conference on empirical methods in natural language processing. Association for Computational Linguistics</title>
		<meeting>the 2010 conference on empirical methods in natural language processing. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="366" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Clustering to find exemplar terms for keyphrase extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Humb: Automatic key term extraction from scientific articles in grobid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Romary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Semantic Evaluation</title>
		<meeting>the 5th International Workshop on Semantic Evaluation<address><addrLine>Stroudsburg, PA, USA, SemEval &apos;10</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="248" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Sumit Chopra Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
		<title level="m">Sequence level training with recurrent neural networks. ICLR</title>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Keyword extraction from a single document using word co-occurrence statistical information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitsuru</forename><surname>Ishizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Artificial Intelligence Tools</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">01</biblScope>
			<biblScope unit="page" from="157" to="169" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Human-competitive tagging using automatic keyphrase extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eibe</forename><surname>Olena Medelyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1318" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Human-competitive tagging using automatic keyphrase extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eibe</forename><surname>Olena Medelyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Stroudsburg, PA, USA, EMNLP &apos;09</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1318" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Topic indexing with wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Olena Medelyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Milne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI WikiAI workshop</title>
		<meeting>the AAAI WikiAI workshop</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="19" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Textrank: Bringing order into texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tarau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Keyphrase extraction in scientific publications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thuy</forename><forename type="middle">Dung</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Asian Digital Libraries</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="317" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Knowledge-based content linking for online textbooks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Shuguang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huang</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Daqing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brusilovsky</forename><surname>Peter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">ACM International Conference on Web Intelligence. The Institute of Electrical and Electronics Engineers</title>
		<imprint>
			<biblScope unit="page" from="18" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D/D15/D15-1044.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-17" />
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Building end-to-end dialogue systems using generative hierarchical neural network models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Iulian V Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI-16)</title>
		<meeting>the 30th AAAI Conference on Artificial Intelligence (AAAI-16)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Minimum risk training for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P16-1159" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1683" to="1692" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Kpspotter: a flexible information gain-based keyphrase extraction system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th ACM international workshop on Web information and data management</title>
		<meeting>the 5th ACM international workshop on Web information and data management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="50" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A language model approach to keyphrase extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Tomokiyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Hurst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL</title>
		<meeting>the ACL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<idno type="doi">10.3115/1119282.1119287</idno>
		<ptr target="https://doi.org/10.3115/1119282.1119287" />
		<title level="m">Workshop on Multiword Expressions: Analysis, Acquisition and Treatment</title>
		<meeting><address><addrLine>Stroudsburg, PA, USA, MWE &apos;03</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2773" to="2781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Single document keyphrase extraction using neighborhood knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">PTR: Phrase-Based Topical Ranking for Automatic Keyphrase Extraction in Scientific Publications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minmei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihua</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="120" to="128" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Kea: Practical automatic keyphrase extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><forename type="middle">W</forename><surname>Ian H Witten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eibe</forename><surname>Paynter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig G Nevill-Manning</forename><surname>Gutwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth ACM conference on Digital libraries</title>
		<meeting>the fourth ACM conference on Digital libraries</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="254" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Efficient summarization with read-again and copy mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03382</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Keyphrase extraction using deep recurrent neural networks on twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeyun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<ptr target="https://aclweb.org/anthology/D16-1080" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="836" to="845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">World wide web site summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nur</forename><surname>Zincir-Heywood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Milios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Web Intelligence and Agent Systems: An International Journal</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="53" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
