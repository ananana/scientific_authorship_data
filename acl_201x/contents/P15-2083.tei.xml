<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:57+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Extended Topic Model for Word Dependency</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wang</surname></persName>
							<email>tongwang0001@gmail.com, Ping.Chen@umb.edu Vish Viswanath@dfci.harvard.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Massachusetts Boston</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vish</forename><surname>Viswanath</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Harvard School of Public Health</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Massachusetts Boston</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Extended Topic Model for Word Dependency</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="506" to="510"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Topic Model such as Latent Dirichlet Allocation(LDA) makes assumption that topic assignment of different words are conditionally independent. In this paper, we propose a new model Extended Global Topic Random Field (EGTRF) to model non-linear dependencies between words. Specifically, we parse sentences into dependency trees and represent them as a graph, and assume the topic assignment of a word is influenced by its adjacent words and distance-2 words. Word similarity information learned from large corpus is incorporated to enhance word topic assignment. Parameters are estimated efficiently by variational inference and experimental results on two datasets show EGTRF achieves lower perplexity and higher log predictive probability.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Probabilistic topic model such as Latent Dirich- let Allocation(LDA) ( <ref type="bibr" target="#b2">Blei et al, 2003</ref>) has been widely used for discovering latent topics from document collections by capturing words' co- occuring relation. However, the "bag of words" assumption is employed in most existing topic models, it assumes the order of words can be ig- nored and topic assignment of each word is condi- tionally independent given the topic mixture of a document.</p><p>To relax the "bag of words" assumption, many extended topic models have been proposed to ad- dress the limitation of conditional independence. Wallach ( <ref type="bibr" target="#b3">Wallach, 2006</ref>) explores a hierarchical generative probabilistic model that incorporates both n-gram statistics and latent topic variables. Gruber ( <ref type="bibr" target="#b1">Gruber et al, 2007</ref>) models the topics of words in the document as a Markov chain, and as- sumes all words in the same sentence are more likely to have the same topic. Zhu ( <ref type="bibr" target="#b5">Zhu et al, 2010</ref>) incorporates Markov dependency between topic assignments of neighboring words, and em- ploys a general structure of the GLM to define a conditional distribution of latent topic assign- ments over words. Most of the models above are limited to model linear topical dependencies be- tween words, word topical dependencies can also be modeled by a non-linear way. In Syntactic topic models <ref type="bibr" target="#b4">(Boyd-Graber et al, 2009)</ref>, each word of a sentence is generated by a distribution that com- bines document-specific topic weights and parse- tree-specific syntactic transitions.</p><p>In Global Topic Random Field(GTRF) model ( <ref type="bibr" target="#b13">Li et al, 2014</ref>), sentences of a document are parsed into dependency trees (Marneffe et al, 2008) ( <ref type="bibr" target="#b6">Manning et al, 2014</ref>) ( <ref type="bibr" target="#b7">Marneffe et al, 2006</ref>). They show topics of semantically or syntactically dependent words achieve the highest similarity and are able to provide more useful information in topic modeling, which is also the basic assumption of our model. Then they propose GTRF to model non-linear topical dependencies, word topics are sampled based on graph structure instead of "bag of words" representation, the conditional independence of word topic assignment is thus relaxed.</p><p>However, GTRF assumes topic assignment of a word vertex depends on the topic mixture of the document and its neighboring word vertices, ig- noring the fact that word vertex can also be influ- enced by the distance-2 or further word vertices. In this paper, we extend GTRF model and present a novel model Extended Global Topic Random Field (EGTRF) to exploit topical dependency be- tween words. In EGTRF, the topic assignment of a word is assumed to depend on both distance-1 and distance-2 word vertices. An example of a simple document that has two sentences shows in <ref type="figure">Figure  1</ref>. The two sentences are parsed into dependency trees respectively, and then merged into a graph. Some hidden dependency relations can also be ex- tracted by merging dependency trees. For exam- ple, word "allocation" has a new distance-2 word "topics" after merging. Therefore, EGTRF can exploit more semantically or syntactically word dependencies. Theoretically, we can also model the distance further than 2, however, it leads to more complicated computation and small increase of performance.</p><p>Another advantage of EGTRF is it incorporates word features. The word vector representations are very interesting because the learned vectors explicitly encode many linguistic regularities and patterns ). We use the pre- trained model from Google News dataset(about 100 billion words) using word2vec 1 tool to repre- sent each word as a 300-dimensional word vector, and apply normalized word similarity as a con- fidence score to indicate how possible two word vertices share same topic.</p><p>We organized the paper as below: EGTRF is presented in Section 2, variational inference and parameter estimation are derived in Section 3, ex- periments on two datasets are showed in Section 4, we conclude the paper in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Extended Global Topic Random Field</head><p>In this section, we first present Extended Global Random Field(EGRF) in section 2.1, then show how to model topical dependencies using EGRF in section 2.2. We incorporate word similarity in- formation into model in section 2.3.</p><p>1 https://code.google.com/p/word2vec/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Extended Global Random Field</head><p>After representing document to undirected graph on previous section, we extend Global Random Field and give the definition of Extended Global Random Field to model the graph as below:</p><p>Given an undirected graph G, word vertex set is denoted as W = {w i |i = 1, 2, ..n}, where w i is a word vertex, and n is the number of unique words in a document. E 1 is distance-1 edge set, E 1 = {(w i , w j )|∃path between w i , w j that length is 1}. E 2 is distance-2 edge set, E 2 = {(w i , w j )|∃path between w i , w j that length is 2}. The state(topic assignment) of a word vertex w is generated from Z = {z i |i = 1, 2, ..., k}, k is the number of topics.</p><formula xml:id="formula_0">P (G) = f G (g) = 1 | E1 | + | E2 | w∈W f (zw)× ( (w 1 ,w 1 )∈E 1 f (1) (z w 1 , z w 1 ) + (w 2 ,w 2 )∈E 2 f (2) (z w 2 , z w 2 ))<label>(1)</label></formula><formula xml:id="formula_1">s.t. 1.f (z) &gt; 0, f (1) (z , z ) &gt; 0, f (2) (z , z ) &gt; 0 2. z∈Z f (z) = 1 3. z ,z ∈Z f (z )f (z )f (1) (z , z ) = 1 4. z ,z ∈Z f (z )f (z )f (2) (z , z ) = 1</formula><p>In Equation (1), f (z) is the function defined on word vertex, which is a probability measure be- cause of the constraints 1 and 2. f (1) (z, z ) and f (2) (z, z ) are the function defined on edge set E 1 and E 2 . f (1) and f <ref type="bibr">(2)</ref> are not necessarily probabil- ity measure, however, summing over all possible states of the product of the edge and the linked word pair should equal to 1, which are from con- straints 3 and 4. So f (z )f (z )f (1) (z , z ) and f (z )f (z )f (2) (z , z ) are probability measure. g is one sample of word topic assignments from graph G. If Equation (1) satisfies all the four con- straints, it is easy to verify P (G) is also a prob- ability measure since summing over all possible samples g equals to 1.</p><p>We define the random field as in Equation (1) a Extended Global Random Field (EGRF). And EGRF does not have normalization factor, which is much simplier than models with intractable nor- malizing factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Topic Model Using EGRF</head><p>We define Extended Global Topic Random Field based on EGRF. EGTRF is a generative proba-bilistic model, the basic idea is that documents are represented as mixtures of topics, words are generated depending on the topic mixtures and graph structure of current document. The genera- tive process for word sequence of a document is described as below:</p><p>For each document d in corpus D:</p><p>Transform document d into graph. Choose θ ∼ Dir(α).</p><p>For each of the n words w n in d:</p><formula xml:id="formula_2">Choose topic z n ∼ P egrf (z | θ), Choose word w n ∼ M ulti(β zn,wn ).</formula><p>Given Dirichlet prior α, word distribution of topics β, topic mixture of document θ, topic assignments z and words w. We obtain the marginal distribu- tion of a document:</p><formula xml:id="formula_3">p(w | α, β) = P (θ | α) z P egrf (z | θ) n P (wn | zw n , β)dθ<label>(2)</label></formula><p>We can see the marginal distribution is similar to LDA except topic assignment of word is sam- pled by Extended Global Random Field instead of Multinomial. So the word topic assignment is no longer conditionally independent. According to EGRF described in section 2.1, we define the probability of topic sequence z as below:</p><formula xml:id="formula_4">P egrf (z | θ) = 1 | E1 | + | E2 | w∈V f (zw)× ( (w 1 ,w 1 )∈E 1 f (1) (z w 1 , z w 1 ) + (w 2 ,w 2 )∈E 2 f (2) (z w 2 , z w 2 ))<label>(3)</label></formula><p>where</p><formula xml:id="formula_5">f (zw) = M ulti(zw|θ)<label>(4)</label></formula><formula xml:id="formula_6">f (1) (z w 1 , z w 1 ) = σz w 1 =z w 1 λ1 + σ z w 1 =z w 1 λ2 (5) f (2) (z w 2 , z w 2 ) = σz w 2 =z w 2 λ3 + σ z w 2 =z w 2 λ4 (6)</formula><p>σ is an indicator function and equals 1 if the topic assignments of two words on an edge are same. In order to model Equation (3) as an EGRF, it must satisfy all the four constraints in Equation (1). Equation (4) defines word vertex as multino- mial distribution, and we assign λ 1 , λ 2 , λ 3 and λ 4 nonzero values, then it is clear to verify constraint 1 and 2 are satisfied. To satisfy the constraint 3 and 4, combine with (5), (6), we get the relation between λ 1 and λ 2 , λ 3 and λ 4 .</p><formula xml:id="formula_7">θ 2 i λ1 + (1 − θ 2 i )λ2 = 1 i = 1, 2, ..|E1| (7) θ 2 i λ3 + (1 − θ 2 i )λ4 = 1 i = 1, 2, ..|E2|<label>(8)</label></formula><p>Lower λ 2 , λ 4 give higher reward to the edge that connects two word vertices with same topic. If <ref type="formula">(7)</ref> and <ref type="formula" target="#formula_7">(8)</ref> hold true, Equation <ref type="formula" target="#formula_4">(3)</ref> is an EGRF. And we define the topic model based on EGRF as Extended Global Topic Random Field(EGTRF). If |E 2 | = 0, |E 1 | = 0, EGTRF is equivalent to GTRF. If |E 1 | = 0, |E 2 | = 0, EGTRF is equiva- lent to LDA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Word Similarity Information</head><p>The coherent edge is the edge that the two linked words have same topic. In distance-i edge set, i= 1, 2. E C i includes all coherent edges, E N C i contains all non-coherent edges. Then equation <ref type="formula" target="#formula_4">(3)</ref> can be represented as below:</p><formula xml:id="formula_8">P egrf (z | θ) = 1 | E1 | + | E2 | w∈V M ulti(zw | θ)× (| E C 1 | λ1+ | E N C 1 | λ2+ | E C 2 | λ3+ | E N C 2 | λ4) = w∈V M ulti(zw | θ) (| E1 | + | E2 |)θ T θ × (| E C 1 | (1 − λ2)+ | E1 | λ2θ T θ+ | E C 2 | (1 − λ4)+ | E2 | λ4θ T θ)<label>(9)</label></formula><p>From the second line to the third line of Equa- tion (9), we represent λ 1 , λ 3 as the function of λ 2 , λ 4 based on (7) and <ref type="bibr">(8)</ref>. The expectation of the number of edges in E c i can be computed as:</p><formula xml:id="formula_9">E(| E C i |) = (w 1 ,w 2 )∈E i φ T w 1 φw 2 Sw 1 ,w 2 (10)</formula><p>φ is the K dimensional variational multinomial parameters and can be thought as the posterior probability of a word given the topic assignment. S w 1 ,w 2 is the similarity measure between word w 1 and w 2 .</p><p>As we discussed in section 1, word similarity information S w 1 ,w 2 works as a confidence score to model how likely two words on an edge have same topic. And we make assumption that two words are more likely to have same topic if they have a higher similarity score. To get the similarity score between words, we use word2vec tool to learn the word representation of each word from pre-trained model. The word representations are computed using neural networks, and the learned representa- tions explicitly encode many linguistic regularities and patterns from the corpus. Normalized similar- ity between word vectors can be regarded as the confidence score of how possible two words have same topic. In this way, knowledge from large cor- pus other than current document collections is in- corporated to guide topic modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Posterior Inference and Parameter Estimation</head><p>We derive Variational Inference for posterior in- ference. The variational function q is same to the original LDA paper <ref type="figure" target="#fig_1">(Blei et al, 2003</ref>). All terms except P (z|θ) in likelihood function are also same to LDA, Based on Equation (9), we obtain:</p><formula xml:id="formula_10">Eq[log P egrf (z | θ)] ≈Eq[log( n M ulti(zw n | θ))]+ 1 − λ2 ζ1 Eq(| E C 1 |) + 1 − λ4 ζ1 Eq(| E C 2 |)+ ( | E1 | λ2+ | E2 | λ4 ζ1 − | E1 | + | E2 | ζ2 )Eq(θ T θ)+ log ζ1 − log ζ2 (11)</formula><p>We get the approximation in Equation (11) from Taylor series, where ζ 1 and ζ 2 are Taylor approximation. E q (| E C i |) is obtained directly from (10), E q (θ T θ) is from the property of Dirich- let distribution. The updating rule of α and β are same to LDA, γ is updated using Newton method since we can not obtain the direct updating rule for γ. φ can be approximated as:</p><formula xml:id="formula_11">φw n,i ∝ βi,vexp(Ψ(γi)+ 1 − λ2 ζ1 × (wn,wm)∈E 1 φw m,i Sm,n+ 1 − λ4 ζ1 × (wn,wp)∈E 2 φw p,i Sp,n) (12)</formula><p>EM algorithm is applied using above updating rules. At E-step, we estimate the best γ and φ given current α and β. At M-step, we update new α and β based on obtained γ and φ. We run such iterations until convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>In this section we study the empirical performance of EGTRF on two datasets. For each dataset, we remove very short documents, and compute a vo- cabulary by removing stop words, rare words, fre- quent words. Eighty percent data are used for training, others for testing.</p><p>• 20 News Groups: After processing, it con- tains 13706 documents with a vocabulary of 5164 terms.</p><p>• NIPS data ( <ref type="bibr" target="#b0">Globerson et al, 2004</ref>): Span- ning from 2000 to 2005. After processing, it contains 843 documents with a vocabulary of 6098 terms.</p><p>We evaluate how well a model fits the data with held-out perplexity ( <ref type="bibr" target="#b2">Blei et al, 2003</ref>) and predic- tive distribution <ref type="figure" target="#fig_1">(Hoffman et al, 2013</ref>). Lower perplexity, higher log predictive probability indi- cate better generalization performance. We im- plement GTRF without adding self defined edges from the original paper, and set λ 2 = 0.2 to give higher reward to edges from E 1 that the two word vertices have same topic. We set λ 4 = 1.2 to give lower(even negative) reward to edges from E 2 that the two word vertices have same topic in EGTRF, since the distance-1 words are expected to have greater topical affects than distance-2 words. Word is represented as vector from pre- trained Google News dataset, we use the word vec- tor learned from original corpus when the word does not exist in pre-trained Google News dataset.</p><p>We choose 10, 20, 30, 50 topics for 20 news dataset, 10, 15, 20, 25 topics for NIPS dataset. <ref type="figure" target="#fig_1">Figure 2</ref> shows the experimental results of four models: lda, gtrf, egtrf(EGTRF without word similarity information), and egtrf+s(EGTRF with word similarity information) on two datasets. The results show EGTRF outperforms LDA and GTRF in general, and EGTRF with word similarity infor- mation achieves best performance.</p><p>We believe modeling distance-2 word vertices can exploit more semantically or syntactically word dependencies from document, and word sim- ilarity information obtained from large corpus can make up the lack of sufficient information from the original corpus. Therefore, adding the influence of distance-2 word vertices and word similarity infor- mation can improve performance of topic model- ing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we extended Global Topic Random Field(GTRF) and proposed a novel topic model Extended Global Topic Random Field(EGTRF) which can model dependency relation between adjacent words and distance-2 words. Word topics are drawed by Extended Global Random Field(EGRF) instead of Multinomial, the con- ditional independence of word topic assignment is thus relaxed. Word similarity information learned from large corpus is incorporated into the model. Experiments on two datasets show EGTRF achieves better performance than GTRF and LDA, which confirm our assumption that adding topical dependency of distance-2 words and incorporating word similarity information can improve model performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1: Dependency tree example</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Experimental results on NIPS(left) and 20 news(right) data</figDesc><graphic url="image-1.png" coords="4,72.00,62.81,226.77,161.72" type="bitmap" /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Naftali Tishby Euclidean Embedding of Co-occurrence Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Amir Globerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="497" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hidden Topic Markov Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Gruber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Rosen-Zvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistic</title>
		<meeting>the Eleventh International Conference on Artificial Intelligence and Statistic</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="163" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent Dirichlet Allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Topic modeling: Beyond bag-ofwords</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hanna M Wallach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="977" to="984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Syntactic topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Graber</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="185" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Conditional Topic Random Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning</title>
		<meeting>the 27th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP Natural Language Processing Toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generating typed dependency parses from phrase structure parses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="449" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The Stanford typed dependencies representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2008 Workshop on Crossframework and Cross-domain Parser Evaluation</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">John Paisley Stochastic Variational Inference The Journal of</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1303" to="1347" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop at ICLR</title>
		<meeting>Workshop at ICLR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Linguistic Regularities in Continuous Space Word Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL HLT</title>
		<meeting>NAACL HLT</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On Modeling Non-linear Topical Dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqiang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th International Conference on Machine Learning</title>
		<meeting>the 31th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="458" to="466" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
