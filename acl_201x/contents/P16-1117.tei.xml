<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:03+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Network-Based Model for Japanese Predicate Argument Structure Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>August 7-12, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomohide</forename><surname>Shibata</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Informatics</orgName>
								<orgName type="institution">Kyoto University Yoshida-honmachi</orgName>
								<address>
									<addrLine>Sakyo-ku</addrLine>
									<postCode>606-8501</postCode>
									<settlement>Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Informatics</orgName>
								<orgName type="institution">Kyoto University Yoshida-honmachi</orgName>
								<address>
									<addrLine>Sakyo-ku</addrLine>
									<postCode>606-8501</postCode>
									<settlement>Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Informatics</orgName>
								<orgName type="institution">Kyoto University Yoshida-honmachi</orgName>
								<address>
									<addrLine>Sakyo-ku</addrLine>
									<postCode>606-8501</postCode>
									<settlement>Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Network-Based Model for Japanese Predicate Argument Structure Analysis</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1235" to="1244"/>
							<date type="published">August 7-12, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper presents a novel model for Japanese predicate argument structure (PAS) analysis based on a neural network framework. Japanese PAS analysis is challenging due to the tangled characteristics of the Japanese language, such as case disappearance and argument omission. To unravel this problem, we learn selectional preferences from a large raw corpus, and incorporate them into a SOTA PAS analysis model, which considers the consistency of all PASs in a given sentence. We demonstrate that the proposed PAS analysis model significantly outperforms the base SOTA system.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Research on predicate argument structure (PAS) analysis has been conducted actively these days. The improvement of PAS analysis would benefit many natural language processing (NLP) applica- tions, such as information extraction, summariza- tion, and machine translation.</p><p>The target of this work is Japanese PAS analy- sis. The Japanese language has the following char- acteristics:</p><p>• head final,</p><p>• free word order (among arguments), and</p><p>• postpositions function as (surface) case markers.</p><p>Japanese major surface cases are (ga), (wo), and (ni), which correspond to Japanese post- positions (case markers). We call them nomina- tive case, accusative case, and dative case, respec- tively. In this paper, we limit our target cases to these three cases. Note that though they are sur- face cases, they roughly correspond to Arg1, Arg2, and Arg3 of English semantic role labeling based on PropBank. Japanese PAS analysis has been considered as one of the most difficult basic NLP tasks, due to the following two phenomena.</p><p>Case disappearance When a topic marker (wa) is used or a noun is modified by a relative clause, their case markings disappear as in the fol- lowing examples. <ref type="bibr">1</ref> (1) a. In the example sentences (1a) and (1b), since a topic marker is used, the NOM and ACC case markers disappear. In the example sentences (2a) and (2b), since a noun is modified by a relative clause, the NOM case of "" (John) for " " (eat) and ACC case of "" (bread) for "" disappear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>John-TOP bread-ACC ate</head><p>Argument omission Arguments are very often omitted in Japanese sentences. This phenomenon is totally different from English sentences, where the word order is fixed and pronouns are used con-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>!"#$%&amp;&amp;'#$(&amp;&amp;)*+,&amp;&amp;&amp;-./0! 1 "#$%&amp;'()1*+,-.&amp;/001 *#12$3&amp;-%.1 -3,1</head><p>.,4,%.,%56!4-+78%21 5-7,!-%-967871 :(;!! &lt;,+#!-%-4$#+-! +,7#91=#%1</p><p>&gt;&amp;:(;1 &gt;&amp;/001</p><p>Figure 1: An example of PAS analysis. Input sen- tence: "" (John bought bread, and ate it.)</p><p>sistently. For example, let us compare the follow- ing parallel Japanese and English sentences:</p><formula xml:id="formula_0">(3) a.</formula><p>John-TOP bread-ACC bought ate b. John bought bread, and ate it.</p><p>The dependency parse of (3a) is shown in <ref type="figure">Figure  1</ref>. In general, the first phrase with a topic marker is treated as modifying the final predicate ac- cording to the guidelines of Japanese dependency annotation. As a result, "" (bought) has no NOM argument (omitted), and "" (ate) has no ACC argument. Note that "" has an ar- gument "" (John), but its case does not ap- pear.</p><p>In the case of the parallel sentences (4) below, again we can witness the difficulty of Japanese PAS analysis. Although all the case arguments of the predicates "bought" and "ate" are explicit in (4b), the case of "" (John) for "" (bought) and that for "" (ate) are hidden, and the ACC argument of "" (ate) is omitted in (4a).</p><p>Many researchers have been tackling Japanese PAS analysis ( <ref type="bibr" target="#b15">Taira et al., 2008;</ref><ref type="bibr" target="#b9">Imamura et al., 2009;</ref><ref type="bibr" target="#b7">Hayashibe et al., 2011;</ref><ref type="bibr" target="#b14">Sasano and Kurohashi, 2011;</ref><ref type="bibr" target="#b5">Hangyo et al., 2013;</ref><ref type="bibr" target="#b13">Ouchi et al., 2015)</ref>. However, because of the two aforemen- tioned characteristics in Japanese sentences, the accuracy of Japanese PAS analysis for omitted (zero) arguments remains around 40%.</p><p>This paper proposes a novel Japanese PAS anal- ysis model based on a neural network (NN) frame- work, which has been proved to be effective for several NLP tasks recently. To unravel the tan- gled situation in Japanese, we learn selectional preferences from a large raw corpus, and incorpo- rate them into a SOTA PAS analysis model pro- posed by <ref type="bibr" target="#b13">Ouchi et al. (2015)</ref>, which considers the consistency of all PASs in a given sentence. This model is achieved by an NN-based two-stage model that acquires selectional preferences in an unsupervised manner in the first stage and predicts PASs in a supervised manner in the second stage as follows.</p><p>1. The most important clue for PAS analysis is selectional preferences, that is, argument pre- diction from a predicate phrase. For exam- ple, how likely the phrase "" (bought bread) takes "" (John) as its NOM argument.</p><p>Such information cannot be learned from a medium-sized PAS annotated corpus with size of the order of ten-thousand sentences; it is necessary to use a huge raw corpus by an unsupervised method. <ref type="bibr" target="#b13">Ouchi et al. (2015)</ref> did not utilize such knowledge extracted from a raw corpus. Some work has utilized PMI be- tween a predicate and an argument, or case frames obtained from a raw corpus. How- ever, this is discrete word-based knowledge, not generalized semantic knowledge.</p><p>As the first stage of the method, we learn a prediction score from a predicate phrase to an argument by an NN-based method. The resultant vector representations of predicates and arguments are used as initial vectors for the second stage of the method.</p><p>2. In the second stage, we calculate a score that a predicate in a given sentence takes an el- ement in the sentence as an argument using NN framework. We use the prediction score in the first stage as one feature for the second stage NN. The system by <ref type="bibr" target="#b13">Ouchi et al. (2015)</ref> used a manually designed feature template to take the interactions of the atomic features into consideration. In the case of an NN framework, no feature template is required, and a hidden layer in an NN can capture the interactions of the atomic features automati- cally and flexibly.</p><p>We demonstrate that the proposed PAS analysis model outperforms the SOTA system by <ref type="bibr" target="#b13">Ouchi et al. (2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Several methods for Japanese PAS analysis have been proposed. The methods can be divided into three types: (i) identifying one case argu- ment independently per predicate ( <ref type="bibr" target="#b15">Taira et al., 2008;</ref><ref type="bibr" target="#b9">Imamura et al., 2009;</ref><ref type="bibr" target="#b7">Hayashibe et al., 2011</ref>), (ii) identifying all the three case arguments (NOM, ACC, and DAT) simultaneously per pred- icate ( <ref type="bibr" target="#b14">Sasano and Kurohashi, 2011;</ref><ref type="bibr" target="#b5">Hangyo et al., 2013)</ref>, and (iii) identifying all case arguments of all predicates in a sentence ( <ref type="bibr" target="#b13">Ouchi et al., 2015)</ref>. The third method can capture interactions between predicates and their arguments, and thus performs the best among the three types. This method is adopted as our base model (see Section 3 for de- tails).</p><p>Most methods for PAS analysis handle both intra-sentential and inter-sentential zero anaphora. For identifying inter-sentential zero anaphora, an antecedent has to be searched in a broad search space, and the salience of discourse entities has to be captured. Therefore, the task of identify- ing inter-sentential zero anaphora is more difficult than that of intra-sentential zero anaphora. Thus, <ref type="bibr" target="#b13">Ouchi et al. (2015)</ref> and <ref type="bibr" target="#b8">Iida et al. (2015)</ref> focused on only intra-sentential zero anaphora. Following this trend, this paper focuses on intra-sentential zero anaphora.</p><p>Recently, NN-based approaches have achieved improvement for several NLP tasks. For exam- ple, in transition-based parsing, <ref type="bibr" target="#b1">Chen and Manning (2014)</ref> proposed an NN-based approach, where the words, POS tags, and dependency la- bels are first represented by embeddings individu- ally. Then, an NN-based classifier is built to make parsing decisions, where an input layer is a con- catenation of embeddings of words, POS tags, and dependency labels. This model has been extended by several studies <ref type="bibr" target="#b18">(Weiss et al., 2015;</ref>. In semantic role la- beling, <ref type="bibr" target="#b20">Zhou and Xu (2015)</ref> propose an end-to-end approach using recurrent NN, where an original text is the input, and semantic role labeling is per- formed without any intermediate syntactic knowl- edge. Following these approaches, this paper pro- poses an NN-based PAS method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Base Model</head><p>The model proposed by <ref type="bibr" target="#b13">Ouchi et al. (2015)</ref> is adopted as our base model ( <ref type="figure">Figure 2</ref>). We briefly introduce this base model before describing our</p><formula xml:id="formula_1">!"#$%&amp;&amp;'#$(&amp;&amp;&amp;)*+,&amp;&amp;&amp;-./0! 1 "#$%&amp;1 !'()#*+,-1 ./012*3441!.(%5)&amp;*1#21 !1&amp;01 !"#! '()#1 !"##! )2! .%61 -.3! 01&amp;! '#! ./0121 )2! .%6! -.3! 01&amp;1 73+1 a 1 a 2 a 3 a 4 a 5 p 1 p 2 344!1 8,91 344!1 73+1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8,91</head><p>Figure 2: Our base model ( <ref type="bibr" target="#b13">Ouchi et al., 2015</ref>).</p><p>proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Predicate-Argument Graph</head><p>In this model, for an input sentence, a bipar- tite graph is constructed, consisting of the set of predicate and argument nodes. This is called Predicate-Argument Graph (PA Graph). A PA graph represents a possible interpretation of the input sentence, including case analysis result and zero anaphora resolution result. A PA graph is a bipartite graph ⟨A, P, E⟩, where A is the node set consisting of candidate arguments, P is the node set consisting of predi- cates, and E is the set of edges. A PA graph is defined as follows:</p><formula xml:id="formula_2">A = {a 1 , . . . , a n , a n+1 = NULL} P = {p 1 , . . . , p m } E = {⟨a, p, c⟩|deg(p, c) = 1, ∀a ∈ A, ∀p ∈ P, ∀c ∈ C}</formula><p>where n and m represent the number of predicates and arguments, and C denotes the case role set (NOM, ACC, and DAT). An edge e ∈ E is rep- resented by a tuple ⟨a, p, c⟩, indicating the edge with a case role c connecting a candidate argu- ment node a and a predicate node p. deg(p, c) is the number of the edges with a case role c outgo- ing from a predicate node p. An admissible PA graph satisfies the constraint deg(p, c) = 1, which means each predicate node p has only one edge with a case role c. A dummy node a n+1 is added, which is defined for the cases where a predicate requires no case argument (e.g. when the pred- icate node "" (exist) connects a NULL node with a case ACC, this means this predicate takes no ACC argument) or the required argument does not appear in the sentence.</p><p>In the bipartite graph shown in <ref type="figure">Figure 2</ref>, the three kinds of edge lines have the meaning as fol- lows:</p><p>solid line: the argument node and the predicate node has a dependency relation, and the ar- gument node is followed by a case mark- ing postposition. In this case, these nodes have a relation through its corresponding case marking postposition. Therefore, this edge is fixed.</p><p>dashed line: the argument node and the predicate node has a dependency relation, and the ar- gument node is not followed by a case mark- ing postposition. These nodes are likely to have a relation 2 , but the case role is unknown. Identifying this case role corresponds to case analysis.</p><p>dotted line: the argument node and the predi- cate node do not have a dependency relation. Identifying this edge and its case role corre- sponds to zero anaphora resolution.</p><p>For an input sentence x, a scoring function Score(x, y) is defined for a candidate graph y, and the PA graph that has the maximum score is searched.</p><formula xml:id="formula_3">˜ y = argmax y∈G(x)</formula><p>Score(x, y)</p><p>where G(x) is a set of admissible PA graphs for the input sentence x. Score(x, y) is defined as fol- lows <ref type="bibr">3</ref> :</p><formula xml:id="formula_5">∑ e∈E(y) score l (x, e)+ ∑ e i ,e j ∈E pair (y) score g (x, e i , e j ). (2) score l (x, e) = θ l · ϕ l (x, e) score g (x, e i , e j ) = θ g · ϕ g (x, e i , e j )<label>(3)</label></formula><p>where E(y) is the edge set on the candidate graph y, E pair (y) is a set of edge pairs in the edge set E(y), score l (x, e) and score g (x, e i , e j ) represent a local score for the edge e and a global score for the edge pair e i and e j , ϕ l (x, e) and ϕ g (x, e i , e j ) represent local features and global features. While ϕ l (x, e) is defined for each edge e, ϕ g (x, e i , e j ) is defined for each edge pair e i , e j (i ̸ = j) . θ l and θ g represent model parameters for local and global features. By using global scores, the inter- action between multiple case assignments of mul- tiple predicates can be considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Inference and Training</head><p>Since global features make the inference of find- ing the maximum scoring PA graph more difficult, the randomized hill-climbing algorithm proposed in ( <ref type="bibr" target="#b19">Zhang et al., 2014</ref>) is adopted. <ref type="figure">Figure 3</ref> describes the pseudo code for hill- climbing algorithm. First, an initial PA graph y (0) is sampled from the set of admissible PA graph G(x). Then, the union Y is constructed from the set of neighboring graphs N eighborG(y (t) ), which is a set of admissible graphs obtained by changing one edge in y (t) , and the current graph y (t) . The current graph y (t) is updated to a higher scoring graph y (t+1) . This process continues until no more improvement is possible, and finally an optimal graph˜ygraph˜ graph˜y can be obtained.</p><p>Input: sentence x, parameter θ Output: a locally optimal PA graph˜ygraph˜ graph˜y</p><formula xml:id="formula_6">1 Sample a PA graph y (0) from G(x) 2 t ← 0 3 repeat 4 Y ← N eighborG(y (t) ) ∪ y (t) 5 y (t+1) ← argmax y∈Y</formula><p>Score(x, y; θ)</p><formula xml:id="formula_7">6 t ← t + 1 7 until y (t) = y (t+1) 8 returñ y ← y (t)</formula><p>Figure 3: Hill climbing algorithm for obtain- ing optimal PA graph.</p><p>Given N training examples D = {(x, ˆ y)} N k , the model parameter θ are estimated. θ is the set of θ l and θ g , and is estimated by averaged per- ceptron <ref type="bibr" target="#b2">(Collins, 2002</ref>) with a max-margin frame- work ( <ref type="bibr" target="#b16">Taskar et al., 2005</ref>). <ref type="figure">Figure 4</ref>: Argument prediction model. In the PAS "" (police) NOM "" (suspect) ACC " " (arrest), "" with the NOM case is pre- dicted given the predicate "" (arrest) and its ACC "" (suspect).</p><formula xml:id="formula_8">!"#$! "##$%&amp;% &amp;'! '()*+$% ()! %,%'$+&amp;! -./% 011% *! 2$%3% +,-! "'')$% 4$5"67$! %"8')$%% ...% '#$2*+&amp;%</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Proposed Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Argument Prediction Model</head><p>No external knowledge is utilized in the base model. One of the most important types of knowl- edge in PAS analysis is selectional preferences. Sasano and Kurohashi (2011) and <ref type="bibr" target="#b5">Hangyo et al. (2013)</ref> extract knowledge of the selectional pref- erences in the form of case frames from a raw cor- pus, and the selectional preference score is used as a feature. In this work, argument prediction model is trained using a neural network from a raw cor- pus, in a similar way to <ref type="bibr" target="#b17">Titov and Khoddam (2015)</ref> and <ref type="bibr" target="#b6">Hashimoto et al. (2014)</ref>. PASs are first extracted from an automatically- parsed raw corpus, and in each PAS, the argu- ment a i is generated with the following probability p(a i |P AS −a i ):</p><formula xml:id="formula_9">p(ai|P AS−a i ) = exp(v T a i W T a i (W pred v pred + ∑ j̸ =i Wa j va j )) Z<label>(4)</label></formula><p>where P AS −a i represents a PAS excluding the target argument a i , v pred , v a i and v a j represent embeddings of the predicate, argument a i and ar- gument a j , and W pred , W a i , and W a j represent transformation matrices for a predicate and an ar- gument a i and a j . Z is the partition function. <ref type="figure">Figure 4</ref> illustrates the argument prediction model. The PAS "" (police) NOM "" (suspect) ACC "" (arrest)" is extracted from a raw corpus, and the probability of NOM argument "" given the predicate "" and its ACC ar- gument "" is calculated.</p><p>All the parameters including predi- cate/argument embeddings and transformation matrices are trained, so that the likelihood given by Equation <ref type="formula" target="#formula_9">(4)</ref> is high. Since the denominator of Equation <ref type="formula" target="#formula_9">(4)</ref> is impractical to be calculated since the number of vocabulary is enormous, negative sampling ( <ref type="bibr" target="#b12">Mikolov et al., 2013</ref>) is adopted. In the example shown in <ref type="figure">Figure 4</ref>, as for a NOM argument, negative examples, such as "" (desk) and "" (apple), are drawn from the noise distribution, which is a unigram distribution raised to the 3/4th power.</p><p>In each PAS, all the arguments are predicted in turn. All the parameters are updated using stochastic gradient descent.</p><p>This model is first trained using the automatic parsing result on a raw corpus, and in performing PAS analysis described in Section 4.2, the score derived from this model is used as a feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Neural Network-Based Score Calculation</head><p>In the base model, the score for an edge (local score) or an edge pair (global score) is calculated using the dot product of a sparse high-dimensional feature vector with a model parameter, as shown in Equation (3). In our proposed model, these scores are calculated in a standard neural network with one hidden layer, as shown in <ref type="figure">Figure 5</ref>.</p><p>We first describe the calculation of the local score score l (x, e). A predicate p and an argument a are represented by embeddings (a d dimensional vector) v p and v a ∈ R d , and v f l ∈ R d f (d f rep- resents a dimensional of v f l ) represents a feature vector obtained by concatenating the case role be- tween a and p, the argument prediction score ob- tained from the model described in Section 4.1, and the other atomic features. An input layer is a concatenation of these vectors, and then, a hidden layer h l ∈ R d h (d h represents a dimension of the hidden layer) is calculated as follows:</p><formula xml:id="formula_10">h l = f (W 1 l [v p ; v a ; v f l ]) (5)</formula><p>where f is an element-wise activation function (tanh is used in our experiments), and</p><formula xml:id="formula_11">W 1 l ∈ R d h (2d+d h )</formula><p>is a weight matrix (for the local score) from the input layer to the hidden layer. The scalar score in an output layer is then calculated as fol- lows:</p><formula xml:id="formula_12">score l (x, e) = f (W 2 l h l )<label>(6)</label></formula><p>where W 2 l ∈ R (2d+d h )·1 is a weight matrix (for the local score) from the hidden layer to the output layer. By calculating the score in this way, all the <ref type="figure">Figure 5</ref>: A score calculation in our proposed neural-network based model. The left part and right part represent a local and global score calculation.</p><formula xml:id="formula_13">!"#$%#&amp;'#$! (")%#&amp;'#$! *+,(*%-,+"#! )*+'(*%-,+"#! W 1 l W 2 l score l (x, e) !"#$%#&amp;'#$! (")%#&amp;'#$! score g (x, e i , e j ) W 1 g W 2 g ,(-#! ,(-#.! ,(-#/! (")0%!"#$%% -,+"#! +12#"% 3#(14"#-! +12#"% 3#(14"#-! v p v a v f l v p i v p j v a i v a j v f g</formula><p>combinations of features in the input layer can be considered.</p><p>Next we describe the calculation of the global score score g (x, e i , e j ). In the base model, the two types of global features are utilized: one is for the two predicates having different arguments, and the other is for the two predicates sharing the same argument. The input layer is a concatenation of involving vectors of predicates/arguments and the other features v fg . For example, when calculat- ing the global score for the two predicates having different arguments, the input layer is a concate- nation of the vectors of two predicates and two ar- guments and v fg .</p><p>A hidden layer h g is calculated as follows:</p><formula xml:id="formula_14">h g = f (W 1 g [v p i ; v p j ; v a i ; v a j ; v fg ])<label>(7)</label></formula><p>where W 1 g is a weight matrix (for the global score) from the input layer to the hidden layer, v p i and v a i are the embeddings of the predicate/argument connected by e i , and v p j and v a j are defined in the same way.</p><p>The scalar score in an output layer is then cal- culated as follows:</p><formula xml:id="formula_15">score g (x, e i , e j ) = f (W 2 g h g )<label>(8)</label></formula><p>where W 2 g is a weight matrix (for the global score) from the hidden layer to the output layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Inference and Training</head><p>While inference is the same as the base model, training is slightly different.</p><p>In our proposed model, the model param- eter θ consists of the embeddings of predi- cates/arguments and weight matrices for the lo- cal/global score in the neural networks. Our ob- jective is to minimize the following loss function: </p><formula xml:id="formula_16">J(θ) = N ∑ k l k (θ),<label>(9)</label></formula><p>where</p><formula xml:id="formula_17">l k (θ) = max y k ∈G(x) (Score(x k , y k ; θ)−Score(x k , ˆ y k ; θ) + ||y k − ˆ y k ||1),<label>(10)</label></formula><p>and ||y k − ˆ y k || 1 denotes the Hamming distance be- tween the gold PA graphˆygraphˆ graphˆy k and a candidate PA graph y k .</p><p>Stochastic gradient descent is used for param- eter inference. Derivatives with respect to pa- rameters are taken using backpropagation. Adam ( <ref type="bibr" target="#b11">Kingma and Ba, 2014</ref>) is adopted as the opti- mizer.</p><p>For initialization of the embeddings of a pred- icate/argument, the embeddings of the predi- cate/argument trained by the method described in Section 4.1 are utilized. The weight matrices are randomly initialized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setting</head><p>The KWDLC (Kyoto University Web Document Leads Corpus) evaluation set ( <ref type="bibr" target="#b4">Hangyo et al., 2012)</ref> was used for our experiments, because it contains a wide variety of Web documents, such as news articles and blogs. This evaluation set consists of the first three sentences of 5,000 Web documents. Morphology, named entities, dependencies, PASs, and coreferences were manually annotated.</p><p>This evaluation set was divided into 3,694 docu- ments (11,558 sents.) for training, 512 documents (1,585 sents.) for development, and 700 docu- ments (2,195 sents.) for testing. <ref type="table">Table 1</ref> shows the statistics of the number of arguments in the test set. While "dep argument" means that the argu- ment and a predicate have a dependency relation, but a specified case marking postposition is hid- den (corresponds to "dashed line" in Section 3.1), "zero argument" means that the argument and a predicate do not have a dependency relation (cor- responds to "dotted line" in Section 3.1).</p><p>Since we want to focus on the accuracy of case analysis and zero anaphora resolution, gold morphological analysis, dependency analysis, and named entities were used.</p><p>The sentences having a predicate that takes mul- tiple arguments in the same case role were ex- cluded from training and test examples, since the base model cannot handle this phenomena (it as- sumes that each predicate has only one argument with one case role). For example, the following sentence, (5) such funny-material full daily life-ACC picture-with , report (I report my daily life full of such funny ma- terials along with pictures.) where the predicate "" (report) takes both "" (daily life) and "" (picture) as ACC case arguments, was excluded from training and testing. About 200 sentences (corresponding to about 1.5% of the whole evaluation set) were ex- cluded.</p><p>In this evaluation set, zero exophora, which is a phenomenon that a referent does not appear in a document, is annotated. Among five types of zero exophora, the two major types, "author" and "reader," are adopted, and the others are discarded. To consider "author" and "reader" as a referent, the two special nodes, AUTHOR and READER, are added as well as a NULL node in a PA graph of the base model. When the argument predication score is calculated for "author" or "reader," be- cause its lemma does not appear in a document, for each noun in the following noun list of "au- thor"/"reader" ( <ref type="bibr" target="#b5">Hangyo et al., 2013)</ref>, the argument prediction score is calculated, and the maximum score is used as a feature.</p><p>• author: "" (I), "" (we), "" (I), " " (our company), · · ·</p><p>• reader: "" (you), "" (customer), " " (you), ""(you all), · · ·</p><p>In the argument prediction model training de- scribed in Section 4.1, a Japanese Web corpus consisting of 10M sentences was used. We pre- formed syntactic parsing with a publicly available Japanese parser, KNP 4 . The number of negative samples was 5, and the number of epochs was 10.</p><p>In the model training described in Section 4.3, the dimensions of both embeddings for predi- cates/arguments and hidden layer were set to 100. The number of epochs was set to 20, following the base model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Result</head><p>We compared the following three methods:</p><p>• Baseline ( <ref type="bibr" target="#b13">Ouchi et al., 2015)</ref> • Proposed model w/o arg. prediction score:</p><p>in the PAS analysis model, the feature de- rived from the argument prediction model was not utilized. The embeddings of a predicate/argument were randomly initial- ized. This method corresponds to adopting the NN-based score calculation in the base model.</p><p>• Proposed model w/ arg. prediction score: the feature derived from the argument pre- diction model was utilized, and the embed- dings of a predicate/argument were initial- ized with those obtained in the argument pre- diction model learning.  over 5 runs. <ref type="table" target="#tab_2">Table 2</ref> shows our experimental re- sults. Our proposed method outperformed the baseline method by about 11 absolute points in F-measure. The comparison of "Proposed model w/o arg. prediction score" with the baseline showed that the neural network-based approach was effective, and the comparison of "Proposed model w/ arg. prediction score" with "Proposed model w/o arg. prediction score" showed that our arg. prediction model was also effective. The following is improved by adding an argu- ment prediction score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(6)</head><p>after a long time part-time job begin to work new step-ACC step forward (It's my first part-time job in a long time. I begin to work, and make a new step.)</p><p>While in the base model, the NOM arguments of the predicate "" (begin to work) and "" (step forward) were wrongly classified as NULL, by adding an argument prediction score, they were correctly identified as "author."</p><p>The phenomenon "case disappearance" occurs in other languages such as Korean, and the phe- nomenon "argument omission" occurs in other languages such as Korean, Hindi, Chinese, and Spanish. We believe that our neural network ap- proach to the argument prediction and the calcula- tion of the local and global scores is also effective for such languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Error Analysis</head><p>Errors in our proposed model are listed below:</p><p>• Recall for ACC and DAT in both case analy- sis and zero anaphora resolution is low.</p><p>One reason is that since the number of the ACC and DAT arguments is smaller than that of the NOM argument, the system tends to assign the ACC and DAT arguments with NULL. Another reason is that since this paper focuses on intra-sentential zero anaphora, the NULL arguments include arguments that ap- pear in previous sentences as well as the case where a predicate takes no argument, which makes the training for NULL arguments dif- ficult. We are planing to tackle with inter- sentential zero anaphora resolution.</p><p>• The distinction of "author" from NULL fails.</p><p>(7) meat-ACC roast-only-NOM BBQ BBQ-(COPULA) (Roasting meat isn't all in BBQ!)</p><p>Although the NOM argument of the predi- cate "" (roast) is "author," our proposed model wrongly classified it as NULL. <ref type="bibr" target="#b5">Hangyo et al. (2013)</ref> identify mentions referring to an author or reader in a document, and uti- lize this result in the zero anaphora resolu-tion. We plan to incorporate the author/reader identification into our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper we presented a novel model for Japanese PAS analysis based on neural network framework. We learned selectional preferences from a large raw corpus, and incorporated them into a PAS analysis model, which considers the consistency of all PASs in a given sentence. In our experiments, we demonstrated that the pro- posed PAS analysis model significantly outper- formed the base SOTA model.</p><p>In the future, we plan to extend our model to incorporate coreference resolution and inter- sentential zero anaphora resolution.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>→ bread-ACC ate John-ACC () John-NOM (ate) (John, who ate bread, ...) b. John-NOM ...→ ate bread-NOM () (ate) bread-ACC (Bread, which John ate, ...)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>John who bought bread ate it in a hurry.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Experimental results on the KWDLC corpus.</head><label>2</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> In this paper, we use the following abbreviations: NOM (nominative), ACC (accusative), DAT (dative) and TOP (topic marker).</note>

			<note place="foot" n="2"> For example, in the sentence &quot; &quot; (today-TOP hot), the predicate &quot;&quot; does not take &quot;&quot;, which represents time, as an argument. Therefore, these nodes do not always have a relation. 3 Ouchi et al. (2015) introduce two models: Per-Case Joint Model and All-Cases Joint Model. Since All-Cases Joint Model performed better than Per-Case Joint Model, AllCases Joint Model is adopted as our base model.</note>

			<note place="foot" n="4"> http://nlp.ist.i.kyoto-u.ac.jp/index.php?KNP</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by CREST, Japan Science and Technology Agency.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Improved transition-based parsing by modeling characters instead of words with lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="349" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar, October</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the ACL-02 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Transitionbased dependency parsing with stack long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="334" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Building a diverse document leads corpus annotated with semantic relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masatsugu</forename><surname>Hangyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Pacific Asia Conference on Language, Information, and Computation</title>
		<meeting>the 26th Pacific Asia Conference on Language, Information, and Computation<address><addrLine>Bali,Indonesia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-11" />
			<biblScope unit="page" from="535" to="544" />
		</imprint>
		<respStmt>
			<orgName>Faculty of Computer Science, Universitas Indonesia</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Japanese zero reference resolution considering exophora and author/reader mentions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masatsugu</forename><surname>Hangyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA, October</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="924" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Jointly learning word representations and composition functions using predicate-argument structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="1544" to="1555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Japanese predicate argument structure analysis exploiting argument position and type</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Hayashibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mamoru</forename><surname>Komachi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 5th International Joint Conference on Natural Language Processing</title>
		<meeting>5th International Joint Conference on Natural Language Processing<address><addrLine>Chiang Mai, Thailand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="201" to="209" />
		</imprint>
	</monogr>
	<note>November. Asian Federation of Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Intrasentential zero anaphora resolution using subject sharing recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryu</forename><surname>Iida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Torisawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chikara</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghoon</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Kloetzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="2179" to="2189" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Discriminative approach to predicateargument structure analysis with zero-anaphora resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Imamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniko</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoko</forename><surname>Izumi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-IJCNLP</title>
		<meeting>the ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Conference</forename><surname>Short Papers</surname></persName>
		</author>
		<title level="m">Suntec, Singapore, August. Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="page" from="85" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Joint case argument identification for Japanese predicate argument structure analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroki</forename><surname>Ouchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A discriminative approach to Japanese zero anaphora resolution with large-scale lexicalized case frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryohei</forename><surname>Sasano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 5th International Joint Conference on Natural Language Processing</title>
		<meeting>5th International Joint Conference on Natural Language Processing<address><addrLine>Chiang Mai, Thailand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="758" to="766" />
		</imprint>
	</monogr>
	<note>November. Asian Federation of Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A Japanese predicate argument structure analysis using decision lists</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirotoshi</forename><surname>Taira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanae</forename><surname>Fujita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2008 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="523" to="532" />
		</imprint>
	</monogr>
	<note>Honolulu, Hawaii, October. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning structured prediction models: A large margin approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassil</forename><surname>Chatalbashev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22Nd International Conference on Machine Learning, ICML &apos;05</title>
		<meeting>the 22Nd International Conference on Machine Learning, ICML &apos;05<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="896" to="903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised induction of semantic roles within a reconstructionerror minimization framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Khoddam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Structured training for neural network transition-based parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="323" to="333" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Greed is good if randomized: New inference for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="1013" to="1024" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">End-to-end learning of semantic role labeling using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1127" to="1137" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
