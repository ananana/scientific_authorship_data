<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:27+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Linguistically debatable or just plain wrong?</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
							<email>bplank@cst.dk,dirk@cst.dk,soegaard@hum.ku.dk</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Language Technology</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
								<address>
									<addrLine>Denmark Njalsgade 140</addrLine>
									<postCode>DK-2300</postCode>
									<settlement>Copenhagen S</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Language Technology</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
								<address>
									<addrLine>Denmark Njalsgade 140</addrLine>
									<postCode>DK-2300</postCode>
									<settlement>Copenhagen S</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Language Technology</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
								<address>
									<addrLine>Denmark Njalsgade 140</addrLine>
									<postCode>DK-2300</postCode>
									<settlement>Copenhagen S</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Linguistically debatable or just plain wrong?</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="507" to="511"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In linguistic annotation projects, we typically develop annotation guidelines to minimize disagreement. However, in this position paper we question whether we should actually limit the disagreements between annotators, rather than embracing them. We present an empirical analysis of part-of-speech annotated data sets that suggests that disagreements are systematic across domains and to a certain extend also across languages. This points to an underlying ambiguity rather than random errors. Moreover, a quantitative analysis of tag confusions reveals that the majority of disagreements are due to linguistically debatable cases rather than annotation errors. Specifically, we show that even in the absence of annotation guidelines only 2% of annotator choices are linguistically unmo-tivated.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In NLP, we often model annotation as if it re- flected a single ground truth that was guided by an underlying linguistic theory. If this was true, the specific theory should be learnable from the annotated data. However, it is well known that there are linguistically hard cases <ref type="bibr">(Zeman, 2010)</ref>, where no theory provides a clear answer, so an- notation schemes commit to more or less arbi- trary decisions. For example, in parsing auxil- iary verbs may head main verbs, or vice versa, and in part-of-speech (POS) tagging, possessive pronouns may belong to the category of deter- miners or the category of pronouns. This posi- tion paper argues that annotation projects should embrace these hard cases rather than pretend they can be unambiguously resolved. Instead of using overly specific annotation guidelines, designed to minimize inter-annotator disagreement <ref type="bibr" target="#b3">(Duffield et al., 2007)</ref>, and adjudicating between annotators of different opinions, we should embrace system- atic inter-annotator disagreements. To motivate this, we present an empirical analysis showing 1. that certain inter-annotator disagreements are systematic, and 2. that actual errors are in fact so infrequent as to be negligible, even when linguists annotate without guidelines.</p><p>The empirical analysis presented below relies on text corpora annotated with syntactic cate- gories or parts-of-speech (POS). POS is part of most linguistic theories, but nevertheless, there are still many linguistic constructions -even very frequent ones -whose POS analysis is widely debated. The following sentences exemplify some of these hard cases that annotators frequently disagree on. Note that we do not claim that both analyses in each of these cases (1-3) are equally good, but that there is some linguistic motivation for either analysis in each case.</p><p>(  <ref type="bibr">(Section 2)</ref>. This suggests that most annotation differences derive from hard cases, rather than random errors.</p><p>We then collect a corpus of such disagreements and have experts mark which ones are due to ac- tual annotation errors, and which ones reflect lin- guistically hard cases (Section 3). The results show that the majority of disagreements are due to hard cases, and only about 20% of conflict- ing annotations are actual errors. This suggests that inter-annotator agreement scores often hide the fact that the vast majority of annotations are actually linguistically motivated. In our case, less than 2% of the overall annotations are linguisti- cally unmotivated.</p><p>Finally, in Section 4, we present an experiment trying to learn a model to distinguish between hard cases and annotation errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Annotator disagreements across domains and languages</head><p>In this study, we had between 2-10 individual an- notators with degrees in linguistics annotate dif- ferent kinds of English text with POS tags, e.g., newswire text (PTB WSJ Section 00), transcripts of spoken language (from a database containing transcripts of conversations, Talkbank 1 ), as well as Twitter posts. Annotators were specifically not presented with guidelines that would help them re- solve hard cases. Moreover, we compare profes- sional annotation to that of lay people. We in- structed annotators to use the 12 universal POS tags of <ref type="bibr" target="#b9">Petrov et al. (2012)</ref>. We did so in or- der to make comparison between existing data sets possible. Moreover, this allows us to fo- cus on really hard cases, as any debatable case in the coarse-grained tag set is necessarily also part of the finer-grained tag set. <ref type="bibr">2</ref> For each domain, we collected exactly 500 doubly-annotated sen- tences/tweets. Besides these English data sets, we also obtained doubly-annotated POS data from the French Social Media Bank project (Seddah et al., 2012). 3 All data sets, except the French one, are publicly available at http://lowlands.ku. dk/.</p><p>We present disagreements as Hinton diagrams in <ref type="figure">Figure 1a</ref>-c. Note that the spoken language data does not include punctuation. The correlations between the disagreements are highly significant, with Spearman coefficients ranging from 0.644 1 http://talkbank.org/ 2 Experiments with variation n-grams on WSJ <ref type="bibr" target="#b2">(Dickinson and Meurers, 2003</ref>) and the French data lead us to estimate that the fine-to-coarse mapping of POS tags disregards about 20% of observed tag-pair confusion types, most of which re- late to fine-grained verb and noun distinctions, e.g. past par- ticiple versus past in "[..] criminal lawyers speculated/VBD vs. VBN that [..]". <ref type="bibr">3</ref> We mapped POS tags into the universal POS tags using the mappings available here: https://code.google. com/p/universal-pos-tags/ (spoken and WSJ) to 0.869 (spoken and Twit- ter). Kendall's τ ranges from 0.498 (Twitter and WSJ) to 0.659 (spoken and Twitter). All diagrams have a vaguely "dagger"-like shape, with the blade going down the diagonal from top left to bot- tom right, and a slightly curved "hilt" across the counter-diagonal, ending in the more pronounced ADP/PRT confusion cells.</p><p>Disagreements are very similar across all three domains. In particular, adpositions (ADP) are con- fused with particles (PRT) (as in the case of "get out"); adjectives (ADJ) are confused with nouns (as in "stone lion"); pronouns (PRON) are con- fused with determiners (DET) ("my house"); nu- merals are confused with adjectives, determiners, and nouns ("2nd time"); and adjectives are con- fused with adverbs (ADV) ("see you later"). In Twitter, the X category is often confused with punctuations, e.g., when annotating punctuation acting as discourse continuation marker.</p><p>Our analyses show that a) experts disagree on the known hard cases when freely annotating text, and b) that these disagreements are the same across text types. More surprisingly, though, we also find that, as discussed next, c) roughly the same disagreements are also observed when com- paring the linguistic intuitions of lay people.</p><p>More specifically, we had lay annotators on the crowdsourcing platform Crowdflower re-annotate the training section of <ref type="bibr" target="#b4">Gimpel et al. (2011)</ref>. They collected five annotations per word. Only annota- tors that had answered correctly on 4 gold items (randomly chosen from a set of 20 gold items provided by the authors) were allowed to submit annotations. In total, 177 individual annotators supplied answers. We paid annotators a reward of $0.05 for 10 items. The full data set con- tains 14,619 items and is described in further de- tail in . Annotators were satis- fied with the task (4.5 on a scale from 1 to 5) and felt that instructions were clear (4.4/5), and the pay reasonable (4.1/5). The crowdsourced annotations aggregated using majority voting agree with the expert annotations in 79.54% of the cases. If we pre-filter the data via Wiktionary and use an item- response model ( <ref type="bibr" target="#b5">Hovy et al., 2013</ref>) rather than ma- jority voting, the agreement rises to 80.58%.  Lastly, we compare the disagreements of anno- tators on a French social media data set <ref type="bibr">(Seddah et al., 2012</ref>), which we mapped to the universal POS tag set. Again, we see the familiar dagger shape. The Spearman coefficient with English Twitter is 0.288; Kendall's τ is 0.204. While the correlation is weaker across languages than across domains, it remains statistically significant (p &lt; 0.001).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Hard cases and annotation errors</head><p>In the previous section, we demonstrated that some disagreements are consistent across domains and languages. We noted earlier, though, that dis- agreements can arise both from hard cases and from annotation errors. This can explain some We use a modified version of the a priori algo- rithm introduced in <ref type="bibr" target="#b2">Dickinson and Meurers (2003)</ref> to identify annotation inconsistencies. It works by collecting "variation n-grams", i.e. the longest sequence of words (n-gram) in a corpus that has been observed with a token being tagged differ- ently in another occurence of the same n-gram in the same corpus. The algorithm starts off by look- ing for unigrams and expands them until no longer n-grams are found.</p><p>For each variation n-gram that we found in WSJ-00, i.e, a word in various contexts and the possible tags associated with it, we present anno- tators with the cross product of contexts and tags. Essentially, we ask for a binary decision: Is the tag plausible for the given context?</p><p>We used 3 annotators with PhD degrees in lin- guistics. In total, our data set contains 880 items, i.e. 440 annotated confusion tag pairs. The raw agreement was 86%. <ref type="figure" target="#fig_3">Figure 4</ref> shows how truly hard cases are distributed over tag pairs (dark gray bars), as well as the proportion of confusions with respect to a given tag pair that are truly hard cases (light gray bars). The figure shows, for instance, that the variation n-gram regarding ADP-ADV is the second most frequent one (dark gray), and approximately 70% of ADP-ADV disagreements are linguistically hard cases (light gray). NOUN- PRON disagreements are always linguistically de- batable cases, while they are less frequent. A survey of hard cases. To further test the idea of there being truly hard cases that probably can- not be resolved by linguistic theory, we presented nine linguistics faculty members with 10 of the above examples and asked them to pick their fa- vorite analyses. In 8/10 cases, the faculty mem- bers disagreed on the right analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Learning to detect annotation errors</head><p>In this section, we examine whether we can learn a classifier to distinguish between hard cases and annotation errors. In order to do so, we train a clas- sifier on the annotated data set containing 440 tag- confusion pairs by relying only on surface form features. If we balance the data set and perform 3- fold cross-validation, a L2-regularized logistic re- gression (L2-LR) model achieves an f 1 -score for detecting errors at 70% (cf. Table 1), which is above average, but not very impressive.</p><p>The two classes are apparently not easily sepa- rable using surface form features, as illustrated in f 1 HARD CASES ERRORS L2-LR 73%(71-77) 70%(65-75) NN 76%(76-77) 71%(68-72) <ref type="table">Table 1</ref>: Classification results <ref type="figure">Figure 5</ref>: Hard cases and errors in 2d-PCA the two-dimensional plot in <ref type="figure">Figure 5</ref>, obtained us- ing PCA. The logistic regression decision bound- ary is plotted as a solid, black line. This is prob- ably also why the nearest neighbor (NN) classi- fier does slightly better, but again, performance is rather low. While other features may reveal that the problem is in fact learnable, our initial experi- ments lead us to conclude that, given the low ratio of errors over truly hard cases, learning to detect errors is often not worthwhile.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related work</head><p>Juergens (2014) presents work on detecting lin- guistically hard cases in the context of word sense annotations, e.g., cases where expert an- notators will disagree, as well as differentiat- ing between underspecified, overspecified and metaphoric cases. This work is similar to ours in spirit, but considers a very different task. While we also quantify the proportion of hard cases and present an analysis of these cases, we also show that disagreements are systematic. Our work also relates to work on automatically correcting expert annotations for inconsistencies <ref type="bibr" target="#b2">(Dickinson and Meurers, 2003</ref>). This work is very different in spirit from our work, but shares an interest in reconsidering expert annotations, and we made use of their mining algorithm here. There has also been recent work on adjudicat-ing noisy crowdsourced annotations <ref type="bibr" target="#b1">(Dawid and Skene, 1979;</ref><ref type="bibr">Smyth et al., 1995;</ref><ref type="bibr" target="#b0">Carpenter, 2008;</ref><ref type="bibr">Whitehill et al., 2009;</ref><ref type="bibr">Welinder et al., 2010;</ref><ref type="bibr">Yan et al., 2010;</ref><ref type="bibr" target="#b11">Raykar and Yu, 2012;</ref><ref type="bibr" target="#b5">Hovy et al., 2013</ref>). Again, their objective is orthogonal to ours, namely to collapse multiple annotations into a gold standard rather than embracing disagree- ments.</p><p>Finally,  use small samples of doubly-annotated POS data to estimate annotator reliability and show how those metrics can be im- plemented in the loss function when inducing POS taggers to reflect confidence we can put in annota- tions. They show that not biasing the theory to- wards a single annotator but using a cost-sensitive learning scheme makes POS taggers more robust and more applicable for downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we show that disagreements between professional or lay annotators are systematic and consistent across domains and some of them are systematic also across languages. In addition, we present an empirical analysis of POS annotations showing that the vast majority of inter-annotator disagreements are competing, but valid, linguis- tic interpretations. We propose to embrace such disagreements rather than using annotation guide- lines to optimize inter-annotator agreement, which would bias our models in favor of some linguistic theory.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 presentsFigure 1 :</head><label>21</label><figDesc>Figure 2 presents the Hinton diagram of the disagreements of lay people. Disagreements are very similar to the disagreements between expert annotators, especially on Twitter data (Figure 1b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Disagreement between lay annotators</figDesc><graphic url="image-4.png" coords="3,114.52,379.94,133.23,132.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Disagreement on French social media</figDesc><graphic url="image-5.png" coords="3,349.80,259.64,133.23,133.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Relative frequency of hard cases</figDesc><graphic url="image-6.png" coords="4,74.84,236.11,212.60,199.87" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank the anonymous reviewers for their feedback, as well as Djamé Seddah for the French data. This research is funded by the ERC Starting Grant LOWLANDS No. 313695.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Multilevel Bayesian models of categorical data annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Carpenter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>LingPipe</publisher>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Maximum likelihood estimation of observer error-rates using the EM algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Dawid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><forename type="middle">M</forename><surname>Skene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Statistics</title>
		<imprint>
			<biblScope unit="page" from="20" to="28" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Detecting errors in part-of-speech annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Detmar</forename><surname>Meurers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Criteria for the manual grouping of verb senses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cecily</forename><surname>Duffield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jena</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Dligach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Vieweg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LAW</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Mills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Heilman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning whom to trust with MACE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Experiments with crowdsourced re-annotation of a POS tagging data set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An analysis of ambiguity in word sense annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Juergens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of English: the Penn Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A universal part-of-speech tagset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning part-of-speech taggers with inter-annotator agreement loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Eliminating Spammers and Ranking Annotators for Crowdsourced Labeling Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vikas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shipeng</forename><surname>Raykar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="491" to="518" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
