<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:23+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bilingual Word Embeddings with Bucketed CNN for Parallel Sentence Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeenu</forename><surname>Grover</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pabitra</forename><surname>Mitra</surname></persName>
						</author>
						<title level="a" type="main">Bilingual Word Embeddings with Bucketed CNN for Parallel Sentence Extraction</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of ACL 2017, Student Research Workshop</title>
						<meeting>ACL 2017, Student Research Workshop <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="11" to="16"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-3003</idno>
					<note>Pabitra Mitra IIT Kharagpur India -721302 pabitra@cse.iitkgp.ernet.in</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a novel model which can be used to align the sentences of two different languages using neural architectures. First, we train our model to get the bilingual word embeddings and then, we create a similarity matrix between the words of the two sentences. Because of different lengths of the sentences involved, we get a matrix of varying dimension. We dynamically pool the similarity matrix into a matrix of fixed dimension and use Convo-lutional Neural Network (CNN) to classify the sentences as aligned or not. To further improve upon this technique, we bucket the sentence pairs to be classified into different groups and train CNN&apos;s separately. Our approach not only solves sentence alignment problem but our model can be regarded as a generic bag-of-words similarity measure for monolingual or bilingual corpora.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Parallel bilingual corpora are very crucial for building natural language processing systems, in- cluding machine translation, word disambigua- tion, and cross-language information retrieval. Machine translation tasks need a lot of parallel data for training purposes <ref type="bibr" target="#b2">(Brown et al., 1993)</ref>. Sentence alignment between two parallel mono- lingual corpora is used for getting the parallel data. But the present sentence alignment al- gorithms rely mainly on surface based proper- ties of the sentence pairs and lexicon-based ap- proaches ( <ref type="bibr" target="#b17">Varga et al., 2007;</ref><ref type="bibr" target="#b13">Moore, 2002;</ref><ref type="bibr" target="#b4">Gale and Church, 1993)</ref>. A little work has been done using the neural network models for sentence alignment. We propose a novel approach based on neural networks for sentence alignment which performs exceedingly well as compared to stan- dard alignment techniques which can not capture the semantics being conveyed by the text. Our model uses distributed word-embeddings which have been behind the success of many NLP ap- plications in recent years. We then leverage upon the use of CNNs ( <ref type="bibr" target="#b18">Zou et al., 2013;</ref><ref type="bibr" target="#b9">Kusner et al., 2015;</ref><ref type="bibr" target="#b14">Norouzi et al., 2013</ref>) for capturing the word- overlapping and word-ordering features in similar- ity matrix for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>The different aspects of our model are presented below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Bilingual word embeddings</head><p>Bilingual Word embeddings ( <ref type="bibr" target="#b1">Bengio and Corrado, 2015;</ref>) are a representation of the words of two languages in the same seman- tic space. The three ways of getting the bilin- gual word representations as mentioned by Lu- ong et al. are: bilingual mapping ( <ref type="bibr" target="#b11">Mikolov et al., 2013a</ref>), monolingual adaptation ( <ref type="bibr" target="#b18">Zou et al., 2013)</ref> and bilingual training <ref type="bibr" target="#b0">(AP et al., 2014;</ref>.</p><p>The semantic relatedness of two words across different languages have been compared by using translation dictionaries for the purpose of sentence alignment tasks. But, this method of telling if the two words across languages are synonyms or not, is very discrete, because a word would be called a synonym only if it is present in list of top k synonyms of the other word. Words having a lit- tle similarity with each other would be totally ig- nored. Our approach intends to mitigate this dif- ference by using bilingual word embeddings. Our method is not discrete, because even if the word is not present in the list of top synonyms for the other, we would still get their similarity (albeit low) using the bilingual embeddings.</p><p>In our paper, we would be using bilingual train- ing for getting word representations as proposed by . Bilingual training does not depend on independently trained word repre- sentation on monolingual corpora of either lan- guage. Instead, we learn monolingual and bilin- gual representations jointly on a parallel corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Similarity Matrix</head><p>Two semantically similar sentences in same or dif- ferent languages would use same or similar words albeit with a change of order introduced by var- ious factors like language grammar, change of narrative, change of tense or even paraphrasing. We assume that if a sentence pair has high word- overlap, then it might be conveying the same se- mantics. Here, we are ignoring the cases in which same set of words may convey different seman- tics because parallel corpora contains a few such instances. Handling such instances would lead to minor changes in precision or recall due to fewer instances and is beyond the scope of this paper. Moreover, none of the existing approaches to sen- tence alignment deal with such cases.</p><p>To capture word-overlap, some alignment tech- niques use measures like TF-IDF similarity ( <ref type="bibr" target="#b3">Fung and Cheung, 2004</ref>) or even the basic Jaccard sim- ilarity along with translation dictionaries. In our approach, we generate a similarity matrix for each sentence pair, where entries in rows are corre- sponding to the words of the sentence of one lan- guage in their order of occurrence. Same way, columns denote the entries for the words of sen- tence of second language. The entry S(i, j) of S denotes a similarity measure between the words w i of s 1 and w j of s 2 . If we find a word in the sentence which is not present in the corresponding vocabulary, we simply omit it. The different sim- ilarity measures that we used include cosine simi- larity and euclidean distance between the embed- dings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Bucketing and Dynamic Poling</head><p>Last step gave us the similarity matrix for the sen- tences s 1 and s 2 , but the size of the matrix is variable owing to different sentence lengths. We would pool this similarity matrix dynamically to convert it into a matrix of fixed dimension. But we were a bit skeptical of this step as even sen- tences with very short and very long lengths would be pooled to the same dimension. To overcome it, we bucketed the sentences into different sentence length ranges. Bucketing is done on the basis of mean length of the two sentences in the pair to be classified.</p><p>Thus, we trained different classifiers for each range of the buckets. The main limitation of this method is that the effective training data reduces for each of the classifiers. This may degrade the performance of the model as compared to a single classifier which has all the data available for train- ing. If parallel annotated data is available in abun- dance, then this method would work better than a single classifier.</p><p>To convert matrix to a fixed size representation, we pool it dynamically to a matrix of fixed dimen- sion as mentioned in Socher et al. We divide each dimension of 2D matrix into dim chunks of len dim size, where dim is the bucket size and len is the length of dimension. If the length len of any di- mension is lesser than dim, we duplicate the ma- trix entries along that dimension till len becomes greater than or equal to dim. If there are l left- over entries where</p><formula xml:id="formula_0">l = len − dim * len dim , we</formula><p>distribute them to the last l chunks. We do it for both the dimensions. Now, for a particular chunk, we can pool the entries in it using methods like average, minimum and maximum pooling. When we take cosine sim- ilarity between words as the matrix entries, we take max-pooling and with euclidean distance, we take min-pooling. This is because we do not want to fade the effect of two words with high similarity entries in the same chunk and the mentioned pool- ing methods for each similarity measure, take care of it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Convolution Neural Network</head><p>CNN's have been found to be performing well where we need to capture the spatial properties of the input in the neural network ( <ref type="bibr" target="#b8">Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b5">Kim, 2014</ref>). The intention behind us- ing CNN's on matrix rather than training a simple neural classifier on input of flattened data is that the similarity matrices not only contain the simi- larity scores between words but they also capture the word-order. Thus a matching phrase would appear as a high intensity diagonal streak in the similarity matrix (Refer <ref type="figure">figure 4)</ref>. A single 1D vector would loose such visual word-ordering fea- tures of the similarity matrix. We also report the performance of a multilayer perceptron classifier as compared to CNN's, justifying our choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data</head><p>We performed our experiment on the sentence alignment dataset ( <ref type="bibr" target="#b19">Zweigenbaum et al., 2016)</ref> provided by shared task of 10 th BUCC work- shop <ref type="bibr">1</ref> . We performed experiments on the English- German (en-de) dataset. The dataset consists of 399,337 sentences of monolingual English Cor- pora and 413,869 sentences of monolingual Ger- man corpora. The gold alignment has been pro- vided with the data. The gold alignment between the two languages consists of 9580 matching sen- tence pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sampling</head><p>As described above, the sizes of monolingual cor- pora are large as compared to actual aligned sen- tences. The total possible sentence pairs are 165 billion which contains just 9580 positive sentence pairs, creating a huge class imbalance. It would be difficult to train or even store 165 billion sen- tence pairs. Moreover, large presence of negative data in the corpora would also make the classifiers less sensitive towards positive data. To overcome these difficulties, we sampled negative examples from the data. <ref type="table" target="#tab_1">Table 1</ref> gives the sizes of datasets after sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training of Word Embeddings</head><p>We used the English German word embeddings as mentioned in the paper by Luong et al. <ref type="bibr">2</ref> The dimension of all embeddings used in our exper- iments is 128. The embeddings are trained on parallel Europarl v7 corpus between German (de) and English (en) ( <ref type="bibr" target="#b7">Koehn, 2005)</ref>, which consists of 1.9M parallel sentences. The training settings as described in <ref type="bibr">Mikolov et al. and Luong et al.</ref> were used. The hyper-parameter α is 1 and β is 4 in our experiments as described in Luong et al. The vo- cabulary size for en is 40,054 and for de is 95,195.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Similarity Matrix and Bucketing</head><p>We split the sentence pairs in data into training, validation and testing set having a ratio of 6:1:3 1 https://comparable.limsi.fr/bucc2017/ bucc2017-task.html <ref type="bibr">2</ref> The code and pre-trained embeddings can be down- loaded from http://stanford.edu/ ˜ lmthang/ bivec  and 9581 respectively. The bucketed data ranges for each data split are shown in Table1. There were no sentence pairs in our dataset with mean length of pair below 5 or above 25. More over, the splits for range <ref type="bibr">(5,</ref><ref type="bibr">8]</ref> and <ref type="bibr">(20,</ref><ref type="bibr">25]</ref> do not contain enough data for training, so we exclude them from the ex- periments. Thus, out of total 8 buckets in <ref type="table" target="#tab_1">Table 1</ref>, we ran the experiments for only 5 buckets as well as all non-bucketed data. We report our results us- ing cosine similarity, as it performed better than using euclidean distance as similarity measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Model Parameters</head><p>The neural network architecture is described be- low:</p><p>• Convolution Layer 1 with Relu Activation: We used a 3D convolution volume of area 3× 3 and depth 12 on input of size dim×dim×1 where dim is the bucket size. The strides of 1 unit were used in each direction. The zero padding was done to keep output height and width same as input. The convolution layer was followed by a layer of Rectified Linear Units (Relu) to introduce the non-linearity in the model</p><p>• Max-Pooling Layer 1: We used max pooling on the output of the previous layer to reduce its size by half. This was done using strides of 2 units for both height and width.</p><p>• • Max-Pooling Layer 2: It again reduces the size of input by half using strides of 2 units in both directions. The output of this layer is flattened from 3D to 1D to pass it to next layer as input.</p><p>• Fully Connected Layer 1 (FC1) with Relu: This layer maps the flattened input to a hid- den layer with 200 units. Relu Activation was used here on the output of hidden units.</p><p>• Dropout Layer: We used a layer with dropout probability of 0.2 to prevent over-fitting in the model.</p><p>• Full Connected layer 2: This layer maps the output of previous layer with 200 hidden units into a single output, which is further passed to sigmoid activation unit. The value of the sigmoid unit is the predicted output.</p><p>The model was trained using 20 epochs with batch-size of 5. The loss function is the mean squared error between actual and predicted output. The Adam optimizer <ref type="bibr" target="#b6">(Kingma and Ba, 2014</ref>) was used for stochastic optimization for backpropaga- tion. The learning rate parameter (eta) is 0.0005. The motivation behind using Relu as activation function is to overcome gradient decays, which hinder the training of the neural networks. All the hyper-parameters were tuned by random search in hyper-parameter space and testing on the valida- tion dataset. The total number of parameters to be trained in the model are 150 × dim 2 + 681, where dim is the bucket size. This gives us num- ber of parameters ranging from 15,681 for lowest size bucket with dim = 10 and 60,681 for largest bucket with dim = 20 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Analysis</head><p>To evaluate, we ran our algorithm on the en-de Test set mentioned beforehand. Our algorithm assigns a score to each sentence pair, denoting the proba- bility of two sentences conveying same semantics. If the score is greater than a certain threshold th, we take it as a positive. <ref type="table">Table 2</ref> shows results for th = 0.5. When we performed experiments for all data <ref type="table">(Total in Table 2</ref>), without bucketing, we chose dim = 15 as highest number of data entries fall in that bucket. We expected that bucketing data would yield better results compared to non- bucketing as each sentence pair would be pooled to a matrix of the dimension comparable to its  <ref type="table">Table 2</ref>: <ref type="table">Table showing</ref> Test dataset type (T Data), True Positives (TP), False Positives (FP), False Negatives (FN), Precision (P), Recall (R), and F1-score (F1).*Includes all non-bucketed data actual length. But as seen in <ref type="table">Table 2</ref>, the non- bucketed approach performs very well and in some cases, even better than a few buckets. This hap- pens because the training data available for non- bucketed approach (18654 pairs) is atleast double of any of the buckets. Ratio column in <ref type="table" target="#tab_1">Table 1</ref> shows the ratio of Training data to number of pa- rameters in the model, which is highest for non- bucketed data with dim = 15. If we had more training data in each bucket, then all the buckets might have achieved better performance than non- bucketed approach. Macro and Micro in <ref type="table">Table 2</ref> denote the Macro-average and Micro-average re- spectively for all the 5 buckets taken for experi- ment. We also used a multilayer perceptron classifier on non-bucketed data with flattened matrices as input (Baseline in <ref type="table">Table 2</ref>), but that performed poorly with F1 score of 0.8564. <ref type="figure" target="#fig_2">Figure 2</ref> and 3 de- pict the similarity matrices for True Positives and True Negatives. We can clearly observe some vi-sual features of the similarity matrices, such as the presence of high intensity streaks along diagonal, which denote high similarity between the entries in close vicinity in one sentence to the entries in close vicinity in the other. This justifies our hy- pothesis that, unlike multilayer perceptron, CNN is able to capture the relations between word sim- ilarity and their ordering, which is represented by the matrices. Our method also performs much bet- ter than other baseline methods such as a classifier using sentence length features.  As an abstraction, our model can be viewed as a neural equivalent of bag-of-words similarity mea- sures such as TF-IDF similarity or Jaccard sim- ilarity as this approach covers the word-overlap between two documents (sentences here). More- over, rather just capturing the overlap, it also cap- tures the order in which the words match in two documents. So, it can be dubbed as neural-bag- of-words like model which remembers matching order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Future Work</head><p>We have used the dynamically pooled similarity matrices with CNN for the purpose of sentence alignment. But as already mentioned, our ap- proach specifies a general way of obtaining the similarity between two texts whether they belong to same or different languages. The sentences be- longing to the same language can be handled in the same way, but only monolingual word embed- dings would suffice for that purpose.</p><p>A unique feature of our similarity measure is that we get the similarity between two texts with- out mapping them to their respective vectors in the vector space. We can interpret it like a kernel func- tion which gives dot product φ(x).φ(y) between two entities without actually transforming x or y to φ(x) or φ(y) respectively. Also, unlike TF-IDF, where each vector is of the size of the vocabulary, our similarity approach takes only dim 2 space per sentence pair, which is much lesser than the for- mer.</p><p>Our current approach assumes that the two texts are of comparable length, because that is generally the case for aligned sentences and that's why we took the dynamically pooled matrices with both dimensions of same size. But, if we want to use our approach for information retrieval purposes, then the size of document D would be much larger than size of the query q. In that case, we would have to take rectangular dynamically pooled ma- trices with appropriate dimensions. We would like to study the efficacy of our approach in all such scenarios.</p><p>Also, since our approach can tell the parts of the document it is matching, unlike TF-IDF, we can use it to assign different scores for matching phrases in different parts of the document. For ex- ample, to search for a query on a webpage which has an article and comments from the readers (just like a blog), our approach can be trained to give more importance to the matches in the article as compared to the reader comments, thus leading to a better information retrieval approach. In the future, we would also like to study how different properties like time and space complexity of our approach scale for large dataset. We would also like to explore the applications of our approach for tasks like cross-lingual as well as monolingual in- formation retrieval, query expansion, cross-lingual recommender systems etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>The novelty of our approach lies in using neu- ral word embeddings in bilingual semantic space along with CNN to capture the sentence similar- ity and we have achieved very good results over the dataset by BUCC. Our model provides a new equivalent of bag-of-words similarity measures which is also aware of the matching order. The architecture proposed by our algorithm is not just for this specific task but can be used for a number of other tasks like Information Retrieval in mono- lingual as well as bilingual corpora, query expan- sion for cross-lingual search etc. We would like to study the different properties and explore the ap- plications of our approach in future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Convolution Layer 2 with Relu : It uses a 3D volume of area 3 × 3 and depth 16. Rest all properties are same as Convolution Layer 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Precision-Recall Curve for all the buckets as well as total data. The different dimensions have been zoomed appropriately to show relevant parts of the plot.</figDesc><graphic url="image-1.png" coords="4,307.28,473.95,226.76,126.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: True Positive examples. Left image shows a sentence pair with high overlap and right image shows a sentence pair with low overlap.</figDesc><graphic url="image-2.png" coords="5,72.00,223.37,108.85,81.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: True Negative examples. Both the images are visually different from True Positives.</figDesc><graphic url="image-4.png" coords="5,72.00,376.37,108.85,81.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Table showing bucket-ranges and the 
number of sentence pairs in train, validation and 
test set. Number of parameters (Par) and Ra-
tio of Training data to Parameter size (Ratio = 
T rain ÷ P ar, in 10 −3 units) are shown for each 
bucket 

respectively, giving data splits of size 18654, 3411 
</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgements</head><p>We would like to thank all the anonymous review-ers for their valuable feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An autoencoder approach to learning bilingual word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarath</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislas</forename><surname>Lauly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitesh</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaraman</forename><surname>Ravindran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vikas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amrita</forename><surname>Raykar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1853" to="1861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<title level="m">Bilbowa: Fast bilingual distributed representations without word alignments</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The mathematics of statistical machine translation: Parameter estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent J Della</forename><surname>Peter F Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen A Della</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert L</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="311" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mining verynon-parallel corpora: Parallel sentence and lexicon extraction via bootstrapping and e</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP. Citeseer</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="57" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A program for aligning sentences in bilingual corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth W</forename><surname>Gale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Church</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="75" to="102" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Europarl: A parallel corpus for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MT summit</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">From word embeddings to document distances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nicholas I Kolkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="957" to="966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bilingual word representations with monolingual quality in mind</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing</title>
		<meeting>the 1st Workshop on Vector Space Modeling for Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="151" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Exploiting similarities among languages for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.4168</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast and accurate sentence alignment of bilingual corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robert C Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the Association for Machine Translation in the Americas</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="135" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Zero-shot learning by convex combination of semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5650</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning distributed representations for multilingual text sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="88" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="801" to="809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Parallel corpora for medium density languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dániel</forename><surname>Varga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Péter</forename><surname>Halácsy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">András</forename><surname>Kornai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AMSTERDAM STUDIES IN THE THEORY AND HISTORY OF LINGUISTIC SCIENCE SERIES</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">247</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>Viktor Nagy, László Németh, and Viktor Trón</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bilingual word embeddings for phrase-based machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Will</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1393" to="1398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Towards preparation of the second bucc shared task: Detecting parallel sentences in comparable corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Zweigenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Sharoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Rapp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ninth Workshop on Building and Using Comparable Corpora</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">38</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
