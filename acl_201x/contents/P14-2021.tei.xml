<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Robust Logistic Regression using Shift Parameters</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Tibshirani</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<postCode>94305</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<postCode>94305</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Robust Logistic Regression using Shift Parameters</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="124" to="129"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Annotation errors can significantly hurt classifier performance, yet datasets are only growing noisier with the increased use of Amazon Mechanical Turk and techniques like distant supervision that automatically generate labels. In this paper, we present a robust extension of logistic regression that incorporates the possibility of mislabelling directly into the objective. This model can be trained through nearly the same means as logistic regression , and retains its efficiency on high-dimensional datasets. We conduct experiments on named entity recognition data and find that our approach can provide a significant improvement over the standard model when annotation errors are present.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Almost any large dataset has annotation errors, especially those complex, nuanced datasets com- monly used in natural language processing. Low- quality annotations have become even more com- mon in recent years with the rise of Amazon Me- chanical Turk, as well as methods like distant su- pervision and co-training that involve automati- cally generating training data.</p><p>Although small amounts of noise may not be detrimental, in some applications the level can be high: upon manually inspecting a relation ex- traction corpus commonly used in distant super- vision, <ref type="bibr" target="#b20">Riedel et al. (2010)</ref> report a 31% false positive rate. In cases like these, annotation er- rors have frequently been observed to hurt perfor- mance. <ref type="bibr" target="#b7">Dingare et al. (2005)</ref>, for example, con- duct error analysis on a system to extract relations from biomedical text, and observe that over half of the system's errors could be attributed to incon- sistencies in how the data was annotated. Simi- larly, in a case study on co-training for natural lan- guage tasks, <ref type="bibr" target="#b17">Pierce and Cardie (2001)</ref> find that the degradation in data quality from automatic la- belling prevents these systems from performing comparably to their fully-supervised counterparts.</p><p>In this work we argue that incorrect exam- ples should be explicitly modelled during train- ing, and present a simple extension of logistic re- gression that incorporates the possibility of mis- labelling directly into the objective. Following a technique from robust statistics, our model intro- duces sparse 'shift parameters' to allow datapoints to slide along the sigmoid, changing class if ap- propriate. It has a convex objective, is well-suited to high-dimensional data, and can be efficiently trained with minimal changes to the logistic re- gression pipeline.</p><p>In experiments on a large, noisy NER dataset, we find that this method can provide an improve- ment over standard logistic regression when anno- tation errors are present. The model also provides a means to identify which examples were misla- belled: through experiments on biological data, we demonstrate how our method can be used to accurately identify annotation errors. This robust extension of logistic regression shows particular promise for NLP applications: it helps account for incorrect labels, while remaining efficient on large, high-dimensional datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Much of the previous work on dealing with anno- tation errors centers around filtering the data be- fore training. <ref type="bibr" target="#b4">Brodley and Friedl (1999)</ref> introduce what is perhaps the simplest form of supervised filtering: they train various classifiers, then record their predictions on a different part of the train set and eliminate contentious examples. <ref type="bibr" target="#b21">Sculley and Cormack (2008)</ref> apply this approach to spam fil- tering with noisy user feedback.</p><p>One obvious issue with these methods is that the noise-detecting classifiers are themselves trained on noisy labels. Unsupervised filtering tries to avoid this problem by clustering training instances based solely on their features, then using the clus- ters to detect labelling anomalies ( <ref type="bibr" target="#b19">Rebbapragada et al., 2009)</ref>. Recently, <ref type="bibr" target="#b12">Intxaurrondo et al. (2013)</ref> applied this approach to distantly-supervised rela- tion extraction, using heuristics such as the num- ber of mentions per tuple to eliminate suspicious examples.</p><p>Unsupervised filtering, however, relies on the perhaps unwarranted assumption that examples with the same label lie close together in feature space. Moreover filtering techniques in general may not be well-justified: if a training example does not fit closely with the current model, it is not necessarily mislabelled. It may represent an important exception that would improve the over- all fit, or appear unusual simply because we have made poor modelling assumptions.</p><p>Perhaps the most promising approaches are those that directly model annotation errors, han- dling mislabelled examples as they train. This way, there is an active trade-off between fitting the model and identifying suspected errors. <ref type="bibr" target="#b3">Bootkrajang and Kaban (2012)</ref> present an extension of logistic regression that models annotation errors through flipping probabilities. While intuitive, this approach has shortcomings of its own: the objec- tive function is nonconvex and the authors note that local optima are an issue, and the model can be difficult to fit when there are many more fea- tures than training examples.</p><p>There is a growing body of literature on learn- ing from several annotators, each of whom may be inaccurate ( <ref type="bibr" target="#b2">Bachrach et al., 2012;</ref><ref type="bibr" target="#b18">Raykar et al., 2009)</ref>. It is important to note that we are consid- ering a separate, and perhaps more general, prob- lem: we have only one source of noisy labels, and the errors need not come from the human annota- tors, but could be introduced through contamina- tion or automatic labelling.</p><p>The field of 'robust statistics' seeks to develop estimators that are not unduly affected by devi- ations from the model assumptions <ref type="bibr">(Huber and Ronchetti, 2009)</ref>. Since mislabelled points are one type of outlier, this goal is naturally related to our interest in dealing with noisy data, and it seems many of the existing techniques would be relevant. A common strategy is to use a modi- fied loss function that gives less influence to points far from the boundary, and several models along  <ref type="formula" target="#formula_4">(2009)</ref> propose a similar approach for principal compo- nent analysis, while Wright and Ma (2009) ex- plore its effectiveness in sparse signal recovery. In this work we adapt the technique to logistic re- gression. To the best of our knowledge, we are the first to experiment with adding 'shift param- eters' to logistic regression and demonstrate that the model is especially well-suited to the type of high-dimensional, noisy datasets commonly used in NLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>Recall that in binary logistic regression, the prob- ability of an example x i being positive is modeled as</p><formula xml:id="formula_0">g(θ T x i ) = 1 1 + e −θ T x i .</formula><p>For simplicity, we assume the intercept term has been folded into the weight vector θ, so θ ∈ R m+1 where m is the number of features. Following She and Owen (2011), we propose the following robust extension: for each datapoint i = 1, . . . , n, we introduce a real-valued shift pa-rameter γ i so that the sigmoid becomes</p><formula xml:id="formula_1">g(θ T x i + γ i ) = 1 1 + e −θ T x i −γ i .</formula><p>Since we believe that most examples are correctly labelled, we L 1 -regularize the shift parameters to encourage sparsity. Letting y i ∈ {0, 1} be the la- bel for datapoint i and fixing λ ≥ 0, our objective is now given by</p><formula xml:id="formula_2">l(θ, γ) = n i=1 y i log g(θ T x i + γ i )<label>(1)</label></formula><formula xml:id="formula_3">+ (1 − y i ) log 1 − g(θ T x i + γ i ) − λ n i=1 |γ i |.</formula><p>These parameters γ i let certain datapoints shift along the sigmoid, perhaps switching from one class to the other. If a datapoint i is correctly an- notated, then we would expect its corresponding γ i to be zero. If it actually belongs to the posi- tive class but is labelled negative, then γ i might be positive, and analogously for the other direction.</p><p>One way to interpret the model is that it al- lows the log-odds of select datapoints to be shifted. Compared to models based on label- flipping, where there is a global set of flipping probabilities, our method has the advantage of tar- geting each example individually.</p><p>It is worth noting that there is no difficulty in regularizing the θ parameters as well. For exam- ple, if we choose to use an L 1 penalty then our objective becomes</p><formula xml:id="formula_4">l(θ, γ) = n i=1 y i log g(θ T x i + γ i )<label>(2)</label></formula><formula xml:id="formula_5">+ (1 − y i ) log 1 − g(θ T x i + γ i ) − κ m j=1 |θ j | − λ n i=1 |γ i |.</formula><p>Finally, it may seem concerning that we have introduced a new parameter for each datapoint. But in many applications the number of features already exceeds n, so with proper regularization, this increase is actually quite reasonable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training</head><p>Notice that adding these shift parameters is equiv- alent to introducing n features, where the ith new feature is 1 for datapoint i and 0 otherwise. With this observation, we can simply modify the fea- ture matrix and parameter vector and train the lo- gistic model as usual. Specifically, we let θ = (θ 0 , . . . , θ m , γ 1 , . . . , γ n ) and X = [X|I n ] so that the objective (1) simplifies to</p><formula xml:id="formula_6">l(θ ) = n i=1 y i log g(θ T x i ) + (1 − y i ) log 1 − g(θ T x i ) − λ m+n j=m+1 |θ (j) |.</formula><p>Upon writing the objective in this way, we imme- diately see that it is convex, just as standard L 1 - penalized logistic regression is convex.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Testing</head><p>To obtain our final logistic model, we keep only the θ parameters. Predictions are then made as usual:</p><formula xml:id="formula_7">I{g( ˆ θ T x) &gt; 0.5}.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Selecting Regularization Parameters</head><p>The parameter λ from equation <ref type="formula" target="#formula_2">(1)</ref> would nor- mally be chosen through cross-validation, but our set-up is unusual in that the training set may con- tain errors, and even if we have a designated devel- opment set it is unlikely to be error-free. We found in simulations that the errors largely do not inter- fere in selecting λ, so in the experiments below we cross-validate as normal.</p><p>Notice that λ has a direct effect on the number of nonzero shifts γ and hence the suspected num- ber of errors in the training set. So if we have in- formation about the noise level, we can directly incorporate it into the selection procedure. For ex- ample, we may believe the training set has no more than 15% noise, and so would restrict the choice of λ during cross-validation to only those values where 15% or fewer of the estimated shift param- eters are nonzero.</p><p>We now consider situations in which the θ pa- rameters are regularized as well. Assume, for ex- ample, that we use L 1 -regularization as in equa- tion (2), so that we now need to optimize over both κ and λ. We perform the following simple proce- dure:</p><p>1. Cross-validate using standard logistic regres- sion to select κ.</p><p>2. Fix this value for κ, and cross-validate using the robust model to find the best choice of λ. method suspects identified false positives <ref type="bibr" target="#b0">Alon et al. (1999)</ref> T2 T30 T33 T36 T37 N8 N12 N34 N36 <ref type="bibr" target="#b10">Furey et al. (2000)</ref> • <ref type="table">Table 1</ref>: Results of various error-identification methods on the colon cancer dataset. The first row lists the samples that are biologically confirmed to be suspicious, and each other row gives the output from an automatic detection method. Bootkrajang et al. report confidences, so we threshold at 0.5 to obtain these results.</p><formula xml:id="formula_8">• • • • • Kadota et al. (2003) • • • • • T6, N2 Malossini et al. (2006) • • • • • • • T8, N2, N28, N29 Bootkrajang et al. (2012) • • • • • • • Robust LR • • • • • • •</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conduct two sets of experiments to assess the effectiveness of the approach, in terms of both identifying mislabelled examples and producing accurate predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Contaminated Data</head><p>Our first experiment is centered around a biologi- cal dataset with suspected labelling errors. Called the colon cancer dataset, it contains the expres- sion levels of 2000 genes from 40 tumor and 22 normal tissues ( <ref type="bibr" target="#b0">Alon et al., 1999</ref>). There is evi- dence in the literature that certain tissue samples may have been cross-contaminated. In particular, 5 tumor and 4 normal samples should have their labels flipped.</p><p>In this experiment, we examine the model's ability to identify mislabelled training examples. Because there are many more features than data- points and it is likely that not all genes are relevant, we choose to place an L 1 penalty on θ.</p><p>Using glmnet, an R package for training reg- ularized models <ref type="figure">(Friedman et al., 2009)</ref>, we se- lect κ and λ using cross-validation. Looking at the resulting values for γ, we find that only 7 of the shift parameters are nonzero and that each one corresponds to a suspicious datapoint. As further confirmation, the signs of the gammas correctly match the direction of the mislabelling. Compared to previous attempts to automatically detect errors in this dataset, our approach identifies at least as many suspicious examples but with no false posi- tives. A detailed comparison is given in <ref type="table">Table 1</ref>. Although <ref type="bibr" target="#b3">Bootkrajang and Kaban (2012)</ref> are quite accurate, it is worth noting that due to its noncon- vexity, their model needed to be trained 20 times to achieve these results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Manually Annotated Data</head><p>We now consider the problem of named entity recognition (NER) to evaluate how our model per- forms in a large-scale prediction task. In tradi- tional NER, the goal is to determine whether each word is a person, organization, location, or not a named entity ('other'). Since our model is binary, we concentrate on the task of deciding whether a word is a person or not. (This task does not triv- ially reduce to finding the capitalized words, as the model must distinguish between people and other named entities like organizations).</p><p>For training, we use a large, noisy NER dataset collected by Jenny Finkel. The data was created by taking various Wikipedia articles and giving them to five Amazon Mechanical Turkers to anno- tate. Few to no quality controls were put in place, so that certain annotators produced very noisy la- bels. To construct the train set we chose a Turker who was about average in how much he disagreed with the majority vote, and used only his annota- tions. Negative examples are subsampled to bring the class ratio to a reasonable level, for a total of 200,000 negative and 24,002 positive examples. We find that in 0.4% of examples, the majority agreed they were negative but the chosen annota- tor marked them positive, and 7.5% were labelled positive by the majority but negative by the an- notator. Note that we still include examples for which there was no majority consensus, so these noise estimates are quite conservative.</p><p>We evaluate on the English development test set from the CoNLL shared task <ref type="bibr" target="#b23">(Tjong Kim Sang and Meulder, 2003)</ref>. This data consists of news arti- cles from the Reuters corpus, hand-annotated by researchers at the University of Antwerp.</p><p>We extract a set of features using Stanford's NER pipeline (   chosen for simplicity and is not highly engineered -it largely consists of lexical features such as the current word, the previous and next words in the sentence, as well as character n-grams and vari- ous word shape features. With a total of 393,633 features in the train set, we choose to use L 2 - regularization, so that our penalty now becomes</p><formula xml:id="formula_9">1 2σ 2 m j=0 |θ j | 2 + λ n i=1 |γ i |.</formula><p>This choice is natural as L 2 is the most common form of regularization in NLP, and we wish to ver- ify that our approach works for penalties besides</p><formula xml:id="formula_10">L 1 .</formula><p>The robust model is fit using Orthant-Wise Limited-Memory Quasi Newton (OWL-QN), a technique for optimizing an L 1 -penalized objec- tive <ref type="bibr" target="#b1">(Andrew and Gao, 2007)</ref>. We tune both models through 5-fold cross-validation to obtain σ 2 = 1.0 and λ = 0.1. Note that from the way we cross-validate (first tuning σ using standard lo- gistic regression, fixing this choice, then tuning λ) our procedure may give an unfair advantage to the baseline.</p><p>We also compare against the algorithm pro- posed in <ref type="bibr" target="#b3">Bootkrajang and Kaban (2012)</ref>, an exten- sion of logistic regression mentioned in the section on prior work. This approach assumes that each example's true label is flipped with a certain prob- ability before being observed, and fits the resulting latent-variable model using EM.</p><p>The results of these experiments are shown in <ref type="table" target="#tab_1">Table 2</ref> as well as <ref type="figure">Figure 2</ref>. Robust logistic re- gression offers a noticeable improvement over the baseline, and this improvement holds at essentially all levels of precision and recall. Interestingly, be- cause of the large dimension, the flipping model consistently learns that no labels have been flipped and thus does not show a substantial difference with standard logistic regression.  <ref type="figure">Figure 2</ref>: Precision-recall curve obtained from training on noisy Wikipedia data and testing on CoNLL. The flipping model refers to the approach from Bootkrajang and Kaban (2012).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Future Work</head><p>A natural direction for future work is to extend the model to a multi-class setting. One option is to introduce a γ for every class except the negative one, so that there are n(c − 1) shift parameters in all. We could then apply a group lasso, with each group consisting of the γ for a particular datapoint <ref type="bibr" target="#b16">(Meier et al., 2008)</ref>. This way all of a datapoint's shift parameters drop out together, which corre- sponds to the example being correctly labelled. CRFs and other sequence models could also benefit from the addition of shift parameters. Since the extra variables can be neatly folded into the linear term, convexity is preserved and the model could essentially be trained as usual.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Fit resulting from a standard vs. robust model, where data is generated from the dashed sigmoid and negative labels flipped with probability 0.2. these lines have been proposed (Ding and Vishwanathan., 2010; Masnadi-Shirazi et al., 2010). Unfortunately these approaches require optimizing nonstandard, often nonconvex objectives, and fail to give insight into which datapoints are mislabelled. In a recent advance, She and Owen (2011) demonstrate that introducing a regularized 'shift parameter' per datapoint can help increase the robustness of linear regression. Candes et al. (2009) propose a similar approach for principal component analysis, while Wright and Ma (2009) explore its effectiveness in sparse signal recovery. In this work we adapt the technique to logistic regression. To the best of our knowledge, we are the first to experiment with adding 'shift parameters' to logistic regression and demonstrate that the model is especially well-suited to the type of high-dimensional, noisy datasets commonly used in NLP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Performance of standard vs. robust logis-
tic regression in the Wikipedia NER experiment. 
The flipping model refers to the approach from 
Bootkrajang and Kaban (2012). 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Stanford University gratefully acknowledges the support of the Defense Advanced Research Projects Agency (DARPA) Deep Exploration and Filtering of Text (DEFT) Program under Air Force Research Laboratory (AFRL) contract no. FA8750-13-2-0040. Any opinions, findings, and conclusion or recommendations expressed in this material are those of the authors and do not nec-essarily reflect the view of the DARPA, AFRL, or the US government. We are especially grateful to Rob Tibshirani and Stefan Wager for their invalu-able advice and encouragement.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Broad patterns of gene expression revealed by clustering analysis of tumor and normal colon tissues probed by oligonucleotide arrays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barkai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Notterman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ybarra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">National Academy of Sciences of the USA</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Scalable Training of L 1-Regularized Log-Linear Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galen</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">How To Grade a Test Without Knowing the Answers: A Bayesian Graphical Model for Adaptive Crowdsourcing and Aptitude Testing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thore</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Minka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Guiver</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.6386</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Labelnoise Robust Logistic Regression and Its Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakramate</forename><surname>Bootkrajang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ata</forename><surname>Kaban</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ECML PKDD</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Identifying mislabeled Training Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carla</forename><forename type="middle">E</forename><surname>Brodley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Friedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAIR</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="131" to="167" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><forename type="middle">J</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wright</surname></persName>
		</author>
		<idno type="arXiv">arXiv:0912.3599</idno>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
<note type="report_type">Robust Principal Component Analysis? arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<idno>regression. NIPS</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
<note type="report_type">t-Logistic</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A system for identifying named entities in biomedical text: How results from two evaluations reflect on both the system and the evaluations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malvina</forename><surname>Shipra Dingare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Nissim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comparative and Functional Genomics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="77" to="85" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trond</forename><surname>Grenager</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
	<note>Christopher Manning</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Regularization Paths for Generalized Linear Models via Coordinate Descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of statistical software</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Support vector machine classification and validation of cancer tissue samples using microarray expression data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrence</forename><forename type="middle">S</forename><surname>Furey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nello</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Duffy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Bednarski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Schummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Haussler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="906" to="914" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elvezio</forename><forename type="middle">M</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ronchetti</surname></persName>
		</author>
		<title level="m">Robust Statistics</title>
		<meeting><address><addrLine>Hoboken, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley &amp; Sons, Inc</publisher>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Removing Noisy Mentions for Distant Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ander</forename><surname>Intxaurrondo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Congreso de la Sociedad Espaola para el Procesamiento del Lenguaje Natural</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Oier Lopez de Lacalle, and Eneko Agirre</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Detecting outlying samples in microarray data: A critical assessment of the effect of outliers on sample</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koji</forename><surname>Kadota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Tominaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Akiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsutoshi</forename><surname>Takahashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ChemBio Informatics Journal</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="45" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Detecting potential labeling errors in microarrays by data perturbation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Malossini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Blanzieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="2114" to="2121" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">On the design of robust classifiers for computer vision. IEEE International Conference Computer Vision and Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Masnadi-Shirazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The group lasso for logistic regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Van De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Geer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Buhlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="71" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Limitations of co-training for natural language learning from large datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shipeng</forename><surname>Vikas Raykar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><forename type="middle">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Jerebko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerardo</forename><forename type="middle">Hermosillo</forename><surname>Florin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Valadez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Bogoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Supervised learning from multiple experts: whom to trust when everyone lies a bit. ICML</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Improving Onboard Analysis of Hyperion Images by Filtering mislabelled Training Data Examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umaa</forename><surname>Rebbapragada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Mandrake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiri</forename><forename type="middle">L</forename><surname>Wagstaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damhnait</forename><surname>Gleeson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Castano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carla</forename><forename type="middle">E</forename><surname>Brodley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>IEEE Aerospace Conference</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Modeling Relations and Their Mentions without Labelled Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>ECML PKDD</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Filtering Email Spam in the Presence of Noisy User Feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>CEAS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Outlier Detection Using Nonconvex Penalized Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiyuan</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Art</forename><surname>Owen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="issue">494</biblScope>
			<biblScope unit="page">106</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F. Tjong Kim</forename><surname>Erik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien</forename><forename type="middle">De</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meulder</surname></persName>
		</author>
		<title level="m">Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition. CoNLL</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
		<title level="m">Dense Error Correction via l 1-Minimization IEEE Transactions on Information Theory</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
