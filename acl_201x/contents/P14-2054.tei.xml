<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:03+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Polynomial Time Joint Structural Inference for Sentence Compression</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Qian</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas</orgName>
								<address>
									<addrLine>at Dallas 800 W. Campbell Rd</addrLine>
									<settlement>Richardson</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas</orgName>
								<address>
									<addrLine>at Dallas 800 W. Campbell Rd</addrLine>
									<settlement>Richardson</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Polynomial Time Joint Structural Inference for Sentence Compression</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="327" to="332"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose two polynomial time inference algorithms to compress sentences under bigram and dependency-factored objectives. The first algorithm is exact and requires O(n 6) running time. It extends Eisner&apos;s cubic time parsing algorithm by using virtual dependency arcs to link deleted words. Two signatures are added to each span, indicating the number of deleted words and the rightmost kept word within the span. The second algorithm is a fast approximation of the first one. It relaxes the compression ratio constraint using Lagrangian relaxation, and thereby requires O(n 4) running time. Experimental results on the popular sentence compression corpus demonstrate the effectiveness and efficiency of our proposed approach.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sentence compression aims to shorten a sentence by removing uninformative words to reduce read- ing time. It has been widely used in compres- sive summarization ( <ref type="bibr" target="#b8">Liu and Liu, 2009;</ref><ref type="bibr" target="#b7">Li et al., 2013;</ref><ref type="bibr" target="#b9">Martins and Smith, 2009;</ref><ref type="bibr" target="#b2">Chali and Hasan, 2012;</ref><ref type="bibr" target="#b14">Qian and Liu, 2013)</ref>. To make the com- pressed sentence readable, some techniques con- sider the n-gram language models of the com- pressed sentence <ref type="bibr" target="#b3">(Clarke and Lapata, 2008;</ref><ref type="bibr" target="#b11">McDonald, 2006</ref>). Recent studies used a subtree dele- tion model for compression <ref type="bibr" target="#b0">(Berg-Kirkpatrick et al., 2011;</ref><ref type="bibr" target="#b12">Morita et al., 2013;</ref><ref type="bibr" target="#b14">Qian and Liu, 2013)</ref>, which deletes a word only if its modifier in the parse tree is deleted. Despite its empirical suc- cess, such a model fails to generate compressions that are not subject to the subtree constraint (see <ref type="figure" target="#fig_0">Figure 1</ref>). In fact, we parsed the Edinburgh sen- tence compression corpus using the MSTparser 1 , and found that 2561 of 5379 sentences (47.6%) do not satisfy the subtree deletion model.</p><p>Methods beyond the subtree model are also ex- plored. <ref type="bibr">Trevor et al.</ref> proposed synchronous tree substitution grammar <ref type="bibr" target="#b4">(Cohn and Lapata, 2009)</ref>, which allows local distortion of the tree topolo- gy and can thus naturally capture structural mis- matches. <ref type="bibr" target="#b6">(Genest and Lapalme, 2012;</ref><ref type="bibr" target="#b16">Thadani and McKeown, 2013)</ref> proposed the joint compres- sion model, which simultaneously considers the n- gram model and dependency parse tree of the com- pressed sentence. However, the time complexity greatly increases since the parse tree dynamical- ly depends on the compression. They used Integer Linear Programming (ILP) for inference which re- quires exponential running time in the worst case.</p><p>In this paper, we propose a new exact decod- ing algorithm for the joint model using dynam- ic programming. Our method extends Eisner's cubic time parsing algorithm by adding signa- tures to each span, which indicate the number of deleted words and the rightmost kept word with- in the span, resulting in O(n 6 ) time complexity and O(n 4 ) space complexity. We further propose a faster approximate algorithm based on Lagrangian relaxation, which has T O(n 4 ) running time and O(n 3 ) space complexity (T is the iteration num- ber in the subgradient decent algorithm the proposed approach is 10 times faster than a high-performance commercial ILP solver.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Definition</head><p>We define the sentence compression task as: given a sentence composed of n words, x = x 1 , . . . , x n , and a length L ≤ n, we need to remove (n − L) words from x, so that the sum of the weights of the dependency tree and word bigrams of the re- maining part is maximized. Formally, we solve the following optimization problem:</p><formula xml:id="formula_0">max z,y ∑ i w tok i z i + ∑ i,j w dep ij z i z j y ij (1) + ∑ i&lt;j w bgr ij z i z j ∏ i&lt;k&lt;j (1 − z k ) s.t. z is binary , ∑ i z i = L</formula><p>y is a projective parse tree over the subgraph:</p><formula xml:id="formula_1">{x i |z i = 1}</formula><p>where z is a binary vector, z i indicates x i is kep- t or not. y is a square matrix denoting the pro- jective dependency parse tree over the remaining words, y ij indicates if x i is the head of x j (note that each word has exactly one head). w tok i is the informativeness of x i , w bgr ij is the score of bigram x i x j in an n-gram model, w dep is the score of de- pendency arc x i → x j in an arc-factored depen- dency parsing model. Hence, the first part of the objective function is the total score of the kep- t words, the second and third parts are the scores of the parse tree and bigrams of the compressed sentence, z i z j ∏ i&lt;k&lt;j (1 − z k ) = 1 indicates both x i and x j are kept, and are adjacent after compres- sion. A graph illustration of the objective function is shown in <ref type="figure">Figure 2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Eisner's Cubic Time Parsing Algorithm</head><p>Throughout the paper, we assume that all the parse trees are projective. Our method is a generaliza- tion of Eisner's dynamic programming algorithm <ref type="bibr" target="#b5">(Eisner, 1996)</ref>, where two types of structures are used in each iteration, incomplete spans and com- plete spans. A span is a subtree over a number of consecutive words, with the leftmost or the right- most word as its root. An incomplete span denoted as I i j is a subtree inside a single arc x i → x j , with root x i . A complete span is denoted as C i j , where x i is the root of the subtree, and x j is the furthest descendant of x i .</p><p>Eisner's algorithm searches the optimal tree in a bottom up order. In each step, it merges two adjacent spans into a larger one. There are two rules for merging spans: one merges two complete spans into an incomplete span, the other merges an incomplete span and a complete span into a large complete span.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Exact O(n 6 ) Time Algorithm</head><p>First we consider an easy case, where the bigram scores w bgr ij in the objective function are ignored. The scores of unigrams w tok i can be transfered to the dependency arcs, so that we can remove al- l linear terms w tok i z i from the objective function. That is:</p><formula xml:id="formula_2">∑ i w tok i z i + ∑ i,j w dep ij z i z j y ij = ∑ i,j (w dep ij + w tok j )z i z j y ij</formula><p>This can be easily verifed. If z j = 0, then in both equations, all terms having z j are zero; If z j = 1, i.e., x j is kept, since it has exactly one head word x k in the compressed sentence, the sum of the terms having z j is w tok j + w dep kj for both equations. Therefore, we only need to consider the scores of arcs. For any compressed sentence, we could augment its dependency tree by adding a virtual arc i − 1 → i for each deleted word x i . If the first word x 1 is deleted, we connect it to the root of the parse tree x 0 , as shown in <ref type="figure" target="#fig_1">Figure 3</ref>. In this way, we derive a full parse tree of the original sentence. This is a one-to-one mapping. We can reversely get the the compressed parse tree by removing all virtual arcs from the full parse tree. We restrict the score of all the virtual arcs to be zero, so that scores of the two parse trees are equivalent. Now the problem is to search the optimal full parse tree with n − L virtual arcs.</p><p>We modify Eisner's algorithm by adding a sig- nature to each span indicating the number of vir- tual arcs within the span. Let I i j (k) and C i j (k) denote the incomplete and complete spans with k virtual arcs respectively. When merging two span- s, there are 4 cases, as shown in <ref type="figure" target="#fig_2">Figure 4</ref>.</p><p>• Case 1 Link two complete spans by a virtual arc :</p><formula xml:id="formula_3">I i i+1 (1) = C i i (0) + C i+1 i+1<label>(0)</label></formula><p>. The two complete spans must be single word- s, as the length of the virtual arc is 1.</p><p>• Case 2 Link two complete spans by a non- virtual arc:</p><formula xml:id="formula_4">I i j (k) = C i r (k ′ ) + C j r+1 (k ′′ ), k ′ + k ′′ = k.</formula><p>• Case 3 Merge an incomplete span and a com- plete span. The incomplete span is covered by a virtual arc:</p><formula xml:id="formula_5">I i j (j − i) = I i i+1 (1) + C i+1 j (j − i − 1).</formula><p>The number of the virtu- al arcs within C i+1 j must be j − i − 1, since the descendants of the modifier of a virtual arc x j must be removed.</p><p>• Case 4 Merge an incomplete span and a com- plete span. The incomplete span is covered by a non-virtual arc:</p><formula xml:id="formula_6">C i j (k) = I i r (k ′ ) + C r j (k ′′ ), k ′ + k ′′ = k.</formula><p>The score of the new span is the sum of the two spans. For case 2, the weight of the dependency arc i → j, w dep ij is also added to the final score. The root node is allowed to have two modifiers: one is the modifier in the compressed sentence, the other is the first word if it is removed.</p><p>For each combination, the algorithm enumer- ates the number of virtual arcs in the left and right spans, and the split position (e.g., k ′ , k ′′ , r in case 2), thus it takes O(n 3 ) running time. The overall time complexity is O(n 5 ) and the space complex- ity is O(n 3 ).</p><p>Next, we consider the bigram scores. The fol- lowing proposition is obvious. Proposition 1. For any right-headed span I i j or C i j , i &gt; j, words x i , x j must be kept.</p><p>Proof. Suppose x j is removed, there must be a vir- tual arc j − 1 → j which is a conflict with the fact that x j is the leftmost word. As x j is a descendant of x i , x i must be kept.</p><p>When merging two spans, a new bigram is cre- ated, which connects the rightmost kept words in the left span and the leftmost kept word in the right span. According to the proposition above, if the right span is right-headed, its leftmost word is kep- t. If the right span is left-headed, there are two cases: its leftmost word is kept, or no word in the span is kept. In any case, we only need to consider the leftmost word in the right span.</p><p>Let I i j (k, p) and C i j (k, p) denote the single and complete span with k virtual arcs and the right- most kept word x p . According to the proposition above, we have, for any right-headed span p = i.</p><p>We slightly modify the two merging rules above, and obtain:</p><p>• Case 2' Link two complete spans by a non-virtual arc:</p><formula xml:id="formula_7">I i j (k, j) = C i r (k ′ , p) + C j r+1 (k ′′ , j), k ′ + k ′′ = k.</formula><p>The score of the new span is the sum of the two spans plus w dep ij + w bgr p,r+1 .</p><p>• Case 4' Merge an incomplete span and a complete span. The incomplete span is cov- ered by a non-virtual arc. For left-headed spans, the rule is C i j (k, q) = I i r (k ′ , p) + C r j (k ′′ , q), k ′ + k ′′ = k, and the score of the new span is the sum of the two span- s plus w bgr pr ; for right-headed spans, the rule is C i j (k, i) = I i r (k ′ , i) + C r j (k ′′ , r), and the score of the new span is the sum of the two spans.</p><p>The modified algorithm requires O(n 6 ) running time and O(n 4 ) space complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Approximate O(n 4 ) Time Algorithm</head><p>In this section, we propose an approximate algo- rithm where the length constraint ∑ i z i = L is re- laxed by Lagrangian Relaxation. The relaxed ver- sion of Problem <ref type="formula">(1)</ref> is</p><formula xml:id="formula_8">min λ max z,y ∑ i w tok i z i + ∑ i,j w dep ij z i z j y ij (2) + ∑ i&lt;j w bgr ij z i z j ∏ i&lt;k&lt;j (1 − z k ) +λ( ∑ i z i − L) s.t. z is binary</formula><p>y is a projective parse tree over the subgraph:</p><formula xml:id="formula_9">{x i |z i = 1}</formula><p>Fixing λ, the optimal z, y can be found using a simpler version of the algorithm above. We drop the signature of the virtual arc number from each span, and thus obtain an O(n 4 ) time algorithm. S- pace complexity is O(n 3 ). Fixing z, y, the dual variable is updated by</p><formula xml:id="formula_10">λ = λ + α(L − ∑ i z i )</formula><p>where α &gt; 0 is the learning rate. In this paper, our choice of α is the same as ( <ref type="bibr" target="#b15">Rush et al., 2010</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data and Settings</head><p>We evaluate our method on the data set from <ref type="bibr" target="#b3">(Clarke and Lapata, 2008)</ref>. It includes 82 newswire articles with manually produced com- pression for each sentence. We use the same par- titions as <ref type="bibr" target="#b9">(Martins and Smith, 2009)</ref>, i.e., 1,188 sentences for training and 441 for testing.</p><p>Our model is discriminative -the scores of the unigrams, bigrams and dependency arcs are the linear functions of features, that is,</p><formula xml:id="formula_11">w tok i = v T f (x i ),</formula><p>where f is the feature vector of x i , and v is the weight vector of features. The learning task is to estimate the feature weight vector based on the manually compressed sentences.</p><p>We run a second order dependency parser trained on the English Penn Treebank corpus to generate the parse trees of the compressed sen- tences. Then we augment these parse trees by adding virtual arcs and get the full parse trees of their corresponding original sentences. In this way, the annoation is transformed into a set of sentences with their augmented parse trees. The learning task is similar to training a parser. We run a CRF based POS tagger to generate POS related features.</p><p>We adopt the compression evaluation metric as used in <ref type="bibr" target="#b9">(Martins and Smith, 2009</ref>) that measures the macro F-measure for the retained unigrams (F ugr ), and the one used in <ref type="bibr" target="#b3">(Clarke and Lapata, 2008</ref>) that calculates the F1 score of the grammat- ical relations labeled by RASP <ref type="bibr" target="#b1">(Briscoe and Carroll, 2002</ref>).</p><p>We compare our method with other 4 state-of- the-art systems. The first is linear chain CRFs, where the compression task is casted as a bina- ry sequence labeling problem. It usually achieves high unigram F1 score but low grammatical rela- tion F1 score since it only considers the local inter- dependence between adjacent words. The second is the subtree deletion model <ref type="bibr" target="#b0">(Berg-Kirkpatrick et al., 2011</ref>) which is solved by integer linear pro- gramming (ILP) 2 . The third one is the bigram model proposed by <ref type="bibr" target="#b11">McDonald (McDonald, 2006</ref>) which adopts dynamic programming for efficient inference. The last one jointly infers tree struc- tures alongside bigrams using ILP <ref type="bibr" target="#b16">(Thadani and McKeown, 2013)</ref>. For fair comparison, system- s were restricted to produce compressions that matched their average gold compression rate if possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Features</head><p>Three types of features are used to learn our mod- el: unigram features, bigram features and depen- dency features, as shown in <ref type="table" target="#tab_1">Table 1</ref>. We also use the in-between features proposed by (McDonald et Features for unigram xi wi−2, wi−1, wi, wi+1, wi+2 ti−2, ti−1, ti, ti+1, ti+2 witi wi−1wi, wiwi+1 ti−2ti−1, ti−1ti, titi+1, ti+1ti+2 ti−2ti−1ti, ti−1titi+1, titi+1ti+2 whether wi is a stopword Features for selected bigram xixj distance between the two words: j − i wiwj, wi−1wj, wi+1wj, wiwj−1, wiwj+1 titj, ti−1tj, ti+1tj, titj−1, titj+1 Concatenation of the templates above {tit k tj|i &lt; k &lt; j} Dependency Features for arc x h → xm distance between the head and modifier h − m dependency type direction of the dependency arc (left/right) w h wm, w h−1 wm, w h+1 wm, w h wm−1, w h wm+1 t h tm, t h−1 tm, t h+1 tm, t h tm−1, t h tm+1 t h−1 t h tm−1tm, t h t h+1 tm−1tm t h−1 t h tmtm+1, t h t h+1 tmtm+1 Concatenation of the templates above {t h t k tm|x k lies between x h and xm}  <ref type="bibr">al., 2005</ref>), which were shown to be very effective for dependency parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>We show the comparison results in <ref type="table" target="#tab_3">Table 2</ref>. As expected, the joint models (ours and TM13) con- sistently outperform the subtree deletion model, s- ince the joint models do not suffer from the sub- tree restriction. They also outperform McDon- ald's, demonstrating the effectiveness of consid- ering the grammar structure for compression. It is not surprising that CRFs achieve high unigram F scores but low syntactic F scores as they do not  , syn- tactic F1 score (RASP), and compression speed (seconds per sentence). C Rate is the compression ratio of the system generated output. For fair com- parison, systems were restricted to produce com- pressions that matched their average gold com- pression rate if possible.</p><p>consider the fluency of the compressed sentence. Compared with TM13's system, our model with exact decoding is not significantly faster due to the high order of the time complexity. On the oth- er hand, our approximate approach is much more efficient, about 10 times faster than TM13' sys- tem, and achieves competitive accuracy with the exact approach. Note that it is worth pointing out that the exact approach can output compressed sentences of all lengths, whereas the approximate method can only output one sentence at a specific compression rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proposed two polynomial time decoding algorithms using joint inference for sen- tence compression. The first one is an exac- t dynamic programming algorithm, and requires O(n 6 ) running time. This one does not show significant advantage in speed over ILP. The sec- ond one is an approximation of the first algorith- m. It adopts Lagrangian relaxation to eliminate the compression ratio constraint, yielding lower time complexity T O(n 4 ). In practice it achieves nearly the same accuracy as the exact one, but is much faster. <ref type="bibr">3</ref> The main assumption of our method is that the dependency parse tree is projective, which is not true for some other languages. In that case, our method is invalid, but <ref type="bibr" target="#b16">(Thadani and McKeown, 2013</ref>) still works. In the future, we will study the non-projective cases based on the recent parsing techniques for 1-endpoint-crossing trees <ref type="bibr" target="#b13">(Pitler et al., 2013</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The compressed sentence is not a subtree of the original sentence. Words in gray are removed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Connect deleted words using virtual arcs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Merging rules for dependency-factored sentence compression. Incomplete spans and complete spans are represented by trapezoids and triangles respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Feature templates. w i denotes the word form of token x i and t i denotes the POS tag of x i .</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Comparison results under various quality 
metrics, including unigram F1 score (F uni )</table></figure>

			<note place="foot" n="2"> We use Gurobi as the ILP solver in the paper. http://www.gurobi.com/</note>

			<note place="foot" n="3"> Our code is available at http://code.google.com/p/sentcompress/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank three anonymous reviewers for their valuable comments. This work is partly support-ed by NSF award IIS-0845484 and DARPA under Contract No. FA8750-13-2-0041. Any opinion-s expressed in this material are those of the au-thors and do not necessarily reflect the views of the funding agencies.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Jointly learning to extract and compress</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-HLT</title>
		<meeting>ACL-HLT</meeting>
		<imprint>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="481" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Robust accurate statistical annotation of general text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carroll</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the effectiveness of using sentence compression models for query-focused multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yllias</forename><surname>Chali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="457" to="474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Global inference for sentence compression: An integer linear programming approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res. (JAIR)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="399" to="429" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sentence compression as tree transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Int. Res</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="637" to="674" />
			<date type="published" when="2009-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Three new probabilistic models for dependency parsing: an exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">M</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fully abstractive approach to guided summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Genest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Lapalme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL</title>
		<meeting>the ACL</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="354" to="358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Document summarization via guided sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuliang</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">From extractive to abstractive meeting summaries: Can it be done by sentence compression?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACLIJCNLP 2009</title>
		<meeting>ACLIJCNLP 2009</meeting>
		<imprint>
			<date type="published" when="2009-08" />
			<biblScope unit="page" from="261" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Summarization with a joint model for sentence extraction and compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing</title>
		<meeting>the Workshop on Integer Linear Programming for Natural Langauge Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Online large-margin training of dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Discriminative Sentence Compression with Soft Syntactic Constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2006-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Subtree extractive summarization via submodular maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajime</forename><surname>Morita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryohei</forename><surname>Sasano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroya</forename><surname>Takamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="page" from="1023" to="1032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Finding optimal 1-endpoint-crossing trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampath</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast joint compression and summarization via graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1492" to="1502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On dual decomposition and linear programming relaxations for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Alexander M Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sentence compression with joint structural inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kapil</forename><surname>Thadani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL</title>
		<meeting>the CoNLL</meeting>
		<imprint>
			<date type="published" when="2013-08" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
