<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Riemannian Optimization for Skip-Gram Negative Sampling</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Fonarev</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksii</forename><surname>Grinchuk</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gleb</forename><surname>Gusev</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Serdyukov</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Oseledets</surname></persName>
						</author>
						<title level="a" type="main">Riemannian Optimization for Skip-Gram Negative Sampling</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2028" to="2036"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1185</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Skip-Gram Negative Sampling (SGNS) word embedding model, well known by its implementation in &quot;word2vec&quot; software, is usually optimized by stochastic gradient descent. However, the optimization of SGNS objective can be viewed as a problem of searching for a good matrix with the low-rank constraint. The most standard way to solve this type of problems is to apply Riemannian optimization framework to optimize the SGNS objective over the manifold of required low-rank matrices. In this paper, we propose an algorithm that optimizes SGNS objective using Riemannian optimization and demonstrates its superiority over popular competitors , such as the original method to train SGNS and SVD over SPPMI matrix.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper, we consider the problem of embed- ding words into a low-dimensional space in order to measure the semantic similarity between them. As an example, how to find whether the word "table" is semantically more similar to the word "stool" than to the word "sky"? That is achieved by constructing a low-dimensional vector repre- sentation for each word and measuring similarity between the words as the similarity between the corresponding vectors.</p><p>One of the most popular word embedding mod- els ( <ref type="bibr" target="#b14">Mikolov et al., 2013</ref>) is a discriminative neu- ral network that optimizes Skip-Gram Negative Sampling (SGNS) objective (see Equation 3). It aims at predicting whether two words can be found close to each other within a text. As shown in Sec- tion 2, the process of word embeddings training * The first two authors contributed equally to this work using SGNS can be divided into two general steps with clear objectives:</p><p>Step 1. Search for a low-rank matrix X that pro- vides a good SGNS objective value;</p><p>Step 2. Search for a good low-rank representation X = W C in terms of linguistic metrics, where W is a matrix of word embeddings and C is a matrix of so-called context em- beddings.</p><p>Unfortunately, most previous approaches mixed these two steps into a single one, what entails a not completely correct formulation of the optimization problem. For example, popular approaches to train embeddings (including the original "word2vec" implementation) do not take into account that the objective from Step 1 depends only on the prod- uct X = W C : instead of straightforward com- puting of the derivative w.r.t. X, these methods are explicitly based on the derivatives w.r.t. W and C, what complicates the optimization proce- dure. Moreover, such approaches do not take into account that parametrization W C of matrix X is non-unique and Step 2 is required. Indeed, for any invertible matrix S, we have</p><formula xml:id="formula_0">X = W 1 C 1 = W 1 SS −1 C 1 = W 2 C 2</formula><p>, therefore, solutions W 1 C 1 and W 2 C 2 are equally good in terms of the SGNS objective but entail different cosine similarities between embeddings and, as a result, different performance in terms of linguistic metrics (see Section 4.2 for details).</p><p>A successful attempt to follow the above de- scribed steps, which outperforms the original SGNS optimization approach in terms of various linguistic tasks, was proposed in ( ). In order to obtain a low-rank matrix X on Step 1, the method reduces the dimensional- ity of Shifted Positive Pointwise Mutual Informa-tion (SPPMI) matrix via Singular Value Decom- position (SVD). On Step 2, it computes embed- dings W and C via a simple formula that depends on the factors obtained by SVD. However, this method has one important limitation: SVD pro- vides a solution to a surrogate optimization prob- lem, which has no direct relation to the SGNS ob- jective. In fact, SVD minimizes the Mean Squared Error (MSE) between X and SPPMI matrix, what does not lead to minimization of SGNS objec- tive in general (see Section 6.1 and Section 4.2 in ( ) for details).</p><p>These issues bring us to the main idea of our paper: while keeping the low-rank matrix search setup on Step 1, optimize the original SGNS objective directly. This leads to an opti- mization problem over matrix X with the low- rank constraint, which is often ( <ref type="bibr" target="#b15">Mishra et al., 2014</ref>) solved by applying Riemannian optimiza- tion framework <ref type="bibr" target="#b20">(Udriste, 1994)</ref>. In our paper, we use the projector-splitting algorithm <ref type="bibr" target="#b13">(Lubich and Oseledets, 2014)</ref>, which is easy to implement and has low computational complexity. Of course, Step 2 may be improved as well, but we regard this as a direction of future work.</p><p>As a result, our approach achieves the signif- icant improvement in terms of SGNS optimiza- tion on Step 1 and, moreover, the improvement on Step 1 entails the improvement on Step 2 in terms of linguistic metrics. That is why, the proposed two-step decomposition of the problem makes sense, what, most importantly, opens the way to applying even more advanced approaches based on it (e.g., more advanced Riemannian opti- mization techniques for Step 1 or a more sophisti- cated treatment of Step 2).</p><p>To summarize, the main contributions of our pa- per are:</p><p>• We reformulated the problem of SGNS word embedding learning as a two-step procedure with clear objectives;</p><p>• For Step 1, we developed an algorithm based on Riemannian optimization framework that optimizes SGNS objective over low-rank ma- trix X directly;</p><p>• Our algorithm outperforms state-of-the-art competitors in terms of SGNS objective and the semantic similarity linguistic met- ric ( <ref type="bibr" target="#b14">Mikolov et al., 2013;</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Setting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Skip-Gram Negative Sampling</head><p>In this paper, we consider the Skip-Gram Negative Sampling (SGNS) word embedding model ( <ref type="bibr" target="#b14">Mikolov et al., 2013)</ref>, which is a prob- abilistic discriminative model. Assume we have a text corpus given as a sequence of words w 1 , . . . , w n , where n may be larger than 10 12 and w i ∈ V W belongs to a vocabulary of words V W . A context c ∈ V C of the word w i is a word from set {w i−L , ..., w i−1 , w i+1 , ..., w i+L } for some fixed window size L. Let w, c ∈ R d be the word em- beddings of word w and context c, respectively. Assume they are specified by the following map- pings:</p><formula xml:id="formula_1">W : V W → R d , C : V C → R d .</formula><p>The ultimate goal of SGNS word embedding train- ing is to fit good mappings W and C.</p><p>Let D be a multiset of all word-context pairs observed in the corpus. In the SGNS model, the probability that word-context pair (w, c) is ob- served in the corpus is modeled as a following dsitribution:</p><formula xml:id="formula_2">P (#(w, c) = 0|w, c) = = σ(w, c) = 1 1 + exp(−−w, c) ,<label>(1)</label></formula><p>where #(w, c) is the number of times the pair (w, c) appears in D and x, y is the scalar product of vectors x and y. Number d is a hyper- parameter that adjusts the flexibility of the model. It usually takes values from tens to hundreds. In order to collect a training set, we take all pairs (w, c) from D as positive examples and k randomly generated pairs (w, c) as negative ones. The number of times the word w and the context c appear in D can be computed as</p><formula xml:id="formula_3">#(w) = c∈Vc #(w, c),</formula><formula xml:id="formula_4">#(c) = w∈Vw #(w, c)</formula><p>accordingly. Then negative examples are gener- ated from the distribution defined by #(c) coun- ters:</p><formula xml:id="formula_5">P D (c) = #(c) |D| .</formula><p>In this way, we have a model maximizing the following logarithmic likelihood objective for all word-context pairs (w, c):</p><formula xml:id="formula_6">l wc = #(w, c)(log σ(w, c)+ +k · E c ∼P D log σ(−−w, c )).<label>(2)</label></formula><p>In order to maximize the objective over all obser- vations for each pair (w, c), we arrive at the fol- lowing SGNS optimization problem over all pos- sible mappings W and C:</p><formula xml:id="formula_7">l = w∈V W c∈V C (#(w, c)(log σ(w, c)+ +k · E c ∼P D log σ(−−w, c ))) → max W,C .<label>(3)</label></formula><p>Usually, this optimization is done via the stochas- tic gradient descent procedure that is performed during passing through the corpus ( <ref type="bibr" target="#b14">Mikolov et al., 2013;</ref><ref type="bibr" target="#b17">Rong, 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Optimization over Low-Rank Matrices</head><p>Relying on the prospect proposed in ( ), let us show that the optimization problem given by (3) can be considered as a prob- lem of searching for a matrix that maximizes a certain objective function and has the rank-d con- straint (Step 1 in the scheme described in Sec- tion 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">SGNS Loss Function</head><p>As shown in ( , the logarithmic likelihood (3) can be represented as the sum of l w,c (w, c) over all pairs (w, c), where l w,c (w, c) has the following form:</p><formula xml:id="formula_8">l w,c (w, c) = #(w, c) log σ(w, c)+ +k #(w)#(c) |D| log σ(−−w, c).<label>(4)</label></formula><p>A crucial observation is that this loss function de- pends only on the scalar product w, c but not on embeddings w and c separately:</p><formula xml:id="formula_9">l w,c (w, c) = f w,c (x w,c ),</formula><p>where f w,c (x w,c ) = a w,c log σ(x w,c )+b w,c log σ(−x w,c ), and x w,c is the scalar product w, c, and</p><formula xml:id="formula_10">a w,c = #(w, c), b w,c = k #(w)#(c) |D|</formula><p>are constants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Matrix Notation</head><p>Denote |V W | as n and |V C | as m. Let W ∈ R n×d and C ∈ R m×d be matrices, where each row w ∈ R d of matrix W is the word embedding of the cor- responding word w and each row c ∈ R d of ma- trix C is the context embedding of the correspond- ing context c. Then the elements of the product of these matrices</p><formula xml:id="formula_11">X = W C</formula><p>are the scalar products x w,c of all pairs (w, c):</p><formula xml:id="formula_12">X = (x w,c ), w ∈ V W , c ∈ V C .</formula><p>Note that this matrix has rank d, because X equals to the product of two matrices with sizes (n × d) and (d × m). Now we can write SGNS objective given by <ref type="formula" target="#formula_7">(3)</ref> as a function of X:</p><formula xml:id="formula_13">F (X) = w∈V W c∈V C f w,c (x w,c ), F : R n×m → R.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(5) This arrives us at the following proposition:</head><p>Proposition 1 SGNS optimization problem given by (3) can be rewritten in the following con- strained form:</p><formula xml:id="formula_14">maximize X∈R n×m F (X), subject to X ∈ M d ,<label>(6)</label></formula><p>where M d is the manifold <ref type="bibr" target="#b20">(Udriste, 1994)</ref> of all matrices in R n×m with rank d:</p><formula xml:id="formula_15">M d = {X ∈ R n×m : rank(X) = d}.</formula><p>The key idea of this paper is to solve the opti- mization problem given by (6) via the framework of Riemannian optimization, which we introduce in Section 3.</p><p>Important to note that this prospect does not suppose the optimization over parameters W and C directly. This entails the optimization in the space with</p><formula xml:id="formula_16">((n + m − d) · d) degrees of free- dom (Mukherjee et al., 2015) instead of ((n + m) · d)</formula><p>, what simplifies the optimization process (see Section 5 for the experimental results).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Computing Embeddings from a Low-Rank Solution</head><p>Once X is found, we need to recover W and C such that X = W C (Step 2 in the scheme described in Section 1). This problem does not have a unique solution, since if (W, C) satisfy this equation, then W S −1 and CS satisfy it as well for any non-singular matrix S. Moreover, different solutions may achieve different values of the lin- guistic metrics (see Section 4.2 for details). While our paper focuses on Step 1, we use, for Step 2, a heuristic approach that was proposed in ( <ref type="bibr" target="#b12">Levy et al., 2015)</ref> and it shows good results in practice. We compute SVD of X in the form</p><formula xml:id="formula_17">X = U ΣV ,</formula><p>where U and V have orthonormal columns, and Σ is the diagonal matrix, and use</p><formula xml:id="formula_18">W = U √ Σ, C = V √ Σ</formula><p>as matrices of embeddings. A simple justification of this solution is the fol- lowing: we need to map words into vectors in a way that similar words would have similar embed- dings in terms of cosine similarities:</p><formula xml:id="formula_19">cos(w 1 , w 2 ) = w 1 , w 2 w 1 · w 2 .</formula><p>It is reasonable to assume that two words are sim- ilar, if they share contexts. Therefore, we can esti- mate the similarity of two words w 1 , w 2 as</p><formula xml:id="formula_20">s(w 1 , w 2 ) = c∈V C x w 1 ,c · x w 2 ,c ,</formula><p>what is the element of the matrix XX with in- dices (w 1 , w 2 ). Note that</p><formula xml:id="formula_21">XX = U ΣV V ΣU = U Σ 2 U .</formula><p>If we choose W = U Σ, we exactly ob- tain w 1 , w 2 = s(w 1 , w 2 ), since W W = XX in this case. That is, the cosine similar- ity of the embeddings w 1 , w 2 coincides with the intuitive similarity s(w 1 , w 2 ). However, scaling by √ Σ instead of Σ was shown in ( <ref type="bibr" target="#b12">Levy et al., 2015</ref>) to be a better solution in experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>3.1 Riemannian Optimization</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">General Scheme</head><p>The main idea of Riemannian optimiza- tion <ref type="bibr" target="#b20">(Udriste, 1994)</ref> is to consider (6) as a constrained optimization problem. Assume we have an approximated solution X i on a current step of the optimization process, where i is the step number. In order to improve X i , the next step of the standard gradient ascent outputs the point</p><formula xml:id="formula_22">X i + F (X i ),</formula><p>where F (X i ) is the gradient of objective F at the point X i . Note that the gradient F (X i ) can be naturally considered as a matrix in R n×m . Point X i + F (X i ) leaves the manifold M d , because its rank is generally greater than d. That is why Riemannian optimization methods map point</p><formula xml:id="formula_23">X i + F (X i ) back to manifold M d .</formula><p>The standard Riemannian gradient method first projects the gradient step onto the tangent space at the current point X i and then retracts it back to the manifold:</p><formula xml:id="formula_24">X i+1 = R (P T M (X i + F (X i ))),</formula><p>where R is the retraction operator, and P T M is the projection onto the tangent space.</p><p>Although the optimization problem is non- convex, Riemannian optimization methods show good performance on it. Theoretical properties and convergence guarantees of such methods are discussed in (Wei et al., 2016) more thoroughly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Projector-Splitting Algorithm</head><p>In our paper, we use a simplified version of such approach that retracts point X i + F (X i ) directly to the manifold and does not require projection onto the tangent space P T M as illustrated in <ref type="figure" target="#fig_1">Fig- ure 1</ref>:</p><formula xml:id="formula_25">X i+1 = R(X i + F (X i )).</formula><p>Intuitively, retractor R finds a rank-d matrix on the manifold M d that is similar to high-rank ma- trix X i + F (X i ) in terms of Frobenius norm. How can we do it? The most straightforward way to reduce the rank of X i + F (X i ) is to perform the SVD, which keeps d largest singular values of it:</p><formula xml:id="formula_26">1: U i+1 , S i+1 , V i+1 ← SVD(X i + F (X i )), 2: X i+1 ← U i+1 S i+1 V i+1 .<label>(7)</label></formula><p>However, it is computationally expensive. In- stead of this approach, we use the projector- splitting method <ref type="bibr" target="#b13">(Lubich and Oseledets, 2014)</ref>, which is a second-order retraction onto the man- ifold (for details, see the review (Absil and Os- eledets, 2015)). Its practical implementation is</p><formula xml:id="formula_27">3. RELATED WORK Mikolov main [?] Levi main [?] rFi Xi = UiSiV T i Xi+1 = Ui+1Si+1V T i+1 retraction</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSIONS</head><p>Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. WOODSTOCK '97 El Paso, Texas USA Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$15.00.</p><p>.</p><note type="other">CONCLUSIONS . RELATED WORK Mikolov main [?] Levi main [?]</note><formula xml:id="formula_28">rFi Xi = UiSiV T i Xi+1 = Ui+1Si+1V T i+1</formula><p>etraction Md . CONCLUSIONS ermission to make digital or hard copies of all or part of this work for ersonal or classroom use is granted without fee provided that copies are ot made or distributed for profit or commercial advantage and that copies ear this notice and the full citation on the first page. To copy otherwise, to publish, to post on servers or to redistribute to lists, requires prior specific ermission and/or a fee. OODSTOCK '97 El Paso, Texas USA opyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$15.00.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><note type="other">sdfdsf 2. CONCLUSIONS</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">RELATED WORK</head><p>Mikolov main <ref type="bibr">[?]</ref> Levi main <ref type="bibr">[?]</ref> rF (Xi)</p><formula xml:id="formula_29">Xi + rF (Xi) Xi = UiSiV T i Xi Xi+1 Xi+1 = Ui+1Si+1V T i+1</formula><p>retraction Md 4. CONCLUSIONS Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. WOODSTOCK '97 El Paso, Texas USA Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$15.00.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">RELATED WORK</head><p>Mikolov main <ref type="bibr">[?]</ref> Levi main <ref type="bibr">[?]</ref> rF (Xi)</p><formula xml:id="formula_30">Xi + rF (Xi) Xi = UiSiV T i Xi Xi+1 Xi+1 = Ui+1Si+1V T i+1</formula><p>retraction Md</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSIONS</head><p>Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. WOODSTOCK '97 El Paso, Texas USA Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$15.00.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><note type="other">sdfdsf 2. CONCLUSIONS 3. RELATED WORK Mikolov main [?]</note><p>Levi main <ref type="bibr">[?]</ref> rF (Xi)   </p><formula xml:id="formula_31">Xi + rF (Xi) Xi = UiSiV T i Xi Xi+1 Xi+1 = Ui+1Si+1V T i+1</formula><formula xml:id="formula_32">X i = U i S i V i ,<label>(8)</label></formula><p>where matrices U i ∈ R n×d and V i ∈ R m×d have d orthonormal columns and S i ∈ R d×d . Then we need to perform two QR-decompositions to retract point X i + F (X i ) back to the manifold:</p><formula xml:id="formula_33">1: U i+1 , S i+1 ← QR ((X i + F (X i ))V i ) , 2: V i+1 , S i+1 ← QR (X i + F (X i )) U i+1 , 3: X i+1 ← U i+1 S i+1 V i+1 .</formula><p>In this way, we always keep the solution X i+1 = U i+1 S i+1 V i+1 on the manifold M d and in the form <ref type="bibr">(8)</ref>.</p><p>What is important, we only need to com- pute F (X i ), so the gradients with respect to U , S and V are never computed explicitly, thus avoiding the subtle case where S is close to singu- lar (so-called singular (critical) point on the man- ifold). Indeed, the gradient with respect to U (while keeping the orthogonality constraints) can be written ( <ref type="bibr" target="#b8">Koch and Lubich, 2007)</ref> as:</p><formula xml:id="formula_34">∂F ∂U = ∂F ∂X V S −1 ,</formula><p>which means that the gradient will be large if S is close to singular. The projector-splitting scheme is free from this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Algorithm</head><p>In case of SGNS objective given by (5), an element of gradient F has the form:</p><formula xml:id="formula_35">(F (X)) w,c = ∂f w,c (x w,c ) ∂x w,c = = #(w, c) · σ (−x w,c ) − k #(w)#(c) |D| · σ (x w,c ) .</formula><p>To make the method more flexible in terms of con- vergence properties, we additionally use λ ∈ R, which is a step size parameter. In this case, re- tractor R returns</p><formula xml:id="formula_36">X i + λF (X i ) instead of X i + F (X i ) onto the manifold.</formula><p>The whole optimization procedure is summa- rized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training Models</head><p>We compare our method ("RO-SGNS" in the ta- bles) performance to two baselines: SGNS embed- dings optimized via Stochastic Gradient Descent, implemented in the original "word2vec", ("SGD- SGNS" in the tables) ( <ref type="bibr" target="#b14">Mikolov et al., 2013)</ref> and embeddings obtained by SVD over SPPMI ma- trix ("SVD-SPPMI" in the tables) ( ). We have also experimented with the blockwise alternating optimization over factors W and C, but the results are almost the same to SGD results, that is why we do not to include them into the paper. The source code of our experiments is available online <ref type="bibr">1</ref> .</p><p>The models were trained on English Wikipedia "enwik9" corpus 2 , which was previously used in most papers on this topic. Like in previous stud- ies, we counted only the words which occur more than 200 times in the training corpus ( <ref type="bibr" target="#b14">Mikolov et al., 2013)</ref>. As a re- sult, we obtained a vocabulary of 24292 unique tokens (set of words V W and set of contexts V C are equal). The size of the context window was set to 5 for all experiments, as it was done in ( <ref type="bibr" target="#b14">Mikolov et al., 2013</ref>). We conduct three series of experiments: for dimen- sionality d = 100, d = 200, and d = 500.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Riemannian Optimization for SGNS</head><p>Require: Dimentionality d, initialization W 0 and C 0 , step size λ, gradient function F : R n×m → R n×m , number of iterations K Ensure: Factor W ∈ R n×d 1:</p><formula xml:id="formula_37">X 0 ← W 0 C 0 # get an initial point at the manifold 2: U 0 , S 0 , V 0 ← SVD(X 0 )</formula><p># compute the first point satisfying the low-rank constraint 3: for i ← 1, . . . , K do 4:</p><formula xml:id="formula_38">U i , S i ← QR ((X i−1 + λF (X i−1 ))V i−1 )</formula><p># perform one step of the block power method 5:</p><formula xml:id="formula_39">V i , S i ← QR (X i−1 + λF (X i−1 )) U i</formula><p>6:</p><formula xml:id="formula_40">X i ← U i S i V i</formula><p># update the point at the manifold 7: end for</p><formula xml:id="formula_41">8: U, Σ, V ← SVD(X K ) 9: W ← U √ Σ # compute word embeddings 10: return W</formula><p>Optimization step size is chosen to be small enough to avoid huge gradient values. However, thorough choice of λ does not result in a signifi- cant difference in performance (this parameter was tuned on the training data only, the exact values used in experiments are reported below).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation</head><p>We evaluate word embeddings via the word simi- larity task. We use the following popular datasets for this purpose: "wordsim-353" <ref type="bibr" target="#b4">((Finkelstein et al., 2001</ref>); 3 datasets), "simlex-999" ( <ref type="bibr" target="#b6">Hill et al., 2016)</ref> and "men" ( <ref type="bibr" target="#b3">Bruni et al., 2014</ref>). Original "wordsim-353" dataset is a mixture of the word pairs for both word similarity and word related- ness tasks. This dataset was split ( <ref type="bibr" target="#b1">Agirre et al., 2009</ref>) into two intersecting parts: "wordsim-sim" ("ws-sim" in the tables) and "wordsim-rel" ("ws- rel" in the tables) to separate the words from dif- ferent tasks. In our experiments, we use both of them on a par with the full version of "wordsim- 353" ("ws-full" in the tables). Each dataset con- tains word pairs together with assessor-assigned similarity scores for each pair. As a quality mea- sure, we use Spearman's correlation between these human ratings and cosine similarities for each pair. We call this quality metric linguistic in our paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results of Experiments</head><p>First of all, we compare the value of SGNS objec- tive obtained by the methods. The comparison is demonstrated in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>We see that SGD-SGNS and SVD-SPPMI methods provide quite similar results, however, the proposed method obtains significantly better  SGNS values, what proves the feasibility of us- ing Riemannian optimization framework in SGNS optimization problem. It is interesting to note that SVD-SPPMI method, which does not opti- mize SGNS objective directly, obtains better re- sults than SGD-SGNS method, which aims at opti- mizing SGNS. This fact additionally confirms the idea described in Section 2.2.2 that the indepen- dent optimization over parameters W and C may decrease the performance. However, the target performance measure of embedding models is the correlation between se- mantic similarity and human assessment (Sec- tion 4.2). <ref type="table" target="#tab_3">Table 2</ref> presents the comparison of the methods in terms of it. We see that our method outperforms the competitors on all datasets except for "men" dataset where it obtains slightly worse results. Moreover, it is important that the higher dimension entails higher performance gain of our method in comparison to the competitors.</p><p>To understand how our model improves or de- grades the performance in comparison to the base- line, we found several words, whose neighbors in terms of cosine distance change significantly. Ta- ble 3 demonstrates neighbors of the words "five", "he" and "main" for both SVD-SPPMI and RO- SGNS models. A neighbor is marked bold if we suppose that it has similar semantic meaning to the Dim. d Algorithm ws-sim ws-rel ws-full simlex men    <ref type="table">Table 4</ref>: Examples of the semantic neighbors from 11th to 20th obtained for the word "usa" by all three methods. Top-10 neighbors for all three methods are exact names of states.</p><formula xml:id="formula_42">d = 100 SGD-SGNS 0</formula><p>source word. First of all, we notice that our model produces much better neighbors of the words de- scribing digits or numbers (see word "five" as an example). Similar situation happens for many other words, e.g. in case of "main" -the nearest neighbors contain 4 similar words for our model instead of 2 in case of SVD-SPPMI. The neigh- bourhood of "he" contains less semantically sim- ilar words in case of our model. However, it fil- ters out irrelevant words, such as "promptly" and "dumbledore". <ref type="table">Table 4</ref> contains the nearest words to the word "usa" from 11th to 20th. We marked names of USA states bold and did not represent top-10 near- est words as they are exactly names of states for all three models. Some non-bold words are ar- guably relevant as they present large USA cities ("akron", "burbank", "madison") or geographi- cal regions of several states ("midwest", "north- east", "southwest"), but there are also some com- pletely irrelevant words ("uk", "cities", "places") presented by first two models.</p><p>Our experiments show that the optimal number of iterations K in the optimization procedure and step size λ depend on the particular value of d. For d = 100, we have K = 7, λ = 5 · 10 −5 , for d = 200, we have K = 8, λ = 5 · 10 −5 , and for d = 500, we have K = 2, λ = 10 −4 . Moreover, the best results were obtained when SVD-SPPMI embeddings were used as an initialization of Rie- mannian optimization process. <ref type="figure">Figure 2</ref> illustrates how the correlation between semantic similarity and human assessment scores changes through iterations of our method. Optimal value of K is the same for both whole testing set and its 10-fold subsets chosen for cross-validation. The idea to stop optimization procedure on some iteration is also discussed in ( <ref type="bibr" target="#b10">Lai et al., 2015</ref>).</p><p>Training of the same dimensional models (d = 500) on English Wikipedia corpus using SGD-SGNS, SVD-SPPMI, RO-SGNS took 20 minutes, 10 minutes and 70 minutes respectively. Our method works slower, but not significantly. Moreover, since we were not focused on the code efficiency optimization, this time can be reduced.  <ref type="figure">Figure 2</ref>: Illustration of why it is important to choose the optimal iteration and stop optimization proce- dure after it. The graphs show semantic similarity metric in dependence on the iteration of optimization procedure. The embeddings obtained by SVD-SPPMI method were used as initialization. Parameters:</p><formula xml:id="formula_43">d = 200, λ = 5 · 10 −5 .</formula><p>6 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Word Embeddings</head><p>Skip-Gram Negative Sampling was introduced in ( <ref type="bibr" target="#b14">Mikolov et al., 2013)</ref>. The "negative sampling" approach is thoroughly described in , and the learning method is explained in <ref type="bibr" target="#b17">(Rong, 2014)</ref>. There are several open-source implementations of SGNS neural network, which is widely known as "word2vec". 12 As shown in Section 2.2, Skip-Gram Negative Sampling optimization can be reformulated as a problem of searching for a low-rank matrix. In or- der to be able to use out-of-the-box SVD for this task, the authors of ( ) used the surrogate version of SGNS as the objec- tive function. There are two general assumptions made in their algorithm that distinguish it from the SGNS optimization:</p><p>1. SVD optimizes Mean Squared Error (MSE) objective instead of SGNS loss function.</p><p>2. In order to avoid infinite elements in SPMI matrix, it is transformed in ad-hoc manner (SPPMI matrix) before applying SVD.</p><p>This makes the objective not interpretable in terms of the original task (3). As mentioned in ( ), SGNS objective weighs dif- ferent (w, c) pairs differently, unlike the SVD, which works with the same weight for all pairs and may entail the performance fall. The compre- hensive explanation of the relation between SGNS and SVD-SPPMI methods is provided in ( <ref type="bibr" target="#b7">Keerthi et al., 2015</ref> give a good overview of highly practical methods to improve these word embedding models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Riemannian Optimization</head><p>An introduction to optimization over Riemannian manifolds can be found in <ref type="bibr" target="#b20">(Udriste, 1994)</ref>. The overview of retractions of high rank matrices to low-rank manifolds is provided in <ref type="bibr" target="#b0">(Absil and Oseledets, 2015</ref>). The projector-splitting algorithm was introduced in <ref type="bibr" target="#b13">(Lubich and Oseledets, 2014)</ref>, and also was mentioned in <ref type="bibr" target="#b0">(Absil and Oseledets, 2015)</ref> as "Lie-Trotter retraction".</p><p>Riemannian optimization is succesfully applied to various data science problems: for example, matrix completion <ref type="bibr" target="#b21">(Vandereycken, 2013)</ref>, large- scale recommender systems ( <ref type="bibr" target="#b19">Tan et al., 2014</ref>), and tensor completion ( <ref type="bibr" target="#b9">Kressner et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>In our paper, we proposed the general two-step scheme of training SGNS word embedding model and introduced the algorithm that performs the search of a solution in the low-rank form via Riemannian optimization framework. We also demonstrated the superiority of our method by providing experimental comparison to existing state-of-the-art approaches.</p><p>Possible direction of future work is to apply more advanced optimization techniques to the Step 1 of the scheme proposed in Section 1 and to explore the Step 2 -obtaining embeddings with a given low-rank matrix.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. WOODSTOCK '97 El Paso, Texas USA Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$15.00.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Geometric interpretation of one step of projector-splitting optimization procedure: the gradient step an the retraction of the high-rank matrix X i +F (X i ) to the manifold of low-rank matrices M d. also quite intuitive: instead of computing the full SVD of X i + F (X i ) according to the gradient projection method, we use just one step of the block power numerical method (Bentbib and Kanber, 2015) which computes the SVD, what reduces the computational complexity. Let us keep the current point in the following factorized form:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>retraction Md 4. CONCLUSIONS Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior</figDesc><table>specific 
permission and/or a fee. 
WOODSTOCK '97 El Paso, Texas USA 
Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$15.00. 

sdfdsf 

2. CONCLUSIONS 

3. RELATED WORK 

Mikolov main [?] 
Levi main [?] 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Comparison of SGNS values (multiplied 
by 10 −9 ) obtained by the models. Larger is better. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Comparison of the methods in terms of the semantic similarity task. Each entry represents the 
Spearman's correlation between predicted similarities and the manually assessed ones. 

five 
he 
main 
SVD-SPPMI 
RO-SGNS 
SVD-SPPMI 
RO-SGNS 
SVD-SPPMI 
RO-SGNS 
Neighbors 
Dist. 
Neighbors 
Dist. 
Neighbors 
Dist. 
Neighbors 
Dist. 
Neighbors 
Dist. 
Neighbors 
Dist. 
lb 
0.748 
four 
0.999 
she 
0.918 
when 
0.904 
major 
0.631 
major 
0.689 
kg 
0.731 
three 
0.999 
was 
0.797 
had 
0.903 
busiest 
0.621 
important 
0.661 
mm 
0.670 
six 
0.997 
promptly 
0.742 
was 
0.901 
principal 
0.607 
line 
0.631 
mk 
0.651 
seven 
0.997 
having 
0.731 
who 
0.892 
nearest 
0.607 
external 
0.624 
lbf 
0.650 
eight 
0.996 
dumbledore 
0.731 
she 
0.884 
connecting 
0.591 
principal 
0.618 
per 
0.644 
and 
0.985 
him 
0.730 
by 
0.880 
linking 
0.588 
primary 
0.612 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Examples of the semantic neighbors obtained for words "five", "he" and "main". 

usa 
SGD-SGNS 
SVD-SPPMI 
RO-SGNS 
Neighbors 
Dist. 
Neighbors 
Dist. 
Neighbors 
Dist. 
akron 
0.536 
wisconsin 
0.700 
georgia 
0.707 
midwest 
0.535 
delaware 
0.693 
delaware 
0.706 
burbank 
0.534 
ohio 
0.691 
maryland 
0.705 
nevada 
0.534 
northeast 
0.690 
illinois 
0.704 
arizona 
0.533 
cities 
0.688 
madison 
0.703 
uk 
0.532 
southwest 
0.684 
arkansas 
0.699 
youngstown 
0.532 
places 
0.684 
dakota 
0.690 
utah 
0.530 
counties 
0.681 
tennessee 
0.689 
milwaukee 
0.530 
maryland 
0.680 
northeast 
0.687 
headquartered 
0.527 
dakota 
0.674 
nebraska 
0.686 

</table></figure>

			<note place="foot" n="1"> https://github.com/AlexGrinch/ro_sgns 2 http://mattmahoney.net/dc/textdata</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported by the Ministry of Education and Science of the Russian Federation (grant 14.756.31.0001).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Low-rank retractions: a survey and new results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P-A</forename><surname>Absil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oseledets</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Optimization and Applications</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="29" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A study on similarity and relatedness using distributional and wordnet-based approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Alfonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Kravalova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Pas¸capas¸ca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Soroa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Block power method for svd decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Bentbib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kanber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Analele Stiintifice Ale Unversitatii Ovidius Constanta-Seria Matematica</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="45" to="58" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multimodal distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Khanh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res.(JAIR)</title>
		<imprint>
			<biblScope unit="page" from="49" to="50" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Placing search in context: The concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="406" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">word2vec explained: deriving mikolov et al.&apos;s negativesampling word-embedding method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1402.3722</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Simlex-999: Evaluating semantic models with (genuine) similarity estimation. Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Sathiya Keerthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajiv</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khanna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02024</idno>
		<title level="m">Towards a better understanding of predict and count models</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dynamical low-rank approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Othmar</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Lubich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Matrix Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="434" to="454" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Low-rank tensor completion by riemannian optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kressner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Steinlechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Vandereycken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BIT Numerical Mathematics</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="447" to="468" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.05523</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural word embedding as implicit matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improving distributional similarity with lessons learned from word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="211" to="225" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A projector-splitting integrator for dynamical lowrank approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Lubich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oseledets</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BIT Numerical Mathematics</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="171" to="188" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fixed-rank matrix factorizations and riemannian low-rank optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bamdev</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Meyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="591" to="621" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Silvère Bonnabel, and Rodolphe Sepulchre</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On the degrees of freedom of reduced-rank estimators in multivariate regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="457" to="477" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Rong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.2738</idno>
		<title level="m">word2vec parameter learning explained</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Evaluation methods for unsupervised word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Labutov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Riemannian pursuit for big matrix recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ivor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1539" to="1547" />
		</imprint>
	</monogr>
	<note>Bart Vandereycken, and Sinno Jialin Pan</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Convex functions and optimization methods on Riemannian manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Constantin</forename><surname>Udriste</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">297</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Low-rank matrix completion by riemannian optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Vandereycken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1214" to="1236" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Guarantees of riemannian optimization for low rank matrix recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Feng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><forename type="middle">F</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shingyu</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Matrix Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1198" to="1222" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
