<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:17+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scalable Large-Margin Structured Learning: Theory and Algorithms</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2014-06">June 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lemao</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Graduate Center and Queens College</orgName>
								<orgName type="institution">City University of New York</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Motivations</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Scalable Large-Margin Structured Learning: Theory and Algorithms</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: Tutorials</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics: Tutorials <address><addrLine>Baltimore, Maryland, USA, 22</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="4" to="5"/>
							<date type="published" when="2014-06">June 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Much of NLP tries to map structured input (sen-tences) to some form of structured output (tag sequences , parse trees, semantic graphs, or trans-lated/paraphrased/compressed sentences). Thus structured prediction and its learning algorithm are of central importance to us NLP researchers. However, when applying machine learning to structured domains, we often face scalability issues for two reasons: 1. Even the fastest exact search algorithms for most NLP problems (such as parsing and translation) is too slow for repeated use on the training data, but approximate search (such as beam search) unfortunately breaks down the nice theoretical properties (such as convergence) of existing machine learning algorithms. 2. Even with inexact search, the scale of the training data in NLP still makes pure online learning (such as perceptron and MIRA) too slow on a single CPU. This tutorial reviews recent advances that address these two challenges. In particular, we will cover principled machine learning methods that are designed to work under vastly inexact search, and parallelization algorithms that speed up learning on multiple CPUs. We will also extend struc-tured learning to the latent variable setting, where in many NLP applications such as translation and semantic parsing the gold-standard derivation is hidden. 2 Contents 1. Overview of Structured Learning (a) key challenge 1: search efficiency (b) key challenge 2: interactions between search and learning 2. Structured Perceptron (a) the basic algorithm (b) the geometry of convergence proof (c) voted and averaged perceptrons, and efficient implementation tricks (d) applications in tagging, parsing, etc. 3. Structured Perceptron under Inexact Search (a) convergence theory breaks under inexact search (b) early update (c) violation-fixing perceptron (d) applications in tagging, parsing, etc.-coffee break-4. From Perceptron to MIRA (a) 1-best MIRA; geometric solution (b) k-best MIRA; hildreth algorithm (c) MIRA with all constraints; loss-augmented decoding (d) MIRA under inexact search 5. Large-Margin Structured Learning with Latent Variables (a) examples: machine translation, semantic parsing, transliteration (b) separability condition and convergence proof (c) latent-variable perceptron under inexact search (d) applications in machine translation 6. Parallelizing Large-Margin Structured Learning (a) iterative parameter mixing (IPM) (b) minibatch perceptron and MIRA 4</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Motivations</head><p>Much of NLP tries to map structured input (sen- tences) to some form of structured output (tag se- quences, parse trees, semantic graphs, or trans- lated/paraphrased/compressed sentences). Thus structured prediction and its learning algorithm are of central importance to us NLP researchers. However, when applying machine learning to structured domains, we often face scalability is- sues for two reasons:</p><p>1. Even the fastest exact search algorithms for most NLP problems (such as parsing and translation) is too slow for repeated use on the training data, but approximate search (such as beam search) unfortunately breaks down the nice theoretical properties (such as con- vergence) of existing machine learning algo- rithms.</p><p>2. Even with inexact search, the scale of the training data in NLP still makes pure online learning (such as perceptron and MIRA) too slow on a single CPU.</p><p>This tutorial reviews recent advances that ad- dress these two challenges. In particular, we will cover principled machine learning methods that are designed to work under vastly inexact search, and parallelization algorithms that speed up learn- ing on multiple CPUs. We will also extend struc- tured learning to the latent variable setting, where in many NLP applications such as translation and semantic parsing the gold-standard derivation is hidden. Liang Huang is an Assistant Professor at the City University of New York (CUNY). He received his Ph.D. in 2008 from Penn and has worked as a Research Scientist at Google and a Re- search Assistant Professor at USC/ISI. His work is mainly on the theoretical aspects (algorithms and formalisms) of computational linguistics, as well as theory and algorithms of structured learn- ing. He has received a Best Paper Award at ACL 2008, several best paper nominations <ref type="bibr">(ACL 2007</ref><ref type="bibr">, EMNLP 2008</ref><ref type="bibr">, and ACL 2010</ref>, two Google Fac- ulty Research Awards <ref type="bibr">(2010 and 2013)</ref>, and a Uni- versity Graduate Teaching Prize at <ref type="bibr">Penn (2005)</ref>. He has given two tutorials at COLING 2008 and NAACL 2009, being the most popular tutorial at both venues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Contents</head><p>Kai Zhao is a Ph.D. candidate at the City Univer- sity of New York (CUNY), working with Liang Huang. He received his B.S. from the Univer- sity of Science and Technology in China (USTC). He has published on structured prediction, online learning, machine translation, and parsing algo- rithms. He was a summer intern with IBM TJ Wat- son Research Center in 2013.</p><p>Lemao Liu is a postdoctoral research associate at the City University of New York (CUNY), work- ing with Liang Huang. He received his Ph.D. from the Harbin Institute of Technology in 2013. Much of his Ph.D. work was done while visiting NICT, Japan, under Taro Watanabe. His research area is machine translation and machine learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 .</head><label>1</label><figDesc>Overview of Structured Learning (a) key challenge 1: search efficiency (b) key challenge 2: interactions between search and learning 2. Structured Perceptron (a) the basic algorithm (b) the geometry of convergence proof (c) voted and averaged perceptrons, and ef- ficient implementation tricks (d) applications in tagging, parsing, etc. 3. Structured Perceptron under Inexact Search (a) convergence theory breaks under inex- act search (b) early update (c) violation-fixing perceptron (d) applications in tagging, parsing, etc. -coffee break- 4. From Perceptron to MIRA (a) 1-best MIRA; geometric solution (b) k-best MIRA; hildreth algorithm (c) MIRA with all constraints; loss- augmented decoding (d) MIRA under inexact search 5. Large-Margin Structured Learning with La- tent Variables (a) examples: machine translation, seman- tic parsing, transliteration (b) separability condition and convergence proof (c) latent-variable perceptron under inexact search (d) applications in machine translation 6. Parallelizing Large-Margin Structured Learning (a) iterative parameter mixing (IPM) (b) minibatch perceptron and MIRA 4</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
