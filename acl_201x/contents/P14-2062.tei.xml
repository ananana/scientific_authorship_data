<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Experiments with crowdsourced re-annotation of a POS tagging data set</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Language Technology</orgName>
								<orgName type="institution">University of Copenhagen Njalsgade 140</orgName>
								<address>
									<postCode>2300</postCode>
									<settlement>Copenhagen</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Language Technology</orgName>
								<orgName type="institution">University of Copenhagen Njalsgade 140</orgName>
								<address>
									<postCode>2300</postCode>
									<settlement>Copenhagen</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Language Technology</orgName>
								<orgName type="institution">University of Copenhagen Njalsgade 140</orgName>
								<address>
									<postCode>2300</postCode>
									<settlement>Copenhagen</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Experiments with crowdsourced re-annotation of a POS tagging data set</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="377" to="382"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Crowdsourcing lets us collect multiple annotations for an item from several annota-tors. Typically, these are annotations for non-sequential classification tasks. While there has been some work on crowdsourc-ing named entity annotations, researchers have largely assumed that syntactic tasks such as part-of-speech (POS) tagging cannot be crowdsourced. This paper shows that workers can actually annotate sequential data almost as well as experts. Further , we show that the models learned from crowdsourced annotations fare as well as the models learned from expert annotations in downstream tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Training good predictive NLP models typically re- quires annotated data, but getting professional an- notators to build useful data sets is often time- consuming and expensive. <ref type="bibr" target="#b22">Snow et al. (2008)</ref> showed, however, that crowdsourced annotations can produce similar results to annotations made by experts. Crowdsourcing services such as Ama- zon's Mechanical Turk has since been successfully used for various annotation tasks in NLP ( <ref type="bibr" target="#b7">Jha et al., 2010;</ref><ref type="bibr" target="#b0">Callison-Burch and Dredze, 2010)</ref>.</p><p>However, most applications of crowdsourcing in NLP have been concerned with classification problems, such as document classification and constructing lexica <ref type="bibr" target="#b0">(Callison-Burch and Dredze, 2010)</ref>. A large part of NLP problems, however, are structured prediction tasks. Typically, sequence labeling tasks employ a larger set of labels than classification problems, as well as complex inter- actions between the annotations. Disagreement among annotators is therefore potentially higher, and the task of annotating structured data thus harder.</p><p>Only a few recent studies have investi- gated crowdsourcing sequential tasks; specifically, named entity recognition ( <ref type="bibr">Finin et al., 2010;</ref><ref type="bibr" target="#b19">Rodrigues et al., 2013)</ref>. Results for this are good. However, named entities typically use only few la- bels (LOC, ORG, and PER), and the data contains mostly non-entities, so the complexity is manage- able. The question of whether a more linguisti- cally involved structured task like part-of-speech (POS) tagging can be crowdsourced has remained largely unaddressed. <ref type="bibr">1</ref> In this paper, we investigate how well lay anno- tators can produce POS labels for Twitter data. In our setup, we present annotators with one word at a time, with a minimal surrounding context (two words to each side). Our choice of annotating Twitter data is not coincidental: with the short- lived nature of Twitter messages, models quickly lose predictive power <ref type="bibr" target="#b6">(Eisenstein, 2013)</ref>, and re- training models on new samples of more represen- tative data becomes necessary. Expensive profes- sional annotation may be prohibitive for keeping NLP models up-to-date with linguistic and topical changes on Twitter. We use a minimum of instruc- tions and require few qualifications.</p><p>Obviously, lay annotation is generally less re- liable than professional annotation. It is there- fore common to aggregate over multiple annota- tions for the same item to get more robust anno- tations. In this paper we compare two aggrega- tion schemes, namely majority voting (MV) and MACE ( <ref type="bibr">Hovy et al., 2013</ref>). We also show how we can use Wiktionary, a crowdsourced lexicon, to fil- ter crowdsourced annotations. We evaluate the an- notations in several ways: (a) by testing their ac- curacy with respect to a gold standard, (b) by eval- uating the performance of POS models trained on the annotations across several existing data sets, as well as (c) by applying our models in down- stream tasks. We show that with minimal context and annotation effort, we can produce structured annotations of near-expert quality. We also show that these annotations lead to better POS tagging models than previous models learned from crowd- sourced lexicons ( <ref type="bibr" target="#b11">Li et al., 2012</ref>). Finally, we show that models learned from these annotations are competitive with models learned from expert annotations on various downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Our Approach</head><p>We crowdsource the training section of the data from <ref type="bibr">Gimpel et al. (2011)</ref>  <ref type="bibr">2</ref> with POS tags. We use Crowdflower, 3 to collect five annotations for each word, and then find the most likely label for each word among the possible annotations. See <ref type="figure" target="#fig_0">Figure  1</ref> for an example. If the correct label is not among the annotations, we are unable to recover the cor- rect answer. This was the case for 1497 instances in our data (cf. the token ":" in the example). We thus report on oracle score, i.e., the best label sequence that could possibly be found, which is correct except for the missing tokens. Note that while we report agreement between the crowd- sourced annotations and the crowdsourced anno- tations, our main evaluations are based on models learned from expert vs. crowdsourced annotations and downstream applications thereof (chunking and NER). We take care in evaluating our models across different data sets to avoid biasing our evaluations to particular annotations. All the data sets used in our experiments are publicly available at http://lowlands.ku.dk/results/.</p><formula xml:id="formula_0">x Z y @USER NOUN,NOUN,X,NOUN,-,NOUN NOUN :</formula><p>.,.,-,.,.,. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Crowdsourcing Sequential Annotation</head><p>In order to use the annotations to train models that can be applied across various data sets, i.e., mak- ing out-of-sample evaluation possible (see Section 5), we follow <ref type="bibr">Hovy et al. (2014)</ref> in using the uni- versal tag set ( <ref type="bibr" target="#b15">Petrov et al., 2012</ref>) with 12 labels. Annotators were given a bold-faced word with two words on either side and asked to select the most appropriate tag from a drop down menu. For each tag, we spell out the name of the syntactic category, and provide a few example words. See <ref type="figure" target="#fig_1">Figure 2</ref> for a screenshot of the interface. Annotators were also told that words can belong to several classes, depending on the context. No additional guidelines were given.</p><p>Only trusted annotators (in Crowdflower: Bronze skills) that had answered correctly on 4 gold tokens (randomly chosen from a set of 20 gold tokens provided by the authors) were allowed to submit annotations. In total, 177 individual annotators supplied answers. We paid annotators a reward of $0.05 for 10 tokens. The full data set contains 14,619 tokens. Completion of the task took slightly less than 10 days. Contributors were very satisfied with the task (4.5 on a scale from 1 to 5). In particular, they felt instructions were clear (4.4/5), and that the pay was reasonable (4.1/5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Label Aggregation</head><p>After collecting the annotations, we need to aggre- gate the annotations to derive a single answer for each token. In the simplest scheme, we choose the majority label, i.e., the label picked by most an- notators. In case of ties, we select the final label at random. Since this is a stochastic process, we average results over 100 runs. We refer to this as MAJORITY VOTING (MV). Note that in MV we trust all annotators to the same degree. However, crowdsourcing attracts people with different mo-tives, and not all of them are equally reliable- even the ones with Bronze level. Ideally, we would like to factor this into our decision process.</p><p>We use MACE 4 ( <ref type="bibr">Hovy et al., 2013</ref>) as our sec- ond scheme to learn both the most likely answer and a competence estimate for each of the annota- tors. MACE treats annotator competence and the correct answer as hidden variables and estimates their parameters via EM <ref type="bibr" target="#b3">(Dempster et al., 1977)</ref>. We use MACE with default parameter settings to give us the weighted average for each annotated example.</p><p>Finally, we also tried applying the joint learn- ing scheme in <ref type="bibr" target="#b19">Rodrigues et al. (2013)</ref>, but their scheme requires that entire sequences are anno- tated by the same annotators, which we don't have, and it expects BIO sequences, rather than POS tags.</p><p>Dictionaries Decoding tasks profit from the use of dictionaries <ref type="bibr" target="#b13">(Merialdo, 1994;</ref><ref type="bibr" target="#b8">Johnson, 2007;</ref><ref type="bibr" target="#b16">Ravi and Knight, 2009)</ref> by restricting the number of tags that need to be considered for each word, also known as type constraints <ref type="bibr">(Täckström et al., 2013</ref>). We follow <ref type="bibr" target="#b11">Li et al. (2012)</ref> in including Wiktionary information as type constraints into our decoding: if a word is found in Wiktionary, we disregard all annotations that are not licensed by the dictionary entry. If the word is not found in Wiktionary, or if none of its annotations is licensed by Wiktionary, we keep the original annotations. Since we aggregate annotations independently (unlike Viterbi decoding), we basically use Wik- tionary as a pre-filtering step, such that MV and MACE only operate on the reduced annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Each of the two aggregation schemes above pro- duces a final label sequencê y for our training cor- pus. We evaluate the resulting annotated data in three ways.</p><p>1. We comparê y to the available expert annota- tion on the training data. This tells us how similar lay annotation is to professional annotation.</p><p>2. Ultimately, we want to use structured anno- tations for supervised training, where annotation quality influences model performance on held-out test data. To test this, we train a CRF model ( <ref type="bibr" target="#b10">Lafferty et al., 2001</ref>) with simple orthographic features and word clusters ( <ref type="bibr" target="#b14">Owoputi et al., 2013)</ref> on the annotated Twitter data described in <ref type="bibr">Gimpel et al. (2011)</ref>. Leaving out the dedicated test set to avoid in-sample bias, we evaluate our mod- els across three data sets: RITTER (the 10% test split of the data in <ref type="bibr" target="#b18">Ritter et al. (2011</ref><ref type="bibr">) used in Derczynski et al. (2013</ref>), the test set from <ref type="bibr">Foster et al. (2011)</ref>, and the data set described in <ref type="bibr">Hovy et al. (2014)</ref>.</p><p>We will make the preprocessed data sets avail- able to the public to facilitate comparison. In ad- dition to a supervised model trained on expert an- notations, we compare our tagging accuracy with that of a weakly supervised system ( <ref type="bibr" target="#b11">Li et al., 2012)</ref> re-trained on 400,000 unlabeled tweets to adapt to Twitter, but using a crowdsourced lexicon, namely Wiktionary, to constrain inference. We use param- eter settings from <ref type="bibr" target="#b11">Li et al. (2012)</ref>, as well as their Wikipedia dump, available from their project web- site. <ref type="bibr">5</ref> 3. POS tagging is often the first step for further analysis, such as chunking, parsing, etc. We test the downstream performance of the POS models from the previous step on chunking and NER. We use the models to annotate the training data portion of each task with POS tags, and use them as features in a chunking and NER model. For both tasks, we train a CRF model on the respective (POS-augmented) training set, and evaluate it on several held-out test sets. For chunking, we use the test sets from Foster et al. (2011) and <ref type="bibr" target="#b18">Ritter et al. (2011)</ref> (with the splits from <ref type="bibr" target="#b4">Derczynski et al. (2013)</ref>). For NER, we use data from <ref type="bibr">Finin et al. (2010)</ref> and again <ref type="bibr" target="#b18">Ritter et al. (2011)</ref>. For chunking, we follow <ref type="bibr" target="#b20">Sha and Pereira (2003)</ref> for the set of features, including token and POS information. For NER, we use standard features, including POS tags (from the previous experiments), indicators for hyphens, digits, single quotes, upper/lowercase, 3-character prefix and suffix information, and Brown word cluster features 6 with 2,4,8,16 bitstring prefixes estimated from a large Twitter corpus ( <ref type="bibr" target="#b14">Owoputi et al., 2013</ref>). We report macro-averages over all these data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>Agreement with expert annotators <ref type="table">Table 1</ref> shows the accuracy of each aggregation compared to the gold labels. The crowdsourced annotations majority 79.54 MACE-EM 79.89 majority+Wiktionary 80.58 MACE-EM+Wiktionary 80.75 oracle 89.63 <ref type="table">Table 1</ref>: Accuracy (%) of different annotations wrt gold data aggregated using MV agree with the expert anno- tations in 79.54% of the cases. If we pre-filter the data using Wiktionary, the agreement becomes 80.58%. MACE leads to higher agreement with expert annotations under both conditions (79.89 and 80.75). The small difference indicates that annotators are consistent and largely reliable, thus confirming the Bronze-level qualification we required. Both schemes cannot recover the correct answer for the 1497 cases where none of the crowdsourced labels matched the gold label, i.e. y / ∈ Z i . The best possible result either of them could achieve (the oracle) would be matching all but the missing labels, an agreement of 89.63%.</p><p>Most of the cases where the correct label was not among the annotations belong to a small set of confusions. The most frequent was mislabeling ":" and ". . .", both mapped to X. Annotators mostly decided to label these tokens as punctu- ation (.). They also predominantly labeled your, my and this as PRON (for the former two), and a variety of labels for the latter, when the gold label is DET.  <ref type="table">Table 2</ref>: POS tagging accuracies (%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RITTER FOSTER HOVY</head><p>Effect on POS Tagging Accuracy Usually, we don't want to match a gold standard, but we rather want to create new annotated training data. Crowdsourcing matches our gold standard to about 80%, but the question remains how useful this data is when training models on it. After all, inter-annotator agreement among professional an- notators on this task is only around 90% ( <ref type="bibr">Gimpel et al., 2011;</ref><ref type="bibr">Hovy et al., 2014</ref>). In order to evalu- ate how much each aggregation scheme influences tagging performance of the resulting model, we train separate models on each scheme's annota- tions and test on the same four data sets. <ref type="table">Table  2</ref> shows the results. Note that the differences be- tween the four schemes are insignificant. More importantly, however, POS tagging accuracy us- ing crowdsourced annotations are on average only 2.6% worse than gold using professional annota- tions. On the other hand, performance is much better than the weakly supervised approach by <ref type="bibr" target="#b11">Li et al. (2012)</ref>, which only relies on a crowdsourced POS lexicon.  <ref type="table">Table 3</ref>: Downstream accuracy for chunking (l) and NER (r) of models using POS. <ref type="table">Table 3</ref> shows the accuracy when using the POS models trained in the previous evaluation step. Note that we present the average over the two data sets used for each task. Note also how the Wiktionary con- straints lead to improvements in downstream per- formance. In chunking, we see that using the crowdsourced annotations leads to worse perfor- mance than using the professional annotations. For NER, however, we find that some of the POS taggers trained on aggregated data produce bet- ter NER performance than POS taggers trained on expert-annotated gold data. Since the only dif- ference between models are the respective POS features, the results suggest that at least for some tasks, POS taggers learned from crowdsourced an- notations may be as good as those learned from expert annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Downstream Performance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>There is considerable work in the literature on modeling answer correctness and annotator com- petence as latent variables <ref type="bibr" target="#b2">(Dawid and Skene, 1979;</ref><ref type="bibr" target="#b21">Smyth et al., 1995;</ref><ref type="bibr" target="#b1">Carpenter, 2008;</ref><ref type="bibr" target="#b25">Whitehill et al., 2009;</ref><ref type="bibr" target="#b24">Welinder et al., 2010;</ref><ref type="bibr" target="#b26">Yan et al., 2010;</ref><ref type="bibr" target="#b17">Raykar and Yu, 2012)</ref>. <ref type="bibr" target="#b19">Rodrigues et al. (2013)</ref> recently presented a sequential model for this. They estimate annotator competence as la- tent variables in a CRF model using EM. They evaluate their approach on synthetic and NER data annotated on Mechanical Turk, showing improve- ments over the MV baselines and the multi-label model by <ref type="bibr" target="#b5">Dredze et al. (2009)</ref>. The latter do not model annotator reliability but rather model label priors by integrating them into the CRF objective, and re-estimating them during learning. Both re- quire annotators to supply a full sentence, while we use minimal context, which requires less anno- tator commitment and makes the task more flexi- ble. Unfortunately, we could not run those mod- els on our data due to label incompatibility and the fact that we typically do not have complete se- quences annotated by the same annotators. Mainzer (2011) actually presents an earlier pa- per on crowdsourcing POS tagging. However, it differs from our approach in several ways. It uses the Penn Treebank tag set to annotate Wikipedia data (which is much more canonical than Twitter) via a Java applet. The applet automatically labels certain categories, and only presents the users with a series of multiple choice questions for the re- mainder. This is highly effective, as it eliminates some sources of possible disagreement. In con- trast, we do not pre-label any tokens, but always present the annotators with all labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We use crowdsourcing to collect POS annotations with minimal context (five-word windows). While the performance of POS models learned from this data is still slightly below that of models trained on expert annotations, models learned from aggregations approach oracle performance for POS tagging. In general, we find that the use of a dictionary tends to make aggregations more useful, irrespective of aggregation method. For some downstream tasks, models using the aggregated POS tags perform even better than models using expert-annotated tags.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Five annotations per token, supplied by 6 different annotators (-= missing annotation), gold label y. θ = competence values for each annotator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Screen shot of the annotation interface on Crowdflower</figDesc><graphic url="image-1.png" coords="2,307.28,164.24,226.77,103.66" type="bitmap" /></figure>

			<note place="foot" n="1"> One of the reviewers alerted us to an unpublished masters thesis, which uses pre-annotation to reduce tagging to fewer multiple-choice questions. See Related Work section for details.</note>

			<note place="foot" n="2"> http://www.ark.cs.cmu.edu/TweetNLP/ 3 http://crowdflower.com</note>

			<note place="foot" n="4"> http://www.isi.edu/publications/ licensed-sw/mace/</note>

			<note place="foot" n="5"> https://code.google.com/p/ wikily-supervised-pos-tagger/ 6 http://www.ark.cs.cmu.edu/TweetNLP/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous review-ers for valuable comments and feedback. This re-search is funded by the ERC Starting Grant LOW-</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Creating Speech and Language Data With Amazon&apos;s Mechanical Turk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon&apos;s Mechanical Turk</title>
		<meeting>the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon&apos;s Mechanical Turk</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Multilevel Bayesian models of categorical data annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Carpenter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>LingPipe</publisher>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Maximum likelihood estimation of observer error-rates using the EM algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Dawid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><forename type="middle">M</forename><surname>Skene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Statistics</title>
		<imprint>
			<biblScope unit="page" from="20" to="28" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><forename type="middle">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
	<note>Rubin</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Twitter part-of-speech tagging for all: overcoming sparse and noisy data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Derczynski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalina</forename><surname>Bontcheva</surname></persName>
		</author>
		<editor>RANLP</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sequence learning from data with multiple labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><forename type="middle">Pratim</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML/PKDD Workshop on Learning from Multi-Label Data</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">What to do about bad language on the internet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Corpus creation for new genres: A crowdsourced approach to pp attachment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mukund</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kapil</forename><surname>Thadani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Rosenthal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon&apos;s Mechanical Turk</title>
		<meeting>the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon&apos;s Mechanical Turk</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Why doesn&apos;t EM find good HMM POS-taggers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<title level="m">Natural Language Processing and Computational Natural Language Learning</title>
		<imprint>
			<publisher>EMNLP-CoNLL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Conditional random fields: probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Wiki-ly supervised part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">João</forename><surname>Graça</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Labeling parts of speech using untrained annotators on mechanical turk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><forename type="middle">Emil</forename><surname>Mainzer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>The Ohio State University</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Tagging English text with a probabilistic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Merialdo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="155" to="171" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improved part-of-speech tagging for online conversational text with word clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olutobi</forename><surname>Owoputi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A universal part-of-speech tagset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Minimized Models for Unsupervised Part-of-Speech Tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP. Association for Computational Linguistics</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Eliminating Spammers and Ranking Annotators for Crowdsourced Labeling Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vikas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shipeng</forename><surname>Raykar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="491" to="518" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Named entity recognition in tweets: an experimental study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sequence labeling with multiple annotators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filipe</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardete</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Shallow parsing with conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Inferring ground truth from subjective labelling of Venus images. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padhraic</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Usama</forename><surname>Fayyad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Burl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Baldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="1085" to="1092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cheap and fast-but is it good? Evaluating non-expert annotations for natural language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Token and type constraints for cross-lingual part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TACL, Mar</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The multidimensional wisdom of crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Whose vote should count more: Optimal integration of labels from labelers of unknown expertise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Whitehill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Ruvolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingfan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Bergsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Movellan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="2035" to="2043" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Modeling annotator expertise: Learning when everybody knows a bit of something</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rómer</forename><surname>Rosales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glenn</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerardo</forename><surname>Hermosillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bogoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Moy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Dy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
