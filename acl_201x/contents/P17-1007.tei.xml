<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Skip-Gram - Zipf + Uniform = Vector Additivity</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Gittens</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Achlioptas</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
						</author>
						<title level="a" type="main">Skip-Gram - Zipf + Uniform = Vector Additivity</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="69" to="76"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1007</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In recent years word-embedding models have gained great popularity due to their remarkable performance on several tasks, including word analogy questions and caption generation. An unexpected &quot;side-effect&quot; of such models is that their vectors often exhibit compositionality, i.e., adding two word-vectors results in a vector that is only a small angle away from the vector of a word representing the semantic composite of the original words, e.g., &quot;man&quot; + &quot;royal&quot; = &quot;king&quot;. This work provides a theoretical justification for the presence of additive composi-tionality in word vectors learned using the Skip-Gram model. In particular, it shows that additive compositionality holds in an even stricter sense (small distance rather than small angle) under certain assumptions on the process generating the corpus. As a corollary, it explains the success of vector calculus in solving word analogies. When these assumptions do not hold, this work describes the correct non-linear composition operator. Finally, this work establishes a connection between the Skip-Gram model and the Sufficient Dimensionality Reduction (SDR) framework of Globerson and Tishby: the parameters of SDR models can be obtained from those of Skip-Gram models simply by adding information on symbol frequencies. This shows that Skip-Gram embeddings are optimal in the sense of Globerson and Tishby and, further, implies that the heuristics commonly used to approximately fit Skip-Gram models can be used to fit SDR models.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>In recent years word-embedding models have gained great popularity due to their remarkable performance on several tasks, including word analogy questions and cap- tion generation. An unexpected "side- effect" of such models is that their vectors often exhibit compositionality, i.e., adding two word-vectors results in a vector that is only a small angle away from the vector of a word representing the semantic com- posite of the original words, e.g., "man" + "royal" = "king".</p><p>This work provides a theoretical justifica- tion for the presence of additive composi- tionality in word vectors learned using the Skip-Gram model. In particular, it shows that additive compositionality holds in an even stricter sense (small distance rather than small angle) under certain assump- tions on the process generating the corpus. As a corollary, it explains the success of vector calculus in solving word analogies. When these assumptions do not hold, this work describes the correct non-linear com- position operator.</p><p>Finally, this work establishes a con- nection between the Skip-Gram model and the Sufficient Dimensionality Reduc- tion (SDR) framework of Globerson and Tishby: the parameters of SDR models can be obtained from those of Skip-Gram models simply by adding information on symbol frequencies. This shows that Skip- Gram embeddings are optimal in the sense of Globerson and Tishby and, further, im- plies that the heuristics commonly used to approximately fit Skip-Gram models can be used to fit SDR models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The strategy of representing words as vectors has a long history in computational linguistics and machine learning. The general idea is to find a map from words to vectors such that word- similarity and vector-similarity are in correspon- dence. Whilst vector-similarity can be readily quantified in terms of distances and angles, quan- tifying word-similarity is a more ambiguous task.</p><p>A key insight in that regard is to posit that the meaning of a word is captured by "the company it keeps" <ref type="bibr" target="#b4">(Firth, 1957)</ref> and, therefore, that two words that keep company with similar words are likely to be similar themselves.</p><p>In the simplest case, one seeks vectors whose inner products approximate the co-occurrence fre- quencies. In more sophisticated methods co- occurrences are reweighed to suppress the effect of more frequent words ( <ref type="bibr" target="#b15">Rohde et al., 2006</ref>) and/or to emphasize pairs of words whose co-occurrence frequency maximally deviates from the indepen- dence assumption <ref type="bibr" target="#b3">(Church and Hanks, 1990</ref>).</p><p>An alternative to seeking word-embeddings that reflect co-occurrence statistics is to extract the vectorial representation of words from non-linear statistical language models, specifically neural networks. ( <ref type="bibr" target="#b2">Bengio et al., 2003</ref>) already proposed (i) associating with each vocabulary word a fea- ture vector, (ii) expressing the probability func- tion of word sequences in terms of the feature vec- tors of the words in the sequence, and (iii) learn- ing simultaneously the vectors and the parame- ters of the probability function. This approach came into prominence recently through works of Mikolov et al. (see below) whose main departure from ( <ref type="bibr" target="#b2">Bengio et al., 2003</ref>) was to follow the sug- gestion of <ref type="bibr" target="#b11">(Mnih and Hinton, 2007)</ref> and trade- away the expressive capacity of general neural- network models for the scalability (to very large corpora) afforded by (the more restricted class of) log-linear models.</p><p>An unexpected side effect of deriving word- embeddings via neural networks is that the word- vectors produced appear to enjoy (approximate) additive compositionality: adding two word- vectors often results in a vector whose nearest word-vector belongs to the word capturing the composition of the added words, e.g., "man" + "royal" = "king" ( <ref type="bibr" target="#b10">Mikolov et al., 2013c</ref>). This un- expected property allows one to use these vectors to answer word-analogy questions algebraically, e.g., answering the question "Man is to king as woman is to " by returning the word whose word-vector is nearest to the vector</p><formula xml:id="formula_0">v(king) -v(man) + v(woman).</formula><p>In this work we focus on explaining the source of this phenomenon for the most prominent such model, namely the Skip-Gram model introduced in <ref type="bibr" target="#b8">(Mikolov et al., 2013a</ref>). The Skip-Gram model learns vector representations of words based on their patterns of co-occurrence in the training cor- pus as follows: it assigns to each word c in the vocabulary V , a "context" and a "target" vector, respectively u c and v c , which are to be used in or- der to predict the words that appear around each occurrence of c within a window of ∆ tokens. Specifically, the log probability of any target word w to occur at any position within distance ∆ of a context word c is taken to be proportional to the inner product between u c and v w , i.e., letting n = |V |,</p><formula xml:id="formula_1">p(w|c) = e u T c vw n i=1 e u T c v i .<label>(1)</label></formula><p>Further, Skip-Gram assumes that the conditional probability of each possible set of words in a win- dow around a context word c factorizes as the product of the respective conditional probabilities:</p><formula xml:id="formula_2">p(w −∆ , . . . , w ∆ |c) = ∆ δ=−∆ δ =0 p(w δ |c).<label>(2)</label></formula><p>(Mikolov et al., 2013a) proposed learning the Skip-Gram parameters on a training corpus by us- ing maximum likelihood estimation under (1) and (2). Thus, if w i denotes the i-th word in the train- ing corpus and T the length of the corpus, we seek the word vectors that maximize</p><formula xml:id="formula_3">1 T T i=1 ∆ δ=−∆ δ =0 log p(w i+δ |w i ) .<label>(3)</label></formula><p>As mentioned, the normalized context vectors obtained from maximizing <ref type="formula" target="#formula_3">(3)</ref> under <ref type="formula" target="#formula_1">(1)</ref> and <ref type="formula" target="#formula_2">(2)</ref> exhibit additive compositionality. For example, the cosine distance between the sum of the context vectors of the words "Vietnam" and "capital" and the context vector of the word "Hanoi" is small.</p><p>While there has been much interest in using algebraic operations on word vectors to carry out semantic operations like composition, and mathematically-flavored explanations have been offered (e.g., in the recent work <ref type="bibr" target="#b12">(Paperno and Baroni, 2016)</ref>), the only published work which at- tempts a rigorous theoretical understanding of this phenomenon is ( <ref type="bibr" target="#b0">Arora et al., 2016)</ref>. This work guarantees that word vectors can be recovered by factorizing the so-called PMI matrix, and that al- gebraic operations on these word vectors can be used to solve analogies, under certain conditions on the process that generated the training corpus. Specifically, the word vectors must be known a priori, before their recovery, and to have been generated by randomly scaling uniformly sampled vectors from the unit sphere 1 . Further, the ith word in the corpus must have been selected with proba- bility proportional to e u T w c i , where the "discourse" vector c i governs the topic of the corpus at the ith word. Finally, the discourse vector is assumed to evolve according to a random walk on the unit sphere that has a uniform stationary distribution.</p><p>By way of contrast, our results assume nothing a priori about the properties of the word vectors. In fact, the connection we establish between the Skip-Gram and the Sufficient Dimensionality Re- duction model of ( <ref type="bibr" target="#b5">Globerson and Tishby, 2003)</ref> shows that the word vectors learned by Skip-Gram are information-theoretically optimal. Further, the context word c in the Skip-Gram model essentially serves the role that the discourse vector does in the PMI model of ( <ref type="bibr" target="#b0">Arora et al., 2016)</ref>: the words neighboring c are selected with probability propor- tional to e u T c vw . We find the exact non-linear com- position operator when no assumptions are made on the context word. When an analogous assump- tion to that of ( <ref type="bibr" target="#b0">Arora et al., 2016</ref>) is made, that the context words are uniformly distributed, we prove that the composition operator reduces to vector ad- dition.</p><p>While our primary motivation has been to pro- vide a better theoretical understanding of word compositionality in the popular Skip-Gram model, our connection with the SDR method illuminates a much more general point about the practical ap- plicability of the Skip-Gram model. In particular, it addresses the question of whether, for a given corpus, fitting a Skip-Gram model will give good embeddings. Even if we are making reasonable linguistic assumptions about how to model words and the interdependencies of words in a corpus, it's not clear that these have to hold universally on all corpuses to which we apply Skip-Gram. However, the fact that when we fit a Skip-Gram model we are fitting an SDR model (up to fre- quency information), and the fact that SDR mod- els are information-theoretically optimal in a cer- tain sense, argues that regardless of whether the Skip-Gram assumptions hold, Skip-Gram always gives us optimal features in the following sense: the learned context embeddings and target embed- dings preserve the maximal amount of mutual in- formation between any pair of random variables X and Y consistent with the observed co-occurence matrix, where Y is the target word and X is the predictor word (in a min-max sense, since there are many ways of coupling X and Y , each of which may have different amounts of mutual in- formation). Importantly, this statement requires no assumptions on the distribution P (X, Y ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Compositionality of Skip-Gram</head><p>In this section, we first give a mathematical formu- lation of the intuitive notion of compositionality of words. We then prove that the composition oper- ator for the Skip-Gram model in full generality is a non-linear function of the vectors of the words being composed. Under a single simplifying as- sumption, the operator linearizes and reduces to the addition of the word vectors. Finally, we ex- plain how linear compositionality allows for solv- ing word analogies with vector algebra.</p><p>A natural way of capturing the compositional- ity of words is to say that the set of context words c 1 , . . . , c m has the same meaning as the single word c if for every other word w,</p><formula xml:id="formula_4">p(w|c 1 , . . . , c m ) = p(w|c) .</formula><p>Although this is an intuitively satisfying defini- tion, we never expect it to hold exactly; instead, we replace exact equality with the minimization of KL-divergence. That is, we state that the best candidate for having the same meaning as the set of context words C is the word</p><formula xml:id="formula_5">arg min c∈V D KL (p(·|C) | p(·|c)) .<label>(4)</label></formula><p>We refer to any vector that minimizes (4) as a paraphrase of the set of words C. There are two natural concerns with (4). The first is that, in general, it is not clear how to define p(·|C). The second is that KL-divergence min- imization is a hard problem, as it involves opti- mization over many high dimensional probability distributions. Our main result shows that both of these problems go away for any language model that satisfies the following two assumptions:</p><p>A1. For every word c, there exists Z c such that for every word w,</p><formula xml:id="formula_6">p(w|c) = 1 Z c exp(u T c v w ) .<label>(5)</label></formula><p>A2. For every set of words C = {c 1 , c 2 , . . . , c m }, there exists Z C such that for every word w,</p><formula xml:id="formula_7">p(w|C) = p(w) 1−m Z C m i=1 p(w|c i ) .<label>(6)</label></formula><p>Clearly, the Skip-Gram model satisfies A1 by definition. We prove that it also satisfies A2 when m ≤ ∆ (Lemma 1). Next, we state a theorem that holds for any model satisfying assumptions A1 and A2, including the Skip-Gram model when m ≤ ∆. Theorem 1. In every word model that satisfies A1 and A2, for every set of words C = {c 1 , . . . , c m }, any paraphase c of C satisfies</p><formula xml:id="formula_8">w∈V p(w|c)v w = w∈V p(w|C)v w .<label>(7)</label></formula><p>Theorem 1 characterizes the composition opera- tor for any language model which satisfies our two assumptions; in general, this operator is not addi- tion. Instead, a paraphrase c is a vector such that the average word vector under p(·|c) matches that under p(·|C). When the expectations in (7) can be computed, the composition operator can be im- plemented by solving a non-linear system of equa- tions to find a vector u for which the left-hand side of (7) equals the right-hand side.</p><p>Our next result proves that although the compo- sition operator is nontrivial in the general case, to recover vector addition as the composition opera- tor, it suffices to assume that the word frequency is uniform.</p><p>Theorem 2. In every word model that satisfies A1, A2, and where p(w) = 1/|V | for every w ∈ V , the paraphrase of C = {c 1 , . . . , c m } is</p><formula xml:id="formula_9">u 1 + . . . + u m .</formula><p>As word frequencies are typically much closer to a Zipf distribution <ref type="bibr" target="#b14">(Piantadosi, 2014)</ref>, the uni- formity assumption of Theorem 2 is not realistic. That said, we feel it is important to point out that, as reported in <ref type="figure">(Mikolov et al., 2013b)</ref>, additivity captures compositionality more accurately when the training set is manipulated so that the prior dis- tribution of the words is made closer to uniform.</p><p>Using composition to solve analogies. It has been observed that word vectors trained using non- linear models like Skip-Gram tend to encode se- mantic relationships between words as linear re- lationships between the word vectors ( <ref type="bibr" target="#b9">Mikolov et al., 2013b;</ref><ref type="bibr" target="#b13">Pennington et al., 2014;</ref><ref type="bibr" target="#b7">Levy and Goldberg, 2014</ref>). In particular, analogies of the form "man:woman::king:?" can often be solved by taking ? to be the word in the vocabulary whose context vector has the smallest angle with u woman + (u king − u man ). Theorems 1 and 2 offer insight into the solution such analogy questions.</p><p>We first consider solving an analogy of the form "m:w::k:?"" in the case where the composition operator is nonlinear. The fact that m and w share a relationship means m is a paraphrase of the set of words {w, R}, where R is a set of words encod- ing the relationship between m and w. Similarly, the fact that k and ? share the same relationship means k is a paraphrase of the set of words {?, R}. By Theorem 1, we have that R and ? must satisfy We see that solving analogies when the compo- sition operator is nonlinear requires the solution of two highly nonlinear systems of equations. In sharp contrast, when the composition operator is linear, the solution of analogies delightfully re- duces to elementary vector algebra. To see this, we again begin with the assertion that the fact that m and w share a relationship means m is a para- phrase of the set of words {w, R}; Similarly, k is a paraphrase of {?, R}. By Theorem 2, u m = u w + u r and</p><formula xml:id="formula_10">u k = u ? + u r ,</formula><p>which gives the expected relationship</p><formula xml:id="formula_11">u ? = u k + (u w − u m ).</formula><p>Note that because this expression for u ? is in terms of k, w, and m, there is actually no need to assume that R is a set of actual words in V .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Proofs</head><p>Proof of Theorem 1. Note that p(w|C) equals</p><formula xml:id="formula_12">p(w) 1−m Z C m i=1 p(w|c i ) = p(w) 1−m Z C exp m i=1 u T c i v w − m i=1 log Z c i = 1 Z p(w) 1−m exp(u T C v w ) ,</formula><p>where</p><formula xml:id="formula_13">Z = Z C m i=1 Z i , and u C = m i=1 u i . Minimizing the KL-divergence D KL (p(·|c 1 , . . . , c m )p(·|c))</formula><p>as a function of c is equivalent to maximizing the negative cross-entropy as a function of u c , i.e., as maximizing</p><formula xml:id="formula_14">Q(u c ) = Z w exp(u T C v w ) p(w) m−1 (u T c v w − log Z c ) .</formula><p>Since Q is concave, the maximizers occur where its gradient vanishes. As uc Q equals</p><formula xml:id="formula_15">Z w exp(u T C v w ) p(w) m−1 v w − n =1 exp(u T c v )v n k=1 exp(u T c v k ) = n =1 exp(u T c v )v n k=1 exp(u T c v k ) − Z w exp(u T C v w )v w p(w) m−1 = w∈V p(w|c)v w − w∈V p(w|c 1 , . . . , c m )v w ,</formula><p>we see that (7) follows.</p><p>Proof of Theorem 2. Recall that u C = m i=1 u i . When p(w) = 1/|V | for all w ∈ V , the negative cross-entropy simplifies to</p><formula xml:id="formula_16">Q(u c ) = Z w exp u T C v w (u T c v w − log Z c ) ,</formula><p>and its gradient uc Q to</p><formula xml:id="formula_17">Z w exp(u C T v w ) v w − n =1 exp(u T c v )v n k=1 exp(u T c v k ) = Z w exp(u C T v w )v w − w exp(u T c v w )v w .</formula><p>Thus, Q(u C ) = 0 and since Q is concave, u C is its unique maximizer.</p><p>Lemma 1. The Skip-Gram model satisfies as- sumption A2 when m ≤ ∆.</p><p>Proof of Lemma 1. First, assume that m = ∆. In the Skip-Gram model target words are condition- ally independent given a context word, i.e.,</p><formula xml:id="formula_18">p(c 1 , . . . , c m |w) = m i=1 p(c i |w).</formula><p>Applying Baye's rule,</p><formula xml:id="formula_19">p(w|c 1 , . . . , c m ) = p(c 1 , . . . , c m |w)p(w) p(c 1 , . . . , c m ) = p(w) p(c 1 , . . . , c m ) m i=1 p(c i |w) = p(w) p(c 1 , . . . , c m ) m i=1 p(w|c i )p(c i ) p(w) = p(w) 1−m Z C m i=1 p(w|c i ) ,<label>(8)</label></formula><p>where</p><formula xml:id="formula_20">Z C = 1/ ( m i=1 p(c i ))</formula><p>. This establishes the result when m = ∆. The cases m &lt; ∆ follow by marginalizing out ∆ − m context words in the equality (8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Projection of paraphrases onto the vocabulary</head><p>Theorem 2 states that if there is a word c in the vo- cabulary V whose context vector equals the sum of the context vectors of the words c 1 , . . . , c m , then c has the same "meaning", in the sense of (4), as the composition of the words c 1 , . . . , c m . For any given set of words C = {c 1 , . . . , c m }, it is un- likely that there exists a word c ∈ V whose con- text vector is exactly equal to the sum of the con- text vectors of the words c 1 , . . . , c m . Similarly, in Theorem 1, the solution(s) to (7) will most likely not equal the context vector of any word in V . In both cases, we thus need to project the vector(s) onto words in our vocabulary in some manner.</p><p>Since Theorem 1 holds for any prior over V , in theory, we could enumerate all words in V and find the word(s) that minimize the difference of the left hand side of <ref type="formula" target="#formula_8">(7)</ref> from the right hand side. In practice, it turns out that the angle between the context vector of a word w ∈ V and solution- vector(s) is a good proxy and one gets very good experimental results by selecting as the paraphrase of a collection of words, the word that minimizes the angle to the paraphrase vector.</p><p>Minimizing the angle has been empirically suc- cessful at capturing composition in multiple log- linear word models. One way to understand the success of this approach is to recall that each word c is characterized by a categorical distribution over all other words w, as stated in (1). The peaks of this categorical distribution are precisely the words with which c co-occurs most often. These words characterize c more than all the other words in the vocabulary, so it is reasonable to expect that a word c whose categorical distribution has simi- lar peaks as the categorical distribution of c is sim- ilar in meaning to c. Note that the location of the peaks of p(·|c) are immune to the scaling of u c (athough the values of p(·|c) may change); thus, the words w which best characterize c are those for which v w has a high inner product with u c /u c 2 . Since</p><formula xml:id="formula_21">u T c v w u c 2 − u T c v w u c 2 ≤ 2 1 − u T c u c u c 2 u c 2 v w 2 ,</formula><p>it is clear that if the angle between the context representations of c and c is small, the distribu- tions p(w|c) and p(w|c ) will tend to have similar peaks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Skip-Gram learns a Sufficient Dimensionality Reduction Model</head><p>The Skip-Gram model assumes that the distribu- tion of the neighbors of a word follows a specific exponential parametrization of a categorical distri- bution. There is empirical evidence that this model generates features that are useful for NLP tasks, but there is no a priori guarantee that the training corpus was generated in this manner. In this sec- tion, we provide theoretical support for the useful- ness of the features learned even when the Skip- Gram model is misspecified.</p><p>To do so, we draw a connection between Skip- Gram and the Sufficient Dimensionality Reduc- tion (SDR) factorization of <ref type="bibr" target="#b5">Globerson and Tishby (Globerson and Tishby, 2003</ref>). The SDR model learns optimal 2 embeddings for discrete random variables X and Y without assuming any para- metric form on the distributions of X and Y , and it is useful in a variety of applications, includ- ing information retrieval, document classification, and association analysis ( <ref type="bibr" target="#b5">Globerson and Tishby, 2003)</ref>. As it turns out, these embeddings, like Skip-Gram, are obtained by learning the param- eters of an exponentially parameterized distribu- tion. In Theorem 3 below, we show that if a Skip- Gram model is fit to the cooccurence statistics of X and Y , then the output can be trivially modified (by adding readily-available information on word frequencies) to obtain the parameters of an SDR model. This connection is significant for two reasons: first, the original algorithm of <ref type="bibr" target="#b5">(Globerson and Tishby, 2003)</ref> for learning SDR embeddings is expensive, as it involves information projections. Theorem 3 shows that if one can efficiently fit a Skip-Gram model, then one can efficiently fit an SDR model. This implies that Skip-Gram specific approximation heuristics like negative- sampling, hierarchical softmax, and Glove, which are believed to return high-quality approxima- tions to Skip-Gram parameters ( <ref type="bibr" target="#b9">Mikolov et al., 2013b;</ref><ref type="bibr" target="#b13">Pennington et al., 2014</ref>), can be used to efficiently approximate SDR model parameters. Second, ( <ref type="bibr" target="#b5">Globerson and Tishby, 2003)</ref> argues for the optimality of the SDR embedding in any do- main where the training information on X and Y consists of their coocurrence statistics; this op- timality and the Skip-Gram/SDR connection ar- gues for the use of Skip-Gram approximations in such domains, and supports the positive experi- mental results that have been observed in appli- cations in network science <ref type="bibr" target="#b6">(Grover and Leskovec, 2016)</ref>, proteinomics <ref type="bibr" target="#b1">(Asgari and Mofrad, 2015)</ref>, and other fields.</p><p>As stated above, the SDR factorization solves the problem of finding information-theoretically optimal features, given co-occurrence statistics for a pair of discrete random variables X and Y . Associate a vector w i to the ith state of X, a vector h j to the jth state of Y , and let</p><formula xml:id="formula_22">W = [w T 1 · · · w T |X| ]</formula><p>T and H be defined similarly. Globerson and Tishby show that such optimal fea- tures can be obtained from a low-rank factoriza-tion of the matrix G of co-occurence measure- ments: G ij counts the number of times state i of X has been observed to co-occur with state j of Y. The loss of this factorization is measured using the KL-divergence, and so the optimal features are obtained from solving the problem arg min</p><formula xml:id="formula_23">W,H D KL G Z G 1 Z W,H e WH T .</formula><p>Here, Z G = ij G ij normalizes G into an esti- mate of the joint pmf of X and Y , and similarly Z W,H is the constant that normalizes e WH T into a joint pmf. The expression e WH T denotes entry- wise exponentiation of WH T . Now we revisit the Skip-Gram training objec- tive, and show that it differs from the SDR ob- jective only slightly. Whereas the SDR objective measures the distance between the pmfs given by (normalized versions of) G and e WH T , the Skip- Gram objective measures the distance between the pmfs given by (normalized versions of) the rows of G and e WH T . That is, SDR emphasizes fitting the entire pmfs, while Skip-Gram emphasizes fit- ting conditional distributions.</p><p>Before presenting our main result, we state and prove the following lemma, which is of indepen- dent interest and is used in the proof of our main theorem. Recall that Skip-Gram represents each word c as a multinomial distribution over all other words w, and it learns the parameters for these distributions by a maximum likelihood estima- tion. It is known that learning model parameters by maximum likelihood estimation is equivalent to minimizing the KL-divergence of the learned model from the empirical distribution; the fol- lowing lemma establishes the KL-divergence that Skip-Gram minimizes.</p><p>Lemma 2. Let G be the word co-occurrence ma- trix constructed from the corpus on which a Skip- Gram model is trained, in which case G cw is the number of times word w occurs as a neighboring word of c in the corpus. For each word c, let g c denote the empirical frequency of the word in the corpus, so that</p><formula xml:id="formula_24">g c = w G cw / t,w G t,w .</formula><p>Given a positive vector x, letˆxletˆ letˆx = x/x 1 . Then, the Skip-Gram model parameters</p><formula xml:id="formula_25">U = u 1 · · · u |V | T and V = v 1 · · · u |V | T minimize the objective c g c D KL ( ˆ g c e u T c V T ),</formula><p>where g c is the cth row of G.</p><p>Proof. Recall that Skip-Gram chooses U and V to maximize</p><formula xml:id="formula_26">Q = 1 T T i=1 C δ=−C δ =0 log p(w i+δ |w i ) ,</formula><p>where</p><formula xml:id="formula_27">p(w|c) = e u T c vw n i=1 e u T c v i .</formula><p>This objective can be rewritten using the pairwise cooccurence statistics as</p><formula xml:id="formula_28">Q= 1 T c,w G cw log p(w|c) = 1 T c t G ct w G cw t G ct log p(w|c) ∝ 1 T c ( t G ct ) ( tw G tw ) w G cw t G ct log p(w|c) = c g c w ˆ g c w log p(w|c) = c g c −D KL ( ˆ g c p(·|c)) − H( ˆ g c ) ,</formula><p>where H(·) denotes the entropy of a distribution. It follows that since Skip-Gram maximizes Q, it minimizes</p><formula xml:id="formula_29">c g c D KL ( ˆ g c p(·|c))= c g c D KL ( ˆ g c e u T c V T ).</formula><p>We now prove our main theorem of this section, which states that SDR parameters can be obtained by augmenting the Skip-Gram embeddings to ac- count for word frequencies. Theorem 3. Let U, V be the results of fitting a Skip-Gram model to G, and consider the aug- mented matrices˜U</p><formula xml:id="formula_30">matrices˜ matrices˜U = [U | α] and˜Vand˜ and˜V = [V | 1], where α c = log g c w e u T c vw and g c = w G c,w t,w G t,w .</formula><p>Then, the features ( ˜ U, ˜ V) constitute a sufficient dimensionality reduction of G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof. For convenience, let G denote the joint pdf matrix G/Z G , and let</head><p>G denote the matrix obtained by normalizing each row of G to be a probability distribution. Then, it suffices to show that D KL (G q W,H ) is minimized over the set of probability distributions q W,H q W,H (w, c) = Using this chain rule, we get</p><formula xml:id="formula_31">D KL (G q W,H (w, c))<label>(9)</label></formula><p>=D KL (g q W,H (c))+D KL ( Gq W,H (w|c)).</p><p>Note that the second term in this sum is, in the notation of Lemma 2,</p><formula xml:id="formula_32">D KL ( Gq W,H (w|c)) = c g c D KL ( ˆ g c e w T c H T ),</formula><p>so the matrices U and V that are returned by fit- ting the Skip-Gram model minimize the second term in this sum. We now show that the augmented matrices W = ˜ U and H = ˜ V also minimize this second term, and in addition they make the first term vanish.</p><p>To see that the first of these claims holds, i.e., that the augmented matrices make the second term in <ref type="formula" target="#formula_31">(9)</ref>  Thus, the choice W = ˜ U and H = ˜ V minimizes the second term in (9).</p><p>To see that the augmented matrices make the first term in (9) vanish, observe that when W = ˜ U and H = ˜ V, we have that q ˜ U, ˜ V (c) = g by con- struction. This can be verified by calculation: Here, the notation x y denotes entry-wise mul- tiplication of vectors. Since α c = log(g c ) − log e UV T 1 c , we have</p><formula xml:id="formula_33">q ˜ U, ˜ V (c) = (e UV T 1) e α c 1 T (e UV T 1) e α = g c t g t = g c .</formula><p>The choice W = ˜ U and H = ˜ V makes the first term in (9) vanish, and it also minimizes the second term in (9). Thus, it follows that the fea- tures ( ˜ U, ˜ V) constitute a sufficient dimensionality reduction of G.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>To establish this result, we use a chain rule for the KL-divergence. Recall that if we denote the expected KL-divergence between two marginal pmfs by D KL (p(·|c)q(·|c)) = c p(c) w p(w|c) log p(w|c) q(w|c) , then the KL-divergence satisfies the chain rule: D KL (p(w, c)q(w, c)) = D KL (p(c)q(c)) + D KL (p(w|c)q(w|c)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>vanish, note that q ˜ U, ˜ V (w|c) ∝ e ˜ u T c ˜ vw = e u T c vw+αc ∝ q U,V (w|c), and the constant of proportionality is independent of w. It follows that q ˜ U, ˜ V (w|c) = q U,V (w|c) and D KL ( G q ˜ U, ˜ V (w|c)) = D KL ( G q U,V (w|c)).</figDesc></figure>

			<note place="foot" n="1"> More generally, it suffices that the word vectors have certain properties consistent with this sampling process.</note>

			<note place="foot" n="2"> Optimal in an information-theoretic sense: they preserve the maximal mutual information between any pair of random variables with the observed coocurrence statistics, without regard to the underlying joint distribution.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A latent variable model approach to PMI-based word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Risteski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="385" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Continuous distributed representation of biological sequences for deep proteomics and genomics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsaneddin</forename><surname>Asgari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mofrad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS One</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Neural Probabilistic Language Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rejean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal Of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Word Association Norms, Mutual Information, and Lexicography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><forename type="middle">Ward</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Hanks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="29" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A synopsis of linguistic theory 19301955</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Firth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Studies in Linguistic Analysis pages</title>
		<imprint>
			<date type="published" when="1957" />
			<biblScope unit="page" from="1" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sufficient Dimensionality Reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1307" to="1331" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">node2vec: Scalable Feature Learning for Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Linguistic Regularities in Sparse and Explicit Word Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="171" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Three New Graphical Models for Statistical Language Modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Machine Learning</title>
		<meeting>the 24th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="641" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">When the Whole is Less than the Sum of Its Parts: How Composition Affects PMI Values in Distributional Semantic Vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Paperno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="345" to="350" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">GloVe: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Zipf&apos;s word frequency law in natural language: A critical review and future directions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Piantadosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1112" to="1130" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An improved model of semantic similarity based on lexical co-occurence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">T</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><forename type="middle">M</forename><surname>Rohde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">C</forename><surname>Gonnerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Plaut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="627" to="633" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
