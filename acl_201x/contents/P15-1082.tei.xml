<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Non-projective Dependency-based Pre-Reordering with Recurrent Neural Network for Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 26-31, 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Valerio Miceli-Barone</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Attardi</surname></persName>
							<email>attardi@di.unipi.it</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Università di Pisa Largo B. Pontecorvo</orgName>
								<address>
									<postCode>3 56127</postCode>
									<settlement>Pisa</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Università di Pisa</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<postCode>56127</postCode>
									<settlement>Pisa</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Non-projective Dependency-based Pre-Reordering with Recurrent Neural Network for Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="846" to="856"/>
							<date type="published">July 26-31, 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The quality of statistical machine translation performed with phrase based approaches can be increased by permuting the words in the source sentences in an order which resembles that of the target language. We propose a class of recurrent neu-ral models which exploit source-side dependency syntax features to reorder the words into a target-like order. We evaluate these models on the German-to-English and Italian-to-English language pairs, showing significant improvements over a phrase-based Moses baseline. We also compare with state of the art German-to-English pre-reordering rules, showing that our method obtains similar or better results.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Statistical machine translation is typically performed using phrase-based systems ( <ref type="bibr" target="#b12">Koehn et al., 2007</ref>). These systems can usually produce accurate local reordering but they have difficulties dealing with the long-distance reordering that tends to occur between certain language pairs ( <ref type="bibr" target="#b3">Birch et al., 2008</ref>).</p><p>The quality of phrase-based machine trans- lation can be improved by reordering the words in each sentence of source-side of the parallel training corpus in a "target-like" or- der and then applying the same transforma- tion as a pre-processing step to input strings during execution.</p><p>When the source-side sentences can be ac- curately parsed, pre-reordering can be per- formed using hand-coded rules. This ap- proach has been successfully applied to German-to-English ( <ref type="bibr" target="#b7">Collins et al., 2005</ref>) and other languages. The main issue with it is that these rules must be designed for each spe- cific language pair, which requires consider- able linguistic expertise.</p><p>Fully statistical approaches, on the other hand, learn the reordering relation from word alignments. Some of them learn reordering rules on the constituency <ref type="bibr" target="#b9">(Dyer and Resnik, 2010</ref>) ( <ref type="bibr">Khalilov and Fonollosa, 2011</ref>) or pro- jective dependency <ref type="bibr">(Genzel, 2010)</ref>, <ref type="bibr" target="#b14">(Lerner and Petrov, 2013</ref>) parse trees of source sen- tences. The permutations that these meth- ods can learn can be generally non-local (i.e. high distance) on the sentences but lo- cal (parent-child or sibling-sibling swaps) on the parse trees. Moreover, constituency or projective dependency trees may not be the ideal way of representing the syntax of non- analytic languages such as German or Ital- ian, which could be better described using non-projective dependency trees ( <ref type="bibr" target="#b5">Bosco and Lombardo, 2004</ref>). Other methods, based on recasting reordering as a combinatorial opti- mization problem <ref type="bibr" target="#b21">(Tromble and Eisner, 2009)</ref>, <ref type="bibr" target="#b22">(Visweswariah et al., 2011)</ref>, can learn to gen- erate in principle arbitrary permutations, but they can only make use of minimal syntactic information (part-of-speech tags) and there- fore can't exploit the potentially valuable structural syntactic information provided by a parser.</p><p>In this work we propose a class of reorder- ing models which attempt to close this gap by exploiting rich dependency syntax features and at the same time being able to process non-projective dependency parse trees and generate permutations which may be non- local both on the sentences and on the parse trees.</p><p>We represent these problems as sequence pre- diction machine learning tasks, which we ad- dress using recurrent neural networks.</p><p>We applied our model to reorder German sentences into an English-like word order as a pre-processing step for phrase-based ma- chine translation, obtaining significant im- provements over the unreordered baseline system and quality comparable to the hand- coded rules introduced by <ref type="bibr" target="#b7">Collins et al. (2005)</ref>. We also applied our model to Italian- to-English pre-reordering, obtaining a con- siderable improvement over the unreordered baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Reordering as a walk on a dependency tree</head><p>In order to describe the non-local reordering phenomena that can occur between language pairs such as German-to-English and Italian- to-English, we introduce a reordering frame- work similar to <ref type="bibr" target="#b15">(Miceli Barone and Attardi, 2013)</ref>, based on a graph walk of the depen- dency parse tree of the source sentence. This framework doesn't restrict the parse tree to be projective, and allows the generation of arbi- trary permutations. Let f ≡ ( f 1 , f 2 , . . . , f L f ) be a source sen- tence, annotated by a rooted dependency parse tree:</p><formula xml:id="formula_0">∀j ∈ 1, . . . , L f , h j ≡ PARENT(j)</formula><p>We define a walker process that walks from word to word across the edges of the parse tree, and at each steps optionally emits the current word, with the constraint that each word must be eventually emitted exactly once. Therefore, the final string of emitted words f is a permutation of the original sentence f , and any permutation can be generated by a suitable walk on the parse tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Reordering automaton</head><p>We formalize the walker process as a non- deterministic finite-state automaton. The state v of the automaton is the tuple v ≡ (j, E, a) where j ∈ 1, . . . , L f is the current ver- tex (word index), E is the set of emitted ver- tices, a is the last action taken by the automa- ton. The initial state is: v(0) ≡ (root f , {}, null) where root f is the root vertex of the parse tree.</p><p>At each step t, the automaton chooses one of the following actions:</p><p>• EMIT: emit the word f j at the current vertex j. This action is enabled only if the current vertex has not been already emit- ted:</p><formula xml:id="formula_1">j / ∈ E (j, E, a) EMIT → (j, E ∪ {j}, EMIT)<label>(1)</label></formula><p>• UP: move to the parent of the current vertex. Enabled if there is a parent and we did not just come down from it:</p><formula xml:id="formula_2">h j = null, a = DOW N j (j, E, a) UP → (h j , E, UP j )<label>(2)</label></formula><p>• DOW N j : move to the child j of the cur- rent vertex. Enabled if the subtree s(j ) rooted at j contains vertices that have not been already emitted and if we did not just come up from it:</p><formula xml:id="formula_3">h j = j, a = UP j , ∃k ∈ s(j ) : k / ∈ E (j, E, a) DOW N j → (j , E, DOW N j )<label>(3)</label></formula><p>The execution continues until all the vertices have been emitted. We define the sequence of states of the walker automaton during one run as an execu- tion ¯ v ∈ GEN( f ). An execution also uniquely specifies the sequence of actions performed by the automation.</p><p>The preconditions make sure that all execu- tion of the automaton always end generating a permutation of the source sentence. Fur- thermore, no cycles are possible: progress is made at every step, and it is not possible to enter in an execution that later turns out to be invalid. Every permutation of the source sentence can be generated by some execution. In fact, each permutation f can be generated by exactly one execution, which we denote as ¯ v( f ).</p><p>We can split the execution ¯ v( f ) into a se- quence of L f emission fragments ¯ v j ( f ), each ending with an EMIT action. The first fragment has zero or more DOW N * actions followed by one EMIT action, while each other fragment has a non-empty se- quence of UP and DOW N * actions (always zero or more UPs followed by zero or more DOWNs) followed by one EMIT action.</p><p>Finally, we define an action in an execution as forced if it was the only action enabled at the step where it occurred.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Application</head><p>Suppose we perform reordering using a typical syntax-based system which pro- cesses source-side projective dependency parse trees and is limited to swaps between pair of vertices which are either in a parent- child relation or in a sibling relation. In such execution the UP actions are always forced, since the "walker" process never leaves a sub- tree before all its vertices have been emitted.</p><p>Suppose instead that we could perform re- ordering according to an "oracle". The ex- ecutions of our automaton corresponding to these permutations will in general contain unforced UP actions. We define these ac- tions, and the execution fragments that ex- hibit them, as non-tree-local.</p><p>In practice we don't have access to a reordering "oracle", but for sentences pairs in a parallel corpus we can compute heuristic "pseudo-oracle" reference permutations of the source sentences from word-alignments. , we generate word alignments in both the source-to-target and the target-to-source directions using IBM model 4 as imple- mented in GIZA++ ( <ref type="bibr" target="#b18">Och et al., 1999</ref>) and then we combine them into a symmetrical word alignment using the "grow-diag-final-and" heuristic implemented in Moses ( <ref type="bibr" target="#b12">Koehn et al., 2007)</ref>.</p><p>Given the symmetric word-aligned corpus, we assign to each source-side word an in- teger index corresponding to the position of the leftmost target-side word it is aligned to (attaching unaligned words to the following aligned word) and finally we perform a sta- ble sort of source-side words according to this index.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Reordering example</head><p>Consider the segment of a German sentence shown in <ref type="figure" target="#fig_1">fig. 1</ref>. The English-reordered segment "die Währungsreserven anfangs lediglich dienen sollten zur Verteidigung" corresponds to the English: "the reserve as- sets were originally intended to provide protection".</p><p>In order to compose this segment from the original German, the reordering automaton described in our framework must perform a complex sequence of moves on the parse tree:</p><p>• Starting from "sollten", de- scend to "dienen", descent to "Währungsreserven" and finally to "die".</p><p>Emit it, then go up to "Währungsreserven", emit it and go up to "dienen" and up again to "sollten". Note that the last UP is unforced since "dienen" has not been emitted at that point and has also un- emitted children. This unforced action indicates non-tree-local reordering.</p><p>• Go down to "anfangs". Note that the in the parse tree edge crosses another edge, indicating non-projectivity. Emit "anfangs" and go up (forced) back to "sollten".</p><p>• Go down to "dienen", down to "zur", down to "lediglich" and emit it. Go up (forced) to "zur", up (unforced) to "dienen", emit it, go up (unforced) to "sollten", emit it. Go down to "dienen", down to "zur" emit it, go down to "Verteidigung" and emit it.</p><p>Correct reordering of this segment would be difficult both for a phrase-based system (since the words are further apart than both the typical maximum distortion distance and maximum phrase length) and for a syntax- based system (due to the presence of non- projectivity and non-tree-locality). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Recurrent Neural Network reordering models</head><p>Given the reordering framework described above, we could try to directly predict the ex- ecutions as Miceli Barone and Attardi <ref type="formula" target="#formula_1">(2013)</ref> attempted with their version of the frame- work. However, the executions of a given sentence can have widely different lengths, which could make incremental inexact decod- ing such as beam search difficult due to the need to prune over partial hypotheses that have different numbers of emitted words. Therefore, we decided to investigate a dif- ferent class of models which have the prop- erty that state transition happen only in corre- spondence with word emission. This enables us to leverage the technology of incremental language models.</p><p>Using language models for reordering is not something new <ref type="bibr" target="#b10">(Feng et al., 2010)</ref>, <ref type="bibr" target="#b8">(Durrani et al., 2011)</ref>, <ref type="bibr" target="#b4">(Bisazza and Federico, 2013)</ref>, but instead of using a more or less standard n-gram language model, we are going to base our model on recurrent neural network language models ( <ref type="bibr">Mikolov et al., 2010)</ref>.</p><p>Neural networks allow easy incorpora- tion of multiple types of features and can be trained more specifically on the types of sequences that will occur during decod- ing, hence they can avoid wasting model space to represent the probabilities of non- permutations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Base RNN-RM</head><p>Let f ≡ ( f 1 , f 2 , . . . , f L f ) be a source sentence. We model the reordering system as a deter- ministic single hidden layer recurrent neural network:</p><formula xml:id="formula_4">v(t) = τ(Θ (1) · x(t) + Θ REC · v(t − 1)) (4)</formula><p>where x(t) ∈ R n is a feature vector associated to the t-th word in a permutation f , v(0) ≡ v init , Θ (1) and Θ REC are parameters 1 and τ(·) is the hyperbolic tangent function.</p><p>If we know the first t − 1 words of the per- mutation f in order to compute the proba- bility distribution of the t-th word we do the following:</p><p>• Iteratively compute the state v(t − 1) from the feature vectors x(1), . . . , x(t − 1).</p><p>• For the all the indices of the words that haven't occurred in the permutation so</p><formula xml:id="formula_5">far j ∈ J(t) ≡ ([1, L f ] − ¯ i t−1: ), compute a score h j,t ≡ h o (v(t − 1), x o (j))</formula><p>, where x o (·) is the feature vector of the candidate target word.</p><p>• Normalize the scores using the logistic softmax function: P(</p><formula xml:id="formula_6">¯ I t = j| f , ¯ i t−1: , t) = exp(h j,t ) ∑ j ∈J(t) exp(h j ,t ) .</formula><p>The scoring function h o (v(t − 1), x o (j)) ap- plies a feed-forward hidden layer to the fea- ture inputs x o (j), and then takes a weighed inner product between the activation of this layer and the state v(t − 1). The result is then linearly combined to an additional fea- ture equal to the logarithm of the remaining words in the permutation (L f − t), 2 and to a bias feature:</p><formula xml:id="formula_7">h j,t ≡&lt; τ(Θ (o) · x o (j)), θ (2) v(t − 1) &gt; + θ (α) · log(L f − t) + θ (bias)<label>(5)</label></formula><p>where</p><formula xml:id="formula_8">h j,t ≡ h o (v(t − 1), x o (j)).</formula><p>We can compute the probability of an entire permutation f just by multiplying the proba- bilities for each word:</p><formula xml:id="formula_9">P( f | f ) = P( ¯ I = ¯ i| f ) = ∏ L f t=1 P( ¯ I t = ¯ i t | f , t)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Training</head><p>Given a training set of pairs of sentences and reference permutations, the training problem is defined as finding the set of parameters</p><formula xml:id="formula_10">θ ≡ (v init , Θ (1) , θ (2) , Θ REC , Θ (o) , θ (α) , θ (bias) )</formula><p>which minimize the per-word empirical cross-entropy of the model w.r.t. the reference permutations in the training set. Gradients can be efficiently computed using backpropa- gation through time (BPTT).</p><p>In practice we used the following training architecture: Stochastic gradient descent, with each train- ing pair ( f , f ) considered as a single mini- batch for updating purposes. Gradients com- puted using the automatic differentiation fa- cilities of Theano ( <ref type="bibr" target="#b2">Bergstra et al., 2010</ref>) (which implements a generalized BPTT). No trun- cation is used. L2-regularization <ref type="bibr">3</ref> . Learn- ing rates dynamically adjusted per scalar pa- rameter using the AdaDelta heuristic <ref type="bibr" target="#b23">(Zeiler, 2012)</ref>. Gradient clipping heuristic to prevent the "exploding gradient" problem <ref type="bibr">(Graves, 2013)</ref>. Early stopping w.r.t. a validation set to prevent overfitting. Uniform random initial- ization for parameters other than the recur- rent parameter matrix Θ REC . Random initial- ization with echo state property for Θ REC , with contraction coefficient σ = 0.99 <ref type="bibr">(Jaeger, 2001)</ref>, ( <ref type="bibr" target="#b11">Gallicchio and Micheli, 2011)</ref>.</p><p>Training time complexity is O(L 2 f ) per sen- tence, which could be reduced to O(L f ) using truncated BTTP at the expense of update ac- curacy and hence convergence speed. Space complexity is O(L f ) per sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Decoding</head><p>In order to use the RNN-RM model for pre- reordering we need to compute the most likely permutation * f of the source sentence</p><formula xml:id="formula_11">f : * f ≡ argmax f ∈GEN( f ) P( f | f )<label>(6)</label></formula><p>Solving this problem to the global optimum is computationally hard 4 , hence we solve it to a local optimum using a beam search strategy. We generate the permutation incrementally from left to right. Starting from an initial state consisting of an empty string and the ini- tial state vector v init , at each step we generate all possible successor states and retain the B- most probable of them (histogram pruning), according to the probability of the entire pre- fix of permutation they represent.</p><p>Since RNN state vectors do not decompose in a meaningful way, we don't use any hy- pothesis recombination. At step t there are L f − t possible succes- sor states, and the process always takes ex- actly L f steps 5 , therefore time complexity is O(B · L 2 f ) and space complexity is O(B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Features</head><p>We use two different feature configurations: unlexicalized and lexicalized.</p><p>In the unlexicalized configuration, the state transition input feature function x(j) is com- posed by the following features, all encoded using the "one-hot" encoding scheme:</p><p>• Unigram: POS(j), DEPREL(j), POS(j) * DEPREL(j). Left, right and parent un- igram: POS(k), DEPREL(k), POS(k) * DEPREL(k), where k is the index of re- spectively the word at the left (in the original sentence), at the right and the dependency parent of word j. Unique tags are used for padding.</p><p>• Pair features:</p><formula xml:id="formula_12">POS(j) * POS(k), POS(j) * DEPREL(k), DEPREL(j) * POS(k), DEPREL(j) * DEPREL(k), for k defined as above. • Triple features POS(j) * POS(le f t j ) * POS(right j ), POS(j) * POS(le f t j ) * POS(parent j ), POS(j) * POS(right j ) * POS(parent j ).</formula><p>• Bigram: POS(j) * POS(k), POS(j) * DEPREL(k), DEPREL(j) * POS(k) where k is the previous emitted word in the permutation.</p><p>• Topological features: three binary fea- tures which indicate whether word j and the previously emitted word are in a parent-child, child-parent or sibling- sibling relation, respectively.</p><p>The target word feature function x o (j) is the same as x(j) except that each feature is also conjoined with a quantized signed dis- tance <ref type="bibr">6</ref> between word j and the previous emit- ted word. Feature value combinations that appear less than 100 times in the training set are replaced by a distinguished "rare" tag.</p><p>The lexicalized configuration is equivalent to the unlexicalized one except that x(j) and x o (j) also have the surface form of word j (not conjoined with the signed distance).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Fragment RNN-RM</head><p>The Base RNN-RM described in the previ- ous section includes dependency informa- tion, but not the full information of reorder- ing fragments as defined by our automa- ton model (sec. 2). In order to determine whether this rich information is relevant to machine translation pre-reordering, we pro- pose an extension, denoted as Fragment RNN- RM, which includes reordering fragment fea- tures, at expense of a significant increase of time complexity. We consider a hierarchical recurrent neural network. At top level, this is defined as the previous RNN. However, the x(j) and x o (j) vectors, in addition to the feature vectors de- scribed as above now contain also the final states of another recurrent neural network. This internal RNN has a separate clock and a separate state vector. For each step t of the top-level RNN which transitions between word f (t − 1) and f (t), the internal RNN is reinitialized to its own initial state and per- forms multiple internal steps, one for each ac- tion in the fragment of the execution that the walker automaton must perform to walk be- tween words f (t − 1) and f (t) in the depen- dency parse (with a special shortcut of length one if they are adjacent in f with monotonic relative order).</p><p>The state transition of the inner RNN is de- fined as:</p><formula xml:id="formula_13">v r (t) = τ(Θ (r 1 ) · x r (t r ) + Θ r REC · v r (t r − 1))(7)</formula><p>where x r (t r ) is the feature function for the word traversed at inner time t r in the execu- tion fragment. v r (0) = v init r , Θ (r 1 ) and Θ r REC are parameters. Evaluation and decoding are performed es- sentially in the same was as in Base RNN- RM, except that the time complexity is now O(L 3 f ) since the length of execution fragments is O(L f ). Training is also essentially performed in the same way, though gradient computation is much more involved since gradients prop- agate from the top-level RNN to the inner RNN. In our implementation we just used the automatic differentiation facilities of Theano.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Features</head><p>The unlexicalized features for the inner RNN input vector x r (t r ) depend on the current word in the execution fragment (at index t r ), the previous one and the action label: UP, DOW N or RIGHT (shortcut). EMIT actions are not included as they always implicitly oc- cur at the end of each fragment. Specifically the features, encoded with the "one-hot" encoding are:</p><p>A * POS(t r ) * POS(t r − 1), A * POS(t r ) * DEPREL(t r − 1), A * DEPREL(t r ) * POS(t r − 1), A * DEPREL(t r ) * DEPREL(t r − 1). These features are also conjoined with the quantized signed distance (in the original sentence) between each pair of words. The lexicalized features just include the surface form of each visited word at t r .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Base GRU-RM</head><p>We also propose a variant of the Base RNN- RM where the standard recurrent hidden layer is replaced by a Gated Recurrent Unit layer, recently proposed by <ref type="bibr" target="#b6">Cho et al. (2014)</ref> for machine translation applications. The Base GRU-RM is defined as the Base RNN-RM of sec. 3.1, except that the recur- rence relation 4 is replaced by <ref type="figure">fig. 2</ref> Features are the same of unlexicalized Base RNN-RM (we experienced difficulties train- ing the Base GRU-RM with lexicalized fea- tures). <ref type="figure">Figure 2</ref>: GRU recurrence equations. v rst (t) and v upd (t) are the activation vectors of the "reset" and "update" gates, respectively, and π(·) is the logistic sigmoid function. .</p><formula xml:id="formula_14">v rst (t) = π(Θ (1) rst · x(t) + Θ REC rst · v(t − 1)) v upd (t) = π(Θ (1) upd · x(t) + Θ REC upd · v(t − 1)) v raw (t) = τ(Θ (1) · x(t) + Θ REC · v(t − 1) v upd (t)) v(t) = v rst (t) v(t − 1) + (1 − v rst (t)) v raw (t) (8)</formula><p>Training is also performed in the same way except that we found more benefi- cial to convergence speed to optimize using Adam (Kingma and Ba, 2014) 7 rather than AdaDelta.</p><p>In principle we could also extend the Frag- ment RNN-RM into a Fragment GRU-RM, but we did not investigate that model in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We performed German-to-English pre- reordering experiments with Base RNN-RM (both unlexicalized and lexicalized), Frag- ment RNN-RM and Base GRU-RM. In order to validate the experimental re- sults on a different language pair, we addi- tionally performed an Italian-to-English pre- reordering experiment with the Base GRU- RM, after assessing that this was the model that obtained the largest improvement on German-to-English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>The German-to-English baseline phrase- based system was trained on the Europarl v7 corpus ( <ref type="bibr" target="#b13">Koehn, 2005</ref>). We randomly split it in a 1,881,531 sentence pairs training set, a 2,000 sentence pairs development set (used for tuning) and a 2,000 sentence pairs test set. The English language model was trained on the English side of the parallel corpus augmented with a corpus of sentences from AP News, for a total of 22,891,001 sentences. The baseline system is phrase-based Moses in a default configuration with maximum distortion distance equal to 6 and lexicalized reordering enabled. Maximum phrase size is equal to 7. The language model is a 5-gram IRSTLM/KenLM. The pseudo-oracle system was trained on the training and tuning corpus obtained by permuting the German source side using the heuristic described in section 2.2 and is otherwise equal to the baseline system. In addition to the test set extracted from Europarl, we also used a 2,525 sentence pairs test set ("news2009") a 3,000 sentence pairs "challenge" set used for the WMT 2013 translation task ("news2013").</p><p>The Italian-to-English baseline system was trained on a parallel corpus assembled from Europarl v7, JRC-ACQUIS v2. <ref type="bibr">2 (Steinberger et al., 2006</ref>) and additional bilingual articles crawled from online newspaper websites 8 , to- taling 3,081,700 sentence pairs, which were split into a 3,075,777 sentence pairs phrase- table training corpus, a 3,923 sentence pairs tuning corpus, and a 2,000 sentence pairs test corpus.</p><p>Non-projective dependency parsing for our models, both for German and Italian was performed with the DeSR transition-based parser <ref type="bibr" target="#b1">(Attardi, 2006</ref>).</p><p>We also trained a German-to-English Moses system with pre-reordering performed by <ref type="bibr" target="#b7">Collins et al. (2005)</ref> rules, implemented by Howlett and Dras (2011). Constituency parsing for <ref type="bibr" target="#b7">Collins et al. (2005)</ref> rules was performed with the Berkeley parser ( <ref type="bibr" target="#b19">Petrov et al., 2006</ref>). For Italian-to-English we did not compare with a hand-coded reordering system as we are not aware of any strong pre-reordering baseline for this language pair.</p><p>For our experiments, we extract approxi-mately 300,000 sentence pairs from the Moses training set based on a heuristic confidence measure of word-alignment quality <ref type="bibr">(Huang, 2009)</ref>, <ref type="bibr" target="#b17">(Navratil et al., 2012</ref>). We randomly removed 2,000 sentences from this filtered dataset to form a validation set for early stop- ping, the rest were used for training the pre- reordering models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>The hidden state size s of the RNNs was set to 100 while it was set to 30 for the GRU model, validation was performed every 2,000 train- ing examples. After 50 consecutive validation rounds without improvement, training was stopped and the set of training parameters that resulted in the lowest validation cross- entropy were saved.</p><p>Training took approximately 1.5 days for the unlexicalized Base RNN-RM, 2.5 days for the lexicalized Base RNN-RM and for the unlexi- calized Base GRU-RM and 5 days for the un- lexicalized Fragment RNN-RM on a 24-core machine without GPU (CPU load never rose to more than 400%). Decoding was performed with a beam size of 4. Decoding the whole German corpus took about 1.0-1.2 days for all the models ex- cept Fragment RNN-RM for which it took about 3 days. Decoding for the Italian corpus for the Base GRU-RM took approximately 1.5 days.</p><p>Effects on monolingual reordering score are shown in <ref type="figure">fig. 3</ref> (German) and <ref type="figure">fig. 4</ref> (Italian), effects on translation quality are shown in <ref type="figure">fig. 5</ref> (German-to-English) and fig. 6 (Italian-to-English) 9 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Discussion and analysis</head><p>All our German-to-English models signifi- cantly improved over the phrase-based base- line, performing as well as or almost as well as ( <ref type="bibr" target="#b7">Collins et al., 2005</ref>), which is an interesting result since our models doesn't require any specific linguistic expertise.</p><p>Surprisingly, the lexicalized version of Base RNN-RM performed worse than the unlexi-calized one. This goes contrary to expectation as neural language models are usually lexical- ized and in fact often use nothing but lexical features.</p><p>The unlexicalized Fragment RNN-RM was quite accurate but very expensive both dur- ing training and decoding, thus it may not be practical.</p><p>The unlexicalized Base GRU-RM per- formed very well, especially on the Europarl dataset (where all the scores are much higher than the other datasets) and it never per- formed significantly worse than the unlexi- calized Fragment RNN-RM which is much slower.</p><p>We also performed exploratory experi- ments with different feature sets (such as lexical-only features) but we couldn't obtain a good training error. Larger network sizes should increase model capacity and may pos- sibly enable training on simpler feature sets.</p><p>The Italian-to-English experiment with Base GRU-RM confirmed that this model per- forms very well on a language pair with dif- ferent reordering phenomena than German- to-English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We presented a class of statistical syntax- based, non-projective, non-tree-local pre- reordering systems for machine translation. Our systems processes source sentences parsed with non-projective dependency parsers and permutes them into a target- like word order, suitable for translation by an appropriately trained downstream phrase-based system.</p><p>The models we proposed are completely trained with machine learning approaches and is, in principle, capable of generating ar- bitrary permutations, without the hard con- straints that are commonly present in other statistical syntax-based pre-reordering meth- ods. Practical constraints depend on the choice of features and are therefore quite flexible, allowing a trade-off between accuracy and speed.</p><p>In our experiments with the RNN-RM and GRU-RM models we managed to achieve translation quality improvements compara- Figure 3: German "Monolingual" reordering scores (upstream system output vs. "oracle"- permuted German) on the Europarl test set. All improvements are significant at 1% level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reordering</head><p>Reordering BLEU improvement none 73.11 unlex. Base GRU-RM 81.09 +7.98</p><p>Figure 4: Italian "Monolingual" reordering scores on the Europarl test set. All improvements are significant at 1% level.  <ref type="figure">Figure 6</ref>: Italian-to-English RNN-RM translation scores. Improvement is significant at 1% level.</p><p>ble to those of the best hand-coded pre- reordering rules.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Section of the dependency parse tree of a German sentence.</figDesc><graphic url="image-1.png" coords="4,93.18,62.81,411.20,102.40" type="bitmap" /></figure>

			<note place="foot" n="1"> we don&apos;t use a bias feature since it is redundant when the layer has input features encoded with the &quot;one-hot&quot; encoding 2 since we are then passing this score to a softmax of variable size (L f − t), this feature helps the model to keep the score already approximately scaled.</note>

			<note place="foot" n="3"> λ = 10 −4 on the recurrent matrix, λ = 10 −6 on the final layer, per minibatch.</note>

			<note place="foot" n="4"> NP-hard for at least certain choices of features and parameters 5 actually, L f − 1, since the last choice is forced</note>

			<note place="foot" n="6"> values greater than 5 and smaller than 10 are quantized as 5, values greater or equal to 10 are quantized as 10. Negative values are treated similarly.</note>

			<note place="foot" n="7"> with learning rate 2 · 10 −5 and all the other hyperparameters equal to the default values in the article.</note>

			<note place="foot" n="8"> Corriere.it and Asianews.it</note>

			<note place="foot" n="9"> Although the baseline systems were trained on the same datasets used in Miceli Barone and Attardi (2013), the results are different since we used a different version of Moses</note>
		</body>
		<back>
			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Distortion models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Al-Onaizan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics, ACL-44</title>
		<meeting>the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics, ACL-44<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="529" to="536" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Experiments with a multilanguage non-projective dependency parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Attardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Conference on Computational Natural Language Learning, CoNLL-X &apos;06</title>
		<meeting>the Tenth Conference on Computational Natural Language Learning, CoNLL-X &apos;06<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="166" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Theano: a CPU and GPU math expression compiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Python for Scientific Computing Conference (SciPy)</title>
		<meeting>the Python for Scientific Computing Conference (SciPy)</meeting>
		<imprint>
			<publisher>Oral Presentation</publisher>
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Predicting success in machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miles</forename><surname>Osborne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;08</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;08<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="745" to="754" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Efficient solutions for word reordering in German-English phrase-based statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Workshop on Statistical Machine Translation</title>
		<meeting>the Eighth Workshop on Statistical Machine Translation<address><addrLine>Sofia, Bulgaria, August</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="440" to="451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dependency and relational structure in treebank annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><surname>Bosco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincenzo</forename><surname>Lombardo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2004 Recent Advances in Dependency Grammar</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">Encoder-decoder approaches. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Clause restructuring for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivona</forename><surname>Kučerová</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd annual meeting on association for computational linguistics</title>
		<meeting>the 43rd annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="531" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A joint sequence translation model with integrated reordering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Fraser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1045" to="1054" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Context-free reordering, finite-state translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT &apos;10</title>
		<meeting><address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="858" to="866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A source-side decoding sequence model for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arne</forename><surname>Mauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the Association for Machine Translation in the Americas (AMTA)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Architectural and markovian factors of echo state networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gallicchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Micheli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="440" to="456" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Moses: open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL &apos;07</title>
		<meeting>the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL &apos;07<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Europarl: A parallel corpus for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference Proceedings: the tenth Machine Translation Summit</title>
		<meeting><address><addrLine>Phuket, Thailand. AAMT, AAMT</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Source-side classifier preordering for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Lerner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP &apos;13)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP &apos;13)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pre-reordering for machine translation using transition-based walks on dependency parse trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Valerio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miceli</forename><surname>Barone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Attardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Workshop on Statistical Machine Translation</title>
		<meeting>the Eighth Workshop on Statistical Machine Translation<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-08" />
			<biblScope unit="page" from="164" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cernock`yCernock` Cernock`y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A comparison of syntactic reordering methods for english-german machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Navratil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Visweswariah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananthakrishnan</forename><surname>Ramanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2043" to="2058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improved alignment models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Tillmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Joint SIGDAT Conf. on Empirical Methods in Natural Language Processing and Very Large Corpora</title>
		<meeting>of the Joint SIGDAT Conf. on Empirical Methods in Natural Language essing and Very Large Corpora</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="20" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning accurate, compact, and interpretable tree annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Thibaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="433" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The jrc-acquis: A multilingual aligned parallel corpus with 20+ languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Steinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Pouliquen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Widiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camelia</forename><surname>Ignat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC&apos;2006)</title>
		<meeting>the 5th International Conference on Language Resources and Evaluation (LREC&apos;2006)<address><addrLine>Genoa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>Tomaz Erjavec, Dan Tufis, and Dniel Varga</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning linear ordering problems for better translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Tromble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1007" to="1016" />
		</imprint>
	</monogr>
	<note>EMNLP &apos;09. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A word reordering model for improved machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Visweswariah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajakrishnan</forename><surname>Rajkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Gandhe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;11</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;11<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="486" to="496" />
		</imprint>
	</monogr>
	<note>Ananthakrishnan Ramanathan, and Jiri Navratil</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: An adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
