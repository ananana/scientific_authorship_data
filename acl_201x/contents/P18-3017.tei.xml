<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploring Chunk Based Templates for Generating a subset of English Text</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhilesh</forename><surname>Bhatnagar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">LTRC</orgName>
								<orgName type="institution" key="instit2">IIIT Hyderabad</orgName>
								<orgName type="institution" key="instit3">LTRC</orgName>
								<orgName type="institution" key="instit4">IIIT Hyderabad</orgName>
								<orgName type="institution" key="instit5">LTRC</orgName>
								<orgName type="institution" key="instit6">IIIT Hyderabad</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manish</forename><surname>Shrivastava</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">LTRC</orgName>
								<orgName type="institution" key="instit2">IIIT Hyderabad</orgName>
								<orgName type="institution" key="instit3">LTRC</orgName>
								<orgName type="institution" key="instit4">IIIT Hyderabad</orgName>
								<orgName type="institution" key="instit5">LTRC</orgName>
								<orgName type="institution" key="instit6">IIIT Hyderabad</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radhika</forename><surname>Mamidi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">LTRC</orgName>
								<orgName type="institution" key="instit2">IIIT Hyderabad</orgName>
								<orgName type="institution" key="instit3">LTRC</orgName>
								<orgName type="institution" key="instit4">IIIT Hyderabad</orgName>
								<orgName type="institution" key="instit5">LTRC</orgName>
								<orgName type="institution" key="instit6">IIIT Hyderabad</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Exploring Chunk Based Templates for Generating a subset of English Text</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of ACL 2018, Student Research Workshop</title>
						<meeting>ACL 2018, Student Research Workshop <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="120" to="126"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>120</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Natural Language Generation (NLG) is a research task which addresses the automatic generation of natural language text representative of an input non-linguistic collection of knowledge. In this paper, we address the task of the generation of grammatical sentences in an isolated context given a partial bag-of-words which the generated sentence must contain. We view the task as a search problem (a problem of choice) involving combinations of smaller chunk based templates extracted from a training corpus to construct a complete sentence. To achieve that, we propose a fitness function which we use in conjunction with an evolutionary algorithm as the search procedure to arrive at a potentially grammatical sentence (modeled by the fitness score) which satisfies the input constraints .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>One of the reasons why NLG is a challenging problem is because there are many ways in which a given content can be represented. These are rep- resented by the stylistic constraints which address syntactic and pragmatic choices (largely) indepen- dent of the information conveyed.</p><p>Classically, there are two major subtasks recog- nized in NLG: Strategic Generation and Tactical Generation (Sentence Planning and Surface Real- ization) <ref type="bibr">1</ref>  <ref type="bibr" target="#b6">(Reiter and Dale, 2000</ref>). Strategic Gen- eration -"what to say" deals with identifying the relevant information to present to the audience and Tactical Generation -"how to say" addresses the problems of linguistic representation of the input concepts. In this work, we address the problem of tactical generation, with a focus on the grammat- icality of the generated sentences. We formulate our task as follows: to generate syntactically cor- rect sentences given a set of constraints such as a bag-of-words, partial ordering, etc.</p><p>So, for example, given a bag of words such as "man", "plays", "football" and length constraints, a sentence like "The man plays football in Octo- ber." would be acceptable.</p><p>Our approach involves a corpus derived formu- lation of template based generation. Templates are instances of canned text with a slot-filler structure ("gaps") which can be filled with the appropriate information thus realizing the sentence. Since they are a manual resource, it is rather expensive and hard to generalize over different types or domains of text.</p><p>Thus, it is desirable to be able to automatically extract templates from a corpus. Also, to increase the syntactic coverage, we use sub-sentence level (smaller) templates to generate a sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Related Work</head><p>Traditionally, template based systems are used in scenarios where the output text is structurally very well defined and/or requires very high quality text as output with little variance. This work is in- spired from <ref type="bibr" target="#b9">(Van Deemter et al., 2005</ref>) who point out that template based systems and "real" NLG systems are "Turing equivalent" meaning that at least in terms of expressiveness, there is no theo- retical disparity between the two. ( <ref type="bibr" target="#b7">Rudnicky and Oh, 2002</ref>) use language models to generate text. In recent years, ( <ref type="bibr" target="#b3">Kondadadi et al., 2013</ref>) present a hy- brid NLG system which generates text by ranking tagged clusters of templates. NaturalOWL ( <ref type="bibr">Galanis and Androutsopoulos, 2007</ref>) use templates for </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Core Assumptions and Motivation</head><p>As an extension of our previous work <ref type="bibr" target="#b0">(Bhatnagar and Mamidi, 2016)</ref>, in this work, we adopt a sim- plistic model of language -a sequence of linguistic units. Given a set of such units, each possible sen- tence is contained in the search space of all pos- sible permutations. Thus, given a grammaticality fitness function, pruning and vocabulary reduction is essential to be able to tractably search this space.</p><p>Since templates are based on canned text, tem- plates are locally grammatical. The core idea is to effectively use the local grammaticality guar- antee of a corpus extracted template to combine, rather than construct the component templates to generate a sentence. These templates themselves contain individual tokens, effectively considering templates as a unit of sentence construction in- stead of tokens. The obvious trade-off is that since there are a lot more templates than tokens, which results in a much larger search space. However, if appropriate abstractions are used, perhaps the vo- cabulary problem can be mitigated somewhat.</p><p>The task is then twofold: how to determine which templates to combine (pruning) and con- straining that with a measure of grammaticality of the generated sentence (fitness).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Templates and Sentence Representation</head><p>We use chunks as a basis for the linear templates because chunks are linguistically sound and hence the vocabulary increase is lesser compared to un- constrained spans of text. In general, an extended feature has syntactic identifiers added to the fea- ture space. This is to better inform syntactic be- havior of the template even though it increases the feature space a little. Abstracted features have multiple features clustered together which reduces the feature space. We describe the extension pro- cess below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Abstraction</head><p>The chunks should be abstracted in such a way so as to have a minimal impact on their syntactic combination behavior. All punctuation and stopwords are not ab- stracted because they are highly relevant, syntac- tically. Following are the mappings applied to the chunks:</p><p>1. Named Entity Abstraction (NE): Each NE is mapped to a unique symbol corresponding to its category. 2. POS Abstraction (POS): POS categories such as "CD", "FW" and "SYM" and continuous "NNP" and "NNPS" sequences are mapped to their corresponding POS tags. 3. Cluster Abstraction (WC): Each token (ex- cept punctuation and stopwords), is mapped to the cluster ID of its syntatico-semantic cluster. Since a token can have multiple POS categories which in turn effect its syntactic behavior, we consider a token with different POS categories as distinct while clustering. The clusters are obtained by computing the KMeans cluster for token-POS pairs with eu- clidean distance of L2-normed vector embed- dings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Extended Categories</head><p>We extend the POS and chunk tag categories to better inform template combinations: 1. Extended POS (EPOS): Each punctuation, stopword and NE is assigned its own unique POS category. 2. Extended Chunk Tags (ECTag): Since the "O" (outside) chunk tag is a "default" cate- gory, it contributes a lot of syntactic confu- sion, we assign a separate chunk tag for all "NP" "NP" Extended Chunk Tag (ECTag) Tokens "a", "popular", "wrestler" "a", "JJ213", "NN5266" Tok-POS cluster (WC) POS "DT", "JJ", "NN" "DT", "JJ" ,"NN" Extended POS (EPOS) Head token "wrestler" "NN5266" Head Tok-POS cluster (HWC) Head POS "NN" "NN" Head Extended POS (HEPOS) "a" and "NN5266" Junction Tok-POS clusters (WCJ) "DT", "NN" Junction Extended POS (EPOSJ) "a BLANK BLANK" "Blank" Construction Feature (BlankCo) "a JJ NN"</p><p>Extended POS Construction Feature (EPOSCo) chunks tagged "O" which contain sentence endings (".", ",", or "!") or brackets ("(" or ")"). In addition, chunk tags for chunks con- taining wh-words (POS tags "WDT", "WP", "WP$" or "WRB") are marked (eg. "NP" be- comes "WNP").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Template features</head><p>A template is created after applying abstractions to a chunk and extending its syntactic categories. The template, however has two more feature types: Construction features represent the chunk as a syntactic construction or a layout, encod- ing the positional combination of the com- ponents comprising it. It is constructed by creating a feature for the ordered tuple of to- kens comprising the chunk where all tokens are mapped to a single "blank" symbol or its extended POS except punctuations and stop- words. Two factors (single "blank" and ex- tended POS) are applicable. <ref type="table" target="#tab_0">Table 1</ref> shows an example chunk and its corre- sponding derived template. A sentence is repre- sented as a sequence of templates described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Scoring the template combinations</head><p>It should be noted that this score is not equivalent to a syntactic correctness score, but rather a sub- set of it. This is because here we are dealing with configurations of untampered templates whose lo- cal syntactic correctness still holds and the syntac- tic incorrectness is a matter of their combinatorial configuration while a grammaticality score needs to deal with "broken" chunks as well.</p><p>The fitness score F is a linear combination of length-normalized total log-probabilities T P of different sequences derived from the sentence computed using an NGram language model. The total probability of an NGram model is defined as: T P (s) = P (w 1 |bos)P (w 2 |w 1 bos)...</p><formula xml:id="formula_0">..P (eos|w k w k−1 ...w k−n+1 )<label>(1)</label></formula><p>where n is the order of the language model, s is a sequence and w i is the i th element in s. F is a weighted sum of length-normalized total log- probabilities of five different sequences:</p><formula xml:id="formula_1">nT P (s) = log 10 (T P (s))/(l s + 1)<label>(2)</label></formula><formula xml:id="formula_2">F (s) = α c nT P (ec) + α co nT P (co)/2 + α j nT P (j)/2 + α h nT P (h)/2 + α l nT P (l)/2<label>(3)</label></formula><p>where nT P (s) is the length normalized total log- probability for the sequence s where l s is the num- ber of elements in that sequence. ec, co, j, h and l represent the sequences in chunk score, construc- tion score, junction score, head score and lexical score. α i are the weight for each component score which are discussed:</p><p>1. Extended Chunk Score nT P (ec) -This score is calculated using the extended chunk tags (ECTags) for the sentence. It is useful in de- termining the global syntactic structure. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.] NP[NN969/NN] VP[VBZ29/VBZ] PP[in/in] NP[the/the JJ347/JJ NN628/NN] .[./.] Extended Chunk Tags (ECTags) NP VP PP NP . Blank Construction (BlankCo)</head><p>BLANK BLANK in the BLANK BLANK . Extended POS Construction (EPOSCo) NN VBZ in the JJ NN . Tok-POS Junction (WCJ) (bos, NN969), (NN969, VBZ29), (VBZ29, in), (in, the), (NN628, .), (., eos) Extended POS Junction (EPJ) (bos, NN), (NN, VBZ), (VBZ, in), (in, the), (NN, .), (.,</p><note type="other">eos) Head Tok-POS (HWC) NN969 VBZ29 in NN628 . Head Extended POS (HEPOS) NN VBZ in NN . Tok-POS Sequence (WCs) NN969 VBZ29 in the JJ347 NN628 . Extended POS Sequence (EPOSs)</note><p>NN VBZ in the JJ NN .</p><p>2. Construction Score nT P (co) -This score is calculated using blank construction layout (BlackCo) and extended POS construction layout (EPOSCo). It is useful in augmenting the overall grammaticality of the sentence as it encodes the syntactic layout of the chunk as a whole. 3. Junction Score JP (j) -This score is calcu- lated as the sum of the bigram probability of the left junction of the right template condi- tioned on the right junction of the left tem- plate for both tok-pos clusters (WC) and ex- tended POS (EPOS) for all junctions in the sentence. This score represents a local view of inter-template cohesiveness. It represents how a sentence "glues" together. 4. Head Score nT P (h) -This score is calcu- lated using head tok-pos clusters (HWC) and head extended POS (HEPOS). Counter to the junction score, it represents a more semantic view of the chunk interactions. This is be- cause generally a chunk head is often the pri- mary content word in that chunk. 5. Lexical Score nT P (l) -This score is com- puted using the tok-pos clusters (WC) and extended POS (EPOS). This score represents a baseline grammaticality score on a lexical level. A sentence with all the sequences are fed to the fitness function are shown in table 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Parameters for pruning the search space</head><p>The search space is the space of all permutations of the templates to form the sentence. Such a search space is huge. We observe that depend- ing on the particular constraints described in sec- tion 7, we can prune the permutation search space. This is done by finding a subset of the template bank that can occur at each position in the tem- plate sequence. The subsets are represented by a set of constraints which we call a search config- uration. Thus, say, if a sentence is determined to have 5 templates, there will be a search configu- ration computed for each template slot, in accor- dance with the global constraints for the sentence. Described below is the specification of a search configuration:</p><p>1. Length The templates must contain exactly the specified number of tokens (tok-pos clus- ters) in it. 2. Extended chunk tag (ECtag) The templates must have the specified extended chunk tag. 3. Tok-POS clusters and extended POS tags (WC-EPOS) All the specified tok-pos clus- ters and their corresponding extended POS tags must be present in the templates.</p><p>Note that neither of the above constraints needs to be specified for a search configuration, in which case all templates can be considered to fill that po- sition. We use memoization to make the pruning computationally feasible.</p><p>Eg. if a search configuration has a length of 5, no preference specified for the extended search tag and (cluster("run"/NN), "NN") in the tok-pos (WC-EPOS) list, all templates containing 5 tokens which also contain the cluster for run used as an "NN" are valid for that configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Search</head><p>To search the very large permutation space, we use a population based searching method which uses only mutation as the genetic operator for generat- ing new solutions. Following are the components of the evolutionary search:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Population Selection</head><p>The search can be parametrized by specifying a collection of sentence level constraints which have their own individual sub-constraints. These con- straints are different from search configurations as defined in section 6 as these constraints operate on a sentence level, while search configurations, which are derived from these constraints operate on a template level. The sentence level constraints are listed below:</p><p>1. Number of tokens: The number of tokens in the generated sentence must be in a specified range. A maximum value is required. 2. Number of templates: The number of tem- plates in the generated sentence must be in the specified range. 3. Chunk specifications (inclusion): For each chunk specification, at least one template must be present in the sentence which follows it and it must satisfy all the sub-constraints such as extended chunk tag and constituent tok-pos clusters. 4. Position Chunk specifications: This is a chunk constraint like the one described above, with the additional constraint of a fixed position. 5. Ordered Chunk specifications: These are list of chunk constraints with the additional con- straint that they be in order in the generated sentence. 6. Tok-pos specifications (inclusion): The sen- tence must contain the specified tok-pos clus- ters. 7. Ordered Tok-pos specifications: The speci- fied tok-pos clusters must occur in the same order in the sentence. Note that every constraint and sub-constraint can be left empty which means that there could be no specification for the extended chunk tag for a chunk constraint. Based on these sentence level constraints, search configurations for each tem- plate position are derived.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.1">Sentence level constraints to template level SearchConfigs</head><p>Based on the number of templates selected be- tween the range given, it distributes the total length specified between all the templates ran- domly. Then, it arranges the Chunks, Ordered- Chunks and PositionFixedChunks and distributes the token level constraints (OrderedTokPOSs and TokPOSConstraints) to the templates. Now, all three parameters of the SearchConfigs have been minimally inferred and the search space can now be pruned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.2">Sampling from the pruned space</head><p>Since we are dealing with naturally occurring text, the distribution of the templates grouped by ex- tended chunk tags follows a Zipf curve meaning that almost 40% of the time, an "NP" is selected and a "." almost never gets a chance even though both templates are prominent in the set of tem- plates having the same chunk tag as them. This drastically hampers the chances of getting struc- tural variance in the templates constructed. To remedy this, we assign weights to the set of tem- plates with a dampening exponent of 0.4 which makes the distribution more uniform, yet pre- serves the selectional biases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Dampen the Zipf for selection of extended chunk tag 2. Dampen the Zipf for selection of templates</head><p>given an extended chunk tag 3. Obtain the distribution for selection of tem- plates by multiplying probabilities taken from the above two distribution. This results in a damped Zipf curve for the se- lection of templates which allows for much more variance in the extended chunk tags of the gener- ated sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Mutation</head><p>To perform mutation, we randomly select a tem- plate to mutate and using the dampened distribu- tion obtained, we sample a template from the sub- space corresponding to the search configuration of that template position.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Selection and Evolution Strategy</head><p>We use elitist selection (pick the top k organisms) with enforced variability. We enforce that at least 8% and 15% of the population have unique blank construction layout and extended POS construc- tion layout respectively. Following are the steps for evolving the population.</p><p>1. Initialize population given a set of Con- straints and sample a population of size 1000 2. Mutate the first half of the population and as- sign it to the second half. 3. compute fitnesses for all organisms and sort population based on fitness score 4. Retain organisms such that the variability constraints are met. 5. Re-initialize 25% of the population so that different search configurations are searched. 6. Repeat 1 to 5 until 100 generations. This evolution run gives a population consisting of grammatical configurations which adhere to the constraints given as input. There are still possibly word clusters remaining which need to be filled since sentences which are generated are comprised of templates which contain clusters, not word forms. We fill these cluster slots with the tokens we gave in the initial bag-of-words constraints. To do that, we run a random search on the best gener- ated sentence and fill the cluster slots which max- imizes the token and head token perplexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Experiment Setup</head><p>Following are the steps we took to conduct our ex- periments:</p><p>1. We tokenized, POS tagged, NER tagged, and chunked and abstracted the English Wikipedia corpus using Stanford CoreNLP ( <ref type="bibr" target="#b5">Manning et al., 2014</ref>), spaCy <ref type="bibr">(Honnibal and Montani, 2017)</ref> and LM-LSTM-CRF ( <ref type="bibr" target="#b4">Liu et al., 2018</ref>) to extract the templates. 2. The number of clusters, k was chosen to be 7500. 3. We used lexvec( <ref type="bibr" target="#b8">Salle et al., 2016)</ref> pretrained vectors for clustering. 4. The weights for the fitness function were em- pirically chosen to be 75, 10, 10, 5 and 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Results</head><p>Following are the top sentences generated with the following constraints:</p><p>1. "cat" should be present: the heated water plant is likewise formed en- tirely of cat . 2. "what" as the first token and sentence con- tains a "?": what does the right thing do ? 3. "the" in position=0 and "." in sentence:</p><p>the mid product can readily be used in poly- nomial practices . 4. "and", "ate" and "ran" in sentence:</p><p>the division ate a plant of cape , ran a princi- pal prying need and stockpiled superconduc- tivity . 5. "on" and "in" in sentence:</p><p>PERSON bumped ORG in DATE on spirits <ref type="figure">Figure 2</ref>: Evolution of error . 6. "influential", "large" and "red" in sentence:</p><p>large influential player vocals defined red to the convenience of the detector . We show the generation improvements for the first sentence. As we can see, the algorithm is able to generate a question with minimal supervision. Also, in sentence 5, multiple PPs can be seen.</p><p>Some level of supervision is necessary to drive the required syntax and content words to gener- ate predictable outputs. We observe that there is a bias in the model e.g. usage of "the" in the starting noun phrase, and chaining verb phrases and prepositional phrases. Hence, to generate a question, one has to specify the position of the wh word, otherwise the sentences often start with a noun phrase.</p><p>Computationally, the search time increases with increasing sentence lengths. On a reasonably modern machine, our implementation generated the above sentences in about 150 seconds while using 2.2 GB of memory. <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Future Work</head><p>As a future work, detailed qualitative analysis and minimum constraints needed to generate specific linguistic structures can be done. Also, automatic extraction of content words and other relevant con- straints can be explored for generation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: System architecture</figDesc><graphic url="image-1.png" coords="2,72.00,62.81,212.75,237.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 .</head><label>1</label><figDesc>Junction Features (WCJ, EPOSJ): Junction features are comprised of the leftmost and rightmost features of the template. These features are used to predict if two templates "glue" together well at the point of contact (the junction). Both factors (tok-pos clusters and extended POS category) are applicable. 2. Head Features (HWC, HEPOS): Head fea- tures represent the "external" features for a chunk used to compute a global grammatical- ity component. Both factors -extended POS and tok-pos clusters are applicable. 3. Construction features (BlankCo, EPOSCo):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : Template Example</head><label>1</label><figDesc></figDesc><table>Chunk Feature Chunk 
Template 
Template Feature 
Chunk Tag 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 : A sentence and sequences used to compute the fitness Template Feature Sequence Sentence NP[Sand] VP[blows] PP[in] NP[the strong wind] .[</head><label>2</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> Because we follow a template based approach, there is some overlap between the Content Determination and Aggregation steps.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">Acknowledgements</head><p>The authors would like to thank Prof. Owen Ram-bow for helping gain insight into the problem. <ref type="bibr">2</ref> The code used for this research will be made available on https://github.com/tingc9/LinearTemplatesSRW</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Experiments in linear template combination using genetic algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhilesh</forename><surname>Bhatnagar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radhika</forename><surname>Mamidi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>22nd Himalayan Languages Symposium</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generating multilingual descriptions from linguistically annotated owl ontologies: the naturalowl system</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh European Workshop on Natural Language Generation</title>
		<meeting>the Eleventh European Workshop on Natural Language Generation</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="143" to="146" />
		</imprint>
	</monogr>
	<note>Dimitrios Galanis and Ion Androutsopoulos. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">2017. spacy 2: Natural language understanding with bloom embeddings, convolutional neural networks and incremental parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Honnibal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ines</forename><surname>Montani</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A statistical nlg framework for aggregated planning and realization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Kondadadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blake</forename><surname>Howald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Schilder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1406" to="1415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Empower Sequence Labeling with Task-Aware Neural Language Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL) System Demonstrations</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Building natural language generation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Dialog annotation for stochastic generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><forename type="middle">H</forename><surname>Alexander I Rudnicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Matrix factorization using window sampling and negative sampling for improved word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Salle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Idiart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aline</forename><surname>Villavicencio</surname></persName>
		</author>
		<idno>abs/1606.00819</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Real versus template-based natural language generation: A false opposition? Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kees</forename><surname>Van Deemter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="15" to="24" />
		</imprint>
	</monogr>
	<note>Emiel Krahmer, and Mariët Theune</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
