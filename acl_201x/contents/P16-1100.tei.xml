<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:03+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1054" to="1063"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Nearly all previous work on neural machine translation (NMT) has used quite restricted vocabularies, perhaps with a subsequent method to patch in unknown words. This paper presents a novel word-character solution to achieving open vocabulary NMT. We build hybrid systems that translate mostly at the word level and consult the character components for rare words. Our character-level recurrent neural networks compute source word representations and recover unknown target words when needed. The twofold advantage of such a hybrid approach is that it is much faster and easier to train than character-based ones; at the same time, it never produces unknown words as in the case of word-based models. On the WMT&apos;15 English to Czech translation task, this hybrid approach offers an addition boost of +2.1−11.4 BLEU points over models that already handle unknown words. Our best system achieves a new state-of-the-art result with 20.7 BLEU score. We demonstrate that our character models can successfully learn to not only generate well-formed words for Czech, a highly-inflected language with a very complex vocabulary, but also build correct representations for English source words.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural Machine Translation (NMT) is a simple new architecture for getting machines to translate. At its core, NMT is a single deep neural network that is trained end-to-end with several advantages such as simplicity and generalization. Despite being relatively new, NMT has already achieved <ref type="figure">Figure 1</ref>: Hybrid NMT -example of a word- character model for translating "a cute cat" into "un joli chat". Hybrid NMT translates at the word level. For rare tokens, the character-level compo- nents build source representations and recover tar- get &lt;unk&gt;. "_" marks sequence boundaries.</p><p>state-of-the-art translation results for several lan- guage pairs such as English-French ( <ref type="bibr" target="#b21">Luong et al., 2015b</ref>), English-German ( <ref type="bibr" target="#b9">Jean et al., 2015a;</ref><ref type="bibr" target="#b20">Luong et al., 2015a;</ref><ref type="bibr" target="#b18">Luong and Manning, 2015)</ref>, and English-Czech ( <ref type="bibr" target="#b10">Jean et al., 2015b</ref>).</p><p>While NMT offers many advantages over tra- ditional phrase-based approaches, such as small memory footprint and simple decoder implemen- tation, nearly all previous work in NMT has used quite restricted vocabularies, crudely treating all other words the same with an &lt;unk&gt; symbol. Sometimes, a post-processing step that patches in unknown words is introduced to alleviate this problem. <ref type="bibr" target="#b21">Luong et al. (2015b)</ref> propose to annotate occurrences of target &lt;unk&gt; with positional infor- mation to track their alignments, after which sim- ple word dictionary lookup or identity copy can be performed to replace &lt;unk&gt; in the translation. <ref type="bibr" target="#b9">Jean et al. (2015a)</ref> approach the problem similarly but obtain the alignments for unknown words from the attention mechanism. We refer to these as the unk replacement technique.</p><p>Though simple, these approaches ignore several important properties of languages. First, monolin- gually, words are morphologically related; how- ever, they are currently treated as independent en- tities. This is problematic as pointed out by <ref type="bibr" target="#b19">Luong et al. (2013)</ref>: neural networks can learn good rep- resentations for frequent words such as "distinct", but fail for rare-but-related words like "distinc- tiveness". Second, crosslingually, languages have different alphabets, so one cannot naïvely memo- rize all possible surface word translations such as name transliteration between "Christopher" (En- glish) and "Kry˘ stof" <ref type="bibr">(Czech)</ref>. See more on this problem in ( <ref type="bibr" target="#b26">Sennrich et al., 2016</ref>).</p><p>To overcome these shortcomings, we propose a novel hybrid architecture for NMT that translates mostly at the word level and consults the char- acter components for rare words when necessary. As illustrated in <ref type="figure">Figure 1</ref>, our hybrid model con- sists of a word-based NMT that performs most of the translation job, except for the two (hypotheti- cally) rare words, "cute" and "joli", that are han- dled separately. On the source side, representa- tions for rare words, "cute", are computed on-the- fly using a deep recurrent neural network that op- erates at the character level. On the target side, we have a separate model that recovers the sur- face forms, "joli", of &lt;unk&gt; tokens character-by- character. These components are learned jointly end-to-end, removing the need for a separate unk replacement step as in current NMT practice.</p><p>Our hybrid NMT offers a twofold advantage: it is much faster and easier to train than character- based models; at the same time, it never produces unknown words as in the case of word-based ones. We demonstrate at scale that on the WMT'15 En- glish to Czech translation task, such a hybrid ap- proach provides an additional boost of +2.1−11.4 BLEU points over models that already handle un- known words. We achieve a new state-of-the- art result with 20.7 BLEU score. Our analysis demonstrates that our character models can suc- cessfully learn to not only generate well-formed words for Czech, a highly-inflected language with a very complex vocabulary, but also build correct representations for English source words.</p><p>We provide code, data, and models at http: //nlp.stanford.edu/projects/nmt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There has been a recent line of work on end-to- end character-based neural models which achieve good results for part-of-speech tagging (dos <ref type="bibr" target="#b7">Santos and Zadrozny, 2014;</ref><ref type="bibr" target="#b16">Ling et al., 2015a</ref>), de- pendency parsing ( <ref type="bibr" target="#b3">Ballesteros et al., 2015</ref>), text classification ( <ref type="bibr" target="#b30">Zhang et al., 2015)</ref>, speech recog- nition ( <ref type="bibr" target="#b5">Chan et al., 2016;</ref><ref type="bibr" target="#b1">Bahdanau et al., 2016)</ref>, and language modeling ( <ref type="bibr" target="#b13">Kim et al., 2016;</ref><ref type="bibr" target="#b11">Jozefowicz et al., 2016)</ref>. However, success has not been shown for cross-lingual tasks such as ma- chine translation. 1 <ref type="bibr" target="#b26">Sennrich et al. (2016)</ref> propose to segment words into smaller units and translate just like at the word level, which does not learn to understand relationships among words.</p><p>Our work takes inspiration from ( <ref type="bibr" target="#b19">Luong et al., 2013)</ref> and ( <ref type="bibr" target="#b14">Li et al., 2015)</ref>. Similar to the former, we build representations for rare words on-the-fly from subword units. However, we utilize recur- rent neural networks with characters as the basic units; whereas <ref type="bibr" target="#b19">Luong et al. (2013)</ref> use recursive neural networks with morphemes as units, which requires existence of a morphological analyzer. In comparison with ( <ref type="bibr" target="#b14">Li et al., 2015)</ref>, our hybrid archi- tecture is also a hierarchical sequence-to-sequence model, but operates at a different granularity level, word-character. In contrast, <ref type="bibr" target="#b14">Li et al. (2015)</ref> build hierarchical models at the sentence-word level for paragraphs and documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background &amp; Our Models</head><p>Neural machine translation aims to directly model the conditional probability p(y|x) of translating a source sentence, x 1 , . . . , x n , to a target sentence, y 1 , . . . , y m . It accomplishes this goal through an encoder-decoder framework <ref type="bibr" target="#b12">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b6">Cho et al., 2014</ref>). The encoder computes a representation s for each source sentence. Based on that source representation, the decoder generates a transla- tion, one target word at a time, and hence, decom- poses the log conditional probability as:</p><formula xml:id="formula_0">log p(y|x) = m t=1</formula><p>log p (y t |y &lt;t , s) (1)</p><p>A natural model for sequential data is the re- current neural network (RNN), used by most of the recent NMT work. Papers, however, differ in terms of: (a) architecture -from unidirectional, to bidirectional, and deep multi-layer RNNs; and (b) RNN type -which are long short-term mem- ory (LSTM) <ref type="bibr" target="#b8">(Hochreiter and Schmidhuber, 1997)</ref> and the gated recurrent unit ( <ref type="bibr" target="#b6">Cho et al., 2014</ref>). All our models utilize the deep multi-layer architec- ture with LSTM as the recurrent unit; detailed for- mulations are in ( <ref type="bibr" target="#b29">Zaremba et al., 2014)</ref>.</p><p>Considering the top recurrent layer in a deep LSTM, with h t being the current target hidden state as in <ref type="figure">Figure 2</ref>, one can compute the proba- bility of decoding each target word y t as:</p><formula xml:id="formula_1">p (y t |y &lt;t , s) = softmax (h t )<label>(2)</label></formula><p>For a parallel corpus D, we train our model by minimizing the below cross-entropy loss:</p><formula xml:id="formula_2">J = (x,y)∈D − log p(y|x)<label>(3)</label></formula><p>Attention Mechanism -The early NMT ap- proaches ( <ref type="bibr" target="#b6">Cho et al., 2014</ref>), which we have described above, use only the last encoder state to initialize the decoder, i.e., setting the input representation s in Eq.</p><p>(1) to [ ¯ h n ]. Re- cently, <ref type="bibr" target="#b0">Bahdanau et al. (2015)</ref> propose an atten- tion mechanism, a form of random access mem- ory for NMT to cope with long input sequences. <ref type="bibr" target="#b20">Luong et al. (2015a)</ref> further extend the attention mechanism to different scoring functions, used to compare source and target hidden states, as well as different strategies to place the attention. In all our models, we utilize the global attention mech- anism and the bilinear form for the attention scor- ing function similar to ( <ref type="bibr" target="#b20">Luong et al., 2015a</ref>).</p><p>Specifically, we set s in Eq.</p><p>(1) to the set of source hidden states at the top layer, [ ¯ h 1 , . . . , ¯ h n ]. As illustrated in <ref type="figure">Figure 2</ref>, the attention mechanism consists of two stages: (a) context vector -the current hidden state h t is compared with individ- ual source hidden states in s to learn an alignment vector, which is then used to compute the context vector c t as a weighted average of s; and (b) atten- tional hidden state -the context vector c t is then</p><formula xml:id="formula_3">y t c t ¯ h 1 ¯ h n h t ˜ h t Figure 2: Attention mechanism.</formula><p>used to derive a new attentional hidden state:</p><formula xml:id="formula_4">˜ h t = tanh(W[c t ; h t ])<label>(4)</label></formula><p>The attentional vector˜hvector˜ vector˜h t then replaces h t in Eq. <ref type="formula" target="#formula_1">(2)</ref> in predicting the next word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Hybrid Neural Machine Translation</head><p>Our hybrid architecture, illustrated in <ref type="figure">Figure 1</ref>, leverages the power of both words and characters to achieve the goal of open vocabulary NMT. The core of the design is a word-level NMT with the advantage of being fast and easy to train. The character components empower the word-level system with the abilities to compute any source word representation on the fly from characters and to recover character-by-character unknown target words originally produced as &lt;unk&gt;.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Word-based Translation as a Backbone</head><p>The core of our hybrid NMT is a deep LSTM encoder-decoder that translates at the word level as described in Section 3. We maintain a vocabulary of |V | frequent words for each language. Other words not inside these lists are represented by a universal symbol &lt;unk&gt;, one per language. We translate just like a word-based NMT system with respect to these source and target vocabularies, ex- cept for cases that involve &lt;unk&gt; in the source in- put or the target output. These correspond to the character-level components illustrated in <ref type="figure">Figure 1</ref>. A nice property of our hybrid approach is that by varying the vocabulary size, one can control how much to blend the word-and character-based models; hence, taking the best of both worlds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Source Character-based Representation</head><p>In regular word-based NMT, for all rare words out- side the source vocabulary, one feeds the univer- sal embedding representing &lt;unk&gt; as input to the encoder. This is problematic because it discards valuable information about the source word. To fix that, we learn a deep LSTM model over char- acters of source words. For example, in <ref type="figure">Figure 1</ref>, we run our deep character-based LSTM over 'c', 'u', 't', 'e', and '_' (the boundary symbol). The fi- nal hidden state at the top layer will be used as the on-the-fly representation for the current rare word.</p><p>The layers of the deep character-based LSTM are always initialized with zero states. One might propose to connect hidden states of the word- based LSTM to the character-based model; how- ever, we chose this design for various reasons. First, it simplifies the architecture. Second, it al- lows for efficiency through precomputation: be- fore each mini-batch, we can compute represen- tations for rare source words all at once. All in- stances of the same word share the same embed- ding, so the computation is per type. <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Target Character-level Generation</head><p>General word-based NMT allows generation of &lt;unk&gt; in the target output. Afterwards, there is usually a post-processing step that handles these unknown tokens by utilizing the alignment infor- mation derived from the attention mechanism and then performing simple word dictionary lookup or identity copy ( <ref type="bibr" target="#b20">Luong et al., 2015a;</ref><ref type="bibr" target="#b9">Jean et al., 2015a)</ref>. While this approach works, it suf- fers from various problems such as alphabet mis- matches between the source and target vocabular- ies and multi-word alignments. Our goal is to ad- dress all these issues and create a coherent frame- work that handles an unlimited output vocabulary.</p><p>Our solution is to have a separate deep LSTM that "translates" at the character level given the current word-level state. We train our system such that whenever the word-level NMT produces an &lt;unk&gt;, we can consult this character-level de- coder to recover the correct surface form of the un- known target word. This is illustrated in <ref type="figure">Figure 1</ref>.</p><p>The training objective in Eq. (3) now becomes:</p><formula xml:id="formula_5">J = J w + αJ c<label>(5)</label></formula><p>Here, J w refers to the usual loss of the word- level NMT; in our example, it is the sum of the negative log likelihood of generating {"un", "&lt;unk&gt;", "chat", "_"}. The remaining component J c corresponds to the loss incurred by the character-level decoder when predicting char- acters, e.g., {'j', 'o', 'l', 'i', '_'}, of those rare words not in the target vocabulary.</p><p>Hidden-state Initialization Unlike the source character-based representations, which are context-independent, the target character-level generation requires the current word-level context to produce meaningful translation. This brings up an important question about what can best represent the current context so as to initialize the character-level decoder. We answer this question in the context of the attention mechanism ( §3). The final vector˜hvector˜ vector˜h t , just before the softmax as shown in <ref type="figure">Figure 2</ref>, seems to be a good candidate to initialize the character-level decoder. The rea- son is that˜hthat˜ that˜h t combines information from both the context vector c t and the top-level recurrent state h t . We refer to it later in our experiments as the same-path target generation approach.</p><p>On the other hand, the same-path approach wor- ries us because all vectors˜hvectors˜ vectors˜h t used to seed the character-level decoder might have similar values, leading to the same character sequence being pro- duced. The reason is because˜hbecause˜ because˜h t is directly used in the softmax, Eq. (2), to predict the same &lt;unk&gt;. That might pose some challenges for the model to learn useful representations that can be used to ac- complish two tasks at the same time, that is to pre- dict &lt;unk&gt; and to generate character sequences. To address that concern, we propose another ap- proach called the separate-path target generation.</p><p>Our separate-path target generation approach works as follows. We mimic the process described in Eq. (4) to create a counterpart vector ˘ h t that will be used to seed the character-level decoder:</p><formula xml:id="formula_6">˘ h t = tanh( ˘ W [c t ; h t ])<label>(6)</label></formula><p>Here, ˘ W is a new learnable parameter matrix, with which we hope to release W from the pres- sure of having to extract information relevant to both the word-and character-generation pro- cesses. Only the hidden state of the first layer is initialized as discussed above. The other com- ponents in the character-level decoder such as the LSTM cells of all layers and the hidden states of higher layers, all start with zero values. Implementation-wise, the computation in the character-level decoder is done per word token in- stead of per type as in the source character com- ponent ( §4.2). This is because of the context- dependent nature of the decoder.</p><p>Word-Character Generation Strategy With the character-level decoder, we can view the fi- nal hidden states as representations for the surface forms of unknown tokens and could have fed these to the next time step. However, we chose not to do so for the efficiency reason explained next; in- stead, &lt;unk&gt; is fed to the word-level decoder "as is" using its corresponding word embedding.</p><p>During training, this design choice decou- ples all executions over &lt;unk&gt; instances of the character-level decoder as soon the word-level NMT completes. As such, the forward and back- ward passes of the character-level decoder over rare words can be invoked in batch mode. At test time, our strategy is to first run a beam search de- coder at the word level to find the best transla- tions given by the word-level NMT. Such trans- lations contains &lt;unk&gt; tokens, so we utilize our character-level decoder with beam search to gen- erate actual words for these &lt;unk&gt;.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluate the effectiveness of our models on the publicly available WMT'15 translation task from English into Czech with newstest2013 (3000 sen- tences) as a development set and newstest2015 (2656 sentences) as a test set. Two metrics are used: case-sensitive NIST BLEU ( <ref type="bibr" target="#b22">Papineni et al., 2002</ref>) and chrF 3 (Popovi´cPopovi´c, 2015). <ref type="bibr">3</ref> The latter measures the amounts of overlapping character n- grams and has been argued to be a better metric for translation tasks out of English. In terms of preprocessing, we apply only the standard tokenization practice. <ref type="bibr">4</ref> We choose for each language a list of 200 characters found in frequent words, which, as shown in <ref type="table">Table 1</ref>, can represent more than 98% of the vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Training Details</head><p>We train three types of systems, purely word- based, purely character-based, and hybrid. Com- mon to these architectures is a word-based NMT since the character-based systems are essentially word-based ones with longer sequences and the core of hybrid models is also a word-based NMT.</p><p>In training word-based NMT, we follow Lu-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vocab</head><p>Perplexity BLEU chrF 3 w c (a) Best WMT'15, big data ( <ref type="bibr">Bojar and Tamchyna, 2015)</ref> - - - 18.8 - Existing NMT (b) RNNsearch + unk replace ( <ref type="bibr" target="#b10">Jean et al., 2015b)</ref> 200K - - 15.7 - (c) Ensemble 4 models + unk replace (Jean et al., 2015b) 200K - - 18.3 - Our word-based NMT (d) Base + attention + unk replace 50K 5.9 - 17.5 42.4 (e) Ensemble 4 models + unk replace 50K - - 18.4 43.9 Our character-based NMT (f) Base-512 (600-step backprop) 200 - 2.4 3.8 25.9 (g) Base-512 + attention (600-step backprop) 200 - 1.6 17.5 46.6 (h) Base-1024 + attention (300-step backprop) 200 - 1.9 15.7 41.1 Our hybrid NMT (i) Base + attention + same-path 10K 4.9 1.7 14.1 37.2 (j) Base + attention + separate-path 10K 4.9 1.7 15.6 39.6 (k) Base + attention + separate-path + 2-layer char 10K 4.7 1.6 17.7 44.1 (l) Base + attention + separate-path + 2-layer char 50K 5.7 1.6 19.6 46.5 (m) Ensemble 4 models 50K - - 20.7 47.5 fled, (e) the gradient is rescaled whenever its norm exceeds 5, and (f) dropout is used with probabil- ity 0.2 according to <ref type="bibr" target="#b24">(Pham et al., 2014</ref>). We now detail differences across the three architectures.</p><p>Word-based NMT -We constrain our source and target sequences to have a maximum length of 50 each; words that go past the boundary are ignored. The vocabularies are limited to the top |V | most frequent words in both languages. Words not in these vocabularies are converted into &lt;unk&gt;. After translating, we will perform dictio- nary 5 lookup or identity copy for &lt;unk&gt; using the alignment information from the attention models. Such procedure is referred as the unk replace tech- nique ( <ref type="bibr" target="#b21">Luong et al., 2015b;</ref><ref type="bibr" target="#b9">Jean et al., 2015a)</ref>.</p><p>Character-based NMT -The source and target sequences at the character level are often about 5 times longer than their counterparts in the word- based models as we can infer from the statistics in <ref type="table">Table 1</ref>. Due to memory constraint in GPUs, we limit our source and target sequences to a maxi- mum length of 150 each, i.e., we backpropagate through at most 300 timesteps from the decoder to the encoder. With smaller 512-dimensional mod- els, we can afford to have longer sequences with <ref type="bibr">5</ref> Obtained from the alignment links produced by the Berkeley aligner ( <ref type="bibr" target="#b15">Liang et al., 2006</ref>) over the training corpus.</p><p>up to 600-step backpropagation.</p><p>Hybrid NMT -The word-level component uses the same settings as the purely word-based NMT. For the character-level source and target components, we experiment with both shallow and deep 1024-dimensional models of 1 and 2 LSTM layers. We set the weight α in Eq. (5) for our character-level loss to 1.0.</p><p>Training Time -It takes about 3 weeks to train a word-based model with |V | = 50K and about 3 months to train a character-based model. Train- ing and testing for the hybrid models are about 10- 20% slower than those of the word-based models with the same vocabulary size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head><p>We compare our models with several strong systems. These include the winning entry in WMT'15, which was trained on a much larger amount of data, 52.6M parallel and 393.0M mono- lingual sentences <ref type="bibr">(Bojar and Tamchyna, 2015)</ref>. <ref type="bibr">6</ref> In contrast, we merely use the provided parallel corpus of 15.8M sentences. For NMT, to the best of our knowledge, ( <ref type="bibr" target="#b10">Jean et al., 2015b</ref>) has the best published performance on English-Czech.</p><p>As shown in <ref type="table" target="#tab_1">Table 2</ref>, for a purely word-based approach, our single NMT model outperforms the best single model in ( <ref type="bibr" target="#b10">Jean et al., 2015b</ref>) by +1.8 points despite using a smaller vocabulary of only 50K words versus 200K words. Our ensemble system (e) slightly outperforms the best previous NMT system with 18.4 BLEU.</p><p>To our surprise, purely character-based models, though extremely slow to train and test, perform quite well. The 512-dimensional attention-based model (g) is best, surpassing the single word- based model in ( <ref type="bibr" target="#b10">Jean et al., 2015b</ref>) despite hav- ing much fewer parameters. It even outperforms most NMT systems on chrF 3 with 46.6 points. This indicates that this model translate words that closely but not exactly match the reference ones as evidenced in Section 6.3. We notice two in- teresting observations. First, attention is critical for character-based models to work as is obvious from the poor performance of the non-attentional model; this has also been shown in speech recog- nition ( <ref type="bibr" target="#b5">Chan et al., 2016)</ref>. Second, long time-step backpropagation is more important as reflected by the fact that the larger 1024-dimensional model (h) with shorter backprogration is inferior to (g).</p><p>Our hybrid models achieve the best results. At 10K words, we demonstrate that our separate- path strategy for the character-level target gener- ation ( §4.3) is effective, yielding an improvement of +1.5 BLEU points when comparing systems (j) vs. (i). A deeper character-level architecture of 2 LSTM layers provides another significant boost of +2.1 BLEU. With 17.7 BLEU points, our hybrid system (k) has surpassed word-level NMT models.</p><p>When extending to 50K words, we further im- prove the translation quality. Our best single model, system (l) with 19.6 BLEU, is already better than all existing systems. Our ensemble model (m) further advances the SOTA result to 20.7 BLEU, outperforming the winning entry in the WMT'15 English-Czech translation task by a large margin of +1.9 points. Our ensemble model is also best in terms of chrF 3 with 47.5 points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Analysis</head><p>This section first studies the effects of vocabulary sizes towards translation quality. We then analyze more carefully our character-level components by visualizing and evaluating rare word embeddings as well as examining sample translations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Effects of Vocabulary Sizes</head><p>As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, our hybrid models of- fer large gains of +2.1-11.4 BLEU points over strong word-based systems which already handle unknown words. With only a small vocabulary, e.g., 1000 words, our hybrid approach can pro- duce systems that are better than word-based mod- els that possess much larger vocabularies. While it appears from the plot that gains diminish as we increase the vocabulary size, we argue that our hy- brid models are still preferable since they under- stand word structures and can handle new complex words at test time as illustrated in Section 6.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Rare Word Embeddings</head><p>We evaluate the source character-level model by building representations for rare words and mea- suring how good these embeddings are.</p><p>Quantitatively, we follow <ref type="bibr" target="#b19">Luong et al. (2013)</ref> in using the word similarity task, specifically on the Rare Word dataset, to judge the learned represen- tations for complex words. The evaluation met- ric is the Spearman's correlation ρ between sim- ilarity scores assigned by a model and by human annotators. From the results in <ref type="table" target="#tab_2">Table 3</ref>, we can see that source representations produced by our hybrid 7 models are significantly better than those of the word-based one. It is noteworthy that our deep recurrent character-level models can outper- form the model of ( <ref type="bibr" target="#b19">Luong et al., 2013)</ref>, which uses recursive neural networks and requires a complex morphological analyzer, by a large margin. Our performance is also competitive to the best Glove  Qualitatively, we visualize embeddings pro- duced by the hybrid model (l) for selected words in the Rare Word dataset. <ref type="figure" target="#fig_3">Figure 4</ref> shows the two-dimensional representations of words com- puted by the Barnes-Hut-SNE algorithm <ref type="bibr" target="#b28">(van der Maaten, 2013</ref>). 8 It is extremely interesting to ob- serve that words are clustered together not only by the word structures but also by the meanings. For example, in the top-left box, the character- based representations for "loveless", "spiritless", "heartlessly", and "heartlessness" are nearby, but clearly separated into two groups. Similarly, in the <ref type="bibr">8</ref> We run Barnes-Hut-SNE algorithm over a set of 91 words, but filter out 27 words for displaying clarity. center boxes, word-based embeddings of "accept- able", "satisfactory", "unacceptable", and "unsat- isfactory", are close by but separated by mean- ings. Lastly, the remaining boxes demonstrate that our character-level models are able to build rep- resentations comparable to the word-based ones, e.g., "impossibilities" vs. "impossible" and "an- tagonize" vs. "antagonist". All of this evidence strongly supports that the source character-level models are useful and effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Sample Translations</head><p>We show in <ref type="table">Table 4</ref> sample translations between various systems. In the first example, our hybrid model translates perfectly. The word-based model fails to translate "diagnosis" because the second &lt;unk&gt; was incorrectly aligned to the word "af- ter". The character-based model, on the other hand, makes a mistake in translating names.</p><p>For the second example, the hybrid model sur- prises us when it can capture the long-distance re- ordering of "fifty years ago" and "p˘ red padesáti lety" while the other two models do not. The word-based model translates "Jr." inaccurately due to the incorrect alignment between the sec- ond &lt;unk&gt; and the word "said". The character- based model literally translates the name "King" into "král" which means "king".</p><p>Lastly, both the character-based and hybrid 1 <ref type="table">Table 4</ref>: Sample translations on newstest2015 -for each example, we show the source, human transla- tion, and translations of the following NMT systems: word model (d), char model (g), and hybrid model (k). We show the translations before replacing &lt;unk&gt; tokens (if any) for the word-based and hybrid models. The following formats are used to highlight correct, wrong, and close translation segments.</p><p>models impress us by their ability to translate compound words exactly, e.g., "11-year-old" and "jedenáctiletá"; whereas the identity copy strategy of the word-based model fails. Of course, our hy- brid model does make mistakes, e.g., it fails to translate the name "Shani Bart". Overall, these ex- amples highlight how challenging translating into Czech is and that being able to translate at the character level helps improve the quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have proposed a novel hybrid architecture that combines the strength of both word-and character-based models. Word-level models are fast to train and offer high-quality translation; whereas, character-level models help achieve the goal of open vocabulary NMT. We have demon- strated these two aspects through our experimental results and translation examples. Our best hybrid model has surpassed the perfor- mance of both the best word-based NMT system and the best non-neural model to establish a new state-of-the-art result for English-Czech transla- tion in WMT'15 with 20.7 BLEU. Moreover, we have succeeded in replacing the standard unk re- placement technique in NMT with our character- level components, yielding an improvement of +2.1−11.4 BLEU points. Our analysis has shown that our model has the ability to not only generate well-formed words for Czech, a highly inflected language with an enormous and complex vocab- ulary, but also build accurate representations for English source words.</p><p>Additionally, we have demonstrated the poten- tial of purely character-based models in produc- ing good translations; they have outperformed past word-level NMT models. For future work, we hope to be able to improve the memory usage and speed of purely character-based models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>ong et al.</head><label></label><figDesc>(2015a) to use the global attention mechanism together with similar hyperparame- ters: (a) deep LSTM models, 4 layers, 1024 cells, and 1024-dimensional embeddings, (b) uniform initialization of parameters in [−0.1, 0.1], (c) 6- epoch training with plain SGD and a simple learn- ing rate schedule -start with a learning rate of 1.0; after 4 epochs, halve the learning rate every 0.5 epoch, (d) mini-batches are of size 128 and shuf-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>System</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Vocabulary size effect-shown are the performances of different systems as we vary their vocabulary sizes. We highlight the improvements obtained by our hybrid models over word-based systems which already handle unknown words.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Barnes-Hut-SNE visualization of source word representations-shown are sample words from the Rare Word dataset. We differentiate two types of embeddings: frequent words in which encoder embeddings are looked up directly and rare words where we build representations from characters. Boxes highlight examples that we will discuss in the text. We use the hybrid model (l) in this visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Among the available language pairs in WMT'15, all involving English, we choose Czech as a target language for several reasons. First and foremost, Czech is a Slavic language with not only rich and</figDesc><table>English 
Czech 
word 
char 
word 
char 
# Sents 
15.8M 
# Tokens 254M 1,269M 224M 1,347M 
# Types 
1,172K 
2003 
1,760K 
2053 
200-char 
98.1% 
98.8% 

Table 1: WMT'15 English-Czech data -shown 
are various statistics of our training data such as 
sentence, token (word and character counts), as 
well as type (sizes of the word and character vo-
cabularies). We show in addition the amount of 
words in a vocabulary expressed by a list of 200 
characters found in frequent words. 

complex inflection, but also fusional morphology 
in which a single morpheme can encode multiple 
grammatical, syntactic, or semantic meanings. As 
a result, Czech possesses an enormously large vo-
cabulary (about 1.5 to 2 times bigger than that of 
English according to statistics in Table 1) and is 
a challenging language to translate into. Further-
more, this language pair has a large amount of 
training data, so we can evaluate at scale. Lastly, 
though our techniques are language independent, 
it is easier for us to work with Czech since Czech 
uses the Latin alphabet with some diacritics. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>WMT'15 English-Czech results -shown are the vocabulary sizes, perplexities, BLEU, and 
chrF 3 scores of various systems on newstest2015. Perplexities are listed under two categories, word (w) 
and character (c). Best and important results per metric are highlighed. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Word similarity task -shown are Spear-
man's correlation ρ on the Rare Word dataset of 
various models (with different vocab sizes |V |). 

</table></figure>

			<note place="foot" n="1"> Recently, Ling et al. (2015b) attempt character-level NMT; however, the experimental evidence is weak. The authors demonstrate only small improvements over word-level baselines and acknowledge that there are no differences of significance. Furthermore, only small datasets were used without comparable results from past NMT work.</note>

			<note place="foot" n="2"> While Ling et al. (2015b) found that it is slow and difficult to train source character-level models and had to resort to pretraining, we demonstrate later that we can train our deep character-level LSTM perfectly fine in an end-to-end fashion.</note>

			<note place="foot" n="3"> For NIST BLEU, we first run detokenizer.pl and then use mteval-v13a to compute the scores as per WMT guideline. For chrF3, we utilize the implementation here https://github.com/rsennrich/subword-nmt.</note>

			<note place="foot" n="4"> Use tokenizer.perl in Moses with default settings.</note>

			<note place="foot" n="6"> This entry combines two independent systems, a phrasebased Moses model and a deep-syntactic transfer-based model. Additionally, there is an automatic post-editing system with hand-crafted rules to correct errors in morphological agreement and semantic meanings, e.g., loss of negation.</note>

			<note place="foot" n="7"> We look up the encoder embeddings for frequent words and build representations for rare word from characters.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was partially supported by NSF Award IIS-1514268 and by a gift from Bloomberg L.P. We thank Dan Jurafsky, Andrew Ng, and Quoc Le for earlier feedback on the work, as well as Sam Bowman, Ziang Xie, and Jiwei Li for their valuable comments on the paper draft. Lastly, we thank NVIDIA Corporation for the donation of Tesla K40 GPUs as well as Andrew Ng and his group for letting us use their computing resources.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>word</head><p>Autor Stephen Jay &lt;unk&gt; zem˘ rel 20 let po &lt;unk&gt; . Autor Stephen Jay Gould zem˘ rel 20 let po po . char</p><p>Autor Stepher Stepher zem˘ rel 20 let po diagnóze .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>hybrid</head><p>Autor &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; zem˘ rel 20 let po &lt;unk&gt;.</p><p>Autor Stephen Jay Gould zem˘ rel 20 let po diagnóze . </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Dmitriy Serdyuk, Philemon Brakel, and Yoshua Bengio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-to-end attention-based large vocabulary speech recognition</title>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improved transition-based parsing by modeling characters instead of words with LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ond˘ rej Bojar and Ale˘ s Tamchyna</title>
	</analytic>
	<monogr>
		<title level="m">CUNI in WMT15: Chimera Strikes Again. In WMT</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Listen, attend and spell</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning character-level representations for part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cícero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianca</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zadrozny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On using very large target vocabulary for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Montreal neural machine translation systems for WMT&apos;15</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WMT</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Exploring the limits of language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Character-aware neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A hierarchical neural autoencoder for paragraphs and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Alignment by agreement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Finding function in form: Compositional character models for open vocabulary word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramon</forename><surname>Fermandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luís</forename><surname>Marujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Luís</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Character-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Stanford neural machine translation systems for spoken language domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IWSLT</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Better word representations with recursive neural networks for morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Addressing the rare word problem in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dropout improves recurrent neural networks for handwriting recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Théodore</forename><surname>Vu Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Bluche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérôme</forename><surname>Kermorvant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Louradour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICFHR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">chrF: character n-gram F-score for automatic MT evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Popovi´cpopovi´c</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WMT</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Barnes-Hut-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Recurrent neural network regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno>abs/1409.2329</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
