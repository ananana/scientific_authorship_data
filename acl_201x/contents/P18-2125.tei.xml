<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:05+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">&apos;Lighter&apos; Can Still Be Dark: Modeling Comparative Color Descriptions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivia</forename><surname>Winn</surname></persName>
							<email>olivia@cs.columbia.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">Data Science Institute</orgName>
								<orgName type="institution" key="instit1">Columbia University</orgName>
								<orgName type="institution" key="instit2">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Smaranda</forename><surname>Muresan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">Data Science Institute</orgName>
								<orgName type="institution" key="instit1">Columbia University</orgName>
								<orgName type="institution" key="instit2">Columbia University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">&apos;Lighter&apos; Can Still Be Dark: Modeling Comparative Color Descriptions</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="790" to="795"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>790</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a novel paradigm of grounding comparative adjectives within the realm of color descriptions. Given a reference RGB color and a comparative term (e.g., &apos;lighter&apos;, &apos;darker&apos;), our model learns to ground the comparative as a direction in the RGB space such that the colors along the vector, rooted at the reference color, satisfy the comparison. Our model generates grounded representations of comparative adjectives with an average accuracy of 0.65 cosine similarity to the desired direction of change. These vectors approach colors with Delta-E scores of under 7 compared to the target colors, indicating the differences are very small with respect to human perception. Our approach makes use of a newly created dataset for this task derived from existing labeled color data.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multimodal approaches to object recognition have achieved a degree of success by grounding adjec- tives and nouns from descriptive text in image fea- tures ( <ref type="bibr" target="#b2">Farhadi et al., 2009;</ref><ref type="bibr" target="#b3">Lampert et al., 2009;</ref><ref type="bibr" target="#b14">Russakovsky and Fei-Fei, 2010;</ref><ref type="bibr" target="#b5">Lazaridou et al., 2015)</ref>. One limitation of this approach, particu- larly for fine-grained object recognition, is when objects are differentiated not by having unique sets of attributes but by a difference in the strengths of their shared attributes ( <ref type="bibr" target="#b16">Wang et al., 2009;</ref><ref type="bibr" target="#b0">Duan et al., 2012;</ref><ref type="bibr" target="#b6">Maji et al., 2013;</ref><ref type="bibr" target="#b15">Vedaldi et al., 2014</ref>). In text, this difference is described using compar- ative adjectives. For example, the sexual dimor- phism of the American black duck is described with the phrase "females tend to be slightly paler than males, with duller olive bills". <ref type="bibr">1</ref> In a recent study of pragmatic referring expres- sion interpretation in the context of color selection, <ref type="bibr" target="#b11">Monroe et al. (2017)</ref> found that speakers almost always used comparative adjectives when the tar- get color was very similar to a distractor, rather than using multiple positive form adjectives to cre- ate a highly specific description of the color in- dependent of its surroundings. Though color has been studied in terms of its contextual dependence and vagueness in grounding <ref type="bibr" target="#b1">(Egré et al., 2013;</ref><ref type="bibr" target="#b7">McMahan and Stone, 2015;</ref><ref type="bibr" target="#b10">Monroe et al., 2016</ref><ref type="bibr" target="#b11">Monroe et al., , 2017</ref>, no approaches have focused explicitly on learning to ground comparative adjective; in this work we focus on comparative color descriptions.</p><p>The presence of distractors in the <ref type="bibr" target="#b11">Monroe et al. (2017)</ref> study is important -comparatives describe a change in a feature with respect to a reference point. While the description light blue can be un- derstood to represent a particular subset of col- ors in RGB, for example, neither 'lighter' nor 'lighter blue' have explicit representations; it is only with a reference that we can image what color either might refer to. If the reference color is a deep navy blue, then we imagine the target to be much closer to navy than, for example, a sky blue.</p><p>We propose a new paradigm of learning to ground comparative adjectives within the realm of color descriptions: given a reference RGB color and a comparative term (e.g. 'lighter', 'paler'), our deep learning model learns to ground the com- parative as a direction in RB space such that the colors along the vector, rooted at the reference color, satisfy the comparison (Section 3). The ref- erence color does more than quantify the specific RGB values to apply the comparative to: it also affects the grounding of the comparative. For ex- ample, 'darker' might seem like a simple change -simply reduce the values of all color channels equally towards 0. But as <ref type="figure">Fig 1 shows, '</ref>darker' refers to a different direction in RGB space de- pending on the reference color, and thus we need a reference-dependent approach.</p><p>Our approach makes use of a newly created dataset for this task derived from an existing la- beled color dataset <ref type="bibr" target="#b7">(McMahan and Stone, 2015)</ref> (Section 2). Our results in Section 5 show that our model generates grounded representations of comparative adjectives with an average accuracy of 0.65 cosine similarity to the desired direction of change. These learned vectors approach colors with Delta-E scores of under 7 compared to the target colors, indicating the differences are very small with respect to human perception.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data</head><p>We utilize the labeled RGB color data originally collected by <ref type="bibr" target="#b12">Munroe (2010)</ref>, through an online survey asking participants to provide free-form la- bels to various RGB samples. This data was then cleaned by <ref type="bibr" target="#b7">McMahan and Stone (2015)</ref>  <ref type="bibr">2</ref> . The cleaned data contains 821 color labels, averaging 600 RGB datapoints per label. These labels do not contain comparative adjectives, but many start with adjectives in the positive form (e.g., dusty, bright). As Lassiter and Goodman (2017) write, "Vague terms . . . are generally thought in linguis- tic semantics to rely on a free threshold vari- able: 'heavy' is interpreted as 'heaver than θ'." Coming back to the example of light blue, im- plicit in the term is the assumption that there is a reference blue, such that light blue is under- stood as 'lighter' than this reference. By rep- resenting this referential blue with the blue RGB samples from the data, we can assume the light blue RGB samples are 'lighter' than these ref- erences, giving us a quantitative θ in which to ground 'lighter'. Applying this process to the rest of the labels, we convert the original dataset into 415 tuples (reference color label, comparative adjective, and target color label), such as ( blue, 'lighter', light blue), where each color label is a set of RGB datapoints as in <ref type="bibr" target="#b7">McMahan and Stone (2015)</ref>. Note that not all labels containing quantifiers could be utilized in this manner; one does not consider cobalt blue to be 'more cobalt' than the average blue. The new dataset of 415 tu- ples contains 79 unique reference color labels and 81 unique comparatives and is made available on- line. <ref type="bibr">3</ref> While it is reasonable to believe that the com- parative adjective describes the relationship be- tween the colors in general, individual pairs of col- ors from the data may not display the appropriate θ. Thus, we make the assumption that the compar- ison holds true for the average of the target light blue samples, and use the average as our ground truth given the blue reference colors and the com- parative adjective 'lighter'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>We have chosen to represent comparative adjec- tives in RGB space as directions, such that given ain input RGB reference color r c and a compar- ative adjective w our model outputs a vector w g pointing from r c in the direction of change in RGB, which in training is measured against the di- rection towars a target color t c . <ref type="figure">Fig 1 is a</ref> good indication for why this representation is appro- priate; our output w g corresponds to the rate of change across the color bar, indicating the direc- tion along with the degree of the compared prop- erty increases. All points along this line are repre- sentations of w in respect to r c .</p><p>The network architecture consists of two fully connected layers, shown in <ref type="figure" target="#fig_1">Fig 2.</ref> The com- parative is represented as a bi-gram to account for comparatives which necessitate using 'more' (e.g. "more electric"); single-word compara- tives are preceded by the zero vector. We used the Google pre-trained word embeddings 4 with d=300 (Mikolov et al., 2013a,b). As these vectors are two orders of magnitude larger than the reference RGB color, we input the reference directly into both lay- ers of the network, helping to mitigate the loss of   <ref type="table">Table 1</ref>: Data Split information this dichotomy in size would other- wise produce (an empirical study of various input configurations determined inputting the color into only one of the layers to be insufficient). The out- put of the first hidden layer has d=30; each layer reduces the dimension of the output by an order of magnitude. The loss function of the model has two metrics. The first is the cosine similarity between w g and the vector from r c to t c . To restrain the length of w g , the second metric is the distance between t c and the result of w g + r c . Training the length of w g to roughly match the distance between r c and t c helps it to capture that the difference should be small enough to warrant a comparison rather than separate descriptors, while still representing enough of a difference to be comparable. <ref type="table">Table 1</ref> shows the data split between training and testing both in terms of tuples (#Tuples column) and in terms of the actual datapoint instances (#Dtpts column) for our experiments. To properly measure the accuracy of our model, our test set covers five input conditions:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>• Seen Pairings. The reference color label, the comparative adjective and their pairing have been seen in the training data.</p><p>• Unseen Pairings. The reference color label and the comparative adjective have been seen in the training data, but not their pairing.</p><p>• Unseen Ref. Color. The reference color la- bel, and thus all the corresponding RGB color datapoints, have not be seen in training, while the comparative has been seen in the training data.</p><p>• Unseen Comparative. The comparative ad- jective has not been seen in training, but the reference color label has been seen.</p><p>• Fully Unseen. Neither the comparative adjec- tive nor the reference color have been seen in the training.</p><p>For the conditions where the reference color la- bel has been seen in training, the actual RGB reference color datapoints associated with the la- bels were different from the ones used in training: 15% of the datapoints from each training reference color label were set aside for testing, providing RGB values close but not equivalent to those seen in training. 10% of the reference color labels were set aside for testing, as were 10% of the compar- ative words; this amounted to 8 reference colors and 8 comparatives. The number of tuples and ac- tual datapoints instances for each test condition is given in <ref type="table">Table 1</ref>. The network was trained at a 0.001 learning rate for 800 epochs, with the output of the first layer of dimension d=30.  <ref type="figure" target="#fig_2">Figure 3</ref> shows examples of learned groundings of comparatives for each of the five test conditions (Test Type column). It shows the reference RGB color datapoint r c (always unseen), the compara- tive word w, the learned grounding vector w g , the target color t c , and two scores: cosine similarity and Delta-E. The upper sample for each test type is an example of a highly accurate result, while the lower sample exemplifies failure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>Delta-E is a metric for understanding how the human eye perceives color differences <ref type="table">(Table 2)</ref>. This is a useful metric as distances in RGB space are not perceived linearly. <ref type="figure" target="#fig_3">Figure 4</ref> shows two example pairs of colors which are spaced equally in terms of distance in RGB, but in terms of the Delta-E metric the green colors are closer together.</p><p>As seen in <ref type="figure" target="#fig_2">Figure 3</ref>, grounding comparatives in directional vectors over RGB allows them to cap- ture a full range of modification of the reference color. Even for some of the error cases the re- sulting outputs tend to capture directions which are reasonable illustrations of the color the com- parative described. Though the 'darker' ground- ing example from unseen pairings is incorrectly de-saturating the reference color, it is also in fact making the color darker. Most impressive is the 'paler' example at the bottom, which is able to capture the direction of the comparative almost perfectly. Regarding failures, we see that they tend</p><formula xml:id="formula_0">Delta-E Perception ≤ 1.0 Imperceptible 1 -2</formula><p>Requires close observation 2 -10 Perceivable 11 -49</p><p>More similar than opposite 100</p><p>Exact opposites <ref type="table">Table 2</ref>: Delta-E Ranges to be of comparatives words that relate to a differ- ent color, such as 'more greenish' and 'bluer', rather than comparatives such as 'lighter'. <ref type="table">Table 3</ref> provides quantitative results in terms of average cosine similarity and average Delta-E. Overall, the average cosine similarity is 0.65, with an average Delta-E of 6.8. Separating the perfor- mance by test condition, we see that the conditions where the reference and comparatives were both seen perform the best (independent of whether the pairing was seen in training); again 'seen refer- ence' refers only to the label being seen and not the reference color datapoint itself. The fully unseen case performs the worst by far with respect to co- sine similarity, though it is not as deviant in Delta- E. It is again apparent that the performance of the model drops when given comparatives which refer to another color. <ref type="figure">Figure 5</ref> shows the comparative 'electric' ap- plied to colors outside of our dataset. With no known t c s we cannot quantitatively measure the accuracy, but we can qualitatively assess the re-   <ref type="table">Table 3</ref>: Results sults as plausible.</p><p>We also examined whether the model could generate plausible comparative terms given a r c and t c . All of the comparatives in the model's vo- cabulary were applied to r c , and the correspond- ing w g were sorted by cosine similarity to given reference-target direction. When given a green reference and a dark green target (both sampled from the test data), the model outputs 'truer', 'deeper', and 'darker' as the closest compara- tives.</p><p>In <ref type="figure">Figure 6</ref>, given a reference sampled from 'purple' and a target sampled from 'soft purple', the model outputs the 5 most plausible comparatives -'softer' was the 9 th closest. They are presented in descending order by distance be- tween the target color and its projection on the modifying vector. We see that the comparatives the model returns are semantically very similar, as are their corresponding w g vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Though color has been studied in terms of its con- textual dependence and vagueness in grounding  <ref type="bibr" target="#b1">Egré et al., 2013;</ref><ref type="bibr" target="#b7">McMahan and Stone, 2015;</ref><ref type="bibr" target="#b10">Monroe et al., 2016</ref><ref type="bibr" target="#b11">Monroe et al., , 2017</ref>, no approaches have focused explicitly on learning to ground compar- atives. Related to this work is that of image ranking, which is inherently a form of compar- ison ( <ref type="bibr" target="#b13">Parikh and Grauman, 2011;</ref><ref type="bibr" target="#b17">Yu and Grauman, 2014</ref>). However, ranking methods do not ground the comparatives themselves in image fea- tures. Besides the fact that no ranked color data exists, ranking methods are not flexible enough to handle the high dependence of color comparatives on the individual reference color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We propose a new paradigm of grounding com- parative adjectives describing colors as directions in RGB space such that the colors along the vector, rooted at the reference color, satisfy the compari- son. We introduce a new methodology for trans- forming labeled color data into comparative color data, and propose a simple but effective learning model that is able to accurately modify unseen col- ors and comparatives. With respect to the desired output, the representations have an average accu- racy of 0.65 cosine similarity, and average Delta- E scores of under 7. Our model can also pro- vide plausible descriptions of the difference be- tween a given reference and target pair, as well as the grounded representations of the comparatives generated, providing an explanation for the model decision. This model is the first step towards fine-grained object recognition through compar- ative descriptions, providing a way to utilize re- lational descriptive text. This approach could be extended to other properties such as size, tex- ture, or curvature. It could also be used to aid in zero-shot learning from text sources, generat- ing human-understandable explanations for cate- gorization of similar objects, or providing descrip- tions of new, unknown objects with respect to known ones.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>(</head><label></label><figDesc>Figure 1: Grounding 'darker'</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Network Architecture</figDesc><graphic url="image-3.png" coords="3,165.57,62.82,266.30,149.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Examples of learned comparatives for each test condition</figDesc><graphic url="image-4.png" coords="4,118.77,62.81,360.00,234.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Same RGB distance, different Delta-E</figDesc><graphic url="image-5.png" coords="5,72.00,62.81,218.25,52.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: Groundings for 'more electric'</figDesc><graphic url="image-6.png" coords="5,72.00,644.47,218.27,94.98" type="bitmap" /></figure>

			<note place="foot" n="1"> https://www.allaboutbirds.org/guide/ American_Black_Duck/id</note>

			<note place="foot" n="2"> A few of the labels (such as &apos;horrible&apos;) were manually discarded, as the corresponding set of colors were too widely spread across RGB space for the label to be considered as describing a distinct color.</note>

			<note place="foot" n="3"> https://bitbucket.org/o_winn/ comparative_colors 4 https://code.google.com/archive/p/ word2vec/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Discovering localized attributes for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Kun Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3474" to="3481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Vagueness and order effects in color categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Egré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vincent De Gardelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ripley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Logic, Language and Information</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="391" to="420" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Describing objects by their attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1778" to="1785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to detect unseen object classes by between-class attribute transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Christoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannes</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="951" to="958" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adjectival vagueness in a Bayesian model of interpretation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Lassiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Noah D Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthese</title>
		<imprint>
			<biblScope unit="volume">194</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3801" to="3836" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Liska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<title level="m">From Visual Attributes to Adjectives through Decompositional Distributional Semantics. Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="183" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.5151</idno>
		<title level="m">Fine-grained visual classification of aircraft</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A bayesian model of grounded color semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association of Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="103" to="115" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop at ICLR</title>
		<meeting>Workshop at ICLR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to generate compositional color descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Noah D Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Colors in context: A pragmatic neural model for grounded language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">D</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Noah D Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="325" to="338" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Color survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randall</forename><surname>Munroe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<title level="m">Relative Attributes. ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="503" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Attribute learning in large-scale datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Understanding objects in detail with fine-grained attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stavros</forename><surname>Tsogkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3622" to="3629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning models for object recognition from natural language descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josiah</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Markert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th British Machine Vision Conference (BMVC2009)</title>
		<meeting>the 20th British Machine Vision Conference (BMVC2009)</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fine-Grained Visual Comparisons with Local Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aron</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
