<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:23+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On metric embedding for boosting semantic similarity computations</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Subercaze</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Université de Lyon</orgName>
								<address>
									<addrLine>Saint-Etienne</addrLine>
									<postCode>F-42023</postCode>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Gravier</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory" key="lab1">UMR5516</orgName>
								<orgName type="laboratory" key="lab2">Laboratoire Hubert Curien</orgName>
								<orgName type="institution">CNRS</orgName>
								<address>
									<addrLine>Saint-Etienne</addrLine>
									<postCode>F-42000</postCode>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederique</forename><surname>Laforest</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Université de Saint-Etienne</orgName>
								<address>
									<addrLine>Jean Monnet</addrLine>
									<postCode>F-42000</postCode>
									<settlement>Saint-Etienne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">On metric embedding for boosting semantic similarity computations</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="8" to="14"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Computing pairwise word semantic similarity is widely used and serves as a building block in many tasks in NLP. In this paper, we explore the embedding of the shortest-path metrics from a knowledge base (Wordnet) into the Hamming hyper-cube, in order to enhance the computation performance. We show that, although an isometric embedding is untractable, it is possible to achieve good non-isometric embeddings. We report a speedup of three orders of magnitude for the task of computing Leacock and Chodorow (LCH) similarity while keeping strong correlations (r = .819, ρ = .826).</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Among semantic relatedness measures, seman- tic similarity encodes the conceptual distance be- tween two units of language -this goes beyond lexical ressemblance. When words are the speech units, semantic similarity is at the very core of many NLP problems. It has proven to be essen- tial for word sense disambiguation <ref type="bibr" target="#b22">(Mavroeidis et al., 2005;</ref><ref type="bibr" target="#b2">Basile et al., 2014</ref>), open domain ques- tion answering <ref type="bibr" target="#b35">(Yih et al., 2014)</ref>, and informa- tion retrieval on the Web ( <ref type="bibr" target="#b30">Varelas et al., 2005</ref>), to name a few. Two established strategies to es- timate pairwise word semantic similarity includes knowledge-based and distributional semantics.</p><p>Knowledge-based approaches exploit the struc- ture of the taxonomy (( <ref type="bibr" target="#b18">Leacock and Chodorow, 1998;</ref><ref type="bibr" target="#b14">Hirst and St-Onge, 1998;</ref><ref type="bibr" target="#b33">Wu and Palmer, 1994)</ref>), its content (( <ref type="bibr" target="#b1">Banerjee and Pedersen, 2002)</ref>), or both <ref type="bibr" target="#b28">(Resnik, 1995;</ref><ref type="bibr" target="#b21">Lin, 1998</ref>). In the earliest applications, Wordnet-based semantic similarity played a predominant role so that se- mantic similarity measures reckon with informa- tion from the lexical hierarchy. It therefore ignores contextual information on word occurrences and relies on humans to encode such hierarchies -a tedious task in practice. In contrast, well-known distributional semantics strategies encode seman- tic similarity using the correlation of statistical ob- servations on the occurrences of words in a textual corpora <ref type="bibr" target="#b21">(Lin, 1998)</ref>.</p><p>While providing a significant impact on a broad range of applications, <ref type="bibr" target="#b12">(Herbelot and Ganesalingam, 2013;</ref><ref type="bibr" target="#b17">Lazaridou et al., 2013;</ref><ref type="bibr" target="#b3">Beltagy et al., 2014;</ref><ref type="bibr" target="#b4">Bernardi et al., 2013;</ref><ref type="bibr" target="#b10">Goyal et al., 2013;</ref><ref type="bibr" target="#b19">Lebret et al., 2013)</ref>, distributional semantics -similarly to knowledge-based strategies -strug- gle to process the ever-increasing size of textual corpora in a reasonable amount of time. As an an- swer, embedding high-dimensional distributional semantics models for words into low-dimensional spaces (henceforth word embedding <ref type="bibr" target="#b8">(Collobert and Weston, 2008)</ref>) has emerged as a popular method. Word embedding utilizes deep learn- ing to learn a real-valued vector representation of words so that any vector distance -usually the cosine similarity -encodes the word-to-word se- mantic similarity. Although word embedding was successfully applied for several NLP tasks <ref type="bibr" target="#b13">(Hermann et al., 2014;</ref><ref type="bibr" target="#b0">Andreas and Klein, 2014;</ref><ref type="bibr" target="#b7">Clinchant and Perronnin, 2013;</ref><ref type="bibr" target="#b34">Xu et al., 2014;</ref><ref type="bibr" target="#b20">Li and Liu, 2014;</ref><ref type="bibr" target="#b10">Goyal et al., 2013)</ref>, it implies a slow training phase -measured in days <ref type="bibr" target="#b8">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b26">Mnih and Kavukcuoglu, 2013;</ref><ref type="bibr" target="#b24">Mikolov et al., 2013)</ref>, though re-embedding words seems promising <ref type="bibr" target="#b16">(Labutov and Lipson, 2013)</ref>. There is another usually under-considered issue: the tractability of the pairwise similarity computa- tion in the vector space for large volume of data. Despite these limitations, the current enthusiasm for word embedding certainly echoes the need for lightning fast word-to-word semantic similar- ity computation.</p><p>In this context, it is surprising that embedding semantic similarity of words in low dimensional spaces for knowledge-based approaches is under- studied. This oversight may well condemn the word-to-word semantic similarity task to remain corpus-dependant -i.e. ignoring the background knowledge provided by a lexical hierarchy.</p><p>In this paper, we propose an embedding of knowledge base semantic similarity based on the shortest path metric ( <ref type="bibr" target="#b18">Leacock and Chodorow, 1998)</ref>, into the Hamming hypercube of size n (the size of targeted binary codes). The Leacock and Chodorow semantic similarity is one of the most meaningful measure. It yields the second rank for highest correlation with the data collected by <ref type="bibr" target="#b25">(Miller and Charles, 1991)</ref>, and the first one within edge centric approaches, as shown by <ref type="bibr" target="#b29">(Seco et al., 2004</ref>). This method is only surpassed by the infor- mation theoretic based similarity from <ref type="bibr" target="#b15">(Jiang and Conrath, 1997)</ref>. A second study present similar result ( <ref type="bibr" target="#b5">Budanitsky and Hirst, 2006</ref>), while a third one ranks this similarity measure at the first rank for precision in paraphrase identification <ref type="bibr" target="#b23">(Mihalcea et al., 2006</ref>).</p><p>The hypercube embedding technique benefits from the execution of Hamming distance within a few cycles on modern CPUs. This allows the com- putation of several millions distances per second. Multi-index techniques allows the very fast com- putation of top-k queries ( <ref type="bibr" target="#b27">Norouzi et al., 2012</ref>) on the Hamming space. However, the dimension of the hypercube (i.e. the number of bits used to represent an element) should obey the threshold of few CPU words (64, 128 . . . , bits) to maintain such efficiency ( <ref type="bibr" target="#b11">Heo et al., 2012</ref>).</p><p>An isometric embedding requires a excessively high number of dimensions to be feasible. How- ever, in this paper we show that practical em- beddings exist and present a method to construct them. The best embedding presents very strong correlations (r = .819, ρ = .829) with the Lea- cock &amp; Chodorow similarity measure (LCH in the rest of this paper). Our experiments against the state-of-the art implementation including caching techniques show that performance is increased by up to three orders of magnitude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Shortest path metric embedding</head><p>Let us first introduce few notations. We denote H n 2 as an n-dimensional hypercube whose nodes are labeled by the 2 n binary n-tuples. The nodes are adjacent if and only if their corresponding n-tuples differ in exactly one position, i.e. their Hamming distance ( 1 ) is equal to one. In what follows, Q n denotes the metric space composed of H n 2 with 1 . We tackle the following problem: We aim at defining a function f that maps every node w of the taxonomy (Wordnet for Leacock &amp; Chodorow) into Q n so that for every pair of nodes:</p><formula xml:id="formula_0">∀(w i , w j ), d(w i , w j ) = λ · 1 (f (w i ), f (w j )),</formula><p>where λ is a scalar. For practical purposes, the construction of the mapping should also be rea- sonable in terms of time complexity.</p><p>Theoretical limitations Wordnet with its hyper- nym relation forms a partially ordered set (poset). The first approach is to perform an isometric em- bedding from the poset with shortest path distance into the Hamming hypercube. Such a mapping would exactly preserve the original distance in the embedding. As proven by <ref type="bibr" target="#b9">(Deza and Laurent, 1997)</ref>, poset lattices, with their shortest path met- ric, can be isometrically embedded into the hyper- cube, but the embedding requires 2 n dimensions. The resulting embedding would not fit in the mem- ory of any existing computer, for a lattice having more than 60 nodes. Using Wordnet, with tens of thousands synsets, this embedding is untractable. The bound given by Deza et al. is not tight, how- ever it would require a more than severe improve- ment to be of any practical interest.</p><p>Tree embedding To reduce the dimensionality, we weaken the lattice into a tree. We build a tree from the Wordnet's Hyponyms/Hypernyms poset by cutting 1,300 links, which correspond to roughly one percent of the edges in the original lat- tice. The nature of the cut to be performed can be subject to discussion. In this preliminary research, we used a simple approach. Since hypernyms are ordered, we decided to preserve only the first hy- pernym -semantically more relevant, or at least statistically -and to cut edges to other hypernyms. Our experiments in <ref type="table" target="#tab_2">Table 1</ref> shows that using the obtained tree instead of the lattice keeps a high correlation (r = .919, ρ = .931) with the origi- nal LCH distance, thus validating the approach. <ref type="bibr" target="#b31">(Wilkeit, 1990)</ref> showed that any k-ary tree of size n can be embedded into Q n−1 . We give an isometric embedding algorithm, which is linear in time and space, exhibiting a much better time complexity than Winkler's generic approach for graphs, running in O(n 5 ) <ref type="bibr" target="#b32">(Winkler, 1984)</ref>. Start- ing with an empty binary signature, the algorithm is the following: at each step of a depth-first pre- order traversal: if the node has k children, we set the signature for the i-th child by appending k ze- roes to the parent's signature and by setting the i-th of the k bits to one. An example is given in <ref type="figure" target="#fig_0">Figure  1</ref>. However, when using real-world datasets such as Wordnet, the embedding still requires several thousands of bits to represent a node. This dimen- sion reduction to tens of kilobits per node remains far from our goal of several CPU words, and calls for a task-specific approach.</p><p>Looking at the construction of the isometric em- bedding, the large dimension results from the ap- pending of bits to all nodes in the tree. This results in a large number of bits that are rarely set to one. At the opposite, the optimal embedding in terms of dimension is given by the approach of <ref type="bibr" target="#b6">(Chen and Stallmann, 1995</ref>) that assigns gray codes to each node. However, the embedding is not isomet- ric and introduces a very large error. As shown in <ref type="table" target="#tab_2">Table 1</ref>, this approach gives the most compact em- bedding with log 2 (87,000) = 17 bits, but leads to poor correlations (r = .235 and ρ = .186).</p><p>An exhaustive search is also out of reach: for a fixed dimension n and r nodes in the tree, the number of combinations C is given by:</p><formula xml:id="formula_1">C = (2 n )! (n − r)!</formula><p>Even with the smallest value of n = 17 and r = 87,000, we have C &gt; 10 10,000 . With n = 64, to align to a CPU word, C &gt; 10 100,000 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Non-isometric Embedding</head><p>Our approach is a trade-off between the isomet- ric embedding and the pre-order gray code solu- tion. When designing our algorithm, we had to decide which tree distance we will preserve, either between parent and children, or among siblings.</p><p>Therefore, we take into account the nature of the tree that we aim to embed into the hypercube. Let  first analyse the characteristics of the tree obtained from the cut. The tree has an average branching factor of 4.9, with a standard deviation of 14 and 96% of the nodes have a branching factor lesser than 20. At the opposite, the depth is very stable with an average of 8.5, a standard deviation of 2, and a maximum of 18. Consequently, we decide to preserve the parent-children distance over the very unstable siblings distance. To lower the di- mensions, we aim at allocating less than k bits for a node with k children, thus avoiding the signature extension taking place for every node in the iso- metric approach. Our approach uses the following principles.</p><p>Branch inheritance: each node inherits the signature from its father, but contrary to isometric embedding, the signature extension does not ap- ply to all the nodes in the tree. This guarantees the compactness of the structure.</p><p>Parentship preservation: when allocating less bits than required for the isometric embedding, we introduce an error. Our allocation favours as much as possible the parentship distance at the expense of the sibling distance. As a first allo- cation, for a node with k children, we allocate log 2 (k + 1) bits for the signatures, in order to guarantee the unicity of the signature. Each child node is assigned a signature extension using a gray code generation on the log 2 (k + 1) bits. The parent node simply extends its signature with log 2 (k + 1) zeroes, which is much more com- pact than the k bits from the isometric embedding algorithm.</p><p>Word alignment: The two previous techniques give a compact embedding for low-depth trees, which is the case of Wordnet. The dimension D of the embedding is not necessarily aligned to a CPU word size W : kW ≤ D ≤ (k + 1)W . We want to exploit the potential (k + 1)W − D bits that are unused but still processed by the CPU. For this purpose we rank the nodes along a value v(i), i ∈ N to decide which nodes are allowed to use extra bits. Since our approach favours parent/child distance, we want to allow additional bits for nodes that are both close to the root and the head of a large branch. To bal- ance the two values, we use the following formula: v(i) = (max depth − depth(i)) · log(size branch (i)) We therefore enable our approach to take full advantage of the otherwise unused bits.</p><p>In order to enhance the quality of the embed- ding, we also introduce two potential optimiza- tions:</p><p>The first is called Children-sorting: we allocate a better preserving signature to children having larger descents. A better signature is among the available the 2 log 2 (k+1) available, the one that re- duces the error with the parent node. We rank the children by the size of their descent and assign the signatures accordingly.</p><p>The second optimization is named Value- sorting and is depicted in <ref type="figure" target="#fig_1">Figure 2</ref>. Among the 2 log 2 (k+1) available signatures, only k + 1 will be assigned (one for the parent and k for the chil- dren). For instance in the case of 5 children as depicted in <ref type="figure" target="#fig_1">Figure 2</ref>, we allocate 3 bits for 6 signa- tures. We favor the parentship distance by select- ing first the signatures where one bit differs from the parent's one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we run two experiments to eval- uate both the soundness and the performance of our approach. In the first experiment, we test the quality of our embedding against the tree distance and the LCH similarity. The goal is to assess the soundness of our approach and to measure the cor- relation between the approximate embedding and the original LCH similarity.</p><p>In the second experiment we compare the com- putational performance of our approach against an optimized in-memory library that implements the LCH similarity.</p><p>Our algorithm called FSE for Fast Similarity Embedding, is implemented in Java and avail- able publicly <ref type="bibr">1</ref>   <ref type="figure">Figure 3</ref>: FSE: influence of optimizations and di- mensions on the correlation over the tree distance on Wordnet.</p><p>1246v3 with 16GB of memory, a 256Go PCI Ex- press SSD. The system runs a 64-bit Linux 3.13.0 kernel with Oracle's JDK 7u67.</p><p>The FSE algorithm is implemented in various flavours. FSE-Base denotes the basic algorithm, containing none of the optimizations detailed in the previous section. FSE-Base can be aug- mented with either or both of the optimizations. This latter version is denoted FSE-Best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Embedding</head><p>We first measure the correlation of the embedded distance with the original tree distance, to validate the approach and to determine the gain induced by the optimizations. <ref type="figure">Figure 3</ref> shows the influence of dimensions and optimizations on the Pearson's product moment correlation r. The base version reaches r = .77 for an embedding of dimension 128. Regarding the optimizations, children sort- ing is more efficient than value sorting, excepted for dimensions under 90. Finally, combined opti- mizations (FSE-Best) exhibit a higher correlation (r = .89) than the other versions.</p><p>We then measure the correlation with the Lea- cock &amp; Chodorow similarity measure. We com- pare our approach to the gray codes embedding from <ref type="bibr" target="#b6">(Chen and Stallmann, 1995)</ref> as well as the isometric embedding. We compute the correlation on 5 millions distances from the Wordnet-Core noun pairs 2 (  ding obtained using gray codes present a very low correlation with the original distance.</p><p>Similarly to the results obtained on the tree dis- tance correlation, FSE-Best exhibits the highest scores with r = .819 and ρ = .829, not far from the theoretical bound of r = .919 and ρ = .931 for the isometric embedding of the same tree. Our approach requires 650 times less bits than the iso- metric one, while keeping strong guarantees on the correlation with the original LCH distance. <ref type="table">Table 4</ref>.2 presents the computation time of the LCH similarity. This is computed using WS4J 3 , an efficient library that enables in-memory caching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Speedup</head><p>Because of the respective computational com- plexities of the Hamming distance and the shortest path algorithms, FSE unsurprisingly boosts LCH similarity computation by orders of magnitudes. When the similarity is computed on a small num- ber of pairs (a situation of the utmost practical in- terest), the factor of improvement is three orders of magnitude. This factor decreases to an amount of 800 times for very large scale applications. The reason of the decrease is that WS4J caching mech- anism becomes more efficient for larger numbers of comparisons. As the caching system stores shortest path between nodes, these computed val- ues are more likely to be a subpath of another query when the number of queries grows.</p><p>3 https://code.google.com/p/ws4j/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed in this paper a novel approach based on metric embedding to boost the computation of shortest-path based similarity measures such as the one of Leacock &amp; Chodorow. We showed that an isometric embedding of the Wordnet's hyper- nym/hyponym lattice does not lead to a practical solution. To tackle this issue, we weaken the lat- tice structure into a tree by cutting less relevant edges. We then devised an algorithm and several optimizations to embed the tree shortest-path dis- tance in a word-aligned number of bits. Such an embedding can be used to boost NLP core algo- rithms -this was demonstrated here on the com- putation of LCH for which our approach offers a factor of improvement of three orders of magni- tude, with a very strong correlation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Construction of isometric embedding on a sample tree. For this six nodes tree, the embedding requires five bits.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Approaches to reduce the tree embedding dimensions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>.</head><label></label><figDesc></figDesc><table>Our testbed is an Intel Xeon E3 

1 Source code, binaries and instructions to reproduce 

80 
90 
100 
110 
120 
130 

0.75 

0.8 

0.85 

0.9 

Embedding dimension 

Pearson's r 

Combined 

Base + value sorting 

Base + children sorting 

FSE-Base 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 ). As expected, the embed-</head><label>1</label><figDesc></figDesc><table>the experiments are available at http://demo-satin. 
telecom-st-etienne.fr/FSE/</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Running time in milliseconds for pairwise 
similarity computations. 

</table></figure>

			<note place="foot" n="2"> https://wordnet.princeton.edu/ wordnet/download/standoff/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is supported by the OpenCloudware project. OpenCloudware is funded by the French FSN (Fond national pour la Société Numérique), and is supported by Pôles Minalogic, Systematic and SCS.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">How much do word embeddings encode about syntax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An adapted lesk algorithm for word sense disambiguation using wordnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satanjeev</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational linguistics and intelligent text processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="136" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An enhanced lesk word sense disambiguation algorithm through a distributional semantic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierpaolo</forename><surname>Basile</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annalina</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Semeraro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1591" to="1600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantic parsing using distributional semantics and probabilistic logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Islam Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Erk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A relatedness benchmark to test the role of determiners in compositional distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="53" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Evaluating wordnet-based measures of lexical semantic relatedness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Budanitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="13" to="47" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On embedding binary trees into hypercubes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias Fm</forename><surname>Woei-Kae Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stallmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Parallel and Distributed Computing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="132" to="138" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Aggregating continuous word embeddings for information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Clinchant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Geometry of Cuts and Metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Deza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Laurent</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">588</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A structured distributional semantic model for event co-reference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartik</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujay</forename><surname>Kumar Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrinmaya</forename><surname>Sachan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashank</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="467" to="473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spherical hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae-Pil</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngwoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung-Eui</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2957" to="2964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Measuring semantic content in distributional vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurélie</forename><surname>Herbelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><surname>Ganesalingam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="440" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semantic frame identification with distributed word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Lexical chains as representations of context for the detection and correction of malapropisms. WordNet: An electronic lexical database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>St-Onge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">305</biblScope>
			<biblScope unit="page" from="305" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semantic similarity based on corpus statistics and lexical taxonomy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conrath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Research on Computational Linguistics International Conference</title>
		<meeting>the 10th Research on Computational Linguistics International Conference</meeting>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Re-embedding words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Labutov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (2)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="489" to="493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Compositional-ly derived representations of morphologically complex words in distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1517" to="1526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Combining local context and wordnet similarity for word sense identification. WordNet: An electronic lexical database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Leacock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Is Deep Learning Really Necessary for Word Embeddings?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Lebret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joël</forename><surname>Legrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
	<note>Idiap</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improving text normalization via unsupervised model and discriminative reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">86</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An information-theoretic definition of similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="296" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Word sense disambiguation for exploiting hierarchical thesauri in text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Mavroeidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tsatsaronis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michalis</forename><surname>Vazirgiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Theobald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Knowledge Discovery in Databases: PKDD 2005</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="181" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Corpus-based and knowledge-based measures of text semantic similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Corley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Strapparava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="775" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chenand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corradoand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Contextual correlates of semantic similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language and cognitive processes</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning word embeddings efficiently with noise-contrastive estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2265" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fast search in hamming space with multiindex hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Punjani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David J</forename><surname>Fleet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3108" to="3115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Using information content to evaluate semantic similarity in a taxonomy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 14th International Joint Conference on Artificial Intelligence<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1995" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="448" to="453" />
		</imprint>
	</monogr>
	<note>IJCAI&apos;95</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An intrinsic information content metric for semantic similarity in wordnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Seco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Veale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jer</forename><surname>Hayes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECAI</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">1089</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semantic similarity methods in wordnet and their application to information retrieval on the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giannis</forename><surname>Varelas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Epimenidis</forename><surname>Voutsakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paraskevi</forename><surname>Raftopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Euripides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Petrakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Milios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th annual ACM international workshop on Web information and data management</title>
		<meeting>the 7th annual ACM international workshop on Web information and data management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="10" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Isometric embeddings in hamming graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elke</forename><surname>Wilkeit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Combinatorial Theory</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="197" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Isometric embedding in products of complete graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="221" to="225" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Verbs semantics and lexical selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibiao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd annual meeting on Association for Computational Linguistics</title>
		<meeting>the 32nd annual meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="133" to="138" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Product feature mining: Semantic clues versus syntactic constituents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="336" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Semantic parsing for single-relation question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
