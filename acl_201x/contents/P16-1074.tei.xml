<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Chinese Zero Pronoun Resolution with Deep Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Human Language Technology Research Institute University of Texas at Dallas Richardson</orgName>
								<address>
									<postCode>75083-0688</postCode>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Human Language Technology Research Institute University of Texas at Dallas Richardson</orgName>
								<address>
									<postCode>75083-0688</postCode>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Chinese Zero Pronoun Resolution with Deep Neural Networks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="778" to="788"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>While unsupervised anaphoric zero pronoun (AZP) resolvers have recently been shown to rival their supervised counterparts in performance, it is relatively difficult to scale them up to reach the next level of performance due to the large amount of feature engineering efforts involved and their ineffectiveness in exploiting lexical features. To address these weaknesses, we propose a supervised approach to AZP resolution based on deep neural networks, taking advantage of their ability to learn useful task-specific representations and effectively exploit lexical features via word embeddings. Our approach achieves state-of-the-art performance when resolving the Chinese AZPs in the OntoNotes corpus.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A zero pronoun (ZP) is a gap in a sentence that is found when a phonetically null form is used to refer to a real-world entity. An anaphoric zero pro- noun (AZP) is a ZP that corefers with one or more preceding mentions in the associated text. Below is an example taken from the Chinese Treebank (CTB), where the ZP (denoted as *pro*) refers to 俄罗斯 (Russia).</p><p>[俄罗斯] 作为米洛舍夫维奇一贯的支持者， *pro* 曾经提出调停这场政治危机。 ( <ref type="bibr">[Russia]</ref> is a consistent supporter of Milošević, *pro* has proposed to mediate the political crisis.)</p><p>As we can see, ZPs lack grammatical attributes that are useful for overt pronoun resolution such as number and gender. This makes ZP resolution more challenging than overt pronoun resolution.</p><p>Automatic ZP resolution is typically composed of two steps. The first step, AZP identification, in- volves extracting ZPs that are anaphoric. The sec- ond step, AZP resolution, aims to identify an an- tecedent of an AZP. State-of-the-art ZP resolvers have tackled both of these steps in a supervised manner, training one classifier for AZP identifica- tion and another for AZP resolution (e.g., <ref type="bibr">Zhao and Ng (2007)</ref>, <ref type="bibr" target="#b14">Kong and Zhou (2010)</ref>).</p><p>More recently, <ref type="bibr">Chen and Ng (2014b;</ref> have proposed unsupervised probabilistic AZP resolu- tion models (henceforth the CN14 model and the CN15 model, respectively) that rival their super- vised counterparts in performance. An appeal- ing aspect of these unsupervised models is that their language-independent generative process en- ables them to be applied to languages where data annotated with ZP links are not readily avail- able. Though achieving state-of-the-art perfor- mance, these models have several weaknesses.</p><p>First, a lot of manual efforts need to be spent on engineering the features for generative proba- bilistic models, as these models are sensitive to the choice of features. For instance, having features that are (partially) dependent on each other could harm model performance. Second, in the absence of labeled data, it is difficult, though not impos- sible, for these models to profitably employ lexi- cal features (e.g., word pairs, syntactic patterns in- volving words), as determining which lexical fea- tures are useful and how to combine the poten- tially large number of lexical features in an un- supervised manner is a very challenging task. In fact, the unsupervised models proposed by <ref type="bibr">Chen and Ng (2014b;</ref> are unlexicalized, presum- ably owing to the aforementioned reasons. Unfor- tunately, as shown in previous work (e.g, <ref type="bibr">Zhao and Ng (2007)</ref>, <ref type="bibr" target="#b1">Chen and Ng (2013)</ref>), the use of lex- ical features contributed significantly to the per- formance of state-of-the-art supervised AZP re- solvers. Finally, owing to the lack of labeled data, the model parameters are learned to maximize data likelihood, which may not correlate well with the desired evaluation measure (i.e., F-score). Hence, while unsupervised resolvers have achieved state- of-the-art performance, these weaknesses together suggest that it is very challenging to scale these models up so that they can achieve the next level of performance.</p><p>Our goal in this paper is to improve the state of the art in AZP resolution. Motivated by the aforementioned weaknesses, we propose a novel approach to AZP resolution using deep neural net- works, which we believe has three key advantages over competing unsupervised counterparts.</p><p>First, deep neural networks are particularly good at discovering hidden structures from the input data and learning task-specific representations via successive transformations of the input vectors, where different layers of a network correspond to different levels of abstractions that are useful for the target task. For the task of AZP resolution, this is desirable. Traditionally, it is difficult to cor- rectly resolve an AZP if its context is lexically dif- ferent from its antecedent's context. This is es- pecially the case for unsupervised resolvers. In contrast, a deep network can handle difficult cases like this via learning representations that make lex- ically different contexts look similar.</p><p>Second, we train our deep network in a super- vised manner. 1 In particular, motivated by re- cent successes of applying the mention-ranking model <ref type="bibr">(Denis and Baldridge, 2008)</ref> to entity coref- erence resolution (e.g., <ref type="bibr" target="#b0">Chang et al. (2013)</ref>, <ref type="bibr">Durrett and Klein (2013)</ref>, <ref type="bibr">Clark and Manning (2015)</ref>, <ref type="bibr" target="#b15">Martschat and Strube (2015)</ref>, <ref type="bibr" target="#b25">Wiseman et al. (2015)</ref>), we propose to employ a ranking-based deep network, which is trained to assign the high- est probability to the correct antecedent of an AZP given a set of candidate antecedents. This con- trasts with existing supervised AZP resolvers, all of which are classification-based. Optimizing this objective function is better than maximizing data likelihood, as the former is more tightly coupled with the desired evaluation metric (F-score) than the latter.</p><p>Finally, given that our network is trained in a su- pervised manner, we can extensively employ lex- 1 Note that deep neural networks do not necessarily have to be trained in a supervised manner. In fact, in early research on extending semantic modeling using auto-encoders <ref type="bibr" target="#b18">(Salakhutdinov and Hinton, 2007)</ref>, the networks were trained in an un- supervised manner, where the model parameters were opti- mized for the reconstruction of the input vectors. ical features and use them in combination with other types of features that have been shown to be useful for AZP resolution. However, rather than employing words directly as features, we employ word embeddings trained in an unsupervised man- ner. The goal of the deep network will then be to take these task-independent word embeddings as input and convert them into embeddings that would work best for AZP resolution via super- vised learning. We call our approach an embed- ding matching approach because the underlying deep network attempts to compare the embedding learned for an AZP with the embedding learned for each of its antecedents.</p><p>To our knowledge, this is the first approach to AZP resolution based on deep networks. When evaluated on the Chinese portion of the OntoNotes 5.0 corpus, our embedding matching approach to AZP resolution outperforms the CN15 model, achieving state-of-the-art results.</p><p>The rest of the paper is organized as follows. Section 2 overviews related work on zero pro- noun resolution for Chinese and other languages. Section 3 describes our embedding matching ap- proach, specifically the network architecture and the way we train and apply the network. We present our evaluation results in Section 4 and our conclusions in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Chinese ZP resolution. Early approaches to Chinese ZP resolution are rule-based. <ref type="bibr">Converse (2006</ref><ref type="bibr">) applied Hobbs' algorithm (Hobbs, 1978</ref> to resolve the ZPs in the CTB documents. <ref type="bibr">Yeh and Chen (2007)</ref> hand-engineered a set of rules for ZP resolution based on Centering The- ory ( <ref type="bibr">Grosz et al., 1995)</ref>.</p><p>In contrast, virtually all recent approaches to this task are learning-based. <ref type="bibr">Zhao and Ng (2007)</ref> are the first to employ a supervised learning ap- proach to Chinese ZP resolution. They trained an AZP resolver by employing syntactic and po- sitional features in combination with a decision tree learner. Unlike Zhao and Ng, Kong and Zhou (2010) employed context-sensitive convolu- tion tree kernels ( <ref type="bibr">Zhou et al., 2008</ref>) in their re- solver to model syntactic information. <ref type="bibr" target="#b1">Chen and Ng (2013)</ref> extended Zhao and Ng's feature set with novel features that encode the context surrounding a ZP and its candidate antecedents, and exploited the coreference links between ZPs as bridges to find textually distant antecedents for ZPs. As men- tioned above, there have been attempts to perform unsupervised AZP resolution. For instance, us- ing only data containing manually resolved overt pronouns, Chen and Ng (2014a) trained a super- vised overt pronoun resolver and applied it to re- solve AZPs. More recently, <ref type="bibr">Chen and Ng (2014b;</ref> have proposed unsupervised probabilistic AZP resolution models that rivaled their super- vised counterparts in performance. While we aim to resolve anaphoric ZPs, <ref type="bibr" target="#b17">Rao et al. (2015)</ref> re- solved deictic non-anaphoric ZPs, which "refer to salient entities in the environment such as the speaker, hearer or pragmatically accessible refer- ent without requiring any introduction in the pre- ceding text''.</p><p>ZP resolution for other languages. There have been rule-based and supervised machine learn- ing approaches for resolving ZPs in other lan- guages. For example, to resolve ZPs in Spanish texts, <ref type="bibr">Ferrández and Peral (2000)</ref> proposed a set of hand-crafted rules that encode preferences for candidate antecedents. In addition, supervised ap- proaches have been extensively employed to re- solve ZPs in Korean (e.g., <ref type="bibr">Han (2006)</ref>), Japanese (e.g., <ref type="bibr" target="#b22">Seki et al. (2002)</ref>, <ref type="bibr" target="#b12">Isozaki and Hirao (2003)</ref>, <ref type="bibr" target="#b6">Iida et al. (2006;</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>In this section, we first introduce our network ar- chitecture (Section 3.1), and then describe how we train it (Section 3.2) and apply it (Section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Network Architecture</head><p>The network architecture is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Since we employ a ranking model to rank the can- didate antecedents of an AZP z, the inputs to the network are (1) a feature vector representing the AZP, and (2) n feature vectors representing its n candidate antecedents, c 1 , c 2 , . . ., c n . As will be explained in detail in Section 3.2.2, the features in each feature vector can be divided into two types: word embedding features and hand-crafted fea- tures. Each input feature vector will then be passed through three hidden layers in the network, which will successively map it into a low-dimensional feature space. The resulting vector can be viewed as the low-dimensional semantic embedding of the corresponding input vector. Finally, the model computes a matching score between z and each of its candidate antecedents based on their low- dimensional representations. These scores are then normalized into probabilities using a softmax.</p><p>More formally, let x e (z) and x h (z) be the vec-tors of embedding and hand-crafted features rep- resenting AZP z respectively, and let x e (c i ) and x h (c i ) be the vectors of embedding and hand- crafted features representing candidate antecedent c i respectively. In addition, let y(z) and y(c i ) be the (low-dimensional) output vectors for z and c i respectively, l 1 , l 2 , and l 3 be the intermediate hid- den layers, W i and W ′ i be the weight matrices as- sociated with z and the c i 's in hidden layer i, b i and b ′ i be the bias terms associated with z and the c i 's. <ref type="bibr">2</ref> We then have:</p><formula xml:id="formula_0">l 1 (z) = f (W 1 x e (z) + b 1 ) l 2 (z) = l 1 (z) ⊕ x h (z) l 3 (z) = f (W 2 l 2 (z) + b 2 ) y(z) = f (W 3 l 3 (z) + b 3 ) (1) l 1 (c i ) = f (W ′ 1 x e (c i ) + b ′ 1 ) l 2 (c i ) = l 1 (c i ) ⊕ x h (c i ) l 3 (c i ) = f (W ′ 2 l 2 (c i ) + b ′ 2 ) y(c i ) = f (W ′ 3 l 3 (z) + b ′ 3 ) (2)</formula><p>where f is the activation function at output layer y and hidden layers l 1 and l 3 . In this network, we employ tanh as the activation function. Hence,</p><formula xml:id="formula_1">f (x) = tanh(x) = 1 − e −2x 1 + e −2x (3)</formula><p>The matching score between an AZP z and a candidate antecedent c i is then measured as:</p><formula xml:id="formula_2">R(z, c i ) = cos(y(z), y(c i )) = y(z) T y(c i ) ||y(z)||||y(c i )|| (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Training Instance Creation</head><p>We create one training instance from each AZP in each training document. Since our model is ranking-based, each training instance corresponds to an AZP z and all of its candidate antecedents C i . In principle, we can follow previous work and assume that the set of candidate antecedents C contains all and only those maximal or modifier noun phrases (NPs) that precede z in the associ- ated text and are at most two sentences away from it. However, to improve training efficiency, we select exactly four candidate antecedents for each AZP z as follows. First, we take the closest correct antecedent z to be one of the four candidate an- tecedents. Next, we compute a salience score for each of its non-coreferent candidate antecedents and select the three with the highest salience scores as the remaining three candidate antecedents.</p><p>We compute salience as follows. For each AZP z, we compute the salience score for each (partial) entity preceding z. 3 To reduce the size of the list of preceding entities, we only consider a partial entity active if at least one of its mentions appears within two sentences of the active AZP z. We compute the salience score of each active entity w.r.t. z us- ing the following equation:</p><formula xml:id="formula_3">∑ m∈E g(m) * decay(m)<label>(5)</label></formula><p>where m is a mention belonging to active entity E, g(m) is a grammatical score which is set to 4, 2, or 1 depending on whether m's grammatical role is Subject, Object, or Other respectively, and decay(m) is a decay factor that is set to 0.5 dis (where dis is the sentence distance between m and z).</p><p>Finally, we assign the correct label (i.e., the matching score) to each candidate antecedent. The score is 1 for the correct antecedent and 0 other- wise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Features</head><p>As we can see from <ref type="figure" target="#fig_0">Figure 1</ref>, each input feature vector, regardless of whether it is representing an AZP or one of its candidate antecedents, is com- posed of two types of features, embedding features and hand-crafted features, as described below.</p><p>Embedding features. To encode the lexical con- texts of the AZP and its candidate antecedents, one could employ one-hot vectors. However, the re- sulting lexical features may suffer from sparsity. To see the reason, assuming that the vocabulary size is V and the number of neurons in the first hidden layer l 1 is L 1 , the size of the weight ma- trices W 1 and W ′ 1 is V * L 1 , which in our dataset is around two million while the number of training examples is much smaller.</p><p>Therefore, instead of using one-hot vectors, we employ embedding features. Specifically, we em- ploy the pre-trained word embeddings (of size 100) Syntactic features <ref type="formula">(13)</ref> whether z is the first gap in an IP clause; whether z is the first gap in a subject-less IP clause, and if so, POS(w1); whether POS(w1) is NT; whether w1 is a verb that appears in a NP or VP; whether P l is a NP node; whether Pr is a VP node; the phrasal label of the parent of the node containing POS(w1); whether V has a NP, VP or CP ancestor; whether C is a VP node; whether there is a VP node whose parent is an IP node in the path from w1 to C. Other features <ref type="formula" target="#formula_4">(6)</ref> whether z is the first gap in a sentence; whether z is in the headline of the text; the type of the clause in which z appears; the grammatical role of z (Subject, Object, or Other); whether w−1 is a punctuation; whether w−1 is a comma. <ref type="table">Table 1</ref>: Hand-crafted features associated with an AZP. z is a zero pronoun. V is the VP node following z. wi is the ith word to the right of z (if i is positive) or the ith word to the left of z (if i is negative). C is lowest common ancestor of w−1 and w1. P l and Pr are the child nodes of C that are the ancestors of w−1 and w1 respectively. <ref type="formula">(2)</ref> whether c is in the headline of the text; whether c is a subject whose governing verb is lexically identical to the verb governing of z. obtained by training word2vec 4 on the Chinese portion of the training data from the OntoNotes 5.0 corpus. For an AZP z, we first find the word pre- ceding it and its governing verb, and then concate- nate the embeddings of these two words to form the AZP's embedding features. (If z happens to begin a sentence, we use a special embedding to represent the word preceding it.) For a candidate antecedent, we employ the word embedding of its head word as its embedding features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Syntactic</head><note type="other">features (12) whether c has an ancestor NP, and if so, whether this NP is a descendent of c's lowest ancestor IP; whether c has an ancestor VP, and if so, whether this VP is a descendent of c's lowest ancestor IP; whether c has an ancestor CP; the grammatical role of c (Subject, Object, or Other); the clause type in which c appears; whether c is an adverbial NP, a temporal NP, a pronoun or a named entity. Distance features (4) the sentence distance between c and z; the segment distance between c and z, where segments are separated by punctuations; whether c is the closest NP to z; whether c and z are siblings in the associated parse tree. Other features</note><p>Hand-crafted features. The hand-crafted fea- tures are (low-dimensional) features that capture the syntactic, positional and other relationships between an AZP and its candidate antecedents. These features are similar to the ones employed in previous work on AZP resolution (e.g., <ref type="bibr">Zhao and Ng (2007)</ref>, <ref type="bibr" target="#b14">Kong and Zhou (2010)</ref>, <ref type="bibr" target="#b1">Chen and Ng (2013)</ref>).</p><p>We split these hand-crafted features into two disjoint sets: those associated with an AZP and those associated with a candidate antecedent. If a feature is computed based on the AZP, then we regard it as a feature associated with the AZP; oth- erwise, we put it in the other feature set. A brief description of the hand-crafted features associated with an AZP and those associated with a candidate antecedent are shown in <ref type="table" target="#tab_0">Table 1 and Table 2</ref> re- spectively. Note that we convert each multi-valued feature into a corresponding set of binary-valued features (i.e., if a feature has N different values, <ref type="bibr">4</ref> https://code.google.com/p/word2vec/ we will create N binary indicators to represent it). To ensure that the number of hand-crafted fea- tures representing an AZP is equal to the number of hand-crafted features representing a candidate an- tecedent <ref type="bibr">5</ref> , we append to the end of a feature vector as many dummy zeroes as needed. <ref type="bibr">6</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Parameter Estimation</head><p>We employ online learning to train the network, with one training example in a mini-batch. In other words, we update the weights after processing each training example based on the correct matching scores of the training example (which is 1 for the correct antecedent and 0 otherwise) and the net- work's predicted matching scores.</p><p>To compute the predicted matching score be- tween AZP z and one of its candidate antecedents c i , we apply the following softmax function:</p><formula xml:id="formula_4">P (c i |z, Λ) = exp(γR(z, c i )) ∑ c ′ ∈C exp(γR(z, c ′ ))<label>(6)</label></formula><p>where (1) γ is a smoothing factor that is empiri- cally set on a held-out data set, (2) R(z, c i ) is the cosine similarity between vector y(z) and vector y(c i ) (see Section 3.1), (3) C denotes the set of can- didate antecedents of z, and (4) Λ denotes the set of parameters of our neural network:</p><formula xml:id="formula_5">Λ = {W 1 , W 2 , W 3 , b 1 , b 2 , b 3 , W ′ 1 , W ′ 2 , W ′ 3 , b ′ 1 , b ′ 2 , b ′ 3 }<label>(7)</label></formula><p>To maximize the matching score of the correct antecedent, we estimate the model parameters to minimize the following loss function:</p><formula xml:id="formula_6">J z (Λ) = − ∑ c i ∈C δ(z, c i )P (c i |z, Λ)<label>(8)</label></formula><p>where δ(z, c i ) is an indicator function indicating whether AZP z and candidate antecedent c i are coreferent:</p><formula xml:id="formula_7">δ(z, c i ) = { 1, if z and c i are coreferent 0, otherwise<label>(9)</label></formula><p>Since J z (Λ) is differentiable w.r.t. to Λ, we train the model using stochastic gradient descent. Specifically, the model parameters Λ are updated according to the following update rule:</p><formula xml:id="formula_8">Λ t = Λ t−1 − α ∂J (Λ t−1 ) ∂Λ t−1<label>(10)</label></formula><p>where α is the learning rate, and Λ t and Λ t−1 are model parameters at the tth iteration and the (t − 1)th iteration respectively. To avoid overfit- ting, we determine the hyperparameters of the net- work using a held-out development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Inference</head><p>After training, we can apply the resulting network to find an antecedent for each AZP. Each test in- stance corresponds to an AZP z and four of its can- didate antecedents. Specifically, the four candi- date antecedents with the highest salience scores will be chosen. Importantly, unlike in training, where we guarantee that the correct antecedents is among the set of candidate antecedents, in testing, we don't. We use the network to rank the candidate antecedents by computing the posterior probability of each of them being a correct antecedent of z, and select the one with the highest probability to be its antecedent.</p><p>The aforementioned resolution procedure can be improved, however. The improvement is moti- vated by a problem we observed previously ( <ref type="bibr" target="#b1">Chen and Ng, 2013)</ref>: an AZP and its closest antecedent can sometimes be far away from each other, thus making it difficult to correctly resolve the AZP. To address this problem, we employ the following res- olution procedure in our experiments. Given a test document, we process its AZPs in a left-to-right <ref type="table" target="#tab_0">Training  Test  Documents  1,391  172  Sentences  36,487  6,083  Words  756,063 110,034  AZPs  12,111  1,713   Table 3</ref>: Statistics on the training and test sets.</p><p>manner. As soon as we resolve an AZP to a pre- ceding NP c, we fill the corresponding AZP's gap with c. Hence, when we process an AZP z, all of its preceding AZPs in the associated text have been resolved, with their gaps filled by the NPs they are resolved to. To resolve z, we create test instances between z and its four most salient can- didate antecedents in the same way as described before. The only difference is that the set of candi- date antecedents of z may now include those NPs that are used to fill the gaps of the AZPs resolved so far. Some of these additional candidate an- tecedents are closer to z than the original candidate antecedents, thereby facilitating the resolution of z. If the model resolves z to the additional can- didate antecedent that fills the gap left behind by, say, AZP z ′ , we postprocess the output by resolv- ing z to the NP that z ′ is resolved to. 7</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Datasets. We employ the Chinese portion of the OntoNotes 5.0 corpus that was used in the official CoNLL-2012 shared task ( <ref type="bibr" target="#b16">Pradhan et al., 2012</ref>).</p><p>In the CoNLL-2012 data, the training set and the development set contain ZP coreference annota- tions, but the test set does not. Therefore, we train our models on the training set and perform eval- uation on the development set. Statistics on the datasets are shown in   <ref type="table" target="#tab_2">Table 4</ref>.</p><p>Evaluation settings. Following <ref type="bibr" target="#b1">Chen and Ng (2013)</ref>, we evaluate our model in three settings.</p><p>In Setting 1, we assume the availability of gold syntactic parse trees and gold AZPs. In Setting 2, we employ gold syntactic parse trees and system (i.e., automatically identified) AZPs. Finally, in Setting 3, we employ system syntactic parse trees and system AZPs. The gold and system syntactic parse trees, as well as the gold AZPs, are obtained from the CoNLL-2012 shared task dataset, while the system AZPs are identified by a learning-based AZP identifier described in the Appendix.</p><p>Baseline system. As our baseline, we employ Chen and Ng's (2015) system, which has achieved the best result on our test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results and Discussion</head><p>Results of the baseline system and our model on entire test set are shown in row 1 of <ref type="table" target="#tab_4">Table 5</ref>. The three major columns in the table show the results obtained in the three settings. As we can see, our model outperforms the baseline significantly by 2.0%, 1.8%, and 1.1% in F-score under Settings 1, 2, and 3, respectively. 8 Rows 2−7 of <ref type="table" target="#tab_4">Table 5</ref> show the resolution re- sults on each of the six sources. As we can see, in Setting 1, our model beats the baseline on all six sources in F-score: by 2.4% (NW), 2.5% (MZ), 4.5% (WB), 1.6% (BN), 1.4% (BC), and 0.4% (TC). All the improvements are significant except for TC. These results suggest that our approach works well across different sources. In Setting 2, our model outperforms the baseline on all sources except NW and BC, where the F-scores drop in- significantly by 0.1% for both sources. Finally, in Setting 3, our model outperforms the baseline on all sources except NW and TC, where F-scores 8 All significance tests are paired t-tests, with p &lt; 0.05. drop significantly by 0.7% for NW and 1.1% for TC.</p><p>Given the challenges in applying supervised learning (in particular, the difficulty and time in- volved in training the deep neural network as well as the time and effort involved in manually anno- tating the data needed to train the network), one may wonder whether the small though statistically significant improvements in these results provide sufficient justification for going back to supervised learning from the previous state-of-the-art unsu- pervised model. We believe that this is the begin- ning, not the end, of applying deep neural networks for AZP resolution. In particular, there is a lot of room for improvements, which may involve incor- porating more sophisticated features and improv- ing the design of the network (e.g., the dimension- ality of the intermediate representations, the num- ber of hidden layers, the objective function), for instance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Results</head><p>Recall that the input of our model is composed of two groups of features, embedding features and hand-crafted features. To investigate the contribu- tion of each of these two feature groups, we con- duct ablation experiments. Specifically, in each ablation experiment, we retrain the network using only one group of features.</p><p>Ablation results under the three settings are shown in <ref type="table">Table 6</ref>. In Setting 1, when the hand- crafted features are ablated, F-score drops signifi- cantly by 12.2%. We attribute the drop to the fact that the syntactic, positional, and other relation- ships encoded in the hand-crafted features play an important role in resolving AZPs. When the em- bedding features are ablated, F-score drops signif- icantly by 3.7%. This result suggest the effective- ness of the embedding features.</p><p>Similar trends can be observed w.r.t. the other two settings: in Setting 2, F-score drops signifi- cantly by 6.8% and 2.2% when the hand-crafted features and the embedding features are ablated re- spectively, while in Setting 3, F-score drops signif- icantly by 4.6% and 1.1% when the hand-crafted features and the embedding features are ablated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Learning Curve</head><p>We show in <ref type="figure" target="#fig_2">Figure 2</ref> the learning curve of the our model obtained under Setting 1. As we can see, after the first epoch, the F-score on the entire test set is around 46%, and it gradually increases to Setting 1:   <ref type="table">Table 6</ref>: Ablation results of AZP resolution on the whole test set. 52% in the 80th epoch when performance starts to plateau. These results provide suggestive evi- dence for our earlier hypothesis that our objective function (Equation <ref type="formula" target="#formula_6">(8)</ref>) is tightly coupled with the desired evaluation metric (F-score).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Setting 2: Setting 3: Gold Parses, Gold AZPs Gold Parses, System AZPs System Parses, System AZPs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Analysis of Results</head><p>To gain additional insights into our approach, we examine the outputs of our model obtained under</p><formula xml:id="formula_9">Setting 1.</formula><p>We first analyze the cases where the AZP was correctly resolved by our model but incorrectly re- solved by the baseline. Consider the following representative example with the corresponding En- glish translation.</p><p>[陈水扁] 在登机前发表简短谈话时表示，[台 湾] 要站起来走出去。... * pro * 也希望此行能 把国际友谊带回来。 [Chen Shui-bian] delivered a short speech before boarding, saying that <ref type="bibr">[Taiwan]</ref> should stand up and go out. ... * pro * also hopes that this trip can bring back international friendship.</p><p>In this example, the correct antecedent of the AZP is 陈水扁 (Chen Shui-bian). However, the baseline incorrectly resolves it to 台湾 (Taiwan). The baseline's mistake can be attributed to the facts that (1) 台 湾 is the most salient candidate an- tecedent in the discourse, and (2) 台湾 is closer to the AZP than the correct antecedent 陈水扁. Nev- ertheless, our model still correctly identifies 陈水 扁 as the AZP's antecedent because of the embed- ding features. A closer inspection of the training data reveals that although the word 陈水扁 never appeared as the antecedent of an AZP whose gov- erning verb is 希 望 (hope) in the training data, many AZPs that are governed by 希望 are corefer- ent with other person names. Because the word 陈 水扁 has a similar word embedding as those per- son names, our approach successfully generalizes such lexical context and makes the right resolution decision.</p><p>Next, we examine the errors made by our model and find that the majority of the mistakes result from insufficient lexical contexts. Currently, to encode the lexical contexts, we only consider the word preceding the AZP and its governing verb, as well as the head word of the candidate antecedent. However, this encoding ignored a lot of potentially useful context information, such as the clause fol- lowing the AZP, the modifier of the candidate an- tecedent and the clause containing the candidate antecedent. Consider the following example:</p><p>[我] 前一会精神上太紧张。... * pro * 现在比较 平静了。 <ref type="bibr">[I]</ref> was too nervous a while ago. ... * pro * am now calmer.</p><p>To resolve the AZP to its correct antecedent 我 (I), one needs to compare the two clauses contain- ing the AZP and 我. However, since our model does not encode a candidate antecedent's context, it does not resolve the AZP correctly. One way to address this problem would be to employ sentence embeddings to represent the clauses containing the AZP and its candidate antecedents, and then per- form sentence embedding matching to resolve the AZP. The primary challenge concerns how to train the model to match two clauses with probably no overlapping words and with a limited number of training examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We proposed an embedding matching approach to zero pronoun resolution based on deep networks. To our knowledge, this is the first neural network- based approach to zero pronoun resolution. When evaluated on the Chinese portion of the OntoNotes corpus, our approach achieved state-of-the-art re- sults. <ref type="bibr">Yaqin Yang and Nianwen Xue. 2010</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix: Anaphoric Zero Pronoun Identification</head><p>Recall that Settings 2 and 3 in our evaluation in- volve the use of system AZPs. Our supervised AZP identification procedure is composed of two steps. First, in the extraction step, we heuristically extract ZPs. Then, in the classification step, we train a classifier to determine which of the ZPs ex- tracted in the first step are AZPs.</p><p>To implement the extraction step, we use Zhao and Ng's (2007) observation: ZPs can only occur before a VP node in a syntactic parse tree. How- ever, according to <ref type="bibr" target="#b14">Kong and Zhou (2010)</ref>, ZPs do not need to be extracted from every VP: if a VP node occurs in a coordinate structure or is modi- fied by an adverbial node, then only its parent VP node needs to be considered. We extract ZPs from all VPs that satisfy the above constraints.</p><p>To implement the classification step, we train a binary classifier using SVM light (Joachims, 1999) on the CoNLL-2012 training set to distinguish AZPs from non-AZPs. Each instance corresponds to a ZP extracted in the first step and is represented Syntactic features <ref type="bibr">(13)</ref> whether z is the first gap in an IP clause; whether z is the first gap in a subject-less IP clause, and if so, POS(w1); whether POS(w1) is NT; whether t1 is a verb that appears in a NP or VP; whether P l is a NP, QP, IP or ICP node; whether Pr is a VP node; the phrasal label of the parent of the node containing POS(t1); whether V has a NP, VP, QP or CP ancestor; whether C is a VP node; whether the parent of V is an IP node; whether V's lowest IP ancestor has (1) a VP node as its parent and (2) a VV node as its left sibling; whether there is a VP node whose parent is an IP node in the path from t1 to C. Lexical features <ref type="formula">(13)</ref> the words surrounding z and/or their POS tags, including w1, w−1, POS(w1), POS(w−1) + POS(w1), POS(w1) + POS(w2), POS(w−2) + POS(w−1), POS(w1) + POS(w2) + POS(w3), POS(w−1) + w1, and w−1 + POS(w1); whether w1 is a transitive verb, an intransi- tive verb or a preposition; whether w−1 is a transitive verb without an object. Other features <ref type="formula" target="#formula_4">(6)</ref> whether z is the first gap in a sentence; whether z is in the headline of the text; the type of the clause in which z appears; the grammatical role of z; whether w−1 is a punctuation; whether w−1 is a comma.  by 32 features, 13 of which were proposed by <ref type="bibr">Zhao and Ng (2007)</ref> and 19 of which were proposed by <ref type="bibr">Yang and Xue (2010)</ref>. A brief description of these features can be found in <ref type="table" target="#tab_6">Table 7</ref>.</p><p>When gold parse trees are employed, the recall, precision and F-score of the AZP identifier on our test set are 75.1%, 50.1% and 60.1% respectively. Using automatic parse trees, the performance of the AZP identifier drops to 43.7% (R), 30.7% (P) and 36.1% (F).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The architecture of our embedding matching model. The number in each box indicates the size of the corresponding vector.</figDesc><graphic url="image-1.png" coords="3,72.00,62.62,453.69,226.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>, Sasano et al. (2008), Taira et al. (2008), Imamura et al. (2009), Sasano et al. (2009), Watanabe et al. (2010), Hayashibe et al. (2011), Iida and Poesio (2011), Sasano and Kuro- hashi (2011), Yoshikawa et al. (2011), Hangyo et al. (2013), Yoshino et al. (2013), Iida et al. (2015)), and Italian (e.g., Iida and Poesio (2011)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The learning curve of our model on the entire test set under Setting 1.</figDesc><graphic url="image-2.png" coords="8,72.00,312.12,216.52,114.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>the right of z (if i is positive) or the ith word to the left of z (if i is negative). C is lowest common ancestor of w−1 and w1. P l and Pr are the child nodes of C that are the ancestors of w−1 and w1 respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Hand-crafted features associated with a candidate antecedent. z is a zero pronoun. c is a candidate antecedent of z. V is the VP node following z in the parse tree.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table>The documents 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Hyperparameter values. 

Hyperparameter tuning. We reserve 20% of 
the training set for tuning hyperparameters. The 
tuned hyperparameter values are shown in </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>AZP resolution results of the baseline and our model on the test set. 

Setting 1: 
Setting 2: 
Setting 3: 
Gold Parses 
Gold Parses 
System Parses 
Gold AZPs 
System AZPs 
System AZPs 
System 
R 
P 
F 
R 
P 
F 
R 
P 
F 
Full system 
51.8 52.5 52.2 39.6 27.0 32.1 21.9 15.8 18.4 
Embedding features only 
39.2 40.8 40.0 30.9 21.5 25.3 16.3 12.0 13.8 
Hand-crafted features only 
48.2 48.7 48.5 37.0 25.1 29.9 20.6 14.9 17.3 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, pages 1382--1390.</head><label></label><figDesc>. Chasing the ghost: recovering empty categories in the Chinese Treebank.</figDesc><table>Ching-Long Yeh and Yi-Chun Chen. 2007. Zero 
anaphora resolution in Chinese with shallow pars-
ing. Journal of Chinese Language and Computing, 
17(1):41--56. 

Katsumasa Yoshikawa, Masayuki Asahara, and Yuji 
Matsumoto. 2011. Jointly extracting Japanese 
predicate-argument relation with Markov Logic. In 
Proceedings of 5th International Joint Conference 
on Natural Language Processing, pages 1125--1133. 

Koichiro Yoshino, Shinsuke Mori, and Tatsuya Kawa-
hara. 2013. Predicate argument structure analysis 
using partially annotated corpora. In Proceedings of 
the Sixth International Joint Conference on Natural 
Language Processing, pages 957--961. 

Shanheng Zhao and Hwee Tou Ng. 2007. Identifica-
tion and resolution of Chinese zero pronouns: A ma-
chine learning approach. In Proceedings of the 2007 
Joint Conference on Empirical Methods on Natu-
ral Language Processing and Computational Natu-
ral Language Learning, pages 541--550. 

GuoDong Zhou, Fang Kong, and Qiaoming Zhu. 2008. 
Context-sensitive convolution tree kernel for pro-
noun resolution. In Proceedings of the 3rd Interna-
tional Joint Conference on Natural Language Pro-
cessing, pages 25--31. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 7 :</head><label>7</label><figDesc>Features for AZP identification. z is a zero pronoun. V is the VP node following z. wi is the ith word to</figDesc><table></table></figure>

			<note place="foot" n="2"> Note that the target AZP and its candidate antecedents use different weight matrices and biases within each layer. This is needed because the features of the AZP and those of the candidate antecedents come from two different feature spaces.</note>

			<note place="foot" n="3"> We compute the list of preceding entities automatically using SinoCoreferencer (Chen and Ng, 2014c), a Chinese entity coreference resolver downloadable from http://www. hlt.utdallas.edu/~yzcchen/coreference/.</note>

			<note place="foot" n="5"> As seen in Figure 1, we set the length of the vector to 50. 6 Appending dummy 0s is solely for the convenience of the network implementation: doing so does not have any effect on any computation.</note>

			<note place="foot" n="7"> This postprocessing step is needed because the additional candidate antecedents are only gap fillers.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the three anonymous reviewers for their detailed comments. This work was supported in part by NSF Grants IIS-1219142 and IIS-1528037. Any opinions, findings, conclusions or recommen-dations expressed in this paper are those of the au-thors and do not necessarily reflect the views or of-ficial policies, either expressed or implied, of NSF.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chen Chen and Vincent Ng. 2014b. Chinese zero pronoun resolution: An unsupervised probabilistic model rivaling supervised resolvers. In Proceed- ings of the 2014 Conference on Empirical Methods</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A constrained latent variable model for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajhans</forename><surname>Samdani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="601" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Chinese zero pronoun resolution: Some recent advances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1360" to="1365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Chinese zero pronoun resolution: An unsupervised approach combining ranking and integer linear programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 28th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1622" to="1628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Japanese predicate argument structure analysis exploiting argument position and type</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Hayashibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mamoru</forename><surname>Komachi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 5th International Joint Conference on Natural Language Processing</title>
		<meeting>5th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="201" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Resolving pronoun references</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry</forename><surname>Hobbs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lingua</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="311" to="338" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A cross-lingual ILP solution to zero anaphora resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryu</forename><surname>Iida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="804" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploting syntactic patterns as clues in zero-anaphora resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryu</forename><surname>Iida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and 44th</title>
		<meeting>the 21st International Conference on Computational Linguistics and 44th</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="page" from="625" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Zero-anaphora resolution by learning rich syntactic pattern features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryu</forename><surname>Iida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Asian Language Information Processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Intrasentential zero anaphora resolution using subject sharing recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryu</forename><surname>Iida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Torisawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chikara</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghoon</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Kloetzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2179" to="2189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Discriminative approach to predicateargument structure analysis with zero-anaphora resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Imamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniko</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoko</forename><surname>Izumi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-IJCNLP</title>
		<meeting>the ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Conference Short Papers</title>
		<imprint>
			<biblScope unit="page" from="85" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Japanese zero pronoun resolution based on ranking rules and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Isozaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsutomu</forename><surname>Hirao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2003 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="184" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Making large-scale SVM learning practical</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><forename type="middle">Joachims</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Kernel Methods-Support Vector Learning</title>
		<editor>Bernhard Scholkopf and Alexander Smola</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="44" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A tree kernelbased unified framework for Chinese zero anaphora resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="882" to="891" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Latent structures for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Martschat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="405" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">CoNLL2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning: Shared Task</title>
		<meeting>2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning: Shared Task</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dialogue focus tracking for zero pronoun resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudha</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allyson</forename><surname>Ettinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="494" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semantic hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGIR Workshop on Information Retrieval and Applications of Graphical Models</title>
		<meeting>the SIGIR Workshop on Information Retrieval and Applications of Graphical Models</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A discriminative approach to Japanese zero anaphora resolution with large-scale lexicalized case frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryohei</forename><surname>Sasano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Joint Conference on Natural Language Processing</title>
		<meeting>the 5th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="758" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A fully-lexicalized probabilistic model for Japanese zero anaphora resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryohei</forename><surname>Sasano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Computational Linguistics</title>
		<meeting>the 22nd International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="769" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The effect of corpus size on case frame acquisition for discourse analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryohei</forename><surname>Sasano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="521" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A probabilistic method for analyzing Japanese anaphora integrating zero pronoun detection and resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuhiro</forename><surname>Seki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsushi</forename><surname>Fujii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tetsuya</forename><surname>Ishikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Computational Linguistics</title>
		<meeting>the 19th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A Japanese predicate argument structure analysis using decision lists</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirotoshi</forename><surname>Taira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanae</forename><surname>Fujita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2008 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="523" to="532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A structured model for joint learning of argument roles and predicate senses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yotaro</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayuki</forename><surname>Asahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2010 Conference Short Papers</title>
		<meeting>the ACL 2010 Conference Short Papers</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="98" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning anaphoricity and antecedent ranking features for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1416" to="1426" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
