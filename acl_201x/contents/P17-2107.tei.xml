<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">How (not) to train a dependency parser: The curious case of jackknifing part-of-speech taggers</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Željko</forename><surname>Agić</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalie</forename><surname>Schluter</surname></persName>
						</author>
						<title level="a" type="main">How (not) to train a dependency parser: The curious case of jackknifing part-of-speech taggers</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="679" to="684"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-2107</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In dependency parsing, jackknifing tag-gers is indiscriminately used as a simple adaptation strategy. Here, we empirically evaluate when and how (not) to use jack-knifing in parsing. On 26 languages, we reveal a preference that conflicts with, and surpasses the ubiquitous ten-folding. We show no clear benefits of tagging the training data in cross-lingual parsing.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Dependency parsers are trained over manually an- notated treebank data. By contrast, when applied in the real world, they parse over sequences of pre- dicted parts of speech. As POS tagging accuracy drops due to domain change, the parsing quality declines proportionally. Bringing these two POS tag sources closer together thus makes for a rea- sonable adaptation strategy.</p><p>Arguably the simplest of such adaptations is n- fold jackknifing. In it, a treebank is divided into n equal parts, and the n-th part is POS-tagged with a tagger trained on the remainder. The procedure is repeated until all n parts are assigned with pre- dicted POS tags. A parser is then trained over the thus altered treebank, under the assumption that its POS features will now more closely resemble those of the input data. Jackknifing is simplistic as it i) has a very lim- ited adaptation range for n ∈ N + , and it ii) does not in any way take the input data into account, other than through a vague assumption of an undefined amount of tagging noise in the input. As such, it exhibits very mixed results. Still, the method is now ubiquitous in the parsing literature.</p><p>In <ref type="figure">Figure 1</ref>, we survey the ACL Anthology 1 for POS jackknifing. We uncover that ∼80% of the 70 <ref type="bibr">1</ref> http://aclweb.org/anthology/ <ref type="figure">Figure 1</ref>: Jackknifing in the ACL Anthology. Dis- tribution of n over 70 parsing papers that use tag- ger n-folding.</p><p>parsing papers we retrieved make use of ten-fold jackknifing. This choice spans across the various languages and domains parsed in these papers, and is even motivated by simply "following the tradi- tions in literature". <ref type="bibr">2</ref> Our contributions. We evaluate jackknifing to establish whether its use is warranted in depen- dency parsing. Controlling for tagging quality in training and testing, we experiment with monolin- gual and delexicalized cross-lingual parsers over 26 languages, showing that: i) Indiscriminate use of ten-fold jackknifing re- sults in sub-optimal parsing. ii) Tagging the training data does not yield clear benefits in realistic cross-lingual parsing. iii) Our jackknifing extension improves parsing through finer-grained adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Jackknifing generally refers to a leave-one-out procedure for reducing bias in parameter estima- tion from an unbiased sample <ref type="bibr" target="#b7">(Quenouille, 1956;</ref><ref type="bibr" target="#b11">Tukey, 1958)</ref>. More recently, in machine learn- ing the term is used synonymously with "cross-validation" for estimation of predictive model per- formance measures. In NLP, jackknifing has com- monly been used to describe a procedure by which the training input is adjusted to correspond more closely to the expected test input, and it is in this latter sense that we use the term here.</p><p>In particular, in parsing research, the n-fold jackknifing proceeds as follows. The treebank is first partitioned into n non-overlapping subsets of equal size. Then, iteratively, each part acts as a test subset and is tagged using a model induced by the remaining n − 1 parts, the training subset, until the entire treebank is tagged.</p><p>We want to control for POS tagging accuracy through the jackknifing method. To do this, we train a tagger on increasing sized subsets of the training set. In fold terminology, this corresponds to dividing the training set into equal parts of size 1 n , training on n−1 n ths of the training set and testing on the remaining 1 n th. However, this constrains the size of the training subset to be larger than half the original data, and thus concentrates our study on models that use almost all the data, since the non- linear curve f (n) = n−1 n becomes very flat very fast. Thus, varying fold numbers reveals very little variation in terms of POS tagging accuracy in the lower accuracy range.</p><p>Linear extension. We now propose a simple ex- tension of the jackknifing paradigm to study parser accuracy given a percentage p of the training set: linear jackknifing.</p><p>Let p ∈ (0, 1) be the percentage of the randomly shuffled training set D used to induce a model to tag some remaining number of instances. A train- ing subset of this size allows a test subset of size at most D ⋅ (1 − p)). Given a test subset to tag, we can induce a model from a random subset of the remaining examples of size approximately p ⋅ D to become our training subset. We randomize the choice of examples in the training subset to avoid introducing bias. In order to tag all of D, the min- imum number of models we need to generate is  Intrinsic evaluation. For increasing values of p, at 5% increments, we carried out linear jackknif- ing on 26 languages. For each p, we averaged the performance of the induced taggers on the respec- tive gold standards. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the dif- ference in informativeness of the two approaches, where each tagging accuracy score is averaged across the 26 languages. We see that with n-fold jackknifing, tagging accuracy is constrained to be- tween approximately 92% and 95%, whereas lin- ear jackknifing explores accuracies as low as ap- proximately 86%. Moreover, the confidence in- tervals are consistent across the p, demonstrating unbiased tagging models generated on less data (lower p). We now show that these smaller lev- els of p are essential for good parser performance in some cases of jackknifing.</p><formula xml:id="formula_0">1(1 − p)).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Our experiment aims at judging the adequacy of jackknifing in dependency parsing. First, we out- line the experiment setup, where we conduct two sets of experiments: i) monolingual, where lexicalized parsers are trained on treebanks for their respective lan- guages, and ii) cross-lingual, that features SINGLE-best and MULTI-source delexicalized parsers.</p><p>Tagging sources. By jackknifing we explore how the mismatch between training and test POS affects parsing. Our setup thus critically relies on the sources of tags. We tag our test sets using: i) PRED, the monolingual taggers, and ii) PROJ, the low-resource taggers by Agi´c <ref type="bibr">Agi´c et al. (2016)</ref>, based on annotation projection.  We do not experiment with gold POS tags in the test sets. Instead, we only focus on realistic pars- ing over predicted tags. The tags in our training sets can be GOLD, PROJ, or they can be predicted through n-fold or linear jackknifing.</p><p>In n-fold jackknifing, we experiment with n ∈ {2, 3, .., 20}, while for the linear extension we set p ∈ {5, 10, ..., 95}. We report the average pars- ing scores over 5 runs for each n and p so as to mitigate the effects of random shuffling in the two jackknifing procedures. In finding the optimal val- ues of the parameters n max and p max , we report the highest values in case of ties. For example, if n = 5 and n = 10 both yield the same maximum UAS, we set n max = 10.</p><p>We emphasize the importance of realistic set- tings especially in cross-lingual parsing. Thus, we commit to using PROJ taggers with an outlook on true low-resource languages.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Results</head><p>In monolingual parsing over PRED tags <ref type="table" target="#tab_1">(Table 1)</ref>, we achieve an identical average UAS with lin- ear and n-fold jackknifing. Our adaptations sur- pass training with GOLD data by +1.1 UAS. Lin- ear jackknifing improves over GOLD training by +8.1 UAS when parsing over low-resource PROJ tags. There, we top GOLD training by n-fold jack- knifing as well, but it trails the linear variant by -3.9 UAS. In the low-resource PROJ setup, PROJ- trained parsers are dominant. They score +6.8 UAS over linear, +10.7 UAS over n-folding, and +14.8 UAS over GOLD training. <ref type="figure" target="#fig_2">Figure 3</ref> plots the relation between the sample size p in linear jackknifing and the resulting UAS in parsing, split for PRED and PROJ test-set tag- gings. Parsing over PRED tags, the UAS generally increases with p, but we note that this increase is rather small: over 26 languages, moving p from 5% to 95% yields only +0.7 UAS on average. By contrast, adapting to the lower-quality PROJ tags sees a larger +5 UAS benefit from decreasing p all the way to 5%, which is well outside the n-fold range, as indicated for n ∈ {2, ..., 20} by the dot- ted lines in the figure.</p><p>Our cross-lingual parsing experiment <ref type="table" target="#tab_3">(Table 2</ref>) contrasts two options: we either tag (PROJ) or do not tag (GOLD) the parser training data. To re- flect realistic low-resource parsing, the test data is tagged with PROJ taggers only. On average, the unadapted parsers are slightly better (UAS: +1.1 MULTI, +0.1 SINGLE). However, they are almost evenly split with the adapted ones in terms of of- fering the best performance for 12-14 out of the 26 test languages each. These results suggest, at least for simplicity, a preference for not tagging the treebanks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>Linear or n-fold? In resource-rich PRED pars- ing, the two jackknifing methods are evenly split, with identical average UAS score and an overlap on 13 languages. In low-resource PROJ parsing, n- folding falls far behind as the constraint for n ≥ 2 prevents it from adapting accordingly. The median p max in PRED and PROJ are 75% and 5%. The first one roughly corresponds to 4-fold jackknif- ing, while the second one is far below the two-fold range. The median n max are 11 and 2, and we note that n max is rarely ∼10 in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>If we simply use ten-fold jackknifing for PRED tags, we manage to match the p max scores for only 9 of 26 languages, and we score -0.2 UAS on av- erage. If using n = 10 with PROJ tags, the discon- nect is much more substantial, and we are unable to reach p max (-4.6 UAS).</p><p>The GOLD training data is never the best choice in our monolingual parsing experiment, regardless of whether the test tags are PRED or PROJ. This result in itself justifies the usage of jackknifing as adaptation for the monolingual setting, provided that it is not indiscriminate.</p><p>FindingˆpFindingˆ Findingˆp max . For choosing the optimal linear jackknifing in real-world parsing, we note that p max closely correlates with test set tagging ac- curacy (Spearman's ρ = 0.76), and negatively with treebank size (ρ = −0.42, for D ≤ 10k sentences). Thus, to adapt via linear jackknifing, we must i) approximate the expected input data tagging accu- racy, while at the same time ii) accounting for the fact that the accuracy associated with any p de- pends on treebank size as well. In other words, given two treebanks D 1 &lt; D 2 , we would typi- cally need p 1 &gt; p 2 with the goal of matching the same test-set accuracy.</p><p>The other parser. Replacing the MATE parser with the transition-based YARA does not change the outcome of our monolingual parsing experi- ment, save for the average 0.58-1.65 drop in UAS. On the other hand, in cross-lingual parsing, YARA highlights the benefits of not tagging the training data, as the GOLDPROJ parsers are there the best choice for parsing 17/26 languages. On average, we see +2.1 UAS for MULTI, and +0.7 for SIN- GLE over PROJPROJ. This is especially worth noting since large-scale parsing generally favors transition-based systems.</p><p>GOLD test tags. Thus far, we have shown the need for more careful jackknifing in parser train- ing with respect to the expected tagging quality at parse time. Fixing n = 10 was suboptimal in parsing over the fully supervised PRED tags, while n = 2, 10 were way below the threshold in low- resource parsing over our cross-lingual PROJ tags. We have purposely excluded GOLD test tags from the discussion so far.</p><p>Still, while parsing over GOLD POS input does not hold much significance for real-world applica- tions, it is worth noting how jackknifing performs in the upper limit: trying to reach the accuracy of parsers trained and tested on GOLD tags. In that particular setup, we observe the maximum UAS of 83.8 for median n max = 12 and p max = 80%. The respective modal values are n = 20 and p = 95%, meaning that for most languages, we come closest to GOLDGOLD by maximizing the tagging ac- curacy. The overall score amounts to -0.7 UAS below the upper bound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related work</head><p>Jackknifing itself is for the most part incidental to the work that employs it. Here, we mention a few notable exceptions. <ref type="bibr" target="#b2">Che et al. (2012)</ref> compare jackknifing to using gold tags in parsing Chinese for constituents and dependencies, where they observe mixed results: improvement with one parser, and decrease with the other. <ref type="bibr" target="#b10">Seeker and Kuhn (2013)</ref> briefly touch upon the importance of jackknifing in bridging the gap between training and test data, and experi- ment with 5-and 10-folds. <ref type="bibr" target="#b4">Honnibal and Johnson (2015)</ref> passingly contrast jackknifing to joint learning, giving precedence to the latter for sim- plicity. Finally, <ref type="bibr" target="#b5">Kong et al. (2015)</ref> follow <ref type="bibr" target="#b13">Zhu et al. (2013)</ref> in ten-folding for Chinese and En- glish, citing 2.0% and 0.4% improvements. Inci- dentally, jackknifing parsers then hurts their per- formance in tree conversions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>The parsing literature is riddled with indiscrimi- nate use of n-fold part-of-speech tagger jackknif- ing as makeshift domain adaptation.</p><p>In this paper we have proposed a careful empir- ical treatment of jackknifing in dependency pars- ing, far surpassing ten-folding via fine-grained control over the data adjustment.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>We thus separate D into test sub- sets f 1 , . . . , f 1(1−p)) each of size approximately D ⋅ (1 − p)). For each f i , we randomly sam- ple a training subset of size approximately p ⋅ D from the remainder of D, induce a model and then tag f i . This results in the original full training set tagged with an accuracy corresponding to the per- formance of a randomly selected tagger trained on approximately p ⋅ D of the examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Tagger learning curve for 26 languages: mean tagging accuracy with 95% confidence intervals. Accuracy ranges for n-fold and linear jackknifing are indicated.</figDesc><graphic url="image-2.png" coords="2,318.19,62.81,196.44,131.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Parsing accuracy (UAS) in relation to linear jackknifing over 26 languages, with two sources of test set POS tags.</figDesc><graphic url="image-3.png" coords="4,307.28,62.81,218.27,97.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>fold jackknifing PROJ Test: PRED PROJ PRED p max PROJ p max PRED n max PROJ n max PRED PROJ</head><label></label><figDesc></figDesc><table>Monolingual parsing 

Train: 

GOLD 

linear jackknifing 
n-Arabic (ar) 79.4 51.4 
79.5 
90 
64.0 
5 
79.5 
12 
55.3 
2 
73.5 74.4 
Bulgarian (bg) 86.8 56.4 
87.2 
80 
66.7 
5 
87.2 
12 
60.2 
2 
82.2 78.0 
Czech (cs) 81.0 58.2 
81.6 
85 
62.5 
5 
81.6 
8 
60.1 
2 
77.3 70.7 
Danish (da) 74.4 65.7 
78.2 
95 
71.8 
5 
78.3 
11 
68.5 
2 
75.1 76.0 
German (de) 79.0 51.8 
80.2 
20 
56.6 
5 
80.1 
2 
54.4 
7 
75.6 69.2 
Greek (el) 81.1 59.4 
81.4 
70 
64.7 
5 
81.4 
20 
61.2 
2 
74.7 75.6 

English (en) 80.9 71.6 
82.3 
80 
77.6 
5 
82.3 
17 
76.0 
2 
80.2 80.9 
Spanish (es) 80.7 75.4 
82.1 
20 
78.3 
5 
81.8 
9 
77.3 
4 
80.2 80.2 
Estonian (et) 74.3 61.6 
76.0 
55 
67.1 
5 
76.1 
13 
65.0 
2 
73.3 70.5 
Persian (fa) 82.3 25.8 
83.2 
35 
46.3 
5 
83.1 
4 
35.1 
2 
66.3 71.9 
Finnish (fi) 72.0 53.9 
72.9 
75 
60.4 
5 
73.0 
18 
56.6 
2 
69.2 64.7 
French (fr) 80.9 65.4 
81.7 
90 
72.7 
5 
81.8 
19 
68.7 
2 
79.2 77.1 
Hebrew (he) 80.6 54.2 
81.5 
75 
68.1 
5 
81.5 
7 
61.7 
2 
77.9 75.7 

Hindi (hi) 89.1 51.0 
91.0 
70 
70.1 
5 
91.0 
11 
63.1 
2 
84.8 85.5 
Croatian (hr) 78.1 54.6 
78.2 
45 
62.9 
5 
78.4 
19 
56.3 
20 
73.6 72.5 
Hungarian (hu) 74.3 55.4 
74.9 
95 
63.2 
5 
75.2 
17 
58.3 
2 
71.0 66.8 
Indonesian (id) 79.4 70.6 
79.6 
90 
74.9 
5 
79.6 
9 
72.3 
2 
77.8 77.7 
Italian (it) 86.2 76.2 
87.6 
55 
82.9 
5 
87.6 
2 
80.6 
2 
86.2 85.5 
Dutch (nl) 72.1 60.2 
73.8 
50 
67.9 
5 
73.8 
12 
65.6 
8 
67.6 72.6 
Norwegian (no) 83.7 74.8 
85.3 
95 
79.9 
5 
85.3 
8 
78.0 
2 
83.7 82.6 

Polish (pl) 83.7 69.8 
84.7 
30 
75.8 
5 
84.9 
20 
73.0 
2 
81.4 78.6 
Portuguese (pt) 82.2 75.7 
83.2 
30 
78.8 
5 
83.1 
2 
77.7 
19 
80.2 82.0 
Romanian (ro) 81.8 66.8 
82.5 
85 
72.9 
5 
82.5 
5 
69.0 
2 
79.7 79.1 
Slovene (sl) 82.0 62.5 
84.2 
55 
71.4 
5 
84.2 
16 
68.9 
7 
79.3 77.4 
Swedish (sv) 80.9 74.4 
81.9 
90 
77.3 
10 
82.0 
19 
76.8 
7 
80.0 79.1 
Tamil (ta) 65.1 33.4 
65.8 
95 
49.5 
5 
65.6 
5 
42.5 
2 
51.5 56.3 

Mean 79.7 60.6 
80.8 67.5 68.6 
5.2 
80.8 
11.4 64.7 
4.2 
76.2 75.4 
Best for #/26 
0 
0 
18 
-
0 
-
21 
-
0 
-
0 
26 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Parsing accuracy (UAS) in relation to the underlying sources of POS tags in training and at 
runtime. Bold: best result for language, separately for PRED and PROJ test sets. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>. We use the Universal Dependencies (UD) treebanks version 1.2 (Nivre et al., 2016). 3 As the projection-based taggers are trained on the WTC dataset by Agi´cAgi´c et al. (2016), we intersect the list of WTC languages with the UD list for a total of 26 languages. Tagging and parsing. For POS tagging, we use the TNT tagger by Brants (2000). The PRED taggers score 94.1±1.1%, while the low-resource PROJ taggers are on average 71.7±5.7% accurate. We experiment with two parsers. Bohnet's (2010)</figDesc><table>Delexicalized transfer 

Train: 

GOLDPROJ 
PROJPROJ 

Test: MULTI 

SINGLE 
MULTI 
SINGLE 

Arabic (ar) 
34.2 
he 38.3 
28.3 
id 37.0 
Bulgarian (bg) 
49.5 
cs 50.1 
50.2 
cs 51.1 
Czech (cs) 
50.4 
sl 50.7 
48.4 
sl 50.8 
Danish (da) 
58.0 
no 58.5 
58.1 
no 61.4 
German (de) 
43.9 
no 45.0 
45.8 
sv 45.4 
Greek (el) 
56.3 
it 55.4 
57.3 
no 55.3 

English (en) 
55.8 
no 56.7 
57.2 
sv 58.2 
Spanish (es) 
67.9 
it 68.5 
64.9 
it 67.6 
Estonian (et) 
45.8 
fi 53.1 
43.9 
fi 50.8 
Persian (fa) 
21.5 
ar 25.7 
18.9 
pl 24.2 
Finnish (fi) 
38.8 
et 45.1 
40.0 
et 45.5 
French (fr) 
52.8 
it 54.4 
54.9 
it 58.9 
Hebrew (he) 
44.6 
ro 45.1 
41.7 
ro 44.0 

Hindi (hi) 
16.9 
ta 38.1 
18.0 
ta 31.0 
Croatian (hr) 
50.9 
sl 50.8 
46.8 
cs 49.3 
Hungarian (hu) 
39.9 
sv 46.4 
40.1 
et 49.3 
Indonesian (id) 
54.5 
ro 56.6 
48.7 
ro 54.4 
Italian (it) 
67.0 
es 67.8 
67.4 
es 69.2 
Dutch (nl) 
55.1 
es 53.9 
52.2 
sv 52.5 
Norwegian (no) 
63.5 
sv 64.3 
62.4 
sv 64.4 

Polish (pl) 
62.9 
cs 64.2 
57.3 
cs 59.2 
Portuguese (pt) 
65.7 
es 67.7 
64.7 
it 66.9 
Romanian (ro) 
53.6 
it 53.7 
50.5 
es 53.9 
Slovene (sl) 
50.5 
cs 53.4 
52.6 
cs 56.3 
Swedish (sv) 
62.7 
no 66.8 
61.9 
no 67.1 
Tamil (ta) 
21.2 
hu 28.9 
24.6 
hi 33.8 

Mean 
49.4 
-52.3 
48.3 
-52.2 
Best for #/26 
14 
-
12 
12 
-
14 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>UAS scores for the delexicalized transfer 
parsers. TRAINTEST indicates the training and 
testing POS. Bold: best result for language, sepa-
rate for MULTI and SINGLE transfer. For SINGLE, 
best source names are also reported. 

second-order graph-based system MATE 4 is the 
primary. Further, we verify all parsing results by 
using a transition-based parser YARA 5 with dy-
namic oracles (Rasooli and Tetreault, 2015). 
The following CoNLL 2009 features are used 
for training the parsers: 6 ID, FORM (in monolin-
gual parsing only), POS, and HEAD. Since ours 
is not a benchmarking effort, we apply all systems 
with their default settings. 

</table></figure>

			<note place="foot" n="2"> Incidentally, using ten-folds with the WSJ data yields roughly the same train-and test-set tagging accuracy, and seems to be where the choice originated.</note>

			<note place="foot" n="3"> http://universaldependencies.org/</note>

			<note place="foot" n="4"> https://code.google.com/archive/p/ mate-tools/ 5 https://github.com/yahoo/YaraParser 6 https://ufal.mff.cuni.cz/ conll2009-st/</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Top accuracy and fast dependency parsing is not a contradiction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/C10-1011" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="89" to="97" />
		</imprint>
	</monogr>
	<note>Coling 2010 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tnt-a statistical part-ofspeech tagger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Brants</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/A00-1031" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Applied Natural Language Processing Conference</title>
		<meeting>the Sixth Applied Natural Language Processing Conference</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A comparison of chinese parsers for stanford dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th</title>
		<meeting>the 50th</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Association for Computational Linguistics</title>
		<ptr target="http://aclweb.org/anthology/P12-2003" />
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="11" to="16" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An improved non-monotonic transition system for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Honnibal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/D15-1162</idno>
		<ptr target="https://doi.org/10.18653/v1/D15-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1373" to="1378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Transforming dependencies into phrase structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Noah</forename><surname>Smith</surname></persName>
		</author>
		<idno type="doi">10.3115/v1/N15-1080</idno>
		<ptr target="https://doi.org/10.3115/v1/N15-1080" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="788" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Universal dependencies v1: A multilingual treebank collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Silveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reut</forename><surname>Tsarfaty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016). European Language Resources Association (ELRA)</title>
		<meeting>the Tenth International Conference on Language Resources and Evaluation (LREC 2016). European Language Resources Association (ELRA)<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1659" to="1666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Notes on Bias in Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maurice</forename><forename type="middle">H</forename><surname>Quenouille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="1956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<idno type="doi">10.2307/2332914</idno>
		<ptr target="https://doi.org/10.2307/2332914" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Sadegh Rasooli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.06733</idno>
		<title level="m">Yara Parser: A Fast and Accurate Dependency Parser</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The effects of syntactic features in automatic prediction of morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Seeker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Kuhn</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D13-1033" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="333" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bias and Confidence in Not Quite Large Samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">W</forename><surname>Tukey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">614</biblScope>
			<date type="published" when="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<idno type="doi">10.1214/aoms/1177706647</idno>
		<ptr target="https://doi.org/10.1214/aoms/1177706647" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast and accurate shiftreduce constituent parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P13-1043" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="434" to="443" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
