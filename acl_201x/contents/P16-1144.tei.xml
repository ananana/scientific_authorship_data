<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The LAMBADA dataset: Word prediction requiring a broad discourse context *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Paperno</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">CIMeC -Center for Mind/Brain Sciences</orgName>
								<orgName type="department" key="dep2">Institute for Logic, Language &amp; Computation</orgName>
								<orgName type="institution" key="instit1">University of Trento</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">CIMeC -Center for Mind/Brain Sciences</orgName>
								<orgName type="department" key="dep2">Institute for Logic, Language &amp; Computation</orgName>
								<orgName type="institution" key="instit1">University of Trento</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">CIMeC -Center for Mind/Brain Sciences</orgName>
								<orgName type="department" key="dep2">Institute for Logic, Language &amp; Computation</orgName>
								<orgName type="institution" key="instit1">University of Trento</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><surname>Quan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">CIMeC -Center for Mind/Brain Sciences</orgName>
								<orgName type="department" key="dep2">Institute for Logic, Language &amp; Computation</orgName>
								<orgName type="institution" key="instit1">University of Trento</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pham</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">CIMeC -Center for Mind/Brain Sciences</orgName>
								<orgName type="department" key="dep2">Institute for Logic, Language &amp; Computation</orgName>
								<orgName type="institution" key="instit1">University of Trento</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">CIMeC -Center for Mind/Brain Sciences</orgName>
								<orgName type="department" key="dep2">Institute for Logic, Language &amp; Computation</orgName>
								<orgName type="institution" key="instit1">University of Trento</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandro</forename><surname>Pezzelle</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">CIMeC -Center for Mind/Brain Sciences</orgName>
								<orgName type="department" key="dep2">Institute for Logic, Language &amp; Computation</orgName>
								<orgName type="institution" key="instit1">University of Trento</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">CIMeC -Center for Mind/Brain Sciences</orgName>
								<orgName type="department" key="dep2">Institute for Logic, Language &amp; Computation</orgName>
								<orgName type="institution" key="instit1">University of Trento</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Boleda</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">CIMeC -Center for Mind/Brain Sciences</orgName>
								<orgName type="department" key="dep2">Institute for Logic, Language &amp; Computation</orgName>
								<orgName type="institution" key="instit1">University of Trento</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Fernández</surname></persName>
							<email>raquel.fernandez@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">CIMeC -Center for Mind/Brain Sciences</orgName>
								<orgName type="department" key="dep2">Institute for Logic, Language &amp; Computation</orgName>
								<orgName type="institution" key="instit1">University of Trento</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">The LAMBADA dataset: Word prediction requiring a broad discourse context *</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1525" to="1534"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We introduce LAMBADA, a dataset to evaluate the capabilities of computational models for text understanding by means of a word prediction task. LAMBADA is a collection of narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage, but not if they only see the last sentence preceding the target word. To succeed on LAM-BADA, computational models cannot simply rely on local context, but must be able to keep track of information in the broader discourse. We show that LAMBADA exemplifies a wide range of linguistic phenomena , and that none of several state-of-the-art language models reaches accuracy above 1% on this novel benchmark. We thus propose LAMBADA as a challenging test set, meant to encourage the development of new models capable of genuine understanding of broad context in natural language text.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The recent spurt of powerful end-to-end-trained neural networks for Natural Language Processing ( <ref type="bibr" target="#b3">Hermann et al., 2015;</ref><ref type="bibr" target="#b12">Rocktäschel et al., 2016;</ref>, a.o.) has sparked interest in tasks to measure the progress they are bringing about in genuine language understanding. Spe- cial care must be taken in evaluating such systems, since their effectiveness at picking statistical gen- eralizations from large corpora can lead to the il- lusion that they are reaching a deeper degree of understanding than they really are. For example, the end-to-end system of <ref type="bibr" target="#b16">Vinyals and Le (2015)</ref>, trained on large conversational datasets, produces dialogues such as the following:</p><formula xml:id="formula_0">(1)</formula><p>Human: what is your job? Machine: i'm a lawyer Human: what do you do? Machine: i'm a doctor Separately, the system responses are appropriate for the respective questions. However, when taken together, they are incoherent. The system be- haviour is somewhat parrot-like. It can locally produce perfectly sensible language fragments, but it fails to take the meaning of the broader dis- course context into account. Much research ef- fort has consequently focused on designing sys- tems able to keep information from the broader context into memory, and possibly even perform simple forms of reasoning about it ( <ref type="bibr" target="#b3">Hermann et al., 2015;</ref><ref type="bibr" target="#b5">Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr" target="#b13">Sordoni et al., 2015;</ref><ref type="bibr" target="#b15">Sukhbaatar et al., 2015;</ref><ref type="bibr">Wang and Cho, 2015, a.o.)</ref>. In this paper, we introduce the LAMBADA dataset (LAnguage Modeling Broadened to Account for Discourse Aspects). LAMBADA pro- poses a word prediction task where the target item is difficult to guess (for English speakers) when only the sentence in which it appears is available, but becomes easy when a broader context is pre- sented. Consider Example (1) in <ref type="figure">Figure 1</ref>. The sentence Do you honestly think that I would want you to have a ? has a multitude of possible con- tinuations, but the broad context clearly indicates that the missing word is miscarriage.</p><p>LAMBADA casts language understanding in the classic word prediction framework of language modeling. We can thus use it to test several ex- isting language modeling architectures, including systems with capacity to hold longer-term contex- tual memories. In our preliminary experiments, none of these models came even remotely close to human performance, confirming that LAMBADA is a challenging benchmark for research on auto- mated models of natural language understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related datasets</head><p>The CNN/Daily Mail (CNNDM) benchmark re- cently introduced by <ref type="bibr" target="#b3">Hermann et al. (2015)</ref> is closely related to LAMBADA. CNNDM includes a large set of online articles that are published to- gether with short summaries of their main points. The task is to guess a named entity that has been removed from one such summary. Although the data are not normed by subjects, it is unlikely that the missing named entity can be guessed from the short summary alone, and thus, like in LAM- BADA, models need to look at the broader con- text (the article). Differences between the two datasets include text genres (news vs. novels; see Section 3.1) and the fact that missing items in CN- NDM are limited to named entities. Most im- portantly, the two datasets require models to per- form different kinds of inferences over broader passages. For CNNDM, models must be able to summarize the articles, in order to make sense of the sentence containing the missing word, whereas in LAMBADA the last sentence is not a summary of the broader passage, but a continuation of the same story. Thus, in order to succeed, models must instead understand what is a plausible devel- opment of a narrative fragment or a dialogue.</p><p>Another related benchmark, CBT, has been in- troduced by <ref type="bibr" target="#b4">Hill et al. (2016)</ref>. Like LAMBADA, CBT is a collection of book excerpts, with one word randomly removed from the last sentence in a sequence of 21 sentences. While there are other design differences, the crucial distinction be- tween CBT and LAMBADA is that the CBT pas- sages were not filtered to be human-guessable in the broader context only. Indeed, according to the post-hoc analysis of a sample of CBT passages re- ported by Hill and colleagues, in a large proportion of cases in which annotators could guess the miss- ing word from the broader context, they could also guess it from the last sentence alone. At the same time, in about one fifth of the cases, the annotators could not guess the word even when the broader context was given. Thus, only a small portion of the CBT passages are really probing the model's ability to understand the broader context, which is instead the focus of LAMBADA.</p><p>The idea of a book excerpt completion task was originally introduced in the MSRCC dataset ( <ref type="bibr" target="#b21">Zweig and Burges, 2011)</ref>. However, the latter limited context to single sentences, not attempting to measure broader passage understanding.</p><p>Of course, text understanding can be tested through other tasks, including entailment detec- tion ( <ref type="bibr" target="#b1">Bowman et al., 2015)</ref>, answering questions about a text ( <ref type="bibr" target="#b11">Richardson et al., 2013;</ref> and measuring inter-clause coher- ence ( <ref type="bibr" target="#b19">Yin and Schütze, 2015)</ref>. While different tasks can provide complementary insights into the models' abilities, we find word prediction par- ticularly attractive because of its naturalness (it's easy to norm the data with non-expert humans) and simplicity. Models just need to be trained to predict the most likely word given the previ- ous context, following the classic language mod- eling paradigm, which is a much simpler setup than the one required, say, to determine whether two sentences entail each other. Moreover, mod- els can have access to virtually unlimited amounts of training data, as all that is required to train a language model is raw text. On a more general methodological level, word prediction has the po- tential to probe almost any aspect of text under- standing, including but not limited to traditional narrower tasks such as entailment, co-reference resolution or word sense disambiguation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The LAMBADA dataset</head><p>3.1 Data collection 1 LAMBADA consists of passages composed of a context (on average 4.6 sentences) and a target sentence. The context size is the minimum num- ber of complete sentences before the target sen- tence such that they cumulatively contain at least 50 tokens (this size was chosen in a pilot study). The task is to guess the last word of the target sen- tence (the target word). The constraint that the target word be the last word of the sentence, while not necessary for our research goal, makes the task more natural for human subjects.</p><p>The LAMBADA data come from the Book Cor- pus ( <ref type="bibr" target="#b20">Zhu et al., 2015</ref>). The fact that it con- tains unpublished novels minimizes the potential Context: "Yes, I thought I was going to lose the baby." "I was scared too," he stated, sincerity flooding his eyes. "You were ?" "Yes, of course. Why do you even ask?" "This baby wasn't exactly planned for." Target sentence: "Do you honestly think that I would want you to have a ?" Target word: miscarriage <ref type="formula">(2)</ref> Context: "Why?" "I would have thought you'd find him rather dry," she said. "I don't know about that," said Gabriel. "He was a great craftsman," said Heather. "That he was," said Flannery. Target sentence: "And Polish, to boot," said . Context: In my palm is a clear stone, and inside it is a small ivory statuette. A guardian angel. "Figured if you're going to be out at night getting hit by cars, you might as well have some backup." I look at him, feeling stunned. Like this is some sort of sign. Target sentence: But as I stare at Harlin, his mouth curved in a confident grin, I don't care about . Target word: signs</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(7)</head><p>Context: Both its sun-speckled shade and the cool grass beneath were a welcome respite after the stifling kitchen, and I was glad to relax against the tree's rough, brittle bark and begin my breakfast of buttery, toasted bread and fresh fruit. Even the water was tasty, it was so clean and cold. Target sentence: It almost made up for the lack of . Target word: coffee <ref type="formula">(8)</ref> Context: My wife refused to allow me to come to Hong Kong when the plague was at its height and -" "Your wife, Johanne? You are married at last ?" Johanne grinned. "Well, when a man gets to my age, he starts to need a few home comforts. Target sentence: After my dear mother passed away ten years ago now, I became . Target word: lonely <ref type="formula">(9)</ref> Context: "Again, he left that up to you. However, he was adamant in his desire that it remain a private ceremony. He asked me to make sure, for instance, that no information be given to the newspaper regarding his death, not even an obituary. Target sentence: I got the sense that he didn't want anyone, aside from the three of us, to know that he'd even . Target word: died <ref type="formula">(10)</ref> Context: The battery on Logan's radio must have been on the way out. So he told himself. There was no other explanation beyond Cygan and the staff at the White House having been overrun. Lizzie opened her eyes with a flutter. They had been on the icy road for an hour without incident. Target sentence: Jack was happy to do all of the . Target word: driving <ref type="figure">Figure 1</ref>: Examples of LAMBADA passages. Underlined words highlight when the target word (or its lemma) occurs in the context. usefulness of general world knowledge and ex- ternal resources for the task, in contrast to other kinds of texts like news data, Wikipedia text, or famous novels. The corpus, after duplicate re- moval and filtering out of potentially offensive ma- terial with a stop word list, contains 5,325 nov- els and 465 million words. We randomly divided the novels into equally-sized training and devel- opment+testing partitions. We built the LAM- BADA dataset from the latter, with the idea that models tackling LAMBADA should be trained on raw text from the training partition, composed of 2662 novels and encompassing more than 200M words. Because novels are pre-assigned to one of the two partitions only, LAMBADA passages are self-contained and cannot be solved by exploiting the knowledge in the remainder of the novels, for example background information about the char- acters involved or the properties of the fictional world in a given novel. The same novel-based di- vision method is used to further split LAMBADA data between development and testing.</p><p>To reduce time and cost of dataset collection, we filtered out passages that are relatively easy for standard language models, since such cases are likely to be guessable based on local context alone. We used a combination of four language models, chosen by availability and/or ease of train- ing: a pre-trained recurrent neural network (RNN) <ref type="bibr" target="#b7">(Mikolov et al., 2011</ref>) and three models trained on the Book Corpus (a standard 4-gram model, a RNN and a feed-forward model; see SM for de- tails, and note that these are different from the models we evaluated on LAMBADA as described in Section 4 below). Any passage whose target word had probability ≥0.00175 according to any of the language models was excluded.</p><p>A random sample of the remaining passages were then evaluated by human subjects through the CrowdFlower crowdsourcing service 2 in three steps. For a given passage, 1. one human subject guessed the target word based on the whole passage (comprising the context and the target sentence); if the guess was right, 2. a second subject guessed the target word based on the whole passage; if that guess was also right, 3. more subjects tried to guess the target word based on the target sentence only, until the word was guessed or the number of unsuc- cessful guesses reached 10; if no subject was able to guess the target word, the passage was added to the LAMBADA dataset.</p><p>The subjects in step 3 were allowed 3 guesses per sentence, to maximize the chances of catch- ing cases where the target words were guessable from the sentence alone.</p><p>Step 2 was added based on a pilot study that revealed that, while step 3 was enough to ensure that the data could not be guessed with the local context only, step 1 alone did not ensure that the data were easy given the discourse context (its output includes a mix of cases ranging from obvious to relatively difficult, guessed by an especially able or lucky step-1 sub- ject). We made sure that it was not possible for the same subject to judge the same item in both passage and sentence conditions (details in SM).</p><p>In the crowdsourcing pipeline, 84-86% items were discarded at step 1, an additional 6-7% at step 2 and another 3-5% at step 3. Only about one in 25 input examples passed all the selection steps.</p><p>Subjects were paid $0.22 per page in steps 1 and 2 (with 10 passages per page) and $0.15 per page in step 3 (with 20 sentences per page). Over- all, each item in the resulting dataset costed $1.24 on average. Alternative designs, such as having step 3 before step 2 or before step 1, were found to be more expensive. Cost considerations also precluded us from using more subjects at stage 1, which could in principle improve the quality of fil- tering at this step.</p><p>Note that the criteria for passage inclusion were very strict: We required two consecutive subjects to exactly match the missing word, and we made sure that no subject (out of ten) was able to provide it based on local context only, even when given 3 guesses. An alternative to this perfect-match ap- proach would have been to include passages where broad-context subjects provided other plausible or synonymous continuations. However, it is very challenging, both practically and methodologi- cally, to determine which answers other than the original fit the passage well, especially when the goal is to distinguish between items that are solv- able in broad-discourse context and those where the local context is enough. Theoretically, substi- tutability in context could be tested with manual annotation by multiple additional raters, but this would not be financially or practically feasible for a dataset of this scale (human annotators received over 200,000 passages at stage 1). For this reason we went for the strict hit-or-miss approach, keep- ing only items that can be unambiguously deter- mined by human subjects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dataset statistics</head><p>The LAMBADA dataset consists of 10,022 pas- sages, divided into 4,869 development and 5,153 test passages (extracted from 1,331 and 1,332 dis- joint novels, respectively). The average passage consists of 4.6 sentences in the context plus 1 tar- get sentence, for a total length of 75.4 tokens (dev) / 75 tokens (test). Examples of passages in the dataset are given in <ref type="figure">Figure 1</ref>.</p><p>The training data for language models to be tested on LAMBADA include the full text of 2,662 novels (disjoint from those in dev+test), compris- ing 203 million words. Note that the training data consists of text from the same domain as the dev+test passages, in large amounts but not fil- tered in the same way. This is partially motivated by economic considerations (recall that each data point costs $1.24 on average), but, more impor- tantly, it is justified by the intended use of LAM- BADA as a tool to evaluate general-purpose mod- els in terms of how they fare on broad-context un- derstanding (just like our subjects could predict the missing words using their more general text understanding abilities), not as a resource to de- velop ad-hoc models only meant to predict the fi- nal word in the sort of passages encountered in LAMBADA. The development data can be used to fine-tune models to the specifics of the LAM- BADA passages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Dataset analysis</head><p>Our analysis of the LAMBADA data suggests that, in order for the target word to be predictable in a broad context only, it must be strongly cued in the broader discourse. Indeed, it is typical for LAM- BADA items that the target word (or its lemma) occurs in the context. <ref type="figure" target="#fig_2">Figure 2</ref>(a) compares the LAMBADA items to a random 5000-item sam- ple from the input data, that is, the passages that were presented to human subjects in the filtering phase (we sampled from all passages passing the automated filters described in Section 3.1 above, including those that made it to LAMBADA). The figure shows that when subjects guessed the word (only) in the broad context, often the word it- self occurred in the context: More than 80% of LAMBADA passages include the target word in the context, while in the input data that was the case for less than 15% of the passages. To guess the right word, however, subjects must still put their linguistic and general cognitive skills to good use, as shown by the examples featuring the target word in the context reported in <ref type="figure">Figure 1</ref>. <ref type="figure" target="#fig_2">Figure 2(b)</ref> shows that most target words in LAMBADA are proper nouns (48%), followed by common nouns (37%) and, at a distance, verbs (7.7%). In fact, proper nouns are hugely over- represented in LAMBADA, while the other cat- egories are under-represented, compared to the POS distribution in the input. A variety of factors converges in making proper nouns easy for sub- jects in the LAMBADA task. In particular, when the context clearly demands a referential expres- sion, the constraint that the blank be filled by a single word excludes other possibilities such as noun phrases with articles, and there are reasons to suspect that co-reference is easier than other dis- course phenomena in our task (see below). How- ever, although co-reference seems to play a big role, only 0.3% of target words are pronouns.</p><p>Common nouns are still pretty frequent in LAMBADA, constituting over one third of the data. Qualitative analysis reveals a mixture of phenomena. Co-reference is again quite common (see Example (3) in <ref type="figure">Figure 1</ref>), sometimes as "par- tial" co-reference facilitated by bridging mecha- nisms (shutter-camera; Example (5)) or through the presence of a near synonym ('lose the baby'- miscarriage; Example (1)). However, we also of- ten find other phenomena, such as the inference of prototypical participants in an event. For instance, if the passage describes someone having breakfast together with typical food and beverages (see Ex- ample <ref type="formula">(7)</ref>), subjects can guess the target word cof- fee without it having been explicitly mentioned.</p><p>In contrast, verbs, adjectives, and adverbs are rare in LAMBADA. Many of those items can be guessed with local sentence context only, as shown in <ref type="figure" target="#fig_2">Figure 2(b)</ref>, which also reports the POS dis- tribution of the set of items that were guessed by subjects based on the target-sentence context only (step 3 in Section 3.1). Note a higher proportion of verbs, adjectives and adverbs in the latter set in <ref type="figure" target="#fig_2">Figure 2(</ref>   <ref type="formula">(8)</ref>, where the tar- get word is an adjective). This contrasts with other types of open-class adverbs (e.g., innocently, con- fidently), which are generally hard to guess with both local and broad context. The low propor- tion of these kinds of adverbs and of verbs among guessed items in general suggests that tracking event-related phenomena (such as script-like se- quences of events) is harder for subjects than co- referential phenomena, at least as framed in the LAMBADA task. Further research is needed to probe this hypothesis. Furthermore, we observe that, while explicit mention in the preceding discourse context is criti- cal for proper nouns, the other categories can often be guessed without having been explicitly intro- duced. This is shown in <ref type="figure" target="#fig_2">Figure 2(c)</ref>, which de- picts the POS distribution of LAMBADA items for which the lemma of the target word is not in the context (corresponding to about 16% of LAMBADA in total). <ref type="bibr">3</ref> Qualitative analysis of items with verbs and adjectives as targets sug- gests that the target word, although not present in the passage, is still strongly implied by the con- <ref type="bibr">3</ref> The apparent 1% of out-of-context proper nouns shown in <ref type="figure" target="#fig_2">Figure 2</ref>(c) is due to lemmatization mistakes (fictional characters for which the lemmatizer did not recognize a link between singular and plural forms, e.g., Wynn -Wynns). A manual check confirmed that all proper noun target words in LAMBADA are indeed also present in the context.</p><p>text. In about one third of the cases examined, the missing word is "almost there". For instance, the passage contains a word with the same root but a different part of speech (e.g., death-died in Example <ref type="formula">(6)</ref>), or a synonymous expression (as mentioned above for "miscarriage"; we find the same phenomenon for verbs, e.g., 'deprived you of water'-dehydrated).</p><p>In other cases, correct prediction requires more complex discourse inference, including guessing prototypical participants of a scene (as in the cof- fee example above), actions or events strongly sug- gested by the discourse (see Examples (1) and (10), where the mention of an icy road helps in predicting the target driving), or qualitative properties of participants or situations (see <ref type="bibr">Example (8)</ref>). Of course, the same kind of discourse reasoning takes place when the target word is al- ready present in the context (cf. Examples <ref type="formula">(3)</ref> and <ref type="formula">(4)</ref>). The presence of the word in context does not make the reasoning unnecessary (the task remains challenging), but facilitates the inference.</p><p>As a final observation, intriguingly, the LAM- BADA items contain (quoted) direct speech sig- nificantly more often than the input items overall (71% of LAMBADA items vs. 61% of items in the input sample), see, e.g., Examples (1) and <ref type="bibr">(2)</ref>. Further analysis is needed to investigate in what way more dialogic discourse might facilitate the prediction of the final target word.</p><p>In sum, LAMBADA contains a myriad of phe- nomena that, besides making it challenging from the text understanding perspective, are of great interest to the broad Computational Linguistics community. To return to Example (1), solving it requires a combination of linguistic skills rang- ing from (morpho)phonology (the plausible target word abortion is ruled out by the indefinite deter- miner a) through morphosyntax (the slot should be filled by a common singular noun) to pragmatics (understanding what the male participant is infer- ring from the female participant's words), in addi- tion to general reasoning skills. It is not surprising, thus, that LAMBADA is so challenging for current models, as we show next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Modeling experiments</head><p>Computational methods We tested several ex- isting language models and baselines on LAM- BADA. We implemented a simple RNN <ref type="bibr" target="#b2">(Elman, 1990</ref>), a Long Short-Term Memory network (LSTM) <ref type="bibr" target="#b5">(Hochreiter and Schmidhuber, 1997</ref>), a traditional statistical N-Gram language model <ref type="bibr" target="#b14">(Stolcke, 2002</ref>) with and without cache, and a Memory Network ( <ref type="bibr" target="#b15">Sukhbaatar et al., 2015</ref>). We remark that at least LSTM, Memory Network and, to a certain extent, the cache N-Gram model have, among their supposed benefits, the ability to take broader contexts into account. Note moreover that variants of RNNs and LSTMs are at the state of the art when tested on standard language model- ing benchmarks <ref type="bibr" target="#b10">(Mikolov, 2014)</ref>. Our Memory Network implementation is similar to the one with which <ref type="bibr" target="#b4">Hill et al. (2016)</ref> reached the best results on the CBT data set (see Section 2 above). While we could not re-implement the models that per- formed best on CNNDM (see again Section 2), our LSTM is architecturally similar to the Deep LSTM Reader of <ref type="bibr" target="#b3">Hermann et al. (2015)</ref>, which achieved respectable performance on that data set. Most importantly, we will show below that most of our models reach impressive performance when tested on a more standard language modeling data set sourced from the same corpus used to build LAMBADA. This control set was constructed by randomly sampling 5K passages of the same shape and size as the ones used to build LAMBADA from the same test novels, but without filtering them in any way. Based on the control set re- sults, to be discussed below, we can reasonably claim that the models we are testing on LAM- BADA are very good at standard language model- ing, and their low performance on the latter cannot be attributed to poor quality.</p><p>In order to test for strong biases in the data, we constructed Sup-CBOW, a baseline model weakly tailored to the task at hand, consisting of a simple neural network that takes as input a bag-of- word representation of the passage and attempts to predict the final word. The input representa- tion comes from adding pre-trained CBOW vec- tors ( <ref type="bibr" target="#b8">Mikolov et al., 2013</ref>) of the words in the pas- sage. <ref type="bibr">4</ref> We also considered an unsupervised vari- ant (Unsup-CBOW) where the target word is pre- dicted by cosine similarity between the passage vector and the target word vector. Finally, we evaluated several variations of a random guess- ing baseline differing in terms of the word pool to sample from. The guessed word could be picked from: the full vocabulary, the words that appear in the current passage and a random uppercased word from the passage. The latter baseline aims at exploiting the potential bias that proper names ac- count for a consistent portion of the LAMBADA data (see <ref type="figure" target="#fig_2">Figure 2</ref> above). Note that LAMBADA was designed to chal- lenge language models with harder-than-average examples where broad context understanding is crucial. However, the average case should not be disregarded either, since we want language mod- els to be able to handle both cases. For this rea- son, we trained the models entirely on unsuper- vised data and expect future work to follow sim- ilar principles. Concretely, we trained the mod- els, as is standard practice, on predicting each up- coming word given the previous context, using the LAMBADA training data (see Section 3.2 above) as input corpus. The only exception to this proce- dure was Sup-CBOW where we extracted from the training novels similar-shaped passages to those in LAMBADA and trained the model on them (about 9M passages). Again, the goal of this model was only to test for potential biases in the data and not to provide a full account for the phenomena we are testing. We restricted the vocabulary of the mod- els to the 60K most frequent words in the training set (covering 95% of the target words in the de- velopment set). The model hyperparameters were tuned on their accuracy in the development set.</p><p>The same trained models were tested on the LAM- BADA and the control sets. See SM for the tuning details.</p><p>Results Results of models and baselines are re- ported in for LAMBADA is the average success of a model at predicting the target word, i.e., accuracy (unlike in standard language modeling, we know that the missing LAMBADA words can be precisely pre- dicted by humans, so good models should be able to accomplish the same feat, rather than just as- signing a high probability to them). However, as we observe a bottoming effect with accuracy, we also report perplexity and median rank of correct word, to better compare the models.</p><p>As anticipated above, and in line with what we expected, all our models have very good perfor- mance when called to perform a standard language modeling task on the control set. Indeed, 3 of the models (the N-Gram models and LSTM) can guess the right word in about 1/5 of the cases.</p><p>The situation drastically changes if we look at the LAMBADA results, where all models are per- forming very badly. Indeed, no model is even able to compete with the simple heuristics of pick- ing a random word from the passage, and, espe- cially, a random capitalized word (easily a proper noun). At the same time, the low performance of the latter heuristic in absolute terms (7% accuracy) shows that, despite the bias in favour of names in the passage, simply relying on this will not suffice to obtain good performance on LAMBADA, and models should rather pursue deeper forms of anal- ysis of the broader context (the Sup-CBOW base- line, attempting to directly exploit the passage in a shallow way, performs very poorly). This con- firms again that the difficulty of LAMBADA relies mainly on accounting for the information available in a broader context and not on the task of predict- ing the exact word missing.</p><p>In comparative terms (and focusing on perplex- ity and rank, given the uniformly low accuracy results) we observe a stronger performance of the traditional N-Gram models over the neural- network-based ones, possibly pointing to the dif- ficulty of tuning the latter properly. In particu- lar, the best relative performance on LAMBADA is achieved by N-Gram w/cache, which takes pas- sage statistics into account. While even this model is effectively unable to guess the right word, it achieves a respectable perplexity of 768.</p><p>We recognize, of course, that the evaluation we performed is very preliminary, and it must only be taken as a proof-of-concept study of the difficulty of LAMBADA. Better results might be obtained simply by performing more extensive tuning, by adding more sophisticated mechanisms such as at- tention ( <ref type="bibr" target="#b0">Bahdanau et al., 2014)</ref>, and so forth. Still, we would be surprised if minor modifications of the models we tested led to human-level perfor- mance on the task.</p><p>We also note that, because of the way we have constructed LAMBADA, standard language mod- els are bound to fail on it by design: one of our first filters (see Section 3.1) was to choose pas- sages where a number of simple language models were failing to predict the upcoming word. How- ever, future research should find ways around this inherent difficulty. After all, humans were still able to solve this task, so a model that claims to have good language understanding ability should be able to succeed on it as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper introduced the new LAMBADA dataset, aimed at testing language models on their ability to take a broad discourse context into ac- count when predicting a word. A number of linguistic phenomena make the target words in LAMBADA easy to guess by human subjects when they can look at the whole passages they come from, but nearly impossible if only the last sentence is considered. Our preliminary experi- ments suggest that even some cutting-edge neural network approaches that are in principle able to track long-distance effects are far from passing the LAMBADA challenge.</p><p>We hope the computational community will be stimulated to develop novel language models that are genuinely capturing the non-local phenomena that LAMBADA reflects. To promote research in this direction, we plan to announce a public com- petition based on the LAMBADA data. <ref type="bibr">5</ref> Our own hunch is that, despite the initially dis- appointing results of the "vanilla" Memory Net- work we tested, the ability to store information in a longer-term memory will be a crucial compo- nent of successful models, coupled with the ability to perform some kind of reasoning about what's stored in memory, in order to retrieve the right in- formation from it.</p><p>On a more general note, we believe that lever- aging human performance on word prediction is a very promising strategy to construct benchmarks for computational models that are supposed to capture various aspects of human text understand- ing. The influence of broad context as explored by LAMBADA is only one example of this idea.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Context: Preston had been the last person to wear those chains, and I knew what I'd see and feel if they were slipped onto my skin-the Reaper's unending hatred of me. I'd felt enough of that emotion already in the amphitheater. I didn't want to feel anymore. "Don't put those on me," I whispered. "Please." Target sentence: Sergei looked at me, surprised by my low, raspy please, but he put down the . Target word: chains (4) Context: They tuned, discussed for a moment, then struck up a lively jig. Everyone joined in, turning the courtyard into an even more chaotic scene, people now dancing in circles, swinging and spinning in circles, everyone making up their own dance steps. I felt my feet tapping, my body wanting to move. Target sentence: Aside from writing, I 've always loved . Target word: dancing (5) Context: He shook his head, took a step back and held his hands up as he tried to smile without losing a cigarette. "Yes you can," Julia said in a reassuring voice. "I 've already focused on my friend. You just have to click the shutter, on top, here." Target sentence: He nodded sheepishly, through his cigarette away and took the . Target word: camera (6)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a) Target word in or not in context; (b) Target word POS distribution in LAMBADA vs. data presented to human subjects (input) and items guessed with sentence context only (PN=proper noun, CN=common noun, V=verb, J=adjective, R=adverb, O=other); (c) Target word POS distribution of LAMBADA passages where the lemma of the target word is not in the context (categories as in (b)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>b). While end-of-sentence context skews input distribution in favour of nouns, subject filter- ing does show a clear differential effect for nouns vs. other POSs. Manual inspection reveals that broad context is not necessary to guess items like</figDesc><table>LAMBADA 

input 

Target word 
in context 
not in context 

0.0 

0.2 

0.4 

0.6 

0.8 

1.0 

PN CN V 
J 
R 
O 

LAMBADA 
input 
sentence 

0.0 

0.1 

0.2 

0.3 

0.4 

0.5 

PN CN V 
J 
R 
O 

0.0 0.1 0.2 0.3 0.4 0.5 

(a) 
(b) 
(c) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 . Note that the measure of interest</head><label>1</label><figDesc></figDesc><table>Data 

Method 
Acc. 
Ppl. 
Rank 

LAMBADA 

baselines 
Random vocabulary word 
0 
60000 30026 
Random word from passage 
1.6 
-
-
Random capitalized word from passage 7.3 
-
-
Unsup-CBOW 
0 
57040 16352 
Sup-CBOW 
0 
47587 4660 
models 
N-Gram 
0.1 
3125 
993 
N-Gram w/cache 
0.1 
768 
87 
RNN 
0 
14725 7831 
LSTM 
0 
5357 
324 
Memory Network 
0 
16318 
846 

Control 

baselines 
Random vocabulary word 
0 
60000 30453 
Random word from passage 
0 
-
-
Random capitalized word from passage 
0 
-
-
Unsup-CBOW 
0 
55190 12950 
Sup-CBOW 
3.5 
2344 
259 
models 
N-Gram 
19.1 
285 
17 
N-Gram w/cache 
19.1 
270 
18 
RNN 
15.4 
277 
24 
LSTM 
21.9 
149 
12 
Memory Network 
8.5 
566 
46 

Table 1: Results of computational methods. Accuracy is expressed in percentage. 

</table></figure>

			<note place="foot" n="1"> Further technical details are provided in the Supplementary Material (SM): http://clic.cimec.unitn.it/ lambada/</note>

			<note place="foot" n="2"> http://www.crowdflower.com</note>

			<note place="foot" n="4"> http://clic.cimec.unitn.it/composes/ semantic-vectors.html</note>

			<note place="foot" n="5"> The development set of LAMBADA, along with the training corpus, can be downloaded at http://clic. cimec.unitn.it/lambada/. The test set will be made available at the time of the competition.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We are grateful to Aurelie Herbelot, Tal Linzen, Nghia The Pham and, especially, Roberto Zam-parelli for ideas and feedback. This project has received funding from the European Union's Hori-zon 2020 research and innovation programme un-der the Marie Sklodowska-Curie grant agreement No 655577 (LOVe); ERC 2011 Starting Indepen-dent Research Grant n. 283554 (COMPOSES); NWO VIDI grant n. 276-89-008 (Asymmetry in Conversation). We gratefully acknowledge the support of NVIDIA Corporation with the donation of the GPUs used in our research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Lisbon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="Portu" to=" gal" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jeffrey L Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočisk´kočisk´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<ptr target="https://papers.nips.cc/book/advances-in-neural-information-processing-systems-28-2015" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Published online</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The Goldilocks principle: Reading children&apos;s books with explicit memory representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<ptr target="http://www.iclr.cc/doku.php?id=iclr2016:main" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR Conference Track</title>
		<meeting>ICLR Conference Track<address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Published online</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="178" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1511.03962" />
		<title level="m">Document context language models</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rnnlm-recurrent neural network language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kombrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Deoras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukar</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Honza</forename><surname>Cernocky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ASRU. IEEE Automatic Speech Recognition and Understanding Workshop</title>
		<meeting>ASRU. IEEE Automatic Speech Recognition and Understanding Workshop</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1301.3781/" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning longer memory in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<ptr target="http://www.iclr.cc/doku.php?id=iclr2015:main" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR Workshop Track</title>
		<meeting>ICLR Workshop Track<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Published online</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Using neural networks for modelling and representing natural languages. Slides presented at COLING</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<ptr target="http://www.coling-2014.org/COLING\2014\Tutorial-fix\-\TomasMikolov.pdf" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">MCTest: A challenge dataset for the open-domain machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><surname>Renshaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="193" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reasoning about entailment with neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočisk´kočisk´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<ptr target="http://www.iclr.cc/doku.php?id=iclr2016:main" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR Conference Track</title>
		<meeting>ICLR Conference Track<address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Published online</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A neural network approach to context-sensitive generation of conversational responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL<address><addrLine>Denver, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="196" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Srilm-an extensible language modeling toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">2002</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1503" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A neural conversational model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<ptr target="https://sites.google.com/site/deeplearning2015/accepted-papers" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICML Deep Learning Workshop</title>
		<meeting>the ICML Deep Learning Workshop<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Published online</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Largercontext language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1511.03729" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Towards AI-complete question answering: A set of prerequisite toy tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1502.05698" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">MultiGranCNN: An architecture for general matching of text chunks on multiple levels of granularity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="63" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV 2015</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Burges</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The Microsoft Research sentence completion challenge</title>
		<idno>MSR-TR-2011-129</idno>
		<imprint>
			<pubPlace>Microsoft Research</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
