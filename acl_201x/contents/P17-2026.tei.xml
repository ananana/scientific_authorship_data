<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:29+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Information-Theory Interpretation of the Skip-Gram Negative-Sampling Objective Function</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Melamud</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
						</author>
						<title level="a" type="main">Information-Theory Interpretation of the Skip-Gram Negative-Sampling Objective Function</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="167" to="171"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-2026</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we define a measure of dependency between two random variables, based on the Jensen-Shannon (JS) divergence between their joint distribution and the product of their marginal distributions. Then, we show that word2vec&apos;s skip-gram with negative sampling embedding algorithm finds the optimal low-dimensional approximation of this JS dependency measure between the words and their contexts. The gap between the optimal score and the low-dimensional approximation is demonstrated on a standard text corpus.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Continuous word representations, derived from un- labeled text, have proven useful in many NLP tasks. Such word representations (or embeddings) asso- ciate a low-dimensional, real-valued vector with each word, typically induced via neural language models or matrix factorization.</p><p>Substantial benefit arises when embeddings can be efficiently trained on large volumes of data. Hence the recent considerable interest in the con- tinuous bag-of-words (CBOW) and skip-gram with negative sampling (SGNS) models, described in ( <ref type="bibr" target="#b9">Mikolov et al., 2013)</ref>, as implemented in the open- source toolkit word2vec. These models are based on a relatively simple log-linear method and avoid hidden layers typical to neural networks. Conse- quently, they can be trained to produce high-quality word embeddings on large corpora like the entirety of English Wikipedia in several hours, compared to days or even weeks in the case of other continuous models. Recent studies obtained state-of-the-art results by using skip-gram embeddings on a va- riety of natural language processing tasks, such as named entity extraction ( <ref type="bibr" target="#b10">Passos et al., 2014)</ref> and dependency parsing ( <ref type="bibr" target="#b1">Bansal et al., 2014</ref>). In recent years, there were several attempts to mathe- matically interpret word embedding models ( <ref type="bibr" target="#b0">Arora et al., 2016;</ref><ref type="bibr" target="#b11">Pennington et al., 2014;</ref><ref type="bibr" target="#b12">Stratos et al., 2015)</ref>. Our study pursues this established line of work, attempting to explain the objective function of the SGNS word embedding algorithm.</p><p>In the SGNS model, the energy function takes the form of a dot product between the vectors of an observed word and an observed context. The objec- tive function is a binary logistic regression classifier that treats a word and its observed context as a pos- itive example, and a word and a randomly sampled context as a negative example. <ref type="bibr" target="#b6">Levy and Goldberg (2014)</ref> offered a motivation for this function by showing that it obtains its global maximum value at the word-context pointwise mutual information (PMI) matrix. In this study, we take their analy- sis one step further and provide an information- theoretical interpretation of the SGNS objective function. In Section 2, we define a new measure of mutual information between random variables based the Jensen-Shennon divergence <ref type="bibr" target="#b8">(Lin, 1991)</ref> instead of the KL divergence. In Section 3, we show that the value of the SGNS objective com- puted at the PMI matrix is this information measure. We then derive an explicit expression for the infor- mation loss caused by the low-dimensional embed- ding learned by the SGNS algorithm. Finally, in Section 4, we illustrate this by computing the infor- mation loss caused by actual SGNS embeddings learned on a standard text corpus.</p><p>There are several standard methods of measuring the distance between two discrete probability dis- tributions, defined on a given finite set A. The Kullback-Leibler (KL) divergence of a distribu- tion p from a distribution q is defined as follows:</p><formula xml:id="formula_0">KL(p||q) = i∈A p i log p i q i</formula><p>. The mutual informa- tion between two jointly distributed random vari- ables X and Y is defined as the KL divergence of the joint distribution p(x, y) from the product p(x)p(y) of the marginal distributions of X and Y, i.e. I(X; Y ) = KL(p(x, y)||p(x)p(y)).</p><p>The Jensen-Shannon (JS) divergence (Lin, 1991) between distributions p and q is:</p><formula xml:id="formula_1">JS α (p, q) = αKL(p||r) + (1−α)KL(q||r) (1) = H(r) − αH(p) − (1−α)H(q)</formula><p>such that 0 &lt; α &lt; 1, r = αp + (1 − α)q and H is the entropy function (i.e. H(p) = − i p i log p i ). Unlike KL divergence, JS divergence is bounded from above and 0 ≤ JS α (p, q) ≤ 1.</p><p>We next propose a new measure for mutual- information using the JS-divergence between p(x, y) and p(x)p(y) instead of the KL-divergence. We define the Jensen-Shannon Mutual information (JSMI) as follows:</p><formula xml:id="formula_2">JSMI α (X, Y ) = JS α (p(x, y), p(x)p(y)). (2)</formula><p>It can be easily verified that X and Y are indepen- dent if and only if JSMI α (X, Y ) = 0.</p><p>We next derive an alternative definition of the JSMI dependency measure. Assume we choose be- tween the two distributions, p(x, y) and the product of marginal distributions p(x)p(y), according to a binary random variable Z, such that p(Z = 1) = α. We first sample a binary value for Z and next, we sample a r.v. W as follows:</p><formula xml:id="formula_3">p(W = (x, y)|Z) = p(x)p(y) if Z = 0 p(x, y) if Z = 1. (3)</formula><p>The divergence measure JSMI α (X, Y ) can be al- ternatively defined in terms of mutual information between W and Z. The mutual-information be- tween W and Z is:</p><formula xml:id="formula_4">I(W;Z) = H(W ) − i=0,1 p(Z = i)H(W |Z = i) = H(αp(x, y) + (1−α)p(x)p(y)) −αH(p(x, y)) − (1−α)H(p(x)p(y)).</formula><p>Eq. (1) thus implies that:</p><formula xml:id="formula_5">JSMI α (X, Y ) = I(W ; Z).<label>(4)</label></formula><p>Applying Bayes rule we obtain:</p><formula xml:id="formula_6">p(Z = 1|W = (x, y)) (5) = αp(x, y) αp(x, y) + (1−α)p(x)p(y) = 1 1 + exp(− log( αp(x,y) (1−α)p(x)p(y) )) = σ(pmi x,y ) such that σ(u) = 1 1+exp(−u)</formula><p>is the sigmoid func- tion and</p><formula xml:id="formula_7">pmi x,y = log p(x, y) p(x)p(y) + log α 1−α (6)</formula><p>is a shifted version of the PMI function. Equa- tions <ref type="formula" target="#formula_5">(4)</ref> and <ref type="formula">(5)</ref> imply that:</p><formula xml:id="formula_8">JSMI α (X, Y ) = H(Z) − H(Z|W )<label>(7)</label></formula><p>= h(α)+α</p><formula xml:id="formula_9">x,y p(x, y) log σ(pmi x,y ) +(1−α) x,y p(x)p(y) log σ(−pmi x,y ) such that h(α) = −α log(α) − (1−α) log(1−α)</formula><p>is the binary entropy function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Skip-Gram Embedding Algorithm</head><p>The SGNS embedding algorithm ( <ref type="bibr" target="#b9">Mikolov et al., 2013</ref>) represents each word x and each context y as d-dimensional vectors x and y, with the purpose that words that are "similar" to each other will have similar vector representations. We can represent a given d-dimensional embedding by a matrix m, such that m(x, y) = x · y. The rank of the embed- ding matrix m is (at most) d. Let p(x, y) be the normalized number of co- occurrences of word x and context-word y in a given corpus and let p(x) and p(y) be the corre- sponding unigram distributions. Consider a binary classifier that treats a word and its observed con- text as a positive example, and a word and a ran- domly sampled context as a negative example. The classification is made based on the embedding in such a way that the probability that (x, y) is a pos- itive example is σ( x · y). The objective function ideally maximized by the SGNS word embedding algorithm is the expectation of the log-likelihood function of the embedding:</p><formula xml:id="formula_10">S(m) = h( 1 k+1 ) + 1 k+1 x,y p(x, y) log σ( x · y) + k k+1</formula><p>x,y p(x)p(y) log σ(− x · y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(8)</head><p>Note that the term h( 1 k+1 ), which does not appear in the original SGNS objective function ( <ref type="bibr" target="#b9">Mikolov et al., 2013)</ref>, is a constant number that was added here to simplify the following presentation.</p><p>The sparsity of p(x, y) (which is obtained as normalized counts from a given learning corpus) makes it feasible to compute the second term of (8). The number of summed-over elements in the third term of <ref type="formula">(8)</ref>, however, is quadratic in the size of the vocabulary, making it hard to compute. Therefore, in practice, we can approximate the expectation by sampling of 'negative' examples. The actual SGNS score, then, is:</p><formula xml:id="formula_11">S(m) ≈ h( 1 k+1 ) + 1 k+1 · 1 n n t=1 (log σ( x t · y t ) + k i=1 log σ(− x t · y ti )).<label>(9)</label></formula><p>such that t goes over all the word-context pairs in a given corpus. The negative examples y ti are created for each pair (x t , y t ) by drawing k random contexts from the context-word distribution p(y).</p><p>As pointed out in ( <ref type="bibr" target="#b7">Levy et al., 2015)</ref>, k has two distinct functions in the SGNS objective function. First, it is used to better estimate the distribution of negative examples. Second, it is used as a weight on the probability of observing a positive example versus a negative example; a higher k means that negative examples are more probable.</p><p>We can compute the SGNS score function S(m) for every real-valued matrix m = (m x,y ). <ref type="bibr" target="#b6">Levy and Goldberg (2014)</ref> showed that the function achieves its global maximal value when for each word-pair (x, y) the inner product of the embed- ding vectors x · y is equal to pmi <ref type="bibr">(x, y)</ref>. In other words they showed that S(m) ≤ S(pmi) for every matrix m. We next show that the value of the func- tion S(m) at its maximum point, the PMI matrix, has a concrete interpretation, namely it is exactly the Jensen-Shannon Mutual Information (JSMI) between words and their contexts.</p><p>Theorem 1: The value of the SGNS score with k negative samples (8) at the PMI matrix satisfies:</p><formula xml:id="formula_12">S(pmi) = JSMI α (X, Y )</formula><p>such that α = 1 k+1 . Proof: It can be easily verified that by substituting α = 1 k+1 in the definition of JSMI (Eq. <ref type="formula" target="#formula_8">(7)</ref>), we ex- actly obtain the SGNS score (8) at the PMI matrix. 2</p><p>Levy and Goldberg <ref type="bibr">(2014)</ref> showed that SGNS's objective achieves its maximal value at the PMI ma- trix. However, this result reveals nothing about the more interesting lower dimensional case, where the PMI matrix factorization is forced to compress the joint distribution and thereby learn a meaningful embedding. We next derive an explicit description of the approximation criterion that quantifies the gap between S(m) and S(pmi).</p><p>Given the word co-occurrences joint distribution p(x, y), we obtained in Eq. <ref type="formula">(5)</ref> a conditional distri- bution on the alphabet of (Z, W ) as follows:</p><formula xml:id="formula_13">p(Z = 1|W = (x, y)) = σ(pmi x,y ).</formula><p>In a similar way, given any matrix m, we can define a conditional distribution p m on the alphabet of (Z, W ) as follows:</p><formula xml:id="formula_14">p m (Z = 1|W = (x, y)) = σ(m x,y ).</formula><p>Note that in the special case where m is the PMI matrix, p pmi (z|w) coincides with the original p(z|w) that was defined in Eq. (5). Theorem 2: The difference between the SGNS score at the PMI matrix and the SGNS score at a given matrix m can be written as:</p><formula xml:id="formula_15">S(pmi) − S(m) = KL(p pmi (Z|W )||p m (Z|W )) (10) Proof: S(pmi)−S(m) = x,y (αp(x, y) log σ(pmi x,y ) σ(m x,y ) +(1−α)p(x)p(y) log σ(−pmi x,y ) σ(−m x,y ) ) = x,y (αp(x, y) log p pmi (Z = 1|x, y) p m (Z = 1|x, y) +(1−α)p(x)p(y) log p pmi (Z = 0|x, y) p m (Z = 0|x, y) ) = w,z p(W = w, Z = z) log p pmi (Z = z|W = w) p m (Z = z|W = w) = KL(p pmi (Z|W )||p m (Z|W )).2</formula><p>The KL divergence between two distributions is always non-negative and is zero only if the two distributions are the same. Therefore, we red- erive the results of ( <ref type="bibr" target="#b6">Levy and Goldberg, 2014</ref>) that S(pmi) = max m S(m). Theorem 2 can be viewed as an instance of the well-known connec- tion between maximizing log-likelihood and mini- mizing KL divergence between the estimated and the true data-generating distribution. In this case, the true distribution is the pmi-based classifier p pmi (Z|W ).</p><p>Combining theorems 1 and 2 we obtain that S(m) ≤ JSMI α (X, Y ) for every low-dimensional embedding matrix. The difference JSMI α (X, Y )− S(m) is the information loss caused by the low- dimensional embedding. We can view it as a Jensen-Shannon variant of the information bottle- neck principle <ref type="bibr" target="#b13">(Tishby et al., 1999;</ref><ref type="bibr" target="#b4">Globerson et al., 2007</ref>) that is defined in terms of the KL divergence. The optimal d-dimensional embedding, is the best d-dimensional approximation of the JSMI depen- dency measure in the sense that it minimizes the information loss. The JSMI is the upper bound that any embedding can obtain. To illustrate that, in the next section we compute the JSMI between words and their contexts based on a standard text corpus and show the information gap between the JSMI and the actual SGNS score as a function of the embedding dimension d.</p><p>From Theorem 2 we can also derive an explicit information-theoretic interpretation of the score function S(m) (7) as the difference between two KL-divergence terms:</p><formula xml:id="formula_16">S(m) = S(pmi) − (S(pmi) − S(m)) = I(Z; W ) − (S(pmi) − S(m)) = KL(p(Z|W )||p(Z)) − KL(p(Z|W )||p m (Z|W ))</formula><p>The word embedding problem can be also viewed as a factorization of the PMI matrix. Previ- ous works suggested other criteria for matrix fac- torization such as least-squares <ref type="bibr" target="#b2">(Eckart and Young, 1936)</ref> and KL-divergence between the original ma- trix and the low-rank matrix approximation <ref type="bibr" target="#b5">(Lee and Seung, 2000</ref>). We have shown that the SGNS algorithm factorizes the PMI matrix based on the JSMI-based criterion stated in Eq. (10). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section we use word2vec to train real skip- gram with negative sampling (SGNS) embedding models. By measuring the value of their objec- tive function and comparing it against the optimal one using exact PMI values, we demonstrate how a well-trained model minimizes the difference in Eq. (10). We note that this is an intrinsic measure that does not necessarily reflect the usefulness of the learned embeddings for other tasks.</p><p>We used the Penn Tree Bank (PTB), a popu- lar small-scale corpus, for our experiments. A version of this dataset is available from Tomas Mikolov. 1 It consists of 929K training words with a 10K word vocabulary, which we used to train our models. To learn the SGNS word embeddings, we used word2vec's default parameter values: window- size = 5, min-count = 5, and number of negative samples k = 5. We varied the dimensionality of the embeddings and the number of training itera- tions performed. Once the models were trained, we measured their score (9) on the training corpus.</p><p>Based on the same learning corpus, we computed S(pmi) = JSMI α (X, Y ) for α = 1 k+1 = 1/6. Note that p(x, y) = 0 implies that pmi x,y = −∞ and therefore log σ(−pmi x,y ) = 0. Hence, as in the second term, to compute the third term of S(m) (8) for the case of m = pmi, we can sum only over the positive pairs (x, y) that actually appear in the corpus. <ref type="bibr" target="#b3">2</ref> In other words, for the special case m = pmi, it is feasible to compute the ex- act score <ref type="bibr">(8)</ref> and not just its approximation (9) that is based on negative sampling. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the optimal PMI-based score, compared with the scores obtained by different models with varied embedding dimensionality and number of training iterations. As can be seen, the embeddings score gets close to the optimal value using higher dimen- sionality and more training iterations, but doesn't surpass it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this study, we developed a new correlation mea- sure between random variables, denoted JSMI. This measure is based on the JS divergence and dif- fers from the standard mutual information measure that is based on the KL divergence. We showed that the optimization of skip-gram embeddings with negative sampling finds the best low-dimensional approximation of the JSMI measure. Thus, we pro- vided an information theory framework that hope- fully contributes to a better understanding of this embedding algorithm. Furthermore, although we focused here on the case of word-context joint dis- tributions, the connection we haven shown between the PMI matrix and the JSMI function is valid for every joint distribution of two random variables.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: SGNS objective function score of trained embeddings models, compared to the optimal PMIbased score. dim and iter denote the dimensionality and training iterations used for each model.</figDesc><graphic url="image-1.png" coords="4,307.31,62.81,218.21,175.75" type="bitmap" /></figure>

			<note place="foot" n="2"> A Dependency Measure based on Jensen-Shannon In this section, we define a dependency measure between two random variables, which is based on the Jensen-Shannon divergence. Later, in Section 3, we show how it relates to the SGNS objective function.</note>

			<note place="foot" n="1"> http://www.fit.vutbr.cz/~imikolov/ rnnlm/simple-examples.tgz</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A latent variable model approach to pmi-based word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Risteski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="385" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tailoring continuous word representations for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The approximation of one matrix by another of lower rank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Eckart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gale</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="211" to="218" />
			<date type="published" when="1936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">We used the exact same positive co-occurrence pairs sam</title>
		<imprint/>
	</monogr>
	<note>pled by word2vec during the training of the SGNS embeddings to compute S(pmi</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Euclidean embedding of cooccurrence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Amir Globerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naftaly</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tishby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="2265" to="2295" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Algorithms for nonnegative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural word embedding as implicit matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving distributional similarity with lessons learned from word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="211" to="225" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Divergence measures based on the shannon entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="145" to="151" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Lexicon infused phrase embeddings for named entity resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Natural Language Learning (CoNLL)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Model-based word embeddings from decompositions of count matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel J</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1282" to="1291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The information bottleneck method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naftaly</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Bialek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Allerton Conf. on Communication, Control, and Computing</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
