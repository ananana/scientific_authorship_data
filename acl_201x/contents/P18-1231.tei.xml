<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:06+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bilingual Sentiment Embeddings: Joint Projection of Sentiment Across Languages</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Barnes</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institut für Maschinelle</orgName>
								<orgName type="institution">Sprachverarbeitung University of Stuttgart</orgName>
								<address>
									<addrLine>Pfaffenwaldring 5b</addrLine>
									<postCode>70569</postCode>
									<settlement>Stuttgart</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Klinger</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institut für Maschinelle</orgName>
								<orgName type="institution">Sprachverarbeitung University of Stuttgart</orgName>
								<address>
									<addrLine>Pfaffenwaldring 5b</addrLine>
									<postCode>70569</postCode>
									<settlement>Stuttgart</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Schulte</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institut für Maschinelle</orgName>
								<orgName type="institution">Sprachverarbeitung University of Stuttgart</orgName>
								<address>
									<addrLine>Pfaffenwaldring 5b</addrLine>
									<postCode>70569</postCode>
									<settlement>Stuttgart</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walde</forename></persName>
							<affiliation key="aff0">
								<orgName type="department">Institut für Maschinelle</orgName>
								<orgName type="institution">Sprachverarbeitung University of Stuttgart</orgName>
								<address>
									<addrLine>Pfaffenwaldring 5b</addrLine>
									<postCode>70569</postCode>
									<settlement>Stuttgart</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Bilingual Sentiment Embeddings: Joint Projection of Sentiment Across Languages</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2483" to="2493"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2483</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Sentiment analysis in low-resource languages suffers from a lack of annotated corpora to estimate high-performing models. Machine translation and bilingual word embeddings provide some relief through cross-lingual sentiment approaches. However , they either require large amounts of parallel data or do not sufficiently capture sentiment information. We introduce Bilingual Sentiment Embeddings (BLSE), which jointly represent sentiment information in a source and target language. This model only requires a small bilingual lexicon, a source-language corpus annotated for sentiment, and monolingual word embed-dings for each language. We perform experiments on three language combinations (Spanish, Catalan, Basque) for sentence-level cross-lingual sentiment classification and find that our model significantly out-performs state-of-the-art methods on four out of six experimental setups, as well as capturing complementary information to machine translation. Our analysis of the resulting embedding space provides evidence that it represents sentiment information in the resource-poor target language without any annotated data in that language.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Cross-lingual approaches to sentiment analysis are motivated by the lack of training data in the vast majority of languages. Even languages spoken by several million people, such as Catalan, often have few resources available to perform sentiment analysis in specific domains. We therefore aim to harness the knowledge previously collected in resource-rich languages.</p><p>Previous approaches for cross-lingual sentiment analysis typically exploit machine translation based methods or multilingual models. Machine trans- lation (MT) can provide a way to transfer senti- ment information from a resource-rich to resource- poor languages ( <ref type="bibr" target="#b23">Mihalcea et al., 2007;</ref><ref type="bibr" target="#b5">Balahur and Turchi, 2014</ref>). However, MT-based methods re- quire large parallel corpora to train the translation system, which are often not available for under- resourced languages.</p><p>Examples of multilingual methods that have been applied to cross-lingual sentiment analysis include domain adaptation methods ( <ref type="bibr" target="#b28">Prettenhofer and Stein, 2011</ref>), delexicalization ( <ref type="bibr" target="#b2">Almeida et al., 2015)</ref>, and bilingual word embeddings ( <ref type="bibr" target="#b24">Mikolov et al., 2013;</ref><ref type="bibr">Hermann and Blunsom, 2014;</ref><ref type="bibr" target="#b3">Artetxe et al., 2016</ref>). These approaches however do not in- corporate enough sentiment information to perform well cross-lingually, as we will show later.</p><p>We propose a novel approach to incorporate sen- timent information in a model, which does not have these disadvantages. Bilingual Sentiment Embed- dings (BLSE) are embeddings that are jointly opti- mized to represent both (a) semantic information in the source and target languages, which are bound to each other through a small bilingual dictionary, and (b) sentiment information, which is annotated on the source language only. We only need three resources: (i) a comparably small bilingual lexicon, (ii) an annotated sentiment corpus in the resource- rich language, and (iii) monolingual word embed- dings for the two involved languages.</p><p>We show that our model outperforms previous state-of-the-art models in nearly all experimental settings across six benchmarks. In addition, we offer an in-depth analysis and demonstrate that our model is aware of sentiment. Finally, we provide a qualitative analysis of the joint bilingual sentiment space. Our implementation is publicly available at https://github.com/jbarnesspain/blse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Machine Translation: Early work in cross-lingual sentiment analysis found that machine translation (MT) had reached a point of maturity that enabled the transfer of sentiment across languages. Re- searchers translated sentiment lexicons ( <ref type="bibr" target="#b23">Mihalcea et al., 2007;</ref><ref type="bibr" target="#b22">Meng et al., 2012)</ref> or annotated corpora and used word alignments to project sentiment an- notation and create target-language annotated cor- pora ( <ref type="bibr" target="#b7">Banea et al., 2008;</ref><ref type="bibr" target="#b13">Duh et al., 2011;</ref><ref type="bibr" target="#b12">Demirtas and Pechenizkiy, 2013;</ref><ref type="bibr" target="#b5">Balahur and Turchi, 2014)</ref>.</p><p>Several approaches included a multi-view repre- sentation of the data ( <ref type="bibr" target="#b6">Banea et al., 2010;</ref><ref type="bibr">Xiao and Guo, 2012</ref>) or co-training <ref type="bibr">(Wan, 2009;</ref><ref type="bibr" target="#b12">Demirtas and Pechenizkiy, 2013</ref>) to improve over a naive implementation of machine translation, where only the translated data is used. There are also ap- proaches which only require parallel data ( <ref type="bibr" target="#b22">Meng et al., 2012;</ref><ref type="bibr">Zhou et al., 2016;</ref><ref type="bibr" target="#b29">Rasooli et al., 2017)</ref>, instead of machine translation.</p><p>All of these approaches, however, require large amounts of parallel data or an existing high qual- ity translation tool, which are not always available. A notable exception is the approach proposed by <ref type="bibr" target="#b11">Chen et al. (2016)</ref>, an adversarial deep averaging network, which trains a joint feature extractor for two languages. They minimize the difference be- tween these features across languages by learning to fool a language discriminator, which requires no parallel data. It does, however, require large amounts of unlabeled data.</p><p>Bilingual Embedding Methods: Recently pro- posed bilingual embedding methods ( <ref type="bibr">Hermann and Blunsom, 2014;</ref><ref type="bibr" target="#b9">Chandar et al., 2014;</ref>) offer a natural way to bridge the language gap. These particular approaches to bilingual em- beddings, however, require large parallel corpora in order to build the bilingual space, which are not available for all language combinations.</p><p>An approach to create bilingual embeddings that has a less prohibitive data requirement is to create monolingual vector spaces and then learn a projec- tion from one to the other. <ref type="bibr" target="#b24">Mikolov et al. (2013)</ref> find that vector spaces in different languages have similar arrangements. Therefore, they propose a linear projection which consists of learning a rota- tion and scaling matrix. <ref type="bibr" target="#b3">Artetxe et al. (2016</ref><ref type="bibr" target="#b4">Artetxe et al. ( , 2017</ref> improve upon this approach by requiring the pro- jection to be orthogonal, thereby preserving the monolingual quality of the original word vectors.</p><p>Given source embeddings S, target embed- dings T , and a bilingual lexicon L, <ref type="bibr" target="#b3">Artetxe et al. (2016)</ref> learn a projection matrix W by minimizing the square of Euclidean distances arg min</p><formula xml:id="formula_0">W i ||S W − T || 2 F ,<label>(1)</label></formula><p>where S ∈ S and T ∈ T are the word embedding matrices for the tokens in the bilingual lexicon L. This is solved using the Moore-Penrose pseudoin- verse S + = (S T S ) −1 S T as W = S + T , which can be computed using SVD. We refer to this ap- proach as ARTETXE. <ref type="bibr" target="#b15">Gouws and Søgaard (2015)</ref> propose a method to create a pseudo-bilingual corpus with a small task- specific bilingual lexicon, which can then be used to train bilingual embeddings (BARISTA). This approach requires a monolingual corpus in both the source and target languages and a set of trans- lation pairs. The source and target corpora are concatenated and then every word is randomly kept or replaced by its translation with a probability of 0.5. Any kind of word embedding algorithm can be trained with this pseudo-bilingual corpus to create bilingual word embeddings.</p><p>These last techniques have the advantage of re- quiring relatively little parallel training data while taking advantage of larger amounts of monolingual data. However, they are not optimized for senti- ment.</p><p>Sentiment Embeddings: Maas et al. (2011) first explored the idea of incorporating sentiment in- formation into semantic word vectors. They pro- posed a topic modeling approach similar to latent Dirichlet allocation in order to collect the semantic information in their word vectors. To incorporate the sentiment information, they included a second objective whereby they maximize the probability of the sentiment label for each word in a labeled document. <ref type="bibr" target="#b31">Tang et al. (2014)</ref> exploit distantly annotated tweets to create Twitter sentiment embeddings. To incorporate distributional information about tokens, they use a hinge loss and maximize the likelihood of a true n-gram over a corrupted n-gram. They include a second objective where they classify the polarity of the tweet given the true n-gram. While these techniques have proven useful, they are not easily transferred to a cross-lingual setting. <ref type="bibr">Zhou et al. (2015)</ref> create bilingual sentiment embeddings by translating all source data to the target language and vice versa. This requires the existence of a machine translation system, which is a prohibitive assumption for many under-resourced languages, especially if it must be open and freely accessible. This motivates approaches which can use smaller amounts of parallel data to achieve similar results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>In order to project not only semantic similarity and relatedness but also sentiment information to our target language, we propose a new model, namely Bilingual Sentiment Embeddings (BLSE), which jointly learns to predict sentiment and to minimize the distance between translation pairs in vector space. We detail the projection objective in Sec- tion 3.1, the sentiment objective in Section 3.2, and the full objective in Section 3.3. A sketch of the model is depicted in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Cross-lingual Projection</head><p>We assume that we have two precomputed vector spaces S = R v×d and T = R v ×d for our source and target languages, where v (v ) is the length of the source vocabulary (target vocabulary) and d (d ) is the dimensionality of the embeddings. We also assume that we have a bilingual lexicon L of length n which consists of word-to-word trans- lation pairs L = {(s 1 , t 1 ), (s 2 , t 2 ), . . . , (s n , t n )} which map from source to target.</p><p>In order to create a mapping from both origi- nal vector spaces S and T to shared sentiment- informed bilingual spaces z andˆzandˆandˆz, we employ two linear projection matrices, M and M . During training, for each translation pair in L, we first look up their associated vectors, project them through their associated projection matrix and finally mini- mize the mean squared error of the two projected vectors. This is very similar to the approach taken by <ref type="bibr" target="#b24">Mikolov et al. (2013)</ref>, but includes an additional target projection matrix.</p><p>The intuition for including this second matrix is that a single projection matrix does not support the transfer of sentiment information from the source language to the target language. Without M , any signal coming from the sentiment classifier (see Section 3.2) would have no affect on the target embedding space T , and optimizing M to predict sentiment and projection would only be detrimental to classification of the target language. We analyze this further in Section 6.3. Note that in this con- figuration, we do not need to update the original vector spaces, which would be problematic with such small training data.</p><p>The projection quality is ensured by minimizing the mean squared error 12</p><formula xml:id="formula_1">MSE = 1 n n i=1 (z i − ˆ z i ) 2 ,<label>(2)</label></formula><p>where z i = S s i · M is the dot product of the embed- ding for source word s i and the source projection matrix andˆzandˆandˆz i = T t i · M is the same for the target word t i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sentiment Classification</head><p>We add a second training objective to optimize the projected source vectors to predict the senti- ment of source phrases. This inevitably changes the projection characteristics of the matrix M , and consequently M and encourages M to learn to predict sentiment without any training examples in the target language.</p><p>To train M to predict sentiment, we re- quire a source-language corpus C source = {(x 1 , y 1 ), (x 2 , y 2 ), . . . , (x i , y i )} where each sen- tence x i is associated with a label y i .</p><p>For classification, we use a two-layer feed- forward averaging network, loosely following <ref type="bibr" target="#b18">Iyyer et al. (2015)</ref> 3 . For a sentence x i we take the word embeddings from the source embedding S and av- erage them to a i ∈ R d . We then project this vector to the joint bilingual space z i = a i · M . Finally, we pass z i through a softmax layer P to get our predictionˆypredictionˆ predictionˆy i = softmax(z i · P ).</p><p>To train our model to predict sentiment, we min- imize the cross-entropy error of our predictions</p><formula xml:id="formula_2">H = − n i=1 y i logˆylogˆ logˆy i − (1 − y i ) log(1 − ˆ y i ) . (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Joint Learning</head><p>In order to jointly train both the projection com- ponent and the sentiment component, we combine the two loss functions to optimize the parameter </p><formula xml:id="formula_3">J = (x,y)∈Csource (s,t)∈L αH(x, y) + (1 − α) · MSE(s, t) , (4)</formula><p>where α is a hyperparameter that weights sentiment loss vs. projection loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Target-language Classification</head><p>For inference, we classify sentences from a target- language corpus C target . As in the training proce- dure, for each sentence, we take the word embed- dings from the target embeddings T and average them to a i ∈ R d . We then project this vector to the joint bilingual spacê z i = a i · M . Finally, we pass  ˆ z i through a softmax layer P to get our predictionˆy predictionˆ predictionˆy i = softmax(ˆ z i · P ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Datasets and Resources</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">OpeNER and MultiBooked</head><p>To evaluate our proposed model, we conduct ex- periments using four benchmark datasets and three bilingual combinations. We use the OpeNER En- glish and Spanish datasets ( <ref type="bibr" target="#b1">Agerri et al., 2013)</ref> and the MultiBooked Catalan and Basque datasets ( <ref type="bibr" target="#b8">Barnes et al., 2018</ref>). All datasets contain hotel reviews which are annotated for aspect-level senti- ment analysis. The labels include Strong Negative (−−), Negative (−), Positive (+), and Strong Pos- itive (++). We map the aspect-level annotations to sentence level by taking the most common label and remove instances of mixed polarity. We also create a binary setup by combining the strong and weak classes. This gives us a total of six experi- ments. The details of the sentence-level datasets are summarized in <ref type="table">Table 1</ref>. For each of the experi- ments, we take 70 percent of the data for training, 20 percent for testing and the remaining 10 percent are used as development data for tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Monolingual Word Embeddings</head><p>For BLSE, ARTETXE, and MT, we require monolin- gual vector spaces for each of our languages. For English, we use the publicly available GoogleNews vectors 4 . For Spanish, Catalan, and Basque, we train skip-gram embeddings using the Word2Vec toolkit 4 with 300 dimensions, subsampling of 10 −4 , window of 5, negative sampling of 15 based on a 2016 Wikipedia corpus 5 (sentence-split, tokenized with IXA pipes ( <ref type="bibr" target="#b0">Agerri et al., 2014</ref>) and lower- cased). The statistics of the Wikipedia corpora are given in <ref type="table" target="#tab_2">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Bilingual Lexicon</head><p>For BLSE, ARTETXE, and BARISTA, we also re- quire a bilingual lexicon. We use the sentiment lexicon from Hu and Liu (2004) (to which we refer in the following as Bing Liu) and its translation into each target language. We translate the lexicon using Google Translate and exclude multi-word ex- pressions. <ref type="bibr">6</ref> This leaves a dictionary of 5700 trans- lations in Spanish, 5271 in Catalan, and 4577 in Basque. We set aside ten percent of the translation pairs as a development set in order to check that the distances between translation pairs not seen during training are also minimized during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Setting</head><p>We compare BLSE (Sections 3.1-3.3) to ARTETXE (Section 2) and BARISTA (Section 2) as baselines, which have similar data requirements and to ma- chine translation (MT) and monolingual (MONO) upper bounds which request more resources. For all models (MONO, MT, ARTETXE, BARISTA), we take the average of the word embeddings in the source-language training examples and train a linear SVM 7 . We report this instead of using the same feed-forward network as in BLSE as it is the stronger upper bound. We choose the parameter c on the target language development set and evalu- ate on the target language test set.</p><p>Upper Bound MONO. We set an empirical up- per bound by training and testing a linear SVM on the target language data. As mentioned in Sec- tion 5.1, we train the model on the averaged em- beddings from target language training data, tuning the c parameter on the development data. We test on the target language test data.</p><p>Upper Bound MT. To test the effectiveness of machine translation, we translate all of the senti- ment corpora from the target language to English using the Google Translate API 8 . Note that this approach is not considered a baseline, as we as- sume not to have access to high-quality machine translation for low-resource languages of interest.</p><p>Baseline ARTETXE. We compare with the ap- proach proposed by <ref type="bibr" target="#b3">Artetxe et al. (2016)</ref> which has shown promise on other tasks, such as word similarity. In order to learn the projection matrix W , we need translation pairs. We use the same word-to-word bilingual lexicon mentioned in Sec- tion 3.1. We then map the source vector space S to the bilingual spacê S = SW and use these embeddings.</p><p>Baseline BARISTA. We also compare with the approach proposed by <ref type="bibr" target="#b15">Gouws and Søgaard (2015)</ref>. The bilingual lexicon used to create the pseudo- bilingual corpus is the same word-to-word bilin- gual lexicon mentioned in Section 3.1. We follow the authors' setup to create the pseudo-bilingual corpus. We create bilingual embeddings by train- ing skip-gram embeddings using the Word2Vec toolkit on the pseudo-bilingual corpus using the same parameters from Section 4.2.</p><p>Our method: BLSE. We implement our model BLSE in Pytorch ( <ref type="bibr" target="#b27">Paszke et al., 2016)</ref> and initial- ize the word embeddings with the pretrained word embeddings S and T mentioned in Section 4.2. We use the word-to-word bilingual lexicon from Section 4.3, tune the hyperparameters α, training epochs, and batch size on the target development set and use the best hyperparameters achieved on the development set for testing. ADAM <ref type="bibr" target="#b19">(Kingma and Ba, 2014</ref>) is used in order to minimize the average loss of the training batches.  <ref type="table">Table 3</ref>: Precision (P), Recall (R), and macro F 1 of four models trained on English and tested on Span- ish (ES), Catalan (CA), and Basque (EU). The bold numbers show the best results for each metric per column and the highlighted numbers show where BLSE is better than the other projection methods, ARTETXE and BARISTA (** p &lt; 0.01, * p &lt; 0.05).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Binary</head><p>Ensembles We create an ensemble of MT and each projection method (BLSE, ARTETXE, BARISTA) by training a random forest classifier on the predictions from MT and each of these ap- proaches. This allows us to evaluate to what extent each projection model adds complementary infor- mation to the machine translation approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>In <ref type="figure" target="#fig_0">Figure 2</ref>, we report the results of all four meth- ods. Our method outperforms the other projection methods (the baselines ARTETXE and BARISTA) on four of the six experiments substantially. It per- forms only slightly worse than the more resource- costly upper bounds (MT and MONO). This is espe- cially noticeable for the binary classification task, where BLSE performs nearly as well as machine translation and significantly better than the other methods. We perform approximate randomization tests <ref type="bibr">(Yeh, 2000</ref>) with 10,000 runs and highlight the results that are statistically significant (**p &lt; 0.01, *p &lt; 0.05) in <ref type="table">Table 3</ref>.</p><p>In more detail, we see that MT generally per- forms better than the projection methods (79-69 F 1 on binary, 52-44 on 4-class). BLSE (75-69 on binary, 41-30 on 4-class) has the best perfor- mance of the projection methods and is comparable with MT on the binary setup, with no significant difference on binary Basque. ARTETXE (67-46 on binary, 35-21 on 4-class) and BARISTA (61- 55 on binary, 40-34 on 4-class) are significantly worse than BLSE on all experiments except Cata- lan and Basque 4-class. On the binary experiment, ARTETXE outperforms BARISTA on Spanish (67.1 vs. 61.2) and Catalan (60.7 vs. 60.1) but suffers more than the other methods on the four-class ex- periments, with a maximum F 1 of <ref type="bibr">34</ref>   is relatively stable across languages. ENSEMBLE performs the best, which shows that BLSE adds complementary information to MT. Fi- nally, we note that all systems perform successively worse on Catalan and Basque. This is presum- ably due to the quality of the word embeddings, as well as the increased morphological complexity of Basque.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Model and Error Analysis</head><p>We analyze three aspects of our model in further detail: (i) where most mistakes originate, (ii) the ef- fect of the bilingual lexicon, and (iii) the effect and necessity of the target-language projection matrix M .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Phenomena</head><p>In order to analyze where each model struggles, we categorize the mistakes and annotate all of the test phrases with one of the following error classes: vo- cabulary (voc), adverbial modifiers (mod), negation (neg), external knowledge (know) or other. <ref type="table" target="#tab_5">Table 4</ref> shows the results.</p><p>Vocabulary: The most common way to express sentiment in hotel reviews is through the use of polar adjectives (as in "the room was great) or the mention of certain nouns that are desirable ("it had a pool"). Although this phenomenon has the largest total number of mistakes (an average of 71 per model on binary and 167 on 4-class), it is mainly due to its prevalence. MT performed the best on the test examples which according to the an- notation require a correct understanding of the vo- cabulary (81 F 1 on binary /54 F 1 on 4-class), with BLSE (79/48) slightly worse. ARTETXE (70/35) and BARISTA (67/41) perform significantly worse. This suggests that BLSE is better ARTETXE and BARISTA at transferring sentiment of the most im- portant sentiment bearing words.</p><p>Negation: Negation is a well-studied phe- nomenon in sentiment analysis ( <ref type="bibr" target="#b26">Pang et al., 2002;</ref><ref type="bibr">Wiegand et al., 2010;</ref><ref type="bibr">Zhu et al., 2014;</ref><ref type="bibr" target="#b30">Reitan et al., 2015)</ref>. Therefore, we are interested in how these four models perform on phrases that include the negation of a key element, for example "In general, this hotel isn't bad". We would like our models to recognize that the combination of two negative elements "isn't" and "bad" lead to a Positive label.</p><p>Given the simple classification strategy, all mod- els perform relatively well on phrases with negation (all reach nearly 60 F 1 in the binary setting). How- ever, while BLSE performs the best on negation in the binary setting (82.9 F 1 ), it has more problems with negation in the 4-class setting (36.9 F 1 ).</p><p>Adverbial Modifiers: Phrases that are modified by an adverb, e. g., the food was incredibly good, are important for the four-class setup, as they often differentiate between the base and Strong labels. In the binary case, all models reach more than 55 F 1 . In the 4-class setup, BLSE only achieves 27.2 F 1 compared to 46.6 or 31.3 of MT and BARISTA, respectively. Therefore, presumably, our model does currently not capture the semantics of the target adverbs well. This is likely due to the fact that it assigns too much sentiment to functional words (see <ref type="figure" target="#fig_4">Figure 6</ref>).</p><p>External Knowledge Required: These errors are difficult for any of the models to get cor- rect. Many of these include numbers which imply positive or negative sentiment (350 meters from the beach is Positive while 3 kilometers from the beach is Negative). BLSE performs the best (63.5 F 1 ) while MT performs comparably well (62.5). BARISTA performs the worst (43.6).</p><p>Binary vs. 4-class: All of the models suffer when moving from the binary to 4-class setting; an average of 26.8 in macro F 1 for MT, 31.4 for ARTETXE, 22.2 for BARISTA, and for 36.6 BLSE. The two vector projection methods (ARTETXE and BLSE) suffer the most, suggesting that they are currently more apt for the binary setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Effect of Bilingual Lexicon</head><p>We analyze how the number of translation pairs affects our model. We train on the 4-class Span- ish setup using the best hyper-parameters from the previous experiment. Figure 4: Average cosine similarity between a subsample of translation pairs of same polarity ("sentiment synonyms") and of opposing polarity ("sentiment antonyms") in both target and source languages in each model. The x-axis shows training epochs. We see that BLSE is able to learn that sentiment synonyms should be close to one another in vector space and sentiment antonyms should not.</p><p>Research into projection techniques for bilingual word embeddings ( <ref type="bibr" target="#b24">Mikolov et al., 2013;</ref><ref type="bibr" target="#b20">Lazaridou et al., 2015;</ref><ref type="bibr" target="#b3">Artetxe et al., 2016</ref>) often uses a lex- icon of the most frequent 8-10 thousand words in English and their translations as training data. We test this approach by taking the 10,000 word- to-word translations from the Apertium English- to-Spanish dictionary 9 . We also use the Google Translate API to translate the NRC hashtag senti- ment lexicon <ref type="bibr" target="#b25">(Mohammad et al., 2013</ref>) and keep the 22,984 word-to-word translations. We perform the same experiment as above and vary the amount of training data from 0, 100, 300, 600, 1000, 3000, 6000, 10,000 up to 20,000 training pairs. Finally, we compile a small hand translated dictionary of 200 pairs, which we then expand using target lan- guage morphological information, finally giving us 657 translation pairs 10 . The macro F 1 score for the Bing Liu dictionary climbs constantly with the increasing translation pairs. Both the Apertium and NRC dictionaries perform worse than the trans- lated lexicon by Bing Liu, while the expanded hand translated dictionary is competitive, as shown in <ref type="figure" target="#fig_1">Figure 3</ref>.</p><p>While for some tasks, e. g., bilingual lexicon induction, using the most frequent words as trans- lation pairs is an effective approach, for sentiment analysis, this does not seem to help. Using a trans- lated sentiment lexicon, even if it is small, gives better results. <ref type="bibr">9</ref> http://www.meta-share.org <ref type="bibr">10</ref> The translation took approximately one hour. We can extrapolate that hand translating a sentiment lexicon the size of the Bing Liu lexicon would take no more than 5 hours. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Analysis of M</head><p>The main motivation for using two projection ma- trices M and M is to allow the original embed- dings to remain stable, while the projection ma- trices have the flexibility to align translations and separate these into distinct sentiment subspaces. To justify this design decision empirically, we perform an experiment to evaluate the actual need for the target language projection matrix M : We create a simplified version of our model without M , using M to project from the source to target and then P to classify sentiment.</p><p>The results of this model are shown in <ref type="figure" target="#fig_3">Figure 5</ref>. The modified model does learn to predict in the source language, but not in the target language. This confirms that M is necessary to transfer sen- timent in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Qualitative Analyses of Joint Bilingual Sentiment Space</head><p>In order to understand how well our model trans- fers sentiment information to the target language, we perform two qualitative analyses. First, we collect two sets of 100 positive sentiment words and one set of 100 negative sentiment words. An effective cross-lingual sentiment classifier using embeddings should learn that two positive words should be closer in the shared bilingual space than a positive word and a negative word. We test if BLSE is able to do this by training our model and after every epoch observing the mean cosine similarity between the sentiment synonyms and sentiment antonyms after projecting to the joint space. We compare BLSE with ARTETXE and BARISTA by replacing the Linear SVM classifiers with the same multi-layer classifier used in BLSE and ob- serving the distances in the hidden layer. <ref type="figure">Figure 4</ref> shows this similarity in both source and target lan- guage, along with the mean cosine similarity be- tween a held-out set of translation pairs and the macro F 1 scores on the development set for both source and target languages for BLSE, BARISTA, and ARTETXE. From this plot, it is clear that BLSE is able to learn that sentiment synonyms should be close to one another in vector space and antonyms should have a negative cosine similarity. While the other models also learn this to some degree, jointly optimizing both sentiment and projection gives better results.</p><p>Secondly, we would like to know how well the projected vectors compare to the original space. Our hypothesis is that some relatedness and simi- larity information is lost during projection. There- fore, we visualize six categories of words in t-SNE (Van der Maaten and Hinton, 2008): positive senti- ment words, negative sentiment words, functional words, verbs, animals, and transport.</p><p>The t-SNE plots in <ref type="figure" target="#fig_4">Figure 6</ref> show that the posi- tive and negative sentiment words are rather clearly separated after projection in BLSE. This indicates that we are able to incorporate sentiment informa- tion into our target language without any labeled data in the target language. However, the downside </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We have presented a new model, BLSE, which is able to leverage sentiment information from a resource-rich language to perform sentiment analy- sis on a resource-poor target language. This model requires less parallel data than MT and performs better than other state-of-the-art methods with sim- ilar data requirements, an average of 14 percentage points in F 1 on binary and 4 pp on 4-class cross- lingual sentiment analysis. We have also performed a phenomena-driven error analysis which showed that BLSE is better than ARTETXE and BARISTA at transferring sentiment, but assigns too much sen- timent to functional words. In the future, we will extend our model so that it can project multi-word phrases, as well as single words, which could help with negations and modifiers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Binary and four class macro F 1 on Spanish (ES), Catalan (CA), and Basque (EU).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Macro F 1 for translation pairs in the Spanish 4-class setup.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: BLSE model (solid lines) compared to a variant without target language projection matrix M (dashed lines). "Translation" lines show the average cosine similarity between translation pairs. The remaining lines show F 1 scores for the source and target language with both varints of BLSE. The modified model cannot learn to predict sentiment in the target language (red lines). This illustrates the need for the second projection matrix M .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: t-SNE-based visualization of the Spanish vector space before and after projection with BLSE. There is a clear separation of positive and negative words after projection, despite the fact that we have used no labeled data in Spanish.</figDesc><graphic url="image-19.png" coords="9,318.19,181.43,193.60,116.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Statistics for the Wikipedia corpora and 
monolingual vector spaces. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>. 9 . BARISTA</head><label>9</label><figDesc></figDesc><table>Model 
voc 
mod 
neg 
know 
other 
total 

MT 

bi 
49 
26 19 14 
5 113 
4 147 
94 19 21 12 293 

ARTETXE 

bi 
80 
44 27 14 
7 172 
4 182 141 19 24 19 385 

BARISTA 

bi 
89 
41 27 20 
7 184 
4 191 109 24 31 15 370 

BLSE 

bi 
67 
45 21 15 
8 156 
4 146 125 29 22 19 341 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Error analysis for different phenomena. See text for explanation of error classes.</figDesc><table></table></figure>

			<note place="foot" n="1"> We omit parameters in equations for better readability. 2 We also experimented with cosine distance, but found that it performed worse than Euclidean distance. 3 Our model employs a linear transformation after the averaging layer instead of including a non-linearity function. We choose this architecture because the weights M and M are also used to learn a linear cross-lingual projection.</note>

			<note place="foot" n="4"> https://code.google.com/archive/p/word2vec/ 5 http://attardi.github.io/wikiextractor/ 6 Note that we only do that for convenience. Using a machine translation service to generate this list could easily be replaced by a manual translation, as the lexicon is comparably small.</note>

			<note place="foot" n="7"> LinearSVC implementation from scikit-learn. 8 https://translate.google.com</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Sebastian Padó, Sebastian Riedel, Eneko Agirre, and Mikel Artetxe for their conversations and feedback.</p></div>
			</div>

			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ixa pipeline: Efficient and ready to use multilingual nlp tools</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Agerri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josu</forename><surname>Bermudez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Rigau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)</title>
		<meeting>the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3823" to="3828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">OpeNER: Open polarity enhanced named entity recognition. Sociedad Española para el Procesamiento del</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Agerri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Montse</forename><surname>Cuadros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Gaines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Rigau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lenguaje Natural</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="215" to="218" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Aligning opinions: Cross-lingual opinion mining with dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Mariana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helena</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Figueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">André</forename><forename type="middle">F T</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="408" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning principled bilingual mappings of word embeddings while preserving monolingual invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2289" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning bilingual word embeddings with (almost) no bilingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="451" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Comparative experiments using supervised learning and machine translation for multilingual sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Balahur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Turchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="56" to="75" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multilingual subjectivity: Are more languages better?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="28" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multilingual subjectivity analysis using machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samer</forename><surname>Hassan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2008 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="127" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multibooked: A corpus of basque and catalan hotel reviews annotated for aspect-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrik</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toni</forename><surname>Badia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 11th Language Resources and Evaluation Conference (LREC&apos;18)</title>
		<meeting>11th Language Resources and Evaluation Conference (LREC&apos;18)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">An autoencoder approach to learning bilingual word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarath</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislas</forename><surname>Lauly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitesh</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaraman</forename><surname>Ravindran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vikas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amrita</forename><surname>Raykar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saha</surname></persName>
		</author>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1853" to="1861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adversarial deep averaging networks for cross-lingual sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cardie</surname></persName>
		</author>
		<idno>CoRR abs/1606.01614</idno>
		<ptr target="http://arxiv.org/abs/1606.01614" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Crosslingual polarity detection with machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erkin</forename><surname>Demirtas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykola</forename><surname>Pechenizkiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Issues of Sentiment Discovery and Opinion Mining-WISDOM &apos;13 pages</title>
		<meeting>the International Workshop on Issues of Sentiment Discovery and Opinion Mining-WISDOM &apos;13 pages</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Is machine translation ripe for cross-lingual sentiment classification?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akinori</forename><surname>Fujino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="429" to="433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">BilBOWA: Fast bilingual distributed representations without word alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 32nd International Conference on Machine Learning</title>
		<meeting>The 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="748" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Simple task-specific bilingual word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1386" to="1390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multilingual models for compositional distributed semantics</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<editor>Karl Moritz Hermann and Phil Blunsom</editor>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="58" to="68" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mining opinion features in customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep unordered composition rivals syntactic methods for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Manjunatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1681" to="1691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations (ICLR)</title>
		<meeting>the 3rd International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hubness and pollution: delving into cross-space mapping for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="270" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cross-lingual mixture model for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinfan</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P12-1060" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="572" to="581" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning multilingual subjective language via cross-lingual projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="976" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Exploiting similarities among languages for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<idno>CoRR abs/1309.4168</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Nrc-canada: Building the state-ofthe-art in sentiment analysis of tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh international workshop on Semantic Evaluation Exercises</title>
		<meeting>the seventh international workshop on Semantic Evaluation Exercises</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>SemEval-2013</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Thumbs up? sentiment classification using machine learning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivakumar</forename><surname>Vaithyanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-02 Conference on Empirical methods in natural language processing</title>
		<meeting>the ACL-02 Conference on Empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Soumith Chintala, and Gregory Chanan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<ptr target="http://pytorch.org" />
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
	<note>Pytorch deeplearning framework</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Crosslingual adaptation using structural correspondence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Cross-lingual sentiment transfer with limited resources. Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Sadegh Rasooli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noura</forename><surname>Farra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axinia</forename><surname>Radeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Negation scope detection for twitter sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Reitan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jørgen</forename><surname>Faret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Gambäck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Bungum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</title>
		<meeting>the 6th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="99" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning sentiment-specific word embedding for twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1555" to="1565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
