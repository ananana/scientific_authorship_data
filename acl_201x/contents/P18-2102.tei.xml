<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:19+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Extracting Commonsense Properties from Embeddings with Limited Human Guidance</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiben</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">Northwestern University</orgName>
								<address>
									<postCode>60208</postCode>
									<settlement>Evanston</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Birnbaum</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical Engineering &amp; Computer Science</orgName>
								<orgName type="institution">Northwestern University</orgName>
								<address>
									<postCode>60208</postCode>
									<settlement>Evanston</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji-Ping</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">Northwestern University</orgName>
								<address>
									<postCode>60208</postCode>
									<settlement>Evanston</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Downey</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical Engineering &amp; Computer Science</orgName>
								<orgName type="institution">Northwestern University</orgName>
								<address>
									<postCode>60208</postCode>
									<settlement>Evanston</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Extracting Commonsense Properties from Embeddings with Limited Human Guidance</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="644" to="649"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>644</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Intelligent systems require common sense, but automatically extracting this knowledge from text can be difficult. We propose and assess methods for extracting one type of commonsense knowledge, object-property comparisons, from pre-trained embeddings. In experiments, we show that our approach exceeds the accuracy of previous work but requires substantially less hand-annotated knowledge. Further, we show that an active learning approach that synthesizes common-sense queries can boost accuracy.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatically extracting common sense from text is a long-standing challenge in natural language processing <ref type="bibr" target="#b10">(Schubert, 2002</ref>; Van Durme and <ref type="bibr" target="#b12">Schubert, 2008;</ref><ref type="bibr" target="#b13">Vanderwende, 2005)</ref>. As argued by <ref type="bibr" target="#b3">Forbes and Yejin (2017)</ref>, typical language use may reflect common sense, but the commonsense knowledge itself is not often explicitly stated, due to reporting bias <ref type="bibr" target="#b4">(Gordon and Van Durme, 2013</ref>). Thus, additional human knowledge or annotated training data are often used to help systems learn common sense.</p><p>In this paper, we study methods for reducing the amount of human input needed to learn com- mon sense. Specifically, we focus on learning relative comparisons of (one-dimensional) object properties, such as the fact that a cantaloupe is more round than a hammer. Methods for learn- ing this kind of common sense have been devel- oped previously (e.g. <ref type="bibr" target="#b3">Forbes and Choi, 2017)</ref>, but the best-performing methods in that previous work requires dozens of manually-annotated frames for each comparison property, to connect the property to how it is indirectly reflected in text-e.g., if text asserts that "x carries y," this implies that x is probably larger than y.</p><p>Our architecture for relative comparisons fol- lows the zero-shot learning paradigm ( <ref type="bibr" target="#b8">Palatucci et al., 2009)</ref>. It takes the form of a neural network that compares a projection of embeddings for each of two objects (e.g. "elephant" and "tiger") to the embeddings for the two poles of the target dimen- sion of comparison (e.g., "big" and "small" for the size property). The projected object embeddings are trained to be closer to the appropriate pole, us- ing a small training set of hand-labeled compar- isons. Our experiments reveal that our architec- ture outperforms previous work, despite using less annotated data. Further, because our architecture takes the property (pole) labels as arguments, it can extend to the zero-shot setting in which we evaluate on properties not seen in training. We find that in zero-shot, our approach outperforms baselines and comes close to supervised results, but providing labels for both poles of the relation rather than just one is important. Finally, because the number of properties we wish to learn is large, we experiment with active learning (AL) over a larger property space. We show that synthesizing AL queries can be effective using an approach that explicitly models which comparison questions are nonsensical (e.g., is Batman taller than Democ- racy?). We release our code base and a new com- monsense data set to the research community. <ref type="bibr">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Definition and Methods</head><p>We define the task of comparing object properties in two different ways: a three-way classification task, and a four-way classification task. In the three-way classification task, we want to estimate the following conditional probability: For example, P rob(An elephant is larger than a dog) can be expressed as P (L = &gt; |O 1 = "elephant", O 2 = "dog", Property = "size"). The three-way classification task has been ex- plored in previous work <ref type="bibr" target="#b3">(Forbes and Choi, 2017)</ref> and is only performed on triples where both ob- jects have the property, so that the comparison is meaningful. In applications, however, we may not know in advance which comparisons are meaning- ful. Thus, we also define a four-way classification task to include "not applicable" as the fourth label, so that inference can be performed on any object- property triples. In the four-way task, the system is tasked with identifying the nonsensical compar- isons. Formally, we want to estimate the following conditional probability:</p><formula xml:id="formula_0">P (L|O 1 , O 2 , Property), L∈{ &lt; , &gt; , ≈ , N/A }.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Three-way Model</head><p>For each comparison property, we pick an adjec- tive and its antonym to represent the { &lt; , &gt; } labels. For example, for the property size, we pick "big" and "small". The adjective "similar" serves as the label for ≈ for all properties. Under this framework, a relative comparison question, for in- stance, "Is a dog bigger than an elephant?", can be formulated as a quintuple query to the model, namely {dog, elephant, small, similar, big}. De- noting the word embeddings for tokens in a quin- tuple query as X, Y , R &lt; , R ≈ , R &gt; , our three-way model is defined as follows:</p><formula xml:id="formula_1">P (L = s|Q) = sof tmax(R s · σ((X ⊕ Y )W )),</formula><p>for s ∈ {&lt;, &gt;, ≈}, where Q is an quintuple query, σ(·) is an activation function and W is a learnable weight matrix. The symbol ⊕ represents concatenation. We refer to this method as PCE (Property Comparison from Embeddings) for the 3-way task. We also experiment with generat- ing label representations from just a single ad- jective (property) embedding R &lt; , namely</p><formula xml:id="formula_2">R ≈ = σ(R &lt; W 2 ), R &gt; = σ(R &lt; W 3 )</formula><p>. We refer to this simpler method as PCE(one-pole).</p><p>We note that in both the three-and four-way settings, the question "A&gt;B?" is equivalent to "B&lt;A?". We leverage this fact at test time by feeding our network a reversed object pair, and taking the average of the aligned network outputs before the softmax layer to reduce prediction vari- ance. We refer to our model without this technique as PCE(no reverse).</p><p>The key distinction of our method is that it learns a projection from the object word embed- ding space to the label embedding space. This allows the model to leverage the property label embeddings to perform zero-shot prediction on properties not observed in training. For example, from a training example "dogs are smaller than elephants", the model will learn a projection that puts "dogs" relatively closer to "small," and far from "big" and "similar." Doing so may also re- sult in projecting "dog" to be closer to "light" than to "heavy," such that the model is able to predict "dogs are lighter than elephants" despite never be- ing trained on any weight comparison examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Four-way Model</head><p>Our four-way model is the same as our three-way model, with an additional module to learn whether the comparison is applicable. Keeping the other output nodes unchanged, we add an additional component into the softmax layer to output the probability of "N/A":</p><formula xml:id="formula_3">h x = σ(XW a ), h y = σ(Y W a ), A i = h i · R &gt; + h i · R &lt; , P (L = N/A |Q) ∝ exp(A x + A y ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Synthesis for Active Learning</head><p>We propose a method to synthesize informative queries to pose to annotators, a form of active learning <ref type="bibr" target="#b11">(Settles, 2009)</ref>. We use the common heuristic that an informative training example will have a high uncertainty in the model's predictive distribution. We adopt the confidence measure <ref type="bibr" target="#b2">(Culotta and McCallum, 2005</ref>) to access the un- certainty of a given example:</p><formula xml:id="formula_4">U ncertainty(x) = 1 − max y P (y|x, D train ).</formula><p>Good candidates for acquisition should have high uncertainty measure, but we also want to avoid querying outliers. As the vocabulary is fi- nite, it is possible to evaluate the uncertainty mea- sures for all possible inputs to synthesize the most uncertain query. However, such a greedy policy is expensive and prone to selecting outliers. Hence, we adopt a sampling based synthesis strategy: at each round, we generate one random object pair per property, and query the one that achieves the highest uncertainty measure.</p><p>A classical difficulty faced by synthesis ap- proaches to active learning is that they may pro-duce unnatural queries that are difficult for a hu- man to label <ref type="bibr" target="#b0">(Baum and Lang, 1992</ref>). However, our task formulation includes "similar" and "N/A" classes that encompass many of the more difficult or confusing comparisons, which we believe aids the effectiveness of the synthesis approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We now present our experimental results on both the three-way and four-way tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Sets</head><p>We test our three-way model on the VERB PHYSICS data set from <ref type="bibr" target="#b3">(Forbes and Choi, 2017)</ref>. As there are only 5 properties in VERB PHYSICS, we also develop a new data set we call PROP- ERTY COMMON SENSE. We select 32 com- monsense properties to form our property set (e.g., value, roundness, deliciousness, intelligence, etc.). We extract object nouns from the McRae Feature Norms dataset <ref type="bibr" target="#b6">(McRae et al., 2005</ref>) and add se- lected named entities to form a object vocabulary of 689 distinct objects. We randomly generate 3148 object-property triples, label them and re- serve 45% of the data for the test set. We fur- ther add 5 manually-selected applicable compar- ison examples per property to our test set, in order to make sure each property has some applicable testing examples. To verify the labeling, we have a second annotator redundantly label 200 examples and find a Cohen's Kappa of 0.64, which indicates good annotator agreement (we analyze the source of the disagreements in Section 4.1). The training set is used for the passive learning and pool-based active learning, and a human oracle provides la- bels in the synthesis active learning setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Setup</head><p>We experiment with three types of embeddings: GloVe, normalized 300-dimensional embeddings trained on a corpus of 6B tokens <ref type="bibr" target="#b9">(Pennington et al., 2014</ref>) (the F&amp;C method (Forbes and Choi, 2017) uses the 100-dimensional version, as it achieves the highest validation accuracy for their methods); Word2vec, normalized 300- dimensional embeddings trained on 100B tokens ( <ref type="bibr" target="#b7">Mikolov et al., 2013)</ref>; and LSTM, the normalized 1024-dimensional weight matrix from the softmax layer of the Google 1B LSTM language model ( <ref type="bibr" target="#b5">Jozefowicz et al., 2016</ref>).</p><p>For training PCE, we use an identity activa- tion function and apply 50% dropout. We use the Adam optimizer with default settings to train the models for 800 epochs, minimizing cross entropy loss. For zero-shot learning, we adopt a hold-one- property-out scheme to test our models' zero-shot performance.</p><p>Finally, for active learning, we use Word2vec embeddings. All the models are trained on 200 random training examples to warm up. We train for 20 epochs after each label acquisition. To smooth noise, we report the average of 20 differ- ent runs of random (passive learning) and least confident (LC) pool-based active learning <ref type="bibr" target="#b2">(Culotta and McCallum, 2005</ref>) baselines. We report the average of only 6 runs for an expected model change (EMC) pool-based active learning <ref type="bibr" target="#b1">(Cai et al., 2013</ref>) baseline due to its high computational cost, and of only 2 runs for our synthesis active learning approach due to its high labeling cost. The pool size is 1540 examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>In <ref type="table">Table 1</ref>, we compare the performance of the three-way PCE model against the existing state of the art on the VERB PHYSICS data set. The use of LSTM embeddings in PCE yields the best accuracy for all properties. Across all embed- ding choices, PCE performs as well or better than F&amp;C, despite the fact that PCE does not use the annotated frames that F&amp;C requires (approx- imately 188 labels per property). Thus, our ap- proach matches or exceeds the performance of previous work using significantly less annotated knowledge. The lower performance of "no re- verse" shows that the simple method of averaging over the reversed object pair is effective. <ref type="table" target="#tab_1">Table 2</ref> evaluates our models on properties not seen in training (zero-shot learning). We compare against a random baseline, and an Emb-Similarity baseline that classifies based on the cosine simi- larity of the object embeddings to the pole label embeddings (i.e., without the projection layer in PCE). PCE outperforms the baselines. Although the one-pole method was shown to perform simi- larly to the two-pole method for properties seen in training <ref type="table">(Table 1)</ref>, we see that for zero-shot learn- ing, using two poles is important.</p><p>In <ref type="table" target="#tab_2">Table 3</ref>, we show that our four-way mod- els with different embeddings beat both the ma- jority and random baselines on the PROPERTY   COMMON SENSE data. Here, the LSTM em- beddings perform similarly to the Word2vec em- beddings, perhaps because the PROPERTY COM- MON SENSE vocabulary consists of less fre- quent nouns than in VERB PHYSICS. Thus, the Word2vec embeddings are able to catch up due to their larger vocabulary and much larger training corpus. Finally, in <ref type="figure" target="#fig_0">Figure 1</ref>, we evaluate in the active learning setting. The synthesis approach performs best, especially later in training when the train- ing pool for the pool-based methods has only un- informative examples remaining. <ref type="figure" target="#fig_1">Figure 2</ref> helps explain the relative advantage of the synthesis ap- proach: it is able to continue synthesizing infor- mative (uncertain) queries throughout the entire training run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Sources of annotator disagreement</head><p>As noted above, we found a "good" level of agree- ment (Cohen's Kappa of 0.64) for our PROPERTY COMMON SENSE data, which is lower than one might expect for task aimed at common sense. We analyzed the disagreements and found that they stem from two sources of subjectivity in the task. The first is that different labelers may have differ- ent thresholds for what counts as similar-a spi- der and an ant might be marked similar in size for one labeler, but not for another labeler. In our data, 58% of the disagreements are cases in which one annotator marks similar while the other says not similar. The second is that different la- belers have different standards for whether a com- parison is N/A. For example, in our data set, one labeler labels that a toaster is physically stronger than alcohol, and the other labeler says the com- parison is N/A. 37% of our disagreements are due to this type of subjectivity. The above two types of subjectivity account for almost all disagreements (95%), and the remaining 5% are due to annota- tion errors (one of the annotators makes mistake).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model Interpretation</head><p>Since we adopt an identity activation function and a single layer design, it is possible to simplify the mathematical expression of our model to make it more interpretable. After accounting for model averaging, we have the following equality:</p><formula xml:id="formula_5">P (L =&lt; |Q) ∝ exp(R &lt; · ((X ⊕ Y )W ) + R &gt; · ((Y ⊕ X)W )) = exp(R T &lt; (XW 1 + Y W 2 ) + R T &gt; (Y W 1 + XW 2 )) ∝ exp((R &lt; − R &gt; ) T (XW 1 + XW 2 )),</formula><p>where W = W 1 ⊕ W 2 . So we can define a score of "R &lt; " for a object with embedding X as the fol- lowing:</p><formula xml:id="formula_6">score(X, R &lt; ) = (R &lt; − R &gt; ) T (XW 1 + XW 2 ).</formula><p>An object with a higher score for R &lt; is more asso- ciated with the R &lt; pole than the R &gt; one. For ex- ample, score("elephant","small") represents how small an elephant is-a larger score indicates a smaller object. <ref type="table" target="#tab_4">Table 4</ref> shows smallness scores for 5 randomly picked objects from the VERB PHYSICS data set. PCE tends to assign higher scores to the smaller objects in the set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Sensitivity to pole labels</head><p>PCE requires labels for the poles of the target ob- ject property.   <ref type="table" target="#tab_3">Table 5</ref>: Trained and zero-shot accuracies for dif- ferent word choices analysis to pole labels, evaluating the test accu- racy of PCE as the pole label varies among dif- ferent combinations of synonyms for the size and speed relations. We evaluate in both the trained setting (comparable to the results in <ref type="table">Table 1</ref>) and the zero-shot setting (comparable to <ref type="table" target="#tab_1">Table 2</ref>). We see that the trained accuracy remains essentially unchanged for different pole labels. In the zero- shot setting, all combinations achieve accuracy that beats the baselines in <ref type="table" target="#tab_1">Table 2</ref>, but the accuracy value is somewhat sensitive to the choice of pole label. Exploring how to select pole labels and ex- perimenting with richer pole representations such as textual definitions are items of future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we presented a method for extracting commonsense knowledge from embeddings. Our experiments demonstrate that the approach is ef- fective at performing relative comparisons of ob- ject properties using less hand-annotated knowl- edge than in previous work. A synthesis active learner was found to boost accuracy, and further experiments with this approach are an item of fu- ture work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Test accuracy as a function of the number of queried training examples. The synthesis approach performs best.</figDesc><graphic url="image-1.png" coords="4,309.96,298.30,229.12,171.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The uncertainty measure of each queried training example. As training proceeds, the synthesis approach continues to select more uncertain examples.</figDesc><graphic url="image-2.png" coords="5,68.18,44.41,236.58,177.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Accuracy of zero-shot learning on the VERB PHYSICS data set(using LSTM embeddings). 
PCE outperforms the baselines, and using both poles is important for accuracy. 

Model 
Test 
Random 
0.25 
Majority Class 
0.51 
PCE(GloVe) 
0.63 
PCE(Word2vec) 0.67 
PCE(LSTM) 
0.67 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Accuracy on the four-way task on the 
PROPERTY COMMON SENSE data. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 5 presents a limited sensitivity</head><label>5</label><figDesc></figDesc><table>Object 
Smallness 
restaurant 
0.077 
gully 
0.416 
lung 
1.182 
bow 
4.036 
scissors 
14.492 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 4 : Scores of smallness for 5 randomly picked objects in VERB PHYSICS data set</head><label>4</label><figDesc></figDesc><table>Word choice 
Trained Zero 
fast vs. slow 
0.71 
0.58 
speedy vs. slow 
0.71 
0.56 
fast vs. plodding 
0.72 
0.48 
speedy vs. plodding 0.72 
0.51 
big vs. small 
0.80 
0.74 
large vs. small 
0.80 
0.76 
big vs. little 
0.80 
0.71 
large vs. little 
0.80 
0.69 

</table></figure>

			<note place="foot" n="1"> https://github.com/yangyiben/PCE</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported in part by NSF Grant IIS-1351029. We thank the anonymous reviewers for helpful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Query learning can work poorly when a human oracle is used</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Baum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International joint conference on neural networks</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Maximizing expected model change for active learning in regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE 13th International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="51" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reducing labeling effort for structured prediction tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aron</forename><surname>Culotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<publisher>AAAI Press / The MIT Press</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03799</idno>
		<title level="m">Verb physics: Relative physical knowledge of actions and objects</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Reporting bias and knowledge acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 workshop on Automated knowledge base construction</title>
		<meeting>the 2013 workshop on Automated knowledge base construction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="25" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Exploring the limits of language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02410</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semantic feature production norms for a large set of living and nonliving things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Mcrae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">S</forename><surname>Cree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">S</forename><surname>Seidenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Mcnorgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="547" to="559" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Zero-shot learning with semantic output codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Palatucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><surname>Pomerleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Y. Bengio, D. Schuurmans, J. D. Lafferty, C. K. I. Williams, and A. Culotta</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1410" to="1418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Can we derive general world knowledge from texts?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lenhart</forename><surname>Schubert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the second international conference on Human Language Technology Research</title>
		<meeting>the second international conference on Human Language Technology Research</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="94" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Active learning literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
		<idno>1648</idno>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin-Madison</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Computer Sciences Technical Report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Open knowledge extraction through compositional language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lenhart</forename><surname>Schubert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Conference on Semantics in Text Processing</title>
		<meeting>the 2008 Conference on Semantics in Text Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="239" to="254" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Volunteers created the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Spring Symposium: Knowledge Collection from Volunteer Contributors</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="84" to="90" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
