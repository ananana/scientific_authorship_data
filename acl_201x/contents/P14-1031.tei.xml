<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:31+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Context-aware Learning for Sentence-level Sentiment Analysis with Posterior Regularization</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
							<email>bishan@cs.cornell.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">Cornell University</orgName>
								<orgName type="institution" key="instit2">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
							<email>cardie@cs.cornell.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">Cornell University</orgName>
								<orgName type="institution" key="instit2">Cornell University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Context-aware Learning for Sentence-level Sentiment Analysis with Posterior Regularization</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="325" to="335"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper proposes a novel context-aware method for analyzing sentiment at the level of individual sentences. Most existing machine learning approaches suffer from limitations in the modeling of complex linguistic structures across sentences and often fail to capture non-local contextual cues that are important for sentiment interpretation. In contrast, our approach allows structured modeling of sentiment while taking into account both local and global contextual information. Specifically, we encode intuitive lexical and discourse knowledge as expressive constraints and integrate them into the learning of conditional random field models via posterior regularization. The context-aware constraints provide additional power to the CRF model and can guide semi-supervised learning when labeled data is limited. Experiments on standard product review datasets show that our method outperforms the state-of-the-art methods in both the supervised and semi-supervised settings.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ability to extract sentiment from text is cru- cial for many opinion-mining applications such as opinion summarization, opinion question answer- ing and opinion retrieval. Accordingly, extract- ing sentiment at the fine-grained level (e.g. at the sentence-or phrase-level) has received increasing attention recently due to its challenging nature and its importance in supporting these opinion analysis tasks <ref type="bibr" target="#b17">(Pang and Lee, 2008)</ref>.</p><p>In this paper, we focus on the task of sentence- level sentiment classification in online reviews. Typical approaches to the task employ supervised machine learning algorithms with rich features and take into account the interactions between words to handle compositional effects such as po- larity reversal (e.g.</p><p>( <ref type="bibr" target="#b15">Nakagawa et al., 2010;</ref><ref type="bibr" target="#b22">Socher et al., 2013)</ref>). Still, their methods can en- counter difficulty when the sentence on its own does not contain strong enough sentiment signals (due to the lack of statistical evidence or the re- quirement for background knowledge). Consider the following review for example, <ref type="bibr">1</ref>. Hearing the music in real stereo is a true reve- lation. 2. You can feel that the music is no longer constrained by the mono recording. 3. In fact, it is more like the players are performing on a stage in front of you ...</p><p>Existing feature-based classifiers may be effective in identifying the positive sentiment of the first sentence due to the use of the word revelation, but they could be less effective in the last two sen- tences due to the lack of explicit sentiment signals. However, if we examine these sentences within the discourse context, we can see that: the second sen- tence expresses sentiment towards the same aspect -the music -as the first sentence; the third sen- tence expands the second sentence with the dis- course connective In fact. These discourse-level relations help indicate that sentence 2 and 3 are likely to have positive sentiment as well.</p><p>The importance of discourse for sentiment anal- ysis has become increasingly recognized. Most existing work considers discourse relations be- tween adjacent sentences or clauses and incor- porates them as constraints <ref type="bibr" target="#b10">(Kanayama and Nasukawa, 2006</ref>; Zhou et al., 2011) or features in classifiers <ref type="bibr" target="#b27">Trivedi and Eisenstein (2013;</ref><ref type="bibr" target="#b11">Lazaridou et al. (2013)</ref>. Very little work has explored long-distance discourse relations for sentiment analysis. <ref type="bibr" target="#b23">Somasundaran et al. (2008)</ref> defines coreference relations on opinion targets and ap- plies them to constrain the polarity of sentences.</p><p>However, the discourse relations were obtained from fine-grained annotations and implemented as hard constraints on polarity.</p><p>Obtaining sentiment labels at the fine-grained level is costly. Semi-supervised techniques have been proposed for sentence-level sentiment classi- fication <ref type="bibr" target="#b25">(Täckström and McDonald, 2011a;</ref><ref type="bibr" target="#b20">Qu et al., 2012</ref>). However, they rely on a large amount of document-level sentiment labels that may not be naturally available in many domains.</p><p>In this paper, we propose a sentence-level senti- ment classification method that can (1) incorporate rich discourse information at both local and global levels; (2) encode discourse knowledge as soft constraints during learning; (3) make use of un- labeled data to enhance learning. Specifically, we use the Conditional Random Field (CRF) model as the learner for sentence-level sentiment classi- fication, and incorporate rich discourse and lexi- cal knowledge as soft constraints into the learn- ing of CRF parameters via Posterior Regulariza- tion (PR) ( <ref type="bibr" target="#b6">Ganchev et al., 2010)</ref>. As a framework for structured learning with constraints, PR has been successfully applied to many structural NLP tasks ( <ref type="bibr" target="#b5">Ganchev et al., 2009;</ref><ref type="bibr" target="#b6">Ganchev et al., 2010;</ref><ref type="bibr" target="#b4">Ganchev and Das, 2013)</ref>. Our work is the first to explore PR for sentiment analysis. Unlike most previous work, we explore a rich set of structural constraints that cannot be naturally encoded in the feature-label form, and show that such constraints can improve the performance of the CRF model. We evaluate our approach on the sentence- level sentiment classification task using two stan- dard product review datasets. Experimental re- sults show that our model outperforms state-of- the-art methods in both the supervised and semi- supervised settings. We also show that dis- course knowledge is highly useful for improving sentence-level sentiment classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There has been a large amount of work on sen- timent analysis at various levels of granular- ity ( <ref type="bibr" target="#b17">Pang and Lee, 2008)</ref>. In this paper, we focus on the study of sentence-level sentiment classifi- cation. Existing machine learning approaches for the task can be classified based on the use of two ideas. The first idea is to exploit sentiment sig- nals at the sentence level by learning the relevance of sentiment and words while taking into account the context in which they occur: <ref type="bibr" target="#b15">Nakagawa et al. (2010)</ref> uses tree-CRF to model word interac- tions based on dependency tree structures; <ref type="bibr" target="#b1">Choi and Cardie (2008)</ref> applies compositional inference rules to handle polarity reversal; <ref type="bibr" target="#b21">Socher et al. (2011) and</ref><ref type="bibr" target="#b22">Socher et al. (2013)</ref> compute composi- tional vector representations for words and phrases and use them as features in a classifier.</p><p>The second idea is to exploit sentiment signals at the inter-sentential level. <ref type="bibr" target="#b18">Polanyi and Zaenen (2006)</ref> argue that discourse structure is important in polarity classification. Various attempts have been made to incorporate discourse relations into sentiment analysis: <ref type="bibr" target="#b16">Pang and Lee (2004)</ref> explored the consistency of subjectivity between neighbor- ing sentences; <ref type="bibr" target="#b13">Mao and Lebanon (2007)</ref>, <ref type="bibr" target="#b14">McDonald et al. (2007), and</ref><ref type="bibr" target="#b25">McDonald (2011a)</ref> developed structured learning models to capture sentiment dependencies between adjacent sentences; <ref type="bibr" target="#b10">Kanayama and Nasukawa (2006)</ref> and <ref type="bibr" target="#b30">Zhou et al. (2011)</ref> use discourse relations to con- strain two text segments to have either the same polarity or opposite polarities; <ref type="bibr" target="#b27">Trivedi and Eisenstein (2013)</ref> and <ref type="bibr" target="#b11">Lazaridou et al. (2013)</ref> encode the discourse connectors as model features in su- pervised classifiers. Very little work has explored long-distance discourse relations. <ref type="bibr" target="#b23">Somasundaran et al. (2008)</ref> define opinion target relations and ap- ply them to constrain the polarity of text segments annotated with target relations. Recently, <ref type="bibr" target="#b29">Zhang et al. (2013)</ref> explored the use of explanatory dis- course relations as soft constraints in a Markov Logic Network framework for extracting subjec- tive text segments.</p><p>Leveraging both ideas, our approach exploits sentiment signals from both intra-sentential and inter-sentential context. It has the advantages of utilizing rich discourse knowledge at different lev- els of context and encoding it as soft constraints during learning.</p><p>Our approach is also semi-supervised. Com- pared to the existing work on semi-supervised learning for sentence-level sentiment classification <ref type="bibr" target="#b25">(Täckström and McDonald, 2011a;</ref><ref type="bibr" target="#b26">Täckström and McDonald, 2011b;</ref><ref type="bibr" target="#b20">Qu et al., 2012)</ref>, our work does not rely on a large amount of coarse-grained (document-level) labeled data, instead, distant supervision mainly comes from linguistically- motivated constraints.</p><p>Our work also relates to the study of posterior regularization (PR) ( <ref type="bibr" target="#b6">Ganchev et al., 2010)</ref>. PR has been successfully applied to many structured NLP tasks such as dependency parsing, information ex- traction and cross-lingual learning tasks ( <ref type="bibr" target="#b5">Ganchev et al., 2009;</ref><ref type="bibr" target="#b0">Bellare et al., 2009;</ref><ref type="bibr" target="#b6">Ganchev et al., 2010;</ref><ref type="bibr" target="#b4">Ganchev and Das, 2013)</ref>. Most previous work using PR mainly experiments with feature- label constraints. In contrast, we explore a rich set of linguistically-motivated constraints which cannot be naturally formulated in the feature-label form. We also show that constraints derived from the discourse context can be highly useful for dis- ambiguating sentence-level sentiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>In this section, we present the details of our pro- posed approach. We formulate the sentence-level sentiment classification task as a sequence label- ing problem. The inputs to the model are sentence- segmented documents annotated with sentence- level sentiment labels (positive, negative or neu- tral) along with a set of unlabeled documents. During prediction, the model outputs sentiment la- bels for a sequence of sentences in the test docu- ment. We utilize conditional random fields and use Posterior Regularization (PR) to learn their param- eters with a rich set of context-aware constraints.</p><p>In what follows, we first briefly describe the framework of Posterior Regularization. Then we introduce the context-aware constraints derived based on intuitive discourse and lexical knowl- edge. Finally we describe how to perform learning and inference with these constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Posterior Regularization</head><p>PR is a framework for structured learning with constraints ( <ref type="bibr" target="#b6">Ganchev et al., 2010)</ref>. In this work, we apply PR in the context of CRFs for sentence- level sentiment classification.</p><p>Denote x as a sequence of sentences within a document and y as a vector of sentiment labels associated with x. The CRF model the following conditional probabilities:</p><formula xml:id="formula_0">p θ (y|x) = exp(θ · f (x, y)) Z θ (x)</formula><p>where f (x, y) are the model features, θ are the model parameters, and Z θ (x) = y exp(θ · f (x, y)) is a normalization constant. The objec- tive function for a standard CRF is to maximize the log-likelihood over a collection of labeled doc- uments plus a regularization term:</p><formula xml:id="formula_1">max θ L(θ) = max θ (x,y) log p θ (y|x) − ||θ|| 2 2 2δ 2</formula><p>PR makes the assumption that the labeled data we have is not enough for learning good model parameters, but we have a set of constraints on the posterior distribution of the labels. We can define the set of desirable posterior distrbutions as</p><formula xml:id="formula_2">Q = {q(Y) : E q [φ(X, Y)] = b} (1)</formula><p>where φ is a constraint function, b is a vector of desired values of the expectations of the constraint functions under the distribution q 1 . Note that the distribution q is defined over a collection of un- labeled documents where the constraint functions apply, and we assume independence between doc- uments.</p><p>The PR objective can be written as the origi- nal model objective penalized with a regulariza- tion term, which minimizes the KL-divergence be- tween the desired model posteriors and the learned model posteriors with an L2 penalty 2 for the con- straint violations.</p><formula xml:id="formula_3">max θ L(θ) − min q∈Q {KL(q(Y)||p θ (Y|X)) + β||E q [φ(X, Y)] − b|| 2 2 } (2)</formula><p>The objective can be optimized by an EM-like scheme that iteratively solves the minimization problem and the maximization problem. Solving the minimization problem is equivalent to solving its dual since the objective is convex. The dual problem is</p><formula xml:id="formula_4">arg max λ λ · b − log Z λ (X) − 1 4β ||λ|| 2 2 (3)</formula><p>We optimize the objective function 2 using stochastic projected gradient, and compute the learning rate using AdaGrad ( <ref type="bibr" target="#b3">Duchi et al., 2010</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Context-aware Posterior Constraints</head><p>We develop a rich set of context-aware poste- rior constraints for sentence-level sentiment anal- ysis by exploiting lexical and discourse knowl- edge. Specifically, we construct the lexical con- straints by extracting sentiment-bearing patterns within sentences and construct the discourse-level constraints by extracting discourse relations that indicate sentiment coherence or sentiment changes both within and across sentences. Each constraint can be formulated as equality between the expec- tation of a constraint function value and a desired value set by prior knowledge. The equality is not strictly enforced (due to the regularization in the PR objective 2). Therefore all the constraints are applied as soft constraints. <ref type="table" target="#tab_0">Table 1</ref> provides in- tuitive description and examples for all the con- straints used in our model.</p><p>Lexical Patterns The existence of a polarity- carrying word alone may not correctly indicate the polarity of the sentence, as the polarity can be re- versed by other polarity-reversing words. We ex- tract lexical patterns that consist of polar words and negators 3 , and apply the heuristics based on compositional semantics <ref type="bibr" target="#b1">(Choi and Cardie, 2008)</ref> to assign a sentiment value to each pattern.</p><p>We encode the extracted lexical patterns along with their sentiment values as feature-label con- straints. The constraint function can be written as</p><formula xml:id="formula_5">φ w (x, y) = i f w (x i , y i )</formula><p>where f w (x i , y i ) is a feature function which has value 1 when sentence x i contains the lexical pat- tern w and its sentiment label y i equals to the ex- pected sentiment value and has value 0 otherwise. The constraint expectation value is set to be the prior probability of associating w with its senti- ment value. Note that sentences with neutral senti- ment can also contain such lexical patterns. There- fore we allow the lexical patterns to be assigned a neutral sentiment with a prior probability r 0 (we compute this value as the empirical probability of neutral sentiment in the training documents). Us- ing the polarity indicated by lexical patterns to constrain the sentiment of sentences is quite ag- gressive. Therefore we only consider lexical pat- terns that are strongly discriminative (many opin- ion words in the lexicon only indicate sentiment with weak strength). The selected lexical patterns include a handful of seed patterns (such as "pros" and "cons") and the lexical patterns that have high precision (larger then 0.9) of predicting sentiment in the training data.</p><p>Discourse Connectives. Lexical patterns can be limited in capturing contextual information since they only look at interactions between words within an expression. To capture context at the clause or sentence level, we consider discourse connectives, which are cue phrases or words that indicate discourse relations between adjacent sen- tences or clauses. To identify discourse connec- tives, we apply a discourse tagger trained on the Penn Discourse Treebank ( <ref type="bibr" target="#b19">Prasad et al., 2008)</ref>  <ref type="bibr">4</ref> to our data. Discourse connectives are tagged with four senses: Expansion, Contingency, Compari- son, Temporal.</p><p>Discourse connectives can operate at both intra- sentential and inter-sentential level. For example, the word "although" is often used to connect two polar clauses within a sentence, while the word "however" is often used to at the beginning of the sentence to connect two polar sentences. It is important to distinguish these two types of dis- course connectives. We consider a discourse con- nective to be intra-sentential if it has the Com- parison sense and connects two polar clauses with opposite polarities (determined by the lexical pat- terns). We construct a feature-label constraint for each intra-sentential discourse connective and set its expected sentiment value to be neutral.</p><p>Unlike the intra-sentential discourse connec- tives, the inter-sentential discourse connectives can indicate sentiment transitions between sen- tences. Intuitively, discourse connectives with the senses of Expansion (e.g. also, for example, furthermore) and Contingency (e.g. as a result, hence, because) are likely to indicate sentiment coherence; discourse connectives with the sense of Comparison (e.g. but, however, nevertheless) are likely to indicate sentiment changes. This in- tuition is reasonable but it assumes the two sen- tences connected by the discourse connective are both polar sentences. In general, discourse con- nectives can also be used to connect non-polar (neutral) sentences. Thus it is hard to directly constrain the posterior expectation for each type of sentiment transitions using inter-sentential dis- course connectives.</p><p>Instead, we impose constraints on the model posteriors by reducing constraint violations. We</p><note type="other">Types Description and Examples Inter-sentential</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lexical patterns</head><p>The sentence containing a polar lexical pattern w tends to have the polarity indicated by w. Example lexical patterns are annoying, hate, amazing, not dis- appointed, no concerns, favorite, recommend.</p><p>Discourse Connectives (clause)</p><p>The sentence containing a discourse connective c which connects its two clauses that have opposite polarities indicated by the lexical patterns tends to have neu- tral sentiment. Example connectives are while, although, though, but.</p><p>Discourse Connectives (sentence)</p><p>Two adjacent sentences which are connected by a discourse connective c tends to have the same polarity if c indicates a Expansion or Contingency relation, e.g. also, for example, in fact, because ; opposite polarities if c indicates a Comparison relation, e.g. otherwise, nevertheless, however.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coreference</head><p>The sentences which contain coreferential entities appeared as targets of opinion expressions tend to have the same polarity.</p><p>Listing patterns A series of sentences connected via a listing tend to have the same polarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global labels</head><p>The sentence-level polarity tends to be consistent with the document-level po- larity. </p><formula xml:id="formula_6">φ c,s (x, y) = i f c,s (x i , y i , y i−1 )</formula><p>where c denotes a discourse connective, s indi- cates its sense, and f c,s is a penalty function that takes value 1.0 when y i and y i−1 form a contradic- tory sentiment transition, that is,</p><formula xml:id="formula_7">y i = polar y i−1 if s ∈ {Expansion, Contingency}, or y i = polar y i−1 if s = Comparison.</formula><p>The desired value for the con- straint expectation is set to 0 so that the model is encouraged to have less constraint violations. Opinion Coreference Sentences in a discourse can be linked by many types of coherence rela- tions ( <ref type="bibr" target="#b9">Jurafsky et al., 2000</ref>). Coreference is one of the commonly used relations in written text. In this work, we explore coreference in the con- text of sentence-level sentiment analysis. We con- sider a set of polar sentences to be linked by the opinion coreference relation if they contain core- ferring opinion-related entities. For example, the following sentences express opinions towards "the speaker phone", "The speaker phone" and "it" re- spectively. As these opinion targets are corefer- ential (referring to the same entity "the speaker phone"), they are linked by the opinion corefer- ence relation 5 .</p><p>My favorite features are the speaker phone and the radio. The speaker phone is very functional. I use it in the car, very audible even with freeway noise. <ref type="bibr">5</ref> In general, the opinion-related entities include both the opinion targets and the opinion holders. In this work, we only consider the targets since we experiment with single- author product reviews. The opinion holders can be included in a similar way as the opinion targets.</p><p>Our coreference relations indicated by opinion targets overlap with the same target relation intro- duced in <ref type="bibr" target="#b24">(Somasundaran et al., 2009</ref>). The dif- ferences are: (1) we encode the coreference re- lations as soft constraints during learning instead of applying them as hard constraints during infer- ence time; (2) our constraints can apply to both polar and non-polar sentences; (3) our identifica- tion of coreference relations is automatic without any fine-grained annotations for opinion targets.</p><p>To extract coreferential opinion targets, we ap- ply Stanford's coreference system ( <ref type="bibr" target="#b12">Lee et al., 2013</ref>) to extract coreferential mentions in the doc- ument, and then apply a set of syntactic rules to identify opinion targets from the extracted men- tions. The syntactic rules correspond to the shortest dependency paths between an opinion word and an extracted mention. We consider the 10 most frequent dependency paths in the training data. Example dependency paths include nsubj(opinion, mention), nobj(opinion, mention), and amod(mention, opinion).</p><p>For sentences connected by the opinion coref- erence relation, we expect their sentiment to be consistent. To encode this intuition, we define the following constraint function:</p><formula xml:id="formula_8">φ coref (x, y) = i,ant(i)=j,j≥0 f coref (x i , x j , y i , y j )</formula><p>where ant(i) denotes the index of the sentence which contains an antecedent target of the target mentioned in sentence i (the antecedent relations over pairs of opinion targets can be constructed using the coreference resolver), and f coref is a penalty function which takes value 1.0 when the expected sentiment coherency is violated, that is, y i = polar y j . Similar to the inter-sentential dis-course connectives, modeling opinion coreference via constraint violations allows the model to han- dle neutral sentiment. The expected value of the constraint functions is set to 0.</p><p>Listing Patterns Another type of coherence re- lations we observe in online reviews is listing, where a reviewer expresses his/her opinions by listing a series of statements followed by a se- quence of numbers. For example, "1. It's smaller than the ipod mini .... 2. It has a removable battery ....". We expect sentences connected by a listing to have consistent sentiment. We implement this constraint in the same form as the coreference con- straint (the antecedent assignments are constructed from the numberings).</p><p>Global Sentiment Previous studies have demonstrated the value of document-level sen- timent in guiding the semi-supervised learning of sentence-level sentiment <ref type="bibr" target="#b26">(Täckström and McDonald, 2011b;</ref><ref type="bibr" target="#b20">Qu et al., 2012)</ref>. In this work, we also take into account this information and encode it as posterior constraints. Note that these constraints are not necessary for our model and can be applied when the document-level sentiment labels are naturally available.</p><p>Based on an analysis of the Amazon review data, we observe that sentence-level sentiment usually doesn't conflict with the document-level sentiment in terms of polarity. For example, the proportion of negative sentences in the positive documents is very small compared to the propor- tion of positive sentences. To encode this intuition, we define the following constraint function:</p><formula xml:id="formula_9">φ g (x, y) = n i δ(y i = polar g)/n</formula><p>where g ∈ {positive, negative} denotes the sen- timent value of a polar document, n is the total number of sentences in x, and δ is an indicator function. We hope the expectation of the con- straint function takes a small value. In our experi- ments, we set the expected value to be the empiri- cal estimate of the probability of "conflicting" sen- timent in polar documents using the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training and Inference</head><p>During training, we need to compute the constraint expectations and the feature expectations under the auxiliary distribution q at each gradient step. We can derive q by solving the dual problem in 3:</p><formula xml:id="formula_10">q(y|x) = exp(θ · f (x, y) + λ · φ(x, y)) Z λ,θ (X)<label>(4)</label></formula><p>where Z λ,θ (X) is a normalization constant. Most of our constraints can be factorized in the same way as factorizing the model features in the first- order CRF model, and we can compute the expec- tations under q very efficiently using the forward- backward algorithm. However, some of our dis- course constraints (opinion coreference and list- ing) can break the tractable structure of the model. For constraints with higher-order structures, we use Gibbs Sampling <ref type="bibr" target="#b7">(Geman and Geman, 1984)</ref> to approximate the expectations. Given a sequence x, we sample a label y i at each position i by com- puting the unnormalized conditional probabilities</p><formula xml:id="formula_11">p(y i = l|y −i ) ∝ exp(θ · f (x, y i = l, y −i ) + λ · φ(x, y i = l, y −i ))</formula><p>and renormalizing them. Since the possible label assignments only differ at posi- tion i, we can make the computation efficient by maintaining the structure of the coreference clus- ters and precomputing the constraint function for different types of violations. During inference, we find the best label assign- ment by computing arg max y q(y|x). For doc- uments where the higher-order constraints apply, we use the same Gibbs sampler as described above to infer the most likely label assignment, other- wise, we use the Viterbi algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We experimented with two product review datasets for sentence-level sentiment classifica- tion: the Customer Review (CR) data ( <ref type="bibr" target="#b8">Hu and Liu, 2004)</ref>  <ref type="bibr">6</ref> which contains 638 reviews of 14 prod- ucts such as cameras and cell phones, and the Multi-domain Amazon (MD) data from the test set of Täckström and McDonald (2011a) which con- tains 294 reivews from 5 different domains. As in <ref type="bibr" target="#b20">Qu et al. (2012)</ref>, we chose the books, electronics and music domains for evaluation. Each domain also comes with 33,000 extra reviews with only document-level sentiment labels.</p><p>We evaluated our method in two settings: su- pervised and semi-supervised. In the supervised setting, we treated the test data as unlabeled data and performed transductive learning. In the semi- supervised setting, our unlabeled data consists of both the available unlabeled data and the test data. For each domain in the MD dataset, we made use of no more than 100 unlabeled documents in which our posterior constraints apply. We adopted the evaluation schemes used in previous work: 10- fold cross validation for the CR dataset and 3-fold cross validation for the MD dataset. We also report both two-way classification (positive vs. negative) and three-way classification results (positive, neg- ative or neutral). We use accuracy as the per- formance measure. In our tables, boldface num- bers are statistically significant by paired t-test for p &lt; 0.05 against the best baseline developed in this paper <ref type="bibr">7</ref> .</p><p>We trained our model using a CRF incorpo- rated with the proposed posterior constraints. For the CRF features, we include the tokens, the part- of-speech tags, the prior polarities of lexical pat- terns indicated by the opinion lexicon and the negator lexicon, the number of positive and neg- ative tokens and the output of the vote-flip algo- rithm ( <ref type="bibr" target="#b2">Choi and Cardie, 2009</ref>). In addition, we in- clude the discourse connectives as local or transi- tion features and the document-level sentiment la- bels as features (only available in the MD dataset).</p><p>We set the CRF regularization parameter σ = 1 and set the posterior regularization parameter β and γ (a trade-off parameter we introduce to bal- ance the supervised objective and the posterior regularizer in 2) by using grid search 8 . For approximation inference with higher-order con- straints, we perform 2000 Gibbs sampling itera- tions where the first 1000 iterations are burn-in it- erations. To make the results more stable, we con- struct three Markov chains that run in parallel, and select the sample with the largest objective value.</p><p>All posterior constraints were developed using the training data on each training fold. For the MD dataset, we also used the dvd domain as additional labeled data for developing the constraints.</p><p>Baselines. We compared our method to a num- ber of baselines: (1) CRF: CRF with the same set of model features as in our method. (2) CRF- INF: CRF augmented with inference constraints. We can incorporate the proposed constraints (con- straints derived from lexical patterns and discourse connectives) as hard constraints into CRF during Methods CR MD CRF 81.1 67.0 CRF-inf lex 80.9 66.4 CRF-inf disc 81.1 67.2 PR lex 81.8 69.7 PR 82.7 70.6 Previous work <ref type="bibr">TreeCRF (Nakagawa et al., 2010)</ref> 81.4 - Dropout LR ( <ref type="bibr" target="#b28">Wang and Manning, 2013</ref>) 82.1 -  <ref type="table">Table 3</ref>: Accuracy results (%) for semi-supervised sentiment classification (three-way) on the MD dataset inference by manually setting λ in equation 4 to a large value, <ref type="bibr">9</ref> . When λ is large enough, it is equiva- lent to adding hard constraints to the viterbi infer- ence. To better understand the different effects of lexical and discourse constraints, we report results for applying only the lexical constraints (CRF- INF lex ) as well as results for applying only the discourse constraints (CRF-INF disc ). (3) PR lex : a variant of our PR model which only applies the lexical constraints. For the three-way classifica- tion task on the MD dataset, we also implemented the following baselines: (4) VOTEFLIP: a rule- based algorithm that leverages the positive, nega- tive and neutral cues along with the effect of nega- tion to determine the sentence sentiment <ref type="bibr" target="#b2">(Choi and Cardie, 2009</ref>). (5) DOCORACLE: assigns each sentence the label of its corresponding doc- ument.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results</head><p>We first report results on a binary (positive or neg- ative) sentence-level sentiment classification task. For this task, we used the supervised setting and performed transductive learning for our model.  We also analyzed the model's performance on a three-way sentiment classification task. By intro- ducing the "neutral" category, the sentiment clas- sification problem becomes harder. <ref type="table" target="#tab_3">Table 4</ref> shows the results in terms of accuracy for each domain in the MD dataset. We can see that both PR and PR lex significantly outperform all other baselines in all domains. The rule-based baseline VOTE- FLIP gave the weakest performance because it has no prediction power on sentences with no opinion words. DOCORACLE performs much better than VOTEFLIP and performs especially well on the Music domain. This indicates that the document- level sentiment is a very strong indicator of the sentence-level sentiment label. For the CRF base- line and its invariants, we observe a similar per- formance trend as in the two-way classification task: there is nearly no performance improve- ment from applying the lexical and discourse- connective-based constraints during CRF infer- ence. In contrast, both PR lex and PR provide substantial improvements over CRF. This con- firms that encoding lexical and discourse knowl- edge as posterior constraints allows the feature- based model to gain additional learning power for sentence-level sentiment prediction. In par- ticular, incorporating discourse constraints leads to consistent improvements to our model. This demonstrates that our modeling of discourse in- formation is effective and that taking into account the discourse context is important for improving sentence-level sentiment analysis. We also com- pare our results to the previously published results on the same dataset. HCRF <ref type="bibr" target="#b25">(Täckström and McDonald, 2011a</ref>) and MEM ( <ref type="bibr" target="#b20">Qu et al., 2012</ref>) are two state-of-the-art semi-supervised methods for sentence-level sentiment classification. We can see that our best model PR gives the best results in most categories. <ref type="table" target="#tab_3">Table 4</ref> shows the results in terms of F1 scores for each sentiment category (positive, negative and neutral). We can see that the PR models are able to provide improvements over all the sentiment cate- gories compared to all the baselines in general. We observe that the DOCORACLE baseline provides very strong F1 scores on the positive and nega- tive categories especially in the Books and Mu- sic domains, but very poor F1 on the neutral cate- gory. This is because it over-predicts the polar sen- tences in the polar documents, and predicts no po- lar sentences in the neutral documents. In contrast, our PR models provide more balanced F1 scores among all the sentiment categories. Compared to the CRF baseline and its variants, we found that the PR models can greatly improve the precision of predicting positive and negative sentences, re- sulting in a significant improvement on the pos- itive/negative F1 scores. However, the improve- ment on the neutral category is modest. A plausi- ble explanation is that most of our constraints fo- cus on discriminating polar sentences. They can help reduce the errors of misclassifying polar sen- tences, but the model needs more constraints in order to distinguish neutral sentences from polar sentences. We plan to address this issue in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Discussion</head><p>We analyze the errors to better understand the mer- its and limitations of the PR model. We found that the PR model is able to correct many CRF errors caused by the lack of labeled data. The first row in <ref type="table">Table 5</ref> shows an example of such errors.</p><p>Example Sentences CRF PR Example 1: neg If I could, I would like to return it or exchange for something better./neg neu × Example 2: neg Things I wasn't a fan of -the ending was to cutesy for my taste./neg neg Also, all of the side characters (particularly the mom, vee, and the teacher) were incredibly flat and stereotypical to me./neg neu pos × Example 3: neg I also have excessive noise when I talk and have phone in my pocket while walking./neg neu But other models are no better./neu neg pos × neg pos × <ref type="table">Table 5</ref>: Example sentences where PR succeeds and fails to correct the mistakes of CRF</p><p>The lexical features return and exchange may be good indicators of negative sentiment for the sentence. However, with limited labeled data, the CRF learner can only associate very weak senti- ment signals to these features. In contrast, the PR model is able to associate stronger sentiment sig- nals to these features by leveraging unlabeled data for indirect supervision. A simple lexicon-based constraint during inference time may also correct this case. However, hard-constraint baselines can hardly improve the performance in general be- cause the contributions of different constraints are not learned and their combination may not lead to better predictions. This is also demonstrated by the limited performance of CRF-INF in our exper- iments. We also found that the discourse constraints play an important role in improving the sentiment prediction. The lexical constraints alone are of- ten not sufficient since their coverage is limited by the sentiment lexicon and they can only constrain sentiment locally. On the contrary, discourse con- straints are not dependent on sentiment lexicons, and more importantly, they can provide sentiment preferences on multiple sentences at the same time. When combining discourse constraints with features from different sentences, the PR model becomes more powerful in disambiguating senti- ment. The second example in <ref type="table">Table 5</ref> shows that the PR model learned with discourse constraints correctly predicts the sentiment of two sentences where no lexical constraints apply.</p><p>However, discourse constraints are not always helpful. One reason is that they do not constrain the neutral sentiment. As a result they could not help disambiguate neutral sentiment from polar sentiment, such as the third example in <ref type="table">Table 5</ref>. This is also a problem for most of our lexical con- straints. In general, it is hard to learn reliable indi- cators for the neutral sentiment. In the MD dataset, a neutral label may be given because the sentence contains mixed sentiment or no sentiment or it is off-topic. We plan to explore more refined con- straints that can deal with the neutral sentiment in future work. Another limitation of the discourse constraints is that they could be affected by the er- rors of the discourse parser and the coreference re- solver. A potential way to address this issue is to learn discourse constraints jointly with sentiment. We plan to study this in future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a context-aware ap- proach for learning sentence-level sentiment. Our approach incorporates intuitive lexical and dis- course knowledge as expressive constraints while training a conditional random field model via pos- terior regularization. We explore a rich set of context-aware constraints at both intra-and inter- sentential levels, and demonstrate their effective- ness in the analysis of sentence-level sentiment. While we focus on the sentence-level task, our ap- proach can be easily extended to handle sentiment analysis at finer levels of granularity. Our exper- iments show that our model achieves better accu- racy than existing supervised and semi-supervised models for the sentence-level sentiment classifica- tion task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Summarization of Posterior Constraints for Sentence-level Sentiment Classification 

define the following constraint function: 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Accuracy results (%) for supervised sen-
timent classification (two-way) 

Books Electronics Music Avg 
VoteFlip 
44.6 
45.0 
47.8 
45.8 
DocOracle 
53.6 
50.5 
63.0 
55.7 
CRF 
57.4 
57.5 
61.8 
58.9 
CRF-inf lex 
56.7 
56.4 
60.4 
57.8 
CRF-inf disc 
57.2 
57.6 
62.1 
59.0 
PR lex 
60.3 
59.9 
63.2 
61.1 
PR 
61.6 
61.0 
64.4 
62.3 
Previous work 
HCRF 
55.9 
61.0 
58.7 
58.5 
MEM 
59.7 
59.6 
63.8 
61.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 shows the accuracy results. We can see</head><label>2</label><figDesc></figDesc><table>Books 

Electronics 
Music 
pos/neg/neu pos/neg/neu pos/neg/neu 
VoteFlip 
43/42/47 
45/46/44 
50/46/46 
DocOracle 
54/60/49 
57/54/42 
72/65/52 
CRF 
47/51/64 
60/61/52 
67/60/58 
CRF-inf lex 
46/52/63 
59/61/50 
65/59/57 
CRF-inf disc 
47/51/64 
60/61/52 
67/61/59 
PR lex 
50/56/66 
64/63/53 
67/64/59 
PR 
52/56/68 
64/66/53 
69/65/60 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>F1 scores for each sentiment cate-
gory (positive, negative and neutral) for semi-
supervised sentiment classification on the MD 
dataset 

that PR significantly outperforms all other base-
lines in both the CR dataset and the MD dataset 
(average accuracy across domains is reported). 
The poor performance of CRF-INF lex indicates 
that directly applying lexical constraints as hard 
constraints during inference could only hurt the 
performance. CRF-INF disc slightly outperforms 
CRF but the improvement is not significant. In 
contrast, both PR lex and PR significantly outper-
form CRF, which implies that incorporating lex-
ical and discourse constraints as posterior con-
straints is much more effective. The superior per-
formance of PR over PR lex further suggests that 
the proper use of discourse information can signif-
icantly improve accuracy for sentence-level senti-
ment classification. 
</table></figure>

			<note place="foot" n="1"> In general, inequality constraints can also be used. We focus on the equality constraints since we found them to express the sentiment-relevant constraints well. 2 Other convex functions can be used for the penalty. We use L2 norm because it works well in practice. β is a regularization constant</note>

			<note place="foot" n="3"> The polar words are identified using the MPQA lexicon and the negators are identified using a handful of seed words extended by the General Inquirer dictionary and WordNet as described in (Choi and Cardie, 2008).</note>

			<note place="foot" n="4"> http://www.cis.upenn.edu/ ˜ epitler/ discourse.html</note>

			<note place="foot" n="6"> Available at http://www.cs.uic.edu/ ˜ liub/ FBS/sentiment-analysis.html.</note>

			<note place="foot" n="7"> Significance test was not conducted over the previous methods as we do not have their results for each fold. 8 We conducted 10-fold cross-validation on each training fold with the parameter space: β : [0.01, 0.05, 0.1, 0.5, 1.0] and γ : [0.1, 0.5, 1.0, 5.0, 10.0].</note>

			<note place="foot" n="9"> We set λ to 1000 for the lexical constraints and-1000 to the discourse connective constraints in the experiments</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported in part by DARPA-BAA-12-47 DEFT grant #12475008 and NSF grant BCS-0904822. We thank Igor Labutov for help-ful discussion and suggestions; Oscar Täckström and Lizhen Qu for providing their Amazon review datasets; and the anonymous reviewers for helpful comments and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Alternating projections for learning 333 with expectation constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kedar</forename><surname>Bellare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Druck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="43" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning with compositional semantics as structural inference for subsentential sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="793" to="801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adapting a polarity lexicon using integer linear programming for domain-specific sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="590" to="598" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Crosslingual discriminative learning of sequence models with posterior regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dependency grammar induction via bitext projection constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Gillenwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-IJCNLP</title>
		<meeting>the ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="369" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Posterior regularization for structured latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Graça</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Gillenwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="2001" to="2049" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Stochastic relaxation, gibbs distributions, and the bayesian restoration of images. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Geman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="721" to="741" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Speech and language processing: An introduction to natural language processing, computational linguistics, and speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Kehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Vander Linden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ward</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>MIT Press</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fully automatic lexicon expansion for domainoriented sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Kanayama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tetsuya</forename><surname>Nasukawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2006 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="355" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A bayesian model for joint unsupervised induction of sentiment, aspect and discourse representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Sporleder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">To Appear in Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deterministic coreference resolution based on entity-centric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heeyoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Peirsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>precision-ranked rules</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Isotonic conditional random fields and local sentiment flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Lebanon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">961</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Structured models for fine-to-coarse sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kerry</forename><surname>Hannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Neylon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Wells</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Reynar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting-Association For Computational Linguistics</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page">432</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dependency tree-based sentiment classification using crfs with hidden variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tetsuji</forename><surname>Nakagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="786" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd annual meeting on Association for Computational Linguistics, page 271. Association for Computational Linguistics</title>
		<meeting>the 42nd annual meeting on Association for Computational Linguistics, page 271. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Opinion mining and sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Now Pub</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Contextual valence shifters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Livia</forename><surname>Polanyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Zaenen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computing attitude and affect in text: Theory and applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The penn discourse treebank 2.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Dinesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Miltsakaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Livio</forename><surname>Robaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aravind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><forename type="middle">L</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Webber</surname></persName>
		</author>
		<editor>LREC. Citeseer</editor>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A weakly supervised model for sentence-level semantic orientation analysis with multiple experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Gemulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="149" to="159" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Discourse level opinion interpretation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swapna</forename><surname>Somasundaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Ruppenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Computational Linguistics</title>
		<meeting>the 22nd International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="801" to="808" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Supervised and unsupervised methods in employing discourse relations for improving opinion polarity classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swapna</forename><surname>Somasundaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="170" to="179" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Discovering fine-grained sentiment with latent variable structured prediction models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="368" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Semisupervised latent variable models for sentence-level sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Discourse connectors for latent subjectivity in sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakshit</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="808" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fast dropout training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning (ICML13)</title>
		<meeting>the 30th International Conference on Machine Learning (ICML13)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="118" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Discourse level explanatory relation extraction from product reviews using firstorder logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihua</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised discovery of discourse relations for eliminating intra-sentence polarity ambiguities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lanjun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="162" to="171" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
