<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:07+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generating Informative Responses with Controlled Sentence Function</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Ke</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Conversational AI group</orgName>
								<orgName type="department" key="dep2">Dept. of Computer Science</orgName>
								<orgName type="laboratory">Lab</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<region>AI</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Guan</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Dept. of Physics</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Conversational AI group</orgName>
								<orgName type="department" key="dep2">Dept. of Computer Science</orgName>
								<orgName type="laboratory">Lab</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<region>AI</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Conversational AI group</orgName>
								<orgName type="department" key="dep2">Dept. of Computer Science</orgName>
								<orgName type="laboratory">Lab</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<region>AI</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Generating Informative Responses with Controlled Sentence Function</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1499" to="1508"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1499</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Sentence function is a significant factor to achieve the purpose of the speaker, which, however, has not been touched in large-scale conversation generation so far. In this paper, we present a model to generate informative responses with controlled sentence function. Our model utilizes a continuous latent variable to capture various word patterns that realize the expected sentence function, and introduces a type controller to deal with the compatibility of controlling sentence function and generating informative content. Conditioned on the latent variable, the type controller determines the type (i.e., function-related, topic, and ordinary word) of a word to be generated at each decoding position. Experiments show that our model outper-forms state-of-the-art baselines, and it has the ability to generate responses with both controlled sentence function and informative content.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sentence function is an important linguistic feature and a typical taxonomy in terms of the purpose of the speaker <ref type="bibr" target="#b19">(Rozakis, 2003)</ref>. There are four major function types in the language including interrog- ative, declarative, imperative, and exclamatory, as described in <ref type="bibr" target="#b19">(Rozakis, 2003)</ref>. Each sentence func- tion possesses its own structure, and transforma- tion between sentence functions needs a series of changes in word order, syntactic patterns and other aspects <ref type="bibr" target="#b1">(Akmajian, 1984;</ref><ref type="bibr" target="#b28">Yule, 2010)</ref>.</p><p>Since sentence function is regarding the purpose of the speaker, it can be a significant factor indi- cating the conversational purpose during interac- * *Corresponding author: Minlie Huang.  Interrogative responses can be used to acquire further information from the user; imperative re- sponses are used to make requests, directions, in- structions or invitations to elicit further interac- tions; and declarative responses commonly make statements to state or explain something. <ref type="bibr">1</ref> Inter- rogative and imperative responses can be used to avoid stalemates ( <ref type="bibr" target="#b14">Li et al., 2016b</ref>), which can be viewed as important proactive behaviors in con- versation ( <ref type="bibr" target="#b27">Yu et al., 2016</ref>). Thus, conversational systems equipped with the ability to control the sentence function can adjust its strategy for dif- ferent purposes within different contexts, behave more proactively, and may lead the dialogue to go further.</p><p>Generating responses with controlled sentence functions differs significantly from other tasks on controllable text generation ( <ref type="bibr" target="#b12">Hu et al., 2017;</ref><ref type="bibr" target="#b8">Ficler and Goldberg, 2017;</ref><ref type="bibr" target="#b2">Asghar et al., 2017;</ref><ref type="bibr" target="#b9">Ghosh et al., 2017;</ref><ref type="bibr" target="#b32">Zhou and Wang, 2017;</ref><ref type="bibr" target="#b7">Dong et al., 2017;</ref><ref type="bibr" target="#b17">Murakami et al., 2017)</ref>. These studies, in- volving the control of sentiment polarity, emotion, or tense, fall into local control, more or less, be- cause the controllable variable can be locally re- 1 Note that we did not include the exclamatory category in this paper because an exclamatory sentence in conversation is only a strong emotional expression of the original sentence with few changes. flected by decoding local variable-related words, e.g., terrible for negative sentiment ( <ref type="bibr" target="#b12">Hu et al., 2017;</ref><ref type="bibr" target="#b9">Ghosh et al., 2017)</ref>, glad for happy emo- tion ( <ref type="bibr" target="#b31">Zhou et al., 2018;</ref><ref type="bibr" target="#b32">Zhou and Wang, 2017)</ref>, and was for past tense ( <ref type="bibr" target="#b12">Hu et al., 2017)</ref>. By contrast, sentence function is a global attribute of text, and controlling sentence function is more challenging in that it requires to adjust the global structure of the entire text, including changing word order and word patterns.</p><p>Controlling sentence function in conversational systems faces another challenge: in order to gen- erate informative and meaningful responses, it has to deal with the compatibility of the sentence func- tion and the content. Similar to most existing neu- ral conversation models ( <ref type="bibr" target="#b13">Li et al., 2016a;</ref><ref type="bibr" target="#b16">Mou et al., 2016;</ref>, we are also strug- gling with universal and meaningless responses for different sentence functions, e.g., "Is that right?" for interrogative responses, "Please!" for imperative responses and "Me, too." for declar- ative responses. The lack of meaningful topics in responses will definitely degrade the utility of the sentence function so that the desired conversa- tional purpose can not be achieved. Thus, the task needs to generate responses with both informative content and controllable sentence functions.</p><p>In this paper, we propose a conversation gen- eration model to deal with the global control of sentence function and the compatibility of control- ling sentence function and generating informative content. We devise an encoder-decoder structure equipped with a latent variable in conditional vari- ational autoencoder (CVAE) ( <ref type="bibr" target="#b23">Sohn et al., 2015)</ref>, which can not only project different sentence func- tions into different regions in a latent space, but also capture various word patterns within each sentence function. The latent variable, supervised by a discriminator with the expected function la- bel, is also used to realize the global control of sen- tence function. To address the compatibility issue, we use a type controller which lexicalizes the sen- tence function and the content explicitly. The type controller estimates a distribution over three word types, i.e., function-related, topic, and ordinary words. During decoding, the word type distribu- tion will be used to modulate the generation distri- bution in the decoder. The type sequence of a re- sponse can be viewed as an abstract representation of sentence function. By this means, the model has an explicit and strong control on the function and the content. Our contributions are as follows:</p><p>• <ref type="table">We investigate how to control sentence func- tions to achieve different conversational pur- poses in open-domain dialogue systems. We  analyze the difference between this task and  other controllable generation tasks.</ref> • We devise a structure equipped with a la- tent variable and a type controller to achieve the global control of sentence function and deal with the compatibility of controllable sentence function and informative content in generation. Experiments show the effective- ness of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Recently, language generation in conversational systems has been widely studied with sequence- to-sequence (seq2seq) learning <ref type="bibr" target="#b24">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b3">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b25">Vinyals and Le, 2015;</ref><ref type="bibr" target="#b22">Shang et al., 2015;</ref><ref type="bibr" target="#b20">Serban et al., 2016</ref><ref type="bibr" target="#b21">Serban et al., , 2017</ref> In addition to the content quality, controllabil- ity is a critical problem in text generation. Vari- ous methods have been used to generate texts with controllable variables such as sentiment polarity, emotion, or tense ( <ref type="bibr" target="#b12">Hu et al., 2017;</ref><ref type="bibr" target="#b9">Ghosh et al., 2017;</ref><ref type="bibr" target="#b32">Zhou and Wang, 2017;</ref><ref type="bibr" target="#b31">Zhou et al., 2018</ref>) . There are mainly two solutions to deal with con- trollable text generation. First, the variables to be controlled are embedded into vectors which are then fed into the models to reflect the character- istics of the variables ( <ref type="bibr" target="#b9">Ghosh et al., 2017;</ref><ref type="bibr" target="#b31">Zhou et al., 2018</ref>). Second, latent variables are used to capture the information of controllable attributes as in the variational autoencoders (VAE) ( <ref type="bibr" target="#b32">Zhou and Wang, 2017)</ref>. ( <ref type="bibr" target="#b12">Hu et al., 2017</ref>) combined the two techniques by disentangling a latent variable into a categorical code and a random part to better control the attributes of the generated text.</p><p>The task in this paper differs from the above tasks in two aspects: (1) Unlike other tasks that realize controllable text generation by decoding attribute-related words locally, our task requires to not only decode function-related words, but also Figure 2: Model overview. During training, the latent variable z is sampled from the recognition network which is supervised by the function label in the discriminator. In the type controller, the latent variable and the decoder's state are used to estimate a type distribution which modulates the final generation distribution. During test, z is sampled from the prior network whose input is only the post. The response encoder in the dotted box appears only in training.</p><p>plan the words globally to realize the function type to be controlled. <ref type="formula" target="#formula_2">(2)</ref> The compatibility of control- lable variables and content quality is less studied in the literature. The most similar work in <ref type="bibr" target="#b29">(Zhao et al., 2017)</ref> proposed to control the dialogue act of a response, which is also a global attribute. However, the model controls dialog act by directly feeding a latent variable into the decoder, instead, our model has a stronger control on the genera- tion process via a type controller in which words of different types are concretely modeled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task Definition and Model Overview</head><p>Our problem is formulated as follows: given a post X = x 1 x 2 · · · x n and a sentence function cate- gory l, our task is to generate a response Y = y 1 y 2 · · · y m that is not only coherent with the spec- ified function category l but also informative in content. We denote c as the concatenation of all the input information, i.e. c = [X; l]. Essentially, the goal is to estimate the conditional probability:</p><formula xml:id="formula_0">P (Y, z|c) = P (z|c) · P (Y |z, c)<label>(1)</label></formula><p>The latent variable z is used to capture the sen- tence function of a response. P (z|c), parameter- ized as the prior network in our model, indicates the sampling process of z, i.e., drawing z from</p><formula xml:id="formula_1">P (z|c). And P (Y |z, c) = m t=1 P (y t |y &lt;t , z, c</formula><p>) is applied to model the generation of the response Y conditioned on the latent variable z and the in- put c, which is implemented by a decoder in our model. <ref type="figure">Figure 2</ref> shows the overview of our model. As aforementioned, the model is constructed in the encoder-decoder framework. The encoder takes a post and a response as input, and obtains the hid- den representations of the input. The recognition network and the prior network, adopted from the CVAE framework ( <ref type="bibr" target="#b23">Sohn et al., 2015)</ref>, sample a la- tent variable z from two normal distributions, re- spectively. Supervised by a discriminator with the function label, the latent variable encodes mean- ingful information to realize a sentence function. The latent variable, along with the decoder's state, is also used to control the type of a word in gen- eration via the type controller. In the decoder, the final generation distribution is mixed by the type distribution which is obtained from the type con- troller. By this means, the latent variable encodes information not only from sentence function but also from word types, and in return, the decoder and the type controller can deal with the compat- ibility of realizing sentence function and informa- tion content in generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Encoder-Decoder Framework</head><p>The encoder-decoder framework has been widely used in language generation ( <ref type="bibr" target="#b24">Sutskever et al., 2014;</ref><ref type="bibr" target="#b25">Vinyals and Le, 2015)</ref>. The encoder trans- forms the post sequence X = x 1 x 2 · · · x n into hidden representations H = h 1 h 2 · · · h n , as fol- lows:</p><formula xml:id="formula_2">h t = GRU(e(x t ), h t−1 )<label>(2)</label></formula><p>where GRU is gated recurrent unit ( <ref type="bibr" target="#b5">Cho et al., 2014)</ref>, and e(x t ) denotes the embedding of the word x t . The decoder first updates the hidden states S = s 1 s 2 · · · s m , and then generates the target se- quence Y = y 1 y 2 · · · y m as follows:</p><formula xml:id="formula_3">s t = GRU(s t−1 , e(y t−1 ), cv t−1 ) (3) y t ∼ P (y t |y &lt;t , s t ) = sof tmax(W s t )<label>(4)</label></formula><p>where this GRU does not share parameters with the encoder's network. The context vector cv t−1 is a dynamic weighted sum of the encoder's hid- den states, i.e., </p><formula xml:id="formula_4">cv t−1 = n i=1 α t−1 i h i ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Recognition/Prior Network</head><p>On top of the encoder-decoder structure, our model introduces the recognition network and the prior network of CVAE framework, and utilizes the two networks to draw latent variable samples during training and test respectively. The latent variable can project different sentence functions into different regions in a latent space, and also capture various word patterns within a sentence function.</p><p>In the training process, our model needs to sam- ple the latent variable from the posterior distribu- tion P (z|Y, c), which is intractable. Thus, the recognition network q φ (z|Y, c) is introduced to approximate the true posterior distribution so that we can sample z from this deterministic parame- terized model. We assume that z follows a mul- tivariate Gaussian distribution whose covariance matrix is diagonal, i.e., q φ (z|Y, c) ∼ N (µ, σ 2 I). Under this assumption, the recognition network can be parameterized by a deep neural network such as a multi-layer perceptron (MLP):</p><formula xml:id="formula_5">[µ, σ 2 ] = MLP posterior (Y, c)<label>(5)</label></formula><p>During test, we use the prior network p θ (z|c) ∼ N (µ , σ 2 I) instead to draw latent variable sam- ples, which can be implemented in a similar way:</p><formula xml:id="formula_6">[µ , σ 2 ] = MLP prior (c)<label>(6)</label></formula><p>To bridge the gap between the recognition and the prior networks, we add the KL divergence term that should be minimized to the loss function:</p><formula xml:id="formula_7">L 1 = KL(q φ (z|Y, c)||p θ (z|c))<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Discriminator</head><p>The discriminator supervises z to encode function-related information in a response with supervision signals. It takes z as input instead of the generated response Y to avoid the vanishing gradient of z, and predicts the function category conditioned on z:</p><formula xml:id="formula_8">P (l|z) = sof tmax(W D · MLP dis (z)) (8)</formula><p>This formulation can enforce z to capture the fea- tures of sentence function and enhance the influ- ence of z in word generation. The loss function of the discriminator is given by:</p><formula xml:id="formula_9">L 2 = −E q φ (z|Y,c) [log P (l|z)]<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Type Controller</head><p>The type controller is designed to deal with the compatibility issue of controlling sentence func- tion and generating informative content. As afore- mentioned, we classify the words in a response into three types: function-related, topic, and or- dinary words. The type controller estimates a dis- tribution over the word types at each decoding po- sition, and the type distribution will be used in the mixture model of the decoder for final word gener- ation. During the decoding process, the decoder's state s t and the latent variable z are taken as input to estimate the type distribution as follows:</p><formula xml:id="formula_10">P (wt|s t , z) = sof tmax(W 0 · MLP type (s t , z))<label>(10)</label></formula><p>Noticeably, the latent variable z introduced to the RNN encoder-decoder framework often fails to learn a meaningful representation and has lit- tle influence on language generation, because the RNN decoder may ignore z during generation, known as the issue of vanishing latent variable ( <ref type="bibr" target="#b4">Bowman et al., 2016)</ref>. By contrast, our model allows z to directly control the word type at each decoding position, which has more influence on language generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Decoder</head><p>Compared with the traditional decoder described in Section 3.2, our decoder updates the hidden state s t with both the input information c and the latent variable z, and generates the response in a mixture form which is combined with the type dis- tribution obtained from the type controller:</p><formula xml:id="formula_11">s t = GRU(s t−1 , e(y t−1 ), cv t−1 , c, z) (11) P (y t |y &lt;t , c, z) = P (y t |y t−1 , s t , c, z) = 3 i=1 P (wt = i|s t , z)P (y t |y t−1 , s t , c, z, wt = i)<label>(12)</label></formula><p>where wt = 1, 2, 3 stand for function-related words, topic words, and ordinary words, respec- tively. The probability for choosing different word types at time t, P (wt = i|s t , z), is obtained from the type controller, as shown in Equation (10). The probabilities of choosing words in different types are introduced as follows: Function-related Word: Function-related words represent the typical words for each sentence func- tion, e.g., what for interrogative responses, and please for imperative responses. To select the function-related words at each position, we simul- taneously consider the decoder's state s t , the latent variable z and the function category l. P (y t |y t−1 , s t , c, z, wt = 1) =</p><formula xml:id="formula_12">sof tmax(W 1 · [s t , z, e(l)]) (13)</formula><p>where e(l) is the embedding vector of the func- tion label. Under the control of z, our model can learn to decode function-related words at proper positions automatically. Topic Word: Topic words are crucial for generat- ing an informative response. The probability for selecting a topic word at each decoding position depends on the current hidden state s t :</p><formula xml:id="formula_13">P (y t |y t−1 , s t , c, z, wt = 2) = sof tmax(W 2 s t )<label>(14)</label></formula><p>This probability is over the topic words we predict conditioned on a post. Section 3.8 will describe the details. Ordinary Word: Ordinary words play a func- tional role in making a natural and grammatical sentence. The probability of generating ordinary words is estimated as below: P (y t |y t−1 , s t , c, z, wt = 3) = sof tmax(W 3 s t ) <ref type="formula" target="#formula_0">(15)</ref> The generation loss of the decoder is given as below:</p><formula xml:id="formula_14">L 3 = −E q φ (z|Y,c) [log P (Y |z, c)] = −E q φ (z|Y,c) [ t log P (y t |y &lt;t , z, c)] (16)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Loss Function</head><p>The overall loss L is a linear combination of the KL term L 1 , the classification loss of the discrim- inator L 2 , and the generation loss of the decoder</p><formula xml:id="formula_15">L 3 : L = αL 1 + L 2 + L 3<label>(17)</label></formula><p>We let α gradually increase from 0 to 1. This technique of KL cost annealing can address the optimization challenges of vanishing latent vari- ables in the RNN encoder-decoder ( <ref type="bibr" target="#b4">Bowman et al., 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8">Topic Word Prediction</head><p>Topic words play a key role in generating an infor- mative response. We resort to pointwise mutual information (PMI) <ref type="bibr" target="#b6">(Church and Hanks, 1990</ref>) for predicting a list of topic words that are relevant to a post. Let x and y indicate a word in a post X and its response Y respectively, and PMI is computed as follows: P M I(x, y) = log P (x, y) P (x)P (y)</p><p>Then, the relevance score of a topic word to a given post x 1 x 2 · · · x n can be approximated as fol- lows, similar to ( <ref type="bibr" target="#b16">Mou et al., 2016)</ref>:</p><formula xml:id="formula_17">REL(x 1 , ..., x n , y) ≈ n i=1 P M I(x i , y) (19)</formula><p>During training, the words in a response with high REL scores to the post are treated as topic words. During test, we use REL to select the top ranked words as topic words for a post.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data Preparation</head><p>We collected a Chinese dialogue dataset from Weibo 2 . We crawled about 10 million post- responses pairs. Since our model needs the sen- tence function label for each pair, we built a clas- sifier to predict the sentence function automati- cally to construct large-scale labeled data. Thus, we sampled about 2,000 pairs from the original dataset and annotated the data manually with four categories, i.e., interrogative, imperative, declara- tive and other. This small dataset was partitioned into the training, validation, and test sets with the ratio of 6:1:1. Three classifiers, including LSTM (Hochreiter and Schmidhuber, 1997), Bi-LSTM ( <ref type="bibr" target="#b10">Graves et al., 2005</ref>) and a self-attentive model ( , were attempted on this dataset. The results in <ref type="table">Table 1</ref> show that the self-attentive classifier outperforms other models and achieves the best accuracy of 0.78 on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Accuracy LSTM 0.60 Bi-LSTM 0.75 Self-Attentive 0.78 <ref type="table">Table 1</ref>: Accuracy of sentence function classifica- tion on the 2,000 post-response pairs.</p><p>We then applied the self-attentive classifier to annotate the large dataset and obtained a dialogue dataset with noisy sentence function labels <ref type="bibr">3</ref> . To balance the distribution of sentence functions, we randomly sampled about 0.6 million pairs for each sentence function to construct the final dataset. The statistics of this dataset are shown in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiment Settings</head><p>Our model was implemented with TensorFlow <ref type="bibr">5</ref> . We applied bidirectional GRU with 256 cells to the encoder and GRU with 512 cells to the de- coder. The dimensions of word embedding and function category embedding were both set to 100. We also set the dimension of latent vari- ables to 128. The vocabulary size was set to 40,000. Stochastic gradient descent <ref type="bibr" target="#b18">(Qian, 1999</ref>) was used to optimize our model, with a learning rate of 0.1, a decay rate of 0.9995, and a momen- tum of 0.9. The batch size was set to 128. Our codes are available at https://github.com/ kepei1106/SentenceFunction. We chose several state-of-the-art baselines, which were implemented with the settings pro- vided in the original papers: Conditional Seq2Seq (c-seq2seq): A Seq2Seq variant which takes the category (i.e., function type) embedding as additional input at each de- coding position <ref type="bibr" target="#b8">(Ficler and Goldberg, 2017)</ref>. Mechanism-aware (MA): This model assumes that there are multiple latent responding mecha- nisms ( <ref type="bibr" target="#b15">Zhou et al., 2017)</ref>. The number of respond- ing mechanisms is set to 3, equal to the number of function types. Knowledge-guided CVAE (KgCVAE): A modi- fied CVAE which aims to control the dialog act of a generated response ( <ref type="bibr" target="#b29">Zhao et al., 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Automatic Evaluation</head><p>Metrics: We adopted Perplexity (PPL) <ref type="bibr" target="#b25">(Vinyals and Le, 2015)</ref>, Distinct-1 (Dist-1), Distinct-2 (Dist-2) ( <ref type="bibr" target="#b13">Li et al., 2016a)</ref>, and Accuracy (ACC) to evaluate the models at the content and function level. Perplexity can measure the grammatical- ity of generated responses. Distinct-1/distinct-2 is the proportion of distinct unigrams/bigrams in all the generated tokens, respectively. Accuracy mea- sures how accurately the sentence function can be controlled. Specifically, we compared the prespec- ified function (as input to the model) with the func- tion of a generated response, which is predicted by the self-attentive classifier (see Section 4.1).  <ref type="table">Table 3</ref>: Automatic evaluation with perplexity (PPL), distinct-1 (Dist-1), distinct-2 (Dist-2), and accuracy (ACC). The integers in the Dist-* cells denote the total number of distinct n-grams.</p><p>Results: Our model has lower perplexity than c- seq2seq and KgCVAE, indicating that the model is comparable with other models in generating gram- matical responses. Note that MA has the lowest perplexity because it tends to generate generic re- sponses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Interrogative Declarative Imperative Gram.</p><p>Appr. Info.</p><p>Gram. Appr. Info.</p><p>Gram. Appr.</p><p>Info. Ours vs. c-seq2seq</p><p>0.534 0.536 0.896* 0.630* 0.573* 0.764* 0.685* 0.504 0.893* Ours vs. MA 0.802* 0.602* 0.675* 0.751* 0.592* 0.617* 0.929* 0.568* 0.577* Ours vs. KgCVAE 0.510 0.626* 0.770* 0.546* 0.515* 0.744* 0.780* 0.521* 0.837* <ref type="table">Table 4</ref>: Manual evaluation results for different functions. The scores indicate the percentages that our model wins the baselines after removing tie pairs. The scores of our model marked with * are significantly better than the competitors (Sign Test, p-value &lt; 0.05).</p><p>As for distinct-1 and distinct-2, our model gen- erates remarkably more distinct unigrams and bi- grams than the baselines, indicating that our model can generate more diverse and informative re- sponses compared to the baselines.</p><p>In terms of sentence function accuracy, our model outperforms all the baselines and achieves the best accuracy of 0.992, which indicates that our model can control the sentence function more precisely. MA has a very low score because there is no direct way to control sentence function, in- stead, it learns automatically from the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Manual Evaluation</head><p>To evaluate the generation quality and how well the models can control sentence function, we con- ducted pair-wise comparison. 200 posts were ran- domly sampled from the test set and each model was required to generate responses with three function types to each post. For each pair of re- sponses (one by our model and the other by a base- line, along with the post), annotators were hired to give a preference (win, lose, or tie). The total annotation amounts to 200×3×3×3=5,400 since we have three baselines, three function types, and three metrics. We resorted to a crowdsourcing ser- vice for annotation, and each pair-wise compari- son was judged by 5 curators. Metrics: We designed three metrics to evaluate the models from the perspectives of sentence func- tion and content: grammaticality (whether a re- sponse is grammatical and coherent with the sen- tence function we prespecified), appropriateness (whether a response is a logical and appropriate reply to its post), and informativeness (whether a response provides meaningful information via the topic words relevant to the post). Note that the three metrics were separately evaluated. Results: The scores in <ref type="table">Table 4</ref> represent the per- centages that our model wins a baseline after re- moving tie pairs. A value larger than 0.5 indi- cates that our model outperforms its competitor. Our model outperforms the baselines significantly in most cases (Sign Test, with p-value &lt; 0.05). Among the three function types, our model per- forms significantly better than the baselines when generating declarative and imperative responses. As for interrogative responses, our model is bet- ter but the difference is not significant in some set- tings. This is because interrogative patterns are more apparent and easier to learn, thereby all the models can capture some of the patterns to gen- erate grammatical and appropriate responses, re- sulting in more ties. By contrast, declarative and imperative responses have less apparent patterns whereas our model is better at capturing the global patterns through modeling the word types explic- itly.</p><p>We can also see that our model obtains particu- larly high scores in informativeness. This demon- strates that our model is better to generate more informative responses, and is able to control sen- tence functions at the same time.</p><p>The annotation statistics are shown in <ref type="table">Table  5</ref>. The percentage of annotations that at least 4 judges assign the same label (at least 4/5 agree- ment) is larger than 50%, and the percentage for at least 3/5 agreement is about 90%, indicating that annotators reached a moderate agreement.  <ref type="table">Table 5</ref>: Annotation statistics. At least n/5 means there are no less than n judges assigning the same label to a record during annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Words and Patterns in Function Control</head><p>To further analyze how our model realizes the global control of sentence function, we presented frequent words and frequent word patterns within each function. Specifically, we counted the fre- quency of a function-related word in the gener- ated responses.  ordinary and topic words of a generated response with variables and treated each response as a se- quence of function-related words and variables. We then used the Apriori algorithm <ref type="bibr" target="#b0">(Agrawal and Srikant, 1994)</ref> to mine frequent patterns in these sequences. We retained frequent patterns that con- sist of at most 5 words and appear in at least 2% of the generated responses. <ref type="figure" target="#fig_3">Figure 3</ref> presents the most frequent words (the second and third columns) and patterns (the fourth and fifth columns) for each function type. Note that the word patterns can be viewed as an abstract representation of sentence function. We observed that:</p><p>First, function-related words are distributed at multiple positions of a sentence, indicating that realizing a sentence function needs a global con- trol by not only predicting the word types but also planning the words of different types properly.</p><p>Second, the frequent words clearly reveal the difference between function types.</p><p>For in- stance, interrogatives like 什么(what), ？(?) and 吗(particle) are commonly seen in interrogative responses, words like 请(please), 来(come), and 要(will) occur frequently in imperative responses. Further, word patterns in different function types differ significantly (see the fourth/fifth columns), indicating that the model is able to learn function- specific word patterns.</p><p>Third, interrogative and imperative responses have explicit patterns, while declarative responses are more implicit and divergent in pattern. Inter- rogative responses fall into Yes-No, Wh-, How-, and other questions. Imperative responses gener- ally start with the base form of verbs or imperative words including 请(please). Our model succeeds in capturing two typical forms in declarative re- sponses: adversative and progressive structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Case Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Post:</head><p>What would you do if I suddenly broke up with you someday?</p><p>Target Function: Interrogative c-seq2seq: Who are you? MA:</p><p>Should I investigate the conclusion of judicial measure related to the law? KgCVAE:</p><p>Did you miss me? Our Model:</p><p>Do you mean that it's my fault?</p><p>Target Function: Imperative c-seq2seq: I'll be fine! MA:</p><p>Break up with guys. Break up with guys. KgCVAE:</p><p>I will tell you! Our Model:</p><p>Rest assured that I would give your gift to you.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target Function: Declarative</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>c-seq2seq:</head><p>I think so, too. I feel that I'm a good man.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MA:</head><p>I don't know what to do, but I'm in fear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KgCVAE:</head><p>I think I'm a good man, too. Our Model:</p><p>I would think that I was stupid and I would be blamed by my mother.</p><p>Figure 4: Generated responses of all the models for different sentence functions. In the responses of our model, function-related words are in red and topic words in blue. The word type is pre- dicted by the type controller.</p><p>We presented an example in <ref type="figure">Figure 4</ref> to show that our model can generate responses of differ- ent function types better compared to baselines. We can see that each function type can be real- ized by a natural composition of function-related words (in red) and topic words (in blue). More- over, function-related words are different and are placed at different positions across function types, indicating that the model learns function-specific word patterns. These examples also show that the compatibility issue of controlling sentence func- tion and generating informative content is well addressed by planning function-related and topic words properly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Post</head><p>What would you do if I suddenly broke up with you someday?</p><p>Interrogative Response #1 Do you mean that it's my fault?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interrogative</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Response #2</head><p>Can you speak normally?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interrogative</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Response #3</head><p>What do you think I should do? Shall I break up with you?</p><p>Figure 5: Different patterns of interrogative re- sponses generated by our model. Furthermore, we verified the ability of our model to capture fine-grained patterns within a sentence function. We took interrogative re- sponses as example and obtained responses by drawing latent variable samples repeatedly. <ref type="figure">Figure  5</ref> shows interrogative responses with different pat- terns generated by our model given the same post. The model generates several Yes-No questions led by words such as 吗(do), 会(can) and 要(shall), and a Wh-question led by 怎样(what). This ex- ample shows that the latent variable can capture the fine-grained patterns and improve the diversity of responses within a function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We present a model to generate responses with both controllable sentence function and informa- tive content. To deal with the global control of sentence function, we utilize a latent variable to capture the various patterns for different sentence functions. To address the compatibility issue, we devise a type controller to handle function-related and topic words explicitly. The model is thus able to control sentence function and generate infor- mative content simultaneously. Extensive exper- iments show that our model performs better than several state-of-the-art baselines.</p><p>As for future work, we will investigate how to apply the technique to multi-turn conversational systems, provided that the most proper sentence function can be predicted under a given conversa- tion context.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Interrogative What did you have at breakfast? Response Imperative Let's have dinner together! Declarative Me, too. But you ate too much at lunch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Responses with three sentence functions. Function-related words are in red, topic words in blue, and others are ordinary words.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>and α t−1 i scores the relevance between the decoder's state s t−1 and the encoder's state h i (Bahdanau et al., 2015).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Frequent function-related words and frequent patterns containing at least 3 function-related words. The letters denote the variables which replace ordinary and topic words in the generated responses. The underlined words in responses are those occurring in patterns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 .</head><label>2</label><figDesc>The dataset 4 is available at http://coai.cs. tsinghua.edu.cn/hml/dataset.</figDesc><table>Training 

#Post 
1,963,382 

#Response 

Interrogative 618,340 
Declarative 
672,346 
Imperative 
672,696 

Validation 

#Post 
24,034 

#Response 

Interrogative 
7,045 
Declarative 
9,685 
Imperative 
7,304 
Test 
#Post 
6,000 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 : Corpus statistics.</head><label>2</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="2"> http://www.weibo.com</note>

			<note place="foot" n="3"> Though the labels are noisy, the data are sufficient to train a generation model in practice. 4 Note that we strictly obeyed the policies of Weibo and anonymized potential private information in dialogues. This dataset is strictly limited for academic use. 5 https://github.com/tensorflow/tensorflow</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast algorithms for mining association rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakesh</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishnan</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th VLDB Conference</title>
		<meeting>the 20th VLDB Conference</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="487" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sentence types and the formfunction fit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Akmajian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural Language Linguistic Theory</title>
		<imprint>
			<date type="published" when="1984" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nabiha</forename><surname>Asghar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Poupart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Hoey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.03968</idno>
		<title level="m">Affective neural response generation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>the 20th SIGNLL Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="10" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</title>
		<meeting>Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="103" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Word association norms, mutual information, and lexicography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><forename type="middle">Ward</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Hanks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational linguistics</title>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="22" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to generate product reviews from attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="623" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Controlling linguistic style aspects in neural language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Ficler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Stylistic Variation</title>
		<meeting>the Workshop on Stylistic Variation</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="94" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Affect-lm: A neural language model for customizable affective text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sayan</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Laksana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="634" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bidirectional lstm networks for improved phoneme classification and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="799" to="804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Toward controlled generation of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A diversity-promoting objective function for neural conversation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="110" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Stalematebreaker: A proactive content-introducing approach to automatic human-computer conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Joint Conference on Artificial Intelligence</title>
		<meeting>International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2845" to="2851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cicero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sequence to backward and forward sequences: A content-introducing approach to generative short-text conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 26th International Conference on Computational Linguistics</title>
		<meeting>26th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3349" to="3358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to generate market comments from stock prices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soichiro</forename><surname>Murakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiko</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keiichi</forename><surname>Goshima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yanase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroya</forename><surname>Takamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1374" to="1384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On the momentum term in gradient descent learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ning Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="145" to="151" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The complete idiot&apos;s guide to grammar and style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurie</forename><forename type="middle">E</forename><surname>Rozakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<pubPlace>Alpha</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Building end-to-end dialogue systems using generative hierarchical neural network models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Iulian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI conference on Artificial Intelligence</title>
		<meeting>AAAI conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A hierarchical latent variable encoder-decoder model for generating dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Vlad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI conference on Artificial Intelligence</title>
		<meeting>AAAI conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural responding machine for short-text conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1577" to="1586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3483" to="3491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A neural conversational model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Topic aware neural response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI conference on Artificial Intelligence</title>
		<meeting>AAAI conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Strategy and policy learning for nontask-oriented conversational systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">I</forename><surname>Rudnicky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 17th Annual SIGdial Meeting on Discourse and Dialogue</title>
		<meeting>17th Annual SIGdial Meeting on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">The study of language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Yule</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning discourse-level diversity for neural dialog models using conditional variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxine</forename><surname>Eskenazi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Mechanism-aware neural machine for dialogue response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganbin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongyu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI conference on Artificial Intelligence</title>
		<meeting>AAAI conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Emotional chatting machine: Emotional conversation generation with internal and external memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI conference on Artificial Intelligence</title>
		<meeting>AAAI conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianda</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William Yang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04090</idno>
		<title level="m">Mojitalk: Generating emotional responses at scale</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
