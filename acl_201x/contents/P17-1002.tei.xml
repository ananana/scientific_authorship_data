<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:26+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural End-to-End Learning for Computational Argumentation Mining</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Eger</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Daxenberger</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
						</author>
						<title level="a" type="main">Neural End-to-End Learning for Computational Argumentation Mining</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="11" to="22"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1002</idno>
					<note>Ubiquitous Knowledge Processing Lab (UKP-DIPF) German Institute for Educational Research and Educational Information</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We investigate neural techniques for end-to-end computational argumentation mining (AM). We frame AM both as a token-based dependency parsing and as a token-based sequence tagging problem, including a multi-task learning setup. Contrary to models that operate on the argument component level, we find that framing AM as dependency parsing leads to subpar performance results. In contrast, less complex (local) tagging models based on BiL-STMs perform robustly across classification scenarios, being able to catch long-range dependencies inherent to the AM problem. Moreover, we find that jointly learning &apos;natural&apos; subtasks, in a multi-task learning setup, improves performance.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Computational argumentation mining (AM) deals with finding argumentation structures in text. This involves several subtasks, such as: (a) separating argumentative units from non-argumentative units, also called 'component segmentation'; (b) classi- fying argument components into classes such as "Premise" or "Claim"; (c) finding relations be- tween argument components; (d) classifying rela- tions into classes such as "Support" or "Attack" <ref type="bibr" target="#b30">(Persing and Ng, 2016;</ref><ref type="bibr" target="#b41">Stab and Gurevych, 2017)</ref>.</p><p>Thus, AM would have to detect claims and premises (reasons) in texts such as the following, where premise P supports claim C:</p><p>Since it killed many marine lives P , While different research has addressed different subsets of the AM problem (see below), the ul- timate goal is to solve all of them, starting from unannotated plain text. Two recent approaches to this end-to-end learning scenario are <ref type="bibr" target="#b30">Persing and Ng (2016)</ref> and <ref type="bibr" target="#b41">Stab and Gurevych (2017)</ref>. Both solve the end-to-end task by first training indepen- dent models for each subtask and then defining an integer linear programming (ILP) model that en- codes global constraints such as that each premise has a parent, etc. Besides their pipeline architec- ture the approaches also have in common that they heavily rely on hand-crafted features.</p><p>Hand-crafted features pose a problem because AM is to some degree an "arbitrary" problem in that the notion of "argument" critically relies on the underlying argumentation theory ( <ref type="bibr" target="#b32">Reed et al., 2008;</ref><ref type="bibr" target="#b2">Biran and Rambow, 2011;</ref><ref type="bibr" target="#b10">Habernal and Gurevych, 2015;</ref><ref type="bibr" target="#b41">Stab and Gurevych, 2017)</ref>. Ac- cordingly, datasets typically differ with respect to their annotation of (often rather complex) argu- ment structure. Thus, feature sets would have to be manually adapted to and designed for each new sample of data, a challenging task. The same cri- tique applies to the designing of ILP constraints. Moreover, from a machine learning perspective, pipeline approaches are problematic because they solve subtasks independently and thus lead to er- ror propagation rather than exploiting interrela- tionships between variables. In contrast to this, we investigate neural techniques for end-to-end learn- ing in computational AM, which do not require the hand-crafting of features or constraints. The models we survey also all capture some notion of "joint"-rather than "pipeline"-learning. We in- vestigate several approaches.</p><p>First, we frame the end-to-end AM problem as a dependency parsing problem. Dependency parsing may be considered a natural choice for AM, because argument structures often form trees, or closely resemble them (see ยง3). Hence, it is not surprising that 'discourse parsing <ref type="bibr" target="#b23">' (Muller et al., 2012</ref>) has been suggested for AM <ref type="bibr" target="#b26">(Peldszus and Stede, 2015)</ref>. What distinguishes our approach from these previous ones is that we op- erate on the token level, rather than on the level of components, because we address the end-to- end framework and, thus, do not assume that non- argumentative units have already been sorted out and/or that the boundaries of argumentative units are given.</p><p>Second, we frame the problem as a sequence tagging problem. This is a natural choice espe- cially for component identification (segmentation and classification), which is a typical entity recog- nition problem for which BIO tagging is a stan- dard approach, pursued in AM, e.g., by <ref type="bibr" target="#b11">Habernal and Gurevych (2016)</ref>. The challenge in the end-to-end setting is to also include relations into the tagging scheme, which we realize by coding the distances between linked components into the tag label. Since related entities in AM are often- times several dozens of tokens apart from each other, neural sequence tagging models are in prin- ciple ideal candidates for such a framing because they can take into account long-range dependen- cies-something that is inherently difficult to cap- ture with traditional feature-based tagging models such as conditional random fields (CRFs).</p><p>Third, we frame AM as a multi-task (tagging) problem <ref type="bibr" target="#b4">(Caruana, 1997;</ref><ref type="bibr" target="#b7">Collobert and Weston, 2008)</ref>. We experiment with subtasks of AM-e.g., component identification-as auxiliary tasks and investigate whether this improves performance on the AM problem. Adding such subtasks can be seen as analogous to de-coupling, e.g., component identification from the full AM problem.</p><p>Fourth, we evaluate the model of <ref type="bibr" target="#b21">Miwa and Bansal (2016)</ref> that combines sequential (entity) and tree structure (relation) information and is in principle applicable to any problem where the aim is to extract entities and their relations. As such, this model makes fewer assumptions than our de- pendency parsing and tagging approaches.</p><p>The contributions of this paper are as follows.</p><p>(1) We present the first neural end-to-end solu- tions to computational AM. (2) We show that sev- eral of them perform better than the state-of-the- art joint ILP model. (3) We show that a framing of AM as a token-based dependency parsing prob- lem is ineffective-in contrast to what has been proposed for systems that operate on the coarser component level and that (4) a standard neural se- quence tagging model that encodes distance in- formation between components performs robustly in different environments. Finally, (5) we show that a multi-task learning setup where natural sub- tasks of the full AM problem are added as auxil- iary tasks improves performance. <ref type="bibr">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>AM has applications in legal decision making ( <ref type="bibr" target="#b25">Palau and Moens, 2009;</ref><ref type="bibr">Moens et al., 2007)</ref>, doc- ument summarization, and the analysis of scien- tific papers <ref type="bibr" target="#b16">(Kirschner et al., 2015)</ref>. Its importance for the educational domain has been highlighted by recent work on writing assistance <ref type="bibr" target="#b45">(Zhang and Litman, 2016)</ref> and essay scoring ( <ref type="bibr" target="#b29">Persing and Ng, 2015;</ref><ref type="bibr" target="#b39">Somasundaran et al., 2016)</ref>.</p><p>Most works on AM address subtasks of AM such as locating/classifying components ( <ref type="bibr" target="#b9">Florou et al., 2013;</ref><ref type="bibr">Moens et al., 2007;</ref><ref type="bibr" target="#b34">Rooney et al., 2012;</ref><ref type="bibr" target="#b17">Knight et al., 2003;</ref><ref type="bibr" target="#b18">Levy et al., 2014;</ref><ref type="bibr" target="#b33">Rinott et al., 2015</ref>). Relatively few works address the full AM problem of component and relation identifi- cation. <ref type="bibr" target="#b27">Peldszus and Stede (2016)</ref> present a cor- pus of microtexts containing only argumentatively relevant text of controlled complexity. To our best knowledge, <ref type="bibr" target="#b41">Stab and Gurevych (2017)</ref> created the only corpus of attested high quality which anno- tates the AM problem in its entire complexity: it contains token-level annotations of components, their types, as well as relations and their types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data</head><p>We use the dataset of persuasive essays (PE) from <ref type="bibr" target="#b41">Stab and Gurevych (2017)</ref>, which contains student essays written in response to controversial top- ics such as "competition or cooperation-which is better?"  notation distinguishes between major claims (the central position of an author with respect to the es- say's topic), claims (controversial statements that are either for or against the major claims), and premises, which give reasons for claims or other premises and either support or attack them. Over- all, there are 751 major claims, 1506 claims, and 3832 premises. There are 5338 relations, most of which are supporting relations (&gt;90%). The corpus has a special structure, illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>. First, major claims relate to no other components. Second, claims always relate to all other major claims. <ref type="bibr">2</ref> Third, each premise relates to exactly one claim or premise. Thus, the argument structure in each essay is-almost-a tree. Since there may be several major claims, each claim po- tentially connects to multiple targets, violating the tree structure. This poses no problem, however, since we can "loss-lessly" re-link the claims to one of the major claims (e.g., the last major claim in a document) and create a special root node to which the major claims link. From this tree, the actual graph can be uniquely reconstructed.</p><p>There is another peculiarity of this data. Each essay is divided into paragraphs, of which there are 2235 in total. The argumentation structure is completely contained within a paragraph, except, possibly, for the relation from claims to major claims. Paragraphs have an average length of 66 tokens and are therefore much shorter than essays, which have an average length of 368 tokens. Thus, prediction on the paragraph level is easier than 2 All MCs are considered as equivalent in meaning. prediction on the essay level, because there are fewer components in a paragraph and hence fewer possibilities of source and target components in argument relations. The same is true for compo- nent classification: a paragraph can never contain premises only, for example, since premises link to other components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Models</head><p>This section describes our neural network fram- ings for end-to-end AM.</p><p>Sequence Tagging is the problem of assign- ing each element in a stream of input tokens a label. In a neural context, the natural choice for tagging problems are recurrent neural nets (RNNs) in which a hidden vector representation h t at time point t depends on the previous hid- den vector representation h tโ1 and the input x t . In this way, an infinite window ("long-range de- pendencies") around the current input token x t can be taken into account when making an out- put prediction y t . We choose particular RNNs, namely, LSTMs (Hochreiter and Schmidhuber, 1997), which are popular for being able to address vanishing/exploding gradients problems. In addi- tion to considering a left-to-right flow of informa- tion, bidirectional LSTMs (BL) also capture infor- mation to the right of the current input token.</p><p>The most recent generation of neural tagging models add label dependencies to BLs, so that successive output decisions are not made indepen- dently. This class of models is called BiLSTM-CRF (BLC) ( <ref type="bibr" target="#b14">Huang et al., 2015)</ref>. The model of <ref type="bibr" target="#b19">Ma and Hovy (2016)</ref> adds convolutional neural nets (CNNs) on the character-level to BiLSTM- CRFs, leading to BiLSTM-CRF-CNN (BLCC) models. The character-level CNN may address problems of out-of-vocabulary words, that is, words not seen during training.</p><p>AM as Sequence Tagging: We frame AM as the following sequence tagging problem. Each in- put token has an associated label from Y, where</p><formula xml:id="formula_0">Y = {(b, t, d, s) | b โ {B, I, O}, t โ {P, C, MC, โฅ}, d โ {. . . , โ2, โ1, 1, 2, . . . , โฅ}, s โ {Supp, Att, For, Ag, โฅ}}.<label>(1)</label></formula><p>In other words, Y consists of all four-tuples (b, t, d, s) where b is a BIO encoding indicating whether the current token is non-argumentative (O) or begins (B) or continues (I) a component; t indicates the type of the component (claim C, premise P, or major claim MC for our data). More- over, d encodes the distance-measured in num- ber of components-between the current compo- nent and the component it relates to. We encode the same d value for each token in a given compo- nent. Finally, s is the relation type ("stance") be- tween two components and its value may be Sup- port (Supp), Attack (Att), or For or Against (Ag). We also have a special symbol โฅ that indicates when a particular slot is not filled: e.g., a non- argumentative unit (b = O) has neither compo- nent type, nor relation, nor relation type. We refer to this framing as STag T (for "Simple Tagging"), where T refers to the tagger used. For the example from ยง1, our coding would hence be: While the size of the label set Y is potentially infinite, we would expect it to be finite even in a potentially infinitely large data set, because hu- mans also have only finite memory and are there- fore expected to keep related components close in textual space. Indeed, as <ref type="figure" target="#fig_3">Figure 2</ref> shows, in our PE essay data set about 30% of all relations be- tween components have distance โ1, that is, they follow the claim or premise that they attach to. Overall, around 2/3 of all relation distances d lie in {โ2, โ1, 1}. However, the figure also illus- trates that there are indeed long-range dependen- cies: distance values between โ11 and +10 are observed in the data. Multi-Task Learning Recently, there has been a lot of interest in so-called multi-task learning (MTL) scenarios, where several tasks are learned jointly ( <ref type="bibr" target="#b36">Sรธgaard and Goldberg, 2016;</ref><ref type="bibr" target="#b28">Peng and Dredze, 2016;</ref><ref type="bibr" target="#b44">Yang et al., 2016;</ref><ref type="bibr">Rusu et al., 2016;</ref><ref type="bibr" target="#b12">Hรฉctor and Plank, 2017)</ref>. It has been argued that such learning scenarios are closer to human learn- ing because humans often transfer knowledge be- tween several domains/tasks. In a neural context, MTL is typically implemented via weight sharing: several tasks are trained in the same network ar- chitecture, thereby sharing a substantial portion of network's parameters. This forces the network to learn generalized representations.</p><p>In the MTL framework of Sรธgaard and Gold- berg (2016) the underlying model is a BiLSTM with several hidden layers. Then, given differ- ent tasks, each task k 'feeds' from one of the hidden layers in the network. In particular, the hidden states encoded in a specific layer are fed into a multiclass classifier f k . The same work has demonstrated that this MTL protocol may be suc- cessful when there is a hierarchy between tasks and 'lower' tasks feed from lower layers.</p><p>AM as MTL: We use the same framework STag T for modeling AM as MTL. However, we in addition train auxiliary tasks in the network- each with a distinct label set Y .</p><p>Dependency Parsing methods can be classified into graph-based and transition-based approaches <ref type="bibr" target="#b15">(Kiperwasser and Goldberg, 2016)</ref>. Transition- based parsers encode the parsing problem as a sequence of configurations which may be modi- fied by application of actions such as shift, reduce, etc. The system starts with an initial configuration in which sentence elements are on a buffer and a stack, and a classifier successively decides which action to take next, leading to different configura- tions. The system terminates after a finite number of actions, and the parse tree is read off the ter- minal configuration. Graph-based parsers solve a structured prediction problem in which the goal is learning a scoring function over dependency trees such that correct trees are scored above all others.</p><p>Traditional dependency parsers used hand- crafted feature functions that look at "core" ele- ments such as "word on top of the stack", "POS of word on top of the stack", and conjunctions of core features such as "word is X and POS is Y" (see <ref type="bibr" target="#b20">McDonald et al. (2005)</ref>). Most neural parsers have not entirely abandoned feature engineering. Instead, they rely, for example, on encoding the core features of parsers as low-dimensional em- bedding vectors <ref type="bibr" target="#b6">(Chen and Manning, 2014</ref>) but ig- nore feature combinations. Kiperwasser and Gold- berg (2016) design a neural parser that uses only four features: the BiLSTM vector representations of the top 3 items on the stack and the first item on the buffer. In contrast, <ref type="bibr" target="#b8">Dyer et al. (2015)</ref>'s neural parser associates each stack with a "stack LSTM" that encodes their contents. Actions are chosen based on the stack LSTM representations of the stacks, and no more feature engineering is neces- sary. Moreover, their parser has thus access to any part of the input, its history and stack contents.</p><p>AM as Dependency Parsing: To frame a prob- lem as a dependency parsing problem, each in- stance of the problem must be encoded as a di- rected tree, where tokens have heads, which in turn are labeled. For end-to-end AM, we propose the framing illustrated in <ref type="figure" target="#fig_4">Figure 3</ref>. We highlight two design decisions, the remaining are analogous and/or can be read off the figure.</p><p>โข The head of each non-argumentative text to- ken is the document terminating token END, which is a punctuation mark in all our cases. The label of this link is O, the symbol for non-argumentative units.</p><p>โข The head of each token in a premise is the first token of the claim or premise that it links to. The label of each of these links is (b, P, Supp) or (b, P, Att) depending on whether a premise "supports" or "attacks" a claim or premise; b โ {B, I}. LSTM-ER Miwa and Bansal (2016) present a neural end-to-end system for identifying both enti- ties as well as relations between them. Their entity detection system is a BLC-type tagger and their re- lation detection system is a neural net that predicts a relation for each pair of detected entities. This relation module is a TreeLSTM model that makes use of dependency tree information. In addition to de-coupling entity and relation detection but jointly modeling them, 3 pretraining on entities and scheduled sampling ( ) is ap- plied to prevent low performance at early training stages of entity detection and relation classifica- tion. To adapt LSTM-ER for the argument struc- ture encoded in the PE dataset, we model three types of entities (premise, claim, major claim) and four types of relations (for, against, support, at- tack). We use the feature-based ILP model from Stab and Gurevych (2017) as a comparison system. This system solves the subtasks of AM-component segmentation, component clas- sification, relation detection and classification- independently. Afterwards, it defines an ILP model with various constraints to enforce valid ar- gumentation structure. As features it uses struc- tural, lexical, syntactic and context features, cf. <ref type="bibr" target="#b41">Stab and Gurevych (2017)</ref> and <ref type="bibr" target="#b30">Persing and Ng (2016)</ref>.</p><p>Summarizing, we distinguish our framings in terms of modularity and in terms of their con- straints. Modularity: Our dependency parsing framing and LSTM-ER are more modular than STag T because they de-couple relation informa- tion from entity information. However, (part of) this modularity can be regained by using STag T in an MTL setting. Moreover, since entity and re- lation information are considerably different, such a de-coupling may be advantageous. Constraints: LSTM-ER can, in principle, model any kind of- even many-to-many-relationships between de- tected entities. Thus, it is not guaranteed to pro- duce trees, as we observe in AM datasets. STag T also does not need to produce trees, but it more severely restricts search space than does LSTM- ER: each token/component can only relate to one (and not several) other tokens/components. The same constraint is enforced by the dependency parsing framing. All of the tagging modelings, in- cluding LSTM-ER, are local models whereas our parsing framing is a global model: it globally en- forces a tree structure on the token-level.</p><p>Further remarks: (1) part of the TreeLSTM modeling inherent to LSTM-ER is ineffective for our data because this modeling exploits de- pendency tree structures on the sentence level, while relationships between components are al- most never on the sentence level. In our data, roughly 92% of all relationships are between com- ponents that appear in different sentences. Sec- ondly, (2) that a model enforces a constraint does not necessarily mean that it is more suitable for a respective task. It has frequently been observed that models tend to produce output consistent with constraints in their training data in such situations ( <ref type="bibr" target="#b46">Zhang et al., 2017;</ref><ref type="bibr" target="#b12">Hรฉctor and Plank, 2017)</ref>; thus, they have learned the constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>This section presents and discusses the empirical results for the AM framings outlined in ยง4. We relegate issues of pre-trained word embeddings, hyperparameter optimization and further practi- cal issues to the supplementary material. Links to software used as well as some additional error analysis can also be found there.</p><p>Evaluation Metric We adopt the evaluation metric suggested in <ref type="bibr" target="#b30">Persing and Ng (2016)</ref>. This computes true positives TP, false positives FP, and false negatives FN, and from these calculates com- ponent and relation F 1 scores as</p><formula xml:id="formula_1">F 1 = 2TP 2TP+FP+FN .</formula><p>For space reasons, we refer to <ref type="bibr" target="#b30">Persing and Ng (2016)</ref> for specifics, but to illustrate, for compo- nents, true positives are defined as the set of com- ponents in the gold standard for which there ex- ists a predicted component with the same type that 'matches'. <ref type="bibr" target="#b30">Persing and Ng (2016)</ref> define a notion of what we may term 'level ฮฑ matching': for ex- ample, at the 100% level (exact match) predicted and gold components must have exactly the same spans, whereas at the 50% level they must only share at least 50% of their tokens (approximate match). We refer to these scores as C-F1 (100%) and C-F1 (50%), respectively. For relations, an analogous F1 score is determined, which we de- note by R-F1 (100%) and R-F1 (50%). We note that R-F1 scores depend on C-F1 scores because correct relations must have correct arguments. We also define a 'global' F1 score, which is the F1- score of C-F1 and R-F1.</p><p>Most of our results are shown in <ref type="table">Table 2</ref>.</p><p>(a) Dependency Parsing We show results for the two feature-based parsers MST ( <ref type="bibr" target="#b20">McDonald et al., 2005</ref>), Mate (Bohnet and Nivre, 2012) as well as the neural parsers by <ref type="bibr" target="#b8">Dyer et al. (2015)</ref> (LSTM-Parser) and <ref type="bibr" target="#b15">Kiperwasser and Goldberg (2016) (Kiperwasser)</ref>. We train and test all parsers on the paragraph level, because training them on essay level was typically too memory-exhaustive.</p><p>MST mostly labels only non-argumentative units correctly, except for recognizing individ- ual major claims, but never finds their exact spans (e.g., "tourism can create negative impacts on" while the gold major claim is "international tourism can create negative impacts on the des- tination countries"). Mate is slightly better and in particular recognizes several major claims cor- rectly. Kiperwasser performs decently on the ap- proximate match level, but not on exact level. Upon inspection, we find that the parser often pre- dicts 'too large' component spans, e.g., by includ- ing following punctuation. The best parser by far is the LSTM-Parser. It is over 100% better than Kiperwasser on exact spans and still several per- centage points on approximate spans.</p><p>How does performance change when we switch to the essay level? For the LSTM-Parser, the best performance on essay level is 32.84%/47.44% C- F1 (100%/50% level), and 9.11%/14.45% on R- F1, but performance result varied drastically be- tween different parametrizations. Thus, the per- formance drop between paragraph and essay level is in any case immense.</p><p>Since the employed features of modern feature- based parsers are rather general-such as distance between words or word identities-we had ex- pected them to perform much better. The mini-   <ref type="table">C-F1  R-F1  F1  Acc.  C-F1  R-F1  F1  100%  50%  100%  50%  100%  50%  100%  50%  100%  50%  100%  50%</ref> MST-Parser <ref type="bibr">31</ref>  <ref type="table">Table 2</ref>: Performance of dependency parsers, STag BLCC , LSTM-ER and ILP (from top to bottom). The ILP model operates on both levels. Best scores in each column in bold (signific. at p &lt; 0.01; Two-sided Wilcoxon signed rank test, pairing F1 scores for documents). We also report token level accuracy. mal feature set employed by Kiperwasser is appar- ently not sufficient for accurate AM but still a lot more powerful than the hand-crafted feature ap- proaches. We hypothesize that the LSTM-Parser's good performance, relative to the other parsers, is due to its encoding of the whole stack history- rather than just the top elements on the stack as in Kiperwasser-which makes it aware of much larger 'contexts'. While the drop in performance from paragraph to essay level is expected, the LSTM-Parser's deterioration is much more severe than the other models' surveyed below. We believe that this is due to a mixture of the following: (1) 'capacity', i.e., model complexity, of the parsers- that is, risk of overfitting; and (2) few, but very long sequences on essay level-that is, little train- ing data (trees), paired with a huge search space on each train/test instance, namely, the number of possible trees on n tokens. See also our discus- sions below, particularly, our stability analysis.</p><p>(b) Sequence Tagging For these experiments, we use the BLCC tagger from <ref type="bibr" target="#b19">Ma and Hovy (2016)</ref> and refer to the resulting system as STag BLCC . Again, we observe that paragraph level is considerably easier than essay level; e.g., for relations, there is โผ5% points increase from essay to paragraph level. Overall, STag BLCC is โผ13% better than the best parser for C-F1 and โผ11% better for R-F1 on the paragraph level. Our explanation is that taggers are simpler local mod- els, and thus need less training data and are less prone to overfitting. Moreover, they can much bet- ter deal with the long sequences because they are largely invariant to length: e.g., it does in princi- ple not matter, from a parameter estimation per- spective, whether we train our taggers on two se- quences of lengths n and m, respectively, or on one long sequence of length n + m.</p><p>(c) MTL As indicated, we use the MTL tagging framework from Sรธgaard and Goldberg (2016) for multi-task experiments. The underlying tagging framework is weaker than that of BLCC: there is no CNN which can take subword information into account and there are no dependencies between output labels: each tagging prediction is made in- dependently of the other predictions. We refer to this system as STag BL . Accordingly, as <ref type="table">Table 3</ref> shows for the essay level (paragraph level omitted for space reasons), results are generally weaker: For exact match, C-F1 values are about โผ10% points below those of STag BLCC , while approximate match perfor- mances are much closer. Hence, the independence assumptions of the BL tagger apparently lead to more 'local' errors such as exact argument span identification (cf. error analysis). An analogous trend holds for argument relations.</p><p>Additional Tasks: We find that when we train STag BL with only its main task-with label set Y as in Eq. (1)-the overall result is worst. In contrast, when we include the 'natural subtasks' "C" (label set Y C consists of the projection on the coordinates (b, t) in Y) and/or "R" (label set Y R consists of the projection on the coordinates (d, s)), performance increases typically by a few percentage points. This indicates that complex se- quence tagging may benefit when we train a "sub- labeler" in the same neural architecture, a find- ing that may be particularly relevant for morpho- logical POS tagging <ref type="bibr">(Mรผller et al., 2013)</ref>. Un- like <ref type="bibr" target="#b36">Sรธgaard and Goldberg (2016)</ref>, we do not find that the optimal architecture is the one in which "lower" tasks (such as C or R) feed from lower layers. In fact, in one of the best parametrizations the C task and the full task feed from the same layer in the deep BiLSTM. Moreover, we find that the C task is consistently more helpful as an aux- iliary task than the R task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C-F1</head><p>R-F1 F1 100% 50% 100% 50% 100% 50%  <ref type="table">Table 3</ref>: Performance of MTL sequence tagging approaches, essay level. Tasks separated by ":". Layers from which tasks feed are indicated by re- spective numbers.</p><p>On essay level, (d) LSTM-ER performs very well on component identification (+5% C-F1 com- pared to STag BLCC ), but rather poor on relation identification (-18% R-F1). Hence, its overall F1 on essay level is considerably below that of STag BLCC . In contrast, LSTM-ER trained and tested on paragraph level substantially outper- forms all other systems discussed, both for com- ponent as well as for relation identification.</p><p>We think that its generally excellent perfor- mance on components is due to LSTM-ER's de-coupling of component and relation tasks. Our findings indicate that a similar result can be achieved for STag T via MTL when com- ponents and relations are included as auxiliary tasks, cf. <ref type="table">Table 3</ref>. For example, the improve- ment of LSTM-ER over STag BLCC , for C-F1, roughly matches the increase for STag BL when including components and relations separately (Y-3:Y C -3:Y R -3) over not including them as aux- iliary tasks (Y-3). Lastly, the better performance of LSTM-ER over STag BLCC for relations on paragraph level appears to be a consequence of its better performance on components. E.g., when both arguments are correctly predicted, STag BLCC has even higher chance of getting their relation correct than LSTM-ER (95.34% vs. 94.17%).</p><p>Why does LSTM-ER degrade so much on essay level for R-F1? As said, text sequences are much longer on essay level than on paragraph level- hence, there are on average many more entities on essay level. Thus, there are also many more pos- sible relations between all entities discovered in a text-namely, there are O(2 m 2 ) possible relations between m discovered components. Due to its  <ref type="figure">Figure 4</ref>: Probability of correct relation identifica- tion given true distance is |d|.</p><p>generality, LSTM-ER considers all these relations as plausible, while STag T does not (for any of choice of T ): e.g., our coding explicitly constrains each premise to link to exactly one other compo- nent, rather than to 0, . . . , m possible components, as LSTM-ER allows. In addition, our explicit cod- ing of distance values d biases the learner T to re- flect the distribution of distance values found in real essays-namely, that related components are typically close in terms of the number of com- ponents between them. In contrast, LSTM-ER only mildly prefers short-range dependencies over long-range dependencies, cf. <ref type="figure">Figure 4</ref>. The (e) ILP has access to both paragraph and essay level information and thus has always more information than all neural systems compared to. Thus, it also knows in which paragraph in an essay it is. This is useful particularly for major claims, which always occur in first or last paragraphs in our data. Still, its performance is equal to or lower than that of LSTM-ER and STag BLCC when both are evaluated on paragraph level. <ref type="table">Table 4</ref> shows averages and standard deviations of two selected models, namely, the STag BLCC tag- ging framework as well as the LSTM-Parser over several different runs (different random initializa- tions as well as different hyperparameters as dis- cussed in the supplementary material). These re- sults detail that the taggers have lower standard de- viations than the parsers. The difference is partic- ularly striking on the essay level where the parsers often completely fail to learn, that is, their perfor- mance scores are close to 0%. As discussed above, we attribute this to the parsers' increased model capacity relative to the taggers, which makes them more prone to overfitting. Data scarcity is another very likely source of error in this context, as the parsers only observe 322 (though very rich) trees in the training data, while the taggers are always roughly trained on 120K tokens. On paragraph level, they do observe more trees, namely, 1786.  <ref type="table">Table 4</ref>: C-F1 (100%) in % for the two indicated systems; essay vs. paragraph level. Note that the mean performances are lower than the majority performances over the runs given in <ref type="table">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stability Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STagBLCC</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Error analysis</head><p>A systematic source of errors for all systems is de- tecting exact argument spans (segmentation). For instance, the ILP system predicts the following premise: "As a practical epitome , students should be prepared to present in society after their grad- uation", while the gold premise omits the pre- ceding discourse marker, and hence reads: "stu- dents should be prepared to present in society af- ter their graduation". On the one hand, it has been observed that even humans have problems exactly identifying such entity boundaries <ref type="bibr" target="#b30">(Persing and Ng, 2016;</ref><ref type="bibr" target="#b43">Yang and Cardie, 2013</ref>). On the other hand, our results in <ref type="table">Table 2</ref> indicate that the neural taggers BLCC and BLC (in the LSTM- ER model) are much better at such exact identi- fication than either the ILP model or the neural parsers. While the parsers' problems are most likely due to model complexity, we hypothesize that the ILP model's increased error rates stem from a weaker underlying tagging model (feature- based CRF vs. BiLSTM) and/or its features. <ref type="bibr">4</ref> In fact, as <ref type="table">Table 5</ref> shows, the macro-F1 scores 5 on only the component segmentation tasks (BIO la- beling) are substantially higher for both LSTM- ER and STag BLCC than for the ILP model. Note- worthy, the two neural systems even outperform the human upper bound (HUB) in this context, re- ported as 88.6% in <ref type="bibr" target="#b41">Stab and Gurevych (2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We present the first study on neural end-to-end AM. We experimented with different framings,   <ref type="table">Table 5</ref>: F1 scores in % on BIO tagging task.</p><p>such as encoding AM as a dependency parsing problem, as a sequence tagging problem with par- ticular label set, as a multi-task sequence tagging problem, and as a problem with both sequential and tree structure information. We show that (1) neural computational AM is as good or (substan- tially) better than a competing feature-based ILP formulation, while eliminating the need for man- ual feature engineering and costly ILP constraint designing. (2) BiLSTM taggers perform very well for component identification, as demonstrated for our STag T frameworks, for T = BLCC and T = BL, as well as for LSTM-ER (BLC tagger). (3) (Naively) coupling component and relation identi- fication is not optimal, but both tasks should be treated separately, but modeled jointly. (4) Re- lation identification is more difficult: when there are few entities in a text ("short documents"), a more general framework such as that provided in LSTM-ER performs reasonably well. When there are many entities ("long documents"), a more re- strained modeling is preferable. These are also our policy recommendations. Our work yields new state-of-the-art results in end-to-end AM on the PE dataset from <ref type="bibr" target="#b41">Stab and Gurevych (2017)</ref>. Another possible framing, not considered here, is to frame AM as an encoder-decoder problem ( <ref type="bibr" target="#b0">Bahdanau et al., 2015;</ref>. This is an even more general modeling than LSTM-ER. Its suitability for the end-to-end learning task is scope for future work, but its adequacy for com- ponent classification and relation identification has been investigated in <ref type="bibr" target="#b31">Potash et al. (2016)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>:</head><label></label><figDesc>:::::: tourism ::: has :::::::::: threatened :::::: nature C . Argument structures in real texts are typically much more complex, cf. Figure 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Bottom: Linear argumentation structure in a student essay. The essay is comprised of nonargumentative units (square) and argumentative units of different types: Premises (P), claims (C) and major claims (MC). Top: Relationsships between argumentative units. Solid arrows are support (for), dashed arrows are attack (against).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Distribution of distances d between components in PE dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Dependency representation of sample sentence from ยง1. Links and selected labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>STagBLCC</head><label></label><figDesc></figDesc></figure>

			<note place="foot" n="1"> Scripts that document how we ran our experiments are available from https://github.com/UKPLab/ acl2017-neural_end2end_AM.</note>

			<note place="foot" n="3"> By &apos;de-coupling&apos;, we mean that both tasks are treated separately rather than merging entity and relation information in the same tag label (output space). Still, a joint model like that of Miwa and Bansal (2016) de-couples the two tasks in such a way that many model parameters are shared across the tasks, similarly as in MTL.</note>

			<note place="foot" n="4"> The BIO tagging task is independent and thus not affected by the ILP constraints in the model of Stab and Gurevych (2017). The same holds true for the model of Persing and Ng (2016). 5 Denoted FscoreM in Sokolova and Lapalme (2009).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Lucie Flekova, Judith Eckle-Kohler, Nils Reimers, and Christian Stab for valuable feedback and discussions. We also thank the anonymous reviewers for their suggestions. The second author was supported by the German Fed-eral Ministry of Education and Research (BMBF) under the promotional reference 01UG1416B (CEDIFOR).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1171" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Identifying justifications in written dialogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Biran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Rambow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifth IEEE International Conference on Semantic Computing (ICSC)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="162" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A transitionbased system for joint part-of-speech tagging and labeled non-projective dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Association for Computational Linguistics</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1455" to="1465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="75" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<idno type="doi">10.1023/A:1007379606734</idno>
		<ptr target="https://doi.org/10.1023/A:1007379606734" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="doi">10.1145/1390156.1390177</idno>
		<ptr target="https://doi.org/10.1145/1390156.1390177" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning</title>
		<meeting>the 25th International Conference on Machine Learning<address><addrLine>New York, NY, USA, ICML &apos;08</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Transitionbased dependency parsing with stack long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="334" to="343" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Argument extraction for supporting public policy formulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirini</forename><surname>Florou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities. Association for Computational Linguistics</title>
		<meeting>the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities. Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="49" to="54" />
		</imprint>
	</monogr>
	<note>Stasinos Konstantopoulos, Antonis Koukourikos, and Pythagoras Karampiperis</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Exploiting debate portals for semi-supervised argumentation mining in user-generated web discourse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Habernal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2127" to="2137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Argumentation Mining in User-Generated Web Discourse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Habernal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1601.02403" />
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">Preprint</note>
	<note>In press</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">When is multitask learning effective? semantic sequence prediction under varying data conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alonso</forename><surname>Martnez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Hรฉctor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Plank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Proceedings of EACL 2017</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jรผrgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="doi">10.1162/neco.1997.9.8.1735</idno>
		<ptr target="https://doi.org/10.1162/neco.1997.9.8.1735" />
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Bidirectional LSTM-CRF models for sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno>CoRR abs/1508.01991</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Simple and accurate dependency parsing using bidirectional lstm feature representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliyahu</forename><surname>Kiperwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="313" to="327" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Linking the thoughts: Analysis of argumentation structures in scientific publications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Kirschner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judith</forename><surname>Eckle-Kohler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Argumentation Mining held in conjunction with the 2015 Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies (NAACL HLT 2015)</title>
		<meeting>the 2nd Workshop on Argumentation Mining held in conjunction with the 2015 Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies (NAACL HLT 2015)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Finding the write stuff: Automatic identification of discourse structure in student essays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jill</forename><surname>Burstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="32" to="39" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Context dependent claim detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bilu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hershcovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Aharoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Slonim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2014, 25th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-08-23" />
			<biblScope unit="page" from="1489" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">End-to-end sequence labeling via bi-directional lstm-cnns-crf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P16-1101" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1064" to="1074" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Non-projective dependency parsing using spanning tree algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiril</forename><surname>Ribarov</surname></persName>
		</author>
		<idno type="doi">10.3115/1220575.1220641</idno>
		<ptr target="https://doi.org/10.3115/1220575.1220641" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Human Language Technology 20 and Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Human Language Technology 20 and Empirical Methods in Natural Language Processing<address><addrLine>Stroudsburg, PA, USA, HLT &apos;05</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="523" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">End-to-end relation extraction using lstms on sequences and tree structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P16-1105" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1105" to="1116" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Automatic detection of arguments in legal texts</title>
		<idno type="doi">10.1145/1276318.1276362</idno>
		<ptr target="https://doi.org/10.1145/1276318.1276362" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Artificial Intelligence and Law</title>
		<editor>Marie-Francine Moens, Erik Boiy, Raquel Mochales Palau, and Chris Reed</editor>
		<meeting>the 11th International Conference on Artificial Intelligence and Law<address><addrLine>New York, NY, USA, ICAIL &apos;07</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="225" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Constrained decoding for text-level discourse parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stergos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Afantenos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Asher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2012, 24th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers</title>
		<meeting><address><addrLine>Mumbai, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-08-15" />
			<biblScope unit="page" from="1883" to="1900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient higher-order CRFs for morphological tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mรผller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schรผtze</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D13-1032" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="322" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Argumentation mining: The detection, classification and structure of arguments in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><forename type="middle">Mochales</forename><surname>Palau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
		<idno type="doi">10.1145/1568234.1568246</idno>
		<ptr target="https://doi.org/10.1145/1568234.1568246" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Artificial Intelligence and Law</title>
		<meeting>the 12th International Conference on Artificial Intelligence and Law<address><addrLine>New York, NY, USA, ICAIL &apos;09</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="98" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Joint prediction in mst-style discourse parsing for argumentation mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Peldszus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Stede</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D15-1110" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="938" to="948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An annotated corpus of argumentative microtexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Peldszus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Stede</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Argumentation and Reasoned Action: Proceedings of the 1st European Conference on Argumentation. Lisabon</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="801" to="815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Multitask multi-domain representation learning for sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<idno>CoRR abs/1608.02689</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Modeling argument strength in student essays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>Persing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="543" to="552" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">End-to-end argumentation mining in student essays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>Persing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N16-1164" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1384" to="1394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Here&apos;s my point: Argumentation Mining with Pointer Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Potash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Romanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rumshisky</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1612.08994" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Language resources for studying argument</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Mochales-Palau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glenn</forename><surname>Rowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Conference on Language Resources and Evaluation. Marrakech, Morocco, LREC &apos;08</title>
		<meeting>the Sixth International Conference on Language Resources and Evaluation. Marrakech, Morocco, LREC &apos;08</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2613" to="2618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Show me your evidence-an automatic method for context dependent evidence detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruty</forename><surname>Rinott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lena</forename><surname>Dankin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><forename type="middle">Alzate</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitesh</forename><forename type="middle">M</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Aharoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Slonim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-17" />
			<biblScope unit="page" from="440" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Applying kernel methods to argumentation mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Browne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TwentyFifth International FLAIRS Conference</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><forename type="middle">C</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04671</idno>
		<title level="m">Razvan Pascanu, and Raia Hadsell. 2016. Progressive neural networks</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep multi-task learning with low level tasks supervised at lower layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Sรธgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<ptr target="http://anthology.aclweb.org/P16-2038" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="231" to="235" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A systematic analysis of performance measures for classification tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marina</forename><surname>Sokolova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Lapalme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="427" to="437" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<idno type="doi">10.1016/j.ipm.2009.03.002</idno>
		<ptr target="https://doi.org/10.1016/j.ipm.2009.03.002" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Evaluating argumentative and narrative essays using graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swapna</forename><surname>Somasundaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Riordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binod</forename><surname>Gyawali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su-Youn</forename><surname>Yoon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<title level="m">COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers</title>
		<meeting><address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1568" to="1578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Parsing argumentation structures in persuasive essays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Stab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1604.07370" />
	</analytic>
	<monogr>
		<title level="m">Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Joint inference for fine-grained opinion extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P13-1161" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1640" to="1649" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Multi-task cross-lingual sequence tagging from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<idno>CoRR abs/1603.06270</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Using context to predict the purpose of argumentative writing revisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><forename type="middle">J</forename><surname>Litman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1424" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Dependency parsing as head selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL 2017 (long papers). Association for Computational Linguistics</title>
		<meeting>EACL 2017 (long papers). Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
