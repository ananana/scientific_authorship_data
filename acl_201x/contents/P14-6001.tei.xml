<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:52+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gaussian Processes for Natural Language Processing</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2014-06">June 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
							<email>trevor.cohn@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computing and Information Systems</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">The University of Melbourne</orgName>
								<orgName type="institution" key="instit2">The University of Sheffield</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Preot¸iuc</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Introduction</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preot¸iuc-Pietro</forename></persName>
							<affiliation key="aff1">
								<orgName type="department">Introduction</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Lawrence</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Introduction</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Gaussian Processes for Natural Language Processing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: Tutorials</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics: Tutorials <address><addrLine>Baltimore, Maryland, USA, 22</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1" to="3"/>
							<date type="published" when="2014-06">June 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Gaussian Processes (GPs) are a powerful mod- elling framework incorporating kernels and Bayesian inference, and are recognised as state- of-the-art for many machine learning tasks. Despite this, GPs have seen few applications in natural language processing (notwithstanding several recent papers by the authors). We argue that the GP framework offers many benefits over commonly used machine learning frameworks, such as linear models (logistic regression, least squares regression) and support vector machines. Moreover, GPs are extremely flexible and can be incorporated into larger graphical models, forming an important additional tool for proba- bilistic inference. Notably, GPs are one of the few models which support analytic Bayesian in- ference, avoiding the many approximation errors that plague approximate inference techniques in common use for Bayesian models (e.g. MCMC, variational Bayes). 1 GPs accurately model not just the underlying task, but also the uncertainty in the predictions, such that uncertainty can be propagated through pipelines of probabilistic components. Overall, GPs provide an elegant, flexible and simple means of probabilistic infer- ence and are well overdue for consideration of the NLP community.</p><p>This tutorial will focus primarily on regression and classification, both fundamental techniques of wide-spread use in the NLP community. Within NLP, linear models are near ubiquitous, because they provide good results for many tasks, support efficient inference (including dynamic program- ming in structured prediction) and support simple parameter interpretation. However, linear mod- els are inherently limited in the types of relation- ships between variables they can model. Often non-linear methods are required for better under- standing and improved performance. Currently, kernel methods such as Support Vector Machines (SVM) represent a popular choice for non-linear modelling. These suffer from lack of interoper- ability with down-stream processing as part of a larger model, and inflexibility in terms of parame- terisation and associated high cost of hyperparam- eter optimisation. GPs appear similar to SVMs, in that they incorporate kernels, however their prob- abilistic formulation allows for much wider appli- cability in larger graphical models. Moreover, sev- eral properties of Gaussian distributions (closure under integration and Gaussian-Gaussian conju- gacy) means that GP (regression) supports analytic formulations for the posterior and predictive infer- ence.</p><p>This tutorial will cover the basic motivation, ideas and theory of Gaussian Processes and several applications to natural language processing tasks. GPs have been actively researched since the early 2000s, and are now reaching maturity: the fun- damental theory and practice is well understood, and now research is focused into their applica- tions, and improve inference algorithms, e.g., for scaling inference to large and high-dimensional datasets. Several open-source packages (e.g. GPy and GPML) have been developed which allow for GPs to be easily used for many applications. This tutorial aims to promote GPs, emphasising their potential for widespread application across many NLP tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview</head><p>Our goal is to present the main ideas and theory behind Gaussian Processes in order to increase awareness within the NLP community. The first part of the tutorial will focus on the basics of Gaus- sian Processes in the context of regression. The Gaussian Process defines a prior over functions which applied at each input point gives a response 1 value. Given data, we can analytically infer the posterior distribution of these functions assuming Gaussian noise. This tutorial will contrast two main applications settings for regression: interpolation and extrapo- lation. Interpolation suits the use of simple radial basis function kernels which bias towards smooth latent functions. For extrapolation, however, the choice of the kernel is paramount, encoding our prior belief about the type of function wish to learn. We present several different kernels, includ- ing non-stationary and kernels for structured data (string and tree kernels). One of the main issues for kernel methods is setting the hyperparameters, which is often done in the support vector literature using grid search on held-out validation data. In the GP framework, we can compute the probabil- ity of the data given the model which involves the integral over the parameter space. This marginal likelihood or Bayesian evidence can be used for model selection using only training data, where by model selection we refer either to choosing from a set of given covariance kernels or choosing from different model hyperparameters (kernel parame- ters). We will present the key algorithms for type- II maximum likelihood estimation with respect to the hyper-parameters, using gradient ascent on the marginal likelihood.</p><p>Many problems in NLP involve learning from a range of different tasks. We present multi-task learning models by representing intra-task transfer simply and explicitly as a part of a parameterised kernel function. GPs are an extremely flexible probabilistic framework and have been success- fully adapted for multi-task learning, by modelling multiple correlated output variables ( <ref type="bibr" target="#b0">Alvarez et al., 2011</ref>). This literature develops early work from geostatistics (kriging and co-kriging), on learning latent continuous spatio-temporal models from sparse point measurements, a problem set- ting that has clear parallels to transfer learning (in- cluding domain adaptation).</p><p>In the application section, we start by present- ing an open-source software package for GP mod- elling in Python: GPy. <ref type="bibr">2</ref> The first application we approach the regression task of predicting user in- fluence on Twitter based on a range or profile and word features ( <ref type="bibr">Lampos et al., 2014</ref>). We exem- plify how to identifying which features are best for predicting user impact by optimising the hy-perparameters (e.g. RBF kernel length-scales) us- ing Automatic Relevance Determination (ARD). This basically gives a ranking in importance of the features, allowing interpretability of the mod- els. Switching to a multi-task regression setting, we present an application to Machine Translation Quality Estimation. Our method shows large im- provements over previous state-of-the-art <ref type="bibr" target="#b1">(Cohn and Specia, 2013)</ref>. Concepts in automatic kernel selection are exemplified in an extrapolation re- gression setting, where we model word time series in Social Media using different kernels <ref type="bibr">(Preot¸iucPreot¸iucPietro and Cohn, 2013)</ref>. The Bayesian evidence helps to select the most suitable kernel, thus giv- ing an implicit classification of time series.</p><p>In the final section of the tutorial we give a brief overview of advanced topics in the field of GPs. First, we look at non-conjugate likelihoods for modelling classification, count and rank data. This is harder than regression, as Bayesian pos- terior inference can no longer be solved analyti- cally. We will outline strategies for non-conjugate inference, such as expectation propagation and the Laplace approximation. Second, we will outline recent work on scaling GPs to big data using vari- ational inference to induce sparse kernel matrices <ref type="bibr" target="#b2">(Hensman et al., 2013)</ref>. Finally -time permitting -we will finish with unsupervised learning in GPs using the latent variable model <ref type="bibr">(Lawrence, 2004</ref>), a non-linear Bayesian analogue of principle com- ponent analysis.  c) Unsupervised inference with the GP- LVM</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Outline</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Instructors</head><p>Trevor Cohn 3 is a Senior Lecturer and ARC Fu- ture Fellow at the University of Melbourne. His research deals with probabilistic machine learn- ing models, particularly structured prediction and non-parametric Bayesian models. He has recently published several seminal papers on Gaussian Pro- cess models for NLP with applications ranging from translation evaluation to temporal dynamics in social media. Daniel Preot¸iucPreot¸iuc-Pietro 4 is a final year PhD stu- dent in Natural Language Processing at the Uni- versity of Sheffield. His research deals with ap- plying Machine Learning models to model large volumes of data, mostly coming from Social Me- dia. Applications include forecasting future be- haviours of text, users or real world quantities (e.g. political voting intention), user geo-location and impact.</p><p>Neil Lawrence 5 is a Professor at the University of Sheffield. He is one of the foremost experts on Gaussian Processes and non-parametric Bayesian inference, with a long history of publications and innovations in the field, including their application to multi-output scenarios, unsupervised learning, deep networks and scaling to big data. He has been programme chair for top machine learning confer- ences (NIPS, AISTATS), and has run several past tutorials on Gaussian Processes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 .</head><label>1</label><figDesc>GP Regression (60 mins) (a) Weight space view (b) Function space view (c) Kernels 2. NLP Applications (60 mins) (a) Sparse GPs: Predicting user impact (b) Multi-output GPs: Modelling multi- annotator data (c) Model selection: Identifying temporal patterns in word frequencies 3. Further topics (45 mins) (a) Non-congjugate likelihoods: classifica- tion, counts and ranking (b) Scaling GPs to big data: Sparse GPs and stochastic variational inference</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(</head><label></label><figDesc></figDesc></figure>

			<note place="foot" n="1"> This holds for GP regression, but note that approximate inference is needed for non-Gaussian likelihoods.</note>

			<note place="foot" n="2"> http://github.com/SheffieldML/GPy</note>

			<note place="foot" n="3"> http://staffwww.dcs.shef.ac.uk/ people/T.Cohn 4 http://www.preotiuc.ro 5 http://staffwww.dcs.shef.ac.uk/ people/N.Lawrence</note>
		</body>
		<back>
			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Kernels for vector-valued functions: A review. Foundations and Trends in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauricio</forename><forename type="middle">A</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="195" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Modelling annotator bias with multi-task Gaussian processes: an application to machine translation quality estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st annual meeting of the Association for Computational Linguistics, ACL</title>
		<meeting>the 51st annual meeting of the Association for Computational Linguistics, ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Gaussian processes for big data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hensman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolo</forename><surname>Fusi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the 29th Conference on Uncertainty in Artificial Intelligence<address><addrLine>UAI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
