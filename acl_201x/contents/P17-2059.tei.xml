<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:03+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Deep Network with Visual Text Composition Behavior</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
							<email>hongyu.guo@nrc-cnrc.gc.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">National Research Council</orgName>
								<address>
									<addrLine>1200 Montreal Road</addrLine>
									<postCode>K1A 0R6</postCode>
									<settlement>Ottawa</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Deep Network with Visual Text Composition Behavior</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="372" to="377"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-2059</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>While natural languages are composi-tional, how state-of-the-art neural models achieve compositionality is still unclear. We propose a deep network, which not only achieves competitive accuracy for text classification, but also exhibits com-positional behavior. That is, while creating hierarchical representations of a piece of text, such as a sentence, the lower layers of the network distribute their layer-specific attention weights to individual words. In contrast, the higher layers compose meaningful phrases and clauses, whose lengths increase as the networks get deeper until fully composing the sentence.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep neural networks leverage task-specific archi- tectures to develop hierarchical representations of the input, where higher level representations are derived from lower level features ( <ref type="bibr" target="#b3">Conneau et al., 2016)</ref>. Such hierarchical representations have visually demonstrated compositionality in im- age processing, i.e., pixels combine to form shapes and then contours <ref type="bibr" target="#b4">(Farabet et al., 2013;</ref><ref type="bibr" target="#b20">Zeiler and Fergus, 2014</ref>). Natural languages are also compositional, i.e., words combine to form phrases and then sentences. Yet unlike in vision, how deep neural models in NLP, which mainly operate on distributed word embeddings, achieve compositionality, is still unclear ( <ref type="bibr" target="#b11">Li et al., 2015</ref><ref type="bibr" target="#b12">Li et al., , 2016</ref>.</p><p>We propose an Attention Gated Transforma- tion (AGT) network, where each layer's feature generation is gated by a layer-specific attention mechanism ( <ref type="bibr" target="#b0">Bahdanau et al., 2014</ref>). Specifically, through distributing its attention to the original given text, each layer of the networks tends to in- crementally retrieve new words and phrases from the original text. The new knowledge is then com- bined with the previous layer's features to create the current layer's representation, thus resulting in composing longer or new phrases and clauses while creating higher layers' representations of the text.</p><p>Experiments on the Stanford Sentiment Tree- bank ( <ref type="bibr" target="#b14">Socher et al., 2013</ref>) dataset show that the AGT method not only achieves very competitive accuracy, but also exhibits compositional behav- ior via its layer-specific attention. We empirically show that, given a piece of text, e.g., a sentence, the lower layers of the networks select individ- ual words, e.g, negative and conjunction words not and though, while the higher layers aim at composing meaningful phrases and clauses such as negation phrase not so much, where the phrase length increases as the networks get deeper until fully composing the whole sentence. Interestingly, after composing the sentence, the compositions of different sentence phrases compete to become the dominating features of the end task.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Attention Gated Transformation Network</head><p>Our AGT network was inspired by the Highway Networks ( <ref type="bibr">Srivastava et al., 2015a,b)</ref>, where each layer is equipped with a transform gate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Transform Gate for Information Flow</head><p>Consider a feedforward neural network with mul- tiple layers. Each layer l typically applies a non- linear transformation f (e.g., tanh, parameterized by W f l ), on its input, which is the output of the most recent previous layer (i.e., y l−1 ), to produce its output y l . Here, l = 0 indicates the first layer and y 0 is equal to the given input text x, namely y 0 = x:</p><formula xml:id="formula_0">y l = f (y l−1 , W f l )<label>(1)</label></formula><p>While in a highway network (the left column of <ref type="figure" target="#fig_0">Figure 1</ref>), an additional non-linear transform gate function T l is added to the l th (l &gt; 0) layer:</p><formula xml:id="formula_1">y l = f (y l−1 , W f l )T l + y l−1 (1 − T l )<label>(2)</label></formula><p>where the function T l expresses how much of the representation y l is produced by transforming the y l−1 (first term in Equation 2), and how much is just carrying from y l−1 (second term in Equa- tion 2). Here T l is typically defined as:</p><formula xml:id="formula_2">T l = σ(W t l y l−1 + b t l )<label>(3)</label></formula><p>where W t l is the weight matrix and b t l the bias vec- tor; σ is the non-linear activation function.</p><p>With transform gate T , the networks learn to de- cide if a feature transformation is needed at each layer. Suppose σ represents a sigmoid function. In such case, the output of T lies between zero and one. Consequently, when the transform gate is one, the networks pass through the transforma- tion f over y l−1 and block the pass of input y l−1 ; when the gate is zero, the networks pass through the unmodified y l−1 , while the transformation f over y l−1 is suppressed.</p><p>The left column of <ref type="figure" target="#fig_0">Figure 1</ref> reflects the high- way networks as proposed by <ref type="bibr" target="#b17">(Srivastava et al., 2015b</ref>). Our AGT method adds the right two columns of <ref type="figure" target="#fig_0">Figure 1</ref>. That is, 1) the transform gate T l now is not a function of y l−1 , but a function of the selection vector s + l , which is determined by the attention distributed to the given input x by the l th layer (will be discussed next), and 2) the function f takes as input the concatenation of y l−1 and s + l to create feature representation y l . These changes result in an attention gated transformation when forming hierarchical representations of the text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Attention Gated Transformation</head><p>In AGT, the activation of the transform gate at each layer depends on a layer-specific attention mecha- nism. Formally, given a piece of text x, such as a sentence with N words, it can be represented as a matrix B ∈ IR N ×d . Each row of the matrix corre- sponds to one word, which is represented by a d- dimensional vector as provided by a learned word embedding table. Consequently, the selection vec- tor s + l , for the l th layer, is the softmax weighted sum over the N word vectors in B:</p><formula xml:id="formula_3">s + l = N n=1 d l,n B[n : n]<label>(4)</label></formula><p>with the weight (i.e., attention) d l,n computed as:</p><formula xml:id="formula_4">d l,n = exp(m l,n ) N n=1 exp(m l,n )<label>(5)</label></formula><formula xml:id="formula_5">m l,n = w m l tanh(W m l (B[n : n]))<label>(6)</label></formula><p>here, w m l and W m l are the weight vector and weight matrix, respectively. By varying the at- tention weight d l,n , the s + l can focus on different rows of the matrix B, namely different words of the given text x, as illustrated by different color curves connecting to s + in <ref type="figure" target="#fig_0">Figure 1</ref>. Intuitively, one can consider s + as a learned word selection component: choosing different sets of words of the given text x by distributing its distinct attention.</p><p>Having built one s + for each layer from the given text x, the activation of the transform gate for layer l (l &gt; 0) (i.e., Equation 3) is calculated:</p><formula xml:id="formula_6">T l = σ(W t l s + l + b t l )<label>(7)</label></formula><p>To generate feature representation y l , the function f takes as input the concatenation of y l−1 and s + l . That is, Equation 2 becomes:</p><formula xml:id="formula_7">y l = s + l , l = 0 f ([y l−1 ; s + l ], W f l )T l + y l−1 (1 − T l ), l &gt; 0 (8)</formula><p>where [...;...] denotes concatenation. Thus, at each layer l, the gate T l can regulate either pass- ing through y l−1 to form y l , or retrieving novel knowledge from the input text x to augment y l−1 to create a better representation for y l . Finally, as depicted in <ref type="figure" target="#fig_0">Figure 1</ref>, the feature rep- resentation of the last layer of the AGT is fed into two fully connected layers followed by a softmax function to produce a distribution over the possi- ble target classes. For training, we use multi-class cross entropy loss.</p><p>Note that, Equation 8 indicates that the repre- sentation y l depends on both s + l and y l−1 . In other words, although Equation 7 states that the gate ac- tivation at layer l is computed by s + l , the gate acti- vation is also affected by y l−1 , which embeds the information from the layers below l.</p><p>Intuitively, the AGT networks are encouraged to consider new words/phrases from the input text at higher layers. Consider the fact that the s + 0 at the bottom layer of the AGT only deploys a lin- ear transformation of the bag-of-words features. If no new words are used at higher layers of the networks, it will be challenge for the AGT to sufficiently explore different combinations of word sets of the given text, which may be im- portant for building an accurate classifier. In con- trast, through tailoring its attention for new words at different layers, the AGT enables the words selected by a layer to be effectively combined with words/phrases selected by its previous lay- ers to benefit the accuracy of the classification task (more discussions are presented in Section 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Main Results</head><p>The Stanford Sentiment Treebank data contains 11,855 movie reviews ( <ref type="bibr" target="#b14">Socher et al., 2013</ref>). We use the same splits for training, dev, and test data as in <ref type="bibr" target="#b9">(Kim, 2014</ref>) to predict the fine- grained 5-class sentiment categories of the sen- tences. For comparison purposes, following <ref type="bibr" target="#b9">(Kim, 2014;</ref><ref type="bibr" target="#b8">Kalchbrenner et al., 2014;</ref><ref type="bibr" target="#b10">Lei et al., 2015)</ref>, we trained the models using both phrases and sentences, but only evaluate sentences at test time. Also, we initialized all of the word em- beddings ( <ref type="bibr" target="#b2">Cherry and Guo, 2015;</ref><ref type="bibr" target="#b1">Chen and Guo, 2015</ref>) using the 300 dimensional pre-trained vec- tors from GloVe ( <ref type="bibr" target="#b13">Pennington et al., 2014</ref>). We learned 15 layers with 200 dimensions each, which requires us to project the 300 dimensional word vectors; we implemented this using a lin- ear transformation, whose weight matrix and bias term are shared across all words, followed by a tanh activation. For optimization, we used Adadelta <ref type="bibr" target="#b19">(Zeiler, 2012)</ref>, with learning rate of 0.0005, mini-batch of 50, transform gate bias of 1, and dropout ( <ref type="bibr" target="#b15">Srivastava et al., 2014</ref>) rate of 0.2. All these hyperparameters were determined through experiments on the validation-set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AGT</head><p>50    <ref type="table" target="#tab_1">Table 1</ref> indi- cate that the AGT method achieved very competi- tive accuracy (with 50.5%), when compared to the state-of-the-art results obtained by the tree-LSTM (51.0%) ( <ref type="bibr" target="#b18">Tai et al., 2015;</ref><ref type="bibr" target="#b22">Zhu et al., 2015</ref>) and high-order CNN approaches (51.2%) ( <ref type="bibr" target="#b10">Lei et al., 2015)</ref>.</p><p>Top subfigure in <ref type="figure" target="#fig_2">Figure 2</ref> depicts the distribu- <ref type="figure">Figure 3</ref>: Transform gate activities of the test-set (top) and the first sentence in <ref type="figure" target="#fig_5">Figure 4</ref> (bottom). tions of the attention weights created by differ- ent layers on all test data, where the attention weights of all words in a sentence, i.e., d l,n in Equation 4, are normalized to the range between 0 and 1 within the sentence. The figure indicates that AGT generated very spiky attention distribu- tion. That is, most of the attention weights are ei- ther 1 or 0. Based on these narrow, peaked bell curves formed by the normal distributions for the attention weights of 1 and 0, we here consider a word has been selected by the networks if its at- tention weight is larger than 0.95, i.e., receiving more than 95% of the full attention, and a phrase has been composed and selected if a set of consec- utive words all have been selected. In the bottom subfigure of <ref type="figure" target="#fig_2">Figure 2</ref> we present the distribution of the phrase lengths on the test set. This figure indicates that the middle layers of the networks e.g., 8 th and 9 th , have longer phrases (green and blue curves) than others, while the lay- ers at the two ends contain shorter phrases (red and pink curves).</p><p>In <ref type="figure">Figure 3</ref>, we also presented the transform gate activities on all test sentences (top) and that of the first example sentence in <ref type="figure" target="#fig_5">Figure 4</ref> (bottom). These curves suggest that the transform gates at the middle layers (green and blue curves) tended to be close to zero, indicating the pass-through of lower layers' representations. On the contrary, the gates at the two ends (red and pink curves) tended to be away from zero with large tails, implying the retrieval of new knowledge from the input text. These are consistent with the results below. 4 presents three sentences with various lengths from the test set, with the attention weights numbered and then highlighted in heat map. <ref type="figure" target="#fig_5">Fig- ure 4</ref> suggests that the lower layers of the networks selected individual words, while the higher layers aimed at phrases. For example, the first and sec- ond layers seem to select individual words carry- ing strong sentiment (e.g., predictable, bad, never and delicate), and conjunction and negation words (e.g., though and not). Also, meaningful phrases were composed and selected by later layers, such as not so much, not only... but also, bad taste, bad luck, emotional development, and big screen. In addition, in the middle layer, i.e., the 8 th layer, the whole sentences were composed by filtering out uninformative words, resulting in concise ver- sions, as follows (selected words and phrases are highlighted in color blocks).</p><p>1) though plot predictable movie never feels formulaic attention nuances emo- tional development delicate characters 2) bad company leaves bad taste not only bad luck but also staleness script 3) not so much movie picture big screen Interestingly, if relaxing the word selection cri- teria, e.g., including words receiving more than the median, rather than 95%, of the full attention, the sentences recruited more conjunction and modifi- cation words, e.g., because, for, a, its and on, thus becoming more readable and fluent: 1) though plot is predictable movie never feels formulaic because attention is on nuances emotional development delicate characters 2) bad company leaves a bad taste not only because its bad luck timing but also staleness its script 3) not so much a movie a picture book for big screen Now, consider the AGT's compositional behavior for a specific sentence, e.g., the last sentence in <ref type="figure" target="#fig_5">Figure 4</ref>. The first layer solely selected the word not (with attention weight of 1 and all other words with weights close to 0), but the 2 nd to 4 th lay- ers gradually pulled out new words book, screen and movie from the given text. Incrementally, the 5 th and 6 th layers further selected words to form phrases not so much, picture book, and big screen. Finally, the 7 th and 8 th layers added some  <ref type="table" target="#tab_1">Three sentences from the test set and their attention received from the 15 layers (L1 to L15).   though the  plot  is  predicta ,  the movie never feels formula ,  becaus the attentionis  on  the nuance of  the emotionadevelopm of  the delicatecharacte  L1</ref> 0.84 0.00 0.06 0.00 1.00 0.00 0.00 0.00 0.97 0.29 0.93 0.00 0.00 0.00 0.08 0.00 0.00 0.00 0.63 0.00 0.00 0.21 0.00 0.00 0.00 1.00 0.01 L2 0.97 0.00 0.33 0.01 1.00 0.00 0.00 0.00 1.00 0.78 0.99 0.00 0.00 0.00 0.38 0.01 0.00 0.00 0.90 0.00 0.00 0.79 0.02 0.00 0.00 1.00 0.03 L3</p><p>1.00 0.00 0.79 0.03 1.00 0.01 0.00 0.15 1.00 0.97 1.00 0.01 0.05 0.00 0.86 0.03 0.01 0.00 0.99 0.00 0.00 0.98 0.15 0.00 0.00 1.00 0.16 L4</p><p>1.00 0.01 0.98 0.29 1.00 0.00 0.01 0.98 1.00 1.00 1.00 0.00 0.27 0.01 0.99 0.29 0.08 0.01 1.00 0.02 0.01 1.00 0.53 0.02 0.01 1.00 0.74 L5</p><p>1.00 0.03 0.99 0.49 1.00 0.01 0.03 0.99 1.00 1.00 1.00 0.01 0.44 0.03 0.99 0.49 0.16 0.03 1.00 0.04 0.03 1.00 0.70 0.04 0.03 1.00 0.84 L6</p><p>1.00 0.06 0.99 0.71 1.00 0.02 0.06 1.00 1.00 1.00 1.00 0.02 0.66 0.06 1.00 0.71 0.32 0.06 1.00 0.07 0.06 1.00 0.84 0.07 0.06 1.00 0.92 L7</p><p>1.00 0.08 1.00 0.83 1.00 0.03 0.08 1.00 1.00 1.00 1.00 0.03 0.76 0.08 1.00 0.83 0.44 0.08 1.00 0.10 0.08 1.00 0.90 0.10 0.08 1.00 0.95 L8</p><p>1.00 0.10 1.00 0.94 1.00 0.04 0.10 1.00 1.00 1.00 1.00 0.04 0.85 0.10 1.00 0.94 0.60 0.10 1.00 0.12 0.10 1.00 0.95 0.12 0.10 1.00 0.98 L9</p><p>1.00 0.05 1.00 0.94 1.00 0.02 0.05 1.00 1.00 1.00 1.00 0.02 0.79 0.05 1.00 0.94 0.49 0.05 1.00 0.07 0.05 1.00 0.92 0.07 0.05 1.00 0.98 L10</p><p>1.00 0.00 0.99 0.81 1.00 0.00 0.00 1.00 1.00 1.00 1.00 0.00 0.44 0.00 1.00 0.81 0.15 0.00 1.00 0.01 0.00 1.00 0.75 0.01 0.00 1.00 0.93 L11</p><p>1.00 0.00 0.99 0.68 1.00 0.00 0.00 1.00 1.00 1.00 1.00 0.00 0.24 0.00 1.00 0.68 0.07 0.00 1.00 0.01 0.00 1.00 0.59 0.01 0.00 1.00 0.88 L12 0.99 0.00 0.98 0.51 1.00 0.01 0.00 0.99 1.00 1.00 1.00 0.01 0.08 0.00 0.99 0.51 0.03 0.00 0.99 0.00 0.00 1.00 0.42 0.00 0.00 1.00 0.81 L13 0.99 0.00 0.90 0.17 1.00 0.01 0.00 0.62 1.00 0.98 1.00 0.01 0.02 0.00 0.98 0.17 0.02 0.00 0.99 0.00 0.00 1.00 0.18 0.00 0.00 1.00 0.43 L14 0.92 0.00 0.65 0.04 1.00 0.00 0.00 0.04 1.00 0.93 0.99 0.00 0.00 0.00 0.88 0.04 0.00 0.00 0.96 0.00 0.00 0.99 0.05 0.00 0.00 1.00 0.14 L15 0.80 0.00 0.60 0.03 1.00 0.00 0.00 0.02 0.99 0.91 0.99 0.00 0.00 0.00 0.84 0.03 0.00 0.00 0.93 0.00 0.00 0.98 0.04 0.00 0.00 1. conjunction and quantification words a and for to make the sentence more fluent. This recursive composing process resulted in the sentence "not so much a movie a picture book for big screen". Interestingly, Figures 4 and 2 also imply that, after composing the sentences by the middle layer, the AGT networks shifted to re-focus on shorter phrases and informative words. Our analysis on the transform gate activities suggests that, dur- ing this re-focusing stage the compositions of sen- tence phrases competed to each others, as well as to the whole sentence composition, for the domi- nating task-specific features to represent the text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Further Observations</head><p>As discussed at the end of Section 2.2, intuitively, including new words at different layers allows the networks to more effectively explore different combinations of word sets of the given text than that of using all words only at the bottom layer of the networks. Empirically, we observed that, if with only s + 0 in the AGT network, namely remov- ing s + i for i &gt; 0, the test-set accuracy dropped from 50.5% to 48.5%. In other words, transform- ing a linear combination of the bag-of-words fea- tures was insufficient for obtaining sufficient ac- curacy for the classification task. For instance, if being augmented with two more selection vectors s + i , namely removing s + i for i &gt; 2, the AGT was able to improve its accuracy to 49.0%. Also, we observed that the AGT networks tended to select informative words at the lower layers. This may be caused by the recursive form of Equation 8, which suggests that the words re- trieved by s + 0 have more chance to combine with and influence the selection of other feature words. In our study, we found that, for example, the top 3 most frequent words selected by the first layer of the AGT networks were all negation words: n't, never, and not. These are important words for sen- timent classification ( <ref type="bibr" target="#b21">Zhu et al., 2014</ref>).</p><p>In addition, like the transform gate in the High- way networks <ref type="bibr" target="#b16">(Srivastava et al., 2015a</ref>) and the forget gate in the LSTM ( <ref type="bibr" target="#b5">Gers et al., 2000</ref>), the attention-based transform gate in the AGT net- works is sensitive to its bias initialization. We found that initializing the bias to one encouraged the compositional behavior of the AGT networks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An AGT network with three layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Soft attention distribution (top) and phrase length distribution (bottom) on the test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure</head><label></label><figDesc>Figure 4 presents three sentences with various lengths from the test set, with the attention weights numbered and then highlighted in heat map. Figure 4 suggests that the lower layers of the networks selected individual words, while the higher layers aimed at phrases. For example, the first and second layers seem to select individual words carrying strong sentiment (e.g., predictable, bad, never and delicate), and conjunction and negation words (e.g., though and not). Also, meaningful phrases were composed and selected by later layers, such as not so much, not only... but also, bad taste, bad luck, emotional development, and big screen. In addition, in the middle layer, i.e., the 8 th layer, the whole sentences were composed by filtering out uninformative words, resulting in concise versions, as follows (selected words and phrases are highlighted in color blocks).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Three sentences from the test set and their attention received from the 15 layers (L1 to L15).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 presents</head><label>1</label><figDesc>the test-set accuracies obtained by different strategies. Results in</figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Future Work</head><p>We have presented a novel deep network. It not only achieves very competitive accuracy for text classification, but also exhibits interesting text compositional behavior, which may shed light on understanding how neural models work in NLP tasks. In the future, we aim to apply the AGT networks to incrementally generating natu-ral text <ref type="bibr" target="#b6">(Guo, 2015;</ref><ref type="bibr" target="#b7">Hu et al., 2017</ref>).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Representation based translation evaluation metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="150" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of word representations for twitter named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="735" to="745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo¨ıclo¨ıc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno>CoRR abs/1606.01781</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clément</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1915" to="1929" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to forget: Continual prediction with lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><forename type="middle">A</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><forename type="middle">A</forename><surname>Cummins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2451" to="2471" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generating text with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS2015 Deep Reinforcement Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00955</idno>
		<title level="m">Controllable text generation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno>CoRR abs/1404.2188</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno>CoRR abs/1408.5882</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Molding cnns for text: non-linear, nonconsecutive convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno>CoRR abs/1508.04112</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Visualizing and understanding neural models in NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno>CoRR abs/1506.01066</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.08220</idno>
		<title level="m">Understanding neural networks through representation erasure</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP &apos;13</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh Kumar</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno>abs/1505.00387</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Training very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh Kumar</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems. NIPS&apos;15</title>
		<meeting>the 28th International Conference on Neural Information Processing Systems. NIPS&apos;15</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2377" to="2385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno>CoRR abs/1503.00075</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">ADADELTA: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno>CoRR abs/1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014-13th European Conference</title>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-09-06" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An empirical study on the effect of negation words on sentiment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saif</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Kiritchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="304" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Long short-term memory over recursive structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parinaz</forename><surname>Sobhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1604" to="1612" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
