<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:03+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generating Steganographic Text with LSTMs</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tina</forename><surname>Fang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Argyraki</surname></persName>
						</author>
						<title level="a" type="main">Generating Steganographic Text with LSTMs</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of ACL 2017, Student Research Workshop</title>
						<meeting>ACL 2017, Student Research Workshop <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="100" to="106"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-3017</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivated by concerns for user privacy , we design a steganographic system (&quot;stegosystem&quot;) that enables two users to exchange encrypted messages without an adversary detecting that such an exchange is taking place. We propose a new linguistic stegosystem based on a Long Short-Term Memory (LSTM) neural network. We demonstrate our approach on the Twit-ter and Enron email datasets and show that it yields high-quality steganographic text while significantly improving capacity (encrypted bits per word) relative to the state-of-the-art.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The business model behind modern communica- tion systems (email services or messaging services provided by social networks) is incompatible with end-to-end message encryption. The providers of these services can afford to offer them free of charge because most of their users agree to receive "targeted ads" (ads that are especially chosen to appeal to each user, based on the needs the user has implied through their messages). This model works as long as users communicate mostly in the clear, which enables service providers to make in- formed guesses about user needs.</p><p>This situation does not prevent users from en- crypting a few sensitive messages, but it does take away some of the benefits of confidentiality. For instance, imagine a scenario where two users want to exchange forbidden ideas or organize forbidden events under an authoritarian regime; in a world where most communication happens in the clear, encrypting a small fraction of messages automat- ically makes these messages-and the users who exchange them-suspicious.</p><p>With this motivation in mind, we want to de- sign a system that enables two users to exchange encrypted messages, such that a passive adversary that reads the messages can determine neither the original content of the messages nor the fact that the messages are encrypted.</p><p>We build on linguistic steganography, i.e., the science of encoding a secret piece of informa- tion ("payload") into a piece of text that looks like natural language ("stegotext"). We propose a novel stegosystem, based on a neural network, and demonstrate that it combines high quality of output (i.e., the stegotext indeed looks like natu- ral language) with the highest capacity (number of bits encrypted per word) published in literature.</p><p>In the rest of the paper, we describe existing lin- guistic stegosystems along with ours ( §2), provide details on our system ( §3), present preliminary ex- perimental results on Twitter and email messages ( §4), and conclude with future directions ( §5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Linguistic Steganography</head><p>In this section, we summarize related work ( §2.1), then present out proposal ( §2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Related Work</head><p>Traditional linguistic stegosystems are based on modification of an existing cover text, e.g., us- ing synonym substitution ( <ref type="bibr" target="#b20">Topkara et al., 2006;</ref><ref type="bibr" target="#b3">Chang and Clark, 2014</ref>) and/or paraphrase sub- stitution ( <ref type="bibr" target="#b1">Chang and Clark, 2010)</ref>. The idea is to encode the secret information in the transfor- mation of the cover text, ideally without affect- ing its meaning or grammatical correctness. Of these systems, the most closely related to ours is CoverTweet ( <ref type="bibr" target="#b23">Wilson et al., 2014</ref>), a state-of-the- art cover modification stegosystem that uses Twit- ter as the medium of cover; we compare to it in our preliminary evaluation ( §4).</p><p>Cover modification can introduce syntactic and semantic unnaturalness ( <ref type="bibr" target="#b5">Grosvald and Orgun, 2011)</ref>; to address this, Grovsald and Orgun pro- posed an alternative stegosystem where a human generates the stegotext manually, thus improving linguistic naturalness at the cost of human ef- fort ( <ref type="bibr" target="#b5">Grosvald and Orgun, 2011</ref>).</p><p>Matryoshka ( <ref type="bibr" target="#b16">Safaka et al., 2016</ref>) takes this fur- ther: in step 1, it generates candidate stegotext au- tomatically based on an n-gram model of the En- glish language; in step 2, it presents the candidate stegotext to the human user for polishing, i.e., ide- ally small edits that improve linguistic naturalness. However, the cost of human effort is still high, be- cause the (automatically generated) candidate ste- gotext is far from natural language, and, as a re- sult, the human user has to spend significant time and effort manually editing and augmenting it.</p><p>Volkhonskiy et al. have applied Generative Ad- versarial Networks ( <ref type="bibr" target="#b4">Goodfellow et al., 2014</ref>) to image steganography ( <ref type="bibr" target="#b21">Volkhonskiy et al., 2017</ref>), but we are not aware of any text stegosystem based on neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Our Proposal: Steganographic LSTM</head><p>Motivated by the fact that LSTMs <ref type="bibr" target="#b6">(Hochreiter and Schmidhuber, 1997</ref>) constitute the state of the art in text generation ( <ref type="bibr" target="#b7">Jozefowicz et al., 2016)</ref>, we propose to automatically generate the stegotext from an LSTM (as opposed to an n-gram model). The output of the LSTM can then be used either directly as the stegotext, or Matryoshka-style, i.e., as a candidate stegotext to be polished by a hu- man user; in this paper, we explore only the for- mer option, i.e., we do not do any manual polish- ing. We describe the main components of our sys- tem in the paragraphs below; for reference, <ref type="figure" target="#fig_0">Fig. 1</ref> outlines the building blocks of a stegosystem <ref type="bibr" target="#b17">(Salomon, 2003</ref>). Secret data. The secret data is the information we want to hide. First, we compress and/or en- crypt the secret data (e.g., in the simplest set- ting using the ASCII coding map) into a secret- containing bit string S. Second, we divide S into smaller bit blocks of length |B|, resulting in a total of |S|/|B| 1 bit blocks. For example, if S = 100001 and |B| = 2, our bit-block sequence is 10, 00, 01. Based on this bit-block sequence, our steganographic LSTM generates words.</p><p>Key. The sender and receiver share a key that maps bit blocks to token sets and is constructed as follows: We start from the vocabulary, which is the set of all possible tokens that may appear in the stegotext; the tokens are typically words, but may also be punctuation marks. We partition the vocabulary into 2 |B| bins, i.e., disjoint token sets, randomly selected from the vocabulary without re- placement; each token appears in exactly one bin, and each bin contains |V |/2 |B| tokens. We map each bit block B to a bin, denoted by W B . This mapping constitutes the shared key.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bit Block</head><p>Tokens 00</p><p>This, am, weather, ... 01 was, attaching, today, ... 10 I, better, an, Great, ... 11</p><p>great, than, NDA, ., ... <ref type="table">Table 1</ref>: Example shared key.</p><p>Embedding algorithm. The embedding algo- rithm uses a modified word-level LSTM for lan- guage modeling ( <ref type="bibr" target="#b11">Mikolov et al., 2010)</ref>. To encode the secret-containing bit string S, we consider one bit block B at a time and have our LSTM select one token from bin W B ; hence, the candidate ste- gotext has as many tokens as the number of bit blocks in S. Even though we restrict the LSTM to select a token from a particular bin, each bin should offer sufficient variety of tokens, allowing the LSTM to generate text that looks natural. For example, given the bit string "1000011011" and the key in <ref type="table">Table 1</ref>, the LSTM can form the partial sentence in <ref type="table">Table 2</ref>. We describe our LSTM model in more detail in the next section.</p><p>Bit String 10 00 01 10 11 Token I am attaching an NDA <ref type="table">Table 2</ref>: Example stegotext generation.</p><p>Decoder. The decoder recovers the original data deterministically and in a straightforward manner: it takes as input the generated stegotext, considers one token at a time, finds the token's bin in the shared key, and recovers the original bit block.</p><p>Common-token variant. We also explore a variant where we add a set of common tokens, C, to all bins. These common tokens do not carry any secret information; they serve only to enhance stegotext naturalness. When the LSTM selects a common token from a bin, we have it select an ex- tra token from the same bin, until it selects a non- common token. The decoder removes all common tokens before decoding. We discuss the choice of common tokens and its implication on our sys- tem's performance in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Steganographic LSTM Model</head><p>In this section, we provide more details on our sys- tem: how we modify the LSTM ( §3.1) and how we evaluate its output ( §3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">LSTM Modification</head><p>Text generation in classic LSTM. Classic LSTMs generate words as follows (Sutskever et al., 2011): Given a word sequence (x 1 , x 2 , . . . , x T ), the model has hidden states (h 1 , . . . , h T ), and resulting output vectors (o 1 , . . . , o T ). Each output vector o t has length |V |, and each output-vector element o (j) t is the unnormalized probability of word j in the vocabulary. Normalized probabilities for each candidate word are obtained by the following softmax activation function:</p><formula xml:id="formula_0">sof tmax(o t ) j := exp(o (j) t ) k exp(o (k) t ).</formula><p>The LSTM then selects the word with the highest probability P [x t+1 | x ≤t ] as its next word.</p><p>Text generation in our LSTM. In our stegano- graphic LSTM, word selection is restricted by the shared key. That is, given bit block B, the LSTM has to select its next word from bin W B . We set P [x = w j ] = 0 for j / ∈ W B , so that the multi- nomial softmax function selects the word with the highest probability within W B .</p><p>Common tokens. In the common-token variant, we restrict P [x = w j ] = 0 only for j / ∈ (W B ∪C), where C is the set of common tokens added to all bins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation Metrics</head><p>We use perplexity to quantify stegotext quality; and capacity (i.e., encrypted bits per output word) to quantify its efficiency in carrying secret infor- mation. In Section 4, we also discuss our stegotext quality as empirically perceived by us as human readers.</p><p>Perplexity. Perplexity is a standard metric for the quality of language models (Martin and Ju- rafsky, 2000), and it is defined as the average per-word log-probability on the valid data set: Instead, we measure the probability of w i by taking the average of p[w i ] over all possible secret bit blocks B, under the assumption that bit blocks are distributed uniformly. By the Law of Large Numbers <ref type="bibr">(Révész, 2014</ref>), if we perform many stegotext-generating trials using different random secret data as input, the probability of each word will tend to the expected value,</p><formula xml:id="formula_1">exp(−1/N i ln p[w i ]) (</formula><formula xml:id="formula_2">p[w i , B]/2 |B| , Hence, we set p[w i ] := p[w i , B]/2 |B| instead of p[w i ] = 0 for w i / ∈ W B .</formula><p>Capacity. Our system's capacity is the num- ber of encrypted bits per output word. Without common tokens, capacity is always |B| bits/word (since each bit block of size |B| is always mapped to one output word). In the common-token variant, capacity decreases because the output includes common tokens that do not carry any secret infor- mation; in particular, if the fraction of common tokens is p, then capacity is (1 − p) · |B|.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we present our preliminary experi- mental evaluation: our Twitter and email datasets ( §4.1), details about the LSTMs used to produce our results ( §4.2), and finally a discussion of our results ( §4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Tweets and emails are among the most popular media of open communication and therefore pro- vide very realistic environments for hiding infor- mation. We thus trained our LSTMs on those two domains, Twitter messages and Enron emails <ref type="bibr" target="#b8">(Klimt and Yang, 2004</ref>), which vary greatly in message length and vocabulary size. For Twitter, we used the NLTK tokenizer to tok- enize tweets <ref type="bibr" target="#b0">(Bird, 2006</ref>) into words and punctua- tion marks. We normalized the content by replac- ing usernames and URLs with a username token (&lt;user&gt;) and a URL token (&lt;url&gt;), respectively. We used 600 thousand tweets with a total of 45 million words and a vocabulary of size 225 thou- sand.</p><p>For Enron, we cleaned and extracted email mes- sage bodies ( <ref type="bibr" target="#b25">Zhou et al., 2007</ref>) from the Enron dataset, and we tokenized the messages into words and punctuation marks. We took the first 100MB of the resulting messages, with 16.8 million tokens and a vocabulary size of 406 thousand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We implemented multi-layered LSTMs based on PyTorch 2 in both experiments. We did not use pre- trained word embeddings ( <ref type="bibr" target="#b14">Pennington et al., 2014</ref>), and instead trained word embeddings of dimension 200 from scratch.</p><p>We optimized with Stochastic Gradient Descent and used a batch size of 20. The initial learning rate was 20 and the decay factor per epoch was 4. The learning rate decay occurred only when the validation loss did not improve. Model training was done on an NVIDIA GeForce GTX TITAN X.</p><p>For Twitter, we used a 2-layer LSTM with 600 units, unrolled for 25 steps for back propagation. We clipped the norm of the gradients ( <ref type="bibr" target="#b13">Pascanu et al., 2013</ref>) at 0.25 and applied 20% dropout <ref type="bibr" target="#b18">(Srivastava et al., 2014</ref>). We stopped the training after 12 epochs (10 hours) based on validation loss con- vergence.</p><p>For Enron, we used a 3-layer LSTM with 600 units and no regularization. We unrolled the net- work for 20 steps for back propagation. We stopped the training after 6 epochs (2 days).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Tweets</head><p>We evaluate resulting tweets generated by LSTMs of 1 (non-steganographic), 2, 4, 8 bins. Fur- thermore, we found empirically that adding 10 most frequent tokens from the Twitter corpus was enough to significantly improve the grammati- cal correctness and semantic reasonableness of tweets. <ref type="table">Table 3</ref> shows the relationship between ca- pacity (bits per word), and quantitative text qual- ity (perplexity). It also compares models with and without adding common tokens using perplexity and bits per word. <ref type="table">Table 4</ref> shows example output texts of LSTMs with and without common tokens added. To re- flect the variation in the quality of the tweets, we represent tweets that are good and poor in quality <ref type="bibr">3</ref> .</p><p>We replaced &lt;user&gt; generated by the LSTM with mock usernames for a more realistic presen- tation in <ref type="table">Table 4</ref>. In practice, we can replace the &lt;user&gt; tokens systematically, randomly selecting followers or followees of that tweet sender, for ex- ample.</p><p>Re-tweet messages starting with "RT" can also be problematic, because it will be easy to check whether the original message of the retweeted message exists. A simple approach to deal with this is to eliminate "RT" messages from training (or at generation). Finally, since we made all tweets lower case in the pre-processing step, we can also post-process tweets to adhere to proper English capitalization rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original</head><p>Common  <ref type="table">Table 3</ref>: An increase of of capacity correlates with an increase of perplexity, which implies that there is a negative correlation between capacity and text quality. After adding common tokens, there is a significant reduction in perplexity (ppl), at the expense of a lower capacity (bits per word).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Emails</head><p>We also tested email generation, and <ref type="table" target="#tab_2">Table 5</ref> shows sample email passages 4 from each bin. We post-processed the emails with untokenization of punctuations.</p><p>The biggest difference between emails and tweets is that emails have a much longer range for   good: i was just looking for someone that i used have. poor: cry and speak! rt @user421: relatable per- sonal hygiene for body and making bad things as a best friend in lifee good: i'm happy with you. i'll take a pic poor: rt: cut your hair, the smallest things get to the body.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4</head><p>good: @user390 looool yeah she likes me then ;). you did? poor: "where else were u making?... i feel fine? -e? lol" * does a voice for me &amp; take it to walmart?</p><p>good: i just wanna move. collapses. poor: i hate being contemplating for something i want to.</p><p>8 good: @user239 hahah. sorry that my bf is amazing because i'm a bad influence ;). poor: so happy this to have been working my ass and they already took the perfect. but it's just cause you're too busy the slows out! love... * dancing on her face, holding two count out cold * ( a link with a roof on punishment... -please :) good: i hate the smell of my house. poor: a few simple i can't. i need to make my specs jump surprisingly. <ref type="table">Table 4</ref>: We observe that the model with common tokens produces tweets simpler in style, and uses more words from the set of common tokens. There is a large improvement in grammatical correctness and context coherence after adding common tokens, especially in the "poor" examples. For example, adding the line break token reduced the length of the tweet generated from the 8-bin LSTM.</p><p>context dependency, with context spanning sen- tences and paragraphs. This is challenging to model even for the non-steganographic LSTM. Once the long-range context dependency of the non-steganographic LSTM improves, the context dependency of the steganographic LSTMs should also improve. If you do like to comment on the above you will not contact me at the above time by 8:00 a.m. on Monday, March 13 and July 16 and Tuesday, May 13 and Tuesday, March 9 - Thursday, June 17, -9:00 to 11:30 AM. 4</p><p>At a moment when my group was working for a few weeks, we were able to get more flexibility through in order that we would not be willing. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with Other Stegosystems</head><p>For all comparisons, we use our 4-bin model with no common tokens added. Our model significantly improves the state- of-the-art capacity. Cover modification based stegosystems hide 1-2 bits per sentence <ref type="bibr" target="#b2">(Chang and Clark, 2012)</ref>. The state-of-the-art Twitter stegosystem hides 2.8 bits of per tweet <ref type="bibr" target="#b24">(Wilson and Ker, 2016)</ref>. Assuming 16.04 words per tweet 5 , our 4-bin system hides 32 bits per tweet, over 11 times higher than <ref type="bibr" target="#b24">(Wilson and Ker, 2016)</ref>.</p><p>We hypothesize that the subjective quality of our generated tweets will be comparable to tweets produced by <ref type="bibr">CoverTweet (2014)</ref>. We present some examples 6 in <ref type="table" target="#tab_3">Table 6</ref> to show there is po- tential for a comparison. This contrasts the previ- ous conception that cover generation methods are fatally weak against human judges ( <ref type="bibr" target="#b23">Wilson et al., 2014)</ref>. CoverTweet was tested to be secure against human judges. Formal experiments will be nec- essary to establish that our system is also secure against human judges. <ref type="bibr">CoverTweet (2014)</ref> Steganographic LSTM yall must have 11:11 set 1 minute early before yall tweet it, because soon as 11:11 hit yall don't wastes no time. lol i wanna go to sleep in the gym, ny in peoples houses &amp; i'm in the gym..! :(( you can tell when some- body hating on you! i would rather marry a reg- ular sunday!! most of the people who got mouth can't beat you. my mom is going so hard to get his jam. Our system also offers flexibility for the user to freely trade-off capacity and text quality. Though we chose the 4-bin model with no common tokens for comparison, user can choose to use more bins to achieve an even higher capacity, or use less bins and add common tokens to increase text quality. This is not the case with existing cover modifi- cation systems, where capacity is bounded above by the number of transformation options <ref type="bibr" target="#b23">(Wilson et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this paper, we opened a new application of LSTMs, namely, steganographic text generation. We presented our steganographic model based on existing language modeling LSTMs, and demon- strated that our model produces realistic tweets and emails while hiding information.</p><p>In comparison to the state-of-the-art stegano- graphic systems, our system has the advantage of encoding much more information (around 2 bits per word). This advantage makes the system more usable and scalable in practice.</p><p>In future work, we will formally evaluate our system's security against human judges and other steganography detection (steganalysis) methods <ref type="bibr" target="#b22">(Wilson et al., 2015;</ref><ref type="bibr" target="#b9">Kodovsky et al., 2012</ref>). When evaluated against an automated classifier, the setup becomes that of a Generative Adversarial Network ( <ref type="bibr" target="#b4">Goodfellow et al., 2014)</ref>, though with additional conditions for the generator (the secret bits) which are unknown to the discriminator, and not neces- sarily employing joint training. Another line of future research is to generate tweets which are per- sonalized to a user type or interest group, instead of reflecting all twitter users. Furthermore, we plan to explore if capacity can be improved even more by using probabilistic encoders/decoders, as e.g. in Matryoshka ( <ref type="bibr">Safaka et al., 2016, Section 4)</ref>.</p><p>Ultimately, we aim to open-source our stegosys- tem so that users of open communication systems (e.g. Twitter, emails) can use our stegosystem to communicate private and sensitive information.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Stegosystem building blocks.</figDesc><graphic url="image-1.png" coords="2,72.00,601.67,218.27,66.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2</head><label>2</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>#</head><label></label><figDesc>of Bins Sample Email 1 --Original Message--From: Nelson, Michelle Sent: Thursday, January 03, 2002 3:35 PM To: Maggi, Mike Subject: Hey, You are probably a list of people that are around asleep about the point of them and our wife. Rob 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>The issue of context inconsistency is 
present for all bins. However, the resulting text 
remains syntactical even as the number of bins in-
creases. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>The tweets generated by the 4-bin LSTM 
(32 bits per tweet) are reasonably comparable in 
quality to tweets produced by CoverTweet (2.8 
bits per tweet). 

</table></figure>

			<note place="foot" n="1"> If |B| | |S|, then we leave the remainder bit string out of encryption.</note>

			<note place="foot" n="2"> https://github.com/pytorch</note>

			<note place="foot" n="3"> For each category, we manually evaluate 60 randomly generated tweets based grammatical correctness, semantic coherence, and resemblance to real tweets. We select tweets from the 25th, 50th, and 75th percentile, and call them &quot;good&quot;, &quot;average&quot;, and &quot;poor&quot; respectively. We limit to tweets that are not offensive in language. 4 We only present passages &quot;average&quot; in quality to conserve space.</note>

			<note place="foot" n="5"> Based a random sample of 2 million tweets. 6 Tweets selected for comparison are &quot;average&quot; in quality.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Nltk: the natural language toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the COLING/ACL on Interactive presentation sessions. Association for Computational Linguistics</title>
		<meeting>the COLING/ACL on Interactive presentation sessions. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="69" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Linguistic steganography using automatically generated paraphrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="591" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adjective deletion for linguistic steganography and secret sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="493" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Practical linguistic steganography using contextual synonym substitution and a novel vertex coding method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="403" to="448" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Free from the cover text: a human-generated natural language approach to text-based steganography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Grosvald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Orhan Orgun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of Information Hiding and Multimedia Signal Processing</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="133" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Exploring the limits of language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02410</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The enron corpus: A new dataset for email classification research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Klimt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="217" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ensemble classifiers for steganalysis of digital media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kodovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Fridrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vojtěch</forename><surname>Holub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="432" to="444" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Speech and language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jurafsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
	<note>International Edition 710</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2010-01" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>Cernock`Cernock`y, and Sanjeev Khudanpur</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1310" to="1318" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The laws of large numbers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pál</forename><surname>Révész</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Academic Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Matryoshka: Hiding secret communication in plain sight</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iris</forename><surname>Safaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christina</forename><surname>Fragouli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Argyraki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th USENIX Workshop on Free and Open Communications on the Internet (FOCI 16). USENIX Association</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Data privacy and security: encryption and information hiding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Salomon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generating text with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1017" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The hiding virtues of ambiguity: quantifiably resilient watermarking of natural language text through synonym substitutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umut</forename><surname>Topkara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mercan</forename><surname>Topkara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Atallah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th workshop on Multimedia and security</title>
		<meeting>the 8th workshop on Multimedia and security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="164" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Volkhonskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Nazarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Borisenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Burnaev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.05502</idno>
		<title level="m">Steganographic generative adversarial networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Detection of steganographic techniques on twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2564" to="2569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Linguistic steganography on twitter: hierarchical language modeling with manual interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew D</forename><surname>Ker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IS&amp;T/SPIE Electronic Imaging. International Society for Optics and Photonics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="902803" to="902803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Avoiding detection on twitter: embedding strategies for linguistic steganography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew D</forename><surname>Ker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">2016</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Strategies for cleaning organizational emails with an application to enron email dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingjie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malik</forename><surname>Magdon-Ismail</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al</forename><surname>Wallace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th Conf. of North American Association for Computational Social and Organizational Science</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
