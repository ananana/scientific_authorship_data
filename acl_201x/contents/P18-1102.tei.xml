<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Control the Specificity in Neural Response Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruqing</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixing</forename><surname>Fan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">CAS Key Lab of Network Data Science and Technology</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Control the Specificity in Neural Response Generation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1108" to="1117"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1108</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In conversation, a general response (e.g., &quot;I don&apos;t know&quot;) could correspond to a large variety of input utterances. Previous generative conversational models usually employ a single model to learn the relationship between different utterance-response pairs, thus tend to favor general and trivial responses which appear frequently. To address this problem, we propose a novel controlled response generation mechanism to handle different utterance-response relationships in terms of specificity. Specifically, we introduce an explicit specificity control variable into a sequence-to-sequence model, which interacts with the usage representation of words through a Gaussian Kernel layer, to guide the model to generate responses at different specificity levels. We describe two ways to acquire distant labels for the specificity control variable in learning. Empirical studies show that our model can significantly outperform the state-of-the-art response generation models under both automatic and human evaluations.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Human-computer conversation is a critical and challenging task in AI and NLP. There have been two major streams of research in this direction, namely task oriented dialog and general purpose dialog (i.e., chit-chat). Task oriented dialog aims to help people complete specific tasks such as buy- ing tickets or shopping, while general purpose dia- log attempts to produce natural and meaningful conversations with people regarding a wide range of topics in open domains ( <ref type="bibr" target="#b14">Perez-Marin, 2011;</ref>. In recent years, the latter has at- tracted much attention in both academia and in- dustry as a way to explore the possibility in de- veloping a general purpose AI system in language (e.g., chatbots).</p><p>A widely adopted approach to general pur- pose dialog is learning a generative conversational model from large scale social conversation data. Most methods in this line are constructed within the statistical machine translation (SMT) frame- work, where a sequence-to-sequence (Seq2Seq) model is learned to "translate" an input utterance into a response. However, general purpose dialog is intrinsically different from machine translation. In machine translation, since every sentence and its translation are semantically equivalent, there exists a 1-to-1 relationship between them. How- ever, in general purpose dialog, a general response (e.g., "I don't know") could correspond to a large variety of input utterances. For example, in the chit-chat corpus used in this study (as shown in <ref type="figure" target="#fig_0">Figure 1</ref>), the top three most frequently appeared responses are "Must support! Cheer!", "Support! It's good.", and "My friends and I are shocked!", where the response "Must support! Cheer!" is used for 1216 different input utterances. Previ- ous Seq2Seq models, which treat all the utterance- response pairs uniformly and employ a single model to learn the relationship between them, will inevitably favor such general responses with high frequency. Although these responses are safe for replying different utterances, they are boring and trivial since they carry little information, and may quickly lead to an end of the conversation.</p><p>There have been a few efforts attempting to ad- dress this issue in literature. <ref type="bibr" target="#b8">Li et al. (2016a)</ref> proposed to use the Maximum Mutual Informa- tion (MMI) as the objective to penalize general re- sponses. It could be viewed as a post-processing approach which did not solve the generation of trivial responses fundamentally. <ref type="bibr" target="#b25">Xing et al. (2017)</ref> pre-defined a set of topics from an external cor- pus to guide the generation of the Seq2Seq model. However, it is difficult to ensure that the top- ics learned from the external corpus are consist- ent with that in the conversation corpus, leading to the introduction of additional noises.  introduced latent responding factors to model multiple responding mechanisms. How- ever, these latent factors are usually difficult in in- terpretation and it is hard to decide the number of the latent factors.</p><p>In our work, we propose a novel controlled re- sponse generation mechanism to handle different utterance-response relationships in terms of spe- cificity. The key idea is inspired by our observa- tion on everyday conversation between humans. In human-human conversation, people often actively control the specificity of responses depending on their own response purpose (which might be af- fected by a variety of underlying factors like their current mood, knowledge state and so on). For example, they may provide some interesting and specific responses if they like the conversation, or some general responses if they want to end it. They may provide very detailed responses if they are familiar with the topic, or just "I don't know" otherwise. Therefore, we propose to simulate the way people actively control the specificity of the response.</p><p>We employ a Seq2Seq framework and further introduce an explicit specificity control variable to represent the response purpose of the agent. Meanwhile, we assume that each word, beyond the semantic representation which relates to its meaning, also has another representation which relates to the usage preference under different re- sponse purpose. We name this representation as the usage representation of words. The specificity control variable then interacts with the usage rep- resentation of words through a Gaussian Kernel layer, and guides the Seq2Seq model to generate responses at different specificity levels. We refer to our model as Specificity Controlled Seq2Seq model (SC-Seq2Seq). Note that unlike the work by <ref type="bibr" target="#b25">(Xing et al., 2017</ref>), we do not rely on any ex- ternal corpus to learn our model. All the model parameters are learned on the same conversation corpus in an end-to-end way.</p><p>We employ distant supervision to train our SC- Seq2Seq model since the specificity control vari- able is unknown in the raw data. We describe two ways to acquire distant labels for the specificity control variable, namely Normalized Inverse Re- sponse Frequency (NIRF) and Normalized Inverse Word Frequency (NIWF). By using normalized values, we restrict the specificity control variable to be within a pre-defined continuous value range with each end has very clear meaning on the spe- cificity. This is significantly different from the dis- crete latent factors in ( ) which are difficult in interpretation.</p><p>We conduct an empirical study on a large pub- lic dataset, and compare our model with several state-of-the-art response generation methods. Em- pirical results show that our model can generate either general or specific responses, and signi- ficantly outperform existing methods under both automatic and human evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we briefly review the related work on conversational models and response specificity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Conversational Models</head><p>Automatic conversation has attracted increasing attention over the past few years. At the very be- ginning, people started the research using hand- crafted rules and templates ( <ref type="bibr" target="#b22">Walker et al., 2001;</ref><ref type="bibr" target="#b24">Williams et al., 2013;</ref><ref type="bibr" target="#b4">Henderson et al., 2014</ref>). These approaches required little data for train- ing but huge manual effort to build the model, which is very time-consuming. For now, con- versational models fall into two major categories: retrieval-based and generation-based. Retrieval- based conversational models search the most suit- able response from candidate responses using dif- ferent schemas <ref type="bibr" target="#b6">(Kearns, 2000;</ref><ref type="bibr" target="#b23">Wang et al., 2013;</ref>). These methods rely on pre- existing responses, thus are difficult to be exten-ded to open domains ( . With the large amount of conversation data available on the Internet, generation-based conversational models developed within a SMT framework ( <ref type="bibr" target="#b15">Ritter et al., 2011;</ref><ref type="bibr" target="#b1">Cho et al., 2014;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref> show promising results. <ref type="bibr" target="#b18">Shang et al. (2015)</ref> generated replies for short-text conversation by encoder-decoder-based neural network with local and global attentions. <ref type="bibr" target="#b16">Serban et al. (2016)</ref> built an end-to-end dialogue system using generative hier- archical neural network. <ref type="bibr" target="#b3">Gu et al. (2016)</ref> intro- duced copynet to simulate the repeating behavior of humans in conversation. Similarly, our model is also based on the encoder-decoder framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Response Specificity</head><p>Some recent studies began to focus on generat- ing more specific or informative responses in con- versation. It is also called a diversity problem since if each response is more specific, it would be more diverse between responses of different ut- terances. As an early work, <ref type="bibr" target="#b8">Li et al. (2016a)</ref> used Maximum Mutual Information (MMI) as the ob- jective to penalize general responses. Later,  proposed a data distillation method, which trains a series of generative models at differ- ent levels of specificity and uses a reinforcement learning model to choose the model best suited for decoding depending on the conversation context. These methods circumvented the general response issue by using either a post-processing approach or a data selection approach.</p><p>Besides, <ref type="bibr" target="#b9">Li et al. (2016b)</ref> tried to build a per- sonalized conversation engine by adding extra per- sonal information. <ref type="bibr" target="#b25">Xing et al. (2017)</ref> incorpor- ated the topic information from an external corpus into the Seq2Seq framework to guide the genera- tion. However, external dataset may not be always available or consistent with the conversation data- set in topics.  introduced latent responding factors to the Seq2Seq model to avoid generating safe responses. However, these latent factors are usually difficult in interpretation and hard to decide the number.</p><p>Moreover, <ref type="bibr" target="#b12">Mou et al. (2016)</ref> proposed a content-introducing approach to generate a re- sponse based on a predicted keyword. <ref type="bibr" target="#b27">Yao et al. (2016)</ref> attempted to improve the specificity with the reinforcement learning framework by using the averaged IDF score of the words in the response as a reward. <ref type="bibr" target="#b19">Shen et al. (2017)</ref> presented a con- ditional variational framework for generating spe- cific responses based on specific attributes. Un- like these existing methods, we introduce an ex- plicit specificity control variable into a Seq2Seq model to handle different utterance-response rela- tionships in terms of specificity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Specificity Controlled Seq2Seq Model</head><p>In this section, we present the Specificity Con- trolled Seq2Seq model (SC-Seq2Seq), a novel Seq2Seq model designed for actively controlling the generated responses in terms of specificity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Overview</head><p>The basic idea of a generative conversational model is to learn the mapping from an input ut- terance to its response, typically using an encoder- decoder framework. Formally, given an input ut- terance sequence X = (x 1 , x 2 , . . . , x T ) and a target response sequence Y = (y 1 , y 2 , . . . , y T ), a neural Seq2Seq model is employed to learn p(Y|X) based on the training corpus D = {(X, Y)|Y is the response of X}. By maximizing the likelihood of all the utterance-response pairs with a single mapping mechanism, the learned Seq2Seq model will inevitably favor those general responses that can correspond to a large variety of input utterances.</p><p>To address this issue, we assume that there are different mapping mechanisms between utterance- response pairs with respect to their specificity re- lation. Rather than involving some latent factors, we propose to introduce an explicit variable s into a Seq2Seq model to handle different utterance- response mappings in terms of specificity. By do- ing so, we hope that (1) s would have explicit meaning on specificity, and (2) s could not only interpret but also actively control the generation of the response Y given the input utterance X. The goal of our model becomes to learn p(Y|X, s) over the corpus D, where we acquire distant labels for s from the same corpus for learning. The overall architecture of SC-Seq2Seq is depicted in <ref type="figure" target="#fig_2">Figure  2</ref>, and we will detail our model as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Encoder</head><p>The encoder is to map the input utterance X into a compact vector that can capture its essential top- ics. Specifically, we use a bi-directional GRU ( <ref type="bibr" target="#b1">Cho et al., 2014</ref>) as the utterance encoder, and each word x i is firstly represented by its semantic representation e i mapped by semantic embedding   matrix E as the input of the encoder. Then, the en- coder represents the utterance X as a series of hid- den vectors {h t } T t=1 modeling the sequence from both forward and backward directions. Finally, we use the final backward hidden state as the initial hidden state of the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic-based &amp; Specificity-based Generation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Decoder</head><p>The decoder is to generate a response Y given the hidden representations of the input utterance X un- der some specificity level denoted by the control variable s. Specifically, at step t, we define the probability of generating any target word y t by a "mixture" of probabilities:</p><formula xml:id="formula_0">p(y t ) = βp M (y t ) + γp S (y t ),<label>(1)</label></formula><p>where p M (y t ) denotes the semantic-based gener- ation probability, p S (y t ) denotes the specificity- based generation probability, β and γ are the coef- ficients. Specifically, p M (y t ) is defined the same as that in traditional Seq2Seq model <ref type="bibr" target="#b21">(Sutskever et al., 2014</ref>):</p><formula xml:id="formula_1">p M (y t = w) = w T (W h M · h yt + W e M · e t−1 + b M ),<label>(2)</label></formula><p>where w is a one-hot indicator vector of the word w and e t−1 is the semantic representation of the t − 1-th generated word in decoder. W h M , W e M and b M are parameters. h yt is the t-th hidden state in the decoder which is computed by:</p><formula xml:id="formula_2">h yt = f (y t−1 , h y t−1 , c t ),<label>(3)</label></formula><p>where f is a GRU unit and c t is the context vec- tor to allow the decoder to pay different attention to different parts of input at different steps (Bah- danau et al., 2015).</p><p>p S (y t ) denotes the generation probability of the target word given the specificity control variable s. Here we introduce a Gaussian Kernel layer to define this probability. Specifically, we assume that each word, beyond its semantic representation e, also has a usage representation u mapped by us- age embedding matrix U. The usage representa- tion of a word denotes its usage preference under different specificity. The specificity control vari- able s then interacts with the usage representations through the Gaussian Kernel layer to produce the specificity-based generation probability p S (y t ):</p><formula xml:id="formula_3">p S (y t = w) = 1 √ 2πσ exp(− (Ψ S (U, w) − s) 2 2σ 2 ), Ψ S (U, w) = σ(w T (U · W U + b U )),<label>(4)</label></formula><p>where σ 2 is the variance, and Ψ S (·) maps the word usage representation into a real value with the spe- cificity control variable s as the mean of the Gaus- sian distribution. W U and b U are parameters to be learned. Note here in general we can use any real- value function to define Ψ S (U, w). In this work, we use the sigmoid function σ(·) for Ψ S (U, w) since we want to define s within the range <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> so that each end has very clear meaning on the spe- cificity, i.e., 0 denotes the most general response while 1 denotes the most specific response. In the next section, we will also keep this property when we define the distant label for the control variable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Distant Supervision</head><p>We train our SC-Seq2Seq model by maximizing the log likelihood of generating responses over the training set D:</p><formula xml:id="formula_4">L = (X,Y)∈D log P (Y|X, s; θ).<label>(5)</label></formula><p>where θ denotes all the model parameters. Note here since s is an explicit control variable in our model, we need the triples (X, Y, s) for training. However, s is not directly available in the raw con- versation corpus, thus we acquire distant labels for s to learn our model. We introduce two ways of distant supervision on the specificity control vari- able s, namely Normalized Inverse Response Fre- quency (NIRF) and Normalized Inverse Word Fre- quency (NIWF).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Normalized Inverse Response Frequency</head><note type="other">Normalized Inverse Response Frequency (NIRF) is based on the assumption that a response is more general if it corresponds to more input utterances in the corpus. Therefore, we use the inverse fre- quency of a response in a conversation corpus to indicate its specificity level. Specifically, we first build the response collection R by extracting all the responses from D. For a response Y ∈ R, let f Y denote its corpus frequency in R, we compute its Inverse Response Frequency (IRF) as:</note><formula xml:id="formula_5">IRF Y = log(1 + |R|)/f Y ,<label>(6)</label></formula><p>where |R| denotes the size of the response col- lection R. Next, we use the min-max normaliz- ation method ( <ref type="bibr" target="#b5">Jain et al., 2005</ref>) to obtain the NIRF value. Namely,</p><formula xml:id="formula_6">NIRF Y = IRF Y − min Y ∈R (IRF Y ) max Y ∈R (IRF Y ) − min Y ∈R (IRF Y ) .<label>(7)</label></formula><p>where max(IRF R ) and min(IRF R ) denotes the maximal and minimum IRF value in R respect- ively. The NIRF value is then used as the distant label of s in training. Note here by using nor- malized values, we aim to constrain the specificity control variable s to be within the pre-defined con- tinuous value range [0,1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Normalized Inverse Word Frequency</head><p>Normalized Inverse Word Frequency (NIWF) is based on the assumption that the specificity level of a response depends on the collection of words it contains, and the sentence is more specific if it contains more specific words. Hence, we can use the inverse corpus frequency of the words to indic- ate the specificity level of a response. Specifically, for a word y in the response Y, we first obtain its Inverse Word Frequency (IWF) by:</p><formula xml:id="formula_7">IWF y = log(1 + |R|)/f y ,<label>(8)</label></formula><p>where f y denotes the number of responses in R containing the word y. Since a response usu- ally contains a collection of words, there would be multiple ways to define the response-level IWF value, e.g., sum, average, minimum or maximum of the IWF values of all the words. In our work, we find that the best performance can be achieved by using the maximum of the IWF of all the words in Y to represent the response-level IWF by</p><formula xml:id="formula_8">IWF Y = max y∈Y (IWF y ).<label>(9)</label></formula><p>This is reasonable since a response is specific as long as it contains some specific words. We do not require all the words in a response to be specific, thus sum, average, and minimum would not be appropriate operators for computing the response- level IWF. Again, we use min-max normalization to obtain the NIWF value for the response Y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Specificity Controlled Response Generation</head><p>Given a new input utterance, we can employ the learned SC-Seq2Seq model to generate responses at different specificity levels by varying the con- trol variable s. In this way, we can simulate hu- man conversations where one can actively con- trol the response specificity depending on his/her own mind. When we apply our model to a chat- bot, there might be different ways to use the con- trol variable for conversation in practice. If we want the agent to always generate informative re- sponses, we can set s to 1 or some values close to 1. If we want the agent to be more dynamic, we can sample s within the range [0,1] to en- rich the styles in the response. We may further employ some reinforcement learning technique to learn to adjust the control variable depending on users' feedbacks. This would make the agent even more vivid, and we leave this as our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>In this section, we conduct experiments to verify the effectiveness of our proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset Description</head><p>We conduct our experiments on the public Short Text Conversation (STC) dataset 1 released in NTCIR-13. STC maintains a large reposit- ory of post-comment pairs from the Sina Weibo which is one of the popular Chinese social sites. STC dataset contains roughly 3.8 million post- comment pairs, which could be used to simu- late the utterance-response pairs in conversation. We employ the Jieba Chinese word segmenter 2 to tokenize the utterances and responses into se- quences of Chinese words, and the detailed data- set statistics are shown in <ref type="table">Table 1</ref>. We randomly selected two subsets as the development and test dataset, each containing 10k pairs. The left pairs are used for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines Methods</head><p>We compare our proposed SC-Seq2Seq model against several state-of-the-art baselines: (1) Seq2Seq-att: the standard Seq2Seq model with the attention mechanism ( <ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref>; (2) MMI-bidi: the Seq2Seq model using Max- imum Mutual Information (MMI) as the object- ive function to reorder the generated responses (Li et al., 2016a); (3) MARM: the Seq2Seq model with a probabilistic framework to model the lat- ent responding mechanisms ( ); (4) Seq2Seq+IDF: an extension of Seq2Seq-att by optimizing specificity under the reinforcement learning framework, where the reward is calcu- lated as the sentence level IDF score of the gen- erated response ( <ref type="bibr" target="#b27">Yao et al., 2016)</ref>. We refer to our model trained using NIRF and NIWF as SC- Seq2Seq NIRF and SC-Seq2Seq NIWF respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details</head><p>As suggested in ( <ref type="bibr" target="#b18">Shang et al., 2015</ref>), we con- struct two separate vocabularies for utterances and responses by using 40,000 most frequent words on each side in the training data, covering 97.7% words in utterances and 96.1% words in responses respectively. All the remaining words are replaced by a special token &lt;UNK&gt; symbol. We implemented our model in Tensorflow <ref type="bibr">3</ref> . We tuned the hyper-parameters via the development set. Specifically, we use one layer of bi-directional GRU for encoder and another uni-directional GRU for decoder, with the GRU hidden unit size set as 300 in both the encoder and decoder. The dimen- sion of semantic word embeddings in both utter- ances and responses is 300, while the dimension of usage word embeddings in responses is 50. We apply the Adam algorithm ( <ref type="bibr" target="#b7">Kingma and Ba, 2015)</ref> for optimization, where the parameters of Adam are set as in ( <ref type="bibr" target="#b7">Kingma and Ba, 2015)</ref>. The variance σ 2 of the Gaussian Kernel layer is set as 1, and all other trainable parameters are randomly initialized by uniform distribution within [-0.08,0.08]. The mini-batch size for the update is set as 128. We clip the gradient when its norm exceeds 5. Our model is trained on a Tesla K80 GPU card, and we run the training for up to 12 epochs, which takes approximately five days. We select the model that achieves the lowest perplexity on the development dataset, and we report results on the test dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation Methodologies</head><p>For evaluation, we follow the existing work and employ both automatic and human evaluations:</p><formula xml:id="formula_9">(1) distinct-1 &amp; distinct-2 (Li et al., 2016a):</formula><p>we count numbers of distinct unigrams and bi- grams in the generated responses, and divide the numbers by total number of generated uni- grams and bigrams. Distinct metrics (both the numbers and the ratios) can be used to evalu- ate the specificity/diversity of the responses. (2) BLEU ( <ref type="bibr" target="#b13">Papineni et al., 2002</ref>): BLEU has been proved strongly correlated with human evalu- ations. BLEU-n measures the average n-gram pre- cision on a set of reference sentences. (3) Average &amp; Extrema (Serban et al., 2017): Average and Extrema projects the generated response and the ground truth response into two separate vectors by taking the mean over the word embeddings or tak- ing the extremum of each dimension respectively, and then computes the cosine similarity between them. (4) Human evaluation: Three labelers with rich Weibo experience were recruited to conduct evaluation. Responses from different models are randomly mixed for labeling. Labelers refer to 300 random sampled test utterances and score the quality of the responses with the following cri- teria: 1) +2: the response is not only semantic- ally relevant and grammatical, but also informat-   ive and interesting; 2) +1: the response is gram- matical and can be used as a response to the utter- ance, but is too trivial (e.g., "I don't know"); 3) +0: the response is semantically irrelevant or ungram- matical (e.g., grammatical errors or UNK). Agree- ments to measure inter-rater consistency among three labelers are calculated with the Fleiss' kappa ( <ref type="bibr" target="#b2">Fleiss and Cohen, 1973</ref>).</p><note type="other">Models distinct-1 distinct-2 BLEU-1 BLEU-2 Average Extrema SC-</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Evaluation Results</head><p>Model Analysis: We first analyze our models trained with different distant supervision inform- ation. For each model, given a test utterance, we vary the control variable s by setting it to five dif- ferent values (i.e., 0, 0.2, 0.5, 0.8, 1) to check whether the learned model can actually achieve different specificity levels. As shown in <ref type="table" target="#tab_3">Table 2</ref>, we find that: (1) The SC-Seq2Seq model trained with NIRF cannot work well. The test perform- ances are almost the same with different s value. This is surprising since the NIRF definition seems to be directly corresponding to the specificity of a response. By conducting further analysis, we find that even though the conversation dataset is large, it is still limited and a general response could appear very few times in this corpus. In other words, the inverse frequency of a response is very weakly correlated with its response spe- cificity. <ref type="formula" target="#formula_1">(2)</ref> The SC-Seq2Seq model trained with NIWF can achieve our purpose. By varying the control variable s from 0 to 1, the generated re- sponses turn from general to specific as measured by the distinct metrics. The results indicate that the max inverse word frequency in a response is a good distant label for the response specificity. <ref type="formula" target="#formula_2">(3)</ref> When we compare the generated responses against ground truth data, we find the SC-Seq2Seq NIWF model with the control variable s set to 0.5 can achieve the best performances. The results indic- ate that there are diverse responses in real data in terms of specificity, and it is necessary to take a balanced setting if we want to fit the ground truth.</p><p>Baseline Comparison: The performance com- parisons between our model and the baselines are shown in <ref type="table" target="#tab_4">Table 3</ref>. We have the following ob- servations: (1) By using MMI as the objective, MMI-bidi can improve the specificity (in terms of distinct ratios) over the traditional Seq2Seq-att model. (2) MARM can achieve the best distinct ratios among the baseline methods, but the worst in terms of the distinct numbers. The results indic- ate that MARM tends to generate specific but very short responses. Meanwhile, its low BLEU scores also show that the responses generated by MARM deviate from the ground truth significantly. (3) By using the IDF information as the reward to train  the Seq2Seq model, the Seq2Seq+IDF does not show much advantages, but only achieves compar- able results as MMI-bidi. (4) By setting the con- trol variable s to 1, our SC-Seq2Seq NIWF model can achieve the best specificity performance as evaluated by the distinct metrics. By setting the control variable s to 0.5, our SC-Seq2Seq NIWF model can best fit the ground truth data as eval- uated by the BLEU scores, Average and Extrema. All the improvements over the baseline models are statistically significant (p-value &lt; 0.01). These results demonstrate the effectiveness as well as the flexibility of our controlled generation model. <ref type="table" target="#tab_6">Table 4</ref> shows the human evaluation results. We can observe that: (1) SC-Seq2Seq NIWF,s=1 gen- erates the most informative responses and inter- esting (labeled as "+2") and the least general re- sponses than all the baseline models. Meanwhile, SC-Seq2Seq NIWF,s=0 generates the most general responses (labeled as "+1"); (2) MARM gener- ates the most bad responses (labeled as "+0"), which indicates the drawbacks of the unknown lat- ent responding mechanisms; (3) The kappa values of our models are all larger than 0.4, considered as "moderate agreement" regarding quality of re- sponses. The largest kappa value is achieved by SC-Seq2Seq NIWF,s=0 , which seems reasonable since it is easy to reach an agreement on general responses. Sign tests demonstrate the improve- ments of SC-Seq2Seq NIWF,s=1 to the baseline mod- els are statistically significant (p-value &lt; 0.01). All the human judgement results again demon- strate the effectiveness of our controlled genera- tion mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Case Study</head><p>To better understand how different models per- form, we conduct some case studies. We randomly sample three utterances from the test dataset, and show the responses generated by different models.</p><p>Utterance: A tent with three rooms and one hall, you're worth it! Seq2Seq-att: It looks good, very good! MMI-bidi: It looks good, I like it. MARM: Good. Seq2Seq+IDF: The tent is good. s=1: I want to know what the brand of this tent is? s=0.8: I want to know where to buy it? s=0.5: ,I want one too, but it's a pity that I can't afford it. s=0.2: Good, good. s=0: Really?</p><p>Utterance: ! ! ? Oh! Got caught! Why is Tang Wei so beautiful? Seq2Seq-att: I really like her! MMI-bidi: So beautiful. I really like her! MARM: I don't like Tang Wei. Seq2Seq+IDF: I really like her, I like. s=1: I really like her eye make-up, and her temperament is so beautiful! s=0.8: I really like her, and her eyes are so bright! s=0.5: I think she is very beautiful, and I really like her! s=0.2: I think she is very beautiful. s=0: Yes.</p><p>Utterance: I will keep such a sweet samoyed, and let it accompany my children to grow up. Seq2Seq-att: Is it a samoyed? MMI-bidi: What a lovely samoyed. MARM: So lovely. Seq2Seq+IDF: What a lovely dog. s=1: ! The samoyed is so lovely, and I really want to keep one! s=0.8: The samoyed is so lovely, and I really want to pinch it. s=0.5: What a lovely dog, what a lovely dog. s=0.2: So lovely, so lovely! s=0: So lovely! As shown in <ref type="table" target="#tab_7">Table 5</ref>, we can find that: (1) The re- sponses generated by the four baselines are often quite general and short, which may quickly lead to an end of the conversation. (2) SC-Seq2Seq NIWF with large control variable values (i.e., s &gt; 0.5) can generate very long and specific responses. In these responses, we can find many informative words. For example, in case 2 with s as 1 and 0.8, we can find words like "(eye make-up)", " (temperament)" and "(bright)" which are quite specific and strongly related to the conversa- tion topic of "beauty". (3) When we decrease the control variable value, the generated responses be- come more and more general and shorter from our SC-Seq2Seq NIWF model.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Analysis on Usage Representations</head><p>We also conduct some analysis to understand the usage representations of words introduced in our model. We randomly sample 500 words from our SC-Seq2Seq NIWF and apply t-SNE <ref type="bibr" target="#b11">(Maaten and Hinton, 2008</ref>) to visualize both usage and se- mantic embeddings. As shown in <ref type="figure" target="#fig_4">Figure 3</ref>, we can see that the two distributions are significantly different. In the usage space, words like " (fatty liver)" and " (outsit)" lie closely which are both specific words, and both are far from the general words like "(fat)". On the contrary, in the semantic space, " (fatty liver)" is close to "(fat)" since they are se- mantically related, and both are far from the word "(outsit)". Furthermore, given some sampled target words, we also show the top-5 similar words based on cosine similarity under both represent- ations in <ref type="table" target="#tab_9">Table 6</ref>. Again, we can see that the nearest neighbors of a same word are quite differ- ent under two representations. Neighbors based on semantic representations are semantically re- lated, while neighbors based on usage representa- tions are not so related but with similar specificity levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose a novel controlled response gener- ation mechanism to handle different utterance- response relationships in terms of specificity. We introduce an explicit specificity control variable into the Seq2Seq model, which interacts with the usage representation of words to generate re- sponses at different specificity levels. Empirical results showed that our model can generate either general or specific responses, and significantly outperform state-of-the-art generation methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Rank-frequency distribution of the responses in the chit-chat corpus, with x and y axes being lg(rank order) and lg(frequency) respectively.</figDesc><graphic url="image-1.png" coords="1,329.84,222.54,173.14,111.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Attentive</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The overall architecture of SC-Seq2Seq model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>(</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: t-SNE embeddings of usage and semantic vectors.</figDesc><graphic url="image-3.png" coords="9,164.84,186.14,138.26,103.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 2 : Model analysis of our SC-Seq2Seq under the automatic evaluation.</head><label>2</label><figDesc></figDesc><table>Models 
distinct-1 
distinct-2 
BLEU-1 
BLEU-2 
Average 
Extrema 
Seq2Seq-att 
5048/0.060 
15976/0.168 
15.062 
6.964 
0.575 
0.376 
MMI-bidi 
5074/0.082 
12162/0.287 
15.772 
7.215 
0.586 
0.381 
MARM 
2566/0.096 
3294/0.312 
7.321 
3.774 
0.512 
0.336 
Seq2Seq+IDF 
4722/0.052 
15384/0.229 
14.423 
6.743 
0.572 
0.369 
SC-Seq2Seq NIWF,s=1 
11588/0.116 
27144/0.347 
12.392 
5.869 
0.554 
0.353 
SC-Seq2Seq NIWF,s=0.5 
2835/0.050 
9537/0.235 
16.122 
7.674 
0.609 
0.399 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Comparisons between our SC-Seq2Seq and the baselines under the automatic evaluation.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Results on the human evaluation. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Examples of response generation from the STC 
test data. s = 1, 0.8, 0.5, 0.2, 0 are the outputs of our 
SC-Seq2Seq NIWF with different s values. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Target words and their top-5 similar words under usage and semantic representations respectively. 

fatty liver 
outsit 

fat 

fatty liver fat 

outsit 

(a) usage 
(b) semantic 

</table></figure>

			<note place="foot" n="1"> http://ntcirstc.noahlab.com.hk/STC2/stc-cn.htm</note>

			<note place="foot" n="2"> https://pypi.python.org/pypi/jieba 3 https://www.tensorflow.org/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on empirical methods in natural language processing</title>
		<meeting>the conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The equivalence of weighted kappa and the intraclass correlation coefficient as measures of reliability. Educational and psychological measurement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Fleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="613" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th annual meeting of the Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 54th annual meeting of the Association for Computational Linguistics. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The second dialog state tracking challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason D</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL)</title>
		<meeting>the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Score normalization in multimodal biometric systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anil</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Nandakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2270" to="2285" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Cobot in lambdamoo: A social statistics agent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kearns</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A diversity-promoting objective function for neural conversation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A persona-based neural conversation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Georgios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Spithourakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Data distillation for controlling specificity in dialogue generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.06703</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sequence to backward and forward sequences: A content-introducing approach to generative short-text conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Perez-Marin</surname></persName>
		</author>
		<title level="m">Conversational Agents and Natural Language Interaction: Techniques and Effective Practices: Techniques and Effective Practices</title>
		<imprint>
			<publisher>IGI Global</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Data-driven response generation in social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on empirical methods in natural language processing</title>
		<meeting>the conference on empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="583" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Building end-to-end dialogue systems using generative hierarchical neural network models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Iulian Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3776" to="3784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A hierarchical latent variable encoder-decoder model for generating dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Iulian Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural responding machine for short-text conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53th annual meeting of the Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 53th annual meeting of the Association for Computational Linguistics. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A conditional variational framework for dialog generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuzi</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akiko</forename><surname>Aizawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A neural network approach to context-sensitive generation of conversational responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<biblScope unit="page" from="196" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Quantitative and qualitative evaluation of darpa communicator spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marilyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><forename type="middle">E</forename><surname>Passonneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 39th Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="515" to="522" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A dataset for research on short-text conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on empirical methods in natural language processing</title>
		<meeting>the conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The dialog state tracking challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Raux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGDIAL 2013 Conference</title>
		<meeting>the SIGDIAL 2013 Conference</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="404" to="413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Topic aware neural response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3351" to="3357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to respond with deep neural networks for retrievalbased human-computer conversation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39st annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 39st annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">An attentional neural conversation model with improved specificity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01292</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mechanism-aware neural machine for dialogue response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganbin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongyu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3400" to="3407" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
