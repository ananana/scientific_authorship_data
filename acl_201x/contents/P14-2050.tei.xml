<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:05+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dependency-Based Word Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Ilan University Ramat-Gan</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Ilan University Ramat-Gan</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dependency-Based Word Embeddings</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="302" to="308"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>While continuous word embeddings are gaining popularity, current models are based solely on linear contexts. In this work, we generalize the skip-gram model with negative sampling introduced by Mikolov et al. to include arbitrary contexts. In particular, we perform experiments with dependency-based contexts, and show that they produce markedly different embeddings. The dependency-based embeddings are less topical and exhibit more functional similarity than the original skip-gram embeddings.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Word representation is central to natural language processing. The default approach of represent- ing words as discrete and distinct symbols is in- sufficient for many tasks, and suffers from poor generalization. For example, the symbolic repre- sentation of the words "pizza" and "hamburger" are completely unrelated: even if we know that the word "pizza" is a good argument for the verb "eat", we cannot infer that "hamburger" is also a good argument. We thus seek a representation that captures semantic and syntactic similarities between words. A very common paradigm for ac- quiring such representations is based on the distri- butional hypothesis of <ref type="bibr" target="#b16">Harris (1954)</ref>, stating that words in similar contexts have similar meanings.</p><p>Based on the distributional hypothesis, many methods of deriving word representations were ex- plored in the NLP community. On one end of the spectrum, words are grouped into clusters based on their contexts ( <ref type="bibr" target="#b5">Brown et al., 1992;</ref><ref type="bibr" target="#b32">Uszkoreit and Brants, 2008)</ref>. On the other end, words * Supported by the European Community's Seventh Framework Programme <ref type="bibr">(FP7/2007</ref><ref type="bibr">(FP7/ -2013</ref> under grant agree- ment no. 287923 (EXCITEMENT).</p><p>are represented as a very high dimensional but sparse vectors in which each entry is a measure of the association between the word and a particu- lar context (see <ref type="bibr" target="#b30">(Turney and Pantel, 2010;</ref><ref type="bibr" target="#b3">Baroni and Lenci, 2010</ref>) for a comprehensive survey). In some works, the dimensionality of the sparse word-context vectors is reduced, using techniques such as SVD ( <ref type="bibr" target="#b6">Bullinaria and Levy, 2007</ref>) or LDA ( <ref type="bibr" target="#b25">Ritter et al., 2010;</ref><ref type="bibr" target="#b26">Séaghdha, 2010;</ref><ref type="bibr" target="#b8">Cohen et al., 2012</ref>). Most recently, it has been proposed to represent words as dense vectors that are de- rived by various training methods inspired from neural-network language modeling ( <ref type="bibr" target="#b4">Bengio et al., 2003;</ref><ref type="bibr" target="#b9">Collobert and Weston, 2008;</ref><ref type="bibr" target="#b23">Mnih and Hinton, 2008;</ref><ref type="bibr" target="#b19">Mikolov et al., 2011;</ref><ref type="bibr" target="#b21">Mikolov et al., 2013b</ref>). These representations, referred to as "neural embeddings" or "word embeddings", have been shown to perform well across a variety of tasks ( <ref type="bibr" target="#b29">Turian et al., 2010;</ref><ref type="bibr" target="#b10">Collobert et al., 2011;</ref><ref type="bibr" target="#b27">Socher et al., 2011;</ref><ref type="bibr" target="#b2">Al-Rfou et al., 2013)</ref>.</p><p>Word embeddings are easy to work with be- cause they enable efficient computation of word similarities through low-dimensional matrix op- erations.</p><p>Among the state-of-the-art word- embedding methods is the skip-gram with nega- tive sampling model (SKIPGRAM), introduced by <ref type="bibr" target="#b21">Mikolov et al. (2013b)</ref> and implemented in the word2vec software. 1 Not only does it produce useful word representations, but it is also very ef- ficient to train, works in an online fashion, and scales well to huge copora (billions of words) as well as very large word and context vocabularies.</p><p>Previous work on neural word embeddings take the contexts of a word to be its linear context - words that precede and follow the target word, typ- ically in a window of k tokens to each side. How- ever, other types of contexts can be explored too.</p><p>In this work, we generalize the SKIP- GRAM model, and move from linear bag-of-words contexts to arbitrary word contexts. Specifically, following work in sparse vector-space models <ref type="bibr" target="#b18">(Lin, 1998;</ref><ref type="bibr" target="#b24">Padó and Lapata, 2007;</ref><ref type="bibr" target="#b3">Baroni and Lenci, 2010)</ref>, we experiment with syntactic con- texts that are derived from automatically produced dependency parse-trees.</p><p>The different kinds of contexts produce no- ticeably different embeddings, and induce differ- ent word similarities. In particular, the bag-of- words nature of the contexts in the "original" SKIPGRAM model yield broad topical similari- ties, while the dependency-based contexts yield more functional similarities of a cohyponym na- ture. This effect is demonstrated using both quali- tative and quantitative analysis (Section 4).</p><p>The neural word-embeddings are considered opaque, in the sense that it is hard to assign mean- ings to the dimensions of the induced represen- tation. In Section 5 we show that the SKIP- GRAM model does allow for some introspection by querying it for contexts that are "activated by" a target word. This allows us to peek into the learned representation and explore the contexts that are found by the learning process to be most discrim- inative of particular words (or groups of words). To the best of our knowledge, this is the first work to suggest such an analysis of discriminatively- trained word-embedding models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Skip-Gram Model</head><p>Our departure point is the skip-gram neural em- bedding model introduced in <ref type="bibr" target="#b20">(Mikolov et al., 2013a</ref>) trained using the negative-sampling pro- cedure presented in ( <ref type="bibr" target="#b21">Mikolov et al., 2013b</ref>). In this section we summarize the model and train- ing objective following the derivation presented by , and highlight the ease of incorporating arbitrary contexts in the model.</p><p>In the skip-gram model, each word w ∈ W is associated with a vector v w ∈ R d and similarly each context c ∈ C is represented as a vector v c ∈ R d , where W is the words vocabulary, C is the contexts vocabulary, and d is the embed- ding dimensionality. The entries in the vectors are latent, and treated as parameters to be learned. Loosely speaking, we seek parameter values (that is, vector representations for both words and con- texts) such that the dot product v w · v c associated with "good" word-context pairs is maximized.</p><p>More specifically, the negative-sampling objec- tive assumes a dataset D of observed (w, c) pairs of words w and the contexts c, which appeared in a large body of text. Consider a word-context pair (w, c). Did this pair come from the data? We de- note by p(D = 1|w, c) the probability that (w, c) came from the data, and by p(D = 0|w, c) = 1 − p(D = 1|w, c) the probability that (w, c) did not. The distribution is modeled as:</p><formula xml:id="formula_0">p(D = 1|w, c) = 1 1+e −vw ·vc</formula><p>where v w and v c (each a d-dimensional vector) are the model parameters to be learned. We seek to maximize the log-probability of the observed pairs belonging to the data, leading to the objective:</p><formula xml:id="formula_1">arg max vw,vc (w,c)∈D log 1 1+e −vc·vw</formula><p>This objective admits a trivial solution in which p(D = 1|w, c) = 1 for every pair (w, c). This can be easily achieved by setting v c = v w and v c ·v w = K for all c, w, where K is large enough number.</p><p>In order to prevent the trivial solution, the ob- jective is extended with (w, c) pairs for which p(D = 1|w, c) must be low, i.e. pairs which are not in the data, by generating the set D of ran- dom (w, c) pairs (assuming they are all incorrect), yielding the negative-sampling training objective:</p><formula xml:id="formula_2">arg max vw,vc (w,c)∈D p(D = 1|c, w) (w,c)∈D p(D = 0|c, w)</formula><p>which can be rewritten as:</p><formula xml:id="formula_3">arg max vw,vc (w,c)∈D log σ(v c · v w ) + (w,c)∈D log σ(−v c · v w )</formula><p>where σ(x) = 1/(1 + e x ). The objective is trained in an online fashion using stochastic-gradient up- dates over the corpus D ∪ D . The negative samples D can be constructed in various ways. We follow the method proposed by <ref type="bibr">Mikolov et al.:</ref> for each (w, c) ∈ D we construct n samples (w, c 1 ), . . . , (w, c n ), where n is a hy- perparameter and each c j is drawn according to its unigram distribution raised to the 3/4 power.</p><p>Optimizing this objective makes observed word-context pairs have similar embeddings, while scattering unobserved pairs. Intuitively, words that appear in similar contexts should have similar embeddings, though <ref type="bibr">we</ref> have not yet found a formal proof that SKIPGRAM does indeed max- imize the dot product of similar words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Embedding with Arbitrary Contexts</head><p>In the SKIPGRAM embedding algorithm, the con- texts of a word w are the words surrounding it in the text. The context vocabulary C is thus identical to the word vocabulary W . However, this restriction is not required by the model; con- texts need not correspond to words, and the num- ber of context-types can be substantially larger than the number of word-types. We generalize SKIPGRAM by replacing the bag-of-words con- texts with arbitrary contexts.</p><p>In this paper we experiment with dependency- based syntactic contexts. Syntactic contexts cap- ture different information than bag-of-word con- texts, as we demonstrate using the sentence "Aus- tralian scientist discovers star with telescope".</p><p>Linear Bag-of-Words Contexts This is the context used by word2vec and many other neu- ral embeddings. Using a window of size k around the target word w, 2k contexts are produced: the k words before and the k words after w. For k = 2, the contexts of the target word w are w −2 , w −1 , w +1 , w +2 . In our example, the contexts of discovers are Australian, scientist, star, with. <ref type="bibr">2</ref> Note that a context window of size 2 may miss some important contexts (telescope is not a con- text of discovers), while including some acciden- tal ones (Australian is a context discovers). More- over, the contexts are unmarked, resulting in dis- covers being a context of both stars and scientist, which may result in stars and scientists ending up as neighbours in the embedded space. A win- dow size of 5 is commonly used to capture broad topical content, whereas smaller windows contain more focused information about the target word.</p><p>Dependency-Based Contexts An alternative to the bag-of-words approach is to derive contexts based on the syntactic relations the word partic- ipates in. This is facilitated by recent advances in parsing technology ( <ref type="bibr" target="#b14">Goldberg and Nivre, 2012;</ref><ref type="bibr" target="#b15">Goldberg and Nivre, 2013</ref>) that allow parsing to syntactic dependencies with very high speed and near state-of-the-art accuracy.</p><p>After parsing each sentence, we derive word contexts as follows: for a target word w with modifiers m 1 , . . . , m k and a head h, we consider the contexts (m 1 , lbl 1 ), . . . , (m k , lbl k ), (h, lbl −1 h ), 2 word2vec's implementation is slightly more compli- cated. The software defaults to prune rare words based on their frequency, and has an option for sub-sampling the fre- quent words. These pruning and sub-sampling happen before the context extraction, leading to a dynamic window size. In addition, the window size is not fixed to k but is sampled uniformly in the range <ref type="bibr">[1, k]</ref> for each word.   where lbl is the type of the dependency relation be- tween the head and the modifier (e.g. nsubj, dobj, prep with, amod) and lbl −1 is used to mark the inverse-relation. Relations that include a preposi- tion are "collapsed" prior to context extraction, by directly connecting the head and the object of the preposition, and subsuming the preposition itself into the dependency label. An example of the de- pendency context extraction is given in <ref type="figure" target="#fig_1">Figure 1</ref>.</p><p>Notice that syntactic dependencies are both more inclusive and more focused than bag-of- words. They capture relations to words that are far apart and thus "out-of-reach" with small win- dow bag-of-words (e.g. the instrument of discover is telescope/prep with), and also filter out "coinci- dental" contexts which are within the window but not directly related to the target word (e.g. Aus- tralian is not used as the context for discovers). In addition, the contexts are typed, indicating, for ex- ample, that stars are objects of discovery and sci- entists are subjects. We thus expect the syntactic contexts to yield more focused embeddings, cap- turing more functional and less topical similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Evaluation</head><p>We experiment with 3 training conditions: BOW5 (bag-of-words contexts with k = 5), BOW2 (same, with k = 2) and DEPS (dependency-based syntactic contexts). We modified word2vec to support arbitrary contexts, and to output the con- text embeddings in addition to the word embed- dings. For bag-of-words contexts we used the original word2vec implementation, and for syn- tactic contexts, we used our modified version. The negative-sampling parameter (how many negative contexts to sample for every correct one) was 15.</p><p>All embeddings were trained on English Wikipedia. For DEPS, the corpus was tagged with parts-of-speech using the Stanford tagger ( <ref type="bibr" target="#b28">Toutanova et al., 2003)</ref> and parsed into labeled Stanford dependencies ( <ref type="bibr" target="#b11">de Marneffe and Manning, 2008</ref>) using an implementation of the parser described in <ref type="bibr" target="#b14">(Goldberg and Nivre, 2012</ref>). All to- kens were converted to lowercase, and words and contexts that appeared less than 100 times were filtered. This resulted in a vocabulary of about 175,000 words, with over 900,000 distinct syntac- tic contexts. We report results for 300 dimension embeddings, though similar trends were also ob- served with 600 dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Qualitative Evaluation</head><p>Our first evaluation is qualitative: we manually in- spect the 5 most similar words (by cosine similar- ity) to a given set of target words ( <ref type="table" target="#tab_2">Table 1)</ref>.</p><p>The first target word, Batman, results in similar sets across the different setups. This is the case for many target words. However, other target words show clear differences between embeddings.</p><p>In Hogwarts -the school of magic from the fictional Harry Potter series -it is evident that BOW contexts reflect the domain aspect, whereas DEPS yield a list of famous schools, capturing the semantic type of the target word. This ob- servation holds for Turing 3 and many other nouns as well; BOW find words that associate with w, while DEPS find words that behave like w. Turney (2012) described this distinction as domain simi- larity versus functional similarity.</p><p>The Florida example presents an ontologi- cal difference; bag-of-words contexts generate meronyms (counties or cities within Florida), while dependency-based contexts provide cohy- ponyms (other US states). We observed the same behavior with other geographical locations, partic- ularly with countries (though not all of them).</p><p>The next two examples demonstrate that simi- larities induced from DEPS share a syntactic func- tion (adjectives and gerunds), while similarities based on BOW are more diverse. Finally, we ob- serve that while both BOW5 and BOW2 yield top- ical similarities, the larger window size result in more topicality, as expected. <ref type="table" target="#tab_3">Target Word  BOW5  BOW2  DEPS   batman   nightwing  superman  superman  aquaman  superboy  superboy  catwoman  aquaman  supergirl  superman  catwoman  catwoman  manhunter  batgirl  aquaman   hogwarts   dumbledore  evernight  sunnydale  hallows  sunnydale  collinwood  half-blood  garderobe  calarts  malfoy  blandings  greendale</ref>   We also tried using the subsampling option ( <ref type="bibr" target="#b21">Mikolov et al., 2013b</ref>) with BOW contexts (not shown). Since word2vec removes the subsam- pled words from the corpus before creating the window contexts, this option effectively increases the window size, resulting in greater topicality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Quantitative Evaluation</head><p>We supplement the examples in <ref type="table" target="#tab_2">Table 1</ref> with quantitative evaluation to show that the qualita- tive differences pointed out in the previous sec- tion are indeed widespread. To that end, we use the WordSim353 dataset ( <ref type="bibr" target="#b12">Finkelstein et al., 2002;</ref><ref type="bibr" target="#b0">Agirre et al., 2009)</ref>. This dataset contains pairs of similar words that reflect either relatedness (top- ical similarity) or similarity (functional similar- ity) relations. <ref type="bibr">4</ref> We use the embeddings in a re- trieval/ranking setup, where the task is to rank the similar pairs in the dataset above the related ones.</p><p>The pairs are ranked according to cosine sim- ilarities between the embedded words. We then draw a recall-precision curve that describes the embedding's affinity towards one subset ("sim- ilarity") over another ("relatedness"). We ex- pect DEPS's curve to be higher than BOW2's curve, which in turn is expected to be higher than BOW5's. The graph in <ref type="figure" target="#fig_2">Figure 2a</ref> shows this is in- deed the case. We repeated the experiment with a different dataset ( <ref type="bibr" target="#b7">Chiarello et al., 1990</ref>) that was used by <ref type="bibr" target="#b31">Turney (2012)</ref> to distinguish between do- main and functional similarities. The results show a similar trend <ref type="figure" target="#fig_2">(Figure 2b</ref>). When reversing the task such that the goal is to rank the related terms above the similar ones, the results are reversed, as expected (not shown). <ref type="bibr">5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Model Introspection</head><p>Neural word embeddings are often considered opaque and uninterpretable, unlike sparse vec- tor space representations in which each dimen- sion corresponds to a particular known context, or LDA models where dimensions correspond to la- tent topics. While this is true to a large extent, we observe that SKIPGRAM does allow a non-trivial amount of introspection. Although we cannot as- sign a meaning to any particular dimension, we can indeed get a glimpse at the kind of informa- tion being captured by the model, by examining which contexts are "activated" by a target word.</p><p>Recall that the learning procedure is attempting to maximize the dot product v c ·v w for good (w, c) pairs and minimize it for bad ones. If we keep the context embeddings, we can query the model for the contexts that are most activated by (have the highest dot product with) a given target word. By doing so, we can see what the model learned to be a good discriminative context for the word.</p><p>To demonstrate, we list the 5 most activated contexts for our example words with DEPS em- beddings in <ref type="table" target="#tab_3">Table 2</ref>. Interestingly, the most dis- criminative syntactic contexts in these cases are <ref type="bibr">5</ref> Additional experiments (not presented in this paper) re- inforce our conclusion. In particular, we found that DEPS perform dramatically worse than BOW contexts on analogy tasks as in ( <ref type="bibr" target="#b22">Mikolov et al., 2013c;</ref>). batman hogwarts turing superman/conj −1 students/prep at −1 machine/nn −1 spider-man/conj −1 educated/prep at −1 test/nn −1 superman/conj student/prep at −1 theorem/poss −1 spider-man/conj stay/prep at −1 machines/nn −1 robin/conj learned/prep at −1 tests/nn −1 florida object-oriented dancing marlins/nn −1 programming/amod −1 dancing/conj beach/appos −1 language/amod −1 dancing/conj −1 jacksonville/appos −1 framework/amod −1 singing/conj −1 tampa/appos −1 interface/amod −1 singing/conj florida/conj −1 software/amod −1 ballroom/nn not associated with subjects or objects of verbs (or their inverse), but rather with conjunctions, ap- positions, noun-compounds and adjectivial modi- fiers. Additionally, the collapsed preposition rela- tion is very useful (e.g. for capturing the school aspect of hogwarts). The presence of many con- junction contexts, such as superman/conj for batman and singing/conj for dancing, may explain the functional similarity observed in Sec- tion 4; conjunctions in natural language tend to en- force their conjuncts to share the same semantic types and inflections.</p><p>In the future, we hope that insights from such model introspection will allow us to develop better contexts, by focusing on conjunctions and prepo- sitions for example, or by trying to figure out why the subject and object relations are absent and finding ways of increasing their contributions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Australian</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Dependency-based context extraction example. Top: preposition relations are collapsed into single arcs, making telescope a direct modifier of discovers. Bottom: the contexts extracted for each word in the sentence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Recall-precision curve when attempting to rank the similar words above the related ones. (a) is based on the WordSim353 dataset, and (b) on the Chiarello et al. dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>scientist discovers star with telescope</head><label></label><figDesc></figDesc><table>amod 
nsubj 
dobj 

prep 

pobj 

Australian scientist discovers star telescope 

amod 
nsubj 
dobj 

prep with 

WORD 
CONTEXTS 

australian scientist/amod −1 
scientist australian/amod, discovers/nsubj −1 
discovers scientist/nsubj, star/dobj, telescope/prep with 
star 
discovers/dobj −1 
telescope discovers/prep with −1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Target words and their 5 most similar words, as in-
duced by different embeddings. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Words and their top syntactic contexts. 

</table></figure>

			<note place="foot" n="1"> code.google.com/p/word2vec/</note>

			<note place="foot" n="3"> DEPS generated a list of scientists whose name ends with &quot;ing&quot;. This is may be a result of occasional POS-tagging errors. Still, the embedding does a remarkable job and retrieves scientists, despite the noisy POS. The list contains more mathematicians without &quot;ing&quot; further down.</note>

			<note place="foot" n="4"> Some word pairs are judged to exhibit both types of similarity, and were ignored in this experiment.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We presented a generalization of the SKIP-GRAM embedding model in which the linear bag-of-words contexts are replaced with arbitrary ones, and experimented with dependency-based con-texts, showing that they produce markedly differ-ent kinds of similarities. These results are ex-pected, and follow similar findings in the distri-butional semantics literature. We also demon-strated how the resulting embedding model can be queried for the discriminative contexts for a given word, and observed that the learning procedure seems to favor relatively local syntactic contexts, as well as conjunctions and objects of preposition. We hope these insights will facilitate further re-search into improved context modeling and better, possibly task-specific, embedded representations. Our software, allowing for experimentation with arbitrary contexts, together with the embeddings described in this paper, are available for download at the authors' websites.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A study on similarity and relatedness using distributional and wordnet-based approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Alfonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Kravalova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Pasca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Soroa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies</title>
		<meeting>Human Language Technologies</meeting>
		<imprint>
			<publisher>The</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<title level="m">Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Boulder, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Polyglot: Distributed word representations for multilingual nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL</title>
		<meeting>of CoNLL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Distributional memory: A general framework for corpus-based semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="673" to="721" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Class-based n-gram models of natural</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Peter F Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent J Della</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenifer C</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Extracting semantic representations from word co-occurrence statistics: A computational study. Behavior Research Methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph P</forename><surname>Bullinaria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="510" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semantic and associative priming in the cerebral hemispheres: Some words do, some words don&apos;t... sometimes, some places</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Chiarello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Curt</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorie</forename><surname>Richards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alma</forename><surname>Pollock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain and Language</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="75" to="104" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Domain adaptation of a dependency parser with a class-class selectional preference model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2012 Student Research Workshop</title>
		<meeting>ACL 2012 Student Research Workshop<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-07" />
			<biblScope unit="page" from="43" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning</title>
		<meeting>the 25th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The Stanford typed dependencies representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation</title>
		<meeting><address><addrLine>Manchester, UK, August. Coling</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Placing search in context: The concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="116" to="131" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">word2vec explained: deriving mikolov et al.&apos;s negativesampling word-embedding method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1402.3722</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A dynamic oracle for the arc-eager system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
		<meeting>of COLING</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Training deterministic parsers with non-deterministic oracles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zellig</forename><surname>Harris</surname></persName>
		</author>
		<title level="m">Distributional structure. Word</title>
		<imprint>
			<date type="published" when="1954" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="146" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Linguistic regularities in sparse and explicit word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning<address><addrLine>Maryland, USA, June</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automatic retrieval and clustering of similar words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics</title>
		<meeting>the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1998" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="768" to="774" />
		</imprint>
	</monogr>
	<note>ACL &apos;98</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Extensions of recurrent neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kombrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Cernocky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="5528" to="5531" />
		</imprint>
	</monogr>
	<note>2011 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Lake Tahoe, Nevada, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-12-05" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
	<note>Proceedings of a meeting held</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A scalable hierarchical distributed language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1081" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dependency-based construction of semantic space models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="161" to="199" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A latent dirichlet allocation method for selectional preferences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mausam</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="424" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Latent variable models of selectional preference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diarmuid´odiarmuid´</forename><surname>Diarmuid´o Séaghdha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="435" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Feature-rich part-of-speech tagging with a cyclic dependency network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">From frequency to meaning: Vector space models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">D</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Domain and function: A dualspace model of semantic relations and compositions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="533" to="585" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Distributed word clustering for large scale class-based language modeling in machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Brants</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="755" to="762" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
