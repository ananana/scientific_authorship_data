<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Max-Margin Tensor Neural Network for Chinese Word Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Pei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ministry of Education School of Electronics Engineering and Computer Science</orgName>
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University Beijing</orgName>
								<address>
									<postCode>100871</postCode>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ge</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ministry of Education School of Electronics Engineering and Computer Science</orgName>
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University Beijing</orgName>
								<address>
									<postCode>100871</postCode>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ministry of Education School of Electronics Engineering and Computer Science</orgName>
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University Beijing</orgName>
								<address>
									<postCode>100871</postCode>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Max-Margin Tensor Neural Network for Chinese Word Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="293" to="303"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recently, neural network models for natural language processing tasks have been increasingly focused on for their ability to alleviate the burden of manual feature engineering. In this paper, we propose a novel neural network model for Chinese word segmentation called Max-Margin Tensor Neural Network (MMTNN). By exploiting tag embeddings and tensor-based transformation, MMTNN has the ability to model complicated interactions between tags and context characters. Furthermore , a new tensor factorization approach is proposed to speed up the model and avoid overfitting. Experiments on the benchmark dataset show that our model achieves better performances than previous neural network models and that our model can achieve a competitive performance with minimal feature engineering. Despite Chinese word segmentation being a specific case, MMTNN can be easily generalized and applied to other sequence labeling tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Unlike English and other western languages, Chi- nese do not delimit words by white-space. There- fore, word segmentation is a preliminary and im- portant pre-process for Chinese language process- ing. Most previous systems address this problem by treating this task as a sequence labeling prob- lem where each character is assigned a tag indi- cating its position in the word. These systems are effective because researchers can incorporate a large body of handcrafted features into the models. However, the ability of these models is restricted * Corresponding author by the design of features and the number of fea- tures could be so large that the result models are too large for practical use and prone to overfit on training corpus.</p><p>Recently, neural network models have been in- creasingly focused on for their ability to mini- mize the effort in feature engineering. <ref type="bibr" target="#b6">Collobert et al. (2011)</ref> developed the SENNA system that ap- proaches or surpasses the state-of-the-art systems on a variety of sequence labeling tasks for English. <ref type="bibr" target="#b35">Zheng et al. (2013)</ref> applied the architecture of <ref type="bibr" target="#b6">Collobert et al. (2011)</ref> to Chinese word segmenta- tion and POS tagging and proposed a perceptron- style algorithm to speed up the training process with negligible loss in performance.</p><p>Workable as previous neural network models seem, a limitation of them to be pointed out is that the tag-tag interaction, tag-character inter- action and character-character interaction are not well modeled. In conventional feature-based lin- ear (log-linear) models, these interactions are ex- plicitly modeled as features. Take phrase "打篮 球(play basketball)" as an example, assuming we are labeling character C 0 ="篮", possible features could be:</p><formula xml:id="formula_0">f 1 =</formula><p>1 C −1 ="打" and C 1 ="球" and y 0 ="B" 0 else f 2 = 1 C 0 ="篮" and y 0 ="B" and y −1 ="S" 0 else</p><p>To capture more interactions, researchers have de- signed a large number of features based on linguis- tic intuition and statistical information. In previ- ous neural network models, however, hardly can such interactional effects be fully captured rely- ing only on the simple transition score and the sin- gle non-linear transformation (See section 2). In order to address this problem, we propose a new model called Max-Margin Tensor Neural Network (MMTNN) that explicitly models the interactions between tags and context characters by exploiting tag embeddings and tensor-based transformation. Moreover, we propose a tensor factorization ap- proach that effectively improves the model effi- ciency and prevents from overfitting. We evalu- ate the performance of Chinese word segmentation on the PKU and MSRA benchmark datasets in the second International Chinese Word Segmentation Bakeoff ( <ref type="bibr" target="#b8">Emerson, 2005</ref>) which are commonly used for evaluation of Chinese word segmentation. Experiment results show that our model outper- forms other neural network models. Although we focus on the question that how far we can go without using feature engineering in this paper, the study of deep learning for NLP tasks is still a new area in which it is currently challenging to surpass the state-of-the-art with- out additional features. Following , we wonder how well our model can per- form with minimal feature engineering. There- fore, we integrate additional simple character bi- gram features into our model and the result shows that our model can achieve a competitive perfor- mance that other systems hardly achieve unless they use more complex task-specific features.</p><p>The main contributions of our work are as fol- lows:</p><p>• We propose a Max-Margin Tensor Neu- ral Network for Chinese word segmentation without feature engineering. The test re- sults on the benchmark dataset show that our model outperforms previous neural network models.</p><p>• We propose a new tensor factorization ap- proach that models each tensor slice as the product of two low-rank matrices. Not only does this approach improve the efficiency of our model but also it avoids the risk of over- fitting.</p><p>• Compared with previous works that use a large number of handcrafted features, our model can achieve a competitive perfor- mance with minimal feature engineering.</p><p>• Despite Chinese word segmentation being a specific case, our approach can be easily gen- eralized to other sequence labeling tasks.</p><p>The remaining part of this paper is organized as follows. Section 2 describes the details of con- ventional neural network architecture. Section 3 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Conventional Neural Network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Lookup Table</head><p>The idea of distributed representation for symbolic data is one of the most important reasons why the neural network works. It was proposed by <ref type="bibr" target="#b10">Hinton (1986)</ref> and has been a research hot spot for more than twenty years ( <ref type="bibr" target="#b0">Bengio et al., 2003;</ref><ref type="bibr" target="#b6">Collobert et al., 2011;</ref><ref type="bibr" target="#b20">Schwenk et al., 2012;</ref><ref type="bibr" target="#b15">Mikolov et al., 2013a</ref>). Formally, in the Chinese word seg- mentation task, we have a character dictionary D of size |D|. Unless otherwise specified, the char- acter dictionary is extracted from the training set and unknown characters are mapped to a special symbol that is not used elsewhere. Each character c ∈ D is represented as a real-valued vector (char- acter embedding) Embed(c) ∈ R d where d is the dimensionality of the vector space. The charac- ter embeddings are then stacked into a embedding matrix M ∈ R d×|D| . For a character c ∈ D that has an associated index k, the corresponding char- acter embedding Embed(c) ∈ R d is retrieved by the Lookup Table layer as shown in <ref type="figure" target="#fig_0">Figure 1</ref>:</p><formula xml:id="formula_1">Embed(c) = M e k<label>(1)</label></formula><p>Here e k ∈ R |D| is a binary vector which is zero in all positions except at k-th index. The Lookup Ta- ble layer can be seen as a simple projection layer where the character embedding for each context character is achieved by table lookup operation ac- cording to their indices. The embedding matrix M is initialized with small random numbers and trained by back-propagation. We will analyze in more detail about the effect of character embed- dings in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Tag Scoring</head><p>The most common tagging approach is the win- dow approach. The window approach assumes that the tag of a character largely depends on its neighboring characters. Given an input sentence c <ref type="bibr">[1:n]</ref> , a window of size w slides over the sentence from character c 1 to c n . We set w = 5 in all experiments. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, at position c i , 1 ≤ i ≤ n, the context characters are fed into the Lookup Table layer. The characters exceeding the sentence boundaries are mapped to one of two special symbols, namely "start" and "end" sym- bols. The character embeddings extracted by the Lookup <ref type="table">Table layer</ref> are then concatenated into a single vector a ∈ R H 1 , where H 1 = w · d is the size of Layer 1. Then a is fed into the next layer which performs linear transformation fol- lowed by an element-wise activation function g such as tanh, which is used in our experiments:</p><formula xml:id="formula_2">h = g(W 1 a + b 1 )<label>(2)</label></formula><p>where</p><formula xml:id="formula_3">W 1 ∈ R H 2 ×H 1 , b 1 ∈ R H 2 ×1 , h ∈ R H 2 . H 2</formula><p>is a hyper-parameter which is the number of hid- den units in Layer 2. Given a set of tags T of size |T |, a similar linear transformation is performed except that no non-linear function is followed:</p><formula xml:id="formula_4">f (t|c [i−2:i+2] ) = W 2 h + b 2 (3)</formula><p>where</p><formula xml:id="formula_5">W 2 ∈ R |T |×H 2 , b 2 ∈ R |T |×1 . f (t|c [i−2:i+2] ) ∈ R |T |</formula><p>is the score vector for each possible tag. In Chinese word segmentation, the most prevalent tag set T is BMES tag set, which uses 4 tags to carry word boundary information. It uses B, M, E and S to denote the Beginning, the Middle, the End of a word and a Single character forming a word respectively. We use this tag set in our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Model Training and Inference</head><p>Despite sharing commonalities mentioned above, previous work models the segmentation task dif- ferently and therefore uses different training and inference procedure.  mod- eled Chinese word segmentation as a series of classification task at each position of the sentence in which the tag score is transformed into proba- bility using softmax function:</p><formula xml:id="formula_6">p(t i |c [i−2:i+2] ) = exp(f (t i |c [i−2:i+2] )) t exp(f (t |c [i−2:i+2] ))</formula><p>The model is then trained in MLE-style which maximizes the log-likelihood of the tagged data.</p><p>Obviously, it is a local model which cannot cap- ture the dependency between tags and does not support to infer the tag sequence globally.</p><p>To model the tag dependency, previous neural network models <ref type="bibr" target="#b6">(Collobert et al., 2011;</ref><ref type="bibr" target="#b35">Zheng et al., 2013</ref>) introduce a transition score A ij for jumping from tag i ∈ T to tag j ∈ T . For a input sentence c <ref type="bibr">[1:n]</ref> with a tag sequence t <ref type="bibr">[1:n]</ref> , a sentence-level score is then given by the sum of transition and network scores:</p><formula xml:id="formula_7">s(c [1:n] , t [1:n] , θ) = n i=1 (A t i−1 t i +f θ (t i |c [i−2:i+2] )) (4) where f θ (t i |c [i−2:i+2] )</formula><p>indicates the score output for tag t i at the i-th character by the network with parameters θ = (M, A, W 1 , b 1 , W 2 , b 2 ). Given the sentence-level score, <ref type="bibr" target="#b35">Zheng et al. (2013)</ref> proposed a perceptron-style training algorithm in- spired by the work of <ref type="bibr" target="#b4">Collins (2002)</ref>. Compared with , their model is a global one where the training and inference is performed at sentence-level.</p><p>Workable as these methods seem, one of the limitations of them is that the tag-tag interaction and the neural network are modeled seperately. The simple tag-tag transition neglects the impact of context characters and thus limits the ability to capture flexible interactions between tags and context characters. Moreover, the simple non- linear transformation in equation <ref type="formula" target="#formula_2">(2)</ref> is also poor to model the complex interactional effects in Chi- nese word segmentation. . Then the tag embedding Embed(t) ∈ R d for tag t ∈ T with index k can be retrieved by the lookup operation:</p><formula xml:id="formula_8">Embed(t) = Le k (5)</formula><p>where e k ∈ R |T |×1 is a binary vector which is zero in all positions except at k-th index. The tag embeddings start from a random initialization and can be automatically trained by back-propagation. <ref type="figure" target="#fig_1">Figure 2</ref> shows the new Lookup <ref type="table">Table layer</ref> with tag embeddings. Assuming we are at the i-th char- acter of a sentence, besides the character embed- dings, the tag embeddings of the previous tags are also considered 1 . For a fast tag inference, only the previous tag t i−1 is used in our model even though a longer history of tags can be considered. The concatenation operation in Layer 1 then con- catenates the character embeddings and tag em- bedding together into a long vector a. In this way, the tag representation can be directly incorporated in the neural network so that the tag-tag interac- tion and tag-character interaction can be explicitly modeled in deeper layers (See Section 3.2). More- over, the transition score in equation <ref type="formula">(4)</ref> is not necessary in our model, because, by incorporating tag embedding into the neural network, the effect of tag-tag interaction and tag-character interaction are covered uniformly in one same model. Now equation <ref type="formula">(4)</ref> can be rewritten as follows:</p><formula xml:id="formula_9">s(c [1:n] , t [1:n] , θ) = n i=1 f θ (t i |c [i−2:i+2] , t i−1 )<label>(6)</label></formula><p>where f θ (t i |c <ref type="bibr">[i−2:i+2]</ref> , t i−1 ) is the score output for tag t i at the i-th character by the network with pa- rameters θ. Like Collobert et al. (2011) and <ref type="bibr" target="#b35">Zheng et al. (2013)</ref>, our model is also trained at sentence- level and carries out inference globally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Tensor Neural Network</head><p>A tensor is a geometric object that describes rela- tions between vectors, scalars, and other tensors. It can be represented as a multi-dimensional array of numerical values. An advantage of the tensor is that it can explicitly model multiple interactions in data. As a result, tensor-based model have been widely used in a variety of tasks ( <ref type="bibr" target="#b19">Salakhutdinov et al., 2007;</ref><ref type="bibr" target="#b22">Socher et al., 2013b</ref>).</p><p>In Chinese word segmentation, a proper model- ing of the tag-tag interaction, tag-character inter- action and character-character interaction is very important. In linear models, these kinds of inter- actions are usually modeled as features. In con- ventional neural network models, however, the in- put embeddings only implicitly interact through the non-linear function which can hardly model the complexity of the interactions. Given the ad- vantage of tensors, we apply a tensor-based trans- formation to the input vector. Formally, we use a 3-way tensor V [1:H 2 ] ∈ R H 2 ×H 1 ×H 1 to directly model the interactions, where H 2 is the size of Layer 2 and H 1 = (w + 1) · d is the size of con- catenated vector a in Layer 1 as shown in <ref type="figure" target="#fig_1">Figure  2</ref>. <ref type="figure" target="#fig_2">Figure 3</ref> gives an example of the tensor-based transformation 2 . The output of a tensor product is a vector z ∈ R H 2 where each dimension z i is the result of the bilinear form defined by each tensor slice</p><formula xml:id="formula_10">V [i] ∈ R H 1 ×H 1 : z = a T V [1:H 2 ] a; z i = a T V [i] a = j,k V [i] jk a j a k (7)</formula><p>Since vector a is the concatenation of character embeddings and the tag embedding, equation <ref type="formula">(7)</ref> can be written in the following form:</p><formula xml:id="formula_11">z i = p,q j,k V [i] (p,q,j,k) E [p] j E [q] k where E [p]</formula><p>j is the j-th element of the p-th embed- ding in <ref type="table">Lookup Table layer</ref> and V <ref type="bibr">[i]</ref> (p,q,j,k) is the cor- responding coefficient for E <ref type="bibr">[p]</ref> j and E <ref type="bibr">[q]</ref> k in V <ref type="bibr">[i]</ref> . As we can see, in each tensor slice i, the em- beddings are explicitly related in a bilinear form which captures the interactions between charac- ters and tags. The multiplicative operations be- tween tag embeddings and character embeddings can somehow be seen as "feature combination", which are hand-designed in feature-based models. Our model learns the information automatically and encodes them in tensor parameters and em- beddings. Intuitively, we can interpret each slice of the tensor as capturing a specific type of tag- character interaction and character-character inter- action.</p><p>Combining the tensor product with linear trans- formation, the tensor-based transformation in Layer 2 is defined as:</p><formula xml:id="formula_12">h = g(a T V [1:H 2 ] a + W 1 a + b 1 )<label>(8)</label></formula><p>where</p><formula xml:id="formula_13">W 1 ∈ R H 2 ×H 1 , b 1 ∈ R H 2 ×1 , h ∈ R H 2 .</formula><p>In fact, equation <ref type="formula" target="#formula_2">(2)</ref> used in previous work is a special case of equation <ref type="formula" target="#formula_12">(8)</ref> when V is set to 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Tensor Factorization</head><p>Despite tensor-based transformation being effec- tive for capturing the interactions, introducing tensor-based transformation into neural network models to solve sequence labeling task is time pro- hibitive since the tensor product operation drasti- cally slows down the model. Without consider- ing matrix optimization algorithms, the complex- ity of the non-linear transformation in equation <ref type="formula" target="#formula_2">(2)</ref> 2 The bias term is omitted in <ref type="figure" target="#fig_2">Figure 3</ref> for simplicity is O <ref type="figure" target="#fig_0">(H 1 H 2 )</ref> while the tensor operation complex- ity in equation <ref type="formula" target="#formula_12">(8)</ref> is O(H 2 1 H 2 ). The tensor-based transformation is H 1 times slower. Moreover, the additional tensor could bring millions of param- eters to the model which makes the model suf- fer from the risk of overfitting. To remedy this, we propose a tensor factorization approach that factorizes each tensor slice as the product of two low-rank matrices. Formally, each tensor slice</p><formula xml:id="formula_14">V [i] ∈ R H 1 ×H 1 is factorized into two low rank matrix P [i] ∈ R H 1 ×r and Q [i] ∈ R r×H 1 :</formula><formula xml:id="formula_15">V [i] = P [i] Q [i] , 1 ≤ i ≤ H 2<label>(9)</label></formula><p>where r H 1 is the number of factors. Substi- tuting equation (9) into equation <ref type="formula" target="#formula_12">(8)</ref>, we get the factorized tensor function: <ref type="figure" target="#fig_3">Figure 4</ref> illustrates the operation in each slice of the factorized tensor. First, vector a is projected into two r-dimension vectors f 1 and f 2 . Then the output z i for each tensor slice i is the dot-product of f 1 and f 2 . The complexity of the tensor op- eration is now O(rH 1 H 2 ). As long as r is small enough, the factorized tensor operation would be much faster than the un-factorized one and the number of free parameters would also be much smaller, which prevent the model from overfitting.</p><formula xml:id="formula_16">h = g(a T P [1:H 2 ] Q [1:H 2 ] a + W 1 a + b 1 ) (10)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Max-Margin Training</head><p>We use the Max-Margin criterion to train our model. Intuitively, the Max-Margin criterion pro- vides an alternative to probabilistic, likelihood- based estimation methods by concentrating di- rectly on the robustness of the decision boundary of a model ( <ref type="bibr" target="#b27">Taskar et al., 2005</ref>). We use Y (x i ) to denote the set of all possible tag sequences for a given sentence x i and the correct tag sequence for x i is y i . The parameters of our model are</p><formula xml:id="formula_17">θ = {W 1 , b 1 , W 2 , b 2 , M, L, P [1:H 2 ] , Q [1:H 2 ] }.</formula><p>We first define a structured margin loss (y i , ˆ y) for predicting a tag sequencê y for a given correct tag sequence y i :</p><formula xml:id="formula_18">(y i , ˆ y) = n j κ1{y i,j = ˆ y j } (11)</formula><p>where n is the length of sentence x i and κ is a dis- count parameter. The loss is proportional to the number of characters with an incorrect tag in the proposed tag sequence, which increases the more incorrect the proposed tag sequence is. For a given training instance (x i , y i ), we search for the tag se- quence with the highest score:</p><formula xml:id="formula_19">y * = arg maxˆy∈Y maxˆ maxˆy∈Y (x) s(x i , ˆ y, θ)<label>(12)</label></formula><p>where the tag sequence is found and scored by the Tensor Neural Network via the function s in equa- tion (6). The object of Max-Margin training is that the highest scoring tag sequence is the correct one: y * = y i and its score will be larger up to a margin to other possible tag sequencesˆysequencesˆ sequencesˆy ∈ Y (x i ):</p><formula xml:id="formula_20">s(x, y i , θ) ≥ s(x, ˆ y, θ) + (y i , ˆ y)</formula><p>This leads to the regularized objective function for m training examples:</p><formula xml:id="formula_21">J(θ) = 1 m m i=1 l i (θ) + λ 2 ||θ|| 2 l i (θ) = maxˆy∈Y maxˆ maxˆy∈Y (x i ) (s(x i , ˆ y, θ) + (y i , ˆ y)) −s(x i , y i , θ))<label>(13)</label></formula><p>By minimizing this object, the score of the correct tag sequence y i is increased and score of the high- est scoring incorrect tag sequencê y is decreased. The objective function is not differentiable due to the hinge loss. We use a generalization of gra- dient descent called subgradient method ( <ref type="bibr" target="#b18">Ratliff et al., 2007</ref>) which computes a gradient-like direc- tion. The subgradient of equation <ref type="formula" target="#formula_1">(13)</ref> is:</p><formula xml:id="formula_22">∂J ∂θ = 1 m i ( ∂s(x i , ˆ y max , θ) ∂θ − ∂s(x i , y i , θ) ∂θ )+λθ</formula><p>wherê y max is the tag sequence with the highest score in equation <ref type="formula" target="#formula_1">(13)</ref>. Following <ref type="bibr" target="#b21">Socher et al. (2013a)</ref>, we use the diagonal variant of AdaGrad PKU MSRA Identical words 5.5 × 10 4 8.8 × 10 4 Total words 1.1 × 10 6 2.4 × 10 6 Identical characters 5 × 10 3 5 × 10 3 Total characters 1.8 × 10 6 4.1 × 10 6   <ref type="bibr" target="#b7">Duchi et al., 2011</ref>) with minibatchs to minimize the objective. The parameter update for the i-th parameter θ t,i at time step t is as follows:</p><formula xml:id="formula_23">θ t,i = θ t−1,i − α t τ =1 g 2 τ,i g t,i<label>(14)</label></formula><p>where α is the initial learning rate and g τ ∈ R |θ i | is the subgradient at time step τ for parameter θ i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data and Model Selection</head><p>We use the PKU and MSRA data provided by the second International Chinese Word Segmentation Bakeoff <ref type="bibr" target="#b8">(Emerson, 2005</ref>) to test our model. They are commonly used by previous state-of-the-art models and neural network models. Details of the data are listed in <ref type="table" target="#tab_0">Table 1</ref>. For evaluation, we use the standard bake-off scoring program to calculate precision, recall, F1-score and out-of-vocabulary (OOV) word recall.</p><p>For model selection, we use the first 90% sen- tences in the training data for training and the rest 10% sentences as development data. The mini- batch size is set to 20. Generally, the number of hidden units has a limited impact on the perfor- mance as long as it is large enough. We found that 50 is a good trade-off between speed and model performance. The dimensionality of char- acter (tag) embedding is set to 25 which achieved the best performance and faster than 50-or 100- dimensional ones. We also validated on the num- ber of factors for tensor factorization. The per- formance is not boosted and the training time in-  creases drastically when the number of factors is larger than 10. We hypothesize that larger factor size results in too many parameters to train and hence perform worse. The final hyperparameters of our model are set as in <ref type="table" target="#tab_1">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiment Results</head><p>We first perform a close test <ref type="bibr">3</ref> on the PKU dataset to show the effect of different model configura- tions. We also compare our model with the CRF model ( <ref type="bibr" target="#b12">Lafferty et al., 2001</ref>), which is a widely used log-linear model for Chinese word segmen- tation. The input feature to the CRF model is sim- ply the context characters (unigram feature) with- out any additional feature engineering. We use an open source toolkit CRF++ 4 to train the CRF model. All the neural networks are trained us- ing the Max-Margin approach described in Sec- tion 3.4. <ref type="table" target="#tab_3">Table 3</ref> summarizes the test results.</p><p>As we can see, by using Tag embedding, the F- score is improved by +0.6% and OOV recall is improved by +1.0%, which shows that tag embed- dings succeed in modeling the tag-tag interaction and tag-character interaction. Model performance is further boosted after using tensor-based trans- formation. The F-score is improved by +0.6% while OOV recall is improved by +3.2%, which denotes that tensor-based transformation captures more interactional information than simple non- linear transformation. Another important result in <ref type="table" target="#tab_3">Table 3</ref> is that our neural network models perform much better than CRF-based model when only unigram features are used. Compared with CRF, there are two differ- ences in neural network models. First, the discrete feature vector is replaced with dense character em- beddings. Second, the non-linear transformation <ref type="table">Table 4</ref>: Examples of character embeddings is used to discover higher level representation. In fact, CRF can be regarded as a special neural net- work without non-linear function ( <ref type="bibr" target="#b30">Wang and Manning, 2013</ref>). <ref type="bibr" target="#b30">Wang and Manning (2013)</ref> conduct an empirical study on the effect of non-linearity and the results suggest that non-linear models are highly effective only when distributed representa- tion is used. To explain why distributed represen- tation captures more information than discrete fea- tures, we show in <ref type="table">Table 4</ref> the effect of character embeddings which are obtained from the lookup table of MMTNN after training. The first row lists three characters we are interested in. In each col- umn, we list the top 5 characters that are near- est (measured by Euclidean distance) to the cor- responding character in the first row according to their embeddings. As we can see, characters in the first column are all Chinese number characters and characters in the second column and the third column are all Chinese family names and Chinese punctuations respectively. Therefore, compared with discrete feature representations, distributed representation can capture the syntactic and se- mantic similarity between characters. As a re- sult, the model can still perform well even if some words do not appear in the training cases.</p><formula xml:id="formula_24">一 一 一(one) 李 李 李(Li) 。 。 。(period) 二(two) 赵(Zhao) ，(comma) 三(three) 蒋(Jiang) ：(colon) 四(four) 孔(Kong) ？(question mark) 五(five) 冯(Feng) "(quotation mark) 六(six) 吴(Wu) 、(Chinese comma)</formula><p>We further compare our model with previous neural network models on both PKU and MSRA datasets. Since <ref type="bibr" target="#b35">Zheng et al. (2013)</ref> did not report the results on the these datasets, we re- implemented their model and tested it on the test data. The results are listed in the first three rows of <ref type="table" target="#tab_5">Table 5</ref>, which shows that our model achieved higher F-score than the previous neural network models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Unsupervised Pre-training</head><p>Previous work found that the performance can be improved by pre-training the character em- beddings on large unlabeled data and using the obtained embeddings to initialize the charac- ter lookup  They constructed a neural network that outputs high scores for windows that occur in the cor- pus and low scores for windows where one char- acter is replaced by a random one. <ref type="bibr" target="#b15">Mikolov et al. (2013a)</ref> proposed a faster skip-gram model word2vec 5 which tries to maximize classification of a word based on another word in the same sen- tence. In this paper, we use word2vec because pre- liminary experiments did not show differences be- tween performances of these models but word2vec is much faster to train. We pre-train the embed- dings on the Chinese Giga-word corpus <ref type="bibr" target="#b9">(Graff and Chen, 2005</ref>). As shown in <ref type="table" target="#tab_5">Table 5</ref> (last three rows), both the F-score and OOV recall of our model boost by using pre-training. Our model still outperforms other models after pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Minimal Feature Engineering</head><p>Although we focus on the question that how far we can go without using feature engineering in this paper, the study of deep learning for NLP tasks is still a new area in which it is currently chal- lenging to surpass the state-of-the-art without ad- ditional features. To incorporate features into the neural network,  proposed the feature-based neural network where each con- text feature is represented as feature embeddings.  A very common feature in Chinese word seg- mentation is the character bigram feature. For- mally, at the i-th character of a sentence c <ref type="bibr">[1:n]</ref> , the bigram features are c k c k+1 (i − 3 &lt; k &lt; i + 2). In our model, the bigram features are extracted in the window context and then the corresponding bigram embeddings are concatenated with char- acter embeddings in Layer 1 and fed into Layer 2. In , the bigram embed- dings are pre-trained on unlabeled data with char- acter embeddings, which significantly improves the model performance. Given the long time for pre-training bigram embeddings, we only pre-train the character embeddings and the bigram embed- dings are initialized as the average of character embeddings of c k and c k+1 . Further improve- ment could be obtained if the bigram embeddings are also pre-trained. <ref type="table" target="#tab_7">Table 6</ref> lists the segmenta- tion performances of our model as well as pre- vious state-of-the-art systems. When bigram fea- tures are added, the F-score of our model improves from 94.0% to 95.2% on PKU dataset and from 94.9% to 97.2% on MSRA dataset. It is a com- petitive result given that our model only use sim- ple bigram features while other models use more complex features. For example, <ref type="bibr" target="#b25">Sun et al. (2012)</ref> uses additional word-based features. <ref type="bibr" target="#b34">Zhang et al. (2013)</ref> uses eight types of features such as Mu- tual Information and Accessor Variety and they extract dynamic statistical features from both an in-domain corpus and an out-of-domain corpus us- ing co-training. Since feature engineering is not the main focus of this paper, we did not experi- ment with more features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Chinese word segmentation has been studied with considerable efforts in the NLP community. The most popular approach treats word segmentation as a sequence labeling problem which was first proposed in <ref type="bibr" target="#b31">Xue (2003)</ref>. Most previous systems address this task by using linear statistical mod- els with carefully designed features such as bi- gram features, punctuation information ( <ref type="bibr" target="#b13">Li and Sun, 2009)</ref> and statistical information ( <ref type="bibr" target="#b23">Sun and Xu, 2011)</ref>. Recently, researchers have tended to explore new approaches for word segmentation which circumvent the feature engineering by au- tomatically learning features with neural network models ( <ref type="bibr" target="#b35">Zheng et al., 2013)</ref>. Our study is consistent with this line of research, however, our model explicitly models the interac- tions between tags and context characters and ac- cordingly captures more semantic information.</p><p>Tensor-based transformation was also used in other neural network models for its ability to cap- ture multiple interactions in data. For example, <ref type="bibr" target="#b22">Socher et al. (2013b)</ref> exploited tensor-based func- tion in the task of Sentiment Analysis to cap- ture more semantic information from constituents. However, given the small size of their tensor ma- trix, they do not have the problem of high time cost and overfitting problem as we faced in mod- eling a sequence labeling task like Chinese word segmentation. That's why we propose to decrease computational cost and avoid overfitting with ten- sor factorization.</p><p>Various tensor factorization (decomposition) methods have been proposed recently for tensor- based dimension reduction <ref type="bibr" target="#b3">(Cohen et al., 2013;</ref><ref type="bibr" target="#b29">Van de Cruys et al., 2013;</ref>. For example,  proposed the Multi-Relational Latent Semantic Analysis. Sim- ilar to LSA, a low rank approximation of the ten- sor is derived using a tensor decomposition ap- proch. Similar ideas were also used for collab- orative filtering <ref type="bibr" target="#b19">(Salakhutdinov et al., 2007)</ref> and object recognition ( <ref type="bibr" target="#b17">Ranzato et al., 2010)</ref>. Our ten- sor factorization is related to these work but uses a different tensor factorization approach. By in- troducing tensor factorization into the neural net- work model for sequence labeling tasks, the model training and inference are speeded up and overfit- ting is prevented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose a new model called Max- Margin Tensor Neural Network that explicitly models the interactions between tags and context characters. Moreover, we propose a tensor fac- torization approach that effectively improves the model efficiency and avoids the risk of overfitting. Experiments on the benchmark datasets show that our model achieve better results than previous neu- ral network models and that our model can achieve a competitive result with minimal feature engi- neering. In the future, we plan to further extend our model and apply it to other structure predic- tion problems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Conventional Neural Network</figDesc><graphic url="image-1.png" coords="2,307.87,62.81,217.08,183.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Max-Margin Tensor Neural Network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The tensor-based transformation in Layer 2. a is the input from Layer 1. V is the tensor parameter. Each dashed box represents one of the H 2-many tensor slices, which defines the bilinear form on vector a.</figDesc><graphic url="image-2.png" coords="4,72.00,62.80,225.04,170.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Tensor product with tensor factorization</figDesc><graphic url="image-4.png" coords="5,327.99,62.81,176.85,141.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 : Details of the PKU and MSRA datasets</head><label>1</label><figDesc></figDesc><table>Window size 
w = 5 
Character(tag) embedding size d = 25 
Hidden unit number 
H 2 = 50 
Number of factors 
r = 10 
Initial learning rate 
α = 0.2 
Margin loss discount 
κ = 0.2 
Regularization 
λ = 10 −4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 : Hyperparameters of our model (</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Test results with different configurations. 
NN stands for the conventional neural network. 
NN+Tag Embed stands for the neural network 
with tag embeddings. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>table instead of random initialization</head><label>instead</label><figDesc></figDesc><table>299 

Models 

PKU 
MSRA 
P 
R 
F 
OOV 
P 
R 
F 
OOV 
(Mansur et al., 2013) 
87.1 87.9 87.5 48.9 
92.3 92.2 92.2 53.7 
(Zheng et al., 2013) 
92.8 92.0 92.4 63.3 
92.9 93.6 93.3 55.7 
MMTNN 
93.7 93.4 93.5 64.2 
94.6 94.2 94.4 61.4 
(Mansur et al., 2013) + Pre-training 91.2 92.7 92.0 68.8 
93.1 93.1 93.1 59.7 
(Zheng et al., 2013) + Pre-training 
93.5 92.2 92.8 69.0 
94.2 93.7 93.9 64.1 
MMTNN + Pre-training 
94.4 93.6 94.0 69.0 
95.2 94.6 94.9 64.8 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Comparison with previous neural network models 

(Mansur et al., 2013; Zheng et al., 2013). Mikolov 
et al. (2013b) show that pre-trained embeddings 
can capture interesting semantic and syntactic in-
formation such as king−man+woman ≈ queen 
on English data. There are several ways to learn 
the embeddings on unlabeled data. Mansur et al. 
(2013) used the model proposed by Bengio et al. 
(2003) which learns the embeddings based on neu-
ral language model. Zheng et al. (2013) followed 
the model proposed by Collobert et al. (2008). 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Comparison with state-of-the-art systems 

Formally, we assume the extracted features form a 
feature dictionary D f . Then each feature f ∈ D f 
is represented by a d-dimensional vector which is 
called feature embedding. Following their idea, 
we try to find out how well our model can perform 
with minimal feature engineering. 
</table></figure>

			<note place="foot" n="3"> Max-Margin Tensor Neural Network 3.1 Tag Embedding To better model the tag-tag interaction given the context characters, distributed representation for tags instead of traditional discrete symbolic representation is used in our model. Similar to character embeddings, given a fixed-sized tag set T , the tag embeddings for tags are stored in a tag embedding matrix L ∈ R d×|T | , where d is the dimensionality</note>

			<note place="foot" n="1"> We also tried the architecture in which the tag embedding of current tag is also considered, but this did not bring much improvement and runs slower</note>

			<note place="foot" n="3"> No other material or knowledge except the training data is allowed 4 http://crfpp.googlecode.com/svn/ trunk/doc/index.html?source=navbar</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by National Natural Science Foundation of China under Grant No. 61273318 and National Key Basic Research Pro-gram of China 2014CB340504.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-relational latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1602" to="1612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unigram language model for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th SIGHAN Workshop on Chinese Language Processing</title>
		<meeting>the 4th SIGHAN Workshop on Chinese Language Processing<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="138" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Approximate pcfg parsing using tensor decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Shay B Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Satta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="487" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-02 conference on Empirical methods in natural language processing</title>
		<meeting>the ACL-02 conference on Empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">999999</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The second international chinese word segmentation bakeoff</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Emerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing</title>
		<meeting>the Fourth SIGHAN Workshop on Chinese Language Processing</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">133</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Chinese gigaword. LDC Catalog No.: LDC2003T09, ISBN</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="58563" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning distributed representations of concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geoffrey E Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighth annual conference of the cognitive science society</title>
		<meeting>the eighth annual conference of the cognitive science society<address><addrLine>Amherst, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1986" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Factored 3-way restricted boltzmann machines for modeling natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="621" to="628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando Cn</forename><surname>Pereira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Punctuation as implicit annotations for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="505" to="512" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Feature-based neural language model and chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mairgup</forename><surname>Mansur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Joint Conference on Natural Language Processing</title>
		<meeting>the Sixth International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACLHLT</title>
		<meeting>NAACLHLT</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Factored 3-way restricted boltzmann machines for modeling natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">(online) subgradient methods for structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Nathan D Ratliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zinkevich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Restricted boltzmann machines for collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
		<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="791" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Large, pruned or continuous space language models on a gpu for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Rousseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Attik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT</title>
		<meeting>the NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="11" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Parsing with compositional vector grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Enhancing chinese word segmentation using unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="970" to="979" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A discriminative latent variable chinese segmenter with hybrid word/character information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaozhong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Matsuzaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="56" to="64" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fast online training with frequency-adaptive learning rates for chinese word segmentation and new word detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Jeju Island</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="253" to="262" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Association for Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">July</forename><surname>Korea</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning structured prediction models: A large margin approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassil</forename><surname>Chatalbashev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on Machine learning</title>
		<meeting>the 22nd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="896" to="903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A conditional random field word segmenter for sighan bakeoff</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huihsin</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichuan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galen</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing</title>
		<meeting>the Fourth SIGHAN Workshop on Chinese Language Processing</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">171</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A tensor-based factorization model of semantic compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Van De Cruys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Poibeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1142" to="1151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Effect of non-linear deep architecture in sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Joint Conference on Natural Language Processing</title>
		<meeting>the Sixth International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Chinese word segmentation as character tagging. Computational Linguistics and Chinese Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="29" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Chinese segmentation with a word-based perceptron algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ANNUAL MEETING-ASSOCIATION FOR COMPUTATIONAL LINGUISTICS</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page">840</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Subword-based tagging by conditional random fields for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Genichiro</forename><surname>Kikui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers</title>
		<meeting>the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="193" to="196" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Exploring representations from unlabeled data with co-training for Chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mairgup</forename><surname>Mansur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="311" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep learning for Chinese word segmentation and POS tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqing</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="647" to="657" />
		</imprint>
	</monogr>
	<note>, October. Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
