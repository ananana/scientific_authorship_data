<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:26+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Understanding the Geometry of Knowledge Graph Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chandrahas</surname></persName>
							<email>chandrahas@iisc.ac.in</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Indian Institute of Science</orgName>
								<orgName type="department" key="dep2">Indian Institute of Science</orgName>
								<orgName type="institution">Indian Institute of Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Sharma</surname></persName>
							<email>adityasharma@iisc.ac.in</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Indian Institute of Science</orgName>
								<orgName type="department" key="dep2">Indian Institute of Science</orgName>
								<orgName type="institution">Indian Institute of Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Indian Institute of Science</orgName>
								<orgName type="department" key="dep2">Indian Institute of Science</orgName>
								<orgName type="institution">Indian Institute of Science</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Understanding the Geometry of Knowledge Graph Embeddings</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="122" to="131"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>122</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Knowledge Graph (KG) embedding has emerged as a very active area of research over the last few years, resulting in the development of several embedding methods. These KG embedding methods represent KG entities and relations as vectors in a high-dimensional space. Despite this popularity and effectiveness of KG em-beddings in various tasks (e.g., link prediction), geometric understanding of such embeddings (i.e., arrangement of entity and relation vectors in vector space) is un-explored-we fill this gap in the paper. We initiate a study to analyze the geometry of KG embeddings and correlate it with task performance and other hyperparame-ters. To the best of our knowledge, this is the first study of its kind. Through extensive experiments on real-world datasets, we discover several insights. For example, we find that there are sharp differences between the geometry of embeddings learnt by different classes of KG embeddings methods. We hope that this initial study will inspire other follow-up research on this important but unexplored problem.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge Graphs (KGs) are multi-relational graphs where nodes represent entities and typed- edges represent relationships among entities. Re- cent research in this area has resulted in the de- velopment of several large KGs, such as NELL ( <ref type="bibr" target="#b8">Mitchell et al., 2015)</ref>, <ref type="bibr">YAGO (Suchanek et al., 2007)</ref>, and Freebase ( <ref type="bibr" target="#b0">Bollacker et al., 2008</ref>), among others. These KGs contain thousands of predicates (e.g., person, city, mayorOf(person, city), etc.), and millions of triples involving such predicates, e.g., (Bill de Blasio, mayorOf, New York City).</p><p>The problem of learning embeddings for Knowledge Graphs has received significant atten- tion in recent years, with several methods being proposed ( <ref type="bibr" target="#b1">Bordes et al., 2013;</ref><ref type="bibr" target="#b4">Lin et al., 2015;</ref><ref type="bibr" target="#b9">Nguyen et al., 2016;</ref><ref type="bibr" target="#b10">Nickel et al., 2016;</ref><ref type="bibr" target="#b16">Trouillon et al., 2016)</ref>. These methods represent enti- ties and relations in a KG as vectors in high di- mensional space. These vectors can then be used for various tasks, such as, link prediction, entity classification etc. Starting with <ref type="bibr">TransE (Bordes et al., 2013)</ref>, there have been many KG embed- ding methods such as TransH ( <ref type="bibr" target="#b17">Wang et al., 2014</ref>), TransR ( <ref type="bibr" target="#b4">Lin et al., 2015)</ref> and <ref type="bibr">STransE (Nguyen et al., 2016</ref>) which represent relations as trans- lation vectors from head entities to tail entities. These are additive models, as the vectors interact via addition and subtraction. Other KG embed- ding models, such as, DistMult <ref type="bibr" target="#b18">(Yang et al., 2014</ref>), HolE ( <ref type="bibr" target="#b10">Nickel et al., 2016), and</ref><ref type="bibr">ComplEx (Trouillon et al., 2016)</ref> are multiplicative where entity- relation-entity triple likelihood is quantified by a multiplicative score function. All these methods employ a score function for distinguishing correct triples from incorrect ones.</p><p>In spite of the existence of many KG embed- ding methods, our understanding of the geometry and structure of such embeddings is very shallow. A recent work <ref type="bibr" target="#b7">(Mimno and Thompson, 2017)</ref> an- alyzed the geometry of word embeddings. How- ever, the problem of analyzing geometry of KG embeddings is still unexplored -we fill this impor- tant gap. In this paper, we analyze the geometry of such vectors in terms of their lengths and conicity, which, as defined in Section 4, describes their po- sitions and orientations in the vector space. We later study the effects of model type and training hyperparameters on the geometry of KG embed- dings and correlate geometry with performance.</p><p>We make the following contributions:</p><p>• <ref type="table">We initiate a study to analyze the geometry of  various Knowledge Graph (KG) embeddings.</ref> To the best of our knowledge, this is the first study of its kind. We also formalize various metrics which can be used to study geometry of a set of vectors.</p><p>• Through extensive analysis, we discover sev- eral interesting insights about the geometry of KG embeddings. For example, we find systematic differences between the geome- tries of embeddings learned by additive and multiplicative KG embedding methods.</p><p>• We also study the relationship between geo- metric attributes and predictive performance of the embeddings, resulting in several new insights. For example, in case of multiplica- tive models, we observe that for entity vec- tors generated with a fixed number of neg- ative samples, lower conicity (as defined in Section 4) or higher average vector length lead to higher performance.</p><p>Source code of all the analysis tools de- veloped as part of this paper is available at https://github.com/malllabiisc/ kg-geometry. We are hoping that these re- sources will enable one to quickly analyze the geometry of any KG embedding, and potentially other embeddings as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In spite of the extensive and growing literature on both KG and non-KG embedding methods, very little attention has been paid towards understand- ing the geometry of the learned embeddings. A re- cent work <ref type="bibr" target="#b7">(Mimno and Thompson, 2017</ref>) is an ex- ception to this which addresses this problem in the context of word vectors. This work revealed a sur- prising correlation between word vector geometry and the number of negative samples used during training. Instead of word vectors, in this paper we focus on understanding the geometry of KG em- beddings. In spite of this difference, the insights we discover in this paper generalizes some of the observations in the work of <ref type="bibr" target="#b7">(Mimno and Thompson, 2017)</ref>. Please see Section 6.2 for more details.</p><p>Since KGs contain only positive triples, nega- tive sampling has been used for training KG em- beddings. Effect of the number of negative sam- ples in KG embedding performance was studied by <ref type="bibr" target="#b15">(Toutanova et al., 2015)</ref>. In this paper, we study the effect of the number of negative samples on KG embedding geometry as well as performance.</p><p>In addition to the additive and multiplicative KG embedding methods already mentioned in Section 1, there is another set of methods where the entity and relation vectors interact via a neu- ral network. Examples of methods in this cate- gory include <ref type="bibr">NTN (Socher et al., 2013</ref>), CONV ( <ref type="bibr" target="#b15">Toutanova et al., 2015)</ref>, <ref type="bibr">ConvE (Dettmers et al., 2017)</ref>, R-GCN ( <ref type="bibr" target="#b12">Schlichtkrull et al., 2017)</ref>, ER- MLP ( <ref type="bibr" target="#b3">Dong et al., 2014</ref>) and ER-MLP-2n <ref type="bibr" target="#b11">(Ravishankar et al., 2017)</ref>. Due to space limitations, in this paper we restrict our scope to the analysis of the geometry of additive and multiplicative KG embedding models only, and leave the analysis of the geometry of neural network-based methods as part of future work.  <ref type="bibr">ComplEx (Trouillon et al., 2016)</ref>. We refer to TransE, TransR and STransE as additive methods because they learn embeddings by modeling relations as translation vectors from one entity to another, which results in vectors interacting via the addition operation dur- ing training. On the other hand, we refer to Dist- Mult, HolE and ComplEx as multiplicative meth- ods as they quantify the likelihood of a triple be- longing to the KG through a multiplicative score function. The score functions optimized by these methods are summarized in <ref type="table" target="#tab_0">Table 1</ref>. Notation: Let G = (E, R, T ) be a Knowledge Graph (KG) where E is the set of entities, R is the set of relations and T ⊂ E × R × E is the set of triples stored in the graph. Most of the KG em- bedding methods learn vectors e ∈ R de for e ∈ E, and r ∈ R dr for r ∈ R. Some methods also learn projection matrices M r ∈ R dr×de for rela- tions. The correctness of a triple is evaluated using a model specific score function σ : E × R × E → R. For learning the embeddings, a loss function L(T , T ; θ), defined over a set of positive triples T , set of (sampled) negative triples T , and the parameters θ is optimized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Overview of KG Embedding Methods</head><p>We use small italics characters (e.g., h, r) to represent entities and relations, and correspond-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Type</head><p>Model Score Function σ(h, r, t)</p><formula xml:id="formula_0">Additive TransE (Bordes et al., 2013) − h + r − t 1 TransR (Lin et al., 2015) − Mrh + r − Mrt 1 STransE (Nguyen et al., 2016) − M 1 r h + r − M 2 r t 1 Multiplicative DistMult (Yang et al., 2014) r (h t) HolE (Nickel et al., 2016) r (h t) ComplEx (Trouillon et al., 2016)</formula><p>Re(r (h ¯ t)) ing bold characters to represent their vector em- beddings (e.g., h, r). We use bold capitalization (e.g., V) to represent a set of vectors. Matrices are represented by capital italics characters (e.g., M ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Additive KG Embedding Methods</head><p>This is the set of methods where entity and rela- tion vectors interact via additive operations. The score function for these models can be expressed as below</p><formula xml:id="formula_1">σ(h, r, t) = − M 1 r h + r − M 2 r t 1 (1)</formula><p>where h, t ∈ R de and r ∈ R dr are vectors for head entity, tail entity and relation respectively. M 1 r , M 2 r ∈ R dr×de are projection matrices from entity space R de to relation space R dr . TransE ( <ref type="bibr" target="#b1">Bordes et al., 2013</ref>) is the simplest addi- tive model where the entity and relation vectors lie</p><formula xml:id="formula_2">in same d−dimensional space, i.e., d e = d r = d.</formula><p>The projection matrices M 1 r = M 2 r = I d are iden- tity matrices. The relation vectors are modeled as translation vectors from head entity vectors to tail entity vectors. Pairwise ranking loss is then used to learn these vectors. Since the model is simple, it has limited capability in capturing many-to-one, one-to-many and many-to-many relations. TransR ( <ref type="bibr" target="#b4">Lin et al., 2015</ref>) is another translation- based model which uses separate spaces for en- tity and relation vectors allowing it to address the shortcomings of TransE. Entity vectors are pro- jected into a relation specific space using the cor- responding projection matrix M 1 r = M 2 r = M r . The training is similar to <ref type="bibr">TransE. STransE (Nguyen et al., 2016</ref>) is a generalization of TransR and uses different projection matrices for head and tail entity vectors. The training is similar to TransE. STransE achieves better perfor- mance than the previous methods but at the cost of more number of parameters.</p><p>Equation 1 is the score function used in STransE. TransE and TransR are special cases of STransE with M 1 r = M 2 r = I d and M 1 r = M 2 r = M r , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multiplicative KG Embedding Methods</head><p>This is the set of methods where the vectors inter- act via multiplicative operations (usually dot prod- uct). The score function for these models can be expressed as</p><formula xml:id="formula_3">σ(h, r, t) = r f (h, t)<label>(2)</label></formula><p>where h, t, r ∈ F d are vectors for head entity, tail entity and relation respectively. f (h, t) ∈ F d mea- sures compatibility of head and tail entities and is specific to the model. F is either real space R or complex space C. Detailed descriptions of the models we consider are as follows. <ref type="bibr">DistMult (Yang et al., 2014</ref>) models entities and relations as vectors in R d . It uses an entry-wise product () to measure compatibility between head and tail entities, while using logistic loss for training the model.</p><formula xml:id="formula_4">σ DistM ult (h, r, t) = r (h t)<label>(3)</label></formula><p>Since the entry-wise product in <ref type="formula" target="#formula_4">(3)</ref>  </p><formula xml:id="formula_5">[h t] k = d−1 i=0 h i t (k+i) mod d</formula><p>The score function is given as</p><formula xml:id="formula_6">σ HolE (h, r, t) = r (h t)<label>(4)</label></formula><p>The circular correlation operator being asymmet- ric, can capture asymmetric and anti-symmetric relations, but at the cost of higher time complexity </p><formula xml:id="formula_7">σ ComplEx (h, r, t) = Re(r (h ¯ t))<label>(5)</label></formula><p>In contrast to (3), using complex vectors in (5) al- lows ComplEx to handle symmetric, asymmetric and anti-symmetric relations using the same score function. Similar to DistMult, logistic loss is used for training the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Metrics</head><p>For our geometrical analysis, we first define a term 'alignment to mean' (ATM) of a vector v belong- ing to a set of vectors V, as the cosine similarity 1 between v and the mean of all vectors in V.</p><formula xml:id="formula_8">ATM(v, V) = cosine v, 1 |V| x∈V x</formula><p>We also define 'conicity' of a set V as the mean ATM of all vectors in V.  By this definition, a high value of Conicity(V) would imply that the vectors in V lie in a nar- row cone centered at origin. In other words, the vectors in the set V are highly aligned with each other. In addition to that, we define the variance of ATM across all vectors in V, as the 'vector spread'(VS) of set V, </p><formula xml:id="formula_9">Conicity(V) = 1 |V| v∈V ATM(v, V) 1 cosine(u, v) = u v uv Dataset FB15k</formula><formula xml:id="formula_10">VS(V) = 1 |V| v∈V ATM(v, V)−Conicity(V)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head><p>Datasets: We run our experiments on subsets of two widely used datasets, viz., Freebase (Bol- lacker et al., 2008) and WordNet <ref type="bibr" target="#b6">(Miller, 1995)</ref>, called FB15k and WN18 ( <ref type="bibr" target="#b1">Bordes et al., 2013</ref>), re- spectively. We detail the characteristics of these datasets in <ref type="table" target="#tab_3">Table 2</ref>. Please note that while the results presented in Section 6 are on the FB15K dataset, we reach the same conclusions on WN18. The plots for our ex- periments on WN18 can be found in the Supple- mentary Section. Hyperparameters: We experiment with multiple values of hyperparameters to understand their ef- fect on the geometry of KG embeddings. Specif- ically, we vary the dimension of the generated vectors between {50, 100, 200} and the number of negative samples used during training between {1, 50, 100}. For more details on algorithm spe- cific hyperparameters, we refer the reader to the Supplementary Section. 2 2 For training, we used codes from https://github.</p><p>Frequency Bins: We follow ( <ref type="bibr" target="#b7">Mimno and Thompson, 2017</ref>) for entity and relation samples used in the analysis. Multiple bins of entities and relations are created based on their frequencies and 100 ran- domly sampled vectors are taken from each bin. These set of sampled vectors are then used for our analysis. For more information about sampling vectors, please refer to ( <ref type="bibr" target="#b7">Mimno and Thompson, 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results and Analysis</head><p>In this section, we evaluate the following ques- tions.</p><p>• Does model type (e.g., additive vs multiplica- tive) have any effect on the geometry of em- beddings? (Section 6.1)  • Does negative sampling have any effect on the embedding geometry? (Section 6.2)</p><p>• Does the dimension of embedding have any effect on its geometry? (Section 6.3)</p><p>• How is task performance related to embed- ding geometry? (Section 6.4)</p><p>In each subsection, we summarize the main findings at the beginning, followed by evidence supporting those findings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Effect of Model Type on Geometry</head><p>Summary of Findings: Additive: Low conicity and high vector spread. Multiplicative: High conicity and low vector spread.</p><p>In this section, we explore whether the type of the score function optimized during the training has any effect on the geometry of the resulting em- bedding. For this experiment, we set the number of negative samples to 1 and the vector dimension to 100 (we got similar results for 50-dimensional vectors). <ref type="figure">Figure 2</ref> and <ref type="figure" target="#fig_4">Figure 3</ref> show the distribu- tion of ATMs of these sampled entity and relation vectors, respectively. 3 Entity Embeddings: As seen in <ref type="figure">Figure 2</ref>, there is a stark difference between the geometries of en- tity vectors produced by additive and multiplica- tive models. The ATMs of all entity vectors pro- duced by multiplicative models are positive with very low vector spread. Their high conicity sug- gests that they are not uniformly dispersed in the vector space, but lie in a narrow cone along the mean vector. This is in contrast to the entity vec- tors obtained from additive models which are both positive and negative with higher vector spread. From the lower values of conicity, we conclude that entity vectors from additive models are evenly dispersed in the vector space. This observation is also reinforced by looking at the high vector spread of additive models in comparison to that of multiplicative models. We also observed that addi- tive models are sensitive to the frequency of enti- ties, with high frequency bins having higher conic- ity than low frequency bins. However, no such pat- tern was observed for multiplicative models and The conicity of relation vectors generated using additive models is almost zero across fre- quency bands. This coupled with the high vec- tor spread observed, suggests that these vectors are scattered throughout the vector space. Re- lation vectors from multiplicative models exhibit high conicity and low vector spread, suggesting that they lie in a narrow cone centered at origin, like their entity counterparts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Effect of Number of Negative Samples on Geometry</head><p>Summary of Findings: Additive: Conicity and average length are in- variant to changes in #NegativeSamples for both entities and relations. Multiplicative: Conicity increases while av- erage vector length decrease with increasing #NegativeSamples for entities. Conicity de- creases, while average vector length remains constant (except HolE) for relations. For experiments in this section, we keep the vector dimension constant at 100. Entity Embeddings: As seen in <ref type="figure" target="#fig_5">Figure 4 (left)</ref>, the conicity of entity vectors increases as the num- ber of negative samples is increased for multi- plicative models. In contrast, conicity of the en- tity vectors generated by additive models is unaf- fected by change in number of negative samples and they continue to be dispersed throughout the vector space. From <ref type="figure" target="#fig_5">Figure 4</ref> (right), we observe that the average length of entity vectors produced by additive models is also invariant of any changes in number of negative samples. On the other hand, increase in negative sampling decreases the aver- age entity vector length for all multiplicative mod- els except HolE. The average entity vector length for HolE is nearly 1 for any number of negative samples, which is understandable considering it constrains the entity vectors to lie inside a unit ball ( <ref type="bibr" target="#b10">Nickel et al., 2016)</ref>. This constraint is also enforced by the additive models: TransE, TransR, and STransE. Relation Embeddings: Similar to entity embed- dings, in case of relation vectors trained using ad- ditive models, the average length and conicity do not change while varying the number of negative samples. However, the conicity of relation vec- tors from multiplicative models decreases with in- crease in negative sampling. The average rela- tion vector length is invariant for all multiplica- tive methods, except for HolE. We see a surpris- ingly big jump in average relation vector length for HolE going from 1 to 50 negative samples, but it does not change after that. Due to space con- straints in the paper, we refer the reader to the Sup- plementary Section for plots discussing the effect of number of negative samples on geometry of re- lation vectors.</p><p>We note that the multiplicative score between two vectors may be increased by either increas- ing the alignment between the two vectors (i.e., in- creasing Conicity and reducing vector spread be- tween them), or by increasing their lengths. It is interesting to note that we see exactly these ef- fects in the geometry of multiplicative methods analyzed above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Correlation with Geometry of Word Embeddings</head><p>Our conclusions from the geometrical analysis of entity vectors produced by multiplicative mod- els are similar to the results in <ref type="bibr" target="#b7">(Mimno and Thompson, 2017)</ref>, where increase in negative sampling leads to increased conicity of word vec- tors trained using the skip-gram with negative sampling (SGNS) method. On the other hand, ad- ditive models remain unaffected by these changes. SGNS tries to maximize a score function of the form w T · c for positive word context pairs, where w is the word vector and c is the context vector ( <ref type="bibr" target="#b5">Mikolov et al., 2013)</ref>. This is very similar to the score function of multiplicative models as seen in <ref type="table" target="#tab_0">Table 1</ref>. Hence, SGNS can be considered as a mul- tiplicative model in the word domain.</p><p>Hence, we argue that our result on the increase in negative samples increasing the conicity of vec- tors trained using a multiplicative score function can be considered as a generalization of the one in (Mimno and Thompson, 2017).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Effect of Vector Dimension on Geometry</head><p>Summary of Findings: Additive: Conicity and average length are in- variant to changes in dimension for both entities and relations. Multiplicative: Conicity decreases for both en- tities and relations with increasing dimension. Average vector length increases for both entities and relations, except for HolE entities.</p><p>Entity Embeddings: To study the effect of vec- tor dimension on conicity and length, we set the number of negative samples to 1, while varying the vector dimension. From <ref type="figure" target="#fig_6">Figure 5</ref> (left), we observe that the conicity for entity vectors gen- erated by any additive model is almost invariant of increase in dimension, though STransE exhibits a slight decrease. In contrast, entity vector from multiplicative models show a clear decreasing pat- tern with increasing dimension.</p><p>As seen in <ref type="figure" target="#fig_6">Figure 5</ref> (right), the average lengths of entity vectors from multiplicative models in- crease sharply with increasing vector dimension, except for HolE. In case of HolE, the average vec- tor length remains constant at one. Deviation in- volving HolE is expected as it enforces entity vec- tors to fall within a unit ball. Similar constraints are enforced on entity vectors for additive models as well. Thus, the average entity vector lengths are not affected by increasing vector dimension for all additive models.</p><p>Relation Embeddings: We reach similar conclu- sion when analyzing against increasing dimension the change in geometry of relation vectors pro- duced using these KG embedding methods. In this setting, the average length of relation vectors learned by HolE also increases as dimension is in- creased. This is consistent with the other meth- ods in the multiplicative family. This is because, unlike entity vectors, the lengths of relation vec- tors of HolE are not constrained to be less than unit length. Due to lack of space, we are unable to show plots for relation vectors here, but the same can be found in the Supplementary Section. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Relating Geometry to Performance</head><p>Summary of Findings: Additive: Neither entites nor relations exhibit correlation between geometry and performance. Multiplicative: Keeping negative samples fixed, lower conicity or higher average vector length for entities leads to improved performance. No relationship for relations.</p><p>In this section, we analyze the relationship be- tween geometry and performance on the Link pre- diction task, using the same setting as in ( <ref type="bibr" target="#b1">Bordes et al., 2013)</ref>. <ref type="figure" target="#fig_7">Figure 6</ref> (left) presents the effects of conicity of entity vectors on performance, while <ref type="figure" target="#fig_7">Figure 6</ref> (right) shows the effects of average entity vector length. <ref type="bibr">4</ref> As we see from <ref type="figure" target="#fig_7">Figure 6</ref> (left), for fixed num- ber of negative samples, the multiplicative model with lower conicity of entity vectors achieves bet- ter performance. This performance gain is larger for higher numbers of negative samples (N). Addi- tive models don't exhibit any relationship between performance and conicity, as they are all clustered around zero conicity, which is in-line with our ob- servations in previous sections. In <ref type="figure" target="#fig_7">Figure 6</ref> (right), for all multiplicative models except HolE, a higher average entity vector length translates to better performance, while the number of negative sam- ples is kept fixed. Additive models and HolE don't exhibit any such patterns, as they are all clustered just below unit average entity vector length.</p><p>The above two observations for multiplicative models make intuitive sense, as lower conicity and higher average vector length would both translate to vectors being more dispersed in the space.</p><p>We see another interesting observation regard- ing the high sensitivity of HolE to the number of negative samples used during training. Using a large number of negative examples (e.g., N = 50 or 100) leads to very high conicity in case of HolE. <ref type="figure" target="#fig_7">Figure 6</ref> (right) shows that average entity vector length of HolE is always one. These two obser- vations point towards HolE's entity vectors lying in a tiny part of the space. This translates to HolE performing poorer than all other models in case of high numbers of negative sampling.</p><p>We also did a similar study for relation vectors, but did not see any discernible patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we have initiated a systematic study into the important but unexplored problem of an- alyzing geometry of various Knowledge Graph (KG) embedding methods. To the best of our knowledge, this is the first study of its kind. Through extensive experiments on multiple real- world datasets, we are able to identify several in- sights into the geometry of KG embeddings. We have also explored the relationship between KG embedding geometry and its task performance. We have shared all our source code to foster fur- ther research in this area.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>For our analysis, we consider six representative KG embedding methods: TransE (Bordes et al., 2013), TransR (Lin et al., 2015), STransE (Nguyen et al., 2016), DistMult (Yang et al., 2014), HolE (Nickel et al., 2016) and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparison of high vs low Conicity. Randomly generated vectors are shown in blue with their sample mean vector M in black. Figure on the left shows the case when vectors lie in narrow cone resulting in high Conicity value. Figure on the right shows the case when vectors are spread out having relatively lower Conicity value. We skipped very low values of Conicity as it was difficult to visualize. The points are sampled from 3d Spherical Gaussian with mean (1,1,1) and standard deviation 0.1 (left) and 1.3 (right). Please refer to Section 4 for more details.</figDesc><graphic url="image-2.png" coords="4,298.77,62.81,207.68,196.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 visually</head><label>1</label><figDesc>Figure 1 visually demonstrates these metrics for randomly generated 3-dimensional points. The left figure shows high Conicity and low vector spread while the right figure shows low Conicity and high vector spread. We define the length of a vector v as L 2-norm of the vector v 2 and 'average vector length' (AVL) for the set of vectors V as AVL(V) = 1 |V| v∈V</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>TransR), https://github.com/datquocnguyen/ STransE (STransE), https://github.com/ mnick/holographic-embeddings (HolE) and https://github.com/ttrouill/complex (Com- plEx and DistMult).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Alignment to Mean (ATM) vs Density plots for relation embeddings learned by various additive (top row) and multiplicative (bottom row) KG embedding methods. For each method, a plot averaged across entity frequency bins is shown. Trends in these plots are similar to those in Figure 2. Main findings from these plots are summarized in Section 6.1.</figDesc><graphic url="image-12.png" coords="6,72.00,203.10,158.40,126.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Conicity (left) and Average Vector Length (right) vs Number of negative samples for entity vectors learned using various KG embedding methods. In each bar group, first three models are additive, while the last three are multiplicative. Main findings from these plots are summarized in Section 6.2</figDesc><graphic url="image-15.png" coords="7,72.00,62.81,230.40,144.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Conicity (left) and Average Vector Length (right) vs Number of Dimensions for entity vectors learned using various KG embedding methods. In each bar group, first three models are additive, while the last three are multiplicative. Main findings from these plots are summarized in Section 6.3.</figDesc><graphic url="image-17.png" coords="8,72.00,62.81,230.40,144.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Relationship between Performance (HITS@10) on a link prediction task vs Conicity (left) and Avg. Vector Length (right). For each point, N represents the number of negative samples used. Main findings are summarized in Section 6.4.</figDesc><graphic url="image-19.png" coords="9,72.00,62.81,230.40,144.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Summary of various Knowledge Graph (KG) embedding methods used in the paper. Please see 
Section 3 for more details. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 : Summary of datasets used in the paper.</head><label>2</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="3"> We also tried using the global mean instead of mean of the sampled set for calculating cosine similarity in ATM, and got very similar results.</note>

			<note place="foot" n="4"> A more focused analysis for multiplicative models is presented in Section 3 of Supplementary material.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank the anonymous reviewers for their con-structive comments. This work is supported in part by the Ministry of Human Resources Devel-opment (Government of India), Intel, Intuit, and by gifts from Google and Accenture.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD international conference on Management of data</title>
		<meeting>the 2008 ACM SIGMOD international conference on Management of data</meeting>
		<imprint>
			<publisher>AcM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garciaduran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<title level="m">Convolutional 2D Knowledge Graph Embeddings</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Knowledge vault: A web-scale approach to probabilistic knowledge fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geremy</forename><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilko</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Strohmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="601" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2181" to="2187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The strange geometry of skip-gram with negative sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laure</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2863" to="2868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Never-ending learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hruschka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mazaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nakashole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Platanios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Samadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wijaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saparov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Greaves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Stranse: a novel embedding model of entities and relationships in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kairit</forename><surname>Dat Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Sirts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="460" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Holographic embeddings of knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><forename type="middle">A</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Revisiting simple neural networks for learning representations of knowledge graphs. 6th Workshop on Automated Knowledge Base Construction (AKBC) at NIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivas</forename><surname>Ravishankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandrahas</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><forename type="middle">Pratim</forename><surname>Talukdar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Modeling Relational Data with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Den Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ArXiv eprints</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Yago: a core of semantic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gjergji</forename><surname>Fabian M Suchanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weikum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Representing Text for Joint Embedding of Text and Knowledge Bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pallavi</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP). ACL Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Théo</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI. Citeseer</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1112" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6575</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
