<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:03+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Commonsense Knowledge Base Completion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Toyota Technological Institute at Chicago</orgName>
								<address>
									<postCode>60637</postCode>
									<settlement>Chicago</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aynaz</forename><surname>Taheri</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois at Chicago</orgName>
								<address>
									<postCode>60607</postCode>
									<settlement>Chicago</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifu</forename><surname>Tu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Toyota Technological Institute at Chicago</orgName>
								<address>
									<postCode>60637</postCode>
									<settlement>Chicago</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Toyota Technological Institute at Chicago</orgName>
								<address>
									<postCode>60637</postCode>
									<settlement>Chicago</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Chicago</orgName>
								<address>
									<postCode>60637</postCode>
									<settlement>Chicago</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Commonsense Knowledge Base Completion</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1445" to="1455"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We enrich a curated resource of common-sense knowledge by formulating the problem as one of knowledge base completion (KBC). Most work in KBC focuses on knowledge bases like Freebase that relate entities drawn from a fixed set. However , the tuples in ConceptNet (Speer and Havasi, 2012) define relations between an unbounded set of phrases. We develop neural network models for scoring tuples on arbitrary phrases and evaluate them by their ability to distinguish true held-out tuples from false ones. We find strong performance from a bilinear model using a simple additive architecture to model phrases. We manually evaluate our trained model&apos;s ability to assign quality scores to novel tuples, finding that it can propose tu-ples at the same quality level as medium-confidence tuples from ConceptNet.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many ambiguities in natural language process- ing (NLP) can be resolved by using knowledge of various forms. Our focus is on the type of knowledge that is often referred to as "common- sense" or "background" knowledge. This knowl- edge is rarely expressed explicitly in textual cor- pora ( <ref type="bibr" target="#b17">Gordon and Van Durme, 2013</ref>). Some re- searchers have developed techniques for inferring this knowledge from patterns in raw text <ref type="bibr" target="#b19">(Gordon, 2014;</ref><ref type="bibr" target="#b2">Angeli and Manning, 2014</ref>), while oth- ers have developed curated resources of common- sense knowledge via manual annotation ( <ref type="bibr" target="#b27">Lenat and Guha, 1989;</ref><ref type="bibr" target="#b40">Speer and Havasi, 2012)</ref> or games with a purpose <ref type="bibr" target="#b45">(von Ahn et al., 2006</ref>).</p><p>Curated resources typically have high preci- sion but suffer from a lack of coverage. For cer-  <ref type="table">Table 1</ref>: ConceptNet tuples with left term "soak in hotspring"; final column is confidence score.</p><p>tain resources, researchers have developed meth- ods to automatically increase coverage by infer- ring missing entries. These methods are com- monly categorized under the heading of knowl- edge base completion (KBC). KBC is widely- studied for knowledge bases like Freebase ( <ref type="bibr" target="#b5">Bollacker et al., 2008</ref>) which contain large sets of enti- ties and relations among them ( <ref type="bibr" target="#b32">Mintz et al., 2009;</ref><ref type="bibr" target="#b35">Nickel et al., 2011;</ref><ref type="bibr" target="#b46">West et al., 2014</ref>), including recent work using neural net- works ( <ref type="bibr" target="#b39">Socher et al., 2013;</ref><ref type="bibr" target="#b49">Yang et al., 2014</ref>).</p><p>We improve the coverage of commonsense re- sources by formulating the problem as one of knowledge base completion. We focus on a par- ticular curated commonsense resource called Con- ceptNet <ref type="bibr" target="#b40">(Speer and Havasi, 2012)</ref>. ConceptNet contains tuples consisting of a left term, a rela- tion, and a right term. The relations come from a fixed set. While terms in Freebase tuples are en- tities, ConceptNet terms can be arbitrary phrases. Some examples are shown in <ref type="table">Table 1</ref>. An NLP ap- plication may wish to query ConceptNet for infor- mation about soaking in a hotspring, but may use different words from those contained in the Con- ceptNet tuples. Our goal is to do on-the-fly knowl- edge base completion so that queries can be an- swered robustly without requiring the precise lin- guistic forms contained in ConceptNet.</p><p>To do this, we develop neural network mod- els to embed terms and provide scores to arbi-trary tuples. We train them on ConceptNet tuples and evaluate them by their ability to distinguish true and false held-out tuples. We consider sev- eral functional architectures, comparing two com- position functions for embedding terms and two functions for converting term embeddings into tu- ple scores. We find that all architectures are able to outperform several baselines and reach similar performance on classifying held-out tuples.</p><p>We also experiment with several training ob- jectives for KBC, finding that a simple cross en- tropy objective with randomly-generated negative examples performs best while also being fastest. We manually evaluate our trained model's abil- ity to assign quality scores to novel tuples, find- ing that it can propose tuples at the same qual- ity level as medium-confidence tuples from Con- ceptNet. We release all of our resources, includ- ing our ConceptNet KBC task data, large sets of randomly-generated tuples scored with our model, training code, and pretrained models with code for calculating the confidence of novel tuples. <ref type="bibr">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our methods are similar to past work on KBC ( <ref type="bibr" target="#b32">Mintz et al., 2009;</ref><ref type="bibr" target="#b35">Nickel et al., 2011;</ref><ref type="bibr" target="#b26">Lao et al., 2011;</ref><ref type="bibr" target="#b36">Nickel et al., 2012;</ref><ref type="bibr" target="#b15">Gardner et al., 2014;</ref><ref type="bibr" target="#b46">West et al., 2014</ref>), particu- larly methods based on distributed representations and neural networks <ref type="bibr" target="#b39">(Socher et al., 2013;</ref><ref type="bibr" target="#b6">Bordes et al., 2013;</ref><ref type="bibr" target="#b7">Bordes et al., 2014a;</ref><ref type="bibr" target="#b8">Bordes et al., 2014b;</ref><ref type="bibr" target="#b49">Yang et al., 2014;</ref><ref type="bibr" target="#b20">Gu et al., 2015;</ref><ref type="bibr" target="#b44">Toutanova et al., 2015</ref>). Most prior work predicts new relational links between terms drawn from a fixed set. In a notable exception, <ref type="bibr" target="#b33">Neelakantan and Chang (2015)</ref> add new entities to KBs using external resources along with prop- erties of the KB itself. Relatedly,  induce an unbounded set of entity categories and associate them with entities in KBs.</p><p>Several researchers have developed techniques for discovering commonsense knowledge from text ( <ref type="bibr" target="#b18">Gordon et al., 2010;</ref><ref type="bibr" target="#b16">Gordon and Schubert, 2012;</ref><ref type="bibr" target="#b19">Gordon, 2014;</ref><ref type="bibr" target="#b2">Angeli and Manning, 2014)</ref>. Open information extraction systems like REVERB <ref type="bibr" target="#b13">(Fader et al., 2011</ref>) and NELL <ref type="bibr" target="#b10">(Carlson et al., 2010</ref>) find tuples with arbitrary terms and relations from raw text. In contrast, we start with a set of commonsense facts to use for train- ing, though our methods could be applied to the output of these or other extraction systems.</p><p>Our goals are similar to those of the Analogy- Space method <ref type="bibr" target="#b41">(Speer et al., 2008)</ref>, which uses ma- trix factorization to improve coverage of Concept- Net. However, AnalogySpace can only return a confidence score for a pair of terms drawn from the training set. Our models can assign scores to tuples that contain novel terms (as long as they consist of words in our vocabulary).</p><p>Though we use ConceptNet, similar techniques can be applied to other curated resources like WordNet <ref type="bibr" target="#b31">(Miller, 1995)</ref> and FrameNet ( <ref type="bibr" target="#b3">Baker et al., 1998</ref>). For WordNet, tuples can contain lexi- cal entries that are linked via synset relations (e.g., "hypernym"). WordNet contains many multi- word entries (e.g., "cold sweat"), which can be modeled compositionally by our term models; al- ternatively, entire glosses could be used as terms.</p><p>To expand frame relationships in FrameNet, tuples can draw relations from the frame relation types (e.g., "is causative of") and terms can be frame lexical units or their definitions.</p><p>Several researchers have used commonsense knowledge to improve language technologies, in- cluding sentiment analysis <ref type="bibr" target="#b9">(Cambria et al., 2012;</ref><ref type="bibr" target="#b0">Agarwal et al., 2015)</ref>, semantic similarity ( <ref type="bibr" target="#b11">Caro et al., 2015)</ref>, and speech recognition ( <ref type="bibr" target="#b28">Lieberman et al., 2005</ref>). Our hope is that our models can en- able many other NLP applications to benefit from commonsense knowledge.</p><p>Our work is most similar to that of <ref type="bibr" target="#b1">Angeli and Manning (2013)</ref>. They also developed methods to assess the plausibility of new facts based on a training set of facts, considering commonsense data from ConceptNet in one of their settings. Like us, they can handle an unbounded set of terms by using (simple) composition functions for novel terms, which is rare among work in KBC. One key difference is that their best method requires iterat- ing over the KB at test time, which can be com- putationally expensive with large KBs. Our mod- els do not require iterating over the training set. We compare to several baselines inspired by their work, and we additionally evaluate our model's ability to score novel tuples derived from both ConceptNet and Wikipedia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Models</head><p>Our goal is to represent commonsense knowledge such that it can be used for NLP tasks. We as-sume this knowledge is given in the form of tuples t 1 , R, t 2 , where t 1 is the left term, t 2 is the right term, and R is a (directed) relation that exists be- tween the terms. Examples are shown in <ref type="table">Table 1</ref>. <ref type="bibr">2</ref> Given a set of tuples, our goal is to develop a parametric model that can provide a confidence score for new, unseen tuples. That is, we want to design and train models that define a function score(t 1 , R, t 2 ) that provides a quality score for an arbitrary tuple t 1 , R, t 2 . These models will be evaluated by their ability to distinguish true held- out tuples from false ones.</p><p>We describe two model families for scoring tu- ples. We assume that we have embeddings for words and define models that use these word em- beddings to score tuples. So our models are lim- ited to tuples in which terms consist of words in the word embedding vocabulary, though future work could consider character-based architectures for open-vocabulary modeling <ref type="bibr" target="#b23">(Huang et al., 2013;</ref><ref type="bibr" target="#b29">Ling et al., 2015</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Bilinear Models</head><p>We first consider bilinear models, since they have been found useful for KBC in past work <ref type="bibr" target="#b35">(Nickel et al., 2011;</ref><ref type="bibr" target="#b24">Jenatton et al., 2012;</ref><ref type="bibr" target="#b14">García-Durán et al., 2014;</ref><ref type="bibr" target="#b49">Yang et al., 2014</ref>). A bilinear model has the following form for a tuple t 1 , R, t 2 :</p><formula xml:id="formula_0">v 1 M R v 2</formula><p>where v 1 ∈ R r is the (column) vector representing t 1 , v 2 ∈ R r is the vector for t 2 , and M R ∈ R r×r is the parameter matrix for relation R.</p><p>To convert terms t 1 and t 2 into term vectors v 1 and v 2 , we consider two possibilities: word aver- aging and a bidirectional long short-term memory (LSTM) recurrent neural network <ref type="bibr" target="#b22">(Hochreiter and Schmidhuber, 1997)</ref>. This provides us with two models: Bilinear AVG and Bilinear LSTM.</p><p>One downside of this architecture is that as the length of the term vectors grows, the size of the re- lation matrices grows quadratically. This can slow down training while requiring more data to learn the large numbers of parameters in the matrices. To address this, we include an additional nonlin- ear transformation of each term:</p><formula xml:id="formula_1">u i = a(W (B) v i + b (B) )</formula><p>2 These examples are from the Open Mind Common Sense (OMCS) part of ConceptNet version 5 <ref type="bibr" target="#b40">(Speer and Havasi, 2012</ref>). In our experiments below, we only use OMCS tuples.</p><p>where a is a nonlinear activation function (tuned among ReLU, tanh, and logistic sigmoid) and where we have introduced additional parameters W (B) and b <ref type="bibr">(B)</ref> . This gives us the following model:</p><formula xml:id="formula_2">score bilinear (t 1 , R, t 2 ) = u 1 M R u 2</formula><p>When using the LSTM, we tune the decision about how to produce the final term vectors to pass to the bilinear model, including possibly using the final vectors from each direction and the output of max or average pooling. We use the same LSTM pa- rameters for each term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Deep Neural Network Models</head><p>Our second family of models is based on deep neu- ral networks (DNNs). While bilinear models have been shown to work well for KBC, their functional form makes restrictions about how terms can inter- act. DNNs make no such restrictions. As above, we define two models, one based on using word averaging for the term model (DNN AVG) and one based on LSTMs (DNN LSTM).</p><p>For the DNN AVG model, we obtain the term vec- tors v 1 and v 2 by averaging word vectors in the re- spective terms. We then concatenate v 1 , v 2 , and a relation vector v R to form the input of the DNN, denoted v in . The DNN uses a single hidden layer:</p><formula xml:id="formula_3">u = a(W (D1) v in + b (D1) ) score DNN (t 1 , R, t 2 ) = W (D2) u + b (D2) (1)</formula><p>where a is again a (tuned) nonlinear activation function. The size of the hidden vector u is tuned, but the output dimensionality (the numbers of rows in W (D2) and b (D2) ) is fixed to 1. We do not use a nonlinear activation for the final layer since our goal is to output a scalar score.</p><p>For the DNN LSTM model, we first create a single vector for the two terms using an LSTM. That is, we concatenate t 1 , a delimiter token, and t 2 to create a single word sequence. We use a bidi- rectional LSTM to convert this word sequence to a vector, again possibly using pooling (the deci- sion is tuned; details below). We concatenate the output of this bidirectional LSTM with the rela- tion vector v r to create the DNN input vector v in , then use Eq. 1 to obtain a score. We found this to work better than separately using an LSTM on each term. We can not try this for the Bilinear LSTM model since its functional form separates the two term vectors.</p><p>The relation vectors v R are learned in addition to the DNN parameters</p><formula xml:id="formula_4">W (D1) , W (D2) , b (D1) , b (D2)</formula><p>, and the LSTM parameters (in the case of the DNN LSTM model). Also, word embedding parameters are updated in all settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Training</head><p>Given a tuple training set T , we train our models using two different loss functions: hinge loss and a binary cross entropy function. Both rely on ways of generating negative examples (Section 4.3). Both also use regularization (Section 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Hinge Loss</head><p>Given a training tuple τ = t 1 , R, t 2 , the hinge loss seeks to make the score of τ larger than the score of negative examples by a margin of at least γ. This corresponds to minimizing the following loss, summed over all examples τ ∈ T :</p><formula xml:id="formula_5">loss hinge (τ ) = max{0, γ − score(τ ) + score(τ neg(t 1 ) )} + max{0, γ − score(τ ) + score(τ neg(R) )} + max{0, γ − score(τ ) + score(τ neg(t 2 ) )}</formula><p>where τ neg(t 1 ) is the negative example obtained by replacing t 1 in τ with some other t 1 , and τ neg(R) and τ neg(t 2 ) are defined analogously for the rela- tion and right term. We describe how we generate these negative examples in Section 4.3 below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Binary Cross Entropy</head><p>Though we only have true tuples in our training set, we can create a binary classification problem by assigning a label of 1 to training tuples and a label of 0 to negative examples. Then we can min- imize cross entropy (CE) as is common when us- ing neural networks for classification. To generate negative examples, we consider the methods de- scribed in Section 4.3 below. We also need to con- vert our models' scores into probabilities, which we do by using a logistic sigmoid σ on score. We denote the label as , where the label is 1 if the tu- ple is from the training set and 0 if it is a negative example. Then the loss is defined:</p><formula xml:id="formula_6">loss CE (τ, ) = − log σ(score(τ )) − (1−) log(1 − σ(score(τ )))</formula><p>When using this loss, we generate three negative examples for each positive example (one for swap- ping each component of the tuple, as in the hinge loss). For a mini-batch of size β, there are β pos- itive examples and 3β negative examples used for training. The loss is summed over these 4β exam- ples yielded by each mini-batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Negative Examples</head><p>For the loss functions above, we need ways of au- tomatically generating negative examples. For ef- ficiency, we consider using the current mini-batch only, as our models are trained using optimiza- tion on mini-batches. We consider the follow- ing three strategies to construct negative examples. Each strategy constructs three negative examples for each positive example τ : one by replacing t 1 , one by replacing R, and one by replacing t 2 .</p><p>Random sampling. We create the three negative examples for τ by replacing each component with its counterpart in a randomly-chosen tuple in the same mini-batch.</p><p>Max sampling. We create the three negative ex- amples for τ by replacing each component with its counterpart in some other tuple in the mini- batch, choosing the substitution to maximize the score of the resulting negative example. For ex- ample, when swapping out t 1 in τ = t 1 , R, t 2 , we choose the substitution t 1 as follows:</p><formula xml:id="formula_7">t 1 = argmax t:t,R ,t 2 ∈µ\τ score(t, R, t 2 )</formula><p>where µ is the current mini-batch of tuples. We perform the analogous procedure for R and t 2 .</p><p>Mix sampling. This is a mixture of the above, using random sampling 50% of the time and max sampling the remaining 50% of the time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Regularization</head><p>We use L 2 regularization. For the DNN models, we add the penalty term λθ 2 to the losses, where λ is the regularization coefficient and θ contains all other parameters. However, for the bilinear mod- els we regularize the relation matrices M R toward the identity matrix instead of all zeroes, adding the following to the loss:</p><formula xml:id="formula_8">λ 1 θ 2 + λ 2 R M R − I r 2 2</formula><p>where I r is the r × r identity matrix, the summa- tion is performed over all relations R, and θ repre- sents all other parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head><p>We now evaluate our tuple models. We measure whether our models can distinguish true and false tuples by training a model on a large set of tuples and testing on a held-out set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Task Design</head><p>The tuples are obtained from the Open Mind Com- mon Sense (OMCS) entries in the ConceptNet 5 dataset (Speer and Havasi, 2012). They are sorted by a confidence score. The most confident 1200 tuples were reserved for creating our test set (TEST). The next most confident 600 tuples (i.e., those numbered 1201-1800) were used to build a development set (DEV1) and the next most confi- dent 600 (those numbered 1801-2400) were used to build a second development set (DEV2). For each set S (S ∈ {DEV1, DEV2, TEST}), for each tuple τ ∈ S, we created a negative example and added it to S. So each set doubled in size. To create a negative example from τ ∈ S, we ran- domly swapped one of the components of τ with another tuple τ ∈ S. One third of the time we swapped t 1 in τ for t 1 in τ , one third of the time we swapped their R's, and the remaining third of the time we swapped their t 2 's. Thus, distinguish- ing positive and negative examples in this task is similar to the objectives optimized during training.</p><p>Each of DEV1 and DEV2 has 1200 tuples (600 positive examples and 600 negative examples), while TEST has 2400 tuples (1200 positive and 1200 negative). For training data, we selected 100,000 tuples from the remaining tuples (num- bered 2401 and beyond).</p><p>The task is to separate the true and false tuples in our test set. That is, the labels are 1 for true tuples and 0 for false tuples. Given a model for scoring tuples, we select a threshold by maximiz- ing accuracy on DEV1 and report accuracies on DEV2. This is akin to learning the bias feature weight (using DEV1) of a linear classifier that uses our model's score as its only feature. We tuned several choices-including word embeddings, hy- perparameter values, and training objectives-on DEV2 and report final performance on TEST. One annotator (a native English speaker) attempted the same classification task on a sample of 100 tu- ples from <ref type="bibr">DEV2</ref> and achieved an accuracy of 95%. We release these datasets to the community so that others can work on this same task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Word Embeddings</head><p>Our tuple models rely on initial word embeddings. To help our models better capture the common- sense knowledge in ConceptNet, we generated word embedding training data using the OMCS sentences underlying our training tuples (we ex- cluded the top 2400 tuples which were used for creating DEV1, DEV2, and TEST). We created training data by merging the information in the tuples and their OMCS sentences. Our goal was to combine the grammatical context of the OMCS sentences with the words in the actual terms, so as to ensure that we learn embeddings for the words in the terms. We also insert the relations into the OMCS sentences so that we can learn embeddings for the relations themselves.</p><p>We describe the procedure by example and also release our generated data for ease of replication. The tuple soak in a hotspring, CAUSES, get pruny skin was automatically extracted/normalized (by the ConceptNet developers) from the OMCS sen- tence "The effect of [soaking in a hotspring] is [getting pruny skin]" where brackets surround terms. We replace the bracketed portions with their corresponding terms and insert the relation between them: "The effect of soak in a hotspring CAUSES get pruny skin". We do this for all train- ing tuples. <ref type="bibr">3</ref> We used the word2vec (Mikolov et al., 2013) toolkit to train skip-gram word embeddings on this data. We trained for 20 iterations, using a dimensionality of 200 and a window size of 5. We refer to these as "CN-trained" embeddings for the remainder of this paper. Similar approaches have been used to learn embeddings for partic- ular downstream tasks, e.g., dependency pars- ing ( <ref type="bibr" target="#b4">Bansal et al., 2014</ref>). We use our CN-trained embeddings within baseline methods and also pro- vide the initial word embeddings of our models. For all of our models, we update the initial word embeddings during learning.</p><p>In the baseline methods described below, we compare our CN-trained embeddings to pretrained word embeddings. We use the GloVe ( <ref type="bibr" target="#b37">Pennington et al., 2014</ref>) embeddings trained on 840 bil- lion tokens of Common Crawl web text and the PARAGRAM-SimLex embeddings of <ref type="bibr" target="#b47">Wieting et al. (2015)</ref>, which were tuned to have strong perfor- mance on the SimLex-999 task <ref type="figure">(Hill et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Baselines</head><p>We consider three baselines inspired by those of Angeli and Manning (2013):</p><p>• Similar Fact Count (Count): For each tuple τ = t 1 , R, t 2 in the evaluation set, we count the number of similar tuples in the training set. A training tuple τ = t 1 , R , t 2 is considered "similar" to τ if R = R , one of the terms matches exactly, and the other term has the same head word. That is, (R = R ) ∧ (t 1 = t 1 ) ∧ (head (t 2 ) = head (t 2 )), or (R = R ) ∧ (t 2 = t 2 ) ∧ (head (t 1 ) = head (t 1 )). The head word for a term was obtained by running the Stanford Parser ( ) on the term. This baseline does not use word embeddings.</p><p>• Argument Similarity (ArgSim): This baseline computes the cosine similarity of the vectors for t 1 and t 2 , ignoring the relation. Vectors for t 1 and t 2 are obtained by word averaging.</p><p>• Max Similarity (MaxSim): For tuple τ in an evaluation set, this baseline outputs the maxi- mum similarity between τ and any tuple in the training set. The similarity is computed by con- catenating the vectors for t 1 , R, and t 2 , then computing cosine similarity. As in ArgSim, we obtain vectors for terms by averaging their words. We only consider R when using our CN-trained embeddings since they contain em- beddings for the relations. When using GloVe and PARAGRAM embeddings for this baseline, we simply use the two term vectors (still con- structed via averaging the words in each term).</p><p>We chose these baselines because they can all han- dle unbounded term sets but differ in their other requirements. ArgSim and MaxSim use word embeddings while Count does not. Count and MaxSim require iterating over the training set dur- ing inference while ArgSim does not. For each baseline, we tuned a threshold on <ref type="bibr">DEV1</ref> to maximize classification accuracy then tested on DEV2 and TEST.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Training and Tuning</head><p>We used AdaGrad ( <ref type="bibr" target="#b12">Duchi et al., 2011</ref>) for opti- mization, training for 20 epochs through the train- ing tuples. We separately tuned hyperparameters for each model and training objective. We tuned the following hyperparameters: the relation ma- trix size r for the bilinear models (also the length of the transformed term vectors, denoted u 1 and u 2 above), the activation a, the hidden layer size g for the DNN models, the relation vector length d for the DNN models, the LSTM hidden vector size h for models with LSTMs, the mini-batch size β, the regularization parameters λ, λ 1 , and λ 2 , and the AdaGrad learning rate α.</p><p>All tuning used early stopping: periodically during training, we used the current model to find the optimal threshold on DEV1 and evaluated on DEV2. Due to computational limitations, we were unable to perform thorough grid searches for all hyperparameters. We combined limited grid searches with greedy hyperparameter tuning based on regions of values that were the most promising.</p><p>For the Bilinear LSTM and DNN LSTM, we did hyperparameter tuning by training on the full training set of 100,000 tuples for 20 epochs, com- puting DEV2 accuracy once per epoch. For the av- eraging models, we tuned by training on a subset of 1000 tuples with β = 200 for 20 epochs; the averaging models showed more stable results and did not require the full training set for tuning. Be- low are the tuned hyperparameter values:</p><p>• Bilinear AVG: for CE: r = 150, a = tanh, β = 200, α = 0.01, λ 1 = λ 2 = 0.001. Hinge loss: same values as above except α = 0.005.</p><p>• Bilinear LSTM: for CE: r = 50, a = ReLU, h = 200, β = 800, α = 0.02, and λ 1 = λ 2 = 0.00001. To obtain vectors from the term bidi- rectional LSTMs, we used max pooling. For hinge loss: r = 50, a = tanh, h = 200, β = 400, α = 0.007, λ 1 = 0.00001, and λ 2 = 0.01. To obtain vectors from the term bidirectional LSTMs, we used the concatena- tion of average pooling and final hidden vectors in each direction. For each sampling method and loss function, α was tuned by grid search with the others fixed to the above values.</p><p>• DNN AVG: for both losses: a = ReLU, d = 200, g = 1000, β = 600, α = 0.01, λ = 0.001.</p><p>• DNN LSTM: for both losses: a = ReLU, d = 200, bidirectional LSTM hidden layer size h = 200, hidden layer dimension g = 800, β = 400, α = 0.005, and λ = 0.00005. To get vectors from the term LSTMs, we used max pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>Word Embedding Comparison. We first evalu- ate the quality of our word embeddings trained on the ConceptNet training tuples.   We use the latter for the remaining experiments in this paper.</p><p>Training Comparison. <ref type="table">Table 3</ref> shows the re- sults of our models with the two loss functions and three sampling strategies. We find that the binary cross entropy loss with random sampling performs best across models. We note that our conclusion differs from some prior work that found max or mix sampling to be better than random ( <ref type="bibr" target="#b48">Wieting et al., 2016)</ref>. We suspect that this difference may stem from characteristics of the ConceptNet train- ing data. It may often be the case that the max- scoring negative example in the mini-batch is ac- tually a true fact, due to the generic nature of the facts expressed. <ref type="table" target="#tab_4">Table 4</ref> shows a runtime comparison of the losses and sampling strategies. <ref type="bibr">4</ref> We find random sampling to be orders of magnitude faster than the others while also performing the best.</p><p>Final Results. Our final results are shown in Ta- ble 5. We show the DEV2 and TEST accuracies for our baselines and for the best configuration (tuned on DEV2) for each model. All models outperform all baselines handily. Our models perform simi- larly, with the Bilinear models and the DNN AVG model all exceeding 90% on both <ref type="bibr">DEV2</ref> and TEST.</p><p>We note that the AVG models performed strongly compared to those that used LSTMs for <ref type="bibr">4</ref> These experiments were performed using 2 threads on a 3.40-GHz Intel Core i7-3770 CPU with 8 cores.  <ref type="table">Table 5</ref>: Accuracies (%) of baselines and final model configurations on DEV2 and TEST. "+ data" uses enlarged training set of size 300,000, and then doubles this training set by including tuples with conjugated forms; see text for details. Human per- formance on DEV2 was estimated from a sample of size 100.</p><p>modeling terms. We suggest two reasons for this. The first is that most terms are short, with an aver- age term length of 2.3 words in our training tu- ples. An LSTM may not be needed to capture long-distance properties. The second reason may be due to hyperparameter tuning. Recall that we used a greedy search for optimal hyperparameter values; we found that models with LSTMs take more time per epoch, more epochs to converge, and exhibit more hyperparameter sensitivity com- pared to models based on averaging. This may have contributed to inferior hyperparameter values for the LSTM models. We also trained the Bilinear AVG model on a larger training set (row labeled "Bilinear AVG + data"). We note that the ConceptNet tuples typ- ically contain unconjugated forms; we sought to use both conjugated and unconjugated words. We began with a larger training set of 300,000 tuples from ConceptNet, then augmented them to include conjugated word forms as in the following exam- ple. For the tuple soak in a hotspring, CAUSES, get pruny skin obtained from the OMCS sentence "The effect of [soaking in a hotspring] is [get- ting pruny skin]", we generated an additional tuple soaking in a hotspring, CAUSES, getting pruny skin. We thus created twice as many training tu- ples. The results with this larger training set im- proved from 91.7 to 92.5 on the test set. We re- lease this final model to the research community.</p><p>We note that the strongest baseline, MaxSim, is a nonparametric model that requires iterating over all training tuples to provide a score to new tuples. This is a serious bottleneck for use in NLP appli- cations that may need to issue large numbers of  <ref type="table" target="#tab_2">Bilinear LSTM  DNN AVG  DNN LSTM  CE  hinge  CE  hinge  CE hinge  CE hinge  random 90  84  91  83  91  87  88  57  mix  90  83  90  87  90  78  82  63  max  86  75  65  66  61  52  56  52   Table 3</ref>: Accuracies (%) on DEV2 of models trained with two loss functions (cross entropy (CE) and hinge) and three sampling strategies (random, mix, and max). The best accuracy for each model is shown in bold. Cross entropy with random sampling is best across models and is also fastest (see <ref type="table" target="#tab_4">Table 4</ref>).</p><p>queries. Our models are parametric models that can compress a large training set into a fixed num- ber of parameters. This makes them extremely fast for answering queries, particularly the AVG mod- els, enabling use in downstream NLP applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Generating and Scoring Novel Tuples</head><p>We now measure our model's ability to score novel tuples generated automatically from ConceptNet and Wikipedia. We first describe simple pro- cedures to generate candidate tuples from these two datasets. We then score the tuples using our MaxSim baseline and the trained Bilinear AVG model. <ref type="bibr">5</ref> We evaluate the highly-scoring tuples us- ing a small-scale manual evaluation.</p><p>The DNN AVG and Bilinear AVG models reached the highest TEST accuracies in our evalua- tion, though in preliminary experiments we found that the Bilinear AVG model appeared to perform better when scoring the novel tuples described be- low. We suspect this is because the DNN func- tion class has more flexibility than the bilinear one. When scoring novel tuples, many of which may be highly noisy, it appears that the constrained struc- ture of the Bilinear AVG model makes it more ro- bust to the noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Generating Tuples From ConceptNet</head><p>In order to get new tuples, we automatically mod- ify existing ConceptNet tuples. We take an exist- ing tuple and randomly change one of the three fields (t 1 , t 2 , or R), ensuring that the result is not a tuple existing in ConceptNet. We then score these tuples using MaxSim and the Bilinear AVG model and analyze the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Generating Tuples from Wikipedia</head><p>We also propose a simple method to extract candi- date tuples from raw text. We first run the Stan- ford part-of-speech (POS) tagger (Toutanova et  We enumerate the 50 most frequent term pair tag sequences for each relation. We do lim- ited manual filtering of the frequent tag sequences, namely removing the sequences "DT NN NN" and "DT JJ NN" for the ISA relation. We do this in or- der to reduce noise in the extracted tuples. To fo- cus on finding nontrivial tuples, for each relation we retain the top 15 POS tag sequences in which t 1 or t 2 has at least two words. We then run the tagger on sentences from En- glish Wikipedia. We extract word sequence pairs corresponding to the relation POS tag sequence pairs, requiring that there be a gap of at least one word between the two terms. We then remove word sequence pairs in which one term is solely one of the following words: be, the, always, there, has, due, however. We also remove tuples con- taining words that are not in the vocabulary of our ConceptNet-trained embeddings. We require that one term does not include the other term. We cre- ate tuples consisting of the two terms and all pos- sible relations that occur with the POS sequences of those two terms. Finally, we remove tuples that exactly match our ConceptNet training tuples.</p><p>We use our trained Bilinear AVG model to score these tuples. We extract term pairs that oc- cur within the same sentence because we hope that these will have higher precision than if we were to pair together arbitrary pairs. Some example tuples for t 1 = bus are shown in <ref type="table" target="#tab_7">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Manual Analysis of Novel Tuples</head><p>To evaluate our models on newly generated tu- ples, we rank them using different models and manually score the high-ranking tuples for qual- ity. We first randomly sampled 3000 tuples from each set of novel tuples. We do so due to the time requirements of the MaxSim baseline, which re- quires iterating through the entire training set for each candidate tuple. We score these sampled tu- ples using MaxSim and the Bilinear AVG model and rank them by their scores. The top 100 tu- ples under each ranking were given to an annota- tor who is a native English speaker. The annota- tor assigned a quality score to each tuple, using the same 0-4 annotation scheme as <ref type="bibr" target="#b42">Speer et al. (2010)</ref>: 0 ("Doesn't make sense"), 1 ("Not true"), 2 ("Opinion/Don't know"), 3 ("Sometimes true"), and 4 ("Generally true"). We report the average quality score across each set of 100 tuples.</p><p>The results are shown in <ref type="table">Table 7</ref>. To calibrate the scores, we also gave two samples of Concept- Net (CN) tuples to the annotator: a sample of 100 high-confidence tuples (first row) and a sample of 100 medium-confidence tuples (second row). We find the high-confidence tuples to be of high quality, recording an average of 3.68, though the medium-confidence tuples drop to 3.14.</p><p>The next two rows show the quality scores of the MaxSim baseline and the Bilinear AVG model. The latter outperforms the baseline and matches the quality of the medium-confidence ConceptNet tuples. Since our novel tuples are not contained in ConceptNet, this result suggests that our model can be used to add medium-confidence tuples to ConceptNet.</p><p>The novel Wikipedia tuples (top 100 tuples ranked by Bilinear AVG model) had a lower qual- ity score (2.78), but this is to be expected due to the difference in domain. Since Wikipedia con- tains a wide variety of text, we found the novel tuples to be noisier than those from ConceptNet. Still, we are encouraged that on average the tuples are judged to be close to "sometimes true."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Text Analysis with Commonsense Tuples</head><p>We note that our method of tuple extraction and scoring could be used as an aid in applications that require sentence understanding. Two example sentences are shown in <ref type="table" target="#tab_8">Table 8</ref>, along with the top tuples extracted and scored using our method. The tuples capture general knowledge about phrases tuples quality high-confidence CN tuples 3.68 medium-confidence CN tuples 3.14 novel CN tuples, ranked by MaxSim 2.74 novel CN tuples, ranked by Bilinear AVG 3.20 novel Wiki tuples, ranked by Bilinear AVG 2.78 <ref type="table">Table 7</ref>: Average quality scores from manual eval- uation of novel tuples. Each row corresponds to a different set of tuples. See text for details.</p><p>After nine years of primary school, students can go to the high school or to an educational institution. t1, R, t2 score school, HASPROPERTY, educational 0.89 school, ISA, educational institution 0.80 school, ISA, institution 0.78 school, HASPROPERTY, high 0.77 high school, ISA, institution 0.71 On March 14, 1964, Ruby was convicted of murder with malice, for which he received a death sentence.</p><p>t1, R, t2 score murder, CAUSES, death * 1.00 murder, CAUSES, death sentence 0.86 murder, HASSUBEVENT, death 0.84 murder, CAPABLEOF, death 0.51 contained in the sentence, rather than necessarily indicating what the sentence means. This proce- dure could provide relevant commonsense knowl- edge for a downstream application that seeks to understand the sentence. We leave further investi- gation of this idea to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We proposed methods to augment curated com- monsense resources using techniques from knowl- edge base completion. By scoring novel tuples, we showed how we can increase the applicability of the knowledge contained in ConceptNet. In fu- ture work, we will explore how to use our model to improve downstream NLP tasks, and consider applying our methods to other knowledge bases. We have released all of our resources-code, data, and trained models-to the research community. 6</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 compares</head><label>2</label><figDesc></figDesc><table>GloVe PARAGRAM CN-trained 
ArgSim 
68 
69 
73 
MaxSim 
73 
70 
82 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 : Accuracies (%) on DEV2 of two base- lines using three different sets of word embed- dings. Our ConceptNet-trained embeddings out- perform GloVe and PARAGRAM embeddings.</head><label>2</label><figDesc></figDesc><table>DNN AVG 
DNN LSTM 
CE 
hinge 
CE 
hinge 
random 
124 
230 
710 
783 
mix 
20755 21045 25928 26380 
max 
39338 41867 49583 49427 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Loss function runtime comparison (sec-
onds per epoch) of the DNN models. 

accuracies on DEV2 for the two baselines that 
use embeddings: ArgSim and MaxSim. We find 
that pretrained GloVe and PARAGRAM embed-
dings perform comparably, but both are outper-
formed by our ConceptNet-trained embeddings. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Top Wikipedia tuples for 3 relations with 
t 1 = bus, scored by Bilinear AVG model. 

al., 2003) on the terms in our ConceptNet training 
tuples. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>Top ranked tuples extracted from two 
example sentences and scored by Bilinear AVG 
model.  *  = contained in ConceptNet. 

</table></figure>

			<note place="foot" n="3"> For reversed relations, indicated by an asterisk in the OMCS sentences, we swap t1 and t2 in the tuple.</note>

			<note place="foot" n="5"> For the results in this section, we used the Bilinear AVG model that achieved 91.7 on TEST rather than the one augmented with additional data.</note>

			<note place="foot" n="6"> Available at http://ttic.uchicago.edu/ ˜ kgimpel/commonsense.html.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers, John Wieting, and Luke Zettlemoyer.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sentiment analysis using common-sense and context information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basant</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namita</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pooja</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonal</forename><surname>Garg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comp. Int. and Neurosc</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Philosophers are mortal: Inferring the truth of unseen facts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL</title>
		<meeting>of CoNLL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">NaturalLI: Natural logic inference for common sense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D. Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The Berkeley FrameNet project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Collin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">J</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John B</forename><surname>Fillmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
		<meeting>of COLING</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Tailoring continuous word representations for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ACM SIGMOD International Conference on Management of Data</title>
		<meeting>of the ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garciaduran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Question answering with subgraph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A semantic matching energy function for learning with multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">94</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sentic activation: A two-level affective common sense reasoning framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Olsher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Toward an architecture for neverending language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom M</forename><surname>Estevam R Hruschka</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Common-sense knowledge for natural language understanding: Experiments in unsupervised and supervised settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><forename type="middle">Di</forename><surname>Caro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Ruggeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loredana</forename><surname>Cupi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guido</forename><surname>Boella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AI*IA</title>
		<meeting>of AI*IA</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Identifying relations for open information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Effective blending of two and threeway interactions for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>García-Durán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Incorporating vector space similarity in random walk inference over knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><forename type="middle">Pratim</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Using textual patterns to learn expected event frequencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lenhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schubert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction</title>
		<meeting>of Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reporting bias and knowledge acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Workshop on Automated Knowledge Base Construction</title>
		<meeting>of Workshop on Automated Knowledge Base Construction</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning from the web: Extracting general world knowledge from noisy text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lenhart K</forename><surname>Schubert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Collaboratively-Built Knowledge Sources and AI</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Inferential Commonsense Knowledge from Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Gordon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
		<respStmt>
			<orgName>University of Rochester</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Traversing knowledge graphs in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SimLex-999: Evaluating semantic models with (genuine) similarity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CIKM</title>
		<meeting>of CIKM</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A latent factor model for highly multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodolphe</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume R</forename><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Accurate unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Random walk inference and learning in a large scale knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Building large knowledge-based systems: representation and inference in the Cyc project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lenat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>AddisonWesley Longman Publishing Co., Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">How to wreck a nice beach you sing calm incense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Lieberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Faaborg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waseem</forename><surname>Daher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José</forename><surname>Espinosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IUI</title>
		<meeting>of IUI</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Finding function in form: Compositional character models for open vocabulary word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramon</forename><surname>Fermandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Marujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Luis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">WordNet: a lexical database for English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Inferring missing entity type instances for knowledge base completion: New dataset and methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Compositional vector space models for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Factorizing YAGO: Scalable machine learning for linked data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WWW</title>
		<meeting>of WWW</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Relation extraction with matrix factorization and universal schemas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Benjamin Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
		<meeting>of NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Representing general relational knowledge in ConceptNet 5</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of LREC</title>
		<meeting>of LREC</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">AnalogySpace: Reducing the dimensionality of common sense knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Lieberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Using verbosity: Common sense data from games with a purpose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harshit</forename><surname>Surana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of FLAIRS</title>
		<meeting>of FLAIRS</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Feature-rich part-ofspeech tagging with a cyclic dependency network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
		<meeting>of NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Representing text for joint embedding of text and knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pallavi</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Verbosity: a game for collecting common-sense facts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihir</forename><surname>Luis Von Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Kedia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CHI</title>
		<meeting>of CHI</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Knowledge base completion via search-based question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WWW</title>
		<meeting>of WWW</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">From paraphrase database to compositional paraphrase model and back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Towards universal paraphrastic sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6575</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Universal schema for entity type prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Workshop on Automated Knowledge Base Construction</title>
		<meeting>of Workshop on Automated Knowledge Base Construction</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
