<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:28+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generative Topic Embedding: a Continuous Representation of Documents</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Li</surname></persName>
							<email>shaohua@gmail.com dcscts@nus.edu.sg dcszj@tsinghua.edu.cn ascymiao@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">Joint NTU-UBC Research Centre of Excellence in Active Living for the Elderly (LILY) 3</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Miao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">Joint NTU-UBC Research Centre of Excellence in Active Living for the Elderly (LILY) 3</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Generative Topic Embedding: a Continuous Representation of Documents</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="666" to="675"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Word embedding maps words into a low-dimensional continuous embedding space by exploiting the local word collocation patterns in a small context window. On the other hand, topic modeling maps documents onto a low-dimensional topic space, by utilizing the global word collocation patterns in the same document. These two types of patterns are complementary. In this paper, we propose a generative topic embedding model to combine the two types of patterns. In our model, topics are represented by embedding vectors, and are shared across documents. The probability of each word is influenced by both its local context and its topic. A variational inference method yields the topic embed-dings as well as the topic mixing proportions for each document. Jointly they represent the document in a low-dimensional continuous space. In two document classification tasks, our method performs better than eight existing methods, with fewer features. In addition, we illustrate with an example that our method can generate coherent topics even based on only one document .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Representing documents as fixed-length feature vectors is important for many document process- ing algorithms. Traditionally documents are rep- resented as a bag-of-words (BOW) vectors. How- ever, this simple representation suffers from being high-dimensional and highly sparse, and loses se- mantic relatedness across the vector dimensions.</p><p>Word Embedding methods have been demon- strated to be an effective way to represent words as continuous vectors in a low-dimensional em- bedding space ( <ref type="bibr">Bengio et al., 2003;</ref><ref type="bibr" target="#b15">Mikolov et al., 2013;</ref><ref type="bibr" target="#b17">Pennington et al., 2014;</ref><ref type="bibr" target="#b8">Levy et al., 2015</ref>). The learned embedding for a word encodes its semantic/syntactic relatedness with other words, by utilizing local word collocation patterns. In each method, one core component is the embed- ding link function, which predicts a word's distri- bution given its context words, parameterized by their embeddings.</p><p>When it comes to documents, we wish to find a method to encode their overall semantics. Given the embeddings of each word in a document, we can imagine the document as a "bag-of-vectors". Related words in the document point in similar di- rections, forming semantic clusters. The centroid of a semantic cluster corresponds to the most rep- resentative embedding of this cluster of words, re- ferred to as the semantic centroids. We could use these semantic centroids and the number of words around them to represent a document.</p><p>In addition, for a set of documents in a partic- ular domain, some semantic clusters may appear in many documents. By learning collocation pat- terns across the documents, the derived semantic centroids could be more topical and less noisy.</p><p>Topic Models, represented by Latent Dirichlet Allocation (LDA) ( <ref type="bibr" target="#b1">Blei et al., 2003)</ref>, are able to group words into topics according to their colloca- tion patterns across documents. When the corpus is large enough, such patterns reflect their seman- tic relatedness, hence topic models can discover coherent topics. The probability of a word is gov- erned by its latent topic, which is modeled as a categorical distribution in LDA. Typically, only a small number of topics are present in each docu- ment, and only a small number of words have high probability in each topic. This intuition motivated <ref type="bibr" target="#b1">Blei et al. (2003)</ref> to regularize the topic distribu- tions with Dirichlet priors.</p><p>Semantic centroids have the same nature as top- ics in LDA, except that the former exist in the em- bedding space. This similarity drives us to seek the common semantic centroids with a model similar to LDA. We extend a generative word embedding model PSDVec ( <ref type="bibr" target="#b9">Li et al., 2015)</ref>, by incorporating topics into it. The new model is named TopicVec. In TopicVec, an embedding link function models the word distribution in a topic, in place of the cat- egorical distribution in LDA. The advantage of the link function is that the semantic relatedness is al- ready encoded as the cosine distance in the em- bedding space. Similar to LDA, we regularize the topic distributions with Dirichlet priors. A varia- tional inference algorithm is derived. The learning process derives topic embeddings in the same em- bedding space of words. These topic embeddings aim to approximate the underlying semantic cen- troids.</p><p>To evaluate how well TopicVec represents doc- uments, we performed two document classifica- tion tasks against eight existing topic modeling or document representation methods. Two setups of TopicVec outperformed all other methods on two tasks, respectively, with fewer features. In addi- tion, we demonstrate that TopicVec can derive co- herent topics based only on one document, which is not possible for topic models.</p><p>The source code of our implementation is avail- able at https://github.com/askerlee/topicvec. <ref type="bibr" target="#b9">Li et al. (2015)</ref> proposed a generative word em- bedding method PSDVec, which is the precur- sor of TopicVec. PSDVec assumes that the con- ditional distribution of a word given its context words can be factorized approximately into inde- pendent log-bilinear terms. In addition, the word embeddings and regression residuals are regular- ized by Gaussian priors, reducing their chance of overfitting. The model inference is approached by an efficient Eigendecomposition and blockwise- regression method ( <ref type="bibr" target="#b11">Li et al., 2016b</ref>). TopicVec differs from PSDVec in that in the conditional dis- tribution of a word, it is not only influenced by its context words, but also by a topic, which is an em- bedding vector indexed by a latent variable drawn from a Dirichlet-Multinomial distribution. <ref type="bibr" target="#b3">Hinton and Salakhutdinov (2009)</ref> proposed to model topics as a certain number of binary hidden variables, which interact with all words in the doc- ument through weighted connections. <ref type="bibr" target="#b6">Larochelle and Lauly (2012)</ref> assigned each word a unique topic vector, which is a summarization of the con- text of the current word. <ref type="bibr" target="#b4">Huang et al. (2012)</ref> proposed to incorporate global (document-level) semantic information to help the learning of word embeddings. The global embedding is simply a weighted average of the embeddings of words in the document. <ref type="bibr" target="#b7">Le and Mikolov (2014)</ref> proposed Paragraph Vector. It assumes each piece of text has a la- tent paragraph vector, which influences the distri- butions of all words in this text, in the same way as a latent word. It can be viewed as a special case of TopicVec, with the topic number set to 1. Typ- ically, however, a document consists of multiple semantic centroids, and the limitation of only one topic may lead to underfitting. <ref type="bibr" target="#b16">Nguyen et al. (2015)</ref> proposed Latent Feature Topic Modeling (LFTM), which extends LDA to incorporate word embeddings as latent features. The topic is modeled as a mixture of the con- ventional categorical distribution and an embed- ding link function. The coupling between these two components makes the inference difficult. They designed a Gibbs sampler for model infer- ence. Their implementation 1 is slow and infeasi- ble when applied to a large corpous. <ref type="bibr">Liu et al. (2015)</ref> proposed Topical Word Em- bedding (TWE), which combines word embed- ding with LDA in a simple and effective way. They train word embeddings and a topic model separately on the same corpus, and then average the embeddings of words in the same topic to get the embedding of this topic. The topic embedding is concatenated with the word embedding to form the topical word embedding of a word. In the end, the topical word embeddings of all words in a doc- ument are averaged to be the embedding of the document. This method performs well on our two classification tasks. Weaknesses of TWE include: 1) the way to combine the results of word embed- ding and LDA lacks statistical foundations; 2) the LDA module requires a large corpus to derive se- mantically coherent topics. <ref type="bibr" target="#b2">Das et al. (2015)</ref> proposed Gaussian LDA. It uses pre-trained word embeddings. It assumes that words in a topic are random samples from a mul- tivariate Gaussian distribution with the topic em- bedding as the mean. Hence the probability that a Topic assignment of the j-th word j in doc di φ i</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><formula xml:id="formula_0">Name Description S Vocabulary {s1, · · · , sW } V Embedding matrix (vs 1 , · · · , vs W ) D Document set {d1, · · · , dM</formula><p>Mixing proportions of topics in doc di </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Notations and Definitions</head><p>Throughout this paper, we use uppercase bold let- ters such as S, V to denote a matrix or set, low- ercase bold letters such as v w i to denote a vector, a normal uppercase letter such as N, W to denote a scalar constant, and a normal lowercase letter as s i , w i to denote a scalar variable. <ref type="table" target="#tab_1">Table 1</ref> lists the notations in this paper.</p><p>In a document, a sequence of words is referred to as a text window, denoted by w i , · · · , w i+l , or w i :w i+l . A text window of chosen size c before a word w i defines the context of w i as w i−c , · · · , w i−1 . Here w i is referred to as the fo- cus word. Each context word w i−j and the focus word w i comprise a bigram w i−j , w i .</p><p>We assume each word in a document is seman- tically similar to a topic embedding. Topic embed- dings reside in the same N -dimensional space as word embeddings. When it is clear from context, topic embeddings are often referred to as topics. Each document has K candidate topics, arranged in the matrix form T i = (t i1 · · · t iK ), referred to as the topic matrix. Specifically, we fix t i1 = 0, referring to it as the null topic.</p><p>In a document d i , each word w ij is assigned to a topic indexed by z ij ∈ {1, · · · , K}. Geometri- cally this means the embedding v w ij tends to align 2 Almost all modern word embedding methods adopt the exponentiated cosine similarity as the link function, hence the cosine similarity may be assumed to be a better estimate of the semantic relatedness between embeddings derived from these methods. with the direction of t i,z ij . Each topic t ik has a document-specific prior probability to be assigned to a word, denoted as φ ik = P (k|d i ). The vector φ i = (φ i1 , · · · , φ iK ) is referred to as the mixing proportions of these topics in document d i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Link Function of Topic Embedding</head><p>In this section, we formulate the distribution of a word given its context words and topic, in the form of a link function.</p><p>The core of most word embedding methods is a link function that connects the embeddings of a fo- cus word and its context words, to define the distri- bution of the focus word. <ref type="bibr" target="#b9">Li et al. (2015)</ref> proposed the following link function:</p><formula xml:id="formula_1">P (w c | w 0 : w c−1 ) ≈P (w c ) exp v wc c−1 l=0 v w l + c−1 l=0 a w l wc . (1)</formula><p>Here a w l wc is referred as the bigram resid- ual, indicating the non-linear part not captured by v wc v w l . It is essentially the logarithm of the nor- malizing constant of a softmax term. Some litera- ture, e.g. ( <ref type="bibr" target="#b17">Pennington et al., 2014</ref>), refers to such a term as a bias term.</p><p>(1) is based on the assumption that the con- ditional distribution P (w c | w 0 : w c−1 ) can be factorized approximately into independent log- bilinear terms, each corresponding to a context word. This approximation leads to an efficient and effective word embedding algorithm PSDVec ( <ref type="bibr" target="#b9">Li et al., 2015</ref>). We follow this assumption, and pro- pose to incorporate the topic of w c in a way like a latent word. In particular, in addition to the con- text words, the corresponding embedding t ik is in- cluded as a new log-bilinear term that influences the distribution of w c . Hence we obtain the fol- lowing extended link function:</p><formula xml:id="formula_2">P (w c | w 0 :w c−1 , z c , d i ) ≈ P (w c )· exp v wc c−1 l=0 v w l + t zc + c−1 l=0 a w l wc +r zc ,<label>(2)</label></formula><p>where d i is the current document, and r zc is the logarithm of the normalizing constant, named the topic residual. Note that the topic embeddings t zc may be specific to d i . For simplicity of notation, we drop the document index in t zc . To restrict the impact of topics and avoid overfitting, we con- strain the magnitudes of all topic embeddings, so that they are always within a hyperball of radius γ.</p><formula xml:id="formula_3">w 1 · · · w 0 w c z c θ d α v si µ i</formula><p>Word Embeddings It is infeasible to compute the exact value of the topic residual r k . We approximate it by the context size c = 0. Then (2) becomes:</p><formula xml:id="formula_4">P (w c | k, d i ) = P (w c ) exp v wc t k + r k .<label>(3)</label></formula><p>It is required that wc∈S P (w c | k) = 1 to make (3) a distribution. It follows that</p><formula xml:id="formula_5">r k = − log s j ∈S P (s j ) exp{v s j t k } .<label>(4)</label></formula><p>(4) can be expressed in the matrix form:</p><formula xml:id="formula_6">r = − log(u exp{V T }),<label>(5)</label></formula><p>where u is the row vector of unigram probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Generative Process and Likelihood</head><p>The generative process of words in documents can be regarded as a hybrid of LDA and PSDVec. Analogous to PSDVec, the word embedding v s i and residual a s i s j are drawn from respective Gaus- sians. For the sake of clarity, we ignore their gen- eration steps, and focus on the topic embeddings.</p><p>The remaining generative process is as follows:</p><p>1. For the k-th topic, draw a topic embedding uni- formly from a hyperball of radius γ, i.e. t k ∼ Unif(B γ ); 2. For each document d i :</p><p>(a) Draw the mixing proportions φ i from the Dirichlet prior Dir(α);</p><p>(b) For the j-th word: i. Draw topic assignment z ij from the cate- gorical distribution Cat(φ i ); ii. Draw word w ij from S according to</p><formula xml:id="formula_7">P (w ij | w i,j−c :w i,j−1 , z ij , d i ).</formula><p>The above generative process is presented in plate notation in <ref type="figure">Figure (</ref>1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Likelihood Function</head><p>Given the embeddings V , the bigram residuals A, the topics T i and the hyperparameter α, the complete-data likelihood of a single document d i is:</p><formula xml:id="formula_8">p(d i , Z i , φ i |α, V , A, T i ) =p(φ i |α)p(Z i |φ i )p(d i |V , A, T i , Z i ) = Γ( K k=1 α k ) K k=1 Γ(α k ) K j=1 φ α j −1 ij · L i j=1 φ i,z ij P (w ij ) · exp v w ij j−1 l=j−c v w il + t z ij + j−1 l=j−c a w il w ij + r i,z ij ,<label>(6)</label></formula><p>where</p><formula xml:id="formula_9">Z i = (z i1 , · · · , z iL i )</formula><p>, and Γ(·) is the Gamma function.</p><p>Let Z, T , φ denote the collection of all the document-specific</p><formula xml:id="formula_10">{Z i } M i=1 , {T i } M i=1 , {φ i } M i=1</formula><p>, respectively. Then the complete-data likelihood of the whole corpus is:</p><formula xml:id="formula_11">p(D, A, V , Z, T , φ|α, γ, µ) = W i=1 P (v s i ; µ i ) W,W i,j=1 P (a s i s j ; f (h ij )) K k Unif(B γ ) · M i=1 {p(φ i |α)p(Z i |φ i )p(d i |V , A, T i , Z i )} = 1 Z(H, µ)U K γ exp{− W,W i,j=1 f (h i,j )a 2 s i s j − W i=1 µ i v s i 2 } · M i=1 Γ( K k=1 α k ) K k=1 Γ(α k ) K j=1 φ α j −1 ij · L i j=1 φ i,z ij P (w ij ) · exp v w ij j−1 l=j−c v w il +t z ij + j−1 l=j−c a w il w ij +r i,z ij ,<label>(7)</label></formula><p>where P (v s i ; µ i ) and P (a s i s j ; f (h ij )) are the two Gaussian priors as defined in ( <ref type="bibr" target="#b9">Li et al., 2015)</ref>.</p><p>Following the convention in ( <ref type="bibr" target="#b9">Li et al., 2015)</ref>, h ij , H are empirical bigram probabilities, µ are the embedding magnitude penalty coefficients, and Z(H, µ) is the normalizing constant for word embeddings. U γ is the volume of the hyperball of radius γ.</p><p>Taking the logarithm of both sides, we obtain log p(D, A, V , Z, T , φ|α, γ, µ)</p><formula xml:id="formula_12">=C 0 − log Z(H, µ) − A 2 f (H) − W i=1 µ i v s i 2 + M i=1 K k=1 log φ ik (m ik + α k − 1) + L i j=1 r i,z ij +v w ij j−1 l=j−c v w il + t z ij + j−1 l=j−c a w il w ij ,<label>(8)</label></formula><p>where m ik = L i j=1 δ(z ij = k) counts the number of words assigned with the k-th topic in</p><formula xml:id="formula_13">d i , C 0 = M log Γ( K k=1 α k ) K k=1 Γ(α k ) + M,L i i,j=1</formula><p>log P (w ij )−K log U γ is constant given the hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Variational Inference Algorithm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Learning Objective and Process</head><p>Given the hyperparameters α, γ, µ, the learning objective is to find the embeddings V , the topics T , and the word-topic and document-topic distri- butions p(Z i , φ i |d i , A, V , T ). Here the hyperpa- rameters α, γ, µ are kept constant, and we make them implicit in the distribution notations.</p><p>However, the coupling between A, V and T , Z, φ makes it inefficient to optimize them si- multaneously. To get around this difficulty, we learn word embeddings and topic embeddings sep- arately. Specifically, the learning process is di- vided into two stages:</p><p>1. In the first stage, considering that the topics have a relatively small impact to word dis- tributions and the impact might be "averaged out" across different documents, we simplify the model by ignoring topics temporarily. Then the model falls back to the original PSDVec. The optimal solution V * , A * is obtained ac- cordingly; 2. In the second stage, we treat V * , A * as constant, plug it into the likelihood func- tion, and find the corresponding optimal T * , p(Z, φ|D, A * , V * , T * ) of the full model. As in LDA, this posterior is analytically in- tractable, and we use a simpler variational dis- tribution q(Z, φ) to approximate it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Mean-Field Approximation and Variational GEM Algorithm</head><p>In this stage, we fix V = V * , A = A * , and seek the optimal T * , p(Z, φ|D, A * , V * , T * ). As V * , A * are constant, we also make them implicit in the following expressions.</p><p>For an arbitrary variational distribution q(Z, φ), the following equalities hold</p><formula xml:id="formula_14">E q log p(D, Z, φ|T ) q(Z, φ) =E q [log p(D, Z, φ|T )] + H(q) = log p(D|T ) − KL(q||p),<label>(9)</label></formula><p>where p = p(Z, φ|D, T ), H(q) is the entropy of q. This implies</p><formula xml:id="formula_15">KL(q||p) = log p(D|T ) − E q [log p(D, Z, φ|T )] + H(q) = log p(D|T ) − L(q, T ).<label>(10)</label></formula><p>In (10), E q [log p(D, Z, φ|T )] + H(q) is usu- ally referred to as the variational free energy L(q, T ), which is a lower bound of log p(D|T ). Directly maximizing log p(D|T ) w.r.t. T is in- tractable due to the hidden variables Z, φ, so we maximize its lower bound L(q, T ) instead. We adopt a mean-field approximation of the true pos- terior as the variational distribution, and use a variational algorithm to find q * , T * maximizing L(q, T ).</p><p>The following variational distribution is used:</p><formula xml:id="formula_16">q(Z, φ; π, θ) = q(φ; θ)q(Z; π) = M i=1    Dir(φ i ; θ i ) L i j=1 Cat(z ij ; π ij )    .<label>(11)</label></formula><p>We can obtain (Li et al., 2016a)</p><formula xml:id="formula_17">L(q, T ) = M i=1 K k=1 L i j=1 π k ij + α k − 1 ψ(θ ik ) − ψ(θ i0 ) + Tr(T i L i j=1 v w ij π ij ) + r i L i j=1 π ij + H(q) + C 1 ,<label>(12)</label></formula><p>where T i is the topic matrix of the i-th docu- ment, and r i is the vector constructed by con- catenating all the topic residuals r ik .</p><formula xml:id="formula_18">C 1 = C 0 −log Z(H, µ)−A 2 f (H) − W i=1 µ i v s i 2 + M,L i i,j=1 v w ij j−1 k=j−c v w ik + j−1 k=j−c a w ik w ij is constant.</formula><p>We proceed to optimize (12) with a General- ized Expectation-Maximization (GEM) algorithm w.r.t. q and T as follows:</p><p>1. Initialize all the topics T i = 0, and correspond- ingly their residuals r i = 0; 2. Iterate over the following two steps until con- vergence. In the l-th step: (a) Let the topics and residuals be T =</p><formula xml:id="formula_19">T (l−1) , r = r (l−1) , find q (l) (Z, φ) that max- imizes L(q, T (l−1) )</formula><p>. This is the Expectation step (E-step). In this step, log p(D|T ) is con- stant. Then the q that maximizes L(q, T (l) ) will minimize KL(q||p), i.e. such a q is the closest variational distribution to p measured by KL-divergence;</p><formula xml:id="formula_20">(b) Given the variational distribution q (l) (Z, φ), find T (l) , r (l) that improve L(q (l) , T )</formula><p>, using Gradient descent method. This is the gener- alized Maximization step (M-step). In this step, π, θ, H(q) are constant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Update Equations of π, θ in E-Step</head><p>In the E-step, T = T (l−1) , r = r (l−1) are con- stant. Taking the derivative of L(q, T (l−1) ) w.r.t. π k ij and θ ik , respectively, we can obtain the opti- mal solutions ( <ref type="bibr" target="#b10">Li et al., 2016a</ref>) at:</p><formula xml:id="formula_21">π k ij ∝ exp{ψ(θ ik ) + v w ij t ik + r ik }.<label>(13)</label></formula><formula xml:id="formula_22">θ ik = L i j=1 π k ij + α k .<label>(14)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Update Equation of T i in M-Step</head><p>In the Generalized M-step, π = π (l) , θ = θ (l) are constant. For notational simplicity, we drop their superscripts (l).</p><p>To update T i , we first take the derivative of (12) w.r.t. T i , and then take the Gradient Descent method.</p><p>The derivative is obtained as ( <ref type="bibr" target="#b10">Li et al., 2016a)</ref>:</p><formula xml:id="formula_23">∂L(q (l) , T ) ∂T i = L i j=1 v w ij π ij + K k=1 ¯ m ik ∂r ik ∂T i ,<label>(15)</label></formula><p>where</p><formula xml:id="formula_24">¯ m ik = L i j=1 π k ij = E[m ik ]</formula><p>, the sum of the variational probabilities of each word being as- signed to the k-th topic in the i-th document. ∂r ik</p><p>∂T i is a gradient matrix, whose j-th column is ∂r ik ∂t ij .</p><p>Remind that r ik = − log</p><formula xml:id="formula_25">E P (s) [exp{v s t ik }] .</formula><p>When j = k, it is easy to verify that ∂r ik</p><p>∂t ij = 0. When j = k, we have</p><formula xml:id="formula_26">∂r ik ∂t ik = e −r ik · E P (s) [exp{v s t ik }v s ] = e −r ik · s∈W exp{v s t ik }P (s)v s = e −r ik · exp{t ik V }(u • V ),<label>(16)</label></formula><p>where u • V is to multiply each column of V with u element-by-element. Therefore ∂r ik</p><p>∂T i</p><formula xml:id="formula_27">= (0, · · · ∂r ik ∂t ik , · · · , 0). Plug- ging it into (15), we obtain ∂L(q (l) , T ) ∂T i = L i j=1 v w ij π ij +( ¯ m i1 ∂r i1 ∂t i1 , · · · , ¯ m iK ∂r iK ∂t iK ).</formula><p>We proceed to optimize T i with a gradient de- scent method:</p><formula xml:id="formula_28">T (l) i = T (l−1) + λ(l, L i ) ∂L(q (l) , T ) ∂T i , where λ(l, L i ) = L 0 λ 0 l·max{L i ,L 0 }</formula><p>is the learning rate function, L 0 is a pre-specified document length threshold, and λ 0 is the initial learning rate. As the magnitude of ∂L(q (l) ,T )</p><formula xml:id="formula_29">∂T i</formula><p>is approximately pro- portional to the document length L i , to avoid the step size becoming too big a on a long document, if L i &gt; L 0 , we normalize it by L i .</p><p>To satisfy the constraint that t</p><formula xml:id="formula_30">(l) ik ≤ γ, when t (l) ik &gt; γ, we normalize it by γ/t (l) ik .</formula><p>After we obtain the new T , we update r (m) i us- ing (5).</p><p>Sometimes, especially in the initial few itera- tions, due to the excessively big step size of the gradient descent, L(q, T ) may decrease after the update of T . Nonetheless the general direction of L(q, T ) is increasing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Sharing of Topics across Documents</head><p>In principle we could use one set of topics across the whole corpus, or choose different topics for different subsets of documents. One could choose a way to best utilize cross-document information.</p><p>For instance, when the document category in- formation is available, we could make the docu- ments in each category share their respective set of topics, so that M categories correspond to M sets of topics. In the learning algorithm, only the update of π k ij needs to be changed to cater for this situation: when the k-th topic is relevant to the document i, we update π k ij using <ref type="formula" target="#formula_4">(13)</ref>; otherwise π k ij = 0. An identifiability problem may arise when we split topic embeddings according to document subsets. In different topic groups, some highly similar redundant topics may be learned. If we project documents into the topic space, portions of documents in the same topic in different docu- ments may be projected onto different dimensions of the topic space, and similar documents may eventually be projected into very different topic proportion vectors. In this situation, directly us- ing the projected topic proportion vectors could cause problems in unsupervised tasks such as clus- tering. A simple solution to this problem would be to compute the pairwise similarities between topic embeddings, and consider these similarities when computing the similarity between two projected topic proportion vectors. Two similar documents will then still receive a high similarity score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experimental Results</head><p>To investigate the quality of document represen- tation of our TopicVec model, we compared its performance against eight topic modeling or doc- ument representation methods in two document classification tasks. Moreover, to show the topic coherence of TopicVec on a single document, we present the top words in top topics learned on a news article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Document Classification Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.1">Experimental Setup</head><p>Compared Methods Two setups of TopicVec were evaluated:</p><p>• TopicVec: the topic proportions learned by TopicVec; • TV+WV: the topic proportions, concate- nated with the mean word embedding of the document (same as the MeanWV below). We compare the performance of our methods against eight methods, including three topic mod- eling methods, three continuous document repre- sentation methods, and the conventional bag-of- words (BOW) method. The count vector of BOW is unweighted.</p><p>The topic modeling methods include:</p><p>• LDA: the vanilla LDA ( <ref type="bibr" target="#b1">Blei et al., 2003</ref>) in the gensim library 3 ; • sLDA: Supervised Topic Model <ref type="bibr">4 (McAuliffe and Blei, 2008)</ref>, which improves the predic- tive performance of LDA by modeling class labels; • LFTM: Latent Feature Topic Modeling <ref type="bibr">5</ref> ( <ref type="bibr" target="#b16">Nguyen et al., 2015</ref>). The document-topic proportions of topic modeling methods were used as their document representa- tion.</p><p>The document representation methods are:</p><p>• Doc2Vec: Paragraph Vector ( <ref type="bibr" target="#b7">Le and Mikolov, 2014</ref>) in the gensim library 6 .</p><p>• TWE: Topical Word Embedding 7 ( <ref type="bibr">Liu et al., 2015)</ref>, which represents a document by concatenating average topic embedding and average word embedding, similar to our TV+WV; • GaussianLDA: Gaussian LDA 8 ( <ref type="bibr" target="#b2">Das et al., 2015)</ref>, which assumes that words in a topic are random samples from a multivariate Gaussian distribution with the mean as the topic embedding. Similar to TopicVec, we derived the posterior topic proportions as the features of each document; • MeanWV: The mean word embedding of the document. Datasets We used two standard document clas- sification corpora: the 20 Newsgroups 9 and the ApteMod version of the Reuters-21578 corpus <ref type="bibr">10</ref> . The two corpora are referred to as the 20News and Reuters in the following.</p><p>20News contains about 20,000 newsgroup doc- uments evenly partitioned into 20 different cate- gories. Reuters contains 10,788 documents, where each document is assigned to one or more cate- gories. For the evaluation of document classifi- cation, documents appearing in two or more cate- gories were removed. The numbers of documents in the categories of Reuters are highly imbalanced, and we only selected the largest 10 categories, leaving us with 8,025 documents in total.</p><p>The same preprocessing steps were applied to all methods: words were lowercased; stop words and words out of the word embedding vocabulary (which means that they are extremely rare) were removed.</p><p>Experimental Settings TopicVec used the word embeddings trained using PSDVec on a March 2015 Wikipedia snapshot. It contains the most fre- quent 180,000 words. The dimensionality of word embeddings and topic embeddings was 500. The hyperparameters were α = (0.1, · · · , 0.1), γ = 5. For 20news and Reuters, we specified 15 and 12 topics in each category on the training set, respec- tively. The first topic in each category was al- ways set to null. The learned topic embeddings were combined to form the whole topic set, where redundant null topics in different categories were removed, leaving us with 281 topics for 20News and 111 topics for Reuters. The initial learning rate was set to 0.1. After 100 GEM iterations on each dataset, the topic embeddings were ob- tained. Then the posterior document-topic distri- butions of the test sets were derived by performing one E-step given the topic embeddings trained on the training set.</p><p>LFTM includes two models: LF-LDA and LF- DMM. We chose the better performing LF-LDA to evaluate. TWE includes three models, and we chose the best performing TWE-1 to compare.</p><p>LDA, sLDA, LFTM and TWE used the spec- ified 50 topics on Reuters, as this is the optimal topic number according to <ref type="bibr" target="#b13">(Lu et al., 2011</ref>). On the larger 20news dataset, they used the specified 100 topics. Other hyperparameters of all com- pared methods were left at their default values.</p><p>GaussianLDA was specified 100 topics on 20news and 70 topics on Reuters. As each sam- pling iteration took over 2 hours, we only had time for 100 sampling iterations.</p><p>For each method, after obtaining the document representations of the training and test sets, we trained an -1 regularized linear SVM one-vs-all classifier on the training set using the scikit-learn library <ref type="bibr">11</ref> . We then evaluated its predictive perfor- mance on the test set.</p><p>Evaluation metrics Considering that the largest few categories dominate Reuters, we adopted macro-averaged precision, recall and F1 measures as the evaluation metrics, to avoid the average re- sults being dominated by the performance of the 11 http://scikit-learn.org/stable/modules/svm.html   top categories. <ref type="table" target="#tab_3">Table 2</ref> presents the perfor- mance of the different methods on the two clas- sification tasks. The highest scores were high- lighted with boldface. It can be seen that TV+WV and TopicVec obtained the best performance on the two tasks, respectively. With only topic pro- portions as features, TopicVec performed slightly better than BOW, MeanWV and TWE, and sig- nificantly outperformed four other methods. The number of features it used was much lower than BOW, MeanWV and TWE <ref type="table" target="#tab_4">(Table 3)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Results</head><p>GaussianLDA performed considerably inferior to all other methods. After checking the generated topic embeddings manually, we found that the em- beddings for different topics are highly similar to each other. Hence the posterior topic proportions were almost uniform and non-discriminative. In addition, on the two datasets, even the fastest Alias sampling in ( <ref type="bibr" target="#b2">Das et al., 2015</ref>) took over 2 hours for one iteration and 10 days for the whole 100 itera- tions. In contrast, our method finished the 100 EM iterations in 2 hours. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Qualitative Assessment of Topics Derived from a Single Document</head><p>Topic models need a large set of documents to ex- tract coherent topics. Hence, methods depending on topic models, such as TWE, are subject to this limitation. In contrast, TopicVec can extract co- herent topics and obtain document representations even when only one document is provided as in- put.</p><p>To illustrate this feature, we ran TopicVec on a New York Times news article about a pharma- ceutical company acquisition 12 , and obtained 20 topics. <ref type="figure" target="#fig_1">Figure 2</ref> presents the most relevant words in the top-6 topics as a topic cloud. We first calcu- lated the relevance between a word and a topic as the frequency-weighted cosine similarity of their embeddings. Then the most relevant words were selected to represent each topic. The sizes of the topic slices are proportional to the topic pro- portions, and the font sizes of individual words are proportional to their relevance to the topics. Among these top-6 topics, the largest and small- est topic proportions are 26.7% and 9.9%, respec- tively.</p><p>As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, words in obtained topics were generally coherent, although the topics were only derived from a single document. The reason is that TopicVec takes advantage of the rich se- mantic information encoded in word embeddings, which were pretrained on a large corpus.</p><p>The topic coherence suggests that the derived topic embeddings were approximately the seman- tic centroids of the document. This capacity may aid applications such as document retrieval, where a "compressed representation" of the query docu- ment is helpful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions and Future Work</head><p>In this paper, we proposed TopicVec, a generative model combining word embedding and LDA, with the aim of exploiting the word collocation patterns both at the level of the local context and the global document. Experiments show that TopicVec can learn high-quality document representations, even given only one document.</p><p>In our classification tasks we only explored the use of topic proportions of a document as its rep- resentation. However, jointly representing a doc- ument by topic proportions and topic embeddings would be more accurate. Efficient algorithms for this task have been proposed <ref type="bibr" target="#b5">(Kusner et al., 2015)</ref>.</p><p>Our method has potential applications in vari- ous scenarios, such as document retrieval, classifi- cation, clustering and summarization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Graphical representation of TopicVec.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Topic Cloud of the pharmaceutical company acquisition news.</figDesc><graphic url="image-1.png" coords="9,63.50,62.81,240.00,240.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Table of notations 

word belongs to a topic is determined by the Eu-
clidean distance between the word embedding and 
the topic embedding. This assumption might be 
improper as the Euclidean distance is not an opti-
mal measure of semantic relatedness between two 
embeddings 2 . 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Performance on multi-class text classifi-
cation. Best score is in boldface. 

Avg. Features BOW MeanWV TWE TopicVec TV+WV 

20News 
50381 
500 
800 
281 
781 

Reuters 
17989 
500 
800 
111 
611 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Number of features of the five best per-
forming methods. 

</table></figure>

			<note place="foot" n="1"> https://github.com/datquocnguyen/LFTM/</note>

			<note place="foot" n="3"> https://radimrehurek.com/gensim/models/ldamodel.html</note>

			<note place="foot" n="1"> Combined features of TopicVec topic proportions and MeanWV.</note>

			<note place="foot" n="12"> http://www.nytimes.com/2015/09/21/business/a-hugeovernight-increase-in-a-drugs-price-raises-protests.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowlegement</head><p>We thank Xiuchao Sui and Linmei Hu for their help and support. We thank the anonymous men-tor provided by ACL for the careful proofread-ing. This research is funded by the National Re-search Foundation, Prime Minister's Office, Sin-gapore under its IDM Futures Funding Initiative and IRC@SG Funding Initiative administered by IDMPO.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<ptr target="https://radimrehurek.com/gensim/models/doc2vec.html7https://github.com/largelymfs/topicalwordembeddings/8https://github.com/rajarshd/GaussianLDA9http://qwone.com/˜jason/20Newsgroups/10http://www.nltk.org/book/ch02.htmlReferencesYoshuaBengio" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<editor>Réjean Ducharme, Pascal Vincent, and Christian Jauvin</editor>
		<imprint>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Latent dirichlet allocation. the Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Gaussian LDA for topic models with word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="795" to="804" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Replicated softmax: an undirected topic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan R</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1607" to="1614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="873" to="882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">From word embeddings to document distances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Kolkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML-15)</title>
		<editor>David Blei and Francis Bach</editor>
		<meeting>the 32nd International Conference on Machine Learning (ICML-15)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="957" to="966" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A neural autoregressive topic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislas</forename><surname>Lauly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2708" to="2716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning (ICML-14)</title>
		<meeting>the 31st International Conference on Machine Learning (ICML-14)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving distributional similarity with lessons learned from word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="211" to="225" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A generative word embedding model and its low rank positive semidefinite solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="1599" to="1609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Generative topic embedding: a continuous representation of documents (extended version with proofs)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Miao</surname></persName>
		</author>
		<ptr target="https://github.com/askerlee/topicvec/blob/master/topicvec-ext.pdf" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">PSDVec: a toolbox for incremental and scalable word embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Miao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>To appear in Neurocomputing</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">Tat-Seng Chua, and Maosong Sun. 2015. Topical word embeddings. In AAAI</title>
		<imprint>
			<biblScope unit="page" from="2418" to="2424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Investigating task performance of probabilistic topic models: an empirical study of PLSA and LDA. Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="178" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Supervised topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David M</forename><surname>Mcauliffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="121" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS 2013</title>
		<meeting>NIPS 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving topic models with latent feature word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Dat Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Billingsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="299" to="313" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Empiricial Methods in Natural Language Processing</title>
		<meeting>the Empiricial Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
