<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Temporal Word Analogies: Identifying Lexical Replacement with Diachronic Word Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrence</forename><surname>Szymanski</surname></persName>
							<email>terrence.szymanski@insight-centre.org</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Insight Centre for Data Analytics University College Dublin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Temporal Word Analogies: Identifying Lexical Replacement with Diachronic Word Embeddings</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="448" to="453"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-2071</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper introduces the concept of temporal word analogies: pairs of words which occupy the same semantic space at different points in time. One well-known property of word embeddings is that they are able to effectively model traditional word analogies (&quot;word w 1 is to word w 2 as word w 3 is to word w 4 &quot;) through vector addition. Here, I show that temporal word analogies (&quot;word w 1 at time t α is like word w 2 at time t β &quot;) can effectively be mod-eled with diachronic word embeddings, provided that the independent embedding spaces from each time period are appropriately transformed into a common vector space. When applied to a diachronic corpus of news articles, this method is able to identify temporal word analogies such as &quot;Ronald Reagan in 1987 is like Bill Clin-ton in 1997&quot;, or &quot;Walkman in 1987 is like iPod in 2007&quot;. 1 Background The meanings of utterances change over time, due both to changes within the linguistic system and to changes in the state of the world. For example, the meaning of the word awful has changed over the past few centuries from something like &quot;awe-inspiring&quot; to something more like &quot;very bad&quot;, due to a process of semantic drift. On the other hand, the phrase president of the United States has meant different things at different points in time due to the fact that different people have occupied that same position at different times. These are very different types of changes, and the latter may not even be considered a linguistic phenomenon, but both types of change are relevant to the concept of temporal word analogies. I define a temporal word analogy (TWA) as a pair of words which occupy a similar semantic space at different points in time. For example, assuming that there is a semantic space associated with &quot;President of the USA&quot;, this space was occupied by Ronald Reagan in the 1980s, and by Bill Clinton in the 1990s. So a temporal analogy holds: &quot;Ronald Reagan in 1987 is like Bill Clinton in 1997&quot;. Distributional semantics methods, particularly vector-space models of word meanings, have been employed to study both semantic change and word analogies, and as such are well-suited for the task of identifying TWAs. The principle behind these models, that the meaning of words can be captured by looking at the contexts in which they appear (i.e. other words), is not a recent idea, and is generally attributed to Harris (1954) or Firth (1957). The modern era of applying this principle algo-rithmically began with latent semantic analysis (LSA) (Landauer and Dumais, 1997), and the recent explosion in popularity of word embeddings is largely due to the very effective word2vec neural network approach to computing word embeddings (Mikolov et al., 2013a). In these types of vector space models (VSMs), the meaning of a word is represented as a multi-dimensional vector, and semantically-related words tend to have vectors that relate to one another in regular ways (e.g. by occupying nearby points in the vector space). One factor in word embeddings&apos; recent popularity is their eye-catching ability to model word analogies using vector addition, as in the well-known example king + man − woman = queen (Mikolov et al., 2013b). Sagi et al. (2011) were the first to advocate the use of distributional semantics methods (specifi-cally LSA) to automate and quantify large-scale studies of semantic change, in contrast to a more traditional approach in which a researcher inspects 448</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Background</head><p>The meanings of utterances change over time, due both to changes within the linguistic system and to changes in the state of the world. For example, the meaning of the word awful has changed over the past few centuries from something like "awe- inspiring" to something more like "very bad", due to a process of semantic drift. On the other hand, the phrase president of the United States has meant different things at different points in time due to the fact that different people have occupied that same position at different times. These are very different types of changes, and the latter may not even be considered a linguistic phenomenon, but both types of change are relevant to the concept of temporal word analogies.</p><p>I define a temporal word analogy (TWA) as a pair of words which occupy a similar semantic space at different points in time. For example, as- suming that there is a semantic space associated with "President of the USA", this space was oc- cupied by Ronald Reagan in the 1980s, and by Bill Clinton in the 1990s. So a temporal analogy holds: "Ronald Reagan in 1987 is like Bill Clinton in 1997".</p><p>Distributional semantics methods, particularly vector-space models of word meanings, have been employed to study both semantic change and word analogies, and as such are well-suited for the task of identifying TWAs. The principle behind these models, that the meaning of words can be captured by looking at the contexts in which they appear (i.e. other words), is not a recent idea, and is gen- erally attributed to <ref type="bibr" target="#b3">Harris (1954)</ref> or <ref type="bibr" target="#b1">Firth (1957)</ref>. The modern era of applying this principle algo- rithmically began with latent semantic analysis (LSA) <ref type="bibr" target="#b6">(Landauer and Dumais, 1997)</ref>, and the re- cent explosion in popularity of word embeddings is largely due to the very effective word2vec neural network approach to computing word embeddings ( <ref type="bibr" target="#b10">Mikolov et al., 2013a</ref>). In these types of vec- tor space models (VSMs), the meaning of a word is represented as a multi-dimensional vector, and semantically-related words tend to have vectors that relate to one another in regular ways (e.g. by occupying nearby points in the vector space). One factor in word embeddings' recent popularity is their eye-catching ability to model word analogies using vector addition, as in the well-known exam- ple king + man − woman = queen ( <ref type="bibr" target="#b11">Mikolov et al., 2013b)</ref>. <ref type="bibr" target="#b13">Sagi et al. (2011)</ref> were the first to advocate the use of distributional semantics methods (specifi- cally LSA) to automate and quantify large-scale studies of semantic change, in contrast to a more traditional approach in which a researcher inspects a handful of selected words by hand. While not using a VSM approach, <ref type="bibr" target="#b9">Mihalcea and Nastase (2012)</ref> used context words as features to perform "word epoch disambiguation", effectively captur- ing changes in word meanings over time. And several recent papers have combined neural em- bedding methods with large-scale diachronic cor- pora (e.g. the Google Books corpus) to study changes in word meanings over time. <ref type="bibr" target="#b4">Kim et al. (2014)</ref> measured the drift (as cosine similarity) of a word's vector over time to identify words like cell and gay whose meaning changed over the past 100 years. <ref type="bibr" target="#b5">Kulkarni et al. (2015)</ref> used a similar approach, combined with word frequencies and changepoint detection, to plot a word's movement through different lexical neighborhoods over time. Most recently, <ref type="bibr" target="#b2">Hamilton et al. (2016)</ref> employed this methodology to discover and support two laws of semantic change, noting that words with higher frequency or higher levels of polysemy are more likely to experience semantic changes.</p><p>While the study of word meanings over time using diachronic text corpora is a relatively niche subject with little commercial applicability, it has recently gained attention in the broader compu- tational linguistics community. A 2015 SemEval task was dedicated to Diachronic Text Evaluation ( <ref type="bibr" target="#b12">Popescu and Strapparava, 2015)</ref>; while systems submitted to the task successfully predicted the date of a text using traditional machine learning algorithms ( <ref type="bibr" target="#b14">Szymanski and Lynch, 2015)</ref>, none of the submissions employed distributional seman- tics methods. Also in 2015, the president of the ACL singled out recent work (cited in the previous paragraph) using word embeddings to study semantic change as valuable examples of "more scientific uses of distributed representations and Deep Learning" in computational linguistics <ref type="bibr" target="#b8">(Manning, 2015)</ref>.</p><p>The present work is inspired by this line of re- search, and is a continuation on the same theme with a twist: whereas past work has investigated specific words whose meanings have changed over time, the present work investigates specific mean- ings whose words have changed over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method for Discovering Temporal Word Analogies</head><p>In general, work using diachronic word embed- dings to study semantic change follows a com- mon procedure: first, train independent VSMs for each time period in the diachronic corpus; second, transform those VSMs into a common space; and finally, compare a word's vectors from different VSMs to identify patterns of change over time. This paper follows a similar methodology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Calculating temporal word analogies</head><formula xml:id="formula_0">V A ← W ord2V ec(CorpusA) V B ← W ord2V ec(CorpusB) T ← F itT ransf orm(V B , V A ) V B * ← ApplyT ransf orm(T, V B ) vec 1 ← LookupV ector(V A , word 1 ) vec 2 ← N earestN eighbor(vec 1 , V B * ) word 2 ← LookupW ord(V B * , vec 2 )</formula><p>The general process of calculating a TWA is given in Algorithm 1. For example, if Corpus A is the 1987 texts, and Corpus B is the 1997 texts, and word 1 is reagan, then word 2 will be clinton. Crucially, it is only by applying the transformation to the B vector space that it becomes possible to directly compare the B * space with vectors from the A space. The Python code used in this paper to implement this method is available to download. <ref type="bibr">1</ref> 1. Training Independent VSMs: The data used in this analysis is a corpus of New York Times articles spanning the years 1987 through 2007, containing roughly 50 million words per year. The texts were lowercased and tokenized, and common bigrams were re-tokenized by a sin- gle pass of word2phrase. A separate embedding model was trained for each year in the corpus us- ing word2vec with default-like parameters. <ref type="bibr">2</ref> This resulted in 21 independent vector space models, each with a vocabulary of roughly 100k words.</p><p>2. Aligning Independent VSMs: Because training word embeddings typically begins with a random initial state, and the training itself is stochastic, the embeddings from different runs (even on the same dataset) are not comparable with one another. (Many properties of the embed- ding space, such as the distances between points, are consistent, but the actual values of the vectors are random.) This poses a challenge to work on diachronic word embeddings, which requires the ability to compare the vectors of the same word in different, independently-trained, VSMs. Pre- vious work has employed different approaches to this problem:</p><p>Non-random initialization. In this approach, used by <ref type="bibr" target="#b4">Kim et al. (2014)</ref>, the values for the VSM are not randomly initialized, but instead are ini- tialized with the values from a previously-trained model, e.g. training of the 1988 model would be- gin with values from the 1987 model.</p><p>Local linear regression. This approach, used by <ref type="bibr" target="#b5">Kulkarni et al. (2015)</ref>, assumes that two VSMs are equivalent under a linear transformation, and that most words' meanings do not change over time. A linear regression model is fit to a sample of vectors from the neighborhood of a word (hence "local"), minimizing the mean squared error, i.e. minimiz- ing the distance between the two vectors of a given word in the two vector spaces. A potential draw- back of this approach is that it must be applied sep- arately for each focal word.</p><p>Orthogonal Procrustes. This approach, used by <ref type="bibr" target="#b2">Hamilton et al. (2016)</ref>, is similar to linear re- gression in that it aims to learn a transformation of one embedding space onto another, minimizing the distance between pairs of points, but uses a dif- ferent mathematical method and is applied glob- ally to the full space.</p><p>A thorough investigation into the relative ben- efits of the different methods listed above would be a valuable contribution to future work in this area. In the present work, I take a global linear regression approach, broadly similar to that used by <ref type="bibr" target="#b5">Kulkarni et al. (2015)</ref>. However, I use a large sample of points to fit the model, and I apply the transformation globally to the entire VSM. Exper- iments showed that the accuracy of the transfor- mation (measured by the mean Euclidean distance between pairs of points) increases as the sample size increases: using 10% of the total vocabulary shared by the two models (i.e. roughly 10k out of 100k tokens) produces good results, but there is little reason not to use all of the points (perhaps excluding the specific words that are the target of study, although even this does not make much practical difference in the outputs).</p><p>3. Solving Temporal Word Analogies: Once the independent VSMs have been transformed, it is possible to compare vectors from one VSM with vectors from another VSM. Solving a TWA is then simply a matter of loading the vector of word w 1 from the VSM V A , and then finding the point in the transformed VSM V * 2 closest to that vector. That point corresponds to word w 2 , the solution to the analogy. It is also possible to align a series of VSMs to a single "root" VSM, and thereby trace the analogues of a word over time. The results of applying this method are discussed next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Example Outputs and Analysis</head><p>Using the New York Times corpus, each of the 20 VSMs from 1998 to 2007 were aligned to the 1987 VSM, making it possible to trace a series of analo- gies over time (e.g. " <ref type="bibr">Which words in 1988</ref><ref type="bibr">Which words in , 1989</ref><ref type="bibr">Which words in , 1990</ref><ref type="bibr">Which words in ... are like reagan in 1987</ref>. A set of il- lustrative words from 1987 was chosen, and their vectors from the 1987 VSM were extracted. Then, for each subsequent year, the nearest word in that years' transformed VSM was found. The outputs are displayed in <ref type="table">Table 1</ref>.</p><p>One way to interpret <ref type="table">Table 1</ref> is to think of each column as representing a single semantic concept, and the words in that column illustrate the dif- ferent words embodying that concept at different points in time. So column 1 represents "President of the USA" and column 2 represents "mayor of New York City". The outputs of the TWA sys- tem perfectly reflect the names of the people hold- ing these offices; likely due to the fact that these concepts are discussed in the corpus with high fre- quency and a well-defined lexical neighborhood (e.g. White House, Oval Office, president).</p><p>While the other columns do not produce quite as clean analogies, they do tell interesting stories. The breakup of the Soviet Union is visible in the transition from soviet to russian in 1992, and later that concept (something like "geopolitical foe of the United States") is taken up in the 2000s by North Korea and Iran, two members of George W. Bush's "Axis of Evil" . Changes in technol- ogy can be observed, with the 1987 vector for Walkman (representing something like "portable listening device") passing through CD player and MP3 player ultimately to iPod in 2007. Cultural changes can also be observed: the TWA "yuppie in 1987 is like hipster in 2003", is validated by reports in the media <ref type="bibr" target="#b0">(Bergstein, 2016)</ref>.</p><p>It is not easy to pin a precise meaning to each of these columns, and the "right" answer to any given TWA is to some degree a subjective judg- ment. Any given entity may fill multiple roles at once: which role should be the focus of the anal- ogy? Each vector in the VSM can be thought of as combining multiple components: for example, the vectors for reagan include a component having to do with Ronald Reagan himself <ref type="table" target="#tab_2">(based on words   450 soviet  iran contra navratilova  yuppie  walkman  1988 reagan  koch  soviet  iran contra  sabatini  yuppie  tape deck  1989 bush  koch  soviet  iran contra  navratilova  yuppie  walkman  1990 bush  dinkins  soviet  iran contra  navratilova  yuppie  headphones  1991 bush  dinkins  soviet  iran contra  navratilova  yuppie  cassette player  1992 bush  dinkins  russian  iran contra  sabatini  yuppie  walkman  1993 clinton dinkins  russian  iran contra  navratilova  yuppie  cd player  1994 clinton mr giuliani  russian  iran contra  sanchez vicario yuppie  walkman  1995 clinton giuliani  russian  white house graf  yuppie  cassette player  1996 clinton giuliani  russian  whitewater  graf  yuppie  walkman  1997 clinton giuliani  russian  iran contra  hingis  yuppie  headphones  1998 clinton giuliani  russian  lewinsky  hingis  yuppie  headphones  1999 clinton mayor giuliani russian  white house hingis  yuppie  buttons  2000 clinton giuliani  russian  white house hingis  yuppie  headset  2001 bush  giuliani  russian  iran</ref>   <ref type="table">Table 1</ref>: Examples of words from 1987 and their analogues over time. Each column corresponds to a single point in vector space, and each row shows the word closest to that point in a given year.</p><p>relating to his personal attributes or names of fam- ily members) as well as a component having to do with the presidency (based on words like pres- ident, veto or White House). The analogies based on the 1987 reagan vector produce the names of other presidents over time (as in <ref type="table">Table 1)</ref>; how- ever, if the 1999 reagan vector is used as the start- ing point, then 17 of the 20 analogies produced are either reagan or ronald reagan. This illustrates how the vector from 1999 contains a stronger com- ponent of the individual man rather than the pres- idency, due to the change in how he was writ- ten about when no longer in office. Similarly, the Iran Contra vector can be viewed as a mix- ture of 'the Iran Contra crisis itself" and a more generic "White House scandal" concept. This sec- ond component causes Clinton-era scandals like Whitewater and Lewinsky to briefly appear in that space, while the first causes Iran Contra to con- tinue to appear over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>Standard evaluation sets of word analogies exist and are commonly used as a measure of the qual- ity of word embeddings (but see Linzen (2016) for why this can be problematic and misleading). No data set of manually-verified TWAs currently ex- ists, so a small evaluation set was assembled by hand: ten TWA categories were selected which could be expected to be both newsworthy and un- ambiguous, and values for each year in the corpus were identified using encyclopedic sources. When all pairs of years are considered, this yields a total of 4,200 individual TWAs. This data set, including the prediction outputs, is available online. For comparison, a baseline system which al- ways predicts the output w 2 to be the same as the input w 1 was implemented. (A system based on word co-occurrence counts was also implemented, but produced no usable output. 3 ) <ref type="table" target="#tab_2">Table 2</ref> shows the accuracy of the embedding-based system and the baseline for each category. Accuracy is de- termined by exact match, so mayor giuliani is in- correct for giuliani. Some categories are clearly much more difficult than others: prediction accu- racy is 96% for "President of the USA", but less than 1% for "Best Actress". The names of Oscar Best Actress winners change every year with very little repetition, and it may be that an actress' role as an award winner only constitutes a small part of her overall news coverage.</p><p>Accuracy is a useful metric, but it is not neces- sarily the best way to evaluate TWAs. Due to the nature of the data (the U.S. President, for example, only changes every four or eight years), the base- line system works quite well when the time depth of the analogy (δ t = |t α − t β |) is small. However,  as time depth increases, its accuracy drops sharply, while the embedding-based method remains effec- tive, as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. And even when the embedding-based system makes the "wrong" pre- diction, the output may still be insightful for data exploration, which is a more likely application for this method rather than prediction. The analogies evaluated here have the benefits of being easy to compile and evaluate, but they represent only one specific subset of TWAs. Other, less-clearly-defined, types of analogies (like the yuppie and walkman examples) would require a less rigid (and more expensive), form of evalua- tion, such as obtaining human acceptability judg- ments of the automatically-produced analogies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper I have presented the concept of tem- poral word analogies and a method for identi- fying them using diachronic word embeddings. The method presented here is effective at solving TWAs, as shown in the evaluation, but its greater strength may be as a tool for data exploration. The small set of examples included in this paper illus- trate political and social changes that unfold over time, and in the hands of users with diverse cor- pora and research questions, many more interest- ing analogies would likely be discovered using this same method.</p><p>The method presented in this paper is not the only way that TWAs could be predicted. If the VSMs could somehow be trained jointly, rather than independently, this would eliminate the need for transformation and the noise it introduces. Or perhaps it is sufficient to look at lexical neighbor- hoods, rather the vectors themselves. One lim- itation of the embedding approach is that it re- quires vast amounts of data from each time pe- riod: tens of millions of words are required to train a model with word2vec. This makes it im- practical for many historical corpora, which tend to be much smaller than the corpus used here. If a simpler, count-based approach could be made to work, this might be more applicable to smaller corpora. A method which incorporates word fre- quency ( <ref type="bibr" target="#b5">Kulkarni et al., 2015)</ref> might be effective at identifying when one word drops from com- mon usage and another word appears. And as- signing a measure of confidence to the proposed TWAs could help automatically identify meaning- ful analogies from the vast combinations of words and years that exist.</p><p>In a domain where large quantities of real-time text are available, this method could potentially be applied as a form of event detection, identify- ing new entrants into a semantic space. And the same method described here could potentially be applied to other, non-diachronic, types of corpora. For example, given corpora of British English and American English, this methodology might be used to identify dialectal analogies, e.g. "elevator in US English is like lift in British English." In- deed, this general approach of comparing words in multiple embedding spaces could have many ap- plications outside of diachronic linguistics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Accuracy as function of time depth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Analogy prediction accuracy.</head><label>2</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> https://github.com/tdszyman/twapy. 2 Example parameters: CBOW model, vector size 200, window size 8, negative samples 5.</note>

			<note place="foot" n="3"> The co-occurrence matrices were expensive to construct due to the volume of data, and despite efforts to smooth the distributions, the analogy outputs were noisy, dominated by low-frequency tokens with relatively few non-zero components and high cosine similarities. But it is possible that with more careful engineering this approach could be effective.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>I would like to thank all the participants at the Insight Centre for Data Analytics special interest group meeting on NLP at NUI Galway for their encouraging and insightful feedback. Thanks also to the anonymous ACL reviewers, for encourag-ing the addition of a quantitative evaluation. This work was partially supported by Science Founda-tion Ireland through the Insight Centre for Data Analytics under grant number SFI/12/RC/2289.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Are hipsters the new yuppies? forbes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachelle</forename><surname>Bergstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-10-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A synopsis of linguistic theory 19301955</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Firth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Studies in Linguistic Analysis</title>
		<imprint>
			<publisher>Philological Society</publisher>
			<date type="published" when="1957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Diachronic word embeddings reveal historical laws of semantic change</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Distributional structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zellig</forename><surname>Harris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1954" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="146" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Temporal analysis of language through neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-I</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Hanaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darshan</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science</title>
		<meeting>the ACL 2014 Workshop on Language Technologies and Computational Social Science</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="61" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Statistically significant detection of linguistic change</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
		<meeting>the 24th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="625" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A solution to Plato&apos;s problem: The latent semantic analysis theory of the acquisition, induction, and representation of knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dumais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="211" to="140" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Issues in evaluating semantic spaces using word analogies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tal Linzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for NLP</title>
		<meeting>the 1st Workshop on Evaluating Vector-Space Representations for NLP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Computational linguistics and deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Word epoch disambiguation: Finding how words change over time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivi</forename><surname>Nastase</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations Workshop</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semeval 2015, task 7: Diachronic text evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Octavian</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Strapparava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation</title>
		<meeting>the 9th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Tracing semantic change with latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eyal</forename><surname>Sagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brady</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Current Methods in Historical Semantics</title>
		<imprint>
			<publisher>De Gruyter Mouton</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">UCD: Diachronic text classification with character, word, and syntactic n-grams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrence</forename><surname>Szymanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Lynch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation</title>
		<meeting>the 9th International Workshop on Semantic Evaluation<address><addrLine>SemEval</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
