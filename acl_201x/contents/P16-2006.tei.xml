<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:16+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Incremental Parsing with Minimal Features Using Bi-Directional LSTM</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Cross</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">Oregon State University Corvallis</orgName>
								<address>
									<settlement>Oregon</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">Oregon State University Corvallis</orgName>
								<address>
									<settlement>Oregon</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Incremental Parsing with Minimal Features Using Bi-Directional LSTM</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="32" to="37"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recently, neural network approaches for parsing have largely automated the combination of individual features, but still rely on (often a larger number of) atomic features created from human linguistic intuition , and potentially omitting important global context. To further reduce feature engineering to the bare minimum, we use bi-directional LSTM sentence representations to model a parser state with only three sentence positions, which automatically identifies important aspects of the entire sentence. This model achieves state-of-the-art results among greedy dependency parsers for English. We also introduce a novel transition system for constituency parsing which does not require binarization, and together with the above architecture, achieves state-of-the-art results among greedy parsers for both En-glish and Chinese.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, neural network-based parsers have be- come popular, with the promise of reducing the burden of manual feature engineering. For ex- ample, <ref type="bibr" target="#b0">Chen and Manning (2014)</ref> and subsequent work replace the huge amount of manual fea- ture combinations in non-neural network efforts ( <ref type="bibr" target="#b9">Nivre et al., 2006;</ref><ref type="bibr" target="#b15">Zhang and Nivre, 2011</ref>) by vector embeddings of the atomic features. How- ever, this approach has two related limitations. First, it still depends on a large number of care- fully designed atomic features. For example, <ref type="bibr" target="#b0">Chen and Manning (2014)</ref> and subsequent work such as <ref type="bibr" target="#b13">Weiss et al. (2015)</ref> use 48 atomic features from <ref type="bibr" target="#b15">Zhang and Nivre (2011)</ref>, including select third- order dependencies. More importantly, this ap- proach inevitably leaves out some nonlocal in- formation which could be useful. In particular, though such a model can exploit similarities be- tween words and other embedded categories, and learn interactions among those atomic features, it cannot exploit any other details of the text.</p><p>We aim to reduce the need for manual induction of atomic features to the bare minimum, by us- ing bi-directional recurrent neural networks to au- tomatically learn context-sensitive representations for each word in the sentence. This approach al- lows the model to learn arbitrary patterns from the entire sentence, effectively extending the general- ization power of embedding individual words to longer sequences. Since such a feature representa- tion is less dependent on earlier parser decisions, it is also more resilient to local mistakes.</p><p>With just three positional features we can build a greedy shift-reduce dependency parser that is on par with the most accurate parser in the published literature for English Treebank. This effort is sim- ilar in motivation to the stack-LSTM of <ref type="bibr" target="#b2">Dyer et al. (2015)</ref>, but uses a much simpler architecture.</p><p>We also extend this model to predict phrase- structure trees with a novel shift-promote-adjoin system tailored to greedy constituency parsing, and with just two more positional features (defin- ing tree span) and nonterminal label embeddings we achieve the most accurate greedy constituency parser for both English and Chinese. Figure 1: The sentence is modeled with an LSTM in each direction whose input vectors at each time step are word and part-of-speech tag embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">LSTM Position Features</head><p>The central idea behind this approach is exploiting the power of recurrent neural networks to let the model decide what apsects of sentence context are important to making parsing decisions, rather than relying on fallible linguistic information (which moreover requires leaving out information which could be useful). In particular, we model an in- put sentence using Long Short-Term Memory net- works (LSTM), which have made a recent resur- gence after being initially formulated by Hochre- iter and <ref type="bibr" target="#b6">Schmidhuber (1997)</ref>.</p><p>The input at each time step is simply a vector representing the word, in this case an embedding for the word form and one for the part-of-speech tag. These embeddings are learned from random initialization together with other network param- eters in this work. In our initial experiments, we used one LSTM layer in each direction (forward and backward), and then concatenate the output at each time step to represent that sentence posi- tion: that word in the entire context of the sen- tence. This network is illustrated in <ref type="figure">Figure 1</ref>.  It is also common to stack multiple such LSTM layers, where the output of the forward and back- ward networks at one layer are concatenated to form the input to the next. We found that parsing performance could be improved by using two bi- directional LSTM layers in this manner, and con- catenating the output of both layers as the posi- tional feature representation, which becomes the input to the fully-connected layer. This architec- input: w 0 . . . w n−1 <ref type="figure">Figure 3</ref>: The arc-standard dependency parsing system (Nivre, 2008) (re omitted). Stack S is a list of heads, j is the start index of the queue, and s 0 and s 1 are the top two head indices on S. ture is shown in <ref type="figure" target="#fig_2">Figure 2</ref>. Intuitively, this represents the sentence position by the word in the context of the sentence up to that point and the sentence after that point in the first layer, as well as modeling the "higher-order" interactions between parts of the sentence in the second layer. In Section 5 we report results us- ing only one LSTM layer ("Bi-LSTM") as well as with two layers where output from each layer is used as part of the positional feature ("2-Layer Bi- LSTM").</p><formula xml:id="formula_0">axiom , 0: ∅ shift S, j : A S|j, j + 1 : A j &lt; n re S|s 1 |s 0 , j : A S|s 0 , j : A ∪ {s 1 s 0 } goal s 0 , n: A</formula><formula xml:id="formula_1">dependency constituency positional s 1 , s 0 , q 0 s 1 , s 0 , q 0 , s 1 .left, s 0 .left labels - s 0 .{left, right, root, head} s 1 .{left, right, root, head}</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Shift-Reduce Dependency Parsing</head><p>We use the arc-standard system for dependency parsing (see <ref type="figure">Figure 4)</ref>. By exploiting the LSTM architecture to encode context, we found that we were able to achieve competitive results using only three sentence-position features to model parser state: the head word of each of the top two trees on the stack (s 0 and s 1 ), and the next word on the queue (q 0 ); see <ref type="table" target="#tab_0">Table 1</ref>.</p><p>The usefulness of the head words on the stack is clear enough, since those are the two words that are linked by a dependency when taking a reduce action. The next incoming word on the queue is also important because the top tree on the stack should not be reduced if it still has children <ref type="bibr">which</ref> have not yet been shifted. That feature thus allows input: w 0 . . . w n−1</p><formula xml:id="formula_2">axiom , 0: ∅ shift S, j S | j, j + 1 j &lt; n pro(X) S | t, j S | X(t), j adj S | t | X(t 1 ...t k ), j S | X(t, t 1 ...t k ), j goal s 0 , n</formula><p>Figure 4: Our shift-promote-adjoin system for constituency parsing (adj omitted).</p><p>the model to learn to delay a right-reduce until the top tree on the stack is fully formed, shifting in- stead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Hierarchical Classification</head><p>The structure of our network model after com- puting positional features is fairly straightforward and similar to previous neural-network parsing ap- proaches such as <ref type="bibr" target="#b0">Chen and Manning (2014)</ref> and <ref type="bibr" target="#b13">Weiss et al. (2015)</ref>. It consists of a multilayer perceptron using a single ReLU hidden layer fol- lowed by a linear classifier over the action space, with the training objective being negative log soft- max.</p><p>We found that performance could be improved, however, by factoring out the decision over struc- tural actions (i.e., shift, left-reduce, or right- reduce) and the decision of which arc label to as- sign upon a reduce. We therefore use separate classifiers for those decisions, each with its own fully-connected hidden and output layers but shar- ing the underlying recurrent architecture. This structure was used for the results reported in Sec- tion 5, and it is referred to as "Hierarchical Ac- tions" when compared against a single action clas- sifier in <ref type="table">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Shift-Promote-Adjoin Constituency Parsing</head><p>To further demonstrate the advantage of our idea of minimal features with bidirectional sentence representations, we extend our work from depen- dency parsing to constituency parsing. However, the latter is significantly more challenging than the former under the shift-reduce paradigm because: </p><note type="other">shift (I) 6 pro (NP) 2 pro (NP) 7 adj 3 shift (like) 8 pro (S) 4 pro (VP) 9 adj 5 shift (sports) Figure 5: Shift-Promote-Adjoin parsing example. Upward and downward arrows indicate promote and (sister-)adjunction actions, respectively.</note><p>• we also need to predict the nonterminal labels</p><p>• the tree is not binarized (with many unary rules and more than binary branching rules)</p><p>While most previous work binarizes the con- stituency tree in a preprocessing step ( <ref type="bibr" target="#b16">Zhu et al., 2013;</ref><ref type="bibr" target="#b12">Wang and Xue, 2014;</ref><ref type="bibr" target="#b8">Mi and Huang, 2015)</ref>, we propose a novel "Shift-Promote- Adjoin" paradigm which does not require any bi- nariziation or transformation of constituency trees (see <ref type="figure">Figure 5</ref>). Note in particular that, in our case only the Promote action produces a new tree node (with a non-terminal label), while the Ad- join action is the linguistically-motivated "sister- adjunction" operation, i.e., attachment <ref type="bibr" target="#b1">(Chiang, 2000;</ref><ref type="bibr" target="#b4">Henderson, 2003)</ref>. By comparison, in pre- vious work, both Unary-X and Reduce-L/R-X ac- tions produce new labeled nodes (some of which are auxiliary nodes due to binarization). Thus our paradigm has two advantages:</p><p>• it dramatically reduces the number of possi- ble actions, from 3X + 1 or more in previ- ous work to 3 + X, where X is the number of nonterminal labels, which we argue would simplify learning;</p><p>• it does not require binarization ( <ref type="bibr" target="#b16">Zhu et al., 2013;</ref><ref type="bibr" target="#b12">Wang and Xue, 2014</ref>) or compression of unary chains ( <ref type="bibr" target="#b8">Mi and Huang, 2015)</ref> There is, however, a more closely-related "shift- project-attach" paradigm by <ref type="bibr" target="#b4">Henderson (2003)</ref>. For the example in <ref type="figure">Figure 5</ref> he would use the fol- lowing actions:</p><p>shift(I), project(NP), project(S), shift(like), project(VP), shift(sports), project(NP), attach, attach.</p><p>The differences are twofold: first, our Promote ac- tion is head-driven, which means we only promote the head child (e.g., VP to S) whereas his Project action promotes the first child (e.g., NP to S); and secondly, as a result, his Attach action is always right-attach whereas our Adjoin action could be ei- ther left or right. The advantage of our method is its close resemblance to shift-reduce dependency parsing, which means that our constituency parser is jointly performing both tasks and can produce both kinds of trees. This also means that we use head rules to determine the correct order of gold actions.</p><p>We found that in this setting, we did need slightly more input features. As mentioned, node labels are necessary to distinguish whether a tree has been sufficiently promoted, and are helpful in any case. We used 8 labels: the current and im- mediate predecessor label of each of the top two stacks on the tree, as well as the label of the left- and rightmost adjoined child for each tree. We also found it helped to add positional features for the leftmost word in the span for each of those trees, bringing the total number of positional features to five. See <ref type="table" target="#tab_0">Table 1</ref> for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>We report both dependency and constituency pars- ing results on both English and Chinese.</p><p>All experiments were conducted with minimal hyperparameter tuning. The settings used for the reported results are summarized in <ref type="table" target="#tab_7">Table 6</ref>. Networks parameters were updated using gradi- ent backpropagation, including backpropagation through time for the recurrent components, using ADADELTA for learning rate scheduling <ref type="bibr" target="#b14">(Zeiler, 2012)</ref>. We also applied dropout ( <ref type="bibr" target="#b5">Hinton et al., 2012</ref>) (with p = 0.5) to the output of each LSTM layer (separately for each connection in the case of the two-layer network).</p><p>We tested both types of parser on the Penn Tree- bank (PTB) and Penn Chinese Treebank (CTB-5), with the standard splits for each of training, de- velopment, and test sets. Automatically predicted part of speech tags with 10-way jackknifing were used as inputs for all tasks except for Chinese de- pendency parsing, where we used gold tags, fol- lowing the traditions in literature. <ref type="table" target="#tab_3">Table 2</ref> shows results for English Penn Tree- bank using Stanford dependencies. Despite the minimally designed feature representation, rela- tively few training iterations, and lack of pre- computed embeddings, the parser performed on par with state-of-the-art incremental dependency parsers, and slightly outperformed the state-of- the-art greedy parser.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dependency Parsing: English &amp; Chinese</head><p>The ablation experiments shown in the <ref type="table">Table 3</ref> indicate that both forward and backward contexts for each word are very important to obtain strong results. Using only word forms and no part-of- speech input similarly degraded performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parser</head><p>Dev   <ref type="table">Table 3</ref>: Ablation studies on PTB dev set (wsj 22). Forward and backward context, and part-of- speech input were all critical to strong performace. <ref type="figure">Figure 6</ref> compares our parser with that of Chen and Manning (2014) in terms of arc recall for var- ious arc lengths. While the two parsers perform similarly on short arcs, ours significantly outpe- forms theirs on longer arcs, and more interestingly our accuracy does not degrade much after length 6. This confirms the benefit of having a global sentence repesentation in our model.  <ref type="table" target="#tab_4">Table 4</ref>: Development and test set results for shift- reduce dependency parser on Penn Chinese Tree- bank (CTB-5) using only (s 1 , s 0 , q 0 ) position fea- tures (trained and tested with gold POS tags). <ref type="table" target="#tab_6">Table 5</ref> compares our constituency parsing re- sults with state-of-the-art incremental parsers. Al- though our work are definitely less accurate than those beam-search parsers, we achieve the highest accuracy among greedy parsers, for both English and Chinese. <ref type="bibr">1</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Constituency Parsing: English &amp; Chinese</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Because recurrent networks are such a natural fit for modeling languages (given the sequential na- ture of the latter), bi-directional LSTM networks are becoming increasingly common in all sorts of linguistic tasks, for example event detection in <ref type="bibr" target="#b3">Ghaeini et al. (2016)</ref>. In fact, we discovered after submission that <ref type="bibr" target="#b7">Kiperwasser and Goldberg (2016)</ref> have concurrently developed an extremely similar approach to our dependency parser. Instead of ex- tending it to constituency parsing, they also apply the same idea to graph-based dependency parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>We have presented a simple bi-directional LSTM sentence representation model for minimal fea- tures in both incremental dependency and incre- mental constituency parsing, the latter using a novel shift-promote-adjoint algorithm. Experi- ments show that our method are competitive with the state-of-the-art greedy parsers on both parsing tasks and on both English and Chinese.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: In the 2-Layer architecture, the output of each LSTM layer is concatenated to create the positional feature vector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Feature templates. Note that, remarkably, 
even though we do labeled dependency parsing, 
we do not include arc label as features. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Development and test set results for shift-
reduce dependency parser on Penn Treebank using 
only (s 1 , s 0 , q 0 ) positional features. 

Parser 
UAS LAS 
Bi-LSTM Hierarchical  † 93.31 91.01 
 † -Hierarchical Actions 92.94 90.96 
 † -Backward-LSTM 
91.12 88.72 
 † -Forward-LSTM 
91.85 88.39 
 † -tag embeddings 
92.46 89.81 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 summarizes</head><label>4</label><figDesc></figDesc><table>the Chinese dependency 
parsing results. Again, our work is competitive 
with the state-of-the-art greedy parsers. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>, 2</head><label>2</label><figDesc></figDesc><table>Parser 
b 
English 
Chinese 

greedy beam greedy beam 

Zhu et al. (2013) 
16 86.08 90.4 75.99 85.6 
Mi &amp; Huang (05) 32 84.95 90.8 75.61 83.9 
Vinyals et al. (05) 10 -
90.5 -
-
Bi-LSTM 
-89.75 -
79.44 -
2-Layer Bi-LSTM -89.95 -
80.13 -

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Test F-scores for constituency parsing on 
Penn Treebank and CTB-5. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 6 : Hyperparameters and training settings.</head><label>6</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> The greedy accuracies for Mi and Huang (2015) are from Haitao Mi, and greedy results for Zhu et al. (2013) come from duplicating experiments with code provided by those authors. 2 The parser of Vinyals et al. (2015) does not use an explicit transition system, but is similar in spirit since generating a right bracket can be viewed as a reduce action.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for comments. We also thank Taro Watanabe, Muhua Zhu, and Yue Zhang for sharing their code, Haitao Mi for producing greedy results from his parser, and Ashish Vaswani and Yoav Goldberg for discus-sions. The authors were supported in part by DARPA FA8750-13-2-0041 (DEFT), NSF IIS-1449278, and a Google Faculty Research Award.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Statistical parsing with an automatically-extracted tree-adjoining grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.08075</idno>
		<title level="m">Transitionbased dependency parsing with stack long shortterm memory</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Event nugget detection with forward-backward recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Ghaeini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoli</forename><forename type="middle">Z</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasad</forename><surname>Tadepalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Inducing history representations for broad coverage statistical parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<idno>abs/1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
	<note>Ilya Sutskever, and Ruslan Salakhutdinov</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Simple and accurate dependency parsing using bidirectional LSTM feature representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliyahu</forename><surname>Kiperwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno>abs/1603.04351</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Shift-reduce constituency parsing with dynamic programming and pos tag lattice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Maltparser: A data-driven parser-generator for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Nilsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of LREC</title>
		<meeting>of LREC</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Algorithms for deterministic incremental dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="513" to="553" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2755" to="2763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Joint pos tagging and transition-based constituent parsing in chinese with non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Structured training for neural network transition-based parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">ADADELTA: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno>abs/1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing with rich non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="188" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fast and accurate shiftreduce constituent parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2013</title>
		<meeting>ACL 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
