<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Provably Correct Learning Algorithm for Latent-Variable PCFGs</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
							<email>scohen@inf.ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Informatics</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Edinburgh</orgName>
								<orgName type="institution" key="instit2">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
							<email>mcollins@cs.columbia.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Informatics</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Edinburgh</orgName>
								<orgName type="institution" key="instit2">Columbia University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Provably Correct Learning Algorithm for Latent-Variable PCFGs</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1052" to="1061"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We introduce a provably correct learning algorithm for latent-variable PCFGs. The algorithm relies on two steps: first, the use of a matrix-decomposition algorithm applied to a co-occurrence matrix estimated from the parse trees in a training sample; second, the use of EM applied to a convex objective derived from the training samples in combination with the output from the matrix decomposition. Experiments on parsing and a language modeling problem show that the algorithm is efficient and effective in practice.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Latent-variable PCFGs (L-PCFGs) ( <ref type="bibr" target="#b17">Matsuzaki et al., 2005;</ref><ref type="bibr" target="#b19">Petrov et al., 2006</ref>) give state-of-the-art performance on parsing problems. The standard approach to parameter estimation in L-PCFGs is the EM algorithm <ref type="bibr" target="#b9">(Dempster et al., 1977)</ref>, which has the usual problems with local optima. Re- cent work <ref type="bibr" target="#b7">(Cohen et al., 2012)</ref> has introduced an alternative algorithm, based on spectral methods, which has provable guarantees. Unfortunately this algorithm does not return parameter estimates for the underlying L-PCFG, instead returning the pa- rameter values up to an (unknown) linear trans- form. In practice, this is a limitation.</p><p>We describe an algorithm that, like EM, re- turns estimates of the original parameters of an L- PCFG, but, unlike EM, does not suffer from prob- lems of local optima. The algorithm relies on two key ideas:</p><p>1) A matrix decomposition algorithm (sec- tion 5) which is applicable to matrices Q of the form Q f,g = h p(h)p(f | h)p(g | h) where p(h), p(f | h) and p(g | h) are multinomial dis- tributions. This matrix form has clear relevance to latent variable models. We apply the matrix decomposition algorithm to a co-occurrence ma- trix that can be estimated directly from a training set consisting of parse trees without latent anno- tations. The resulting parameter estimates give us significant leverage over the learning problem.</p><p>2) Optimization of a convex objective function using EM. We show that once the matrix decom- position step has been applied, parameter estima- tion of the L-PCFG can be reduced to a convex optimization problem that is easily solved by EM.</p><p>The algorithm provably learns the parameters of an L-PCFG (theorem 1), under an assumption that each latent state has at least one "pivot" feature. This assumption is similar to the "pivot word" as- sumption used by <ref type="bibr" target="#b2">Arora et al. (2013)</ref> and <ref type="bibr" target="#b1">Arora et al. (2012)</ref> in the context of learning topic models.</p><p>We describe experiments on learning of L- PCFGs, and also on learning of the latent-variable language model of <ref type="bibr" target="#b20">Saul and Pereira (1997)</ref>. A hy- brid method, which uses our algorithm as an ini- tializer for EM, performs at the same accuracy as EM, but requires significantly fewer iterations for convergence: for example in our L-PCFG exper- iments, it typically requires 2 EM iterations for convergence, as opposed to 20-40 EM iterations for initializers used in previous work.</p><p>While this paper's focus is on L-PCFGs, the techniques we describe are likely to be applicable to many other latent-variable models used in NLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Recently a number of researchers have developed provably correct algorithms for parameter esti- mation in latent variable models such as hidden Markov models, topic models, directed graphical models with latent variables, and so on ( <ref type="bibr" target="#b13">Hsu et al., 2009;</ref><ref type="bibr" target="#b3">Bailly et al., 2010;</ref><ref type="bibr" target="#b21">Siddiqi et al., 2010;</ref><ref type="bibr" target="#b18">Parikh et al., 2011;</ref><ref type="bibr" target="#b4">Balle et al., 2011;</ref><ref type="bibr" target="#b2">Arora et al., 2013;</ref><ref type="bibr" target="#b10">Dhillon et al., 2012;</ref><ref type="bibr" target="#b0">Anandkumar et al., 2012;</ref><ref type="bibr" target="#b1">Arora et al., 2012;</ref><ref type="bibr" target="#b2">Arora et al., 2013</ref>). Many of these algorithms have their roots in spec- tral methods such as canonical correlation analy- sis (CCA) <ref type="bibr" target="#b12">(Hotelling, 1936)</ref>, or higher-order ten- sor decompositions. Previous work <ref type="bibr" target="#b7">(Cohen et al., 2012;</ref><ref type="bibr" target="#b8">Cohen et al., 2013</ref>) has developed a spec- tral method for learning of L-PCFGs; this method learns parameters of the model up to an unknown linear transformation, which cancels in the inside- outside calculations for marginalization over la- tent states in the L-PCFG. The lack of direct pa- rameter estimates from this method leads to prob- lems with negative or unnormalized probablities; the method does not give parameters that are in- terpretable, or that can be used in conjunction with other algorithms, for example as an initializer for EM steps that refine the model. Our work is most directly related to the algo- rithm for parameter estimation in topic models de- scribed by <ref type="bibr" target="#b2">Arora et al. (2013)</ref>. This algorithm forms the core of the matrix decomposition algo- rithm described in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background</head><p>This section gives definitions and notation for L- PCFGs, taken from (Cohen et al., 2012).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">L-PCFGs: Basic Definitions</head><p>An L-PCFG is an 8-tuple (N , I, P, m, n, π, t, q) where: N is the set of non-terminal symbols in the grammar. I ⊂ N is a finite set of in-terminals. P ⊂ N is a finite set of pre-terminals. We as- sume that N = I ∪ P, and I ∩ P = ∅. Hence we have partitioned the set of non-terminals into two subsets.</p><p>[m] is the set of possible hidden states. <ref type="bibr">1</ref> [n] is the set of possible words. For all (a, b, c) ∈ I × N × N , and</p><formula xml:id="formula_0">(h 1 , h 2 , h 3 ) ∈ [m] × [m] × [m], we have a context-free rule a(h 1 ) → b(h 2 ) c(h 3 ). The rule has an associ- ated parameter t(a → b c, h 2 , h 3 | a, h 1 ). For all a ∈ P, h ∈ [m], x ∈ [n]</formula><p>, we have a context-free rule a(h) → x. The rule has an associated param- eter q(a → x | a, h). For all a ∈ I, h ∈ [m], π(a, h) is a parameter specifying the probability of a(h) being at the root of a tree.</p><p>A skeletal tree (s-tree) is a sequence of rules r 1 . . . r N where each r i is either of the form a → b c or a → x. The rule sequence forms a top-down, left-most derivation under a CFG with skeletal rules.</p><p>A full tree consists of an s-tree r 1 . . . r N , to- gether with values h 1 . . . h N . Each h i is the value for the hidden variable for the left-hand-side of rule r i . Each h i can take any value in <ref type="bibr">[m]</ref>.</p><p>For a given skeletal tree r 1 . . . r N , define a i to be the non-terminal on the left-hand-side of rule r i . For any i ∈ <ref type="bibr">[N ]</ref> such that r i is of the form a → b c, define h as the hidden state <ref type="bibr">1</ref> For any integer n, we use [n] to denote the set {1, 2, . . . n}.</p><p>value of the left and right child respectively. The model then defines a distribution as</p><formula xml:id="formula_1">p(r1 . . . rN , h1 . . . hN ) = π(a1, h1) i:a i ∈I t(ri, h (2) i , h (3) i | ai, hi) i:a i ∈P q(ri | ai, hi)</formula><p>The distribution over skeletal trees is</p><formula xml:id="formula_2">p(r 1 . . . r N ) = h 1 ...h N p(r 1 . . . r N , h 1 . . . h N ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Definition of Random Variables</head><p>Throughout this paper we will make reference to random variables derived from the distribution over full trees from an L-PCFG. These random variables are defined as follows. First, we select a random internal node, from a random tree, as follows: 1) Sample a full tree r 1 . . . r N , h 1 . . . h N from the PMF p(r 1 . . . r N , h 1 . . . h N ); 2) Choose a node i uniformly at random from <ref type="bibr">[N ]</ref>. We then give the following definition:</p><p>Definition 1 (Random Variables). If the rule r i for the node i is of the form a → b c, we define ran- dom variables as follows: R 1 is equal to the rule r i (e.g., NP → D N). A, B, C are the labels for node i, the left child of node i, and the right child of node i respectively. (E.g., A = NP, B = D, C = N.) T 1 is the inside tree rooted at node i. T 2 is the inside tree rooted at the left child of node i, and T 3 is the inside tree rooted at the right child of node i. O is the outside tree at node i. H 1 , H 2 , H 3 are the hid- den variables associated with node i, the left child of node i, and the right child of node i respectively. E is equal to 1 if node i is at the root of the tree (i.e., i = 1), 0 otherwise.</p><p>If the rule r i for the selected node i is of the form a → x, we have random vari- ables R 1 , T 1 , H 1 , A 1 , O, E as defined above, but H 2 , H 3 , T 2 , T 3 , B, and C are not defined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The Learning Algorithm for L-PCFGs</head><p>Our goal is to design a learning algorithm for L- PCFGs. The input to the algorithm will be a train- ing set consisting of skeletal trees, assumed to be sampled from some underlying L-PCFG. The out- put of the algorithm will be estimates for the π, t, and q parameters. The training set does not include values for the latent variables; this is the main challenge in learning.</p><p>This section focuses on an algorithm for recov- ery of the t parameters. A description of the al- gorithms for recovery of the π and q parameters is deferred until section 6.1 of this paper; these steps are straightforward once we have derived the method for the t parameters.</p><p>We describe an algorithm that correctly recov- ers the parameters of an L-PCFG as the size of the training set goes to infinity (this statement is made more precise in section 4.2). The algorithm relies on an assumption-the "pivot" assumption-that we now describe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Features, and the Pivot Assumption</head><p>We assume a function τ from inside trees to a fi- nite set F, and a function ρ that maps outside trees to a finite set G. The function τ (t) (ρ(o)) can be thought of as a function that maps an inside tree t (outside tree o) to an underlying feature. As one example, the function τ (t) might return the context-free rule at the root of the inside tree t; in this case the set F would be equal to the set of all context-free rules in the grammar. As an- other example, the function ρ(o) might return the context-free rule at the foot of the outside tree o.</p><p>In the more general case, we might have K sep- arate functions τ (k) (t) for k = 1 . . . K mapping inside trees to K separate features, and similarly we might have multiple features for outside trees. <ref type="bibr" target="#b8">Cohen et al. (2013)</ref> describe one such feature def- inition, where features track single context-free rules as well as larger fragments such as two or three-level sub-trees. For simplicity of presenta- tion we describe the case of single features τ (t) and ρ(o) for the majority of this paper. The exten- sion to multiple features is straightforward, and is discussed in section 6.2; the flexibility allowed by multiple features is important, and we use multiple features in our experiments.</p><p>Given functions τ and ρ, we define additional random variables:</p><formula xml:id="formula_3">F = τ (T 1 ), F 2 = τ (T 2 ), F 3 = τ (T 3 ), and G = ρ(O).</formula><p>We can now give the following assumption:</p><formula xml:id="formula_4">Assumption 1 (The Pivot Assumption)</formula><p>. Under the L-PCFG being learned, there exist values α &gt; 0 and β &gt; 0 such that for each non-terminal a, for each hidden state h ∈ [m], the following state- ments are true:</p><formula xml:id="formula_5">1) ∃f ∈ F such that P (F = f | H 1 = h, A = a) &gt; α and for all h = h, P (F = f | H 1 = h , A = a) = 0; 2) ∃g ∈ G such that P (G = g | H 1 = h, A = a) &gt; β and for all h = h, P (G = g | H 1 = h , A = a) = 0.</formula><p>This assumption is very similar to the assump- tion made by <ref type="bibr" target="#b1">Arora et al. (2012)</ref> in the con- text of learning topic models. It implies that for each (a, h) pair, there are inside and outside tree features-which following <ref type="bibr" target="#b1">Arora et al. (2012)</ref> we refer to as pivot features-that occur only 2 in the presence of latent-state value h. As in ( <ref type="bibr" target="#b1">Arora et al., 2012</ref>), the pivot features will give us consider- able leverage in learning of the model. <ref type="figure" target="#fig_2">Figure 1</ref> shows the learning algorithm for L- PCFGs. The algorithm consists of the following steps:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The Learning Algorithm</head><p>Step 0:</p><formula xml:id="formula_6">Calculate estimatesˆpestimatesˆ estimatesˆp(a → b c | a), ˆ p(g, f 2 , f 3 | a → b c) andˆpandˆ andˆp(f, g | a)</formula><p>. These estimates are easily calculated using counts taken from the training examples.</p><p>Step 1: Calculate valuesˆrvaluesˆ valuesˆr(f | h, a) andˆsandˆ andˆs(g | h, a); these are estimates of p(f | h 1 , a) and p(g | h 1 , a) respectively. This step is achieved us- ing a matrix decomposition algorithm, described in section 5 of this paper, on the matrixˆQmatrixˆ matrixˆQ a with</p><formula xml:id="formula_7">entries [ ˆ Q a ] f,g = ˆ p(f, g | a).</formula><p>Step 2: Use the EM algorithm to findˆtfindˆ findˆt values that maximize the objective function in Eq. 1 (see <ref type="figure" target="#fig_2">figure 1</ref>). Crucially, this is a convex optimization problem, and the EM algorithm will converge to the global maximum of this likelihood function.</p><p>Step 3: Rule estimates are calculated using an application of the laws of probability.</p><p>Before giving a theorem concerning correctness of the algorithm we introduce two assumptions: Assumption 2 (Strict Convexity). If we have the equalitiesˆsequalitiesˆ equalitiesˆs</p><formula xml:id="formula_8">(g | h 1 , a) = P (G = g | H 1 = h 1 , A = a), ˆ r(f 2 | h 2 , b) = P (F 2 = f 2 | H 2 = h 2 , B = b) andˆrandˆ andˆr(f 3 | h 3 , c) = P (F 3 = f 3 | H 2 = h 3 , C = c), then the function in Eq. 1 (fig- ure 1) is strictly concave.</formula><p>The function in Eq. 1 is always concave; this assumption adds the restriction that the function must be strictly concave-that is, it has a unique global maximum-in the case that thê r andˆsandˆ andˆs es- timates are exact estimates. Assumption 3 (Infinite Data). After running Step 0 of the algorithm we havê</p><formula xml:id="formula_9">havê p(a → b c | a) = p(a → b c | a) ˆ p(g, f 2 , f 3 | a → b c) = p(g, f 2 , f 3 | a → b c) ˆ p(f, g | a) = p(f, g | a)</formula><p>where p(. . .) is the probability under the underly- ing L-PCFG.</p><p>We use the term "infinite data" because under standard arguments, ˆ p(. . .) converges to p(. . .) as M goes to ∞.</p><p>The theorem is then as follows:</p><p>Theorem 1. Consider the algorithm in <ref type="figure" target="#fig_2">figure 1</ref>. Assume that assumptions 1-3 (the pivot, strong convexity, and infinite data assumptions) hold for the underlying L-PCFG. Then there is some per-</p><formula xml:id="formula_10">mutation σ : [m] → [m] such that for all a → b c, h 1 , h 2 , h 3 , ˆ t(a → b c, h 2 , h 3 | a → b c, h 1 ) = t(a → b c, σ(h 2 ), σ(h 3 ) | a → b c, σ(h 1 ))</formula><p>wherê t are the parameters in the output, and t are the parameters of the underlying L-PCFG.</p><p>This theorem states that under assumptions 1- 3, the algorithm correctly learns the t parameters of an L-PCFG, up to a permutation over the la- tent states defined by σ. Given the assumptions we have made, it is not possible to do better than re- covering the correct parameter values up to a per- mutation, due to symmetries in the model. As- suming that the π and q parameters are recovered in addition to the t parameters (see section 6.1), the resulting model will define exactly the same distribution over full trees as the underlying L- PCFG up to this permutation, and will define ex- actly the same distribution over skeletal trees, so in this sense the permutation is benign.</p><p>Proof of theorem 1: Under the assumptions of the theorem,</p><formula xml:id="formula_11">ˆ Q a f,g = p(f, g | a) = h p(h | a)p(f | h, a)p(g | h, a).</formula><p>Under the pivot assump- tion, and theorem 2 of section 5, step 1 (the matrix decomposition step) will therefore recover valuesˆr valuesˆ valuesˆr andˆsandˆ andˆs such thatˆrthatˆ thatˆr</p><formula xml:id="formula_12">(f | h, a) = p(f | σ(h), a) andˆs andˆ andˆs(g | h, a) = p(g | σ(h), a) for some permuta- tion σ : [m] → [m]</formula><p>. For simplicity, assume that σ(j) = j for all j ∈ [m] (the argument for other permutations involves a straightforward extension of the following argument). Under the assump- tions of the theorem,</p><formula xml:id="formula_13">ˆ p(g, f 2 , f 3 | a → b c) = p(g, f 2 , f 3 | a → b c), hence the function being optimized in Eq. 1 is equal to g,f 2 ,f 3 p(g, f 2 , f 3 | a → b c) log κ(g, f 2 , f 3 ) where κ(g, f 2 , f 3 ) = h 1 ,h 2 ,h 3 ˆ t(h 1 , h 2 , h 3 | a → b c) ×p(g | h 1 , a)p(f 2 | h 2 , b)p(f 3 | h 3 , c))</formula><p>Now consider the optimization problem in Eq. 1. By standard results for cross entropy, the maxi- mum of the function</p><formula xml:id="formula_14">g,f 2 ,f 3 p(g, f 2 , f 3 | a → b c) log q(g, f 2 , f 3 | a → b c)</formula><p>with respect to the q values is achieved at q(g, f 2 , f 3 | a → b c) = p(g, f 2 , f 3 | a → b c). In addition, under the assumptions of the L-PCFG,</p><formula xml:id="formula_15">p(g, f 2 , f 3 | a → b c) = h 1 ,h 2 ,h 3 (p(h 1 , h 2 , h 3 | a → b c) ×p(g | h 1 , a)p(f 2 | h 2 , b)p(f 3 | h 3 , c))</formula><p>Hence the maximum of Eq. 1 is achieved atˆt</p><formula xml:id="formula_16">atˆ atˆt(h 1 , h 2 , h 3 | a → b c) = p(h 1 , h 2 , h 3 | a → b c) (2) because this gives κ(g, f 2 , f 3 ) = p(g, f 2 , f 3 | a → b c).</formula><p>Under the strict convexity assump- tion the maximum of Eq. 1 is unique, hence thêthê t values must satisfy Eq. 2. Finally, it follows from Eq. 2, and the equalityˆpequalityˆ equalityˆp</p><formula xml:id="formula_17">(a → b c | a) = p(a → b c | a), that Step 3 of the algorithm givesˆt givesˆ givesˆt(a → b c, h 2 , h 3 | a, h 1 ) = t(a → b c, h 2 , h 3 | a, h 1 ).</formula><p>We can now see how the strict convexity as- sumption is needed. Without this assumption, there may be multiple settings forˆtforˆ forˆt that achieve</p><formula xml:id="formula_18">κ(g, f 2 , f 3 ) = p(g, f 2 , f 3 | a → b c); the valuesˆt valuesˆ valuesˆt(h 1 , h 2 , h 3 | a → b c) = p(h 1 , h 2 , h 3 | a → b c)</formula><p>will be included in this set of solutions, but other, inconsistent solutions will also be included.</p><p>As an extreme example of the failure of the strict convexity assumption, consider a feature- vector definition with |F| = |G| = 1. In this case the function in Eq. 1 reduces to log</p><formula xml:id="formula_19">h 1 ,h 2 ,h 3 ˆ t(h 1 , h 2 , h 3 | a → b c)</formula><p>. This func- tion has a maximum value of 0, achieved at all val- ues ofˆtofˆ ofˆt. Intuitively, this definition of inside and outside tree features loses all information about the latent states, and does not allow successful learning of the underlying L-PCFG. More gener- ally, it is clear that the strict convexity assumption will depend directly on the choice of feature func- tions τ (t) and ρ(o).</p><p>Remark: The infinite data assumption, and sample complexity. The infinite data assump- tion deserves more discussion. It is clearly a strong assumption that there is sufficient data for</p><note type="other">Input: A set of M skeletal trees sampled from some underlying L-PCFG. The count[. . .] function counts the number of times that event .</note><p>. . occurs in the training sample. For example, count <ref type="bibr">[A = a]</ref> is the number of times random variable A takes value a in the training sample.</p><p>Step 0: Calculate the following estimates from the training samples:</p><formula xml:id="formula_20">• ˆ p(a → b c | a) = count[R1 = a → b c]/count[A = a] • ˆ p(g, f2, f3 | a → b c) = count[G = g, F2 = f2, F3 = f3, R1 = a → b c]/count[R1 = a → b c] • ˆ p(f, g | a) = count[F = f, G = g, A = a]/count[A = a]</formula><p>• ∀a ∈ I, define a matrixˆQmatrixˆ</p><formula xml:id="formula_21">matrixˆQ a ∈ R d×d where d = |F| and d = |G| as [ ˆ Q a ] f,g = ˆ p(f, g | a).</formula><p>Step 1: ∀a ∈ I, use the algorithm in <ref type="figure">figure 2</ref> with inputˆQinputˆ inputˆQ a to derive estimatesˆrestimatesˆ estimatesˆr(f | h, a) andˆsandˆ andˆs(g | h, a).</p><p>Remark: These quantities are estimates of P (F1 = f | H1 = h, A = a) and P (G = g | H = h, A = a) respectively. Note that under the independence assumptions of the L-PCFG,</p><formula xml:id="formula_22">P (F1 = f | H1 = h, A = a) = P (F2 = f | H2 = h, A2 = a) = P (F3 = f | H3 = h, A3 = a).</formula><p>Step 2: For each rule a → b c, findˆtfindˆ findˆt(h1, h2, h3 | a → b c) values that maximize</p><formula xml:id="formula_23">g,f 2 ,f 3 ˆ p(g, f2, f3 | a → b c) log h 1 ,h 2 ,h 3 ˆ t(h1, h2, h3 | a → b c)ˆ s(g | h1, a)ˆ r(f2 | h2, b)ˆ r(f3 | h3, c)<label>(1)</label></formula><p>under the constraintsˆtconstraintsˆ constraintsˆt(h1, h2, h3 | a → b c) ≥ 0, and</p><formula xml:id="formula_24">h 1 ,h 2 ,h 3 ˆ t(h1, h2, h3 | a → b c) = 1.</formula><p>Remark: the function in Eq. 1 is concave in the valuesˆtvaluesˆ valuesˆt being optimized over. We use the EM algorithm, which converges to a global optimum.</p><p>Step 3: ∀a → b c, h1, h2, h3, calculate rule parameters as follows:  the estimatesˆpestimatesˆ estimatesˆp in assumption 3 to have converged to the correct underlying values. A more detailed analysis of the algorithm would derive sample complexity results, giving guarantees on the sam- ple size M required to reach a level of accuracy in the estimates, with probability at least 1 − δ, as a function of , δ, and other relevant quantities such as n, d, d , m, α, β and so on.</p><formula xml:id="formula_25">ˆ t(a → b c, h2, h3 | a, h1) = ˆ t(a → b c, h1, h2, h3 | a)/ b,c,h 2 ,h 3 ˆ t(a → b c, h1, h2, h3 | a) wherê t(a → b c, h1, h2, h3 | a) = ˆ p(a → b c | a) × ˆ t(h1, h2, h3 | a → b c).</formula><p>In spite of the strength of the infinite data as- sumption, we stress the importance of this result as a guarantee for the algorithm. First, a guar- antee of correct parameter values in the limit of infinite data is typically the starting point for a sample complexity result (see for example ( <ref type="bibr" target="#b13">Hsu et al., 2009;</ref><ref type="bibr" target="#b0">Anandkumar et al., 2012)</ref>). Sec- ond, our sense is that a sample complexity result can be derived for our algorithm using standard methods: specifically, the analysis in ( <ref type="bibr" target="#b1">Arora et al., 2012)</ref> gives one set of guarantees; the remain- ing optimization problems we solve are convex maximum-likelihood problems, which are also relatively easy to analyze. Note that several pieces of previous work on spectral methods for latent- variable models focus on algorithms that are cor- rect under the infinite data assumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">The Matrix Decomposition Algorithm</head><p>This section describes the matrix decomposition algorithm used in Step 1 of the learning algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Problem Setting</head><p>Our goal will be to solve the following matrix de- composition problem:</p><p>Matrix Decomposition Problem (MDP) 1. De- sign an algorithm with the following inputs, as- sumptions, and outputs: Inputs: Integers m, d and d , and a matrix Q ∈ R d×d such that Q f,g = m h=1 π(h)r(f | h)s(g | h) for some unknown parameters π(h), r(f | h) and s(g | h) satisfying:</p><formula xml:id="formula_26">1) π(h) ≥ 0, m h=1 π(h) = 1; 2) r(f | h) ≥ 0, d f =1 r(f | h) = 1; 3) s(g | h) ≥ 0, d g=1 s(g | h) = 1. Assumptions:</formula><p>There are values α &gt; 0 and β &gt; 0 such that the r parameters of the model are α- separable, and the s parameters of the model are β-separable.</p><formula xml:id="formula_27">Outputs: EstimatesˆπEstimatesˆ Estimatesˆπ(h), ˆ r(f | h) andˆsandˆ andˆs(g | h) such that there is some permutation σ : [m] → [m] such that ∀h, ˆ π(h) = π(σ(h)), ∀f, h, ˆ r(f | h) = r(f | σ(h)), and ∀g, h, ˆ s(g | h) = s(g | σ(h)).</formula><p>The definition of α-separability is as follows (β- separability for s(g | h) is analogous):</p><formula xml:id="formula_28">Definition 2 (α-separability). The parameters r(f | h) are α-separable if for all h ∈ [m], there is some j ∈ [d] such that: 1) r(j | h) ≥ α; and 2) r(j | h ) = 0 for h = h.</formula><p>This matrix decomposition problem has clear relevance to problems in learning of latent- variable models, and in particular is a core step of the algorithm in <ref type="figure" target="#fig_2">figure 1</ref>. When given a matrixˆQmatrixˆ matrixˆQ a with entriesˆQentriesˆ entriesˆQ a f,g = h p(h | a)p(f | h, a)p(g | h, a), where p(. . .) refers to a distribution derived from an underlying L-PCFG which satisfies the pivot assumption, the method will recover the val- ues for p(h | a), p(f | h, a) and p(g | h, a) up to a permutation over the latent states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">The Algorithm of Arora et al. (2013)</head><p>This section describes a variant of the algorithm of <ref type="bibr" target="#b2">Arora et al. (2013)</ref>, which is used as a component of our algorithm for MDP 1. One of the proper- ties of this algorithm is that it solves the following problem:</p><p>Matrix Decomposition Problem (MDP) 2. De- sign an algorithm with the following inputs, as- sumptions, and outputs: Inputs: Same as matrix decomposition problem 1. Assumptions: The parameters r(f | h) of the model are α-separable for some value α &gt; 0.</p><formula xml:id="formula_29">Outputs: EstimatesˆπEstimatesˆ Estimatesˆπ(h) andˆrandˆ andˆr(f | h) such that ∃σ : [m] → [m] such that ∀h, ˆ π(h) = π(σ(h)), ∀f, h, ˆ r(f | h) = r(f | σ(h)).</formula><p>This is identical to Matrix Decomposition Prob- lem 1, but without the requirement that the values s(g | h) are returned by the algorithm. Thus an algorithm that solves MDP 2 in some sense solves "one half" of MDP 1.</p><p>For completeness we give a sketch of the algo- rithm that we use; it is inspired by the algorithm of <ref type="bibr" target="#b1">Arora et al. (2012)</ref>, but has some important dif- ferences. The algorithm is as follows:</p><p>Step 1: Derive a function φ : [d ] → R l that maps each integer g ∈ [d ] to a representation φ(g) ∈ R l . The integer l is typically much smaller than d , implying that the representation is of low dimension. <ref type="bibr" target="#b1">Arora et al. (2012)</ref> derive φ as a ran- dom projection with a carefully chosen dimension l. In our experiments, we use canonical correlation analysis (CCA) on the matrix Q to give a represen- tation φ(g) ∈ R l where l = m.</p><p>Step 2:</p><formula xml:id="formula_30">For each f ∈ [d], calculate v f = E[φ(g) | f ] = d g=1 p(g | f )φ(g)</formula><p>where</p><formula xml:id="formula_31">p(g | f ) = Q f,g / g Q f,g . It follows that v f = d g=1 m h=1 p(h | f )p(g | h)φ(g) = m h=1 p(h | f )w h</formula><p>where w h ∈ R l is equal to d g=1 p(g | h)φ(g). Hence the v f vectors lie in the convex hull of a set of vectors w 1 . . . w m ∈ R l . Crucially, for any pivot word f for latent state h, we have p(h | f ) = 1, hence v f = w h . Thus by the pivot assump- tion, the set of points v 1 . . . v d includes the ver- tices of the convex hull. Each point v j is a convex combination of the vertices w 1 . . . w m , where the weights in this combination are equal to p(h | j).</p><p>Step 3: Use the FastAnchorWords algo- rithm of ( <ref type="bibr" target="#b1">Arora et al., 2012)</ref> to identify m vectors v s 1 , v s 2 , . . . v sm . The FastAnchorWords algo- rithm has the guarantee that there is a permutation σ : [m] → [m] such that v s i = w σ(i) for all i. This algorithm recovers the vertices of the convex hull described in step 2, using a method that greedily picks points that are as far as possible from the subspace spanned by previously picked points.</p><p>Step 4: For each f ∈ [d] solve the problem arg min</p><formula xml:id="formula_32">γ 1 ,γ 2 ,...,γm || h γ h v s h − v f || 2</formula><p>subject to γ h ≥ 0 and h γ h = 1. We use the algorithm of <ref type="bibr" target="#b11">(Frank and Wolfe, 1956;</ref><ref type="bibr" target="#b6">Clarkson, 2010)</ref> for this purpose. Set q(h | f ) = γ h .</p><p>Return the final quantities:</p><formula xml:id="formula_33">ˆ π(h) = f p(f )q(h|f ) ˆ r(f |h) = p(f )q(h|f ) f p(f )q(h|f )</formula><p>where p(f ) = g Q f,g . <ref type="figure">Figure 2</ref> shows an algorithm that solves MDP 1. In steps 1 and 2 of the algorithm, the algorithm of section 5.2 is used to recover estimatesˆrestimatesˆ estimatesˆr(f | h) andˆsandˆ andˆs(g | h). These distributions are equal to p(f | h) and p(g | h) up to permutations σ and σ of the latent states respectively; unfortunately there is no guarantee that σ and σ are the same permutation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">An Algorithm for MDP 1</head><p>Step 3 estimates parameters t(h | h) that effectively map the permutation implied byˆr byˆ byˆr(f | h) to the permutation implied byˆsbyˆ byˆs(g | h); the latter distribution is recalculated as</p><formula xml:id="formula_34">h ˆ t(h | h)ˆ s(g | h ).</formula><p>We now state the following theorem:</p><p>Theorem 2. The algorithm in <ref type="figure">figure 2</ref> solves Ma- trix Decomposition Problem 1.</p><p>Proof: See the supplementary material.</p><p>Remark: A natural alternative to the algorithm presented would be to run Step 1 of the original algorithm, but to replace steps 2 and 3 with a step that findsˆsfindsˆ findsˆs(g | h) values that maximize</p><formula xml:id="formula_35">f,g Q f,g log h ˆ r(h | f )ˆ s(g | h)</formula><p>This is again a convex optimization problem. We may explore this algorithm in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Additional Details of the Algorithm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Recovery of the π and q Parameters</head><p>The recovery of the π and q parameters relies on the following additional (but benign) assumptions on the functions τ and ρ:</p><p>1) For any inside tree t such that t is a unary rule of the form a → x, the function τ is defined as τ (t) = t. <ref type="bibr">3</ref> 2) The set of outside tree features G contains a special symbol 2, and g(o) = 2 if and only if the outside tree o is derived from a non-terminal node at the root of a skeletal tree. decomposition step) can be extended to provide estimatesˆrestimatesˆ estimatesˆr (k) (f (k) | h, a) andˆsandˆ andˆs (l) (g (l) | h, a). In brief, this involves running CCA on a matrix E[φ(T )(ψ(O)) | A = a] where φ and ψ are in- side and outside binary feature vectors derived di- rectly from the inside and outside features, using a one-hot representation. CCA results in a low- dimensional representation that can be used in the steps described in section 5.2; the remainder of the algorithm is the same. In practice, the addition of multiple features may lead to better CCA repre- sentations.</p><p>Next, we modify the objective function in Eq. 1 to be the following:</p><formula xml:id="formula_36">i,j,k g i ,f j 2 ,f k 3 p(g i , f j 2 , f k 3 | a → b c) log κ i,j,k (g i , f j 2 , f k 3 )</formula><p>where</p><formula xml:id="formula_37">κ i,j,k (g i , f j 2 , f k 3 ) = h 1 ,h 2 ,h 3 ˆ t(h 1 , h 2 , h 3 | a → b c) ×ˆs×ˆs i (g i | h 1 , a)ˆ r j (f j 2 | h 2 , b)ˆ r k (f k 3 | h 3 , c)</formula><p>Thus the new objective function consists of a sum of L×M 2 terms, each corresponding to a different combination of inside and outside features. The function remains concave.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Use as an Initializer for EM</head><p>The learning algorithm for L-PCFGs can be used as an initializer for the EM algorithm for L- PCFGs. Two-step estimation methods such as these are well known in statistics; there are guar- antees for example that if the first estimator is con- sistent, and the second step finds the closest local maxima of the likelihood function, then the result- ing estimator is both consistent and efficient (in terms of number of samples required). See for example page 453 or Theorem 4.3 (page 454) of ( <ref type="bibr" target="#b15">Lehmann and Casella, 1998</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experiments on Parsing</head><p>This section describes parsing experiments using the learning algorithm for L-PCFGs. We use the Penn WSJ treebank ( <ref type="bibr" target="#b16">Marcus et al., 1993</ref>) for our experiments. Sections 2-21 were used as training data, and sections 0 and 22 were used as develop- ment data. Section 23 was used as the test set. The experimental setup is the same as described by <ref type="bibr" target="#b8">Cohen et al. (2013)</ref>. The trees are bina- rized ( <ref type="bibr" target="#b19">Petrov et al., 2006</ref>) and for the EM algo- rithm we use the initialization method described in <ref type="bibr" target="#b17">Matsuzaki et al. (2005)</ref>. For the pivot algo- rithm we use multiple features τ 1 (t) . . . τ K (t) and ρ 1 (o) . . . ρ L (o) over inside and outside trees, us- ing the features described by <ref type="bibr" target="#b8">Cohen et al. (2013)</ref>. <ref type="table">Table 1</ref> gives the F1 accuracy on the develop- ment and test sets for the following methods: For the EM and Pivot+EM algorithms, we give the number of iterations of EM required to reach optimal performance on the development data. The results show that the EM, Spectral, and Pivot+EM algorithms all perform at a very similar level of accuracy. The Pivot+EM results show that very few EM iterations-just 2 iterations in most conditions-are required to reach optimal perfor- mance when the Pivot model is used as an ini- tializer for EM. The Pivot results lag behind the Pivot+EM results by around 2-3%, but they are close enough to optimality to require very few EM iterations when used as an initializer.</p><formula xml:id="formula_38">EM</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Experiments on the Saul and Pereira (1997) Model for Language Modeling</head><p>We now describe a second set of experiments, on the Saul and Pereira (1997) model for language modeling. Define V to be the set of words in the vocabulary. For any w 1 , w 2 ∈ V , the Saul and Pereira (1997) model then defines p(w 2 | w 1 ) = m h=1 r(h | w 1 )s(w 2 | h) where r(h | w 1 ) and <ref type="table">Table 2</ref>: Language model perplexity with the Brown corpus and the Gigaword corpus (New York Times portion) for the second half of the development set, and the test set. With EM and Pivot+EM, the number of iterations for EM to reach convergence is given below the perplexity. The best result for each column (for each m value) is in bold. The "test" column gives perplexity results on the test set. Each perplexity calculation on the test set is done using the best model on the development set. bi-KN+int and tri-KN+int are bigram and trigram Kneser-Ney interpolated models <ref type="bibr" target="#b14">(Kneser and Ney, 1995)</ref>, using the SRILM toolkit.</p><p>s(w 2 | h) are parameters of the approach. The conventional approach to estimation of the param- eters r(h | w 1 ) and s(w 2 | h) from a corpus is to use the EM algorithm. In this section we com- pare the EM algorithm to a pivot-based method. It is straightforward to represent this model as an L-PCFG, and hence to use our implementation for estimation.</p><p>In this special case, the L-PCFG learning al- gorithm is equivalent to a simple algorithm, with the following steps: 1) define the matrix Q with entries Q w 1 ,w 2 = count(w 1 , w 2 )/N where count(w 1 , w 2 ) is the number of times that bi- gram (w 1 , w 2 ) is seen in the data, and N = w 1 ,w 2 count(w 1 , w 2 ). Run the algorithm of sec- tion 5.2 on Q to recover estimatesˆsestimatesˆ estimatesˆs(w 2 | h); 2) estimatê r(h | w 1 ) using the EM algorithm to op- timize the function w 1 ,w 2 Q w 1 ,w 2 log h ˆ r(h | w 1 )ˆ s(w 2 | h) with respect to thê r parameters; this function is concave in these parameters.</p><p>We performed the language modeling experi- ments for a number of reasons. First, because in this case the L-PCFG algorithm reduces to a sim- ple algorithm, it allows us to evaluate the core ideas in the method very directly. Second, it al- lows us to test the pivot method on the very large datasets that are available for language modeling.</p><p>We use two corpora for our experiments. The first is the Brown corpus, as used by <ref type="bibr" target="#b5">Bengio et al. (2006)</ref> in language modeling experiments. Fol- lowing <ref type="bibr" target="#b5">Bengio et al. (2006)</ref>, we use the first 800K words for training (and replace all words that ap- pear once with an UNK token), the next 200K words for development, and the remaining data (165,171 tokens) as a test set. The size of the vocabulary is 24,488 words. The second corpus we use is the New York Times portion of the Gi- gaword corpus. Here, the training set consists of 1.31 billion tokens. We use 159 million tokens for development set and 156 million tokens for test. All words that appeared less than 20 times in the training set were replaced with the UNK token. The size of the vocabulary is 235,223 words. Un- known words in test data are ignored when calcu- lating perplexity (this is the standard set-up in the SRILM toolkit).</p><p>In our experiments we use the first half of each development set to optimize the number of itera- tions of the EM or Pivot+EM algorithms. As be- fore, Pivot+EM uses 1 or more EM steps with pa- rameter initialization from the Pivot method. <ref type="table">Table 2</ref> gives perplexity results for the differ- ent algorithms. As in the parsing experiments, the Pivot method alone performs worse than EM, but the Pivot+EM method gives results that are com- petitive with EM. The Pivot+EM method requires fewer iterations of EM than the EM algorithm. On the Brown corpus the difference is quite dra- matic, with only 1 or 2 iterations required, as op- posed to 10 or more for EM. For the NYT cor- pus the Pivot+EM method requires more iterations (around 10 or 20), but still requires significantly fewer iterations than the EM algorithm.</p><p>On the Gigaword corpus, with m = 256, EM takes 12h57m (32 iterations at 24m18s per itera- tion) compared to 1h50m for the Pivot method. On Brown, EM takes 1m47s (8 iterations) compared to 5m44s for the Pivot method. Both the EM and pivot algorithm implementations were highly op- timized, and written in Matlab. Results at other values of m are similar. From these results the Pivot method appears to become more competitive speed-wise as the data size increases (the Giga- word corpus is more than 1,300 times larger than the Brown corpus).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>We have described a new algorithm for parameter estimation in L-PCFGs. The algorithm is provably correct, and performs well in practice when used in conjunction with EM.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Output:</head><label></label><figDesc>Parameter estimatesˆtestimatesˆ estimatesˆt(a → b c, h2, h3 | a, h1) for all rules a → b c, for all (h1, h2, h3) ∈ [m] × [m] × [m].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The learning algorithm for the t(a → b c, h1, h2, h3 | a) parameters of an L-PCFG.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>:</head><label></label><figDesc>The EM algorithm as used by Matsuzaki et al. (2005) and Petrov et al. (2006). Spectral: The spectral algorithm of Cohen et al. (2012) and Cohen et al. (2013). Pivot: The algorithm described in this paper. Pivot+EM: The algorithm described in this pa- per, followed by 1 or more iterations of the EM algorithm with parameters initialized by the pivot algorithm. (See section 6.3.)</figDesc></figure>

			<note place="foot" n="2"> The requirements P (F = f | H1 = h , A = a) = 0 and P (G = g | H1 = h , A = a) = 0 are almost certainly overly strict; in theory and practice these probabilities should be able to take small but strictly positive values.</note>

			<note place="foot" n="3"> Note that if other features on unary rules are desired, we can use multiple feature functions τ 1 (t). .. τ K (t), where τ 1 (t) = t for inside trees, and the functions τ 2 (t). .. τ K (t) define other features.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Assumptions: As in Matrix Decomposition Problem 1.</head><p>Algorithm:</p><p>Step 1. Run the algorithm of section 5.2 on the matrix Q to derive estimatesˆrestimatesˆ estimatesˆr(f | h) andˆπandˆ andˆπ(h). Note that under the guarantees of the algorithm, there is some permutation σ such thatˆrthatˆ thatˆr(f | h) = r(f | σ(h)). Definê</p><p>Step 2. Run the algorithm of section 5.2 on the matrix Q to derive estimatesˆsestimatesˆ estimatesˆs(g | h). Under the guarantees of the algorithm, there is some permutation σ such thatˆsthatˆ thatˆs(g | h) = s(g | σ (h)). Note however that it is not necessarily the case that σ = σ .</p><p>Step</p><p>Remark: the function in Eq. 3 is concave in thê t parame- ters. We use the EM algorithm to find a global optimum.</p><p>Step 4. Return the following values:</p><p>• ˆ π(h) for all h, as an estimate of π(σ(h)) for some permutation σ.</p><p>• ˆ r(f | h) for all f, h as an estimate of r(f | σ(h)) for the same permutation σ.</p><p>•</p><p>as an estimate of s(f | σ(h)) for the same permutation σ. </p><p>Note thatˆp thatˆ thatˆp(h | a) can be derived from the matrix decompo- sition step when applied tô Q a , andˆpandˆ andˆp(a) is easily recovered from the training examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Extension to Include Multiple Features</head><p>We now describe an extension to allow K separate functions τ (k) (t) for k = 1 . . . K mapping inside trees to features, and L feature functions ρ (l) (o) for l = 1 . . . L over outside trees.</p><p>The algorithm in <ref type="figure">figure 1</ref> can be extended as follows. First, Step 1 of the algorithm <ref type="table">(the matrix NYT  m  2  4  8  16  32  128 256 test  2  4  8  16  32  128 256 test   EM  737  14  599  14  488  19  468  12  430  10  388  9  365  8  364  926  36  733  39  562  42  420  33  361  38  284  35  265  32  267   bi-KN +int.  408  415  271  279  tri-KN+int.  386  394  150  158  pivot  852 718 605 559 537 426 597 560  1227 1264 896 717 738 782 886 715   pivot+EM  758  2  582  3  502  2  425  1  374  1  310  1  327  1  357  898  20  754  14  553  13  441  15  394  10  279  19  292  12  281</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Telgarsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1210.7559</idno>
		<title level="m">Tensor decompositions for learning latent-variable models</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning topic models-going beyond SVD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of FOCS</title>
		<meeting>FOCS</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A practical algorithm for topic modeling with provable guarantees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Halpern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A spectral approach for probabilistic grammatical inference on trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bailly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Habrar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Denis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ALT</title>
		<meeting>ALT</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A spectral learning algorithm for finite state transducers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Balle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Carreras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECML</title>
		<meeting>ECML</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural probabilistic language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-S</forename><surname>Senécal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-L</forename><surname>Gauvain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Innovations in Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="137" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Coresets, sparse greedy approximation, and the Frank-Wolfe algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Clarkson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Algorithms (TALG)</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">63</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spectral learning of latent-variable PCFGs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Experiments with spectral learning of latent-variable PCFGs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Maximum likelihood estimation from incomplete data via the EM algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spectral dependency parsing with latent variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rodu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An algorithm for quadratic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wolfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval research logistics quarterly</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="95" to="110" />
			<date type="published" when="1956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Relations between two sets of variates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hotelling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3/4</biblScope>
			<biblScope unit="page" from="321" to="377" />
			<date type="published" when="1936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A spectral algorithm for learning hidden Markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLT</title>
		<meeting>COLT</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improved backing-off for m-gram language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kneser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Casella</surname></persName>
		</author>
		<title level="m">Theory of Point Estimation</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
	<note>Second edition</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of English: The Penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Probabilistic CFG with latent annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matsuzaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A spectral algorithm for latent tree graphical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning accurate, compact, and interpretable tree annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Thibaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING-ACL</title>
		<meeting>COLING-ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Aggregate and mixedorder markov models for statistical language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Saul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reducedrank hidden markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Siddiqi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boots</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="741" to="748" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
