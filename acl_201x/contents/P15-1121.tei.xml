<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:34+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Machine Comprehension with Discourse Relations</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narasimhan</forename><surname>Karthik</surname></persName>
							<email>karthikn@csail.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">CSAIL</orgName>
								<address>
									<region>MIT</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>MIT</roleName><surname>Csail</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">CSAIL</orgName>
								<address>
									<region>MIT</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
							<email>regina@csail.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">CSAIL</orgName>
								<address>
									<region>MIT</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Machine Comprehension with Discourse Relations</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1253" to="1262"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper proposes a novel approach for incorporating discourse information into machine comprehension applications. Traditionally, such information is computed using off-the-shelf discourse analyz-ers. This design provides limited opportunities for guiding the discourse parser based on the requirements of the target task. In contrast, our model induces relations between sentences while optimizing a task-specific objective. This approach enables the model to benefit from discourse information without relying on explicit annotations of discourse structure during training. The model jointly identifies relevant sentences, establishes relations between them and predicts an answer. We implement this idea in a discrim-inative framework with hidden variables that capture relevant sentences and relations unobserved during training. Our experiments demonstrate that the discourse aware model outperforms state-of-the-art machine comprehension systems. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The task of machine comprehension concerns the automatic extraction of answers from a given pas- sage. Often, the relevant information required to answer a question is distributed across multiple sentences. Understanding the relation(s) between these sentences is key to finding the correct an- swer. Consider the example in <ref type="figure" target="#fig_0">fig. 1</ref>. To answer the question about why Sally put on her shoes , we need to infer that She put on her shoes and She went outside to walk are connected by a causality relation.</p><p>Sally liked going outside. She put on her shoes. She went outside to walk. <ref type="bibr">[...]</ref> Missy the cat meowed to Sally. Sally waved to Missy the cat.</p><p>[...] Sally hears her name. "Sally, Sally, come home", Sally's mom calls out. Sally runs home to her Mom. Sally liked going outside.</p><p>Why did Sally put on her shoes? A) To wave to Missy the cat B) To hear her name C) Because she wanted to go outside D) To come home Prior work has demonstrated the value of dis- course relations in related applications such as question answering <ref type="bibr" target="#b9">(Jansen et al., 2014</ref>). Tradi- tionally, however, these approaches rely on out- puts from off-the-shelf discourse analyzers, us- ing them as features for target applications. Such pipeline designs provide limited opportunities for guiding the discourse parser based on the require- ments of the end task. Given a wide spectrum of discourse frameworks <ref type="bibr" target="#b12">(Mann and Thompson, 1988;</ref><ref type="bibr" target="#b16">Prasad et al., 2008;</ref><ref type="bibr" target="#b20">Wolf and Gibson, 2005</ref>), it is not clear a priori what the optimal set of dis- course annotations is for the task. Moreover, a generic discourse parser may introduce additional errors due to the mismatch between its training corpus and a dataset used in an application. In fact, the largest discourse treebanks are based on news- paper corpora <ref type="bibr" target="#b16">(Prasad et al., 2008;</ref><ref type="bibr" target="#b3">Carlson et al., 2002</ref>), which differ significantly in style from text used in machine comprehension corpora <ref type="bibr" target="#b17">(Richardson et al., 2013)</ref>.</p><p>In this paper, we propose a novel approach for incorporating discourse structure into machine comprehension applications. Rather than using a standalone parser that is trained on external su- pervised data to annotate discourse relations, the model induces relations between sentences while optimizing a task-specific objective. This design biases the model to learn relations at a granu- larity optimized for the machine comprehension task. In contrast to a generic discourse analyzer, our method can also utilize additional information available in the machine comprehension context. For instance, question types provide valuable cues for determining discourse relations, and thus can facilitate learning.</p><p>We implement these ideas in a discrimina- tive log-linear model with hidden variables. The model jointly identifies relevant sentences, estab- lishes relations between them and predicts an an- swer. Since the same set of sentences can give rise to multiple questions, we do not limit the model to a single discourse relation, but rather model a distribution over possible relations. During train- ing, we only have access to questions and gold answers. Since relevant sentences and their rela- tions are not known, we model them as hidden variables. To guide the model towards linguisti- cally plausible discourse relations, we add a few seed markers that are typical of each relation. The model predicts relations not only based on the sen- tences, but also incorporates information about the question. By decomposing the dependencies be- tween model components, we can effectively train the model using a standard gradient descent ap- proach.</p><p>We evaluate our model using a recently re- leased machine comprehension dataset ( <ref type="bibr" target="#b17">Richardson et al., 2013)</ref>. In this corpus, roughly half of the questions rely on multiple sentences in the pas- sage to generate the correct answer. For baselines, we use the best published results on this dataset. Our results demonstrate that our relation-aware model outperforms the individual baselines by up to 5.7% and rivals the performance of a state-of- the-art combination system. Moreover, we show that the discourse relations it predicts for sentence pairs exhibit considerable overlap with relations identified by human annotators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Machine Comprehension Following traditional methods in question answering, most approaches to machine comprehension focus on analyzing the connection between the question, candidate an- swer and the document. For instance, <ref type="bibr" target="#b17">Richardson et al. (2013)</ref> show that using word overlap alone provides a good starting point for the task. Using textual entailment output ( <ref type="bibr" target="#b18">Stern and Dagan, 2011)</ref> and embedding-based representations <ref type="bibr" target="#b8">(Iyyer et al., 2014</ref>) further improves the result. Even though these methods operate at a paragraph level, they do not model relations between sentences. For in- stance, in their work on factoid question answer- ing using recursive neural networks, <ref type="bibr" target="#b8">Iyyer et al. (2014)</ref> average the sentence vectors element-wise when considering more than one sentence.</p><p>A notable exception is the approach proposed by <ref type="bibr" target="#b0">Berant et al. (2014)</ref>. Their approach builds on a semantic representation that encodes a number of inter-event relations, such as cause and enable. These relations straddle the boundary between dis- course and semantic connections, since most of them are specific to the domain of interest. These relations are identified in a supervised fashion us- ing a significant amount of manual annotations. In contrast, we are interested in extracting discourse relations with minimal additional annotation, re- lying primarily on the available question-answer pairs. As a result, we look at a smaller set of ba- sic relations that can be learned without explicit annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discourse analysis for Question Answering</head><p>Prior work has established the value of domain- independent discourse relations in question an- swering applications <ref type="bibr" target="#b19">(Verberne et al., 2007;</ref><ref type="bibr" target="#b9">Jansen et al., 2014;</ref><ref type="bibr" target="#b4">Chai and Jin, 2004</ref>). For instance, <ref type="bibr" target="#b19">Verberne et al. (2007)</ref> propose an answer extrac- tion technique that treats question topics and an- swers as siblings in a Rhetorical Structure The- ory (RST) tree, significantly improving perfor- mance on why-questions. <ref type="bibr" target="#b4">Chai and Jin (2004)</ref> ar- gue that incorporating discourse processing can significantly help context question answering, a task in which subsequent questions may refer to entities or concepts in previous questions. <ref type="bibr" target="#b9">Jansen et al. (2014)</ref> utilize discourse information to im- prove reranking of human-written answers for non-factoid questions. They experiment with both shallow discourse markers and deep representa- tions based on RST parsers to rerank answers for how and why-type questions <ref type="bibr">3</ref> .</p><p>While the above approaches vary greatly in terms of their design, they incorporate discourse information in a similar fashion, adding it as fea- tures to a supervised model. The discourse in- formation is typically computed using discourse parsers based on frameworks like RST <ref type="bibr" target="#b7">(Feng and Hirst, 2014</ref>) or PDTB ( <ref type="bibr" target="#b11">Lin et al., 2014</ref>), trained using supervised data. In contrast, our goal is to learn discourse relations driven by the task objec- tive. The set of these relations does not capture the richness of discourse representations consid- ered in traditional discourse theories <ref type="bibr" target="#b12">(Mann and Thompson, 1988;</ref><ref type="bibr" target="#b16">Prasad et al., 2008</ref>). However, we learn them without explicit annotations of dis- course structure, and demonstrate that they im- prove model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task Description and Approach</head><p>We focus on the task of machine comprehension, which involves answering questions based on a passage of text. Concretely, let us consider a pas- sage p i = {Z i , Q i } to consist of a set of sentences Z i = {z in } and a set of questions Q i = {q ij }, with each question also having a set of answer choices A ij = {a ijk }. We denote the correct an- swer choice for a question q ij as a * ij . Given a set of training passages P train with questions annotated with the correct answer choice, the task is to be able to answer questions accurately in a different set of passages P test . <ref type="figure" target="#fig_0">Figure 1</ref> shows an example of a passage, along with a question and answer choices. The only (weak) source of supervision available is the cor- rect answer choice for each question in training.</p><p>We do not use any extra annotations during train- ing. We propose joint probabilistic models to ad- dress this task, that can learn to identify single or multiple relevant sentences given a question, es- tablish a relation between them and score the an- swer choices.</p><p>We explore three different discriminative mod- els, ranging from a simple one that answers ques- tions using a single sentence in the passage, to one that infers relations between multiple sentences to score answer choices. We defer the description of the features used in our models to section 3.1.</p><p>Model 1 In our first model, we assume that each question can be answered using a single sentence from the passage. Treating the sentence as a hid- den variable, we define a joint model for a sen- tence z ∈ Z and an answer choice a ∈ A j , given a question q j .</p><formula xml:id="formula_0">(1) P (a, z | q j ) = P (z | q j ) · P (a | z, q j )</formula><p>We define the joint probability as a product of two distributions. The first is the conditional distribu- tion of sentences in the paragraph given the ques- tion. This is to help identify the right sentence re- quired to answer the question. The second compo- nent models the conditional probability of an an- swer given the question q and a sentence z. For both component probabilities, we use distributions from the exponential family with features and as- sociated weights:</p><formula xml:id="formula_1">P (z | q) ∝ e θ 1 ·φ 1 (q,z) P (a | z, q) ∝ e θ 2 ·φ 2 (q,a,z)</formula><p>where φs are the feature functions and θs are the corresponding weight vectors.</p><p>We cast the learning problem as estimation of the parameter weights to maximize the likelihood of the correct answers in the training data. We con- sider soft assignments to z and marginalize over all its values to get the likelihood of an answer choice:</p><formula xml:id="formula_2">(3) P (a jk | q j ) = n P (a jk , z n |q j )</formula><p>This results in the following regularized likeli- hood objective to maximize:</p><formula xml:id="formula_3">(4) L 1 (θ; P train ) = log |P train | i=1 |Q i | j=1 P (a * ij | q ij ) − λ||θ|| 2</formula><p>Model 2 We now propose a model for the multi- sentence case where we make use of more than a single relevant sentence pertaining to a question.</p><p>Considering that a majority of the questions in the dataset can be answered using two sentences, we restrict ourselves to sentence pairs for purposes of computational tractability. We define the new joint model as:</p><formula xml:id="formula_4">(5) P (a, z 1 , z 2 | q) = P (z 1 | q) · P (z 2 | z 1 , q) · P (a | z 1 , z 2 , q)</formula><p>where the new components are also exponential- family distributions:</p><formula xml:id="formula_5">P (z 2 | z 1 , q) ∝ e θ 3 ·φ 3 (q,z 1 ,z 2 ) P (a | z 1 , z 2 , q) ∝ e θ 2 ·φ 2 (q,a,z 1 ,z 2 )</formula><p>Here, we have three components: the condi- tional probability of a sentence z 1 given q, of a sec- ond sentence z 2 given q and z 1 , and of the answer a given q and the sentences. <ref type="bibr">4</ref> Ideally, we would be able to consider all possible pairs of sentences in a given paragraph. However, to reduce computation costs in practice, we use a sentence window k and consider only sentences that are at most k away from each other. <ref type="bibr">5</ref> We hence maximize:</p><formula xml:id="formula_6">(7) L 2 (θ; P train ) = log |P train |,|Q i |,|Z i | i =1,j=1,m=1 n∈[m−k,m+k] P (a * ij , z im , z in | q ij ) − λ||θ|| 2</formula><p>Model 3 In our next model, we aim to cap- ture important relations between sentences. This model has two novel aspects. First, we consider a distribution over relations between sentence pairs as opposed to a single relation. Second, we utilize the cues from the question as context to resolve ambiguities in sentences pairs with multiple plau- sible relations.</p><p>We add in a hidden variable r ∈ R to represent the relation type. We incorporate features that tie in the question type with the relation type, and that connect the type of relation to the lexical and syn- tactic similarities between sentences. Our relation set R consists of the following relations:</p><p>• Causal : Causes of events or reasons for facts.</p><p>• Temporal : Time-ordering of events • Explanation : Predominantly dealing with how- type questions.</p><p>• Other : A relation other than the above <ref type="bibr">6</ref> We can now modify the joint probability from (5) by adding in relation type r to get:</p><formula xml:id="formula_7">(8) P (a, r, z 1 , z 2 | q) = P (z 1 | q) · P (r | q) · P (z 2 | z 1 , r, q) · P (a | z 1 , z 2 , r, q)</formula><p>where</p><formula xml:id="formula_8">P (r | q) ∝ e θ 4 ·φ 4 (q,r) (9a) P (z 2 | z 1 , r, q) ∝ e θ 3 ·φ 3 (q,r,z 1 ,z 2 ) (9b) P (a | z 1 , z 2 , r, q) ∝ e θ 2 ·φ 2 (q,r,a,z 1 ,z 2 ) (9c)</formula><p>The extra component P (r | q) is the conditional distribution of the relation type r depending on the question. This is to encourage the model to learn, for instance, that why-questions correspond to the causal relation. We also add in extra features to P (z 2 | z 1 , r), that help select a sentence pair con- ditioned on a relation. The likelihood objective to maximize is:</p><formula xml:id="formula_9">(10) L 3 (θ; P train ) = log i,j,m,r∈R n∈[m−k,m+k] P (a * ij , z im , z in , r | q ij ) − λ||θ|| 2</formula><p>We maximize the likelihood objectives using LBFGS-B ( <ref type="bibr" target="#b2">Byrd et al., 1995)</ref>. We compute the gradients required using Automatic Differentia- tion <ref type="bibr" target="#b5">(Corliss, 2002)</ref>.</p><p>To predict an answer for a test question q j , we simply marginalize over all the hidden vari- ables and choose the answer that maximizes P (a jk | q j ):</p><formula xml:id="formula_10">ˆ a j = argmax k P (a jk |q j )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Features</head><p>We use a variety of lexical and syntactic features in our model. We employ the Stanford CoreNLP tool ( ) to pre-process the data. Other than commonly used features in Q&amp;A sys- tems such as unigram and bigram matches, part- of-speech tags, syntactic features, we also add in features specific to our model. We first define some terms used in our descrip- tion. Entities are coreference-resolved nouns or pronouns. Actions refer to verbs other than aux- iliary ones such as is, are, was and were. An en- tity graph is a graph between entities present in a sentence. We create an entity graph by collapsing nodes in the dependency graph and storing the in- termediate nodes between any two entity nodes in the edge between the nodes. We refer to the words in a question q as q-words and similarly to words in an answer a as a-words and those in a sentence z as z-words. <ref type="figure" target="#fig_2">Figure 2</ref> shows an example of an en- tity graph constructed from the dependency graph of a sentence.</p><p>We divide the features into 4 sets (φ 1−4 ), cor- responding to each component probability in (8).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation</head><p>Word list Causal because, why, due, so Temporal when, between, soon, before, after, during, then, finally, now, nowadays, first Explanation how, by, using <ref type="table">Table 1</ref>: Seed marker words for relations used by the model.</p><p>Type 1 (φ 1 ) These features are primarily in- tended to help the model select the most relevant sentence from the passage for a question. We add commonly used features such as unigram and bi- gram matches, syntactic root match, entity and ac- tion matches, missed entities/actions (in q but ab- sent in z) and fractional coverage of q-words in z.</p><p>In addition, we use matches between the edges of the entity graph of q and z. We also have second- order features that are a cross of each feature men- tioned above with the question word (how, what, when, etc.).</p><p>Type 2 (φ 2 ) Features in φ 2 capture interactions between the answer a, question q and sentence(s) (z 1 , z 2 in models 2,3 or z in model 1). For the first-order features, we use ones similar to those in φ 1 for lexical, syntactic, entity and ac- tion matches/misses between a and z. In addition, we add in a neighbor match feature, which checks for matches between the neighborhood of a word from a that occurs in z, and q-words. Another fea- ture we employ is the joint match between z-words and the union of a-words and q-words. Finally, we add in a sliding window (SW) feature, computing its value as in <ref type="bibr" target="#b17">Richardson et al. (2013)</ref>.</p><formula xml:id="formula_11">Type 3 (φ 3 )</formula><p>The next set of features are spe- cific to only models 2 and 3, used to connect sen- tences z 1 and z 2 (and a relation r in model 3 only). <ref type="table" target="#tab_1">Train  70  280  300  1200  Dev  30  120  50  200  Test  60  240  150  600   Table 2</ref>: Dataset Statistics</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Split</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MC160 MC500 Passages Questions Passages Questions</head><p>We use features like the inter-sentence distance and the presence of relation-specific markers in the sentences. We also cross the latter features with entity and action matches between z 1 and z 2 . For the relation-specific words for each relation (except Other), we use words (see <ref type="table">Table 1</ref>) de- rived mainly from Marcu (1997)'s list of discourse markers.</p><p>Type 4 (φ 4 ) The final set of features (used only in model 3) are present to help the model learn connections between the words in the question and the relation type r. Specifically, we check if the interrogative word in the question matches the class represented by r. For instance, the word why matches the Causal relation. For the match-type features of all four types, we use the match count as the feature value if the count is non-zero. If the count is zero, we instead set a corresponding zero feature 7 to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental setup</head><p>Data and Setup We run our experiments on a recently compiled dataset for machine comprehen- sion: MCTest ( <ref type="bibr" target="#b17">Richardson et al., 2013</ref>). The data consists of two distinct sets: MC160 and MC500, which are of different sizes. <ref type="table">Table 2</ref> gives details on the data splits for each dataset. Each passage has 4 questions, with 4 answer choices each. The questions are also annotated into 2 types: single, if the question can be answered using a single sen- tence in the passage, or multi otherwise. We do not use the type information in our learning; we only use it for categorizing accuracy during evaluation. We report final results on all our models trained with λ = 0.1, tuned using the Dev sets.</p><p>Evaluation We report accuracy scores for each model averaged over the questions in the test data. For each question, the system gains 1 point if it scores the correct answer highest and 0 other- wise. In case of ties, we use an inverse weighting   scheme to assign partial credit. So, if three an- swers (including the correct one) tie for the highest score, the system gains 1/3 points.</p><p>Baselines We use the systems proposed by <ref type="bibr" target="#b17">Richardson et al. (2013)</ref> as our baselines. These systems have the best reported scores on this dataset. The first baseline, SWD, uses a sliding window to count matches between the passage words and the words in the answer. This is then combined with a score representing the average distance between answer and question words in the passage. The second baseline, RTE, uses a textual entailment recognizer ( <ref type="bibr" target="#b18">Stern and Dagan, 2011</ref>) to determine if the answer (turned into a statement along with the question) is entailed by the passage. The third system, RTE+SWD, is a weighted combination of the first two baselines and achieves the highest accuracy on the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>Comprehension accuracy <ref type="table" target="#tab_1">Table 3</ref> shows that our relation-aware model 3 outperforms individ- ual baselines on both test sets. On the MC160 test set, the model achieves the best performance of 73.23% accuracy, outperforming the SWD base- line by 5.7% and the RTE+SWD combination by 4.07%. The major gains of model 3, which uti- lizes inter-sentential relations, over model 1 can be seen in the accuracy of multi type questions with a jump of almost 5% absolute in accuracy (statis- tically significant with p &lt; 0.05). On the MC500 test set, we again find that model 3, with a score of 63.75%, provides a gain of 3.5% over SWD and is comparable to the performance of RTE+SWD (63.33%) The importance of utilizing multiple relevant sentences to score answers is evident from the higher scores of models 2 and 3 on multi type questions in both test sets. However, model 1, which retrieves only a single relevant sentence for each question, achieves the best scores on the sin- gle type questions up to 83.25% on MC160 test. One reason for this could be the larger search space for model 3 over pairs of sentences com- pared to just single sentences for model 1. <ref type="table" target="#tab_3">Table 4</ref> shows the variation of our model's accu- racy with the question type. We see that the model deals well with what, where and why type ques- tions in MC500, achieving almost 67-69% accu- racy. <ref type="bibr">8</ref> The major errors (in MC500) seem to come from the how-questions, where the model's accu- racy is low (48%). In MC160, the accuracy is even higher for what-questions (almost 80%). On the other hand, the model does slightly worse on why- questions, with only 60% accuracy.</p><p>RST augmented model Further, we experiment with adding in relations extracted by a publicly available RST parser <ref type="bibr" target="#b6">(Feng and Hirst, 2012)</ref>. The parser extracts a tree with the passage sentences as its leaves and relations as interior nodes in the tree. From this tree, we compute the relation be- tween a pair of sentences as their lowest common ancestor. If one of the sentences is broken down into clauses, we use them all to gather multiple re- lations. We add in features that combine the RST- predicted relation with the interrogation word of the question, and with entity and action matches between sentence pairs. We can see from 100.0 (1) 40.00 (5) 50.00 <ref type="formula">(2)</ref> 14.28 (7) same performance as model 3. In fact, the model performs slightly worse than model 2, which does not utilize inter-sentential relations. Our analysis of the RST trees reveals that for a vast majority of sentence pairs (77%), the RST algorithm predicts the elaboration relation which does not provide an informative distinction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Analysis</head><p>To gain further insight into the workings of our model, we perform several analyses on model 3 using human judgements. We annotate 240 ques- tions from the test set of MC160 with the most relevant sentences 9 in the passage for each ques- tion. In addition, if they chose more than a sin- gle relevant sentence, we also asked the annota- tors to mark the most appropriate relation (from our set of relations used in model 3) between the sentence pairs. <ref type="bibr">10</ref> We find that 146 question anno- tations contain a single relevant sentence and 94 contain multiple sentences. <ref type="bibr">11</ref> We obtain 103 sen- tence pairs with annotated relations.</p><p>Annotation statistics We select a random sub- set of 134 questions from this data to annotate twice and compute inter-annotator agreement. The second annotator agreed completely with the sen- tence predictions of the first annotator in 76.11% cases and both annotators agreed on at least one sentence in 94.77% of the questions. The agree- ment on relations annotated over common sen- <ref type="bibr">9</ref> The annotators are native English speakers. <ref type="bibr">10</ref> If there were more than two relevant sentences, we asked them to mark relations between all pairs. This was a very rare occurrence though. <ref type="bibr">11</ref> We found that some of the multiple questions did not re- quire multiple sentences to answer and conversely, some sin- gle questions required more than one sentence to answer. tence pairs is 68.6%, with κ = 0.462. We find that out of the 103 annotated sentence pairs, 67 are next to each other in the passage while 27 are at a distance of two and 9 pairs are at a distance of three or more.</p><p>It has been well documented that identifying discourse relations without explicit markers is sig- nificantly harder than with markers ( <ref type="bibr">Pitler et al., 2008;</ref><ref type="bibr" target="#b10">Lin et al., 2009;</ref><ref type="bibr" target="#b15">Park and Cardie, 2012)</ref>. We compute statistics on the presence of discourse markers anywhere in the manually picked sen- tence(s) for each question. We find that only 33.89% of these pairs have a relevant discourse marker present in either sentence. We consider a discourse marker as relevant if it occurs in our marker list for the annotated relation. Further, if we only consider markers occurring at the begin- ning or end of the sentences, this number drops to 9.23% of sentences. Since we consider relations between sentence pairs, most explicit markers that could help identify these relations would occur at an extremity of either sentence. We point out that these numbers are an over-estimation since many of the markers occur in syntactic roles as opposed to discourse in the sentences (ex. so in This is so good compared to So, he decided to ...). These statistics reflect the difficulty of the problem since operating over implicit relations is much harder.</p><p>Sentence Retrieval We analyze our models' ability to predict relevant sentences given only the question. For each question, we order the pairs scored by a model in descending order of their probability according to P (z 1 , z 2 | q) and com- pare them to the annotated pairs, reporting recall at various thresholds. This is a stringent evaluation primarily due to  two reasons. First, we do not use the candidate answers in selecting relevant sentences. Second, on the machine comprehension task, the model predicts answers by marginalizing over the sen- tences/sentence pairs. Hence, the model can score answers correctly even if the relevant sentence(s) are not at the top of its sentence distribution calcu- lated here. We compute the distribution over sen- tence pairs as:</p><formula xml:id="formula_12">P (z 1 , z 2 |q) = r∈R P (z 1 | q) · P (r | q) · P (z 2 |z 1 , r, q)</formula><p>For comparison, we add in a baseline (Freq) that orders sentences using the sum of unigram and bigram matches with the question (in descending frequency). <ref type="table" target="#tab_5">Table 5</ref> shows that our models perform signifi- cantly better than the Freq baseline over all ques- tion types. For the single-question case, we ob- serve that model 3 ranks the annotated sentence at the top of its distribution around 51% of the time and 80% of the time in the top 5. For multi- sentences, these recall numbers drop to around 25% (@1) and 50% (@5). We also observe that models 2 and 3 perform better than model 1 on the multi-sentence cases. The similar sentence recall of models 2 and 3 also point to the fact that the gains from model 3 on comprehension accuracy are due to its ability to utilize relations between the sentences.</p><p>We observe that where, when and who questions have the highest recalls. This is likely because these questions often have characteristic words oc- curring in the sentences (such as here, there, af- ter, before, him, her). In contrast, questions asking how, which and why have lower recalls since they often involve reasoning over multiple sentences. What-questions are somewhere in between since their complexity varies from question to question.</p><p>Relation Retrieval We examine how well our model can predict relations between given sen- tence pairs. For each annotated pair of sentences, we calculate the relation distribution and compute the relation recall at various thresholds of the rank- ing by probability. The relation distribution is computed as: P (r|z 1 , z 2 , q) = P (r | q) · P (z 2 |z 1 , r, q) r ∈R P (r | q) · P (z 2 |z 1 , r , q) From table 6, we observe that our model's top prediction matches the manual annotations (over- all) 51% of the time. The model predicts causal and other relations more accurately than the other two.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation (#) R @ 1 R @ 2</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we propose a new approach for in- corporating discourse information into machine comprehension applications. The key idea is to implant discourse analysis into a joint model for comprehension. Our results demonstrate that the discourse-aware model outperforms state-of-the- art standalone systems, and rivals the performance of a system combination. We also find that fea- tures derived from an off-the-shelf parser do not improve performance of the model. Our analysis also demonstrates that the model accuracy varies significantly according to the question type. Fi- nally, we show that the predicted discourse rela- tions exhibit considerable overlap with relations identified by human annotators.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Sample story excerpt from a passage in the MCTest dataset. 2 Correct answer is in italics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Types 1 and 2 are inspired by prior work in question classification/answering (Blunsom et al., 2006; Jansen et al., 2014). Feature types 3 and 4 are specific to our models, primarily dealing with relation types.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Top: Dependency graph, Bottom: Entity graph for an example sentence. Entities are in bold, actions are in italics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Model</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Accuracy (%) of the different baselines (in italics) and our models. Single: questions requiring 
single sentence to answer; Multi: questions requiring multiple sentences to answer. Sentence window 
(k) = 4 for models 2 and 3. Best scores are shown in bold. Statistical significance (shown only for All 
columns) of p &lt; 0.05 using two-tailed paired t-test:  *  vs SWD,  † vs RTE. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 that adding in RST features to model 2 (M2+RST) does not give the</head><label>3</label><figDesc></figDesc><table>Question 

MC160 
MC500 
Type 
Dev 
Test 
Dev 
Test 
how 
50.00 (10) 71.42 (21) 
54.54 (11) 
48.83 (43) 
what 
64.40 (59) 79.36 (126) 63.15 (114) 67.19 (317) 
where 
30.76 (13) 91.66 (12) 
82.60 (23) 
68.96 (58) 
which 
75.00 (4) 
33.33 (6) 
25.00 (4) 
48.00 (25) 
who 
70.50 (17) 67.85 (28) 
62.50 (16) 
59.74 (77) 
why 
85.71 (14) 59.45 (37) 
65.38 (26) 
69.35 (62) 
when 
100.0 (2) 
80.00 (5) 
100.0 (4) 
62.50 (8) 
whose 
-
-
-
66.67 (3) 
(other) 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Accuracy (%) of model 3 by question type for question in MC160 and MC500 dev and test sets. 
Numbers in parentheses indicate the number of questions of each type. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Recall (%) of relevant sentence(s) in the ranking by models 1, 2 and 3 compared with a match-
frequency baseline (Freq) at various thresholds, for different question types in MC160. Question fre-
quencies are in parentheses. Bold numbers represent best scores. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Recall of annotated relations at various 
thresholds in the ordered relation distribution pre-
dicted by model 3. Relation frequencies are in 
parentheses. 

</table></figure>

			<note place="foot" n="1"> Code and data are available at http://people. csail.mit.edu/karthikn/mcdr.</note>

			<note place="foot" n="2"> http://research.microsoft.com/en-us/ um/redmond/projects/mctest/</note>

			<note place="foot" n="3"> They use data from Yahoo! Answers and a Biology textbook.</note>

			<note place="foot" n="4"> Since this component replaces the second component in model 1, we use the same subscript 2 for its feature set φ. 5 Including the case where z1 = z2. 6 This includes the no-relation cases</note>

			<note place="foot" n="7"> For each match feature, like Entity-Match, we have a corresponding zero feature, Entity-Match-Zero</note>

			<note place="foot" n="8"> Note that what-questions may also require causal/temporal/explanation relations to answer.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Lyla Fischer, Rushi Ganmukhi, Hrishikesh Joshi and Deepak Narayanan for helping with annotating the MC160 test set. We also thank the anonymous ACL reviewers and members of MIT's NLP group for their insightful comments and suggestions. This research is developed in a collaboration of MIT with the Arabic Language Technologies (ALT) group at Qatar Computing Research Institute (QCRI) within the Interactive sYstems for Answer Search (IYAS) project.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Modeling biological processes for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="1499" to="1510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Question classification with log-linear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 29th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="615" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A limited memory algorithm for bound constrained optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Byrd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1190" to="1208" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Linguistic Data Consortium, et al. 2002. RST discourse treebank. Linguistic Data Consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Carlson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Discourse structure for context question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Joyce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Pragmatics of Question Answering at HLT-NAACL</title>
		<meeting>the Workshop on Pragmatics of Question Answering at HLT-NAACL</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="23" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Automatic differentiation of algorithms: from simulation to optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Corliss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Text-level discourse parsing with rich linguistic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirst2012] Vanessa</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="60" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A linear-time bottom-up discourse parser with constraints and post-editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirst2014] Vanessa</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="511" to="521" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A neural network for factoid question answering over paragraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Iyyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Discourse complements lexical semantics for non-factoid answer reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Jansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="977" to="986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recognizing implicit discourse relations in the penn discourse treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="343" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A pdtb-styled end-to-end discourse parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rhetorical structure theory: Toward a functional theory of text organization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><forename type="middle">A</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Text</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="243" to="281" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The Rhetorical Parsing, Summarization and Generation of Natural Language Texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving implicit discourse relation recognition through feature set optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cardie2012] Joonsuk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<editor>Emily Pitler, Mridhula Raghupathy, Hena Mehta, Ani Nenkova, Alan Lee, and Aravind K Joshi</editor>
		<meeting>the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="108" to="112" />
		</imprint>
	</monogr>
	<note>Pitler et al.2008. Easily identifiable discourse relations</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The penn discourse treebank 2.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Prasad</surname></persName>
		</author>
		<editor>LREC. Citeseer</editor>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mctest: A challenge dataset for the open-domain machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page" from="193" to="203" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A confidence model for syntacticallymotivated entailment proofs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asher</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference Recent Advances in Natural Language Processing</title>
		<meeting>the International Conference Recent Advances in Natural Language Processing<address><addrLine>Hissar, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-09" />
			<biblScope unit="page" from="455" to="462" />
		</imprint>
	</monogr>
	<note>Stern and Dagan2011</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Evaluating discourse-based answer extraction for why-question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Verberne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 30th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="735" to="736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Representing discourse coherence: A corpus-based study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Gibson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="249" to="287" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
