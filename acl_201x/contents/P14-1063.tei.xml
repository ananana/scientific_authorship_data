<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Online Learning in Tensor Space</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Language &amp; Speech Processing and Human Language Technology Center of Excellence</orgName>
								<orgName type="institution">The Johns Hopkins University Baltimore</orgName>
								<address>
									<postCode>21218</postCode>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
							<email>{yuan.cao, khudanpur}@jhu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Language &amp; Speech Processing and Human Language Technology Center of Excellence</orgName>
								<orgName type="institution">The Johns Hopkins University Baltimore</orgName>
								<address>
									<postCode>21218</postCode>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Online Learning in Tensor Space</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="666" to="675"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose an online learning algorithm based on tensor-space models. A tensor-space model represents data in a compact way, and via rank-1 approximation the weight tensor can be made highly struc-tured, resulting in a significantly smaller number of free parameters to be estimated than in comparable vector-space models. This regularizes the model complexity and makes the tensor model highly effective in situations where a large feature set is defined but very limited resources are available for training. We apply with the proposed algorithm to a parsing task, and show that even with very little training data the learning algorithm based on a ten-sor model performs well, and gives significantly better results than standard learning algorithms based on traditional vector-space models.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many NLP applications use models that try to in- corporate a large number of linguistic features so that as much human knowledge of language can be brought to bear on the (prediction) task as pos- sible. This also makes training the model param- eters a challenging problem, since the amount of labeled training data is usually small compared to the size of feature sets: the feature weights cannot be estimated reliably.</p><p>Most traditional models are linear models, in the sense that both the features of the data and model parameters are represented as vectors in a vector space. Many learning algorithms applied to NLP problems, such as the Perceptron <ref type="bibr">(Collins, 2002</ref>), MIRA ( <ref type="bibr">Crammer et al., 2006;</ref><ref type="bibr">McDonald et al., 2005;</ref><ref type="bibr">Chiang et al., 2008)</ref>, PRO <ref type="bibr">(Hopkins and May, 2011</ref>), RAMPION ( <ref type="bibr">Gimpel and Smith, 2012)</ref> etc., are based on vector-space mod- els. Such models require learning individual fea- ture weights directly, so that the number of param- eters to be estimated is identical to the size of the feature set. When millions of features are used but the amount of labeled data is limited, it can be dif- ficult to precisely estimate each feature weight.</p><p>In this paper, we shift the model from vector- space to tensor-space. Data can be represented in a compact and structured way using tensors as containers. Tensor representations have been ap- plied to computer vision problems ( <ref type="bibr">Hazan et al., 2005;</ref><ref type="bibr">Shashua and Hazan, 2005</ref>) and information retrieval <ref type="bibr">(Cai et al., 2006a</ref>) a long time ago. More recently, it has also been applied to parsing <ref type="bibr">(Cohen and Collins, 2012;</ref><ref type="bibr">Cohen and Satta, 2013)</ref> and se- mantic analysis <ref type="bibr">(Van de Cruys et al., 2013)</ref>. A linear tensor model represents both features and weights in tensor-space, hence the weight tensor can be factorized and approximated by a linear sum of rank-1 tensors. This low-rank approxi- mation imposes structural constraints on the fea- ture weights and can be regarded as a form of regularization. With this representation, we no longer need to estimate individual feature weights directly but only a small number of "bases" in- stead. This property makes the the tensor model very effective when training a large number of fea- ture weights in a low-resource environment. On the other hand, tensor models have many more de- grees of "design freedom" than vector space mod- els. While this makes them very flexible, it also creates much difficulty in designing an optimal tensor structure for a given training set.</p><p>We give detailed description of the tensor space model in Section 2. Several issues that come with the tensor model construction are addressed in Section 3. A tensor weight learning algorithm is then proposed in 4. Finally we give our exper- imental results on a parsing task and analysis in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Tensor Space Representation</head><p>Most of the learning algorithms for NLP problems are based on vector space models, which represent data as vectors φ ∈ R n , and try to learn feature weight vectors w ∈ R n such that a linear model y = w · φ is able to discriminate between, say, good and bad hypotheses. While this is a natural way of representing data, it is not the only choice. Below, we reformulate the model from vector to tensor space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Tensor Space Model</head><p>A tensor is a multidimensional array, and is a gen- eralization of commonly used algebraic objects such as vectors and matrices. Specifically, a vec- tor is a 1 st order tensor, a matrix is a 2 nd order tensor, and data organized as a rectangular cuboid is a 3 rd order tensor etc. In general, a D th order tensor is represented as T ∈ R n 1 ×n 2 ×...n D , and an entry in T is denoted by T i 1 ,i 2 ,...,i D . Different di- mensions of a tensor 1, 2, . . . , D are named modes of the tensor. Using a D th order tensor as container, we can assign each feature of the task a D-dimensional index in the tensor and represent the data as ten- sors. Of course, shifting from a vector to a tensor representation entails several additional degrees of freedom, e.g., the order D of the tensor and the sizes {n d } D d=1 of the modes, which must be ad- dressed when selecting a tensor model. This will be done in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Tensor Decomposition</head><p>Just as a matrix can be decomposed as a lin- ear combination of several rank-1 matrices via SVD, tensors also admit decompositions 1 into lin- ear combinations of "rank-1" tensors. A D th or- der tensor A ∈ R n 1 ×n 2 ×...n D is rank-1 if it can be <ref type="bibr">1</ref> The form of tensor decomposition defined here is named as CANDECOMP/PARAFAC(CP) decomposition <ref type="bibr">(Kolda and Bader, 2009)</ref>. Another popular form of tensor decom- position is called Tucker decomposition, which decomposes a tensor into a core tensor multiplied by a matrix along each mode. We focus only on the CP decomposition in this paper.</p><p>written as the outer product of D vectors, i.e.</p><formula xml:id="formula_0">A = a 1 ⊗ a 2 ⊗, . . . , ⊗a D , where a i ∈ R n d , 1 ≤ d ≤ D.</formula><p>A D th order tensor T ∈ R n 1 ×n 2 ×...n D can be factorized into a sum of component rank-1 tensors as</p><formula xml:id="formula_1">T = R r=1 A r = R r=1 a 1 r ⊗ a 2 r ⊗, . . . , ⊗a D r</formula><p>where R, called the rank of the tensor, is the mini- mum number of rank-1 tensors whose sum equals T . Via decomposition, one may approximate a tensor by the sum of H major rank-1 tensors with H ≤ R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Linear Tensor Model</head><p>In tensor space, a linear model may be written (ig- noring a bias term) as</p><formula xml:id="formula_2">f (W ) = W • Φ,</formula><p>where Φ ∈ R n 1 ×n 2 ×...n D is the feature tensor, W is the corresponding weight tensor, and • denotes the Hadamard product. If W is further decom- posed as the sum of H major component rank-1</p><formula xml:id="formula_3">tensors, i.e. W ≈ H h=1 w 1 h ⊗ w 2 h ⊗, . . . , ⊗w D h , then f (w 1 1 , . . . , w D 1 , . . . , w 1 h , . . . , w D h ) = H h=1 Φ × 1 w 1 h × 2 w 2 h . . . × D w D h , (1)</formula><p>where × l is the l-mode product operator between a D th order tensor T and a vector a of dimension n d , yielding a (D − 1) th order tensor such that</p><formula xml:id="formula_4">(T × l a) i 1 ,...,i l−1 ,i l+1 ,...,i D = n d i l =1 T i 1 ,...,i l−1 ,i l ,i l+1 ,...,i D · a i l .</formula><p>The linear tensor model is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Why Learning in Tensor Space?</head><p>So what is the advantage of learning with a ten- sor model instead of a vector model? Consider the case where we have defined 1,000,000 features for our task. A vector space linear model requires es- timating 1,000,000 free parameters. However if we use a 2 nd order tensor model, organize the fea- tures into a 1000 × 1000 matrix Φ, and use just one rank-1 matrix to approximate the weight ten- sor, then the linear model becomes</p><formula xml:id="formula_5">f (w 1 , w 2 ) = w T 1 Φw 2 ,</formula><p>where w 1 , w 2 ∈ R 1000 . That is to say, now we only need to estimate 2000 parameters! In general, if V features are defined for a learn- ing problem, and we (i) organize the feature set as a tensor Φ ∈ R n 1 ×n 2 ×...n D and (ii) use H component rank-1 tensors to approximate the cor- responding target weight tensor. Then the total number of parameters to be learned for this ten- sor model is H D d=1 n d , which is usually much smaller than V = D d=1 n d for a traditional vec- tor space model. Therefore we expect the tensor model to be more effective in a low-resource train- ing environment.</p><p>Specifically, a vector space model assumes each feature weight to be a "free" parameter, and es- timating them reliably could therefore be hard when training data are not sufficient or the fea- ture set is huge. By contrast, a linear tensor model only needs to learn H D d=1 n d "bases" of the m feature weights instead of individual weights di- rectly. The weight corresponding to the feature Φ i 1 ,i 2 ,...,i D in the tensor model is expressed as</p><formula xml:id="formula_6">w i 1 ,i 2 ,...,i D = H h=1 w 1 h,i 1 w 2 h,i 2 . . . w D h,i D ,<label>(2)</label></formula><p>where w j h,i j is the i th j element in the vector w j h . In other words, a true feature weight is now ap- proximated by a set of bases. This reminds us of the well-known low-rank matrix approximation of images via SVD, and we are applying similar techniques to approximate target feature weights, which is made possible only after we shift from vector to tensor space models. This approximation can be treated as a form of model regularization, since the weight tensor is represented in a constrained form and made highly structured via the rank-1 tensor approximation. Of course, as we reduce the model complexity, e.g. by choosing a smaller and smaller H, the model's ex- pressive ability is weakened at the same time. We will elaborate on this point in Section 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Tensor Model Construction</head><p>To apply a tensor model, we first need to con- vert the feature vector into a tensor Φ. Once the structure of Φ is determined, the structure of W is fixed as well. As mentioned in Section 2.1, a tensor model has many more degrees of "design freedom" than a vector model, which makes the problem of finding a good tensor structure a non- trivial one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Tensor Order</head><p>The order of a tensor affects the model in two ways: the expressiveness of the model and the number of parameters to be estimated. We assume H = 1 in the analysis below, noting that one can always add as many rank-1 component tensors as needed to approximate a tensor with arbitrary pre- cision.</p><p>Obviously, the 1 st order tensor (vector) model is the most expressive, since it is structureless and any arbitrary set of numbers can always be repre- sented exactly as a vector. The 2 nd order rank-1 tensor (rank-1 matrix) is less expressive because not every set of numbers can be organized into a rank-1 matrix. In general, a D th order rank-1 tensor is more expressive than a (D + 1) th order rank-1 tensor, as a lower-order tensor imposes less structural constraints on the set of numbers it can express. We formally state this fact as follows: Theorem 1. A set of real numbers that can be rep- resented by a (D + 1) th order tensor Q can also be represented by a D th order tensor P, provided P and Q have the same volume. But the reverse is not true.</p><p>Proof. See appendix.</p><p>On the other hand, tensor order also affects the number of parameters to be trained. Assuming that a D th order has equal size on each mode (we will elaborate on this point in Section 3.2) and the volume (number of entries) of the tensor is fixed as V , then the total number of parameters of the model is DV </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D . This is a convex func- tion of D, and the minimum 2 is reached at either</head><formula xml:id="formula_7">D * = ln V or D * = ln V .</formula><p>Therefore, as D increases from 1 to D * , we lose more and more of the expressive power of the model but reduce the number of parameters to be trained. However it would be a bad idea to choose a D beyond D * . The optimal tensor order depends on the nature of the actual problem, and we tune this hyper-parameter on a held-out set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Mode Size</head><p>The size n d of each tensor mode, d = 1, . . . , D, determines the structure of feature weights a ten- sor model can precisely represent, as well as the number of parameters to estimate (we also as- sume H = 1 in the analysis below). For exam- ple, if the tensor order is 2 and the volume V is 12, then we can either choose n 1 = 3, n 2 = 4 or n 1 = 2, n 2 = 6. For n 1 = 3, n 2 = 4, the numbers that can be precisely represented are di- vided into 3 groups, each having 4 numbers, that are scaled versions of one another. Similarly for n 1 = 2, n 2 = 6, the numbers can be divided into 2 groups with different scales. Obviously, the two possible choices of (n 1 , n 2 ) also lead to different numbers of free parameters <ref type="bibr">(7 vs. 8)</ref>.</p><p>Given D and V , there are many possible combi- nations of n d , d = 1, . . . , D, and the optimal com- bination should indeed be determined by the struc- ture of target features weights. However it is hard to know the structure of target feature weights be- fore learning, and it would be impractical to try ev- ery possible combination of mode sizes, therefore we choose the criterion of determining the mode sizes as minimization of the total number of pa- rameters, namely we solve the problem:</p><formula xml:id="formula_8">min n 1 ,...,n D D d=1 n d s.t D d=1 n d = V The optimal solution is reached when n 1 = n 2 = . . . = n D = V 1 D . Of course it is not guaran- teed that V 1 D is an integer, therefore we choose n d = V 1 D or V 1 D , d = 1, . . . , D such that D d=1 n d ≥ V and D d=1 n d − V is minimized.</formula><p>The D d=1 n d − V extra entries of the tensor correspond to no features and are used just for <ref type="bibr">2</ref> The optimal integer solution can be determined simply by comparing the two function values. padding. Since for each n d there are only two possible values to choose, we can simply enumer- ate all the possible 2 D (which is usually a small number) combinations of values and pick the one that matches the conditions given above. This way n 1 , . . . , n D are fully determined.</p><p>Here we are only following the principle of min- imizing the parameter number. While this strat- egy might work well with small amount of train- ing data, it is not guaranteed to be the best strategy in all cases, especially when more data is avail- able we might want to increase the number of pa- rameters, making the model more complex so that the data can be more precisely modeled. Ideally the mode size needs to be adaptive to the amount of training data as well as the property of target weights. A theoretically guaranteed optimal ap- proach to determining the mode sizes remains an open problem, and will be explored in our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Number of Rank-1 Tensors</head><p>The impact of using H &gt; 1 rank-1 tensors is ob- vious: a larger H increases the model complexity and makes the model more expressive, since we are able to approximate target weight tensor with smaller error. As a trade-off, the number of param- eters and training complexity will be increased. To find out the optimal value of H for a given prob- lem, we tune this hyper-parameter too on a held- out set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Vector to Tensor Mapping</head><p>Finally, we need to find a way to map the orig- inal feature vector to a tensor, i.e. to associate each feature with an index in the tensor. Assum- ing the tensor volume V is the same as the number of features, then there are in all V ! ways of map- ping, which is an intractable number of possibili- ties even for modest sized feature sets, making it impractical to carry out a brute force search. How- ever while we are doing the mapping, we hope to arrange the features in a way such that the corre- sponding target weight tensor has approximately a low-rank structure, this way it can be well approx- imated by very few component rank-1 tensors.</p><p>Unfortunately we have no knowledge about the target weights in advance, since that is what we need to learn after all. As a way out, we first run a simple vector-model based learning algorithm (say the Perceptron) on the training data and es- timate a weight vector, which serves as a "surro-gate" weight vector. We then use this surrogate vector to guide the design of the mapping. Ide- ally we hope to find a permutation of the surro- gate weights to map to a tensor in such a way that the tensor has a rank as low as possible. How- ever matrix rank minimization is in general a hard problem <ref type="bibr">(Fazel, 2002</ref>). Therefore, we follow an approximate algorithm given in <ref type="figure">Figure 2a</ref>, whose main idea is illustrated via an example in <ref type="figure">Figure  2b</ref>.</p><p>Basically, what the algorithm does is to di- vide the surrogate weights into hierarchical groups such that groups on the same level are approx- imately proportional to each other. Using these groups as units we are able to "fill" the tensor in a hierarchical way. The resulting tensor will have an approximate low-rank structure, provided that the sorted feature weights have roughly group-wise proportional relations.</p><p>For comparison, we also experimented a trivial solution which maps each entry of the feature ten- sor to the tensor just in sequential order, namely φ 0 is mapped to Φ 0,0,...,0 , φ 1 is mapped to Φ 0,0,...,1 etc. This of course ignores correlation between features since the original feature order in the vec- tor could be totally meaningless, and this strategy is not expected to be a good solution for vector to tensor mapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Online Learning Algorithm</head><p>We now turn to the problem of learning the feature weight tensor. Here we propose an online learning algorithm similar to MIRA but modified to accom- modate tensor models.</p><p>Let the model be f (T ) = T • Φ(x, y), where</p><formula xml:id="formula_9">T = H h=1 w 1 h ⊗ w 2 h ⊗, . . . , ⊗w D h</formula><p>is the weight tensor, Φ(x, y) is the feature tensor for an input- output pair (x, y). Training samples (x i , y i ), i = 1, . . . , m, where x i is the input and y i is the ref- erence or oracle hypothesis, are fed to the weight learning algorithm in sequential order. A predic- tion z t is made by the model T t at time t from a set of candidates Z(x t ), and the model updates the weight tensor by solving the following problem:</p><formula xml:id="formula_10">min T ∈R n 1 ×n 2 ×...n D 1 2 T − T t 2 + Cξ (3) s.t. L t ≤ ξ, ξ ≥ 0</formula><p>where T is a decomposed weight tensor and</p><formula xml:id="formula_11">L t = T • Φ(x t , z t ) − T • Φ(x t , y t ) + ρ(y t , z t ) Input:</formula><p>Tensor order D, tensor volume V , mode size</p><formula xml:id="formula_12">n d , d = 1, . . . , D, surrogate weight vector v Let v + = [v + 1 , . . . , v + p ] be the non-negative part of v v − = [v − 1 , . . . , v − q ]</formula><p>be the negative part of v Algorithm:</p><formula xml:id="formula_13">˜ v + = sort(v + ) in descending order˜v order˜ order˜v − = sort(v − ) in ascending order u = V /n D e = p − mod(p, u), f = q − mod(q, u) Construct vector X = [˜ v + 1 , . . . , ˜ v + e , ˜ v − 1 , . . . , ˜ v − f , ˜ v + e+1 , . . . , ˜ v + p , ˜ v − f +1 , . . . , ˜ v − q ]</formula><p>Map X a , a = 1, . . . , p + q to the tensor entry  <ref type="figure">Figure 2</ref>: Algorithm for mapping a surrogate weight vector X to a tensor. (2a) provides the al- gorithm; (2b) illustrates it by mapping a vector of length V = 12 to a (n 1 , n 2 , n 3 ) = (2, 2, 3) ten- sor. The bars X i represent the surrogate weights -after separately sorting the positive and nega- tive parts -and the labels along a path of the tree correspond to the tensor-index of the weight rep- resented by the leaf resulting from the mapping. is the structured hinge loss.</p><formula xml:id="formula_14">T i 1 ,...,i D , such that a = D d=1 (i d − 1)l d−1 + 1 where l d = l d</formula><p>This problem setting follows the same "passive- aggressive" strategy as in the original MIRA. To optimize the vectors w d h , h = 1, . . . , H, d = 1, . . . , D, we use a similar iterative strategy as pro- posed in <ref type="bibr">(Cai et al., 2006b</ref>). Basically, the idea is that instead of optimizing w d h all together, we op- timize w 1 1 , w 2 1 , . . . , w D H in turn. While we are up- dating one vector, the rest are fixed. For the prob- lem setting given above, each of the sub-problems that need to be solved is convex, and according to <ref type="bibr">(Cai et al., 2006b</ref>) the objective function value will decrease after each individual weight update and eventually this procedure will converge.</p><p>We now give this procedure in more detail. Denote the weight vector of the d th mode of the h th tensor at time t as w d h,t . We will up- date the vectors in turn in the following order:</p><formula xml:id="formula_15">w 1 1,t , . . . , w D 1,t , w 1 2,t , . . . , w D 2,t , . . . , w 1 H,t , . . . , w D H,t .</formula><p>Once a vector has been updated, it is fixed for future updates.</p><p>By way of notation, define</p><formula xml:id="formula_16">W d h,t = w 1 h,t+1 ⊗, . . . , ⊗w d−1 h,t+1 ⊗ w d h,t ⊗, . . . , ⊗w D h,t (and let W D+1 h,t w 1 h,t+1 ⊗, . . . , ⊗w D h,t+1 ), W d h,t = w 1 h,t+1 ⊗, . . . , ⊗w d−1 h,t+1 ⊗ w d ⊗, . . . , ⊗w D h,t (where w d ∈ R n d ), T d h,t = h−1 h =1 W D+1 h ,t + W d h,t + H h =h+1 W 1 h ,t (4) T d h,t = h−1 h =1 W D+1 h ,t + W d h,t + H h =h+1 W 1 h ,t φ d h,t (x, y) = Φ(x, y) × 2 w 2 h,t+1 . . . × d−1 w d−1 h,t+1 × d+1 w d+1 h,t . . . × D w D h,t<label>(5)</label></formula><p>In order to update from w d h,t to get w d h,t+1 , the sub-problem to solve is:</p><formula xml:id="formula_17">min w d ∈R n d 1 2 T d h,t − T d h,t 2 + Cξ = min w d ∈R n d 1 2 W d h,t − W d h,t 2 + Cξ = min w d ∈R n d 1 2 β 1 h,t+1 . . . β d−1 h,t+1 β d+1 h,t . . . β D h,t w d − w d h,t 2 + Cξ s.t. L d h,t ≤ ξ, ξ ≥ 0.</formula><p>where</p><formula xml:id="formula_18">β d h,t = w d h,t 2 L d h,t = T d h,t • Φ(x t , z t ) − T d h,t • Φ(x t , y t ) +ρ(y t , z t ) = w d · φ d h,t (x t , z t ) − φ d h,t (x t , y t ) − h−1 h =1 W D+1 h ,t + H h =h+1 W 1 h ,t • (Φ(x t , y t ) − Φ(x t , z t )) +ρ(y t , z t ) Letting ∆φ d h,t φ d h,t (x t , y t ) − φ d h,t (x t , z t ) and s d h,t h−1 h =1 W D+1 h ,t + H h =h+1 W 1 h ,t • (Φ(x t , y t ) − Φ(x t , z t ))</formula><p>we may compactly write</p><formula xml:id="formula_19">L d h,t = ρ(y t , z t ) − s d h,t − w d · ∆φ d h,t .</formula><p>This convex optimization problem is just like the original MIRA and may be solved in a similar way. The updating strategy for w d h,t is derived as</p><formula xml:id="formula_20">w d h,t+1 = w d h,t + τ ∆φ d h,t τ = (6) min C, ρ(y t , z t ) − T d h,t • (Φ(x t , y t ) − Φ(x t , z t )) ∆φ d h,t 2</formula><p>The initial vectors w i h,1 cannot be made all zero, since otherwise the l-mode product in Equation (5) would yield all zero φ d h,t (x, y) and the model would never get a chance to be updated. There- fore, we initialize the entries of w i h,1 uniformly such that the Frobenius-norm of the weight tensor W is unity.</p><p>We call the algorithm above "Tensor-MIRA" and abbreviate it as T-MIRA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>671</head><p>In this section we shows empirical results of the training algorithm on a parsing task. We used the Charniak parser <ref type="bibr">(Charniak et al., 2005</ref>) for our ex- periment, and we used the proposed algorithm to train the reranking feature weights. For compari- son, we also investigated training the reranker with Perceptron and MIRA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Settings</head><p>To simulate a low-resource training environment, our training sets were selected from sections 2-9 of the Penn WSJ treebank, section 24 was used as the held-out set and section 23 as the evaluation set. We applied the default settings of the parser. There are around V = 1.33 million features in all defined for reranking, and the n-best size for reranking is set to 50. We selected the parse with the highest f -score from the 50-best list as the or- acle.</p><p>We would like to observe from the experiments how the amount of training data as well as dif- ferent settings of the tensor degrees of freedom affects the algorithm performance. Therefore we tried all combinations of the following experimen- tal parameters:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameters</head><p>Settings Training data (m) Sec. 2, 2-3, 2-5, 2-9 Tensor order (D) 2, 3, 4 # rank-1 tensors (H) 1, 2, 3 Vec. to tensor mapping approximate, sequential Here "approximate" and "sequential" means us- ing, respectively, the algorithm given in <ref type="figure">Figure 2</ref> and the sequential mapping mentioned in Section 3.4. According to the strategy given in 3.2, once the tensor order and number of features are fixed, the sizes of modes and total number of parameters to estimate are fixed as well, as shown in the tables below: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results and Analysis</head><p>The f -scores of the held-out and evaluation set given by T-MIRA as well as the Perceptron and MIRA baseline are given in <ref type="table">Table 1</ref>. From the re- sults, we have the following observations:</p><p>1. When very few labeled data are available for training (compared with the number of fea- tures), T-MIRA performs much better than the vector-based models MIRA and Percep- tron. However as the amount of training data increases, the advantage of T-MIRA fades away, and vector-based models catch up. This is because the weight tensors learned by T-MIRA are highly structured, which sig- nificantly reduces model/training complex- ity and makes the learning process very ef- fective in a low-resource environment, but as the amount of data increases, the more complex and expressive vector-based models adapt to the data better, whereas further im- provements from the tensor model is impeded by its structural constraints, making it insen- sitive to the increase of training data.</p><p>2. To further contrast the behavior of T-MIRA, MIRA and Perceptron, we plot the f -scores on both the training and held-out sets given by these algorithms after each training epoch in <ref type="figure">Figure 3</ref>. The plots are for the exper- imental setting with mapping=surrogate, # rank-1 tensors=2, tensor order=2, training data=sections 2-3. It is clearly seen that both MIRA and Perceptron do much better than T- MIRA on the training set. Nevertheless, with a huge number of parameters to fit a limited amount of data, they tend to over-fit and give much worse results on the held-out set than T-MIRA does.</p><p>As an aside, observe that MIRA consistently outperformed Perceptron, as expected.</p><p>3. Properties of linear tensor model: The heuris- tic vector-to-tensor mapping strategy given by <ref type="figure">Figure 2</ref> gives consistently better results than the sequential mapping strategy, as ex- pected.</p><p>To make further comparison of the two strate- gies, in <ref type="figure">Figure 4</ref> we plot the 20 largest sin- gular values of the matrices which the surro- gate weights (given by the Perceptron after running for 1 epoch) are mapped to by both strategies (from the experiment with training data sections 2-5). From the contrast between the largest and the 2 nd -largest singular val- ues, it can be seen that the matrix generated by the first strategy approximates a low-rank structure much better than the second strat- egy. Therefore, the performance of T-MIRA is influenced significantly by the way features are mapped to the tensor. If the correspond- ing target weight tensor has internal struc- ture that makes it approximately low-rank, the learning procedure becomes more effec- tive.</p><p>The best results are consistently given by 2 nd order tensor models, and the differences be- tween the 3 rd and 4 th order tensors are not significant. As discussed in Section 3.1, al- though 3 rd and 4 th order tensors have less pa- rameters, the benefit of reduced training com- plexity does not compensate for the loss of expressiveness. A 2 nd order tensor has al- ready reduced the number of parameters from the original 1.33 million to only 2310, and it does not help to further reduce the number of parameters using higher order tensors.</p><p>4. As the amount of training data increases, there is a trend that the best results come from models with more rank-1 component tensors. Adding more rank-1 tensors increases the model's complexity and ability of expression, making the model more adaptive to larger data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this paper, we reformulated the traditional lin- ear vector-space models as tensor-space models, and proposed an online learning algorithm named Tensor-MIRA. A tensor-space model is a com- pact representation of data, and via rank-1 ten- sor approximation, the weight tensor can be made highly structured hence the number of parame- ters to be trained is significantly reduced. This can be regarded as a form of model regular- ization.Therefore, compared with the traditional vector-space models, learning in the tensor space is very effective when a large feature set is defined, but only small amount of training data is available.</p><p>Our experimental results corroborated this argu- ment.</p><p>As mentioned in Section 3.2, one interesting problem that merits further investigation is how to determine optimal mode sizes. The challenge of applying a tensor model comes from finding a proper tensor structure for a given problem, and  <ref type="figure">Figure 3</ref>: f -scores given by three algorithms on training and held-out set (see text for the setting).</p><p>the key to solving this problem is to find a bal- ance between the model complexity (indicated by the order and sizes of modes) and the number of parameters. Developing a theoretically guaran- teed approach of finding the optimal structure for a given task will make the tensor model not only perform well in low-resource environments, but adaptive to larger data sets. <ref type="bibr">89</ref>    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A 3 rd order linear tensor model. The feature weight tensor W can be decomposed as the sum of a sequence of rank-1 component tensors.</figDesc><graphic url="image-1.png" coords="3,79.09,62.81,204.09,91.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1</head><label>1</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>−1 n d , and l 0 = 1 (a) Mapping a surrogate weight vector to a tensor (b) Illustration of the algorithm</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1 :Figure 4 :</head><label>14</label><figDesc>Figure 4: The top 20 singular values of the surrogate weight matrices given by two mapping algorithms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgements</head><p>This work was partially supported by IBM via DARPA/BOLT contract number HR0011-12-C-0015 and by the National Science Foundation via award number IIS-0963898. </p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mapping</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approximate</head><p>Sequential <ref type="table">Rank-1 tensors  1  2  3  1  2  3  Tensor order  2  3  4  2  3  4  2  3  4  2  3  4  2  3  4  2  3</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proof of Theorem 1</head><p>Proof. For D = 1, it is obvious that if a set of real numbers {x 1 , . . . , x n } can be represented by a rank-1 matrix, it can always be represented by a vector, but the reverse is not true.</p><p>For D &gt; 1, if {x 1 , . . . , x n } can be repre-</p><p>, then for any compo- nent vector in mode d,</p><p>where n p d is the size of mode d of P, s d j is a con- stant and</p><p>and this representation is unique for a given D(up to the ordering of p j and s d j in p j , which simply assigns {x 1 , . . . , x n } with different indices in the tensor), due to the pairwise proportional constraint imposed by x i /x j , i, j = 1, . . . , n.</p><p>If x i can also be represented by Q, then</p><p>, where t d j has a similar definition as s d j . Then it must be the case that</p><p>. . , x n } would be repre- sented by a different set of factors than those given in Equation <ref type="formula">(7)</ref>. Therefore, in order for tensor Q to represent the same set of real numbers that P represents, there needs to exist a vector</p><p>] that can be represented by a rank-1 matrix as indicated by Equation (8), which is in general not guaranteed.</p><p>On the other hand, if {x 1 , . . . , x n } can be rep- resented by Q, namely Hence {x 1 , . . . , x n } can also be represented by a D th order tensor Q .</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<title level="m">Tensor Space Model for Document Analysis Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval(SIGIR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="625" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning with Tensor Representation Technical Report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Illinois at Urbana-Champaign</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
