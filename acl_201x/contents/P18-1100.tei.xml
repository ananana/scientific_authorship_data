<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:23+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TDNN: A Two-stage Deep Neural Network for Prompt-independent Automated Essay Scoring</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cancan</forename><surname>Jin</surname></persName>
							<email>jincancan15@mails.ucas.ac.cn, benhe@ucas.ac.cn kai.hui@sap.com , sunle@iscas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer &amp; Control Engineering</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer &amp; Control Engineering</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Hui</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">SAP SE</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Sun</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Beijing Advanced Innovation Center for Language Resources</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TDNN: A Two-stage Deep Neural Network for Prompt-independent Automated Essay Scoring</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1088" to="1097"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1088</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Existing automated essay scoring (AES) models rely on rated essays for the target prompt as training data. Despite their successes in prompt-dependent AES, how to effectively predict essay ratings under a prompt-independent setting remains a challenge, where the rated essays for the target prompt are not available. To close this gap, a two-stage deep neural network (TDNN) is proposed. In particular, in the first stage, using the rated essays for non-target prompts as the training data, a shallow model is learned to select essays with an extreme quality for the target prompt, serving as pseudo training data; in the second stage, an end-to-end hybrid deep model is proposed to learn a prompt-dependent rating model consuming the pseudo training data from the first step. Evaluation of the proposed TDNN on the standard ASAP dataset demonstrates a promising improvement for the prompt-independent AES task.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automated essay scoring (AES) utilizes natural language processing and machine learning tech- niques to automatically rate essays written for a target prompt <ref type="bibr" target="#b6">(Dikli, 2006</ref>). Currently, the AES systems have been widely used in large-scale En- glish writing tests, e.g. Graduate Record Exami- nation (GRE), to reduce the human efforts in the writing assessments <ref type="bibr" target="#b1">(Attali and Burstein, 2006</ref>).</p><p>Existing AES approaches are prompt- dependent, where, given a target prompt, rated essays for this particular prompt are required for training <ref type="bibr" target="#b6">(Dikli, 2006;</ref><ref type="bibr" target="#b27">Williamson, 2009;</ref><ref type="bibr" target="#b9">Foltz et al., 1999</ref>). While the established models are effective <ref type="bibr" target="#b3">(Chen and He, 2013;</ref><ref type="bibr" target="#b24">Taghipour and Ng, 2016;</ref><ref type="bibr" target="#b0">Alikaniotis et al., 2016;</ref><ref type="bibr" target="#b5">Cummins et al., 2016;</ref>, we argue that the models for prompt-independent AES are also desirable to allow for better feasibility and flexibility of AES systems especially when the rated essays for a target prompt are difficult to obtain or even unaccessible. For example, in a writing test within a small class, students are asked to write essays for a target prompt without any rated examples, where the prompt-dependent methods are unlikely to provide effective AES due to the lack of training data. Prompt-independent AES, however, has drawn little attention in the literature, where there only exists unrated essays written for the target prompt, as well as the rated essays for several non-target prompts.</p><p>We argue that it is not straightforward, if possible, to apply the established prompt- dependent AES methods for the mentioned prompt-independent scenario. On one hand, es- says for different prompts may differ a lot in the uses of vocabulary, the structure, and the gram- matic characteristics; on the other hand, howev- er, established prompt-dependent AES models are designed to learn from these prompt-specific fea- tures, including the on/off-topic degree, the tf - idf weights of topical terms <ref type="bibr" target="#b1">(Attali and Burstein, 2006;</ref><ref type="bibr" target="#b6">Dikli, 2006</ref>), and the n-gram features ex- tracted from word semantic embeddings ( <ref type="bibr" target="#b7">Dong and Zhang, 2016;</ref><ref type="bibr" target="#b0">Alikaniotis et al., 2016)</ref>. Conse- quently, the prompt-dependent models can hardly learn generalized rules from rated essays for non- target prompts, and are not suitable for the prompt- independent AES.</p><p>Being aware of this difficulty, to this end, a two- stage deep neural network, coined as TDNN, is proposed to tackle the prompt-independent AES problem. In particular, to mitigate the lack of the prompt-dependent labeled data, at the first stage, a shallow model is trained on a number of rated essays for several non-target prompts; given a tar- get prompt and a set of essays to rate, the trained model is employed to generate pseudo training da- ta by selecting essays with the extreme quality. At the second stage, a novel end-to-end hybrid deep neural network learns prompt-dependent features from these selected training data, by considering semantic, part-of-speech, and syntactic features.</p><p>The contributions in this paper are threefold: 1) a two-stage learning framework is proposed to bridge the gap between the target and non-target prompts, by only consuming rated essays for non- target prompts as training data; 2) a novel deep model is proposed to learn from pseudo labels by considering semantic, part-of-speech, and syntac- tic features; and most importantly, 3) to the best of our knowledge, the proposed TDNN is actual- ly the first approach dedicated to addressing the prompt-independent AES. Evaluation on the stan- dard ASAP dataset demonstrates the effectiveness of the proposed method.</p><p>The rest of this paper is organized as follows. In Section 2, we describe our novel TDNN mod- el, including the two-stage framework and the pro- posed deep model. Following that, we describe the setup of our empirical study in Section 3, there- after present the results and provide analyzes in Section 4. Section 5 recaps existing literature and put our work in context, before drawing final con- clusions in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Two-stage Deep Neural Network for AES</head><p>In this section, the proposed two-stage deep neu- ral network (TDNN) for prompt-independent AES is described. To accurately rate an essay, on one hand, we need to consider its pertinence to the giv- en prompt; on the other hand, the organization, the analyzes, as well as the uses of the vocabu- lary are all crucial for the assessment. Henceforth, both prompt-dependent and -independent factors should be considered, but the latter ones actual- ly do not require prompt-dependent training da- ta. Accordingly, in the proposed framework, a supervised ranking model is first trained to learn from prompt-independent data, hoping to rough- ly assess essays without considering the promp- t; subsequently, given the test dataset, namely, a set of essays for a target prompt, a subset of es- says are selected as positive and negative training data based on the prediction of the trained model from the first stage; ultimately, a novel deep mod- el is proposed to learn both prompt-dependent and -independent factors on this selected subset. As indicated in <ref type="figure" target="#fig_3">Figure 1</ref>, the proposed framework in- cludes two stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview</head><p>Figure 1: The architecture of the TDNN frame- work for prompt-independent AES.</p><p>Prompt-independent stage. Only the prompt- independent factors are considered to train a shal- low model, aiming to recognize the essays with the extreme quality in the test dataset, where the rated essays for non-target prompts are used for training. Intuitively, one could recognize essays with the highest and the lowest scores correctly by solely examining their quality of writing, e.g., the num- ber of typos, without even understanding them, and the prompt-independent features such as the number of grammatic and spelling errors should be sufficient to fulfill this screening procedure. Accordingly, a supervised model trained solely on prompt-independent features is employed to i- dentify the essays with the highest and lowest s- cores in a given set of essays for the target prompt, which are used as the positive and negative train- ing data in the follow-up prompt-dependent learn- ing phase.</p><p>Prompt-dependent stage. Intuitively, most es- says are with a quality in between the extremes, re- quiring a good understanding of their meaning to make an accurate assessment, e.g., whether the ex- amples from the essay are convincing or whether the analyzes are insightful, making the consider- ation of prompt-dependent features crucial. To achieve that, a model is trained to learn from the comparison between essays with the highest and lowest scores for the target prompt according to the predictions from the first step. Akin to the settings in transductive transfer learning <ref type="bibr" target="#b17">(Pan and Yang, 2010)</ref>, given essays for a particular prompt, quite a few confident essays at two extremes are selected and are used to train another model for a fine-grained content-based prompt-dependent as- sessment. To enable this, a powerful deep mod- el is proposed to consider the content of the es- says from different perspectives using semantic, part-of-speech (POS) and syntactic network. Af- ter being trained with the selected essays, the deep model is expected to memorize the properties of a good essay in response to the target prompt, there- after accurately assessing all essays for it. In Sec- tion 2.2, building blocks for the selection of the training data and the proposed deep model are de- scribed in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Building Blocks</head><p>Select confident essays as training data. The i- dentification of the extremes is relatively simple, where a RankSVM <ref type="bibr" target="#b11">(Joachims, 2002</ref>) is trained on essays for different non-target prompts, avoiding the risks of over-fitting some particular prompts. A set of established prompt-independent features are employed, which are listed in <ref type="table">Table 2</ref>. Giv- en a prompt and a set of essays for evaluation, to begin with, the trained RankSVM is used to as- sign prediction scores to individual prompt-essay pairs, which are uniformly transformed into a 10- point scale. Thereafter, the essays with predict- ed scores in <ref type="bibr">[0,</ref><ref type="bibr">4]</ref> and <ref type="bibr">[8,</ref><ref type="bibr">10]</ref> are selected as nega- tive and positive examples respectively, serving as the bad and good templates for training in the nex- t stage. Intuitively, an essay with a score beyond eight out of a 10-point scale is considered good, while the one receiving less than or equal to four, is considered to be with a poor quality.</p><p>A hybrid deep model for fine-grained assess- ment. To enable a prompt-dependent assessmen- t, a model is desired to comprehensively capture the ways in which a prompt is described or dis- cussed in an essay. In this paper, semantic mean- ing, part-of-speech (POS), and the syntactic tag- gings of the token sequence from an essay are considered, grasping the quality of an essay for a target prompt. The model architecture is sum- marized in <ref type="figure" target="#fig_0">Figure 2</ref>. Intuitively, the model learns the semantic meaning of an essay by encoding it in terms of a sequence of word embeddings, de- noted as − → e sem , hoping to understand what the essay is about; in addition, the part-of-speech in- formation is encoded as a sequence of POS tag- gings, coined as − → e pos ; ultimately, the structural connections between different components in an essay (e.g., terms or phrases) are further captured via syntactic network, leading to − → e synt , where the model learns the organization of the essay. Akin to ( <ref type="bibr" target="#b14">Li et al., 2015)</ref> and ( <ref type="bibr" target="#b31">Zhou and Xu, 2015)</ref>, bi- LSTM is employed as a basic component to en- code a sequence. Three features are separate- ly captured using the stacked bi-LSTM layers as building blocks to encode different embeddings, whose outputs are subsequently concatenated and fed into several dense layers, generating the ulti- mate rating. In the following, the architecture of the model is described in details.</p><p>-Semantic embedding. Akin to the existing works ( <ref type="bibr" target="#b0">Alikaniotis et al., 2016;</ref><ref type="bibr" target="#b24">Taghipour and Ng, 2016)</ref>, semantic word embeddings, namely, the pre-trained 50-dimension GloVe ( <ref type="bibr" target="#b18">Pennington et al., 2014</ref>), are employed. On top of the word embeddings, two bi-LSTM layers are stacked, namely, the essay layer is constructed on top of the sentence layer, ending up with the semantic repre- sentation of the whole essay, which is denoted as − → e sem in <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>-Part-Of-Speech (POS) embeddings for individu- al terms are first generated by the Stanford Tag- ger ( <ref type="bibr" target="#b26">Toutanova et al., 2003)</ref>, where 36 differen- t POS tags present. Accordingly, individual words are embedded with 36-dimensional one-hot repre- sentation, and is transformed to a 50-dimensional vector through a lookup layer. After that, two bi- LSTM layers are stacked, leading to − → e pos . Take <ref type="figure" target="#fig_1">Figure 3</ref> for example, given a sentence "Attention please, here is an example.", it is first converted in- to a POS sequence using the tagger, namely, VB, VBP, RB, VBZ, DT, NN; thereafter it is further mapped to vector space through one-hot embed- ding and a lookup layer.</p><p>-Syntactic embedding aims at encoding an essay in terms of the syntactic relationships among d- ifferent syntactic components, by encoding an es- say recursively. The Stanford Parser ( <ref type="bibr" target="#b21">Socher et al., 2013</ref>) is employed to label the syntactic structure of words and phrases in sentences, accounting for 59 different types in total. Similar to <ref type="bibr" target="#b25">(Tai et al., 2015)</ref>, we opt for three stacked bi-LSTM, aiming at encoding individual phrases, sentences, and ul- timately the whole essay in sequence. In partic- ular, according to the hierarchical structure from a parsing tree, the phrase-level bi-LSTM first en- codes different phrases by consuming syntactic  <ref type="figure" target="#fig_0">Figure 2</ref>) from a lookup ta- ble of individual syntactic units in the tree; there- after, the encoded dense layers in individual sen- tences are further consumed by a sentence-level bi-LSTM, ending up with sentence-level syntac- tic representations, which are ultimately combined by the essay-level bi-LSTM, resulting in − → e synt . For example, the parsed tree for a sentence "At- tention please, here is an example." is displayed in <ref type="figure" target="#fig_1">Figure 3</ref>. To start with, the sentence is parsed into ((NP VP)(NP VP NP)), and the dense embed- dings are fetched from a lookup table for all to- kens, namely, NP and VP; thereafter, the phrase- level bi-LSTM encodes (NP VP) and (NP VP N- P) separately, which are further consumed by the sentence-level bi-LSTM. Afterward, essay-level bi-LSTM further combines the representations of different sentences into − → e synt . -Combination. A feed-forward network linearly transforms the concatenated representations of an essay from the mentioned three perspectives into a scalar, which is further normalized into <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> with a sigmoid function.</p><formula xml:id="formula_0">embeddings ( − → St i in</formula><formula xml:id="formula_1">(ROOT (S (S (NP (VB Attention)) (VP (VBP please))) (, ,) (NP (RB here)) (VP (VBZ is) (NP (DT an) (NN example))) (. .)))</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Objective and Training</head><p>Objective. Mean square error (MSE) is opti- mized, which is widely used as a loss function in regression tasks. Given N pairs of a target promp- t p i and an essay e i , MSE measures the average value of square error between the normalized gold standard rating r * (p i , e i ) and the predicted rating r(p i , e i ) assigned by the AES model, as summa- rized in Equation 1.</p><formula xml:id="formula_2">1 N N ∑ i=1 ( r(p i , e i ) − r * (p i , e i ) ) 2<label>(1)</label></formula><p>Optimization. Adam ( <ref type="bibr" target="#b12">Kingma and Ba, 2014</ref>) is employed to minimize the loss over the train- ing data. The initial learning rate η is set to 0.01 and the gradient is clipped between [−10, 10] during training. In addition, dropout <ref type="bibr" target="#b23">(Srivastava et al., 2014</ref>) is introduced for regularization with a dropout rate of 0.5, and 64 samples are used in each batch with batch normalization <ref type="bibr" target="#b10">(Ioffe and Szegedy, 2015</ref>). 30% of the training data are reserved for validation. In addition, early stop- ping ( <ref type="bibr" target="#b29">Yao et al., 2007</ref>) is employed according to the validation loss, namely, the training is termi- nated if no decrease of the loss is observed for ten consecutive epochs. Once training is finished, Prompt #Essays Avg Length Score <ref type="table" target="#tab_1">Range   1  1783  350  2-12  2  1800  350  1-6  3  1726  150  0-3  4  1772  150  0-3  5  1805  150  0-4  6  1800  150  0-4  7  1569  250  0-30  8  723  650</ref> 0-60 <ref type="table">Table 1</ref>: Statistics for the ASAP dataset.</p><p>akin to ( , the model with the best quadratic weighted kappa on the validation set is selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Setup</head><p>Dataset. The Automated Student Assessmen- t Prize (ASAP) dataset has been widely used for AES ( <ref type="bibr" target="#b0">Alikaniotis et al., 2016;</ref><ref type="bibr" target="#b3">Chen and He, 2013;</ref>, and is also employed as the prime evaluation instrument herein. In total, AS- AP consists of eight sets of essays, each of which associates to one prompt, and is originally written by students between Grade 7 and Grade 10. As summarized in <ref type="table">Table 1</ref>, essays from different sets differ in their rating criteria, length, as well as the rating distribution 1 .</p><p>Cross-validation. To fully employ the rated data, a prompt-wise eight-fold cross validation on the ASAP is used for evaluation. In each fold, essays corresponding to a prompt is reserved for testing, and the remaining essays are used as training data. Evaluation metric. The model outputs are first u- niformly re-scaled into <ref type="bibr">[0,</ref><ref type="bibr">10]</ref>, mirroring the range of ratings in practice. Thereafter, akin to <ref type="bibr" target="#b28">(Yannakoudakis et al., 2011;</ref><ref type="bibr" target="#b3">Chen and He, 2013;</ref><ref type="bibr" target="#b0">Alikaniotis et al., 2016)</ref>, we report our results pri- marily based on the quadratic weighted Kappa (QWK), examining the agreement between the predicted ratings and the ground truth. Pearson correlation coefficient (PCC) and Spearman rank- order correlation coefficient (SCC) are also re- ported. The correlations obtained from individual folds, as well as the average over all eight folds, are reported as the ultimate results.</p><p>Competing models.</p><p>Since the prompt- independent AES is of interests in this work, the existing AES models are adapted for prompt-independent rating prediction, serving as baselines. This is due to the facts that the  Mean &amp; variance of word length in characters 2</p><p>Mean &amp; variance of sentence length in words 3</p><p>Essay length in characters and words 4</p><p>Number of prepositions and commas 5</p><p>Number of unique words in an essay 6</p><p>Mean number of clauses per sentence 7</p><p>Mean length of clauses 8</p><p>Maximum number of clauses of a sentence in an essay 9</p><p>Number of spelling errors 10</p><p>Average depth of the parser tree of each sen- tence in an essay 11</p><p>Average depth of each leaf node in the parser tree of each sentence <ref type="table">Table 2</ref>: Handcrafted features used in learning the prompt-independent RankSVM.</p><p>prompt-dependent and -independent models differ a lot in terms of problem settings and model designs, especially in their requirements for the training data, where the latter ones release the prompt-dependent requirements and thereby are accessible to more data. -RankSVM, using handcrafted features for AES <ref type="bibr" target="#b28">(Yannakoudakis et al., 2011;</ref><ref type="bibr" target="#b4">Chen et al., 2014)</ref>, is trained on a set of pre-defined prompt- independent features as listed in <ref type="table">Table 2</ref>, where the features are standardized beforehand to remove the mean and variance. The RankSVM is also used for the prompt-independent stage in our pro- posed TDNN model. In particular, the linear ker- nel RankSVM 2 is employed, where C is set to 5 according to our pilot experiments. -2L-LSTM. Two-layer bi-LSTM with GloVe for AES ( <ref type="bibr" target="#b0">Alikaniotis et al., 2016</ref>) is employed as an- other baseline. Regularized word embeddings are dropped to avoid over-fitting the prompt-specific features. -CNN-LSTM. This model (Taghipour and Ng, 2016) employs a convolutional (CNN) layer over one-hot representations of words, followed by an LSTM layer to encode word sequences in a given essay. A linear layer with sigmoid activation func- tion is then employed to predict the essay rating. -CNN-LSTM-ATT. This model ( ) employs a CNN layer to encode word se- quences into sentences, followed by an LSTM lay- er to generate the essay representation. An atten- tion mechanism is added to model the influence of individual sentences on the final essay representa- tion.</p><p>For the proposed TDNN model, as introduced in Section 2.2, different variants of TDNN are ex- amined by using one or multiple components out of the semantic, POS and the syntactic network- s. The combinations being considered are listed in the following. In particular, the dimensions of POS tags and syntactic network are fixed to 50, whereas the sizes of the hidden units in LSTM, as well as the output units of the linear layers are tuned by grid search. -TDNN(Sem) only includes the semantic build- ing block, which is similar to the two-layer LSTM neural network from (Alikaniotis et al., 2016) but without regularizing the word embeddings; -TDNN(Sem+POS) employs the semantic and the POS building blocks; -TDNN(Sem+Synt) uses the semantic and the syntactic network building blocks; -TDNN(POS+Synt) includes the POS and the syntactic network building blocks; -TDNN(ALL) employs all three building blocks.</p><p>The use of POS or syntactic network alone is not presented for brevity given the facts that they perform no better than TDNN(POS+Synt) in our pilot experiments. Source code of the TDNN mod- el is publicly available to enable further compari- son 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Analyzes</head><p>In this section, the evaluation results for differ- ent competing methods are compared and ana- lyzed in terms of their agreements with the manu- al ratings using three correlation metrics, namely, QWK, PCC and SCC, where the best results for each prompt is highlighted in bold in <ref type="table" target="#tab_1">Table 3</ref>.</p><p>It can be seen that, for seven out of all eight prompts, the proposed TDNN variants outperfor- m the baselines by a margin in terms of QWK, and the TDNN variant with semantic and syn- tactic features, namely, TDNN(Sem+Synt), con- sistently performs the best among different com- peting methods. More precisely, as indicated in the bottom right corner in <ref type="table" target="#tab_1">Table 3</ref>, on average, TDNN(Sem+Synt) outperforms the baselines by at least 25.52% under QWK, by 10.28% under PCC, and by 15.66% under SCC, demonstrating that the proposed model not only correlates bet- ter with the manual ratings in terms of QWK, but also linearly (PCC) and monotonically (SCC) cor- relates better with the manual ratings. As for the four baselines, note that, the relatively underper- formed deep models suffer from larger variances of performance under different prompts, e.g., for prompts two and eight, 2L-LSTM's QWK is low- er than 0.3. This actually confirms our choice of RankSVM for the first stage in TDNN, since a more complicated model (like 2L-LSTM) may end up with learning prompt-dependent signals, making it unsuitable for the prompt-independent rating prediction. As a comparison, RankSVM performs more stable among different prompts.</p><p>As for the different TDNN variants, it turns out that the joint uses of syntactic network with se- mantic or POS features can lead to better perfor- mances. This indicates that, when learning the prompt-dependent signals, apart from the widely- used semantic features, POS features and the sen- tence structure taggings (syntactic network) are al- so essential in learning the structure and the ar- rangement of an essay in response to a particu- lar prompt, thereby being able to improve the re- sults. It is also worth mentioning, however, when using all three features, the TDNN actually per- forms worse than when only using (any) two fea- tures. One possible explanation is that the uses of all three features result in a more complicated model, which over-fits the training data.</p><p>In addition, recall that the prompt-independent RankSVM model from the first stage enables the proposed TDNN in learning prompt-dependent in- formation without manual ratings for the target prompt. Therefore, one would like to understand how good the trained RankSVM is in feeding training data for the model in the second stage. In particular, the precision, recall and F-score (P/R/F) of the essays selected by RanknSVM, namely, the negative ones rated between <ref type="bibr">[0,</ref><ref type="bibr">4]</ref>, and the positive ones rated between <ref type="bibr">[8,</ref><ref type="bibr">10]</ref>, are dis- played in <ref type="figure" target="#fig_4">Figure 4</ref>. It can be seen that the P/R/F scores of both positive and negative classes differ a lot among different prompts. Moreover, it turns out that the P/R/F scores do not necessarily cor- relate with the performance of the TDNN model. Take TDNN(Sem+Synt), the best TDNN variant, as an example: as indicated in <ref type="table" target="#tab_2">Table 4</ref>, the perfor- mance and the P/R/F scores of the pseudo exam- ples are only weakly correlated in most cases.</p><p>To gain a better understanding in how the qual- ity of pseudo examples affects the performance of TDNN, the sanctity of the selected essays are examined. In <ref type="figure" target="#fig_5">Figure 5</ref>, the relative precision of    the selected positive and negative training data by RankSVM are displayed for all eight prompts in terms of their concordance with the manual rat- ings, by computing the number of positive (nega- tive) essays that are better (worse) than all negative (positive) essays. It can be seen that, such relative precision is at least 80% and mostly beyond 90% on different prompts, indicating that the overlap of the selected positive and negative essays are fairly small, guaranteeing that the deep model in the sec- ond stage at least learns from correct labels, which are crucial for the success of our TDNN model.</p><note type="other">Eval. Metric QWK PCC SCC QWK PCC SCC QWK PCC SCC Method Prompt 1 Prompt 2 Prompt</note><p>Beyond that, we further investigate the class balance of the selected training data from the first  stage, which could also influence the ultimate re- sults. The number of selected positive and neg- ative essays are reported in <ref type="table" target="#tab_3">Table 5</ref>, where for prompts three and eight the training data suffer- s from serious imbalanced problem, which may explain their lower performance (namely, the two lowest QWKs among different prompts). On one hand, this is actually determined by real distribu- tion of ratings for a particular prompt, e.g., how many essays are with an extreme quality for a giv- en prompt in the target data. On the other hand, a fine-grained tuning of the RankSVM (e.g., tun- ing C + and C − for positive and negative exam- ples separately) may partially resolve the problem, which is left for the future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Classical regression and classification algorithm- s are widely used for learning the rating mod- el based on a variety of text features including lexical, syntactic, discourse and semantic features <ref type="bibr" target="#b13">(Larkey, 1998;</ref><ref type="bibr" target="#b20">Rudner, 2002;</ref><ref type="bibr" target="#b1">Attali and Burstein, 2006;</ref><ref type="bibr">Mcnamara et al., 2015;</ref><ref type="bibr" target="#b19">Phandi et al., 2015)</ref>. There are also approaches that see AES as a pref- erence ranking problem by applying learning to ranking algorithms to learn the rating model. Re- sults show improvement of learning to rank ap- proaches over classical regression and classifica- tion algorithms <ref type="bibr" target="#b4">(Chen et al., 2014;</ref><ref type="bibr" target="#b28">Yannakoudakis et al., 2011</ref>). In addition, Chen &amp; He propose to incorporate the evaluation metric into the loss function of listwise learning to rank for AES <ref type="bibr" target="#b3">(Chen and He, 2013)</ref>. Recently, there have been efforts in develop- ing AES approaches based on deep neural net- works (DNN), for which feature engineering is not required. Taghipour &amp; Ng explore a variety of neural network model architectures based on recurrent neural networks which can effectively encode the information required for essay scor- ing and learn the complex connections in the da- ta through the non-linear neural layers <ref type="bibr" target="#b24">(Taghipour and Ng, 2016)</ref>. Alikaniotis et al. introduce a neu- ral network model to learn the extent to which spe- cific words contribute to the text's score, which is embedded in the word representations. Then a two-layer bi-directional Long-Short Term Memo- ry networks (bi-LSTM) is used to learn the mean- ing of texts, and finally the essay score is predict- ed through a mutli-layer feed-forward network <ref type="bibr" target="#b0">(Alikaniotis et al., 2016)</ref>. Dong &amp; Zhang employ a hierarchical convolutional neural network (CN- N) model, with a lower layer representing sen- tence structure and an upper layer representing es- say structure based on sentence representations, to learn features automatically <ref type="bibr" target="#b7">(Dong and Zhang, 2016)</ref>. This model is later improved by employ- ing attention layers. Specifically, the model learns text representation with LSTMs which can model the coherence and co-reference among sequences of words and sentences, and uses attention pool- ing to capture more relevant words and sentences that contribute to the final quality of essays ( ). Song et al. propose a deep model for identifying discourse modes in an essay ( <ref type="bibr" target="#b22">Song et al., 2017)</ref>.</p><p>While the literature has shown satisfactory performance of prompt-dependent AES, how to achieve effective essay scoring in a prompt- independent setting remains to be explored. Chen &amp; He studied the usefulness of prompt- independent text features and achieved a human- machine rating agreement slightly lower than the use of all text features <ref type="bibr" target="#b3">(Chen and He, 2013</ref>) for prompt-dependent essay scoring prediction. A constrained multi-task pairwise preference learn- ing approach was proposed in ( <ref type="bibr" target="#b5">Cummins et al., 2016</ref>) to combine essays from multiple prompt- s for training. However, as shown by <ref type="bibr" target="#b7">(Dong and Zhang, 2016;</ref><ref type="bibr" target="#b30">Zesch et al., 2015;</ref><ref type="bibr" target="#b19">Phandi et al., 2015)</ref>, straightforward applications of existing AES methods for prompt-independent AES lead to a poor performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions &amp; Future Work</head><p>This study aims at addressing the prompt- independent automated essay scoring (AES), where no rated essay for the target prompt is avail- able. As demonstrated in the experiments, two kinds of established prompt-dependent AES mod- els, namely, RankSVM for AES <ref type="bibr" target="#b28">(Yannakoudakis et al., 2011;</ref><ref type="bibr" target="#b4">Chen et al., 2014</ref>) and the deep mod- els for AES ( <ref type="bibr" target="#b0">Alikaniotis et al., 2016;</ref><ref type="bibr" target="#b24">Taghipour and Ng, 2016;</ref>, fail to pro- vide satisfactory performances, justifying our ar- guments in Section 1 that the application of estab- lished prompt-dependent AES models on prompt- independent AES is not straightforward. There- fore, a two-stage TDNN learning framework was proposed to utilize the prompt-independent fea- tures to generate pseudo training data for the target prompt, on which a hybrid deep neural network model is proposed to learn a rating model consum- ing semantic, part-of-speech, and syntactic signal- s. Through the experiments on the ASAP dataset, the proposed TDNN model outperforms the base- lines, and leads to promising improvement in the human-machine agreement.</p><p>Given that our approach in this paper is simi- lar to the methods for transductive transfer learn- ing <ref type="bibr" target="#b17">(Pan and Yang, 2010)</ref>, we argue that the pro- posed TDNN could be further improved by mi- grating the non-target training data to the target prompt <ref type="bibr" target="#b2">(Busto and Gall, 2017)</ref>. Further study of the uses of transfer learning algorithms on prompt- independent AES needs to be undertaken.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The model architecture of the proposed hybrid deep learning model.</figDesc><graphic url="image-2.png" coords="4,72.00,62.41,453.81,212.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An example of the context-free phrase structure grammar tree.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1</head><label>1</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The precision, recall and F-score of the pseudo negative or positive examples, which are rated within [0, 4] or [8, 10] by RankSVM.</figDesc><graphic url="image-5.png" coords="8,46.05,317.69,263.64,197.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The sanctity of the selected positive and negative essays by RankSVM. The x-axis indicates different prompts and the y-axis is the relative precision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Correlations between AES and manual ratings for different competing methods are reported for 
individual prompts. The average results among different prompts are summarized in the bottom right. 
The best results are highlighted in bold for individual prompts. 

Neg/Pos Metric 
QWK 
PCC 
SCC 

[0, 4] 

Precision +0.5151 +0.4286 +0.4471 
Recall 
-0.2362 -0.1363 -0.3491 
F-score 
+0.4135 +0.4062 +0.1703 

[8, 10] 

Precision +0.3526 +0.3224 +0.3885 
Recall 
+0.0063 -0.0415 -0.2112 
F-score 
+0.8339 +0.6905 +0.4221 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Linear correlations between the performance of 

TDNN(Sem+Synt) and the precision, recall, and F-score of 
the selected pseudo examples. 

Prpt 1 
2 
3 
4 
5 
6 
7 
8 

Neg 191 245 847 428 501 209 454 60 
Pos 
623 470 65 
295 277 426 267 418 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>The numbers of the selected positive and negative 

essays for each prompt. 

</table></figure>

			<note place="foot" n="1"> Details of this dataset can be found at https://www. kaggle.com/c/asap-aes.</note>

			<note place="foot" n="2"> http://svmlight.joachims.org/</note>

			<note place="foot" n="3"> https://github.com/ucasir/TDNN4AES</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported in part by the National Nat-ural Science Foundation of China <ref type="formula">(61472391)</ref>, and the Project of Beijing Advanced Innovation Center for Language Resources (451122512).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automatic text scoring using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Alikaniotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Rei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1). The Association for Computer Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automated essay scoring with e-rater R ⃝ v. 2. The Journal of Technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Attali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Burstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Learning and Assessment</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Open set domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panareda</forename><surname>Pau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Busto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="754" to="763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automated essay scoring by maximizing human-machine agreement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP. ACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1741" to="1752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automated essay scoring by capturing relative writing quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. J</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1318" to="1330" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Constrained multi-task learning for automated essay scoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>The Association for Computer Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An overview of automated scoring of essays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Technology, Learning and Assessment</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatic features for essay scoring-an empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP. The Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1072" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Attentionbased recurrent convolutional neural network for automatic essay scoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL. Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="153" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automated essay scoring: Applications to educational technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename><surname>Peter W Foltz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas K</forename><surname>Laham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Landauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">World Conference on Educational Multimedia, Hypermedia and Telecommunications</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">1999</biblScope>
			<biblScope unit="page" from="939" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. JMLR.org</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Optimizing search engines using clickthrough data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><forename type="middle">Joachims</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD. ACM</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="133" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>ab- s/1412.6980</idno>
		<ptr target="http://arxiv.org/abs/1412.6980" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatic essay grading using text categorization techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Larkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR. ACM</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="90" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<title level="m">When are tree structures necessary for deep learning of representations? In EMNLP. The Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2304" to="2314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danielle</forename><forename type="middle">S</forename><surname>Mcnamara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">A</forename><surname>Crossley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rod</forename><forename type="middle">D</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A hierarchical classification approach to automated essay scoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><forename type="middle">K</forename><surname>Roscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Assessing Writing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="35" to="59" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP. ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Flexible domain adaptation for automated essay scoring using correlated linear regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Phandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kian Ming Adam</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP. The Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="431" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automated essay scoring using bayes&apos; theorem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rudner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">National Council on Measurement in Education New Orleans La</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="3" to="21" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Parsing with compositional vector grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1). The Association for Computer Linguistics</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Discourse mode identification in essays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiji</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1). Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="112" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A neural approach to automated essay scoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaveh</forename><surname>Taghipour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP. The Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1882" to="1891" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1). The Association for Computer Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Feature-rich part-ofspeech tagging with a cyclic dependency network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<publisher>The Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A framework for implementing automated scoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the American Educational Research Association and the National Council on Measurement in Education</title>
		<meeting><address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A new dataset and method for automatically grading ESOL texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Medlock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL. The Association for Computer Linguistics</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On early stopping in gradient descent learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Caponnetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Constructive Approximation</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="289" to="315" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Task-independent features for automated essay grading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Zesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wojatzki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Scholtenakoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BEA@NAACL-HLT. The Association for Computer Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="224" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">End-to-end learning of semantic role labeling using recurrent neural networks. In ACL (1). The Association for Computer Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1127" to="1137" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
