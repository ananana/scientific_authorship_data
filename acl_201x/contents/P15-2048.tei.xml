<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:52+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Embedding Methods for Fine Grained Entity Type Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
							<email>dyogatama@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Language Technologies Institute Carnegie Mellon University Pittsburgh</orgName>
								<orgName type="institution" key="instit2">Google Research</orgName>
								<address>
									<addrLine>1600 Amphitheatre Parkway Mountain View</addrLine>
									<postCode>15213, 94043</postCode>
									<region>PA, CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Language Technologies Institute Carnegie Mellon University Pittsburgh</orgName>
								<orgName type="institution" key="instit2">Google Research</orgName>
								<address>
									<addrLine>1600 Amphitheatre Parkway Mountain View</addrLine>
									<postCode>15213, 94043</postCode>
									<region>PA, CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nevena</forename><surname>Lazic</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Language Technologies Institute Carnegie Mellon University Pittsburgh</orgName>
								<orgName type="institution" key="instit2">Google Research</orgName>
								<address>
									<addrLine>1600 Amphitheatre Parkway Mountain View</addrLine>
									<postCode>15213, 94043</postCode>
									<region>PA, CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Embedding Methods for Fine Grained Entity Type Classification</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers)</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers) <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="291" to="296"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a new approach to the task of fine grained entity type classifications based on label embeddings that allows for information sharing among related labels. Specifically, we learn an embedding for each label and each feature such that labels which frequently co-occur are close in the embedded space. We show that it out-performs state-of-the-art methods on two fine grained entity-classification benchmarks and that the model can exploit the finer-grained labels to improve classification of standard coarse types.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Entity type classification is the task of assign- ing type labels (e.g., person, location, organization) to mentions of entities in doc- uments. These types are useful for deeper natural language analysis such as coreference resolution <ref type="bibr" target="#b9">(Recasens et al., 2013)</ref>, relation extraction ( <ref type="bibr" target="#b15">Yao et al., 2010)</ref>, and downstream applications such as knowledge base construction <ref type="bibr" target="#b0">(Carlson et al., 2010)</ref> and question answering ( <ref type="bibr" target="#b6">Lin et al., 2012)</ref>.</p><p>Standard entity type classification tasks use a small set of coarse labels, typically fewer than 20 <ref type="bibr" target="#b5">(Hirschman and Chinchor, 1997;</ref><ref type="bibr" target="#b10">Sang and Meulder, 2003;</ref><ref type="bibr" target="#b2">Doddington et al., 2004</ref>). Recent work has focused on a much larger set of fine grained labels ( <ref type="bibr" target="#b7">Ling and Weld, 2012;</ref><ref type="bibr" target="#b16">Yosef et al., 2012;</ref><ref type="bibr" target="#b4">Gillick et al., 2014</ref>). Fine grained labels are typ- ically subtypes of the standard coarse labels (e.g., artist is a subtype of person and author is a subtype of artist), so the label space forms a tree-structured is-a hierarchy. See <ref type="figure" target="#fig_0">Figure 1</ref> for the label sets used in our experiments. A mention la- beled with type artist should also be labeled with all ancestors of artist. Since we allow mentions to have multiple labels, this is a multi- label classification task. Multiple labels typically correspond to a single path in the tree (from root to a leaf or internal node).</p><p>An important aspect of context-dependent fine grained entity type classification is that mentions of an entity can have different types depending on the context. Consider the following example: Madonna starred as Breathless Mahoney in the film Dick Tracy. In this context, the most appropri- ate label for the mention Madonna is actress, since the sentence talks about her role in a film. In the majority of other cases, Madonna is likely to be labeled as a musician.</p><p>The main difficulty in fine grained entity type classification is the absence of labeled training ex- amples. Training data is typically generated au- tomatically (e.g. by mapping Freebase labels of resolved entities), without taking context into ac- count, so it is common for mentions to have noisy labels. In our example, the labels for the mention Madonna would include musician, actress, author, and potentially others, even though not all of these labels apply here. Ideally, a fine grained type classification system should be ro- bust to such noisy training data, as well as capable of exploiting relationships between labels during learning. We describe a model that uses a rank- ing loss-which tends to be more robust to la- bel noise-and that learns a joint representation of features and labels, which allows for information sharing among related labels. 1 A related idea to learn output representations for multiclass docu- ment classification and part-of-speech tagging was considered in <ref type="bibr" target="#b11">Srikumar and Manning (2014)</ref>. We show that it outperforms state-of-the-art methods on two fine grained entity-classification bench- marks. We also evaluate our model on standard coarse type classification and find that training em- bedding models on all fine grained labels gives better results than training it on just the coarse types of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Models</head><p>In this section, we describe our approach, which is based on the WSABIE ) model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Notation</head><p>We use lower case letters to denote variables, bold lower case letters to denote vectors, and bold upper case letters to denote matrices. Let x ∈ R D be the feature vector for a mention, where D is the number of features and x d is the value of the d-th feature. Let y ∈ {0, 1} T be the corre- sponding binary label vector, where T is the num- ber of labels. y t = 1 if and only if the mention is of type t. We use y t to denote a one-hot binary vector of size T , where y t = 1 and all other entries are zero.</p><p>Model To leverage the relationships among the fine grained labels, we would like a model that can learn an embedding space for labels. Our model, based on WSABIE, learns to map both feature vec- tors and labels to a low dimensional space R H (H is the embedding dimension size) such that each instance is close to its label(s) in this space; see <ref type="figure" target="#fig_1">Figure 2</ref> for an illustration. Relationships be- tween labels are captured by their distances in the embedded space: co-occurring labels tend to be closer, whereas mutually exclusive labels are fur- ther apart.</p><p>Formally, we are interested in learning the map- ping functions:</p><formula xml:id="formula_0">f (x) : R D → R H ∀t ∈ {1, 2, . . . , T }, g(y t ) : {0, 1} T → R H</formula><p>In this work, we parameterize them as linear func- tions f (x, A) = Ax and g(y t , B) = By t , where A ∈ R H×D and B ∈ R H×T are parameters. The score of a label t (represented as a one-hot label vector y t ) and a feature vector x is the dot product between their embeddings:</p><formula xml:id="formula_1">s(x, y t ; A, B) = f (x, A) · g(y t , B) = Ax · By t</formula><p>For brevity, we denote this score by s(x, y t ). Note that the total number of parameters is (D+T )×H, which is typically less than the number of pa- rameters in standard classification models that use regular conjunctions of input features with label classes (e.g., logistic regression) when H &lt; T .</p><p>Learning Since we expect the training data to contain some extraneous labels, we use a ranking loss to encourage the model to place positive la- bels above negative labels without competing with each other. Let Y denote the set of positive labels for a mention, and let ¯ Y denote its complement. Intuitively, we try to rank labels in Y higher than labels in ¯ Y. Specifically, we use the weighted ap- proximate pairwise (WARP) loss of . For a mention {x, y}, the WARP loss is:</p><formula xml:id="formula_2">t∈Y ¯ t∈ ¯ Y R(rank(x, yt)) max(1 − s(x, yt) + s(x, y¯ t ), 0)</formula><p>where rank(x, y t ) is the margin-infused rank of label t:</p><formula xml:id="formula_3">rank(x, y t ) = ¯ t∈ ¯ Y I(1 + s(x, y¯ t ) &gt; s(x, y t )), R(rank(x, y t )</formula><p>) is a function that trans- forms this rank into a weight. In this work, since each mention can have multiple positive labels, we choose to optimize precision at k by setting</p><formula xml:id="formula_4">R(k) = k i=1 1</formula><p>i . Favoring precision over recall in fine grained entity type classification makes sense because if we are not certain about a particular fine grained label for a mention, we should use its an- cestor label in the hierarchy.</p><p>In order to learn the parameters with this WARP loss, we use stochastic (sub)gradient descent.</p><p>Inference During inference, we consider the top-k predicted labels, where k is the maximum depth of the label hierarchy, and greedily remove labels that are not consistent with other labels (i.e., not on the same path of the tree). For example, if the (ordered) top-k labels are person, artist, and location, we output only person and artist as the predicted labels. We use a thresh- old δ such thatˆythatˆ thatˆy t = 1 if s(x, y t ) &gt; δ andˆyandˆ andˆy t = 0 otherwise.</p><p>Kernel extension We extend the WSABIE model to include a weighting function between each feature and label, similar in spirit to . Recall that the WSABIE scoring function is:</p><formula xml:id="formula_5">s(x, y t ) = Ax · By t = d (A d x d ) B t ,</formula><p>where A d and B t denote the col- umn vectors of A and B. We can weight each (feature, label) pair by a kernel function prior to computing the embedding:</p><formula xml:id="formula_6">s(x, y t ) = d K d,t (A d x d ) B t ,</formula><p>where K ∈ R D×T is the kernel matrix. We use a N -nearest neighbor kernel 2 and set K d,t = 1 if A d is one of N -nearest neighbors of the label vector B t , and K d,t = 0 otherwise. In all our experiments, we set N = 200.</p><p>To incorporate the kernel weighting function, we only need to make minor modifications to the learning procedure. At every iteration, we first compute the similarity between each feature em- bedding and each label embedding. For each label t, we then set the kernel values for the N most similar features to 1, and the rest to 0 (update K). We can then follow the learning algorithm for the standard WSABIE model described above. At in- ference time, we fix K so this extension is only slightly slower than the standard model. The nearest-neighbor kernel introduces nonlin- earities to the embedding model. It implicitly plays the role of a label-dependent feature selector, learning which features can interact with which la- bels and turns off potentially noisy features that are not in the relevant label's neighborhood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Setup and Baselines We evaluate our methods on two publicly available datasets that are man- ually annotated with gold labels for fine grained entity type classification: GFT (Google Fine Types; <ref type="bibr" target="#b4">Gillick et al., 2014</ref>) and FIGER ( <ref type="bibr" target="#b7">Ling and Weld, 2012</ref>). On the GFT dataset, we compare with state-of-the-art baselines from <ref type="bibr" target="#b4">Gillick et al. (2014)</ref>: flat logistic regression (FLAT), an exten- sion of multiclass logistic regression for multilabel classification problems; and multiple independent binary logistic regression (BINARY), one per label t ∈ {1, 2, . . . , T }. On the FIGER dataset, we com- pare with a state-of-the-art baseline from <ref type="bibr" target="#b7">Ling and Weld (2012)</ref>.</p><p>We denote the standard embedding method by WSABIE and its extension by K-WSABIE. We fix our embedding size to H = 50. We report micro- averaged precision, recall, and F1-score for each of the competing methods (this is called Loose Mi- cro by Ling and Weld). When development data is available, we use it to tune δ by optimizing F1- score.</p><p>Training data Because we have no manually annotated data, we create training data using the technique described in <ref type="bibr" target="#b4">Gillick et al. (2014)</ref>. A set of 133,000 news documents are automatically an- notated by a parser, a mention chunker, and an entity resolver that assigns Freebase types to en- tites, which we map to fine grained labels. This approach results in approximately 3 million train- ing examples which we use to train all the mod- els evaluated below. The only difference between models trained for different tasks is the mapping from Freebase types. See <ref type="bibr" target="#b4">Gillick et al. (2014)</ref> for details. <ref type="table" target="#tab_0">Table 1</ref> lists the features we use-the same set as used by <ref type="bibr" target="#b4">Gillick et al. (2014)</ref>, and very similar to those used by Ling and Weld. String features are randomly hashed to a value in 0 to 999,999, which simplifies feature extraction and adds some addi- tional regularization ( <ref type="bibr" target="#b3">Ganchev and Dredze, 2008)</ref>. The syntactic head of the mention phrase "Obama" Non-head Each non-head word in the mention phrase "Barack", "H." Cluster Word cluster id for the head word "59" Characters Each character trigram in the mention head ":ob", "oba", "bam", "ama", "ma:" Shape</p><p>The word shape of the words in the mention phrase "Aa A. Aa" Role Dependency label on the mention head "subj" Context Words before and after the mention phrase "B:who", "A:first" Parent The head's lexical parent in the dependency tree "picked" Topic</p><p>The most likely topic label for the document "politics"  GFT evaluation There are T = 86 fine grained labels in the GFT dataset, as listed in <ref type="figure" target="#fig_0">Figure 1</ref>. The four top-level labels are: person, location, organization, and other; the remaining la- bels are subtypes of these labels. The maximum depth of a label is 3. We split the dataset into a development set (for tuning hyperparameters) and test set (see <ref type="table" target="#tab_1">Table 2</ref>). The overall experimental results are shown in <ref type="table">Table 3</ref>. Embedding methods performed well. Both WSABIE and K-WSABIE outperformed the baselines by substantial margins in F1-score, though the advantage of the kernel version over the linear version is only marginally significant.</p><p>To visualize the learned embeddings, we project label embeddings down to two dimensions using PCA in <ref type="figure" target="#fig_3">Figure 3</ref>. Since there are only 4 top-level labels here, the fine grained labels are color-coded according to their top-level labels for readability. We can see that related labels are clustered to- gether, and the four major clusters correspond to to the top-level labels. We note that these first two components only capture 14% of the total variance of the full 50-dimensional space.  <ref type="table">Table 3</ref>: Precision (P), Recall (R), and F1-score on the GFT test dataset for four competing models. The improvements for WSABIE and K-WSABIE over both baselines are statisti- cally significant (p &lt; 0.01). FIGER evaluation Our second evaluation dataset is FIGER from <ref type="bibr" target="#b7">Ling and Weld (2012)</ref>. In this dataset, there are T = 112 labels organized in a two-level hierarchy; however, only 102 appear in our training data (see <ref type="figure" target="#fig_0">Figure 1</ref>, taken from their paper, for the complete set of labels). The training labels include 37 top-level labels (e.g., person, location, product, art, etc.) and 75 second-level labels (e.g., actor, city, engine, etc.) The FIGER dataset is much smaller than the GFT dataset (see <ref type="table" target="#tab_1">Table 2</ref>).</p><p>Our experimental results are shown in Ta- ble 4. Again, K-WSABIE performed the best, followed by the standard WSABIE model. Both of these methods significantly outperformed Ling and Weld's best result.  Feature learning We investigate whether hav- ing a large fine grained label space is helpful in learning a good representation for feature vec- tors (recall that WSABIE learns representations for both feature vectors and labels). We focus on the task of coarse type classification, where we want to classify a mention into one of the four top-level GFT labels. We fix the training mentions and learn WSABIE embeddings for feature vectors and la- bels by (1) training only on coarse labels and <ref type="formula">(2)</ref> training on all labels; we evaluate the models only on coarse labels. Training with all labels gives an improvement of about 2 points (F1 score) over training with just coarse labels, as shown in Ta- ble 5. This suggests that including additional sub- type labels can help us learn better feature embed- dings, even if we are not explicitly interested in the deeper labels.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>Design of fine grained label hierarchy Results at different levels of the hierarchies in <ref type="table">Table 6</ref> show that it is more difficult to discriminate among deeper labels. However, it appears that the depth- 2 FIGER types are easier to discriminate than the depth-2 (and depth-3) GFT labels. This may sim- ply be an artifact of the very small FIGER dataset, but it suggests it may be worthwhile to flatten the other subtree ini GFT since many of its subtypes do not obviously share any information.  <ref type="table">Table 6</ref>: WSABIE model's Precision (P), Recall (R), and F1-score at each level of the label hierarchies for GFT (top) and FIGER (bottom).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We introduced embedding methods for fine grained entity type classifications that outperforms state-of-the-art methods on benchmark entity- classification datasets. We showed that these methods learned reasonable embeddings for fine- type labels which allowed information sharing across related labels.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Label sets for Gillick et al. (2014)-left, GFT-and Ling and Weld (2012)-right, FIGER.</figDesc><graphic url="image-2.png" coords="2,301.04,48.64,222.23,138.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An illustration of the standard WSABIE model. x is the feature vector extracted from a mention, and yt is its label. Here, black cells indicate non-zero and white cells indicate zero values. The parameters are matrices A and B which are used to map the feature vector x and the label vector yt into an embedding space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Two-dimensional projections of label embeddings for GFT dataset. See text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>List of features used in our experiments, similar to features in Gillick et al. (2014). Features are extracted from each 

mention. The example mention in context is ... who Barack H. Obama first picked .... 

GFT Dev GFT Test FIGER 
Total mentions 
6,380 
11,324 
778 
at Level 1 
3,934 
7,975 
568 
at Level 2 
2,215 
2,994 
210 
at Level 3 
251 
335 
-

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 : Mention counts in our datasets.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Precision (P), Recall (R), and F1-score on the 

FIGER dataset for three competing models. We took the F1 
score from Ling and Weld's best result (no precision and re-
call numbers were reported). The improvements for WSABIE 
and K-WSABIE over the baseline are statistically significant 
(p &lt; 0.01). </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Comparison of two WSABIE models on coarse 

type classification for GFT. The first model only used coarse 
top-level labels, while the second model was trained on all 86 
labels. 

</table></figure>

			<note place="foot" n="1"> Turian et al. (2010), Collobert et al. (2011), and Qi et al. (2014) consider representation learning for coarse label named entity recognition.</note>

			<note place="foot" n="2"> We explored various kernels in preliminary experiments and found that the nearest neighbor kernel performs the best.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Andrew McCallum for helpful discus-sions and anonymous reviewers for feedback on an earlier draft of this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Coupled semi-supervised learning for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Estevam</forename><forename type="middle">R</forename><surname>Hruschka</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WSDM</title>
		<meeting>of WSDM</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The automatic content extraction (ACE) program tasks, data, and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Doddington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Przybocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of LREC</title>
		<meeting>of LREC</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Small statistical models by random feature mixing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL08 HLT Workshop on Mobile Language Processing</title>
		<meeting>the ACL08 HLT Workshop on Mobile Language Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="19" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Contextdependent fine-grained entity type tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nevena</forename><surname>Lazic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Kirchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Huynh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>In arXiv</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">MUC7 named entity task definition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynette</forename><surname>Hirschman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nancy</forename><surname>Chinchor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of MUC-7</title>
		<meeting>of MUC-7</meeting>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">No noun phrase left behind: Detecting and typing unlinkable entities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mausam</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP-CoNLL</title>
		<meeting>of EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fine-grained entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep learning for character-based information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujatha</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ECIR</title>
		<meeting>of ECIR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The life and death of discourse entities: Identifying singleton mentions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2003 shared task: language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien De</forename><surname>Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of HLT-NAACL</title>
		<meeting>of HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning distributed representations for structured output prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Srikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Word representations: A simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Wsabie: Scaling up to large vocabulary image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCAI</title>
		<meeting>of IJCAI</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Affinity weighted embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hector</forename><surname>Yee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Collective cross-document relation extraction without labelled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">HYENA: Hierarchical type classification for entity names</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Amir Yosef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandro</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Hoffart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Spaniol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
		<meeting>of COLING</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
