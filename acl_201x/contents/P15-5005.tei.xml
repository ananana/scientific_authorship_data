<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:00+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Matrix and Tensor Factorization Methods for Natural Language Processing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 26-31, 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
							<email>guillaume.bouchard@xerox.com</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution" key="instit1">Xerox Research Centre Europe</orgName>
								<orgName type="institution" key="instit2">University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Naradowsky</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution" key="instit1">Xerox Research Centre Europe</orgName>
								<orgName type="institution" key="instit2">University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution" key="instit1">Xerox Research Centre Europe</orgName>
								<orgName type="institution" key="instit2">University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution" key="instit1">Xerox Research Centre Europe</orgName>
								<orgName type="institution" key="instit2">University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution" key="instit1">Xerox Research Centre Europe</orgName>
								<orgName type="institution" key="instit2">University College London</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Matrix and Tensor Factorization Methods for Natural Language Processing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the Tutorials of the 53rd Annual Meeting of the ACL and the 7th IJCNLP</title>
						<meeting>the Tutorials of the 53rd Annual Meeting of the ACL and the 7th IJCNLP <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="16" to="18"/>
							<date type="published">July 26-31, 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>1 Tutorial Objectives Tensor and matrix factorization methods have attracted a lot of attention recently thanks to their successful applications to information extraction, knowledge base population, lexical semantics and dependency parsing. In the first part, we will first cover the basics of matrix and tensor factorization theory and optimization, and then proceed to more advanced topics involving convex surrogates and alternative losses. In the second part we will discuss recent NLP applications of these methods and show the connections with other popular methods such as transductive learning, topic models and neural networks. The aim of this tutorial is to present in detail applied factorization methods, as well as to introduce more recently proposed methods that are likely to be useful to NLP applications. 2 Tutorial Overview 2.1 Matrix/Tensor Factorization Basics In this part, we first remind essential results on bilinear forms, spectral representations of matrices and low-rank approximation theorems, which are often omitted in undergraduate linear algebra courses. This includes the link between eigen-value decomposition and singular value decomposition and the trace-norm (a.k.a. nuclear norm) as a convex surrogate of the low-rank constraint on optimization problems. Then, an overview of the most efficient algorithms to solve low-rank constrained problems is made, from the power iteration method, the Lanczos algorithm and the implicitly restarted Arnoldi method that is implemented in the LAPACK library (Anderson et al., 1999). We show how to interpret low-rank models as probabilistic models (Bishop, 1999) and how we can extend SVD algorithms that can factor-ize non-standard matrices (i.e. with non-Gaussian noise and missing data) using gradient descent, re-weighted SVD or Frank-Wolfe algorithms. We then show that combining different convex objectives can be a powerful tool, and we illustrate it by deriving the robust PCA algorithm by adding an L 1 penalty term in the objective function (Candès and Recht, 2009). Furthermore, we introduce Bayesian Personalized Ranking (BPR) for matrix and tensor factorization which deals with implicit feedback in ranking tasks (Rendle et al., 2009). Finally , will introduce the collective matrix factor-ization model (Singh and Gordon, 2008) and ten-sor extensions (Nickel et al., 2011) for relational learning. 2.2 Applications in NLP In this part we will discuss recent work applying matrix/tensor factorization methods in the context of NLP. We will review the Universal Schema paradigm for knowledge base construction (Riedel et al., 2013) which relies on matrix factoriza-tion and BPR, as well as recent extensions of the RESCAL tensor factorization (Nickel et al., 2011) approach and methods of injecting logic into the embeddings learned (Rocktäschel et al., 2015). These applications will motivate the connections between matrix factorization and trans-ductive learning (Goldberg et al., 2010), as well as tensor factorization and multi-task learning (Romera-Paredes et al., 2013). Furthermore, we will review work on applying matrix and tensor factorization to sparsity reduction in syntactic dependency parsing (Lei et al., 2014) and word representation learning (Pennington et al., 2014). In addition, we will discuss the connections between matrix factorization, latent semantic analysis and topic modeling (Stevens et al., 2012). 16</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Tutorial Objectives</head><p>Tensor and matrix factorization methods have at- tracted a lot of attention recently thanks to their successful applications to information extraction, knowledge base population, lexical semantics and dependency parsing. In the first part, we will first cover the basics of matrix and tensor factorization theory and optimization, and then proceed to more advanced topics involving convex surrogates and alternative losses. In the second part we will dis- cuss recent NLP applications of these methods and show the connections with other popular methods such as transductive learning, topic models and neural networks. The aim of this tutorial is to present in detail applied factorization methods, as well as to introduce more recently proposed meth- ods that are likely to be useful to NLP applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Tutorial Overview</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Matrix/Tensor Factorization Basics</head><p>In this part, we first remind essential results on bilinear forms, spectral representations of matri- ces and low-rank approximation theorems, which are often omitted in undergraduate linear algebra courses. This includes the link between eigen- value decomposition and singular value decompo- sition and the trace-norm (a.k.a. nuclear norm) as a convex surrogate of the low-rank constraint on optimization problems. Then, an overview of the most efficient algorithms to solve low-rank con- strained problems is made, from the power itera- tion method, the Lanczos algorithm and the im- plicitly restarted Arnoldi method that is imple- mented in the LAPACK library <ref type="bibr" target="#b0">(Anderson et al., 1999</ref>). We show how to interpret low-rank models as probabilistic models <ref type="bibr" target="#b1">(Bishop, 1999</ref>) and how we can extend SVD algorithms that can factor- ize non-standard matrices (i.e. with non-Gaussian noise and missing data) using gradient descent, re- weighted SVD or Frank-Wolfe algorithms. We then show that combining different convex objec- tives can be a powerful tool, and we illustrate it by deriving the robust PCA algorithm by adding an L 1 penalty term in the objective function <ref type="bibr" target="#b2">(Candès and Recht, 2009</ref>). Furthermore, we introduce Bayesian Personalized Ranking (BPR) for matrix and tensor factorization which deals with implicit feedback in ranking tasks ( <ref type="bibr" target="#b7">Rendle et al., 2009)</ref>. Fi- nally, will introduce the collective matrix factor- ization model <ref type="bibr" target="#b10">(Singh and Gordon, 2008)</ref> and ten- sor extensions (Nickel et al., 2011) for relational learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Applications in NLP</head><p>In this part we will discuss recent work apply- ing matrix/tensor factorization methods in the con- text of NLP. We will review the Universal Schema paradigm for knowledge base construction ( <ref type="bibr" target="#b8">Riedel et al., 2013</ref>) which relies on matrix factoriza- tion and BPR, as well as recent extensions of the RESCAL tensor factorization <ref type="bibr" target="#b5">(Nickel et al., 2011</ref>) approach and methods of injecting logic into the embeddings learned ( <ref type="bibr">Rocktäschel et al., 2015)</ref>. These applications will motivate the con- nections between matrix factorization and trans- ductive learning ( <ref type="bibr" target="#b3">Goldberg et al., 2010)</ref>, as well as tensor factorization and multi-task learning <ref type="bibr">(Romera-Paredes et al., 2013</ref>). Furthermore, we will review work on applying matrix and tensor factorization to sparsity reduction in syntactic de- pendency parsing ( <ref type="bibr" target="#b4">Lei et al., 2014</ref>) and word rep- resentation learning ( <ref type="bibr" target="#b6">Pennington et al., 2014</ref>). In addition, we will discuss the connections between matrix factorization, latent semantic analysis and topic modeling (Stevens et al., 2012).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Structure Part I: Matrix/Tensor Factorization Basics (90 minutes)</head><p>• Matrix factorization basics (40 min): bilin- ear forms, spectral representations, low rank approximations theorems, optimization with stochastic gradient descent, losses • Information extraction, knowledge base pop- ulation with connections to transductive learning and multitask learning (35 minutes)</p><p>• Lexical semantics with connections to neural networks, latent semantic analysis and topic models (30 minutes)</p><p>• Structured prediction (10 minutes)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">About the Speakers</head><p>Guillaume Bouchard is a senior researcher in statistics and machine learning at Xerox, focusing on statistical learning using low-rank model for large relational databases. His research includes text understanding, user modeling, and social me- dia analytics. The theoretical part of his work is related to the efficient algorithms to compute high dimensional integrals, essential to deal with un- certainty (missing and noisy data, latent variable models, Bayesian inference). The main applica- tion areas of his work includes the design of vir- tual conversational agents, link prediction (predic- tive algorithms for relational data), social media monitoring and transportation analytics. His web page is available at www.xrce.xerox.com/ people/bouchard. Jason Naradowsky is a postdoc at the Machine Reading group at UCL. Having previously ob- tained a PhD at UMass Amherst under the supervi- sion of David Smith and Mark Johnson, his current research aims to improve natural language under- standing by performing task-specific training of word representations and parsing models. He is also interested in semi-supervised learning, joint inference, and semantic parsing. His web page is available at http://narad.github.io/.</p><p>Sebastian Riedel is a senior lecturer at Univer- sity College London and an Allen Distinguished Investigator, leading the Machine Reading Lab. Before, he was a postdoc and research scientist with Andrew McCallum at UMass Amherst, a re- searcher at Tokyo University and DBCLS with Tsujii Junichi, and a PhD student with Ewan Klein at the University of Edinburgh. He is interested in teaching machines how to read and works at the intersection of Natural Language Processing (NLP) and Machine Learning, investigating vari- ous stages of the NLP pipeline, in particular those that require structured prediction, as well as fully probabilistic architectures of end-to-end reading and reasoning systems. Recently he became inter- ested in new ways to represent textual knowledge using low-rank embeddings and how to reason with such representations. His web page is avail- able at http://www.riedelcastro.org/.</p><p>Tim Rocktäschel is a PhD student in Sebas- tian Riedel's Machine Reading group at Univer- sity College London. Before that he worked as research assistant in the Knowledge Management in Bioinformatics group at Humboldt-Universität zu Berlin, where he also obtained his Diploma in Computer Science. He is broadly interested in representation learning (e.g. matrix/tensor fac- torization, deep learning) for NLP and automated knowledge base completion, and how these meth- ods can take advantage of symbolic background knowledge. His webpage is available at http: //rockt.github.io/.</p><p>Andreas Vlachos is postdoc at the Machine Reading group at UCL working with Sebastian Riedel on automated fact-checking using low- rank factorization methods. Before that he was a postdoc at the Natural Language and Infor- mation Processing group at the University of Cambridge and at the University of Wisconsin- Madison. He is broadly interested in natural lan- guage understanding (e.g. information extraction, semantic parsing) and in machine learning ap- proaches that would help us towards this goal. He has also worked on active learning, cluster- ing and biomedical text mining. His web page is available at http://sites.google.com/ site/andreasvlachos/.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>•</head><label></label><figDesc>Tensor factorization basics (20 minutes): representations,notation decompositions (Tucker etc.) • Advanced topics (30 minutes): convex sur- rogates, L1 regularization, alternative losses (ranking loss, logistic loss) Break (15 minutes) Part II: Applications in NLP (75 minutes)</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">LAPACK Users&apos; guide</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anderson</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>SIAM</publisher>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bayesian PCA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="382" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exact matrix completion via convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Candès</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Emmanuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations of Computational mathematics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="717" to="772" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Transduction with matrix completion: Three birds with one stone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="757" to="765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Low-rank tensors for scoring dependency structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1381" to="1391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nickel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pennington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bpr: Bayesian personalized ranking from implicit feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Injecting Logical Background Knowledge into Embeddings for Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, GA; Bernardino Romera</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Proceedings of the 2015 Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics. Romera-Paredes et al.2013</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multilinear multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hane</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadia</forename><surname>Aung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Bianchi-Berthouze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning</title>
		<meeting>the 30th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1444" to="1452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Relational learning via collective matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ajit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="650" to="658" />
		</imprint>
	</monogr>
	<note>Singh and Gordon2008</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Exploring topic coherence over many models and many topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stevens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="952" to="961" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
