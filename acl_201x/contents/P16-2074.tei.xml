<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:16+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Integrating Distributional Lexical Contrast into Word Embeddings for Antonym-Synonym Distinction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><forename type="middle">Anh</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institut für Maschinelle Sprachverarbeitung</orgName>
								<orgName type="institution">Universität Stuttgart Pfaffenwaldring 5B</orgName>
								<address>
									<postCode>70569</postCode>
									<settlement>Stuttgart</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Schulte Im Walde</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institut für Maschinelle Sprachverarbeitung</orgName>
								<orgName type="institution">Universität Stuttgart Pfaffenwaldring 5B</orgName>
								<address>
									<postCode>70569</postCode>
									<settlement>Stuttgart</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><forename type="middle">Thang</forename><surname>Vu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institut für Maschinelle Sprachverarbeitung</orgName>
								<orgName type="institution">Universität Stuttgart Pfaffenwaldring 5B</orgName>
								<address>
									<postCode>70569</postCode>
									<settlement>Stuttgart</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Integrating Distributional Lexical Contrast into Word Embeddings for Antonym-Synonym Distinction</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="454" to="459"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a novel vector representation that integrates lexical contrast into distri-butional vectors and strengthens the most salient features for determining degrees of word similarity. The improved vectors significantly outperform standard models and distinguish antonyms from synonyms with an average precision of 0.66-0.76 across word classes (adjectives, nouns, verbs). Moreover, we integrate the lexical contrast vectors into the objective function of a skip-gram model. The novel embedding outperforms state-of-the-art models on predicting word similarities in SimLex-999, and on distinguishing antonyms from synonyms.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Antonymy and synonymy represent lexical se- mantic relations that are central to the organization of the mental lexicon <ref type="bibr" target="#b11">(Miller and Fellbaum, 1991)</ref>. While antonymy is defined as the oppositeness be- tween words, synonymy refers to words that are similar in meaning <ref type="bibr">(Deese, 1965;</ref><ref type="bibr" target="#b8">Lyons, 1977)</ref>. From a computational point of view, distinguish- ing between antonymy and synonymy is impor- tant for NLP applications such as Machine Trans- lation and Textual Entailment, which go beyond a general notion of semantic relatedness and require to identify specific semantic relations. However, due to interchangeable substitution, antonyms and synonyms often occur in similar contexts, which makes it challenging to automatically distinguish between them.</p><p>Distributional semantic models (DSMs) offer a means to represent meaning vectors of words and to determine their semantic "relatedness" <ref type="bibr">(Budanitsky and Hirst, 2006;</ref><ref type="bibr" target="#b23">Turney and Pantel, 2010)</ref>.</p><p>They rely on the distributional hypothesis <ref type="bibr" target="#b2">(Harris, 1954;</ref><ref type="bibr" target="#b1">Firth, 1957)</ref>, in which words with similar distributions have related meaning. For computa- tion, each word is represented by a weighted fea- ture vector, where features typically correspond to words that co-occur in a particular context. How- ever, DSMs tend to retrieve both synonyms (such as formal-conventional) and antonyms (such as formal-informal) as related words and cannot suf- ficiently distinguish between the two relations.</p><p>In recent years, a number of distributional ap- proaches have accepted the challenge to distin- guish antonyms from synonyms, often in combi- nation with lexical resources such as thesauruses or taxonomies. For example, <ref type="bibr" target="#b7">Lin et al. (2003)</ref> used dependency triples to extract distributionally similar words, and then in a post-processing step filtered out words that appeared with the patterns 'from X to Y' or 'either X or Y' significantly of- ten. <ref type="bibr" target="#b13">Mohammad et al. (2013)</ref> assumed that word pairs that occur in the same thesaurus category are close in meaning and marked as synonyms, while word pairs occurring in contrasting thesaurus cat- egories or paragraphs are marked as opposites. <ref type="bibr" target="#b21">Scheible et al. (2013)</ref> showed that the distribu- tional difference between antonyms and synonyms can be identified via a simple word space model by using appropriate features. <ref type="bibr" target="#b17">Santus et al. (2014a)</ref> and <ref type="bibr" target="#b18">Santus et al. (2014b)</ref> aimed to identify the most salient dimensions of meaning in vector rep- resentations and reported a new average-precision- based distributional measure and an entropy-based measure to discriminate antonyms from synonyms (and further paradigmatic semantic relations).</p><p>Lately, antonym-synonym distinction has also been a focus of word embedding models. For ex- ample, <ref type="bibr">Adel and Schütze (2014)</ref> integrated coref- erence chains extracted from large corpora into a skip-gram model to create word embeddings that identified antonyms. <ref type="bibr" target="#b14">Ono et al. (2015)</ref> pro-posed thesaurus-based word embeddings to cap- ture antonyms. They proposed two models: the WE-T model that trains word embeddings on the- saurus information; and the WE-TD model that in- corporated distributional information into the WE- T model. <ref type="bibr" target="#b15">Pham et al. (2015)</ref> introduced the multi- task lexical contrast model (mLCM) by incorpo- rating WordNet into a skip-gram model to opti- mize semantic vectors to predict contexts. Their model outperformed standard skip-gram models with negative sampling on both general seman- tic tasks and distinguishing antonyms from syn- onyms.</p><p>In this paper, we propose two approaches that make use of lexical contrast information in distri- butional semantic space and word embeddings for antonym-synonym distinction. Firstly, we incor- porate lexical contrast into distributional vectors and strengthen those word features that are most salient for determining word similarities, assum- ing that feature overlap in synonyms is stronger than feature overlap in antonyms. Secondly, we propose a novel extension of a skip-gram model with negative sampling ( <ref type="bibr" target="#b10">Mikolov et al., 2013b</ref>) that integrates the lexical contrast information into the objective function. The proposed model opti- mizes the semantic vectors to predict degrees of word similarity and also to distinguish antonyms from synonyms. The improved word embeddings outperform state-of-the-art models on antonym- synonym distinction and a word similarity task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Our Approach</head><p>In this section, we present the two contributions of this paper: a new vector representation that improves the quality of weighted features to dis- tinguish between antonyms and synonyms (Sec- tion 2.1), and a novel extension of skip-gram mod- els that integrates the improved vector representa- tions into the objective function, in order to pre- dict similarities between words and to identify antonyms (Section 2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Improving the weights of feature vectors</head><p>We aim to improve the quality of weighted feature vectors by strengthening those features that are most salient in the vectors and by putting less emphasis on those that are of minor importance, when distinguishing degrees of similarity be- tween words. We start out with standard corpus co-occurrence frequencies and apply local mutual information (LMI) <ref type="bibr" target="#b0">(Evert, 2005)</ref> to determine the original strengths of the word features. Our score weight SA (w, f ) subsequently defines the weights of a target word w and a feature f :</p><formula xml:id="formula_0">weight SA (w, f ) = 1 #(w,u) u∈W (f )∩S(w) sim(w, u) − 1 #(w ,v) w ∈A(w) v∈W (f )∩S(w ) sim(w , v) (1)</formula><p>The new weight SA scores of a target word w and a feature f exploit the differences between the av- erage similarities of synonyms to the target word (sim(w, u), with u ∈ S(w)), and the average similarities between antonyms of the target word (sim(w , v), with w ∈ A(w) and v ∈ S(w )). Only those words u and v are included in the cal- culation that have a positive original LMI score for the feature f : W (f ). To calculate the similarity sim between two word vectors, we rely on cosine distances. If a word w is not associated with any synonyms or antonyms in our resources (cf. Sec- tion 3.1), or if a feature does not co-occur with a word w, we define weight SA (w, f ) = 0.</p><p>The intuition behind the lexical contrast infor- mation in our new weight SA is as follows. The strongest features of a word also tend to repre- sent strong features of its synonyms, but weaker features of its antonyms. For example, the fea- ture conception only occurs with synonyms of the adjective formal but not with the antonym in- formal, or with synonyms of the antonym infor- mal. weight SA (f ormal, conception), which is calculated as the average similarity between for- mal and its synonyms minus the average similarity between informal and its synonyms, should thus return a high positive value. In contrast, a fea- ture such as issue that occurs with many differ- ent adjectives, would enforce a feature score near zero for weight SA (f ormal, issue), because the similarity scores between formal and its synonyms and informal and its synonyms should not differ strongly. Last but not least, a feature such as ru- mor that only occurs with informal and its syn- onyms, but not with the original target adjective formal and its synonyms, should invoke a very low value for weight SA (f ormal, rumor). <ref type="figure" target="#fig_1">Figure 1</ref> provides a schematic visualization for computing the new weight SA scores for the target formal.</p><p>Since the number of antonyms is usually much smaller than the number of synonyms, we enrich the number of antonyms: Instead of using the A w w ( )= '="i n f o r ma l " direct antonym links, we consider all synonyms of an antonym w ∈ A(w) as antonyms of w. For example, the target word good has only two antonyms in WordNet (bad and evil), in compar- ison to 31 synonyms. Thus, we also exploit the synonyms of bad and evil as antonyms for good.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Integrating the distributional lexical contrast into a skip-gram model</head><p>Our model relies on <ref type="bibr" target="#b5">Levy and Goldberg (2014)</ref> who showed that the objective function for a skip-gram model with negative sampling (SGNS) can be defined as follows:</p><p>w∈V c∈V {#(w, c) log σ(sim(w, c))</p><formula xml:id="formula_1">+k#(w)P 0 (c) log σ(−sim(w, c))}<label>(2)</label></formula><p>The first term in Equation (2) represents the co- occurrence between a target word w and a context c within a context window. The number of ap- pearances of the target word and that context is defined as #(w, c). The second term refers to the negative sampling where k is the number of nega- tively sampled words, and #(w) is the number of appearances of w as a target word in the unigram distribution P 0 of its negative context c. To incorporate our lexical contrast information into the SGNS model, we propose the objective function in Equation (3) to add distributional con- trast followed by all contexts of the target word. V is the vocabulary; σ(x) = 1 1+e −x is the sigmoid function; and sim(w 1 , w 2 ) is the cosine similarity between the two embedded vectors of the corre- sponding two words w 1 and w 2 . We refer to our distributional lexical-contrast embedding model as dLCE. </p><formula xml:id="formula_2">+k#(w)P 0 (c) log σ(−sim(w, c))) +( 1 #(w,u) u∈W (c)∩S(w) sim(w, u) − 1 #(w,v) v∈W (c)∩A(w) sim(w, v))}<label>(3)</label></formula><p>Equation <ref type="formula" target="#formula_2">(3)</ref> integrates the lexical contrast in- formation in a slightly different way compared to Equation (1): For each of the target words w, we only rely on its antonyms A(w) instead of using the synonyms of its antonyms S(w ). This makes the word embeddings training more efficient in running time, especially since we are using a large amount of training data.</p><p>The dLCE model is similar to the WE- TD model ( <ref type="bibr" target="#b14">Ono et al., 2015</ref>) and the mLCM model ( <ref type="bibr" target="#b15">Pham et al., 2015)</ref>; however, while the WE-TD and mLCM models only apply the lexi- cal contrast information from WordNet to each of the target words, dLCE applies lexical contrast to every single context of a target word in order to better capture and classify semantic contrast.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Settings</head><p>The corpus resource for our vector representations is one of the currently largest web corpora: EN- COW14A ( <ref type="bibr" target="#b19">Schäfer and Bildhauer, 2012;</ref><ref type="bibr" target="#b20">Schäfer, 2015)</ref>, containing approximately 14.5 billion to- kens and 561K distinct word types. As distri- butional information, we used a window size of 5 tokens for both the original vector represen- tation and the word embeddings models. For word embeddings models, we trained word vec- tors with 500 dimensions; k negative sampling was set to 15; the threshold for sub-sampling was set to 10 −5 ; and we ignored all words that oc- curred &lt; 100 times in the corpus. The param- eters of the models were estimated by backpropa- gation of error via stochastic gradient descent. The learning rate strategy was similar to <ref type="bibr" target="#b9">Mikolov et al. (2013a)</ref> in which the initial learning rate was set to 0.025. For the lexical contrast information, we used WordNet <ref type="bibr" target="#b12">(Miller, 1995)</ref> and Wordnik 1 to collect antonyms and synonyms, obtaining a total of 363,309 synonym and 38,423 antonym pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Distinguishing antonyms from synonyms</head><p>The first experiment evaluates our lexical con- trast vectors by applying the vector representa- tions with the improved weight SA scores to the task of distinguishing antonyms from synonyms. As gold standard resource, we used the English dataset described in <ref type="bibr" target="#b16">(Roth and Schulte im Walde, 2014)</ref>, containing 600 adjective pairs (300 antony- mous pairs and 300 synonymous pairs), 700 noun pairs (350 antonymous pairs and 350 synonymous pairs) and 800 verb pairs (400 antonymous pairs and 400 synonymous pairs). For evaluation, we applied Average Precision (AP) <ref type="bibr" target="#b24">(Voorhees and Harman, 1999</ref>), a common metric in informa- tion retrieval previously used by <ref type="bibr" target="#b4">Kotlerman et al. (2010)</ref> and <ref type="bibr" target="#b17">Santus et al. (2014a)</ref>, among others. <ref type="table" target="#tab_1">Table 1</ref> presents the results of the first ex- periment, comparing our improved vector rep- resentations with the original LMI representa- tions across word classes, without/with apply- ing singular-value decomposition (SVD), respec- tively. In order to evaluate the distribution of word pairs with AP, we sorted the synonymous and antonymous pairs by their cosine scores. A synonymous pair was considered correct if it be- longed to the first half; and an antonymous pairs was considered correct if it was in the second half. The optimal results would thus achieve an AP score of 1 for SY N and 0 for AN T . The re- sults in the tables demonstrate that weight SA sig- nificantly 2 outperforms the original vector repre- sentations across word classes.</p><p>In addition, <ref type="figure" target="#fig_4">Figure 2</ref> compares the medians of cosine similarities between antonymous pairs (red) vs. synonymous pairs (green) across word classes, and for the four conditions (1) LMI, (2) weight SA , (3) SVD on LMI, and (4) SVD on weight SA . The plots show that the cosine sim- ilarities of the two relations differ more strongly with our improved vector representations in com- parison to the original LMI representations, and even more so after applying SVD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Effects of distributional lexical contrast on word embeddings</head><p>The second experiment evaluates the performance of our dLCE model on both antonym-synonym distinction and a word similarity task. The similar- ity task requires to predict the degree of similarity for word pairs, and the ranked list of predictions is evaluated against a gold standard of human rat- ings, relying on the Spearman rank-order correla- tion coefficient ρ ( <ref type="bibr" target="#b22">Siegel and Castellan, 1988)</ref>.</p><p>In this paper, we use the SimLex-999 dataset ( <ref type="bibr" target="#b3">Hill et al., 2015)</ref> to evaluate word embedding models on predicting similarities. The resource contains 999 word pairs (666 noun, 222 verb and 111 adjective pairs) and was explicitly built to test models on capturing similarity rather than relatedness or association. <ref type="table" target="#tab_2">Table 2</ref> shows that our dLCE model outperforms both SGNS and mLCM, proving that the lexical contrast information has a positive effect on predicting similarity.      Therefore, the improved distinction between synonyms (strongly similar words) and antonyms (often strongly related but highly dissimilar words) in the dLCE model also supports the dis- tinction between degrees of similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adjectives</head><p>For distinguishing between antonyms and syn- onyms, we computed the cosine similarities be- tween word pairs on the dataset described in Sec- tion 3.2, and then used the area under the ROC curve (AUC) to evaluate the performance of dLCE compared to SGNS and mLCM. The results in Ta- ble 3 report that dLCE outperforms SGNS and mLCM also on this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This paper proposed a novel vector representation which enhanced the prediction of word similar- ity, both for a traditional distributional semantics model and word embeddings. Firstly, we signifi- cantly improved the quality of weighted features to distinguish antonyms from synonyms by us- ing lexical contrast information. Secondly, we in- corporated the lexical contrast information into a skip-gram model to successfully predict degrees of similarity and also to identify antonyms. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of the weight SA scores for the adjective target formal. The feature conception only occurs with formal and synonyms of formal, so weight SA (f ormal, conception) should return a positive value; the feature rumor only occurs with the antonym informal and with synonyms of informal, so weight SA (f ormal, rumor) should return a negative value; the feature issue occurs with both formal and informal and also with synonyms of these two adjectives, so weight SA (f ormal, issue) should return a feature score near zero.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Differences between cosine scores for antonymous vs. synonymous word pairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>AP evaluation on DSMs. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Spearman's ρ on SimLex-999. 

Adjectives Nouns Verbs 
SGNS 
0.64 
0.66 
0.65 
mLCM 
0.85 
0.69 
0.71 
dLCE 
0.90 
0.72 
0.81 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 : AUC scores for identifying antonyms.</head><label>3</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> http://www.wordnik.com</note>

			<note place="foot" n="2"> χ 2 , * * * p &lt; .001, * * p &lt; .005, * p &lt; .05</note>

			<note place="foot">James Deese. 1965. The Structure of Associations in Language and Thought. The John Hopkins Press, Baltimore, MD.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><forename type="middle">Evert</forename></persName>
		</author>
		<title level="m">The Statistics of Word Cooccurrences</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>Stuttgart University</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Firth</surname></persName>
		</author>
		<title level="m">Papers in Linguistics 1934-51. Longmans</title>
		<meeting><address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zellig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harris</surname></persName>
		</author>
		<title level="m">Distributional structure. Word</title>
		<imprint>
			<date type="published" when="1954" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="146" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Simlex-999: Evaluating semantic models with (genuine) similarity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="665" to="695" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Directional distributional similarity for lexical inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Kotlerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Szpektor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maayan</forename><surname>Zhitomirsky-Geffet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="359" to="389" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Neural word embedding as implicit matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N.D</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Identifying synonyms among distributionally similar words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 18th International Joint Conference on Artificial Intelligence<address><addrLine>Acapulco, Mexico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="1492" to="1493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lyons</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977" />
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">Computing Research Repository</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta</addrLine></address></meeting>
		<imprint>
			<publisher>Georgia</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semantic networks of English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christiane</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fellbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="197" to="229" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">WordNet: A lexical database for English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><forename type="middle">J</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">D</forename><surname>Hirst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing lexical contrast. Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="555" to="590" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Word embedding-based antonym detection using thesauri and distributional information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masataka</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Sasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="984" to="989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A multitask objective to inject lexical contrast into distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Nghia The Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="21" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Combining word patterns and discourse markers for paradigmatic relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Schulte Im Walde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="524" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Chasing hypernyms in vector spaces with entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Santus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Schulte Im Walde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="38" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Taking antonymy mask off in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Santus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Ren</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Pacific Asia Conference on Language, Information and Computation</title>
		<meeting>the 28th Pacific Asia Conference on Language, Information and Computation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="135" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Building large corpora from the web using a new efficient tool chain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Bildhauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Language Resources and Evaluation</title>
		<meeting>the 8th International Conference on Language Resources and Evaluation<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="486" to="493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Processing and querying large web corpora with the COW14 architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Schäfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Challenges in the Management of Large Corpora</title>
		<meeting>the 3rd Workshop on Challenges in the Management of Large Corpora</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="28" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Uncovering distributional differences between synonyms and antonyms in a word space model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silke</forename><surname>Scheible</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Schulte Im Walde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvia</forename><surname>Springorum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Joint Conference on Natural Language Processing</title>
		<meeting>the 6th International Joint Conference on Natural Language Processing<address><addrLine>Nagoya, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="489" to="497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Nonparametric Statistics for the Behavioral Sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sidney</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N. John</forename><surname>Castellan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<publisher>McGraw-Hill</publisher>
			<pubPlace>Boston, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">From frequency to meaning: Vector space models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donna</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</author>
		<title level="m">The 7th Text REtrieval Conference (trec-7). National Institute of Standards and Technology</title>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
