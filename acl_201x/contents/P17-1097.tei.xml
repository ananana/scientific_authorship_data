<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">From Language to Programs: Bridging Reinforcement Learning and Maximum Marginal Likelihood</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><forename type="middle">Zheran</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
						</author>
						<title level="a" type="main">From Language to Programs: Bridging Reinforcement Learning and Maximum Marginal Likelihood</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1051" to="1062"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1097</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Our goal is to learn a semantic parser that maps natural language utterances into ex-ecutable programs when only indirect supervision is available: examples are labeled with the correct execution result, but not the program itself. Consequently, we must search the space of programs for those that output the correct result, while not being misled by spurious programs: incorrect programs that coinci-dentally output the correct result. We connect two common learning paradigms, reinforcement learning (RL) and maximum marginal likelihood (MML), and then present a new learning algorithm that combines the strengths of both. The new algorithm guards against spurious programs by combining the systematic search traditionally employed in MML with the randomized exploration of RL, and by updating parameters such that probability is spread more evenly across consistent programs. We apply our learning algorithm to a new neural semantic parser and show significant gains over existing state-of-the-art results on a recent context-dependent semantic parsing task.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We are interested in learning a semantic parser that maps natural language utterances into executable programs (e.g., logical forms). For example, in <ref type="figure" target="#fig_0">Figure 1</ref>, a program corresponding to the utter- ance transforms an initial world state into a new world state. We would like to learn from indirect supervision, where each training example is only labeled with the correct output (e.g. a target world state), but not the program that produced that out- The task is to map natural language ut- terances to a program that manipulates the world state. The correct program captures the true mean- ing of the utterances, while spurious programs ar- rive at the correct output for the wrong reasons. We develop methods to prevent the model from being drawn to spurious programs. put ( <ref type="bibr" target="#b9">Clarke et al., 2010;</ref><ref type="bibr" target="#b22">Liang et al., 2011;</ref><ref type="bibr" target="#b18">Krishnamurthy and Mitchell, 2012;</ref><ref type="bibr" target="#b3">Artzi and Zettlemoyer, 2013;</ref><ref type="bibr" target="#b21">Liang et al., 2017)</ref>.</p><p>The process of constructing a program can be formulated as a sequential decision-making pro- cess, where feedback is only received at the end of the sequence when the completed program is executed. In the natural language processing lit- erature, there are two common approaches for handling this situation: 1) reinforcement learn- ing (RL), particularly the REINFORCE algorithm <ref type="bibr" target="#b42">(Williams, 1992;</ref><ref type="bibr" target="#b40">Sutton et al., 1999</ref>), which max- imizes the expected reward of a sequence of actions; and 2) maximum marginal likelihood (MML), which treats the sequence of actions as a latent variable, and then maximizes the marginal likelihood of observing the correct program output <ref type="bibr" target="#b11">(Dempster et al., 1977)</ref>.</p><p>While the two approaches have enjoyed success on many tasks, we found them to work poorly out of the box for our task. This is because in addi- tion to the sparsity of correct programs, our task also requires weeding out spurious programs : incorrect interpretations of the utterances that accidentally produce the cor- rect output, as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>We show that MML and RL optimize closely re- lated objectives. Furthermore, both MML and RL methods have a mechanism for exploring program space in search of programs that generate the cor- rect output. We explain why this exploration tends to quickly concentrate around short spurious pro- grams, causing the model to sometimes overlook the correct program. To address this problem, we propose RANDOMER, a new learning algorithm with two parts:</p><p>First, we propose randomized beam search, an exploration strategy which combines the system- atic beam search traditionally employed in MML with the randomized off-policy exploration of RL. This increases the chance of finding correct pro- grams even when the beam size is small or the pa- rameters are not pre-trained.</p><p>Second, we observe that even with good explo- ration, the gradients of both the RL and MML objectives may still upweight entrenched spuri- ous programs more strongly than correct programs with low probability under the current model. We propose a meritocratic parameter update rule, a modification to the MML gradient update, which more equally upweights all programs that produce the correct output. This makes the model less likely to overfit spurious programs.</p><p>We apply RANDOMER to train a new neural se- mantic parser, which outputs programs in a stack- based programming language. We evaluate our re- sulting system on SCONE, the context-dependent semantic parsing dataset of <ref type="bibr" target="#b23">Long et al. (2016)</ref>. Our approach outperforms standard RL and MML methods in a direct comparison, and achieves new state-of-the-art results, improving over <ref type="bibr" target="#b23">Long et al. (2016)</ref> in all three domains of SCONE, and by over 30% accuracy on the most challenging one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task</head><p>We consider the semantic parsing task in the SCONE dataset <ref type="bibr">1 (Long et al., 2016)</ref>. As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, each example consists of a world con- taining several objects (e.g., people), each with certain properties (e.g., shirt color and hat color). Given the initial world state w 0 and a sequence of M natural language utterances u = (u 1 , . . . , u M ), the task is to generate a program that manipulates the world state according to the utterances. Each utterance u m describes a single action that trans- forms the world state w m−1 into a new world state w m . For training, the system receives weakly su- pervised examples with input x = (u, w 0 ) and the target final world state y = w M .</p><p>The dataset includes 3 domains: ALCHEMY, TANGRAMS, and SCENE. The description of each domain can be found in Appendix B. The do- mains highlight different linguistic phenomena: ALCHEMY features ellipsis (e.g., "throw the rest out", "mix"); TANGRAMS features anaphora on actions (e.g., "repeat step 3", "bring it back"); and SCENE features anaphora on entities (e.g., "he moves back", ". . . to his left"). Each domain con- tains roughly 3,700 training and 900 test exam- ples. Each example contains 5 utterances and is labeled with the target world state after each utter- ance, but not the target program.</p><p>Spurious programs. Given a training example (u, w 0 , w M ), our goal is to find the true underly- ing program z * which reflects the meaning of u. The constraint that z * must transform w 0 into w M , i.e. z(w 0 ) = w M , is not enough to uniquely iden- tify the true z * , as there are often many z satisfy- ing z(w 0 ) = w M : in our experiments, we found at least 1600 on average for each example. Al- most all do not capture the meaning of u (see <ref type="figure" target="#fig_0">Fig- ure 1)</ref>. We refer to these incorrect z's as spurious programs. Such programs encourage the model to learn an incorrect mapping from language to program operations: e.g., the spurious program in <ref type="figure" target="#fig_0">Figure 1</ref> would cause the model to learn that "man in the yellow hat" maps to hasShirt(red).</p><p>Spurious programs in SCONE. In this dataset, utterances often reference objects in different ways (e.g. a person can be referenced by shirt color, hat color, or position). Hence, any target programming language must also support these different reference strategies. As a result, even a single action such as moving a person to a tar- get destination can be achieved by many different programs, each selecting the person and destina- tion in a different way. Across multiple actions, the number of programs grows combinatorially. <ref type="bibr">2</ref> Only a few programs actually implement the cor- rect reference strategy as defined by the utterance. This problem would be more severe in any more general-purpose language (e.g. Python).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>We formulate program generation as a sequence prediction problem. We represent a program as a sequence of program tokens in postfix nota- tion; for example, move(hasHat(yellow), leftOf(hasShirt(blue))) is linearized as yellow hasHat blue hasShirt leftOf move. This representation also allows us to incre- mentally execute programs from left to right using a stack: constants (e.g., yellow) are pushed onto the stack, while functions (e.g., hasHat) pop appropriate arguments from the stack and push back the computed result (e.g., the list of people with yellow hats). Appendix B lists the full set of program tokens, Z, and how they are executed. Note that each action always ends with an action token (e.g., move).</p><p>Given an input x = (u, w 0 ), the model gener- ates program tokens z 1 , z 2 , . . . from left to right using a neural encoder-decoder model with atten- tion ( <ref type="bibr" target="#b4">Bahdanau et al., 2015)</ref>. Throughout the gen- eration process, the model maintains an utterance pointer, m, initialized to 1. To generate z t , the model's encoder first encodes the utterance u m into a vector e m . Then, based on e m and pre- viously generated tokens z 1:t−1 , the model's de- coder defines a distribution p(z t | x, z 1:t−1 ) over the possible values of z t ∈ Z. The next token z t is sampled from this distribution. If an action token (e.g., move) is generated, the model increments the utterance pointer m. The process terminates when all M utterances are processed. The final probability of generating a particular program z = (z 1 , . . . , z T ) is p(z | x) = T t=1 p(z t | x, z 1:t−1 ). Encoder. The utterance u m under the pointer is encoded using a bidirectional LSTM:</p><formula xml:id="formula_0">h F i = LSTM(h F i−1 , Φ u (u m,i )) h B i = LSTM(h B i+1 , Φ u (u m,i )) h i = [h F i ; h B i ],</formula><p>where Φ u (u m,i ) is the fixed GloVe word embed- ding ( <ref type="bibr" target="#b33">Pennington et al., 2014</ref>) of the ith word in u m . The final utterance embedding is the concate- <ref type="bibr" target="#b4">Bahdanau et al. (2015)</ref>, which used a recurrent network for the decoder, we opt for a feed-forward network for simplicity. We use e m and an embedding f (z 1:t−1 ) of the previ- ous execution history (described later) as inputs to compute an attention vector c t :</p><formula xml:id="formula_1">nation e m = [h F |um| ; h B 1 ]. Decoder. Unlike</formula><formula xml:id="formula_2">q t = ReLU(W q [e m ; f (z 1:t−1 )]) α i ∝ exp(q t W a h i ) (i = 1, . . . , |u m |) c t = i α i h i .</formula><p>Finally, after concatenating q t with c t , the distri- bution over the set Z of possible program tokens is computed via a softmax:</p><formula xml:id="formula_3">p(z t | x, z 1:t−1 ) ∝ exp(Φ z (z t ) W s [q t ; c t ]),</formula><p>where Φ z (z t ) is the embedding for token z t .</p><p>Execution history embedding. We compare two options for f (z 1:t−1 ), our embedding of the execution history. A standard approach is to sim- ply take the k most recent tokens z t−k:t−1 and con- catenate their embeddings. We will refer to this as TOKENS and use k = 4 in our experiments.</p><p>We also consider a new approach which lever- ages our ability to incrementally execute programs using a stack. We summarize the execution history by embedding the state of the stack at time t − 1, achieved by concatenating the embeddings of all values on the stack. (We limit the maximum stack size to 3.) We refer to this as STACK.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Reinforcement learning versus maximum marginal likelihood</head><p>Having formulated our task as a sequence pre- diction problem, we must still choose a learn- ing algorithm. We first compare two standard paradigms: reinforcement learning (RL) and max- imum marginal likelihood (MML). In the next sec- tion, we propose a better alternative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparing objective functions</head><p>Reinforcement learning. From an RL perspec- tive, given a training example (x, y), a policy makes a sequence of decisions z = (z 1 , . . . , z T ), and then receives a reward at the end of the episode: R(z) = 1 if z executes to y and 0 oth- erwise (dependence on x and y has been omitted from the notation). We focus on policy gradient methods, in which a stochastic policy function is trained to maximize the expected reward. In our setup, p θ (z | x) is the policy (with parameters θ), and its expected reward on a given example (x, y) is</p><formula xml:id="formula_4">G(x, y) = z R(z) p θ (z | x),<label>(1)</label></formula><p>where the sum is over all possible programs. The overall RL objective, J RL , is the expected reward across examples:</p><formula xml:id="formula_5">J RL = (x,y) G(x, y).<label>(2)</label></formula><p>Maximum marginal likelihood. The MML perspective assumes that y is generated by a partially-observed random process: conditioned on x, a latent program z is generated, and con- ditioned on z, the observation y is generated. This implies the marginal likelihood:</p><formula xml:id="formula_6">p θ (y | x) = z p(y | z) p θ (z | x).<label>(3)</label></formula><p>Note that since the execution of z is deterministic, p θ (y | z) = 1 if z executes to y and 0 otherwise. The log marginal likelihood of the data is then</p><formula xml:id="formula_7">J MML = log L MML ,<label>(4)</label></formula><p>where</p><formula xml:id="formula_8">L MML = (x,y) p θ (y | x).<label>(5)</label></formula><p>To estimate our model parameters θ, we maximize J MML with respect to θ. With our choice of reward, the RL expected re- ward (1) is equal to the MML marginal probabil- ity (3). Hence the only difference between the two formulations is that in RL we optimize the sum of expected rewards (2), whereas in MML we opti- mize the product (5). <ref type="bibr">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparing gradients</head><p>In both policy gradient and MML, the objectives are typically optimized via (stochastic) gradient ascent. The gradients of J RL and J MML are closely related. They both have the form:</p><formula xml:id="formula_9">θ J = (x,y) E z∼q [R(z) log p θ (z | x)] (6) = (x,y) z q(z)R(z) log p θ (z | x),</formula><p>where q(z) equals</p><formula xml:id="formula_10">q RL (z) = p θ (z | x) for J RL ,<label>(7)</label></formula><formula xml:id="formula_11">q MML (z) = R(z)p θ (z | x) ˜ z R(˜ z)p θ (˜ z | x) (8) = p θ (z | x, R(z) = 0) for J MML .</formula><p>Taking a step in the direction of log p θ (z | x) upweights the probability of z, so we can heuris- tically think of the gradient as attempting to up- weight each reward-earning program z by a gradi- ent weight q(z). In Subsection 5.2, we argue why q MML is better at guarding against spurious pro- grams, and propose an even better alternative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparing gradient approximation strategies</head><p>It is often intractable to compute the gradient (6) because it involves taking an expectation over all possible programs. So in practice, the expectation is approximated.</p><p>In the policy gradient literature, Monte Carlo integration (MC) is the typical approximation strategy. For example, the popular REINFORCE algorithm <ref type="bibr" target="#b42">(Williams, 1992)</ref> uses Monte Carlo sampling to compute an unbiased estimate of the gradient:</p><formula xml:id="formula_12">∆ MC = 1 B z∈S [R(z) − c] log p θ (z | x), (9)</formula><p>where S is a collection of B samples z (b) ∼ q(z), and c is a baseline (Williams, 1992) used to re- duce the variance of the estimate without altering its expectation.</p><p>In the MML literature for latent sequences, the expectation is typically approximated via numeri- cal integration (NUM) instead:</p><formula xml:id="formula_13">∆ NUM = z∈S q(z)R(z) log p θ (z | x). (10)</formula><p>where the programs in S come from beam search.</p><p>Beam search. Beam search generates a set of programs via the following process. At step t of beam search, we maintain a beam B t of at most B search states. Each state s ∈ B t represents a par- tially constructed program, s = (z 1 , . . . , z t ) (the first t tokens of the program). For each state s in the beam, we generate all possible continuations,</p><formula xml:id="formula_14">cont(s) = cont((z 1 , . . . , z t )) = {(z 1 , . . . , z t , z t+1 ) | z t+1 ∈ Z} .</formula><p>We then take the union of these continuations, cont(B t ) = s∈Bt cont(s). The new beam B t+1 is simply the highest scoring B continuations in cont(B t ), as scored by the policy, p θ (s | x). Search is halted after a fixed number of iterations or when there are no continuations possible. S is then the set of all complete programs discovered during beam search. We will refer to this as beam search MML (BS-MML).</p><p>In both policy gradient and MML, we think of the procedure used to produce the set of programs S as an exploration strategy which searches for programs that produce reward. One advantage of numerical integration is that it allows us to de- couple the exploration strategy from the gradient weights assigned to each program.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Tackling spurious programs</head><p>In this section, we illustrate why spurious pro- grams are problematic for the most commonly used methods in RL (REINFORCE) and MML (beam search MML). We describe two key prob- lems and propose a solution to each, based on insights gained from our comparison of RL and MML in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Spurious programs bias exploration</head><p>As mentioned in Section 4, REINFORCE and BS- MML both employ an exploration strategy to ap- proximate their respective gradients. In both meth- ods, exploration is guided by the current model policy, whereby programs with high probability under the current policy are more likely to be ex- plored. A troubling implication is that programs with low probability under the current policy are likely to be overlooked by exploration.</p><p>If the current policy incorrectly assigns low probability to the correct program z * , it will likely fail to discover z * during exploration, and will consequently fail to upweight the probability of z * . This repeats on every gradient step, keep- ing the probability of z * perpetually low. The same feedback loop can also cause already high- probability spurious programs to gain even more probability. From this, we see that exploration is sensitive to initial conditions: the rich get richer, and the poor get poorer.</p><p>Since there are often thousands of spurious pro- grams and only a few correct programs, spurious programs are usually found first. Once spurious programs get a head start, exploration increasingly biases towards them.</p><p>As a remedy, one could try initializing parame- ters such that the model puts a uniform distribution over all possible programs. A seemingly reason- able tactic is to initialize parameters such that the</p><note type="other">Spurious: move(hasShirt(red), 1) Correct: move(hasHat(yellow), leftOf(hasShirt(blue))) 1 2 3 1 2 3 z* z' 0.1 0.1 0.1 0.1 0.1 0.</note><formula xml:id="formula_15">1 0.1 0.1 0.1 0.1 p(z') = 10 -4</formula><p>p(z*) = 10 -6 red yellow hasHat blue hasShirt leftOf move move 1 hasShirt <ref type="figure">Figure 2</ref>: Two possible paths in the tree of all possible programs. One path leads to the spurious program z (red) while the longer path leads to the correct program z * (gold). Each edge represents a decision and shows the probability of that decision under a uniform policy. The shorter program has two orders of magnitude higher probability. model policy puts near-uniform probability over the decisions at each time step. However, this causes shorter programs to have orders of mag- nitude higher probability than longer programs, as illustrated in <ref type="figure">Figure 2</ref> and as we empirically observe. A more sophisticated approach might involve approximating the total number of pro- grams reachable from each point in the program- generating decision tree. However, we instead propose to reduce sensitivity to the initial distri- bution over programs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Solution: randomized beam search</head><p>One solution to biased exploration is to simply rely less on the untrustworthy current policy. We can do this by injecting random noise into exploration.</p><p>In REINFORCE, a common solution is to sam- ple from an -greedy variant of the current policy. On the other hand, MML exploration with beam search is deterministic. However, it has a key ad- vantage over REINFORCE-style sampling: even if one program occupies almost all probability un- der the current policy (a peaky distribution), beam search will still use its remaining beam capacity to explore at least B − 1 other programs. In contrast, sampling methods will repeatedly visit the mode of the distribution.</p><p>To get the best of both worlds, we propose a simple -greedy randomized beam search. Like regular beam search, at iteration t we compute the set of all continuations cont(B t ) and sort them by their model probability p θ (s | x). But instead of selecting the B highest-scoring continuations, we choose B continuations one by one without re- placement from cont(B t ). When choosing a con- tinuation from the remaining pool, we either uni- formly sample a random continuation with prob- ability , or pick the highest-scoring continuation in the pool with probability 1 − . Empirically, we find that this performs much better than both clas- sic beam search and -greedy sampling <ref type="table" target="#tab_0">(Table 3)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Spurious programs dominate gradients</head><p>In both RL and MML, even if exploration is per- fect and the gradient is exactly computed, spurious programs can still be problematic.</p><p>Even if perfect exploration visits every pro- gram, we see from the gradient weights q(z) in <ref type="formula" target="#formula_10">(7)</ref> and <ref type="formula">(8)</ref> that programs are weighted proportional to their current policy probability. If a spurious pro- gram z has 100 times higher probability than z * as in <ref type="figure">Figure 2</ref>, the gradient will spend roughly 99% of its magnitude upweighting towards z and only 1% towards z * even though the two programs get the same reward.</p><p>This implies that it would take many updates for z * to catch up. In fact, z * may never catch up, de- pending on the gradient updates for other training examples. Simply increasing the learning rate is inadequate, as it would cause the model to take overly large steps towards z , potentially causing optimization to diverge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Solution: the meritocratic update rule</head><p>To solve this problem, we want the upweighting to be more "meritocratic": any program that obtains reward should be upweighted roughly equally.</p><p>We first observe that J MML already improves over J RL in this regard. From (6), we see that the gradient weight q MML (z) is the policy distribution restricted to and renormalized over only reward- earning programs. This renormalization makes the gradient weight uniform across examples: even if all reward-earning programs for a particular exam- ple have very low model probability, their com- bined gradient weight z q MML (z) is always 1. In our experiments, J MML performs significantly better than J RL <ref type="table" target="#tab_3">(Table 4)</ref>.</p><p>However, while J MML assigns uniform weight across examples, it is still not uniform over the programs within each example. Hence we propose a new update rule which goes one step further in pursuing uniform updates. Extending q MML (z), we define a β-smoothed version:</p><formula xml:id="formula_16">q β (z) = q MML (z) β ˜ z q MML (˜ z) β .<label>(11)</label></formula><p>When β = 0, our weighting is completely uni- form across all reward-earning programs within an example while β = 1 recovers the original MML weighting. Our new update rule is to simply take a modified gradient step where q = q β . <ref type="bibr">4</ref> We will refer to this as the β-meritocratic update rule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Summary of the proposed approach</head><p>We described two problems <ref type="bibr">5</ref> and their solutions: we reduce exploration bias using -greedy ran- domized beam search and perform more balanced optimization using the β-meritocratic parameter update rule. We call our resulting approach RAN- DOMER. <ref type="table">Table 1</ref> summarizes how RANDOMER combines desirable qualities from both REIN- FORCE and BS-MML.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>Evaluation. We evaluate our proposed methods on all three domains of the SCONE dataset. Accu- racy is defined as the percentage of test examples where the model produces the correct final world state w M . All test examples have M = 5 (5utts), but we also report accuracy after processing the first 3 utterances (3utts). To control for the effects of randomness, we train 5 instances of each model with different random seeds. We report the median accuracy of the instances unless otherwise noted.  <ref type="bibr">Abadi et al., 2015)</ref>. Model parameters are ran- domly initialized <ref type="bibr" target="#b14">(Glorot and Bengio, 2010)</ref>, with no pre-training. We use the Adam optimizer ( <ref type="bibr" target="#b17">Kingma and Ba, 2014)</ref> (which is applied to the gradient in <ref type="formula">(6)</ref>), a learning rate of 0.001, a mini- batch size of 8 examples (different from the beam size), and train until accuracy on the validation set converges (on average about 13,000 steps). We</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approximation of Eq [·]</head><p>Exploration strategy Gradient weight q(z) REINFORCE Monte Carlo integration independent sampling <ref type="table">Table 1</ref>: RANDOMER combines qualities of both REINFORCE (RL) and BS-MML. For approximating the expectation over q in the gradient, we use numerical integration as in BS-MML. Our exploration strategy is a hybrid of search (MML) and off-policy sampling (RL). Our gradient weighting is equivalent to MML when β = 1 and more "meritocratic" than both MML and REINFORCE for lower values of β.</p><formula xml:id="formula_17">p θ (z | x) BS-MML numerical integration beam search p θ (z | x, R(z) = 0) RANDOMER numerical integration randomized beam search q β (z)</formula><p>use fixed GloVe vectors ( <ref type="bibr" target="#b33">Pennington et al., 2014)</ref> to embed the words in each utterance.</p><p>Hyperparameters. For all models, we per- formed a grid search over hyperparameters to maximize accuracy on the validation set. Hy- perparameters include the learning rate, the baseline in REINFORCE, -greediness and β- meritocraticness. For REINFORCE, we also ex- perimented with a regression-estimated baseline ( <ref type="bibr" target="#b34">Ranzato et al., 2015</ref>), but found it to perform worse than a constant baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Main results</head><p>Comparison to prior work. <ref type="table" target="#tab_1">Table 2</ref> compares RANDOMER to results from Long et al. <ref type="formula" target="#formula_4">(2016)</ref> as well as two baselines, REINFORCE and BS- MML (using the same neural model but differ- ent learning algorithms). Our approach achieves new state-of-the-art results by a significant mar- gin, especially on the SCENE domain, which fea- tures the most complex program syntax. We report the results for REINFORCE, BS-MML, and RAN- DOMER on the seed and hyperparameters that achieve the best validation accuracy. We note that REINFORCE performs very well on TANGRAMS but worse on ALCHEMY and very poorly on SCENE. This might be because the pro- gram syntax for TANGRAMS is simpler than the other two: there is no other way to refer to objects except by index.</p><p>We also found that REINFORCE required - greedy exploration to make any progress. Us- ing -greedy greatly skews the Monte Carlo ap- proximation of J RL , making it more uniformly weighted over programs in a similar spirit to us- ing β-meritocratic gradient weights q β . However, q β increases uniformity over reward-earning pro- grams only, rather than over all programs.</p><p>Effect of randomized beam search.   search to 128, it still does not surpass randomized beam search with a beam of 32, and further in- creases yield no additional improvement.</p><p>Effect of β-meritocratic updates. <ref type="table" target="#tab_3">Table 4</ref> eval- uates the impact of β-meritocratic parameter up- dates (gradient weight q β ). More uniform up- weighting across reward-earning programs leads to higher accuracy and fewer spurious programs, especially in SCENE. However, no single value of β performs best over all domains.</p><p>Choosing the right value of β in RANDOMER significantly accelerates training. <ref type="figure" target="#fig_2">Figure 3</ref> illus- trates that while β = 0 and β = 1 ultimately achieve similar accuracy on ALCHEMY, β = 0 reaches good performance in half the time.</p><p>Since lowering β reduces trust in the model pol- icy, β &lt; 1 helps in early training when the cur- rent policy is untrustworthy. However, as it grows more trustworthy, β &lt; 1 begins to pay a price for ignoring it. Hence, it may be worthwhile to anneal β towards 1 over time. <ref type="bibr">ALCHEMY TANGRAMS SCENE q(z)</ref> 3utts 5utts 3utts 5utts 3utts 5utts qRL 0.2 0.0 0.9 0.6 0.0 0.0 qMML (q β=1 ) 61.3 48.3</p><p>65.2 34.3 50.8 33.5 q β=0. <ref type="bibr">25</ref> 64.4 48.9 60.6 29.0 42.4 29.7 q β=0</p><p>63.6 46.3 54.0 23.5 61.0 42.4  Effect of execution history embedding. Ta- ble 5 compares our two proposals for embed- ding the execution history: TOKENS and STACK. STACK performs better in the two domains where an object can be referenced in multiple ways (SCENE and ALCHEMY). STACK directly embeds objects on the stack, invariant to the way in which they were pushed onto the stack, unlike TOKENS. We hypothesize that this invariance increases ro- bustness to spurious behavior: if a program acci- dentally pushes the right object onto the stack via spurious means, the model can still learn the re- maining steps of the program without conditioning on a spurious history.</p><p>Fitting vs overfitting the training data. Ta- ble 6 reveals that BS-MML and RANDOMER use different strategies to fit the training data. On the depicted training example, BS-MML actually achieves higher expected reward / marginal prob- ability than RANDOMER, but it does so by putting most of its probability on a spurious program- a form of overfitting. In contrast, RANDOMER spreads probability mass over multiple reward- earning programs, including the correct ones. As a consequence of overfitting, we observed at test time that BS-MML only references people by positional indices instead of by shirt or hat color, whereas RANDOMER successfully learns to use multiple reference strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related work and discussion</head><p>Semantic parsing from indirect supervision.  <ref type="bibr" target="#b2">and Zettlemoyer, 2011</ref><ref type="bibr" target="#b3">and Zettlemoyer, , 2013</ref><ref type="bibr" target="#b35">Reddy et al., 2014;</ref><ref type="bibr" target="#b31">Pasupat and Liang, 2015)</ref>. We are interested in the initial stages of training from scratch, where getting any training signal is difficult due to the combinatorially large search space. We also high- lighted the problem of spurious programs which capture reward but give incorrect generalizations.</p><p>Maximum marginal likelihood with beam search (BS-MML) is traditionally used to learn se- mantic parsers from indirect supervision.</p><p>Reinforcement learning. Concurrently, there has been a recent surge of interest in reinforce- ment learning, along with the wide application of the classic REINFORCE algorithm <ref type="bibr" target="#b42">(Williams, 1992)</ref>-to troubleshooting <ref type="bibr" target="#b7">(Branavan et al., 2009)</ref>, dialog generation ( <ref type="bibr" target="#b20">Li et al., 2016)</ref>, game playing ( <ref type="bibr" target="#b25">Narasimhan et al., 2015)</ref>, coreference resolution <ref type="bibr" target="#b8">(Clark and Manning, 2016)</ref>, machine translation ( , and even semantic parsing ( <ref type="bibr" target="#b21">Liang et al., 2017)</ref>. Indeed, the challenge of train- ing semantic parsers from indirect supervision is perhaps better captured by the notion of sparse re- wards in reinforcement learning.</p><p>The RL answer would be better exploration, which can take many forms including simple action-dithering such as -greedy, entropy regular- ization <ref type="bibr" target="#b43">(Williams and Peng, 1991)</ref>, Monte Carlo tree search <ref type="bibr" target="#b10">(Coulom, 2006</ref>), randomized value functions ( <ref type="bibr" target="#b30">Osband et al., 2014</ref><ref type="bibr" target="#b29">Osband et al., , 2016</ref>, and meth- ods which prioritize learning environment dynam- ics <ref type="bibr" target="#b13">(Duff, 2002</ref>) or under-explored states ( <ref type="bibr" target="#b16">Kearns and Singh, 2002;</ref><ref type="bibr" target="#b5">Bellemare et al., 2016;</ref><ref type="bibr" target="#b24">Nachum et al., 2016)</ref>. The majority of these methods em- ploy Monte Carlo sampling for exploration. In Utterance: the man in the purple shirt and red hat moves just to the right of the man in the red shirt and yellow hat program prob RANDOMER ( = 0.15, β = 0) * move(hasHat(red), rightOf(hasHat(red))) 0.009 x move(index(allPeople, 2), 3) 0.008 <ref type="table">Table 6</ref>: Top-scoring predictions for a training ex- ample from SCENE (* = correct, o = spurious, x = incorrect). RANDOMER distributes probabil- ity mass over numerous reward-earning programs (including the correct ones), while classic beam search MML overfits to one spurious program, giving it very high probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.122</head><p>contrast, we find randomized beam search to be more suitable in our setting, because it explores low-probability states even when the policy distri- bution is peaky. Our β-meritocratic update also depends on the fact that beam search returns an entire set of reward-earning programs rather than one, since it renormalizes over the reward-earning set. While similar to entropy regularization, β- meritocratic update is more targeted as it only in- creases uniformity of the gradient among reward- earning programs, rather than across all programs.</p><p>Our strategy of using randomized beam search and meritocratic updates lies closer to MML than RL, but this does not imply that RL has nothing to offer in our setting. With the simple connec- tion between RL and MML we established, much of the literature on exploration and variance reduc- tion in RL can be directly applied to MML prob- lems. Of special interest are methods which incor- porate a value function such as actor-critic.</p><p>Maximum likelihood and RL. It is tempting to group our approach with sequence learning meth- ods which interpolate between supervised learn- ing and reinforcement learning ( <ref type="bibr" target="#b34">Ranzato et al., 2015;</ref><ref type="bibr" target="#b41">Venkatraman et al., 2015;</ref><ref type="bibr" target="#b37">Ross et al., 2011;</ref><ref type="bibr" target="#b19">Levine, 2014</ref>). These methods generally seek to make RL training easier by pre-training or "warm-starting" with fully supervised learning. This requires each training example to be labeled with a reasonably correct output sequence. In our setting, this would amount to labeling each example with the correct program, which is not known. Hence, these meth- ods cannot be directly applied.</p><p>Without access to correct output sequences, we cannot directly maximize likelihood, and in- stead resort to maximizing the marginal likelihood (MML). Rather than proposing MML as a form of pre-training, we argue that MML is a superior sub- stitute for the standard RL objective, and that the β-meritocratic update is even better.</p><p>Simulated annealing. Our β-meritocratic up- date employs exponential smoothing, which bears resemblance to the simulated annealing strategy of Och <ref type="formula" target="#formula_5">(2003)</ref>; <ref type="bibr" target="#b39">Smith and Eisner (2006)</ref>; <ref type="bibr" target="#b38">Shen et al. (2015)</ref>. However, a key difference is that these methods smooth the objective function whereas we smooth an expectation in the gradient. To un- derscore the difference, we note that fixing β = 0 in our method (total smoothing) is quite effective, whereas total smoothing in the simulated anneal- ing methods would correspond to a completely flat objective function, and an uninformative gradient of zero everywhere.</p><p>Neural semantic parsing. There has been re- cent interest in using recurrent neural networks for semantic parsing, both for modeling logical forms <ref type="bibr" target="#b12">(Dong and Lapata, 2016;</ref><ref type="bibr" target="#b15">Jia and Liang, 2016;</ref><ref type="bibr" target="#b21">Liang et al., 2017)</ref> and for end-to-end execution <ref type="bibr" target="#b44">(Yin et al., 2015;</ref><ref type="bibr" target="#b26">Neelakantan et al., 2016</ref>). We develop a neural model for the context-dependent setting, which is made possible by a new stack- based language similar to <ref type="bibr" target="#b36">Riedel et al. (2016)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>"Figure 1 :</head><label>1</label><figDesc>Figure 1: The task is to map natural language utterances to a program that manipulates the world state. The correct program captures the true meaning of the utterances, while spurious programs arrive at the correct output for the wrong reasons. We develop methods to prevent the model from being drawn to spurious programs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Training.</head><label></label><figDesc>Following Long et al. (2016), we de- compose each training example into smaller ex- amples. Given an example with 5 utterances, u = [u 1 , . . . , u 5 ], we consider all length-1 and length-2 substrings of u: [u 1 ], [u 2 ], . . . , [u 3 , u 4 ], [u 4 , u 5 ] (9 total). We form a new training example from each substring, e.g., (u , w 0 , w M ) where u = [u 4 , u 5 ], w 0 = w 3 and w M = w 5 . All models are implemented in TensorFlow (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Validation set accuracy (y-axis) across training iterations (x-axis) on ALCHEMY. We compare RANDOMER, BS-MML and REINFORCE. Vertical lines mark the first time each model surpasses 60% accuracy. RANDOMER with β = 0 reaches this point twice as fast as β = 1. REINFORCE plateaus for a long time, then begins to climb after 40k iterations (not shown). Training runs are averaged over 5 seeds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 3 shows</head><label>3</label><figDesc>that -greedy randomized beam search con- sistently outperforms classic beam search. Even when we increase the beam size of classic beam</figDesc><table>ALCHEMY TANGRAMS 
SCENE 
system 
3utts 5utts 3utts 5utts 3utts 5utts 
LONG+16 
56.8 52.3 
64.9 27.6 
23.2 14.7 
REINFORCE 58.3 44.6 
68.5 37.3 
47.8 33.9 
BS-MML 
58.7 47.3 
62.6 32.2 
53.5 32.5 
RANDOMER 
66.9 52.9 
65.8 37.1 
64.8 46.2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Comparison to prior work. LONG+16 
results are directly from Long et al. (2016). Hy-
perparameters are chosen by best performance on 
validation set (see Appendix A). 

ALCHEMY TANGRAMS 
SCENE 
random beam 3utts 5utts 3utts 5utts 3utts 5utts 
classic beam search 
None 
32 
30.3 23.2 
0.0 0.0 
33.4 20.1 
None 
128 
59.0 46.4 
60.9 28.6 
24.5 13.9 
randomized beam search 
= 0.05 32 
58.7 45.5 
61.1 32.5 
33.4 23.0 
= 0.15 32 
61.3 48.3 
65.2 34.3 
50.8 33.5 
= 0.25 32 
60.5 48.6 
60.0 27.3 
54.1 35.7 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Randomized beam search. All listed 
models use gradient weight q MML and TOKENS to 
represent execution history. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 : β-meritocratic updates.</head><label>4</label><figDesc></figDesc><table>All listed 
models use randomized beam search, = 0.15 
and TOKENS to represent execution history. 

ALCHEMY TANGRAMS 
SCENE 
3utts 5utts 3utts 5utts 3utts 5utts 
HISTORY 61.3 48.3 
65.2 34.3 
50.8 33.5 
STACK 
64.2 53.2 
63.0 32.4 
59.5 43.1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>TOKENS vs STACK embedding. Both 
models use = 0.15 and gradient weight q MML . 

</table></figure>

			<note place="foot" n="1"> https://nlp.stanford.edu/projects/scone</note>

			<note place="foot" n="2"> The number of well-formed programs in SCENE exceeds 10 15</note>

			<note place="foot" n="3"> Note that the log of the product in (5) does not equal the sum in (2).</note>

			<note place="foot" n="4"> Also, note that if exploration were exhaustive, β = 0 would be equivalent to supervised learning using the set of all reward-earning programs as targets. 5 These problems concern the gradient w.r.t. a single example. The full gradient averages over multiple examples, which helps separate correct from spurious. E.g., if multiple examples all mention &quot;yellow hat&quot;, we will find a correct program parsing this as hasHat(yellow) for each example, whereas the spurious programs we find will follow no consistent pattern. Consequently, spurious gradient contributions may cancel out while correct program gradients will all &quot;vote&quot; in the same direction.</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REINFORCE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1061</head><p>B SCONE domains and program tokens token type semantics Shared across ALCHEMY, TANGRAMS, SCENE 1, 2, 3, . . . constant push: number -1, -2, -3, . . . red, yellow, green, constant push: color orange, purple, brown allObjects constant push: the list of all objects index function pop: a list L and a number i push: the object L[i] (the index starts from 1; negative indices are allowed) prevArgj (j = 1, 2) function pop: a number i push: the j argument from the ith action prevAction action pop: a number i perform: fetch the ith action and execute it using the arguments on the stack Additional tokens for the ALCHEMY domain An ALCHEMY world contains 7 beakers. Each beaker may contain up to 4 units of colored chemical. 1/1 constant push: fraction (used in the drain action) hasColor function pop: a color c push: list of beakers with chemical color c drain action pop: a beaker b and a number or fraction a perform: remove a units of chemical (or all chemical if a = 1/1) from b pour action pop: two beakers b1 and b2 perform: transfer all chemical from b1 to b2 mix action pop: a beaker b perform: turn the color of the chemical in b to brown Additional tokens for the TANGRAMS domain A TANGRAMS world contains a row of tangram pieces with different shapes. The shapes are anonymized; a tangram can be referred to by an index or a history reference, but not by shape. swap action pop: two tangrams t1 and t2 perform: exchange the positions of t1 and t2 remove action pop: a tangram t perform: remove t from the stage add action pop: a number i and a previously removed tangram t perform: insert t to position i Additional tokens for the SCENE domain A SCENE world is a linear stage with 10 positions. Each position may be occupied by a person with a colored shirt and optionally a colored hat. There are usually 1-5 people on the stage. noHat constant push: pseudo-color (indicating that the person is not wearing a hat) hasShirt, hasHat function pop: a color c push: the list of all people with shirt or hat color c hasShirtHat function pop: two colors c1 and c2 push: the list of all people with shirt color c1 and hat color c2 leftOf, rightOf function pop: a person p push: the location index left or right of p create action pop: a number i and two colors c1, c2 perform: add a new person at position i with shirt color c1 and hat color c2 move action pop: a person p and a number i perform: move p to position i swapHats action pop: two people p1 and p2 perform: have p1 and p2 exchange their hats leave action pop: a person p perform: remove p from the stage</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Józefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">B</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<title level="m">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bootstrapping semantic parsers from conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="421" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of semantic parsers for mapping instructions to actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="49" to="62" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unifying countbased exploration and intrinsic motivation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Saxton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1471" to="1479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1171" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Reinforcement learning for mapping instructions to actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACLIJCNLP)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="82" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning for mention-ranking coreference models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08667</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Driving semantic parsing from the world&apos;s response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Goldwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Natural Language Learning (CoNLL)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="18" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient selectivity and backup operators in Monte-Carlo tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Coulom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computers and Games</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="72" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">N M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Language to logical form with neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Optimal Learning: Computational procedures for Bayes-adaptive Markov decision processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">O</forename><surname>Duff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts Amherst</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Data recombination for neural semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Near-optimal reinforcement learning in polynomial time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="209" to="232" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Weakly supervised training of semantic parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="754" to="765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Motor Skill Learning with Local Trajectory Methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for dialogue generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neural symbolic machines: Learning semantic parsers on Freebase with weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">D F N</forename><surname>Lao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning dependency-based compositional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="590" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Simpler context-dependent logical forms via model projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Improving policy gradient by exploring under-appreciated rewards</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nachum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09321</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Language understanding for text-based games using deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.08941</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Neural programmer: Inducing latent programs with gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Reward augmented maximum likelihood for neural structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1723" to="1731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Minimum error rate training in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep exploration via bootstrapped DQN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4026" to="4034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1402.0635</idno>
		<title level="m">Generalization and exploration via randomized value functions</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Compositional semantic parsing on semi-structured tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Inferring logical forms from denotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06732</idno>
		<title level="m">Sequence level training with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Largescale semantic parsing without question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="377" to="392" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Programming with a differentiable forth interpreter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bosnjak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rocktäschel</surname></persName>
		</author>
		<idno>abs/1605.06640</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A reduction of imitation learning and structured prediction to noregret online learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics (AISTATS)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.02433</idno>
		<title level="m">Minimum risk training for neural machine translation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Minimum risk annealing for training log-linear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Linguistics and Association for Computational Linguistics (COLING/ACL)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="787" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Policy gradient methods for reinforcement learning with function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Improving multi-step prediction of learned time series models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Venkatraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3024" to="3030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Function optimization using connectionist reinforcement learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Connection Science</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="241" to="268" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.00965</idno>
		<title level="m">Neural enquirer: Learning to query tables</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
