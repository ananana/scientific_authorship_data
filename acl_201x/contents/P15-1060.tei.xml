<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:25+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sentiment-Aspect Extraction based on Restricted Boltzmann Machines</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 26-31, 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Interdisciplinary Information Sciences</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Interdisciplinary Information Sciences</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Interdisciplinary Information Sciences</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sentiment-Aspect Extraction based on Restricted Boltzmann Machines</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="616" to="625"/>
							<date type="published">July 26-31, 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Aspect extraction and sentiment analysis of reviews are both important tasks in opinion mining. We propose a novel sentiment and aspect extraction model based on Restricted Boltzmann Machines to jointly address these two tasks in an unsupervised setting. This model reflects the generation process of reviews by introducing a heterogeneous structure into the hidden layer and incorporating informative priors. Experiments show that our model outper-forms previous state-of-the-art methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Nowadays, it is commonplace for people to ex- press their opinion about various sorts of entities, e.g., products or services, on the Internet, espe- cially in the course of e-commerce activities. Ana- lyzing online reviews not only helps customers ob- tain useful product information, but also provide companies with feedback to enhance their prod- ucts or service quality. Aspect-based opinion min- ing enables people to consider much more fine- grained analyses of vast quantities of online re- views, perhaps from numerous different merchant sites. Thus, automatic identification of aspects of entities and relevant sentiment polarities in Big Data is a significant and urgent task <ref type="bibr" target="#b15">(Liu, 2012;</ref><ref type="bibr" target="#b18">Pang and Lee, 2008;</ref><ref type="bibr" target="#b20">Popescu and Etzioni, 2005)</ref>.</p><p>Identifying aspect and analyzing sentiment words from reviews has the ultimate goal of dis- cerning people's opinions, attitudes, emotions, etc. towards entities such as products, services, orga- nizations, individuals, events, etc. In this con- text, aspect-based opinion mining, also known as feature-based opinion mining, aims at extracting and summarizing particular salient aspects of enti- ties and determining relevant sentiment polarities * Corresponding Author: Kang Liu (kliu@nlpr.ia.ac.cn) from reviews ( <ref type="bibr" target="#b6">Hu and Liu, 2004</ref>). Consider re- views of computers, for example. A given com- puter's components (e.g., hard disk, screen) and attributes (e.g., volume, size) are viewed as aspects to be extracted from the reviews, while sentiment polarity classification consists in judging whether an opinionated review expresses an overall posi- tive or negative opinion.</p><p>Regarding aspect identification, previous meth- ods can be divided into three main categories: rule-based, supervised, and topic model-based methods. For instance, association rule-based methods ( <ref type="bibr" target="#b6">Hu and Liu, 2004;</ref><ref type="bibr" target="#b11">Liu et al., 1998)</ref> tend to focus on extracting product feature words and opinion words but neglect connecting product features at the aspect level. Existing rule-based methods typically are not able to group the ex- tracted aspect terms into categories. Supervised ( <ref type="bibr" target="#b8">Jin et al., 2009;</ref><ref type="bibr" target="#b3">Choi and Cardie, 2010)</ref> and semi- supervised learning methods <ref type="bibr" target="#b23">(Zagibalov and Carroll, 2008;</ref><ref type="bibr" target="#b17">Mukherjee and Liu, 2012)</ref> were intro- duced to resolve certain aspect identification prob- lems. However, supervised training requires hand- labeled training data and has trouble coping with domain adaptation scenarios.</p><p>Hence, unsupervised methods are often adopted to avoid this sort of dependency on labeled data. Latent Dirichlet Allocation, or LDA for short, ( <ref type="bibr" target="#b1">Blei et al., 2003</ref>) performs well in automatically extracting aspects and grouping corresponding representative words into categories. Thus, a num- ber of LDA-based aspect identification approaches have been proposed in recent years ( <ref type="bibr" target="#b2">Brody and Elhadad, 2010;</ref><ref type="bibr" target="#b22">Titov and McDonald, 2008;</ref><ref type="bibr" target="#b24">Zhao et al., 2010)</ref>. Still, these methods have several im- portant drawbacks. First, inaccurate approxima- tions of the distribution over topics may reduce the computational accuracy. Second, mixture models are unable to exploit the co-occurrence of topics to yield high probability predictions for words that are sharper than the distributions predicted by in-dividual topics <ref type="bibr" target="#b5">(Hinton and Salakhutdinov, 2009)</ref>.</p><p>To overcome the weaknesses of existing meth- ods and pursue the promising direction of jointly learning aspect and sentiment, we present the novel Sentiment-Aspect Extraction RBM (SERBM) model to simultaneously extract as- pects of entities and relevant sentiment-bearing words. This two-layer structure model is inspired by conventional Restricted Boltzmann machines (RBMs). In previous work, RBMs with shared parameters (RSMs) have achieved great success in capturing distributed semantic representations from text <ref type="bibr" target="#b5">(Hinton and Salakhutdinov, 2009)</ref>.</p><p>Aiming to make the most of their ability to model latent topics while also accounting for the structured nature of aspect opinion mining, we propose replacing the standard hidden lay- ers of RBMs with a novel heterogeneous struc- ture. Three different types of hidden units are used to represent aspects, sentiments, and back- ground words, respectively. This modification bet- ter reflects the generative process for reviews, in which review words are generated not only from the aspect distribution but also affected by senti- ment information. Furthermore, we blend back- ground knowledge into this model using priors and regularization to help it acquire more accurate fea- ture representations. After m-step Contrastive Di- vergence for parameter estimation, we can capture the required data distribution and easily compute the posterior distribution over latent aspects and sentiments from reviews. In this way, aspects and sentiments are jointly extracted from reviews, with limited computational effort. This model is hence a promising alternative to more complex LDA- based models presented previously. Overall, our main contributions are as follows:</p><p>1. Compared with previous LDA-based meth- ods, our model avoids inaccurate approxima- tions and captures latent aspects and senti- ment both adequately and efficiently.</p><p>2. Our model exploits RBMs' advantage in properly modeling distributed semantic rep- resentations from text, but also introduces heterogeneous structure into the hidden layer to reflect the generative process for online re- views. It also uses a form of regularization to incorporate prior knowledge into the model. Due these modifications, our model is very well-suited for solving aspect-based opinion mining tasks.</p><p>3. The optimal weight matrix of this RBM model can exactly reflect individual word features toward aspects and sentiment, which is hard to achieve with LDA-based models due to the mixture model sharing mechanism.</p><p>4. Last but not the least, this RBM model is ca- pable of jointly modeling aspect and senti- ment information together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>We summarize prior state-of-the-art models for as- pect extraction. In their seminal work, <ref type="bibr" target="#b6">Hu and Liu (2004)</ref> propose the idea of applying classical information extraction to distinguish different as- pects in online reviews. Methods following their approach exploit frequent noun words and depen- dency relations to extract product features without supervision ( <ref type="bibr" target="#b25">Zhuang et al., 2006</ref>; <ref type="bibr" target="#b12">Liu et al., 2005;</ref><ref type="bibr" target="#b21">Somasundaran and Wiebe, 2009</ref>). These methods work well when the aspect is strongly associated with a single noun, but obtain less satisfactory re- sults when the aspect emerges from a combination of low frequency items. Additionally, rule-based methods have a common shortcoming in failing to group extracted aspect terms into categories. Supervised learning methods ( <ref type="bibr" target="#b8">Jin et al., 2009;</ref><ref type="bibr" target="#b3">Choi and Cardie, 2010;</ref><ref type="bibr" target="#b7">Jakob and Gurevych, 2010;</ref><ref type="bibr" target="#b9">Kobayashi et al., 2007</ref>) such as Hidden Markov Models, one-class SVMs, and Condi- tional Random Fields have been widely used in aspect information extraction. These supervised approaches for aspect identification are generally based on standard sequence labeling techniques. The downside of supervised learning is its require- ment of large amounts of hand-labeled training data to provide enough information for aspect and opinion identification.</p><p>Subsequent studies have proposed unsuper- vised learning methods, especially LDA-based topic modeling, to classify aspects of comments. Specific variants include the Multi-Grain LDA model <ref type="bibr" target="#b22">(Titov and McDonald, 2008</ref>) to capture local rateable aspects, the two-step approach to detect aspect-specific opinion words <ref type="bibr" target="#b2">(Brody and Elhadad, 2010)</ref>, the joint sentiment/topic model (JST) by <ref type="bibr" target="#b10">Lin and He (2009)</ref>, the topic-sentiment mixture model with domain adaption ( <ref type="bibr" target="#b16">Mei et al., 2007</ref>), which treats sentiment as different topics, and MaxEnt-LDA ( <ref type="bibr" target="#b24">Zhao et al., 2010)</ref>, which inte- grates a maximum entropy approach into LDA. However, these LDA-based methods can only adopt inaccurate approximations for the posterior distribution over topics rather than exact inference. Additionally, as a mixture model, LDA suffers from the drawbacks mentioned in Section 1 that are common to all mixture models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>In order to improve over previous work, we first introduce a basic RBM-based model and then de- scribe our modified full model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Basic RBM-based Model</head><p>Restricted Boltzmann Machines can be used for topic modeling by relying on the structure shown in <ref type="figure" target="#fig_0">Figure 1</ref>. As shown on the left side of the fig- ure, this model is a two-layer neural network com- posed of one visible layer and one hidden layer. The visible layer consists of a softmax over dis- crete visible units for words in the text, while the hidden layer captures its topics. More precisely, the visible layer is represented as a K × D ma- trix v, where K is the dictionary size, and D is the document length. Here, if visible unit i in v takes the k-th value, we set v k i = 1. The hidden layer can be expressed as h ∈ {0, 1} F , where F is the number of hidden layer nodes, corresponding to topics. The right side of <ref type="figure" target="#fig_0">Figure 1</ref> is another way of viewing the network, with a single multinomial visible unit <ref type="bibr" target="#b5">(Hinton and Salakhutdinov, 2009</ref>).</p><p>The energy function of the model can be defined as</p><formula xml:id="formula_0">E(v, h) = − D i=1 F j=1 K k=1 W k ij h j v k i − D i=1 K k=1 v k i b k i − F j=1 h j a j ,<label>(1)</label></formula><p>where W k ij specifies the connection weight from the i-th visible node of value k to the j-th hidden node, b k i corresponds to a bias of v k i , and a j corre- sponds to a bias of h j .</p><p>The probability of the input layer v is defined as</p><formula xml:id="formula_1">P (v) = 1 Z h exp(−E(v, h)),<label>(2)</label></formula><p>where Z is the partition function to normalize the probability. The conditional probabilities from the hidden to the visible layer and from the visible to the hidden one are given in terms of a softmax and logistic function, respectively, i.e.</p><formula xml:id="formula_2">P ( v k i = 1 | h) = exp b k i + F j=1 h j W k ij K q=1 exp b q i + F j=1 h j W q ij , P ( h j = 1 | v) = σ a j + D i=1 K k=1 v k i W k ij , (3) where σ(x) = 1/(1 + exp(−x))</formula><p>is the logistic function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Our Sentiment-Aspect Extraction model</head><p>While the basic RBM-based method provides a simple model of latent topics, real online reviews require a more fine-grained model, as they con- sist of opinion aspects and sentiment information. Therefore, aspect identification is a different task from regular topic modeling and the basic RBM- based model may not perform well in aspect ex- traction for reviews.</p><p>To make the most of the ability of the basic RBM-based model in extracting latent topics, and obtain an effective method that is well-suited to solve aspect identification tasks, we present our novel Sentiment-Aspect Extraction RBM model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Generative Perspective</head><p>From a generative perspective, product reviews can be regarded as follows. Every word in a review text may describe a specific aspect (e.g. "expensive" for the price aspect), or an opinion (e.g. "amazing" for a positive sentiment and "ter- rible" for a negative one), or some irrelevant back- ground information (e.g. "Sunday"). In a genera- tive model, a word may be generated from a latent aspect variable, a sentiment variable, or a back- ground variable. Also, there may exist certain re- lations between such latent variables.  </p><formula xml:id="formula_3">! id1_DT id2_NN id3_CC id4_NNS id5_NN id6_JJ id7_VBZ id8_JJ Sentence POS ! v1 vD hi hk ! ! ! h1 hi ! ! hj hk hF hF Aspect Sentiment Background φ2 φ4 φ1 φ id_i"#"word count_i D v1 vD ! vD ! φ v1 W1,F</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Structure</head><p>To simulate this generative process for reviews, we adapt the standard RBM structure to reflect the aspect-sentiment identification task.</p><p>Undirected Model. Our Sentiment-Aspect Ex- traction model structure is illustrated in <ref type="figure" target="#fig_2">Figure 2</ref>.</p><p>Compared to standard RBMs, a crucial differ- ence is that hidden units now have a heterogeneous structure instead of being homogeneous as in the standard basic RBM model. In particular, we rely on three types of hidden units, representing aspect, sentiment, and background, respectively. The first two types are self-explanatory, while the back- ground units are intended to reflect the kind of words that do not contribute much to the aspect or sentiment information of review documents. Since the output of the hidden units is a re-encoding of the information in the visible layer, we obtain a deeper representation and a more precise expres- sion of information in the input reviews. Thus, this approach enables the model to learn multi-faceted information with a simple yet expressive structure.</p><p>To formalize this, we denote</p><formula xml:id="formula_4">v k = D i=1 v k i</formula><p>as the count for the k-th word, where D is the doc- ument length. The energy function can then be defined as follows:</p><formula xml:id="formula_5">E(v, h) = − F j=1 K k=1 W k j h j v k − K k=1 v k b k − F j=1 h j a j ,<label>(4)</label></formula><p>where W k j denotes the weight between the k-th visible unit and the j-th hidden unit. The conditional probability from visible to hid- den unit can be expressed as:</p><formula xml:id="formula_6">P (h j = 1|v) = σ(a j + K k=1 v k W k j ).<label>(5)</label></formula><p>In an RBM, every hidden unit can be activated or restrained by visible units. Thus, every visible unit has a potential contribution towards the acti- vation of a given hidden unit. The probability of whether a given visible unit affects a specific hid- den unit is described as follows (cf. appendix for details):</p><formula xml:id="formula_7">P (h j = 1 | v k ) =P (h j = 1 | h −j , v k ) =σ(a j + W k j v k ).<label>(6)</label></formula><p>Under this architecture, this equation can be ex- plained as the conditional probability from visible unit k to hidden unit j (softmax of words to as- pect or sentiment). According to Eq. 6, the con- ditional probability for the k-th word feature to- wards the j-th aspect or sentiment p(h j = 1 | v k ) is a monotone function of W k j , the (k, j)-th entry of the optimal weight matrix. Thus, the optimal weight matrix of this RBM model can directly re- flect individual word features toward aspects and sentiment.</p><p>Informative Priors. To improve the ability of the model to extract aspects and identify senti- ments, we capture priors for words in reviews and incorporate this information into the learning pro- cess of our Sentiment-Aspect Extraction model. We regularize our model based on these priors to constrain the aspect modeling and improve its ac- curacy. <ref type="figure">Figure 3</ref> provides an example of how such priors can be applied to a sentence, with φ i repre- senting the prior knowledge.</p><p>Research has found that most aspect words are nouns (or noun phrases), and sentiment is often expressed with adjectives. This additional infor- mation has been utilized in previous work on as- pect extraction ( <ref type="bibr" target="#b6">Hu and Liu, 2004;</ref><ref type="bibr" target="#b0">Benamara et al., 2007;</ref><ref type="bibr" target="#b19">Pang et al., 2002</ref>). Inspired by this, we first rely on Part of Speech (POS) Tagging to iden- tify nouns and adjectives. For all noun words, we first calculate their term frequency (TF) in the re- view corpus, and then compute their inverse doc- ument frequency (IDF) from an external Google n-gram corpus <ref type="bibr">1</ref> . Finally, we rank their TF * IDF . For all adjective words, if the words are also included in the online sentiment resource SentiWordNet 2 , we assign prior probabil- ity p s,v k to suggest that these words are generally recognized as sentiment words. Apart from these general priors, we obtain a small amount of fine-grained information as an- other type of prior knowledge. This fine-grained prior knowledge serves to indicate the probabil- ity of a known aspect word belonging to a specific aspect, denoted as p A j ,v k and an identified senti- ment word bearing positive or negative sentiment, denoted as p S j ,v k . For instance, "salad" is always considered as a general word that belongs to the specific aspect food, and "great" is generally con- sidered a positive sentiment word.</p><p>To extract p A j ,v k , we apply regular LDA on the review dataset. Since the resulting topic clusters are unlabeled, we manually assign top k words from the topics to the target aspects. We thus obtain fine-grained prior probabilities to suggest these words as belonging to specific aspects. To obtain p S j ,v k , we rely on SentiWordNet and sum up the probabilities of an identified sentiment word being positive or negative sentiment-bearing, respectively. Then we adopt the corresponding percentage value as a fine-grained specific senti- ment prior.</p><p>It is worthwhile to mention that the priors are not a compulsory component. However, the pro- cedure for obtaining priors is generic and can eas-ily be applied to any given dataset. Furthermore, we only obtain such fine-grained prior knowledge for a small amount of words in review sentences and rely on the capability of model itself to deal with the remaining words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Objective Function</head><p>We now construct an objective function for our SERBM model that includes regularization based on the priors defined above in Section 3.2.2. Sup- pose that the training set is S = v 1 , v 2 , . . . , v ns , where n s is the number of training objects. Each element has the form v i = (v i 1 , v i 2 , . . . , v i K ) D , where i = 1, 2, . . . , n s , and these data points are assumed to be independent and identically dis- tributed.</p><p>We define the following novel log-likelihood function ln L S , with four forms of regularization corresponding to the four kinds of priors:</p><formula xml:id="formula_8">ln L S = ln ns i=1 P (v i ) − ns i=1 λ 1 ln F 1 −1 j=1 k∈R 1 P (h j = 1 | v k ) − p A j ,v k 2 + λ 2 ln k∈R 2 F 1 j=1 P (h j = 1 | v k ) − p A,v k 2 + λ 3 ln F 2 +1 j=F 2 k∈R 3 P (h j = 1 | v k ) − p S j ,v k 2 + λ 4 ln k∈R 4 F 2 +1 j=F 2 P (h j = 1 | v k ) − p S,v k 2<label>(7)</label></formula><p>Here, P (h j = 1 | v k ) stands for the probability of a given input word belonging to a specific hidden unit. We assume all λ i &gt; 0 for i = 1 . . . 4, while F 1 and F 2 are integers for the offsets within the hidden layer. Units up to index F 1 capture aspects, with the last one reserved for miscellaneous Other Aspects, while units from F 2 capture the sentiment (with F 1 = F 2 + 1 &lt; F for convenience).</p><p>Our goal will be to maximize the log-likelihood ln L S in order to adequately model the data, in ac- cordance with the regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Training</head><p>We use Stochastic Gradient Descent (SGD) to find suitable parameters that maximize the objective function. Given a single training instance v from the training set S, we obtain</p><formula xml:id="formula_9">∂ ln L ∂θ = ∂ ln P (v) ∂θ − λ 1 F 1 −1 j=1 k∈R 1 ∂ ln P (h j = 1 | v k ) − p A j ,v k 2 ∂θ − λ 2 k∈R 2 ∂ ln F 1 j=1 P (h j = 1 | v k ) − p A,v k 2 ∂θ − λ 3 F 2 +1 j=F 2 k∈R 3 ∂ ln P (h j = 1 | v k ) − p S j ,v k 2 ∂θ − λ 4 k∈R 4 ∂ ln F 2 +1 j=F 2 P (h j = 1 | v k ) − p S,v k 2</formula><p>∂θ (8) where θ = {W, a j , b i } stands for the parameters. Given N documents {v n } N n=1 , the first term in the log-likelihood function with respect to W is:  <ref type="bibr" target="#b5">Hinton and Salakhutdinov, 2009)</ref>. Due to the m steps of transfer between input and hidden layers in a CD-m run of the algorithm, the two types of hidden units, aspect and sentiment, will jointly affect input reviews together with the connection matrix between the two layers.</p><formula xml:id="formula_10">1 N N n=1 ∂ ln P (v n ) ∂W k j = E D 1 [ˆ v k h j ] − E D 2 [ˆ v k h j ].<label>(9)</label></formula><p>Finally, we consider the partial derivative of the entire log-likelihood function with respect to the parameter W . Denoting ln ∂L ∂W as W , in each step we update W k j by adding</p><formula xml:id="formula_11">λ P (h j = 1|v (0) )v (0) k − P (h j = 1|v (cdm) )v (cdm) k − λ 1 F 1 −1 j=1 k∈R 1 2G j v k (1 + G j ) 2 ( 1 1+G j − p A j ,v k ) − λ 2 k∈R 2 2 v k F 1 j=1 1 (1+G j ) − p A,v k F 1 j=1 G j (1 + G j ) 2 − λ 3 F 2 +1 j=F 2 k∈R 3 2G j v k (1 + G j ) 2 ( 1 1+G j − p S j ,v k ) − λ 4 k∈R 4 2 v k F 2 +1 j=F 2 1 (1+G j ) − p S,v k F 2 +1 j=F 2 G j (1 + G j ) 2 , where G j =e −(a j +W k j v k )</formula><p>for convenience, and v (cdm) is the result from the CD-m steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We present a series of experiments to evaluate our model's performance on the aspect identification and sentiment classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data</head><p>For this evaluation, we rely on a restaurant review dataset widely adopted by previous work ( <ref type="bibr" target="#b4">Ganu et al., 2009;</ref><ref type="bibr" target="#b2">Brody and Elhadad, 2010;</ref><ref type="bibr" target="#b24">Zhao et al., 2010)</ref>, which contains 1,644,923 tokens and 52,574 documents in total. Documents in this dataset are annotated with one or more labels from a gold standard label set S = {Food, Staff, Ambi- ence, Price, Anecdote, Miscellaneous}. Following the previous studies, we select reviews with less than 50 sentences and remove stop words. The Stanford POS Tagger 3 is used to distinguish noun and adjective words from each other.</p><p>We later also rely on the Polarity dataset v2.0 4 to conduct an additional experiment on senti- ment classification in order to better assess the model's overall performance. This dataset focuses on movie reviews and consists of 1000 positive review documents and 1000 negative ones. It has also been used in the experiments by <ref type="bibr" target="#b10">Lin &amp; He (2009)</ref>, among others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Aspect Identification</head><p>We first apply our novel model to identify aspects from documents in the restaurant review dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Experimental Setup</head><p>For the experimental setup, we use ten hidden units in our Sentiment-Aspect Extraction RBM (SERBM), where units 0-6 capture aspects, units 7-8 capture sentiment information, and unit 9 stores background information. In particular, we fix hidden units 0-6 to represent the target aspects Food, Staff, Ambience, Price, Ambience, Miscella- neous, and Other Aspects, respectively. Units 7-8 represent positive and negative sentiment, respec- tively. The remaining hidden unit is intended to capture irrelevant background information.</p><p>Note that the structure of our model needs no modifications for new reviews. There are two cases for datasets from a new domain. If the new Method RBM RSM SERBM PPL 49.73 39.19 21.18 <ref type="table">Table 1</ref>: Results in terms of perplexity dataset has a gold standard label set, then we as- sign one hidden unit to represent each label in the gold standard set. If not, our model only obtains the priors p A,v k and p S,v k , and the aspect set can be inferred as in the work of <ref type="bibr" target="#b24">Zhao et al. (2010)</ref>. For evaluation, following previous work, the an- notated data is fed into our unsupervised model, without any of the corresponding labels. The model is then evaluated in terms of how well its prediction matches the true labels. As for hyperpa- rameter optimization, we use the perplexity scores as defined in Eq. 10 to find the optimal hyper- parameters.</p><p>As a baseline, we also re-implement standard RBMs and the RSM model <ref type="bibr" target="#b5">(Hinton and Salakhutdinov, 2009</ref>) to process this same restaurant re- view dataset and identify aspects for every doc- ument in this dataset under the same experimental conditions. We recall that RSM is a similar undi- rected graphical model that models topics from raw text.</p><p>Last but not the least, we conduct addi- tional comparative experiments, including with LocLDA ( <ref type="bibr" target="#b2">Brody and Elhadad, 2010)</ref>, MaxEnt-LDA ( <ref type="bibr" target="#b24">Zhao et al., 2010</ref>) and the SAS model (Mukherjee and Liu, 2012) to extract aspects for this restaurant review dataset under the same experimental conditions. In the following, we use the abbreviated name MELDA to stand for the MaxEnt LDA method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Evaluation</head><p>Brody and Elhadad (2010) and <ref type="bibr" target="#b24">Zhao et al. (2010)</ref> utilize three aspects to perform a quantitative eval- uation and only use sentences with a single label for evaluation to avoid ambiguity. The three major aspects chosen from the gold standard labels are S = {Food, Staff, Ambience}. The evaluation cri- terion essentially is to judge how well the predic- tion matches the true label, resulting in Precision, Recall, and F 1 scores. Besides these, we consider perplexity (PPL) as another evaluation metric to analyze the aspect identification quality. The aver- age test perplexity PPL over words is defined as:  where N is the number of documents, D n repre- sents the word number, and v n stands for the word- count of document n.</p><formula xml:id="formula_12">exp − 1 N N n=1 1 D n log P (v n ) ,<label>(10</label></formula><p>Average perplexity results are reported in Ta- ble 1, while Precision, Recall, and F 1 evaluation results for aspect identification are given in Ta- ble 2. Some LDA-based methods require manual mappings for evaluation, which causes difficulties in obtaining a fair PPL result, so a few methods are only considered in <ref type="table" target="#tab_2">Table 2</ref>.</p><p>To illustrate the differences, in <ref type="table">Table 3</ref>, we list representative words for aspects identified by var- ious models and highlight words without an obvi- ous association or words that are rather unspecific in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Discussion</head><p>Considering the results from <ref type="table">Table 1</ref> and the RBM, RSM, and SERBM-related results from Ta- ble 2, we find that the RSM performs better than the regular RBM model on this aspect identifi- cation task. However, the average test perplex- ity is greatly reduced even further by the SERBM method, resulting in a relative improvement by 45.96% over the RSM model. Thus, despite the elaborate modification, our SERBM inherits RBMs' ability in modeling latent topics, but sig- nificantly outperforms other RBM family models Aspect RSM RBM <ref type="table">Loc-LDA  ME-LDA  SAS  SERBM  great  menu,drink  chicken  chocolate  food,menu  salad,cheese  dessert  food,pizza  menu,salad  dessert  dessert  dessert  beef  chicken  good  cream  drinks  chicken  Food  drink,BBQ  seafood  fish  ice,cake  chicken  sauce  menu  good  drinks  desserts  cheeses  rice,pizza  delicious</ref>   <ref type="table">Table 3</ref>: Aspects and representative words on the aspect identification task.</p><p>In <ref type="table" target="#tab_2">Table 2</ref>, we also observe that SERBM achieves a higher accuracy compared with other state-of-the-art aspect identification meth- ods. More specifically, it is evident that our SERBM model outperforms previous methods' F 1 scores. Compared with MELDA, the F 1 scores for the SERBM lead to relative improvements of 5.31%, 6.58%, and 2.10%, respectively, for the Food, Staff, and Ambience aspects. Compared with SAS, the F 1 scores yield relative improve- ments by 6.73%, 5.10%, and 6.56%, respectively, on those same aspects. As for Precision and Re- call, the SERBM also achieves a competitive per- formance compared with other methods in aspect identification.</p><p>Finally, we conclude from <ref type="table">Table 3</ref> that the SERBM method has the capability of extracting word with obvious aspect-specific features and makes less errors compared with other models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Sentiment Classification</head><p>We additionally conduct two experiments to eval- uate the model's performance on sentiment classi- fication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Comparison with SentiWordNet</head><p>We assign a sentiment score to every document in the restaurant review dataset based on the output of SERBM's sentiment-type hidden units. To ana- lyze SERBM's performance in sentiment classifi- cation, we compare these results with SentiWord- Net 5 , a well-known sentiment lexicon. For this SentiWordNet baseline, we consult the resource to obtain a sentiment label for every word and ag- gregate these to judge the sentiment information of an entire review document in terms of the sum of word-specific scores. <ref type="table">Table 4</ref> provides a com- parison between SERBM and SentiWordNet, with Accuracy as the evaluation metric.</p><p>We observe in <ref type="table">Table 4</ref> that the sentiment Method SentiWordNet SERBM Accuracy 0.703 0.788 <ref type="table">Table 4</ref>: Accuracy for SERBM and SentiWordNet classification accuracy on the restaurant review dataset sees a relative improvement by 12.1% with SERBM over the SentiWordNet baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Comparison with JST</head><p>We additionally utilize the Polarity dataset v2.0 to conduct an additional sentiment classification ex- periment in order to assess SERBM's performance more thoroughly. We compare SERBM with the advanced joint sentiment/topic model (JST) by <ref type="bibr" target="#b10">Lin &amp; He (2009)</ref>. For the JST and the Trying- JST methods only, we use the filtered subjectiv- ity lexicon (subjective MR) as prior information, containing 374 positive and 675 negative entries, which is the same experimental setting as in <ref type="bibr" target="#b10">Lin &amp; He (2009)</ref>. For SERBM, we use the same gen- eral setup as before except for the fact that aspect- specific priors are not used here. <ref type="table" target="#tab_5">Table 5</ref> provides the sentiment classification ac- curacies on both the overall dataset and on the sub- sets for each polarity, where pos. and neg. refer to the positive and negative reviews in the dataset, re- spectively.   <ref type="table" target="#tab_5">Table 5</ref>, we observe that SERBM outper- forms JST both in terms of the overall accu- racy and for the positive/negative-specific subsets. SERBM yields a relative improvement in the over- all accuracy by 5.31% over JST and by 8.66% over Trying-JST.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we have proposed the novel Sentiment-Aspect Extraction RBM (SERBM) model to jointly extract review aspects and sen- timent polarities in an unsupervised setting. Our approach modifies the standard RBM model by introducing a heterogeneous structure into the hid- den layer and incorporating informative priors into the model. Our experimental results show that this model can outperform LDA-based methods.</p><p>Hence, our work opens up the avenue of uti- lizing RBM-based undirected graphical models to solve aspect extraction and sentiment classifica- tion tasks as well as other unsupervised tasks with similar structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>The joint probability distribution is defined as</p><formula xml:id="formula_13">p θ (v, h) = 1 Z θ e E θ (v,h) ,<label>(11)</label></formula><p>where Z θ is the partition function. In conjunction with Eq. 1, we obtain</p><formula xml:id="formula_14">E θ ( v k , h) = −b i v k − F j=1 a j h j − F j=1 h j W k j v k<label>(12)</label></formula><p>Then, we can obtain the derivation in Eq. 6.</p><formula xml:id="formula_15">P (h j = 1 | v k ) =P (h j = 1 | h −j , v k ) = P (h j = 1, h −j , v k ) P (h −j , v k ) = P (h j = 1, h −j , v k ) P (h j = 1, h −j , v k ) + P (h j = 0, h −j , v k ) = 1 Z e −E(h j =1,h −j , v k ) 1 Z e −E(h j =1,h −j , v k ) + 1 Z e −E(h j =0,h −j , v k ) = e −E(h j =1,h −j , v k ) e −E(h j =1,h −j , v k ) + e −E(h j =0,h −j , v k ) = 1 1 + e −E(h j =0,h −j , v k )+E(h j =1,h −j , v k ) =σ(a j + W k j v k )<label>(13)</label></formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: RBM Schema</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Sentiment-Aspect Extraction Model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 3: Prior Feature Extraction</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>)</head><label></label><figDesc></figDesc><table>Aspect Method Precision Recall 
F 1 
RBM 
0.753 
0.680 0.715 
RSM 
0.718 
0.736 0.727 
food 
LocLDA 
0.898 
0.648 0.753 
MELDA 
0.874 
0.787 0.828 
SAS 
0.867 
0.772 0.817 
SERBM 
0.891 
0.854 0.872 
RBM 
0.436 
0.567 0.493 
RSM 
0.430 
0.310 0.360 
staff 
LocLDA 
0.804 
0.585 0.677 
MELDA 
0.779 
0.540 0.638 
SAS 
0.774 
0.556 0.647 
SERBM 
0.819 
0.582 0.680 
RBM 
0.489 
0.439 0.463 
RSM 
0.498 
0.441 0.468 
ambi LocLDA 
0.603 
0.677 0.638 
-ence MELDA 
0.773 
0.588 0.668 
SAS 
0.780 
0.542 0.640 
SERBM 
0.805 
0.592 0.682 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Aspect identification results in terms of 
precision, recall, and F 1 scores on the restaurant 
reviews dataset 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 : Accuracy for SERBM and JST In</head><label>5</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> http://books.google.com/ngrams/datasets</note>

			<note place="foot" n="2"> http://sentiwordnet.isti.cnr.it</note>

			<note place="foot" n="3"> http://nlp.stanford.edu/software/tagger.shtml 4 http://www.cs.cornell.edu/people/pabo/ movie-review-data/</note>

			<note place="foot" n="5"> http://sentiwordnet.isti.cnr.it</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The research at IIIS was supported by China 973 Program Grants 2011CBA00300, 2011CBA00301 and NSFC Grants 61033001, 61361136003, 61450110088. The research at CASIA was sup-ported by the National Basic Research Program of China Grant No. 2012CB316300 and NSFC Grants 61272332 and 61202329.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sentiment analysis: Adjectives and adverbs are better than adjectives alone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farah</forename><surname>Benamara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmine</forename><surname>Cesarano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Picariello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><forename type="middle">Reforgiato</forename><surname>Recupero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatramana</forename><surname>Subrahmanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICWSM</title>
		<meeting>ICWSM</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An unsupervised aspect-sentiment model for online reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Brody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noemie</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT 2010</title>
		<meeting>NAACL-HLT 2010</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="804" to="812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hierarchical sequential learning for extracting opinions and their attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2010</title>
		<meeting>ACL 2010</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="269" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Beyond the stars: Improving rating predictions using review text content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gayatree</forename><surname>Ganu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noemie</forename><surname>Elhadad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amélie</forename><surname>Marian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WebDB</title>
		<meeting>WebDB</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Replicated softmax: an undirected topic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS 2009)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1607" to="1614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD 2004</title>
		<meeting>KDD 2004<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Extracting opinion targets in a single-and cross-domain setting with Conditional Random Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niklas</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2010</title>
		<meeting>EMNLP 2010</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1035" to="1045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A novel lexicalized HMM-based learning framework for Web opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung</forename><forename type="middle">Hay</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srihari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML 2009</title>
		<meeting>ICML 2009</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="465" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Extracting aspect-evaluation and aspect-of relations in opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nozomi</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-CoNLL</title>
		<meeting>EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1065" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Joint sentiment/topic model for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenghua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM Conference on Information and Knowledge Management (CIKM 2009)</title>
		<meeting>the 18th ACM Conference on Information and Knowledge Management (CIKM 2009)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="375" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Integrating classification and association rule mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wynne</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD 1998</title>
		<meeting>KDD 1998</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="80" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Opinion observer: analyzing and comparing opinions on the Web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsheng</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th international conference on World Wide Web</title>
		<meeting>the 14th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="342" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Opinion target extraction using word-based translation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-CoNLL 2012</title>
		<meeting>EMNLP-CoNLL 2012</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1346" to="1356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Opinion target extraction using partially-supervised word alignment model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Joint Conference on Artificial Intelligence (IJCAI 2013)</title>
		<meeting>the 23rd International Joint Conference on Artificial Intelligence (IJCAI 2013)</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2134" to="2140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sentiment analysis and opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Human Language Technologies</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="167" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Topic sentiment mixture: modeling facets and opinions in weblogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Wondra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th international conference on the World Wide Web</title>
		<meeting>the 16th international conference on the World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="171" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Aspect extraction through semi-supervised modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2012</title>
		<meeting>ACL 2012</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="339" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Opinion mining and sentiment analysis. Foundations and trends in information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Thumbs up?: Sentiment classification using machine learning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivakumar</forename><surname>Vaithyanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
	<note>Proceedings of EMNLP 2002</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Extracting product features and opinions from reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana-Maria</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orena</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT/EMNLP 2005</title>
		<meeting>HLT/EMNLP 2005</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recognizing stances in online debates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swapna</forename><surname>Somasundaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-IJCNLP 2009</title>
		<meeting>ACL-IJCNLP 2009</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="226" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Modeling online reviews with multi-grain topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th international conference on the World Wide Web</title>
		<meeting>the 17th international conference on the World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="111" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automatic seed word selection for unsupervised sentiment classification of Chinese text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taras</forename><surname>Zagibalov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2008</title>
		<meeting>COLING 2008</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1073" to="1080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Jointly modeling aspects and opinions with a MaxEnt-LDA hybrid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Wayne Xin Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongfei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2010</title>
		<meeting>EMNLP 2010</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="56" to="65" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Movie review mining and summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM international Conference on Information and Knowledge Management (CIKM 2006)</title>
		<meeting>the 15th ACM international Conference on Information and Knowledge Management (CIKM 2006)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="43" to="50" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
