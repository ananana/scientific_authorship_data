<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:57+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DKPro TC: A Java-based Framework for Supervised Learning Experiments on Textual Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>June 23-24, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Daxenberger</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Ferschke</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Zesch</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Language Technology Lab</orgName>
								<orgName type="institution">University of Duisburg-Essen</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">DIPF</orgName>
								<orgName type="institution" key="instit1">UKP Lab</orgName>
								<orgName type="institution" key="instit2">Technische Universität Darmstadt ‡ Information Center for Education</orgName>
								<address>
									<settlement>Frankfurt</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DKPro TC: A Java-based Framework for Supervised Learning Experiments on Textual Data</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
						<meeting>52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations <address><addrLine>Baltimore, Maryland USA</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="61" to="66"/>
							<date type="published">June 23-24, 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present DKPro TC, a framework for supervised learning experiments on tex-tual data. The main goal of DKPro TC is to enable researchers to focus on the actual research task behind the learning problem and let the framework handle the rest. It enables rapid prototyping of experiments by relying on an easy-to-use workflow engine and standardized document prepro-cessing based on the Apache Unstruc-tured Information Management Architecture (Ferrucci and Lally, 2004). It ships with standard feature extraction modules, while at the same time allowing the user to add customized extractors. The extensive reporting and logging facilities make DKPro TC experiments fully replicable.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Supervised learning on textual data is a ubiquitous challenge in Natural Language Processing (NLP). Applying a machine learning classifier has be- come the standard procedure, as soon as there is annotated data available. Before a classifier can be applied, relevant information (referred to as features) needs to be extracted from the data. A wide range of tasks have been tackled in this way including language identification, part-of-speech (POS) tagging, word sense disambiguation, sen- timent detection, and semantic similarity.</p><p>In order to solve a supervised learning task, each researcher needs to perform the same set of steps in a predefined order: reading input data, preprocessing, feature extraction, machine learn- ing, and evaluation. Standardizing this process is quite challenging, as each of these steps might vary a lot depending on the task at hand. To com- plicate matters further, the experimental process is usually embedded in a series of configuration changes. For example, introducing a new fea- ture often requires additional preprocessing. Re- searchers should not need to think too much about such details, but focus on the actual research task. DKPro TC is our take on the standardization of an inherently complex problem, namely the imple- mentation of supervised learning experiments for new datasets or new learning tasks.</p><p>We will make some simplifying assumptions wherever they do not harm our goal that the frame- work should be applicable to the widest possible range of supervised learning tasks. For example, DKPro TC only supports a limited set of machine learning frameworks, as we argue that differences between frameworks will mainly influence run- time, but will have little influence on the final con- clusions to be drawn from the experiment. The main goal of DKPro TC is to enable the researcher to quickly find an optimal experimental configura- tion. One of the major contributions of DKPro TC is the modular architecture for preprocessing and feature extraction, as we believe that the focus of research should be on a meaningful and expressive feature set. DKPro TC has already been applied to a wide range of different supervised learning tasks, which makes us confident that it will be of use to the research community.</p><p>DKPro TC is mostly written in Java and freely available under an open source license. <ref type="bibr">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Requirements</head><p>In the following, we give a more detailed overview of the requirements and goals we have identified for a general-purpose text classification system. These requirements have guided the development of the DKPro TC system architecture.  Flexibility Users of a system for supervised learning on textual data should be able to choose between different machine learning approaches depending on the task at hand. In supervised ma- chine learning, we have to distinguish between ap- proaches based on classification and approaches based on regression. In classification, given a document d ∈ D and a set of labels C = {c 1 , c 2 , ..., c n }, we want to label each document d with L ⊂ C, where L is the set of relevant or true labels. In single-label classification, each document d is labeled with exactly one label, i.e. |L| = 1, whereas in multi-label classification, a set of labels is assigned, i.e. |L| ≥ 1. Single- label classification can further be divided into bi- nary classification (|C| = 2) and multi-class clas- sification (|C| &gt; 2). In regression, real numbers instead of labels are assigned.</p><p>Feature extraction should follow a modular de- sign in order to facilitate reuse and to allow seam- less integration of new features. However, the way in which features need to be extracted from the in- put documents depends on the the task at hand. We have identified several typical scenarios in su- pervised learning on textual data and propose the following feature modes:</p><p>• In document mode, each input document will be used as its own entity to be classified, e.g. an email classified as wanted or unwanted (spam).</p><p>• In unit/sequence mode, each input document contains several units to be classified. The units in the input document cannot be divided into separate documents, either because the context of each unit needs to be preserved (e.g. to disambiguate named entities) or be- cause they form a sequence which needs to be kept (in sequence tagging).</p><p>• The pair mode is intended for problems which require a pair of texts as input, e.g. a pair of sentences to be classified as para- phrase or non-paraphrase. It represents a special case of multi-instance learning (Sur- deanu et al., 2012), in which a document con- tains exactly two instances.</p><p>Considering the outlined learning approaches and feature modes, we have summarized typical sce- narios in supervised learning on textual data in Ta- ble 1 and added example applications in NLP.</p><p>Replicability and Reusability As it has been recently noted by <ref type="bibr" target="#b3">Fokkens et al. (2013)</ref>, NLP ex- periments are not replicable in most cases. The problem already starts with undocumented pre- processing steps such as tokenization or sentence boundary detection that might have heavy impact on experimental results. In a supervised learning setting, this situation is even worse, as e.g. fea- ture extraction is usually only partially described in the limited space of a research paper. For ex- ample, a paper might state that "n-gram features" were used, which encompasses a very broad range of possible implementations. In order to make NLP experiments replicable, a text classification framework should (i) encourage the user to reuse existing components which they can refer to in research papers rather than writ- ing their own components, (ii) document all per- formed steps, and (iii) make it possible to re-run experiments with minimal effort.</p><p>Apart from helping the replicability of experi- ments, reusing components allows the user to con- centrate on the new functionality that is specific to the planned experiment instead of having to reinvent the wheel. The parts of a text classifi- cation system which can typically be reused are preprocessing components, generic feature extrac- tors, machine learning algorithms, and evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Architecture</head><p>We now give an overview of the DKPro TC archi- tecture that was designed to take into account the requirements outlined above. A core design deci- sion is to model each of the typical steps in text classification (reading input data and preprocess- ing, feature extraction, machine learning and eval- uation) as separate tasks. This modular architec- ture helps the user to focus on the main problem, i.e. developing and selecting good features.</p><p>In the following, we describe each module in more detail, starting with the workflow engine that is used to assemble the tasks into an experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Configuration and Workflow Engine</head><p>We rely on the DKPro Lab (Eckart de Castilho and Gurevych, 2011) workflow engine, which al- lows fine-grained control over the dependencies between single tasks, e.g. the pre-processing of a document obviously needs to happen before the feature extraction. In order to shield the user from the complex "wiring" of tasks, DKPro TC currently provides three pre-defined workflows: Train/Test, Cross-Validation, and Prediction (on unseen data). Each workflow supports the feature modes described above: document, unit/sequence, and pair.</p><p>The user is still able to control the behavior of the workflow by setting parameters, most impor- tantly the sources of input data, the set of feature extractors, and the classifier to be used. Internally, each parameter is treated as a single dimension in the global parameter space. Users may pro- vide more than one value for a certain parame- ter, e.g. specific feature sets or several classifiers. The workflow engine will automatically run all possible parameter value combinations (a process called parameter sweeping).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Reading Input Data</head><p>Input data for supervised learning tasks comes in myriad different formats which implies that read- ing data cannot be standardized, but needs to be handled individually for each data set. However, the internal processing should not be dependent on the input format. We therefore use the Common Analysis Structure (CAS), provided by the Apache Unstructured Information Management Architec- ture (UIMA), to represent input documents and annotations in a standardized way.</p><p>Under the UIMA model, reading input data means to transform arbitrary input data into a CAS representation. DKPro TC already provides a wide range of readers from UIMA component repositories such as DKPro Core. <ref type="bibr">2</ref> The reader also needs to assign to each classification unit an outcome attribute that represents the relevant label (single-label), labels (multi-label), or a real value (regression). In unit/sequence mode, the reader additionally needs to mark the units in the CAS. In pair mode, a pair of texts (instead of a single document) is stored within one CAS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Preprocessing</head><p>In this step, additional information about the docu- ment is added to the CAS, which efficiently stores large numbers of stand-off annotations. In pair mode, the preprocessing is automatically applied to both documents.</p><p>DKPro TC allows the user to run arbitrary UIMA-based preprocessing components as long as they are compatible with the DKPro type sys- tem that is currently used by DKPro Core and EOP. <ref type="bibr">3</ref> Thus, a large set of ready-to-use prepro- cessing components for more than ten languages is available, containing e.g. sentence boundary de- tection, lemmatization, POS-tagging, or parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Feature Extraction</head><p>DKPro TC ships a constantly growing number of feature extractors. Feature extractors have access to the document text as well as all the additional information that has been added in the form of UIMA stand-off annotations during the prepro- cessing step. Users of DKPro TC can add cus- tomized feature extractors for particular use cases on demand.</p><p>Among the ready-to-use feature extractors con- tained in DKPro TC, there are several ones ex- tracting grammatical information, e.g. the plural- singular ratio or the ratio of modal to all verbs. Other features collect information about stylistic cues of a document, e.g. the number of exclama- tions or the type-token-ratio. DKPro TC is able to extract n-grams or skip n-grams of tokens, charac- ters, and POS tags.</p><p>Some feature extractors need access to informa- tion about the entire document collection, e.g. in order to weigh lexical features with tf.idf scores. Such extractors have to declare that they depend on collection level information and DKPro TC will automatically include a special task that is executed before the actual features are extracted. Depending on the feature mode which has been configured, DKPro TC will extract information on document level, unit-and/or sequence-level, or document pair level.</p><p>DKPro TC stores extracted features in its inter- nal feature store. When the extraction process is finished, a configurable data writer converts the content from the feature store into a format which can be handled by the utilized machine learning tool. DKPro TC currently ships data writers for the Weka <ref type="figure">(Hall et al., 2009</ref>), Meka 4 , and Mallet <ref type="bibr" target="#b6">(McCallum, 2002</ref>) frameworks. Users can also add dedicated data writers that output features in the format used by the machine learning frame- work of their choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Supervised Learning</head><p>For the actual machine learning, DKPro TC cur- rently relies on Weka (single-label and regres- sion), Meka (multi-label), and Mallet (sequence labeling). It contains a task which trains a freely configurable classifier on the training data and evaluates the learned model on the test data.</p><p>Before training and evaluation, the user may ap- ply dimensionality reduction to the feature set, i.e. select a limited number of (expectedly meaning- ful) features to be included for training and eval- uating the classifier. DKPro TC uses the feature selection capabilities of Weka (single-label and re- gression) and Mulan (multi-label) ( <ref type="bibr" target="#b11">Tsoumakas et al., 2010)</ref>.</p><p>DKPro TC can also predict labels on unseen (i.e. unlabeled) data, using a trained classifier. In that case, no evaluation will be carried out, but the classifier's prediction for each document will be written to a file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Evaluation and Reporting</head><p>DKPro TC calculates common evaluation scores including accuracy, precision, recall, and F 1 - score. Whenever sensible, scores are reported for each individual label as well as aggregated over all labels. To support users in further analyz- ing the performance of a classification workflow, DKPro TC outputs the confusion matrix, the ac-tual predictions assigned to each document, and a ranking of the most useful features based on the configured feature selection algorithm. Additional task-specific reporting can be added by the user.</p><p>As mentioned before, a major goal of DKPro TC is to increase the replicability of NLP experiments. Thus, for each experiment, all con- figuration parameters are stored and will be re- ported together with the classification results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Tweet Classification: A Use Case</head><p>We now give a brief summary of what a supervised learning task might look like in DKPro TC using a simple Twitter sentiment classification example. Assuming that we want to classify a set of tweets either as "emotional" or "neutral", we can use the setup shown in Listing 1. The example uses the Groovy programming language which yields bet- ter readable code, but pure Java is also supported. Likewise, a DKPro TC experiment can also be set up with the help of a configuration file, e.g. in JSON or via Groovy scripts.</p><p>First, we create a workflow as a BatchTask- CrossValidation which can be used to run a cross-validation experiment on the data (using 10 folds as configured by the corresponding pa- rameter). The workflow uses LabeledTweet- Reader in order to import the experiment data from source text files into the internal document representation (one document per tweet). This reader adds a UIMA annotation that specifies the gold standard classification outcome, i.e. the rel- evant label for the tweet. In this use case, pre- processing consists of a single step: running the ArkTweetTagger ( <ref type="bibr" target="#b4">Gimpel et al., 2011</ref>), a spe- cialized Twitter tokenizer and POS-tagger that is integrated in DKPro Core. The feature mode is set to document (one tweet per CAS), and the learning mode to single-label (each tweet is labeled with exactly one label), cf. <ref type="table">Table 1</ref>.</p><p>Two feature extractors are configured: One for returning the number of hashtags and another one returning the ratio of emoticons to tokens in the tweet. Listing 2 shows the Java code for the sec- ond extractor. Two things are noteworthy: (i) doc- ument text and UIMA annotations are readily available through the JCas object, and (ii) this is really all that the user needs to write in order to add a new feature extractor.</p><p>The next item to be configured is the Weka- DataWriter which converts the internal fea-BatchTaskCrossValidation batchTask = [ experimentName: "Twitter-Sentiment", preprocessingPipeline: createEngineDescription(ArkTweetTagger), // Preprocessing parameterSpace: [ // multi-valued parameters in the parameter space will be swept Dimension.createBundle("reader", [ readerTrain: LabeledTweetReader, readerTrainParams: [LabeledTweetReader.PARAM_CORPUS_PATH, "src/main/resources/tweets.txt"]]), Dimension.create("featureMode", "document"), Dimension.create("learningMode", "singleLabel"), Dimension.create("featureSet", [EmoticonRatioExtractor.name, NumberOfHashTagsExtractor.name]), Dimension.create("dataWriter", WekaDataWriter.name), Dimension.create("classificationArguments", [NaiveBayes.name, RandomForest.name])], reports: <ref type="bibr">[BatchCrossValidationReport]</ref>, // collects results from folds numFolds: 10];</p><p>Listing 1: Groovy code to configure a DKPro TC cross-validation BatchTask on Twitter data. ture representation into the Weka ARFF format. For the classification, two machine learning algo- rithms will be iteratively tested: a Naive Bayes classifier and a Random Forest classifier. Pass- ing a list of parameters into the parameter space will automatically make DKPro TC test all pos- sible parameter combinations. The classification task automatically trains a model on the training data and stores the results of the evaluation on the test data for each fold on the disk. Finally, the evaluation scores for each fold are collected by the BatchCrossValidationReport and written to a single file using a tabulated format.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>This section will give a brief overview about tools with a scope similar to DKPro TC. We only list freely available software, most of which is open- source. Unless otherwise indicated, all of the tools are written in Java.</p><p>ClearTK <ref type="bibr" target="#b7">(Ogren et al., 2008</ref>) is conceptually closest to DKPro TC and shares many of its dis- tinguishing features like the modular feature ex- tractors. It provides interfaces to machine learn- ing libraries such as Mallet or libsvm, offers wrap- pers for basic NLP components, and comes with a feature extraction library that facilitates the de- velopment of custom feature extractors within the UIMA framework. In contrast to DKPro TC, it is rather designed as a programming library than a customizable research environment for quick ex- periments and does not provide predefined text classification setups. Furthermore, it does not sup- port parameter sweeping and has no explicit sup- port for creating experiment reports.</p><p>Argo ( <ref type="bibr" target="#b9">Rak et al., 2013</ref>) is a web-based work- bench with support for manual annotation and au- tomatic analysis of mainly bio-medical data. Like DKPro TC, Argo is based on UIMA, but focuses on sequence tagging, and it lacks DKPro TC's pa- rameter sweeping capabilities.</p><p>NLTK ( <ref type="bibr" target="#b0">Bird et al., 2009</ref>) is a general-purpose NLP toolkit written in Python. It offers com- ponents for a wide range of preprocessing tasks and also supports feature extraction and machine learning for supervised text classification. Like DKPro TC, it can be used to quickly setup baseline experiments. As opposed to DKPro TC, NLTK lacks a modular structure with respect to prepro- cessing and feature extraction and does not sup- port parameter sweeping.</p><p>Weka ( <ref type="bibr" target="#b5">Hall et al., 2009</ref>) is a machine learning framework that covers only the last two steps of DKPro TC's experimental process, i.e. machine learning and evaluation. However, it offers no ded- icated support for preprocessing and feature gener- ation. Weka is one of the machine learning frame- works that can be used within DKPro TC for ac- tual machine learning.</p><p>Mallet <ref type="bibr" target="#b6">(McCallum, 2002</ref>) is another machine learning framework implementing several super- vised and unsupervised learning algorithms. As opposed to Weka, is also supports sequence tag- ging, including Conditional Random Fields, as well as topic modeling. Mallet can be used as ma- chine learning framework within DKPro TC. Scikit-learn <ref type="bibr" target="#b8">(Pedregosa et al., 2011</ref>) is a ma- chine learning framework written in Python. It offers basic functionality for preprocessing, fea- ture selection, and parameter tuning. It provides some methods for preprocessing such as convert- ing documents to tf.idf vectors, but does not offer sophisticated and customizable feature extractors for textual data like DKPro TC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Summary and Future Work</head><p>We have presented DKPro TC, a comprehensive and flexible framework for supervised learning on textual data. DKPro TC makes setting up exper- iments and creating new features fast and simple, and can therefore be applied for rapid prototyp- ing. Its extensive logging capabilities emphasize the replicability of results. In our own research lab, DKPro TC has successfully been applied to a wide range of tasks including author identification, text quality assessment, and sentiment detection.</p><p>There are some limitations to DKPro TC which we plan to address in future work. To reduce the runtime of experiments with very large document collections, we want to add support for parallel processing of documents. While the current main goal of DKPro TC is to bootstrap experiments on new data sets or new applications, we also plan to make DKPro TC workflows available as resources to other applications, so that a model trained with DKPro TC can be used to automatically label tex- tual data in different environments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Single</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>List&lt;Feature&gt; extract(JCas annoDb) throws TextClassificationException { int nrOfEmoticons = JCasUtil.select(annoDb, EMO.class).size(); int nrOfTokens = JCasUtil.select(annoDb, Token.class).size(); double ratio = (double) nrOfEmoticons / nrOfTokens; return new Feature("EmoticonRatio", ratio).asList(); } } Listing 2: A DKPro TC document mode feature extractor measuring the ratio of emoticons to tokens.</figDesc></figure>

			<note place="foot" n="1"> http://dkpro-tc.googlecode.com</note>

			<note place="foot" n="2"> http://dkpro-core-asl.googlecode.com 3 http://hltfbk.github.io/Excitement-Open-Platform/</note>

			<note place="foot" n="4"> http://meka.sourceforge.net</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work has been supported by the Volks-wagen Foundation as part of the Lichtenberg-Professorship Program under grant No. I/82806, and by the Hessian research excellence pro-gram "Landes-Offensive zur Entwicklung Wissenschaftlich-ökonomischer Exzellenz" (LOEWE) as part of the research center "Digital Humanities". The authors would like give special thanks to Richard Eckhart de Castilho, Nicolai Erbs, Lucie Flekova, Emily Jamison, Krish Perumal, and Artem Vovk for their contributions to the DKPro TC framework.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Natural Language Processing with Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Klein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Reilly Media Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Lightweight Framework for Reproducible Parameter Sweeping in Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Eckart De Castilho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Workshop on Data Infrastructures for Supporting Information Retrieval Evaluation</title>
		<meeting>of the Workshop on Data Infrastructures for Supporting Information Retrieval Evaluation</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="7" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">UIMA: An Architectural Approach to Unstructured Information Processing in the Corporate Research Environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ferrucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="327" to="348" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Offspring from Reproduction Problems: What Replication Failure Teaches Us</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fokkens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Erp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Postma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pedersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vossen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Freire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1691" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Part-of-speech tagging for Twitter: annotation, features, and experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heilman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="42" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Reutemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Witten</surname></persName>
		</author>
		<title level="m">The WEKA Data Mining Software: An Update. SIGKDD Explorations</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">MALLET: A Machine Learning for Language Toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ClearTK: A UIMA toolkit for statistical natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ogren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wetzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bethard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Towards Enhanced Interoperability for Large HLT Systems: UIMA for NLP workshop at LREC</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="32" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine Learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Development and Analysis of NLP Pipelines in Argo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ananiadou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="115" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-instance multi-label learning for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP-CoNLL</title>
		<meeting>EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mining Multi-label Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vlahavas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transformation</title>
		<imprint>
			<biblScope unit="volume">135</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
