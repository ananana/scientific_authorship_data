<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Exploration of Embeddings for Generalized Phrases</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 22-27 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
							<email>wenpeng@cis.lmu.de</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing</orgName>
								<orgName type="institution">University of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing</orgName>
								<orgName type="institution">University of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sch¨</forename><surname>Schütze</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing</orgName>
								<orgName type="institution">University of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Exploration of Embeddings for Generalized Phrases</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the ACL 2014 Student Research Workshop</title>
						<meeting>the ACL 2014 Student Research Workshop <address><addrLine>Baltimore, Maryland USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="41" to="47"/>
							<date type="published">June 22-27 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Deep learning embeddings have been successfully used for many natural language processing problems. Embeddings are mostly computed for word forms although lots of recent papers have extended this to other linguistic units like morphemes and word sequences. In this paper, we define the concept of generalized phrase that includes conventional linguistic phrases as well as skip-bigrams. We compute em-beddings for generalized phrases and show in experimental evaluations on corefer-ence resolution and paraphrase identification that such embeddings perform better than word form embeddings.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Motivation</head><p>One advantage of recent work in deep learning on natural language processing (NLP) is that linguis- tic units are represented by rich and informative embeddings. These embeddings support better performance on a variety of NLP tasks <ref type="bibr" target="#b1">(Collobert et al., 2011</ref>) than symbolic linguistic representa- tions that do not directly represent information about similarity and other linguistic properties. Embeddings are mostly derived for word forms al- though a number of recent papers have extended this to other linguistic units like morphemes ( <ref type="bibr" target="#b6">Luong et al., 2013</ref>), phrases and word sequences <ref type="bibr">(Socher et al., 2010;</ref><ref type="bibr" target="#b8">Mikolov et al., 2013)</ref>. <ref type="bibr">1</ref> Thus, an important question is: what are the basic lin- guistic units that should be represented by embed- dings in a deep learning NLP system? Building on the prior work in <ref type="bibr">(Socher et al., 2010;</ref><ref type="bibr" target="#b8">Mikolov et al., 2013)</ref>, we generalize the notion of phrase to include skip-bigrams (SkipBs) and lexicon entries, where lexicon entries can be both "continuous" and "noncontinuous" linguistic phrases. Exam- ples of skip-bigrams at distance 2 in the sentence "this tea helped me to relax" are: "this*helped", "tea*me", "helped*to" . . . Examples of linguistic phrases listed in a typical lexicon are continuous phrases like "cold cuts" and "White House" that only occur without intervening words and discon- tinous phrases like "take over" and "turn off" that can occur with intervening words. We consider it promising to compute embeddings for these phrases because many phrases, including the four examples we just gave, are noncompositional or weakly compositional, i.e., it is difficult to com- pute the meaning of the phrase from the meaning of its parts. We write gaps as "*" for SkipBs and " " for phrases.</p><p>We can approach the question of what basic linguistic units should have representations from a practical as well as from a cognitive point of view. In practical terms, we want representations to be optimized for good generalization. There are many situations where a particular task involv- ing a word cannot be solved based on the word itself, but it can be solved by analyzing the con- text of the word. For example, if a coreference resolution system needs to determine whether the unknown word "Xiulan" (a Chinese first name) in "he helped Xiulan to find a flat" refers to an animate or an inanimate entity, then the SkipB "helped*to" is a good indicator for the animacy of the unknown word -whereas the unknown word itself provides no clue.</p><p>From a cognitive point of view, it can be argued that many basic units that the human cognitive sys- tem uses have multiple words. Particularly con- vincing examples for such units are phrasal verbs in English, which often have a non-compositional meaning. It is implausible to suppose that we retrieve atomic representations for, say, "keep", "up", "on" and "from" and then combine them to form the meanings of the expressions "keep your head up," "keep the pressure on," "keep him from laughing". Rather, it is more plausible that we rec- ognize "keep up", "keep on" and "keep from" as relevant basic linguistic units in these contexts and that the human cognitive systems represents them as units.</p><p>We can view SkipBs and discontinuous phrases as extreme cases of treating two words that do not occur next to each other as a unit. SkipBs are de- fined purely statistically and we will consider any pair of words as a potential SkipB in our exper- iments below. In contrast, discontinuous phrases are well motivated. It is clear that the words "picked" and "up" in the sentences "I picked it up" belong together and form a unit very similar to the word "collected" in "I collected it". The most useful definition of discontinuous units probably lies in between SkipBs and phrases: we definitely want to include all phrases, but also some (but not all) statistical SkipBs. The initial work presented in this paper may help in finding a good "compro- mise" definition.</p><p>This paper contributes to a preliminary inves- tigation of generalized phrase embeddings and shows that they are better suited than word em- bedding for a coreference resolution classification task and for paraphrase identification. Another contribution lies in that the phrase embeddings we release 2 could be a valuable resource for others.</p><p>The remainder of this paper is organized as fol- lows. Section 2 and Section 3 introduce how to learn embeddings for SkipBs and phrases, respec- tively. Experiments are provided in Section 4. Subsequently, we analyze related work in Section 5, and conclude our work in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Embedding learning for SkipBs</head><p>With English Gigaword Corpus ( <ref type="bibr" target="#b11">Parker et al., 2009)</ref>, we use the skip-gram model as imple- mented in word2vec <ref type="bibr">3 (Mikolov et al., 2013)</ref> to in- duce embeddings. Word2vec skip-gram scheme is a neural network language model, using a given word to predict its context words within a window size. To be able to use word2vec directly with- out code changes, we represent the corpus as a sequence of sentences, each consisting of two to- kens: a SkipB and a word that occurs between the two enclosing words of the SkipB. The distance k between the two enclosing words can be var- ied. In our experiments, we use either distance k = 2 or distance 2 ≤ k ≤ 3. For example, for k = 2, the trigram w i−1 w i w i+1 generates the sin- gle sentence "w i−1 *w i+1 w i "; and for 2 ≤ k ≤ 3, the fourgram w i−2 w i−1 w i w i+1 generates the four sentences "w i−2 *w i w i−1 ", "w i−1 *w i+1 w i ", "w i−2 *w i+1 w i−1 " and "w i−2 *w i+1 w i ".</p><p>In this setup, the middle context of SkipBs are kept (i.e., the second token in the new sentences), and the surrounding context of words of original sentences are also kept (i.e., the SkipB in the new sentences). We can run word2vec without any changes on the reformatted corpus to learn embed- dings for SkipBs. As a baseline, we run word2vec on the original corpus to compute embeddings for words. Embedding size is set to 200.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Embedding learning for phrases</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Phrase collection</head><p>Phrases defined by a lexicon have not been deeply investigated before in deep learning. To collect canonical phrase set, we extract two-word phrases defined in Wiktionary 4 , and two-word phrases de- fined in Wordnet ( <ref type="bibr" target="#b9">Miller and Fellbaum, 1998</ref>) to form a collection of size 95218. This collection contains phrases whose parts always occur next to each other (e.g., "cold cuts") and phrases whose parts more often occur separated from each other (e.g., "take (something) apart").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Identification of phrase continuity</head><p>Wiktionary and WordNet do not categorize phrases as continuous or discontinous. So we need a heuristic for determining this automatically.</p><p>For each phrase "A B", we compute [c 1 , c 2 , c 3 , c 4 , c 5 ] where c i , 1 ≤ i ≤ 5, indi- cates there are c i occurrences of A and B in that order with a distance of i. We compute these statistics for a corpus consisting of Gigaword and Wikipedia. We set the maximal distance to 5 because discontinuous phrases are rarely separated by more than 5 tokens.</p><p>If c 1 is 10 times higher than (c 2 +c 3 +c 4 +c 5 )/4, we classify "A B" as continuous, otherwise as dis- continuous. Taking phrase "pick off" as an ex- ample, it gets vector <ref type="bibr">[1121,</ref><ref type="bibr">632,</ref><ref type="bibr">337,</ref><ref type="bibr">348,</ref><ref type="bibr">4052]</ref>, c 1 (1121) is smaller than the average 1342.25, so "pick off" is set as "discontinuous". Further con- sider "Cornell University" which gets <ref type="bibr">[14831,</ref><ref type="bibr">16,</ref><ref type="bibr">177,</ref><ref type="bibr">331,</ref><ref type="bibr">3471]</ref>, satisfying above condition, hence it is treated as a continuous phrase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Sentence reformatting</head><p>Given the continuity information of phrases, sentence "· · · A · · · B · · · " is reformated into "· · · A B · · · A B · · · " if "A B" is a discontinu- ous phrase and is separated by maximal 4 words, and sentence "· · · AB · · · " into "· · · A B · · · " if "A B" is a continuous phrase.</p><p>In the first case, we use phrase "A B" to replace each of its component words for the purpose of making the context of both constituents available to the phrase in learning. For the second situation, it is natural to combine the two words directly to form an independent semantic unit.</p><p>Word2vec is run on the reformatted corpus to learn embeddings for both words and phrases. Embedding size is also set to 200.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Examples of phrase neighbors</head><p>Usually, compositional methods for learning rep- resentations of multi-word text suffer from the dif- ficulty in integrating word form representations, like word embeddings. To our knowledge, there is no released embeddings which can directly facil- itate measuring the semantic affinity between lin- guistic units of arbitrary lengths. <ref type="table">Table 1</ref> attempts to provide some nearest neighbors for given typ- ical phrases to show the promising perspective of our work. Note that discontinuous phrases like "turn off" have plausible single word nearest neighbors like "unplug".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Our motivation for generalized phrases in Sec- tion 1 was that they can be used to infer the at- tributes of the context they enclose and that they can capture non-compositional semantics. Our hy- pothesis was that they are more suitable for this than word embeddings. In this section we carry out two experiments to test this hypothesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Animacy classification for markables</head><p>A markable in coreference resolution is a linguis- tic expression that refers to an entity in the real world or another linguistic expression. Examples of markables include noun phrases ("the man"), named entities ("Peter") and nested nominal ex- pressions ("their"). We address the task of ani- macy classification of markables: classifying them as animate/inanimate. This feature is useful for coreference resolution systems because only ani- mate markables can be referred to using masculine and feminine pronouns in English like "him" and "she". Thus, this is an important clue for automat- ically clustering the markables of a document into correct coreference chains.</p><p>To create training and test sets, we extract all 39,689 coreference chains from the CoNLL2012 OntoNotes corpus. <ref type="bibr">5</ref> We label chains that con- tain an animate pronoun markable ("she", "her", "he", "him" or "his") and no inanimate pronoun markable ("it" or "its") as animate; and chains that contain an inanimate pronoun markable and no animate pronoun markable as inanimate. Other chains are discarded.</p><p>We extract 39,942 markables and their contexts from the 10,361 animate and inanimate chains. The context of a markable is represented as a SkipB: it is simply the pair of the two words occur- ring to the left and right of the markable. The gold label of a markable and its SkipB is the animacy status of its chain: either animate or inanimate. We divide all SkipBs having received an embedding in the embedding learning phase into a training set of 11,301 (8097 animate, 3204 inanimate) and a bal- anced test set of 4036.</p><p>We use LIBLINEAR <ref type="bibr" target="#b4">(Fan et al., 2008</ref>) for clas- sification, with penalty factors 3 and 1 for inan- imate and animate classes, respectively, because the training data are unbalanced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Experimental results</head><p>We compare the following representations for an- imacy classification of markables. (i) Phrase em- bedding: Skip-bigram embeddings with skip dis- tance k = 2 and 2 ≤ k ≤ 3; (ii) Word em- bedding: concatenation of the embeddings of the two enclosing words where the embeddings are either standard word2vec embeddings (see Sec- tion 2) or the embeddings published by (Collobert et al., 2011); 6 (iii) the one-hot vector representa- tion of a SkipB: the concatentation of two one-hot vectors of dimensionality V where V is the size of the vocabulary. The first (resp. second) vector turn off caught up take over macular degeneration telephone interview   <ref type="table">Table 2</ref>: Classification accuracy. Mark "*" means significantly lower than "phrase embedding", k = 2; " †" means significantly lower than "phrase em- bedding", 2 ≤ k ≤ 3. As significance test, we use the test of equal proportion, p &lt; .05, throughout.</p><p>The results show that phrase embeddings have an obvious advantage in this classification task, both for k = 2 and 2 ≤ k ≤ 3. This validates our hypothesis that learning embeddings for dis- continuous linguistic units is promising.</p><p>In our error analysis, we found two types of frequent errors. (i) Unspecific SkipBs. Many SkipBs are equally appropriate for animate and inanimate markables. Examples of such SkipBs include "take*in" and "then*goes". (ii) Untypical use of specific SkipBs. Even SkipBs that are spe- cific with respect to what type of markable they enclose sometimes occur with the "wrong" type of markable. For example, most markables oc- curring in the SkipB "of*whose" are animate be- cause "whose" usually refers to an animate mark- able. However, in the context ". . . the southeast- ern area of Fujian whose economy is the most ac- tive" the enclosed markable is Fujian, a province of China. This example shows that "whose" occa- sionally refers to an inanimate entity even though these cases are infrequent. <ref type="table" target="#tab_2">Table 3</ref> shows some SkipBs and their nearest neighbors in descending order, where similarity is computed with cosine measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Nearest neighbors of SkipBs</head><p>A general phenomenon is that phrase embed- dings capture high degree of consistency in infer- ring the attributes of enclosed words. Considering the neighbor list in the first column, we can esti- mate that a verb probably appears as the middle token. Furthermore, noun, pronoun, adjective and adverb can roughly be inferred for the remaining columns, respectively. <ref type="bibr">7</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Paraphrase identification task</head><p>Paraphrase identification depends on semantic analysis. Standard approaches are unlikely to as- sign a high similarity score to the two sentences "he started the machine" and "he turned the ma- chine on". In our approach, embedding of the phrase "turned on" can greatly help us to infer cor- rectly that the sentences are paraphrases. Hence, phrase embeddings and in particular embeddings of discontinuous phrases seem promising in para- phrase detection task.</p><p>We  We tackle the paraphrase identification task via supervised binary classification. Sentence repre- sentation equals to the addition over all the to- ken embeddings (words as well as phrases). A slight difference is that when dealing with a sen- tence like "· · · A B · · · A B · · · " we only consider "A B" embedding once. The system "word em- bedding" is based on the embeddings of single words only. Subsequently, pair representation is derived by concatenating the two sentence vectors. This concatentation is then classified by LIBLIN- EAR as "paraphrase" or "no paraphrase". <ref type="table" target="#tab_4">Table 4</ref> shows the performance of two methods. Phrase embeddings are apparently better. Most work on paraphrase detection has devised intri- cate features and achieves performance numbers higher than what we report here <ref type="bibr" target="#b5">(Ji and Eisenstein, 2013;</ref><ref type="bibr" target="#b7">Madnani et al., 2012;</ref><ref type="bibr" target="#b0">Blacoe and Lapata, 2012)</ref>. Our objective is only to demonstrate the superiority of considering phrase embedding over merely word embedding in this standard task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Experimental results and analysis</head><p>We are interested in how phrase embeddings make an impact on this task. To that end, we per- form an analysis on test examples where word em- beddings are better than phrase embeddings and vice versa. <ref type="table">Table 5</ref> shows four pairs, of which "phrase em- bedding" outperforms "word embedding" in the  first two examples, "word embedding" defeats "phrase embedding" in the last two examples. In the first pair, successful phrase detection enables to split sentences into better units, thus the gener- ated representation can convey the sentence mean- ing more exactly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Accuracy</head><p>The meaning difference in the second pair orig- inates from the synonym substitution between "take over as chief financial officer" and "fill the position". The embedding of the phrase "take over" matches the embedding of the single word "fill" in this context. "Phrase embedding" in the third pair suffers from wrong phrase detection. Actually, "in" and "on" can not be treated as a sound phrase in that situation even though "in on" is defined by Wik- tionary. Indeed, this failure, to some extent, re- sults from the shortcomings of our method in dis- covering true phrases. Furthermore, figuring out whether two words are a phrase might need to analyse syntactic structure in depth. This work is directly based on naive intuitive knowledge, acting as an initial exploration. Profound investigation is left as future work.</p><p>Our implementation discovers the contained phrases in the fourth pair perfectly. Yet, "word em- bedding" defeats "phrase embedding" still. The pair is not a paraphrase partly because the numbers are different; e.g., there is a big difference between "5.8 basis points" and "50 basis points". Only a method that can correctly treat numerical informa- tion can succeed here. However, the appearance of phrases "central bank", "interest rates" and "ba- sis points" makes the non-numerical parts more expressive and informative, leading to less dom- inant for digital quantifications. On the contrary, though "word embedding" fails to split the sen-G W P sentence 1 sentence 2 1 0 1 Common side effects include nasal congestion, runny nose, sore throat and cough, the FDA said .</p><p>The most common side effects after get- ting the nasal spray were nasal congestion, runny nose, sore throat and cough . 1 0 1 Douglas Robinson, a senior vice president of finance, will take over as chief financial officer on an interim basis .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Douglas</head><p>Robinson, CA senior vice president, finance, will fill the position in the interim . 1 1 0 They were being held Sunday in the Camden County Jail on $ 100,000 bail each . The Jacksons remained in on Camden County jail $ 100,000 bail . 0 0 1 The interest rate sensitive two year Schatz yield was down 5.8 basis points at 1.99 per- cent .</p><p>The Swedish central bank cut inter- est rates by 50 basis points to 3.0 percent . <ref type="table">Table 5</ref>: Four typical sentence pairs in which the predictions of word embedding system and phrase embedding system differ. G = gold annotation, W = prediction of word embedding system, P = prediction of phrase embedding system. The formatting used by the system is shown. The original word order of sentence 2 of the third pair is "· · · in Camden County jail on $ 100,000 bail".</p><p>tences into better units, it weakens unexpectedly the expressiveness of subordinate context. This example demonstrates the difficulty of paraphrase identification. Differing from simple similarity tasks, two sentences are often not paraphrases even though they may contain very similar words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related work</head><p>To date, approaches to extend embedding (or more generally "representation") beyond individ- ual words are either compositional or holistic (Turney, 2012). The best known work along the first line is by <ref type="bibr">(Socher et al., 2010;</ref><ref type="bibr">Socher et al., 2011;</ref><ref type="bibr">Socher et al., 2012;</ref><ref type="bibr" target="#b0">Blacoe and Lapata, 2012)</ref>, in which distributed representations of phrases or even sen- tences are calculated from the distributed repre- sentations of their parts. This approach is only plausible for units that are compositional, i.e., whose properties are systematically predictable from their parts. As well, how to develop a ro- bust composition function still faces big hurdles; cf. <ref type="table">Table 5</ref>.1 in ( <ref type="bibr" target="#b10">Mitchell and Lapata, 2010)</ref>. Our approach (as well as similar work on continuous phrases) makes more sense for noncompositional units.</p><p>Phrase representations can also be derived by methods other than deep learning of embed- dings, e.g., as vector space representations <ref type="bibr">(Turney, 2012;</ref><ref type="bibr">Turney, 2013;</ref><ref type="bibr" target="#b2">Dinu et al., 2013</ref>). The main point of this paper -generalizing phrases to discontinuous phrases and computing representa- tions for them -is orthogonal to this issue. It would be interesting to evaluate other types of rep- resentations for generalized phrases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>We have argued that generalized phrases are part of the inventory of linguistic units that we should compute embeddings for and we have shown that such embeddings are superior to word form em- beddings in a coreference resolution task and stan- dard paraphrase identification task.</p><p>In this paper we have presented initial work on several problems that we plan to continue in the future: (i) How should the inventory of continu- ous and discontinous phrases be determined? We used a purely statistical definition on the one hand and dictionaries on the other. A combination of the two methods would be desirable. (ii) How can we distinguish between phrases that only occur in continuous form and phrases that must or can oc- cur discontinuously? (iii) Given a sentence that contains the parts of a discontinuous phrase in cor- rect order, how do we determine that the cooccur- rence of the two parts constitutes an instance of the discontinuous phrase? (iv) Which tasks benefit most significantly from the introduction of gener- alized phrases?</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>use the Microsoft Paraphrase Corpus (Dolan et al., 2004) for evaluation. It consists of a training set with 2753 true paraphrase pairs and 1323 false paraphrase pairs, along with a test set with 1147 true and 578 false pairs. After discarding pairs in which neither sentence contains phrases, 3027 training pairs (2123 true vs. 904 false) and 1273 test pairs (871 true vs. 402 false) remain.</figDesc><table>who*afghanistan, 

some*told 
women*have with*responsibility he*worried 

had*afghanistan 
other*told 
men*have 
of*responsibility 
she*worried 

he*afghanistan 
two*told 
children*have and*responsibility 
was*worried 

who*iraq 
-*told 
girls*have 
"*responsibility 
is*worried 

have*afghanistan 
but*told 
parents*have 
that*responsibility said*worried 

fighters*afghanistan 
one*told 
students*have 
's*responsibility 
that*worried 

who*kosovo 
because*told young*have 
the* responsibility they*worried 

was*afghanistan 
and*told 
people*have 
for*responsibility 
's*worried 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 : SkipBs and their nearest neighbors</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 : Paraphrase task results.</head><label>4</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> Socher et al. use the term &quot;word sequence&quot;. Mikolov et al. use the term &quot;phrase&quot; for word sequences that are mostly frequent continuous collocations.</note>

			<note place="foot" n="2"> http://www.cis.lmu.de/pub/ phraseEmbedding.txt.bz2 3 https://code.google.com/p/word2vec/</note>

			<note place="foot" n="4"> http://en.wiktionary.org/wiki/ Wiktionary:Main_Page</note>

			<note place="foot" n="5"> http://conll.cemantix.org/2012/data. html 6 http://metaoptimize.com/projects/ wordreprs/</note>

			<note place="foot" n="7"> A reviewer points out that this is only a suggestive analysis and that corpus statistics about these contexts would be required to establish that phrase embeddings can predict partof-speech with high accuracy.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was funded by DFG (grant SCHU 2246/4). We thank Google for a travel grant to support the presentation of this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A comparison of vector-based representations for semantic composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Blacoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="546" to="556" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">General estimation and evaluation of compositional distributional semantic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nghia The</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting>the Workshop on Continuous Vector Space Models and their Compositionality</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="50" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on Computational Linguistics, page 350. Association for Computational Linguistics</title>
		<meeting>the 20th international conference on Computational Linguistics, page 350. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Liblinear: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Rong-En Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Discriminative improvements to distributional sentence similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="891" to="896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Better word representations with recursive neural networks for morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computational Natural Language Learning</title>
		<meeting>the Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="104" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Re-examining machine translation metrics for paraphrase identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Madnani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="182" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.4546</idno>
		<title level="m">Distributed representations of words and phrases and their compositionality</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Wordnet: An electronic lexical database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Composition in distributional models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1388" to="1429" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">English gigaword fourth edition. Linguistic Data Consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguistic</forename><surname>Data Consortium</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
