<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Compositional Learning of Embeddings for Relation Paths in Knowledge Bases and Text</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Xi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">Microsoft Research Redmond</orgName>
								<address>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Microsoft Research Redmond</orgName>
								<address>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Redmond</orgName>
								<address>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Research Redmond</orgName>
								<address>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Compositional Learning of Embeddings for Relation Paths in Knowledge Bases and Text</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1434" to="1444"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Modeling relation paths has offered significant gains in embedding models for knowledge base (KB) completion. However , enumerating paths between two entities is very expensive, and existing approaches typically resort to approximation with a sampled subset. This problem is particularly acute when text is jointly modeled with KB relations and used to provide direct evidence for facts mentioned in it. In this paper, we propose the first exact dynamic programming algorithm which enables efficient incorporation of all relation paths of bounded length, while modeling both relation types and intermediate nodes in the composi-tional path representations. We conduct a theoretical analysis of the efficiency gain from the approach. Experiments on two datasets show that it addresses representa-tional limitations in prior approaches and improves accuracy in KB completion.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Intelligent applications benefit from structured knowledge about the entities and relations in their domains. For example, large-scale knowl- edge bases (KB), such as Freebase ( <ref type="bibr" target="#b2">Bollacker et al., 2008</ref>) or DBPedia ( <ref type="bibr" target="#b0">Auer et al., 2007)</ref>, have proven to be important resources for supporting open-domain question answering <ref type="bibr" target="#b1">(Berant et al., 2013;</ref>. In biomedicine, KBs such as the Pathway Interaction Database (NCI-PID) ( <ref type="bibr" target="#b19">Schaefer et al., 2009</ref>) are crucial for understanding complex diseases such as cancer and for advancing precision medicine.</p><p>While these knowledge bases are often carefully curated, they are far from complete. In non-static domains, new facts become true or are discovered at a fast pace, making the manual expansion of knowledge bases impractical. Extracting relations from a text corpus ( <ref type="bibr" target="#b12">Mintz et al., 2009;</ref><ref type="bibr" target="#b22">Surdeanu et al., 2012;</ref> or inferring facts from the relationships among known entities <ref type="bibr" target="#b9">(Lao and Cohen, 2010)</ref> are thus important approaches for populating existing knowledge bases.</p><p>Originally proposed as an alternative statis- tical relational learning method, the knowledge base embedding approach has gained a signifi- cant amount of attention, due to its simple predic- tion time computation and strong empirical perfor- mance ( <ref type="bibr" target="#b14">Nickel et al., 2011;</ref><ref type="bibr" target="#b4">Chang et al., 2014</ref>). In this framework, entities and relations in a knowl- edge base are represented in a continuous space, such as vectors and matrices. Whether two enti- ties have a previously unknown relationship can be predicted by simple functions of their corre- sponding vectors or matrices. Early work in this direction focuses on exploring various kinds of learning objectives and frameworks, but the model is learned solely from known direct relationships between two entities (e.g., father(barack, sasha)) <ref type="bibr" target="#b14">(Nickel et al., 2011;</ref><ref type="bibr" target="#b20">Socher et al., 2013;</ref><ref type="bibr" target="#b3">Bordes et al., 2013;</ref><ref type="bibr" target="#b4">Chang et al., 2014;</ref><ref type="bibr" target="#b26">Yang et al., 2015)</ref>. In contrast, using multi-step relation paths (e.g., husband(barack, michelle) ∧ mother(michelle, sasha) to train KB em- beddings has been proposed very recently ( <ref type="bibr" target="#b8">Guu et al., 2015;</ref><ref type="bibr" target="#b5">Garcia-Duran et al., 2015;</ref><ref type="bibr" target="#b11">Lin et al., 2015;</ref><ref type="bibr" target="#b13">Neelakantan et al., 2015)</ref>.</p><p>While using relation paths improves model per- formance, it also poses a critical technical chal- lenge. As the number of possible relation paths between pairs of entities grows exponentially with path length, the training complexity increases sharply. Consequently, existing methods need to make approximations by sampling or pruning. The problem is worsened when the input is aug- mented with unlabeled text, which has been shown to improve performance ( <ref type="bibr" target="#b10">Lao et al., 2012;</ref><ref type="bibr" target="#b6">Gardner et al., 2013;</ref><ref type="bibr" target="#b17">Riedel et al., 2013;</ref><ref type="bibr" target="#b7">Gardner et al., 2014;</ref>. More- over, none of the prior methods distinguish rela- tion paths that differ in the intermediate nodes they pass through (e.g., michelle in our example); all represent paths as a sequence of relation types.</p><p>In this work, we aim to develop a KB comple- tion model that can incorporate relation paths ef- ficiently. We start from analyzing the procedures in existing approaches, focusing on their time and space complexity. Based on the observation that compositional representations of relation paths are in fact decomposable, we propose a novel dynamic programming method that enables efficient model- ing of all possible relation paths, while also repre- senting both relation types and nodes on the paths.</p><p>We evaluated our approach on two datasets. The first is from the domain of gene regulatory networks. Apart from its obvious significance in biomedicine, it offers an excellent testbed for learning joint embedding of KBs and text, as it features existing knowledge bases such as NCI- PID and an even larger body of text that grows rapidly (over one million new articles per year). By modeling intermediate nodes on relation paths, we improve the model by 3 points in mean aver- age precision compared to previous work, while also providing a more efficient algorithm. The sec- ond dataset is based on a network derived from WordNet and previously used in work on knowl- edge base completion. On that dataset we demon- strate the ability of the model to effectively handle longer relation paths composed of a larger set of knowledge base relation types, with smaller posi- tive impact of modeling intermediate nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>In this section, we first give a brief overview of the knowledge base and text representation used in this work. We then describe the task of knowledge base completion more formally and introduce our basic model setting and training objective.</p><p>Knowledge Base A knowledge base (KB) is represented as a collection of subject-predicate- object triples (s, r, t), where s and t are the sub- ject and object entities from a set E, and r is the predicate from a set R that denotes a relation- ship between s and t. For example, in a KB of movie facts, we may find a triple (Han Solo, character, Star Wars), indicating that "Han Solo is a character in the Star Wars movie."</p><p>Let (s, π, t) = (s, r 1 , e 1 , r 2 , e 2 . . . , e n−1 , r n , t) be a path in G with s/t as the start/end entities, r 1 , . . . , r n as the relation edges and e 1 , . . . , e n−1 as the intermediate entities.</p><p>In the domain of gene regulations, entities are genes and directed edges represent regulations. At the abstract level, there are two key relations, pos- itive reg and negative reg, which signify that the subject gene increases or decreases the activity of the object gene, respectively. <ref type="figure" target="#fig_0">Figure 1</ref> shows a snapshot of a gene regulatory network. Genes such as GRB2 and MAPK3 are denoted by the light grey nodes. Regulations such as positive reg are denoted by long edges pointing from subject to ob- ject. In addition, some genes stem from a com- mon evolutionary origin and share similar func- tions. They form a "gene family", such as the MAPK family that includes MAPK1 and MAPK3.</p><p>To jointly embed text, we use sentences contain- ing co-occurring gene pairs, such as "GRB2 was involved in the activation of gene MAPK3...", and augment the above knowledge graph with depen- dency paths between the gene mentions, following the general approach of <ref type="bibr" target="#b17">Riedel et al. (2013)</ref>.</p><p>Task and Scoring Models The KB completion task is to predict the existence of links (s, r, t) that are not seen in the training knowledge base. More specifically, we focus on ranking object and sub- ject entities for given queries (s, r, * ) and ( * , r, t).</p><p>The basic KB embedding models learn latent vector/matrix representations for entities and rela- tions, and score each possible triple using learned parameters θ and a scoring function f (s, r, t|θ).</p><p>Below, we describe two basic model variations which we build on in the rest of the paper. BILINEAR The BILINEAR model learns a square matrix W r ∈ R d×d for each relation r ∈ R and a vector x e ∈ R d for each entity e ∈ E. The scoring function of a relation triple (s, r, t) is de- fined as:</p><formula xml:id="formula_0">f (s, r, t|θ) = x s W r x t .<label>(1)</label></formula><p>For a knowledge graph G with |E| = N e and |R| = N r , the parameter size of the BILINEAR model is O d 2 . The large number of parameters make it prone to overfitting, which motivates its diagonal approximation, BILINEAR-DIAG.</p><p>BILINEAR-DIAG The BILINEAR-DIAG model restricts the relation representations to the class of diagonal matrices. 1 In this case, the parame- ter size is reduced to O d</p><p>. Although we present model variants in terms of the BILINEAR repre- sentation, all experiments in this work are based on the BILINEAR-DIAG special case. All models are trained using the same loss function, which maximizes the probability of cor- rect subject/object fillers of a given set of triples with relations or relation paths. The probabil- ity is defined by a log-linear model normalized over a set of negative samples: P (t|s, π; θ) = e f (s,π,t|θ) t ∈N eg(s,π, * )∪{t} e f (s,π,t |θ) . The probability of subject entities is defined analogously.</p><p>Define the loss for a given training triple L(s, π, t|θ) = − log P (t|s, π; θ) − log P (s|t, π; θ). The overall loss function is the sum of losses over all triples, with an L 2 regularization term.</p><formula xml:id="formula_1">L(θ) = i L(s i , π i , t i ; θ) + λθ 2<label>(2)</label></formula><p>3 Relation-path-aware Models</p><p>We first review two existing methods using vec- tor space relation paths modeling for KB comple- tion in §3.1. We then introduce our new algorithm that can efficiently take into account all relation paths between two nodes as features and simul- taneously model intermediate nodes on relation paths in §3.2. We present a detailed theoretical comparison of the efficiency of these three types of methods in §3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Prior Approaches</head><p>The two approaches we consider here are: using relation paths to generate new auxiliary triples for training ( <ref type="bibr" target="#b8">Guu et al., 2015</ref>) and using relation paths as features for scoring ( <ref type="bibr" target="#b11">Lin et al., 2015)</ref>. Both approaches take into account embeddings of relation paths between entities, and both of them used vector space compositions to combine the embeddings of individual relation links r i into an embedding of the path π. The intermediate nodes e i are neglected. The natural composition function of a BILINEAR model is matrix multipli- cation ( <ref type="bibr" target="#b8">Guu et al., 2015</ref>). For this model, the em- bedding of a length-n path Φ π ∈ R d×d is defined as the matrix product of the sequence of relation matrices for the relations in π.</p><formula xml:id="formula_2">Φ π = W r 1 . . . W rn .<label>(3)</label></formula><p>For the BILINEAR-DIAG model, all the matri- ces are diagonal and the computation reduces to coordinate-wise product of vectors in R d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Relation Paths as a Compositional</head><p>Regularizer In <ref type="bibr" target="#b8">Guu et al. (2015)</ref>, information from relation paths was used to generate additional auxiliary terms in training, which serve to provide a com- positional regularizer for the learned node and re- lation embeddings. A more limited version of the same method was simultaneously proposed in <ref type="bibr" target="#b5">Garcia-Duran et al. (2015)</ref>.</p><p>The method works as follows: starting from each node in the knowledge base, it samples m random walks of length 2 to a maximum length L, resulting in a list of samples {[s i , π i , t i ]}. s i and t i are the start and end nodes of the random walk, respectively, and π i consists of a sequence of inter- mediate edges and nodes. Each of these samples is used to define a new triple used in the training loss function (eq. 2).</p><p>The score of each triple under a BILINEAR composition model is defined as f (s i , π i , t i |θ) = x t s i Φ π i x t i , where Φ π i is the product of matrices for relation link types in the path (eq. 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">PRUNED-PATHS: Relation Paths as</head><p>Compositional Features Instead of using relation paths to augment the set of training triples, <ref type="bibr" target="#b11">Lin et al. (2015)</ref> proposed to use paths (s, π, t) to define the scoring function f (s, r, t|θ, Π s,t ). Here Π s,t denotes the sum of the embeddings of a set of paths π between the two nodes in the graph, weighted by path-constrained random walk probabilities. Their implementation built on the TransE ( <ref type="bibr" target="#b3">Bordes et al., 2013</ref>) embed- ding model; in comparison, we formulate a similar model using the BILINEAR model.</p><p>We refer to such an approach as the PRUNED- PATHS model, for it discards paths with weights below a certain threshold. We define the model under a BILINEAR composition as follows: Let {π 1 , π 2 , . . . , π K } denote a fixed set of path types that can be used as a source of features for the model. We abuse notation slightly to refer to a sequence of types of relation links r 1 , r 2 , . . . , r n in a path in G as π. We denote by P (t|s, π) the path-constrained random walk probability of reaching node t starting from node s and follow- ing sequences of relation types as specified in π. We define the weighted path representation F (s, t) for a node pair as the weighted sum of the repre- sentations of paths π in the set {π 1 , π 2 , . . . , π K } that have non-zero path-constrained random walk probabilities. The representation is defined as:</p><formula xml:id="formula_3">F (s, t) = π w |π| P (t|s, π)Φ(π)</formula><p>, where Φ(π) is the path relation type representation from (eq. 3). The weights w |π| provide a shared parameter for paths of each length, so that the model may learn to trust the contribution of paths of differ- ent lengths differentially. The dependence on θ and the set of paths Π s,t between the two nodes in the graph was dropped for simplicity. <ref type="figure" target="#fig_1">Figure 2</ref> il- lustrates the weighted path representation between nodes GRB2 and MAPK3 from <ref type="figure" target="#fig_0">Figure 1</ref>. <ref type="bibr">2</ref> Using the above definition, the score of a candi- date triple f (s, r, t|θ, Π s,t ) is defined as:</p><formula xml:id="formula_4">f (s, r, t) = x s W r x t + vec(F (s, t)) vec(W r ) (4)</formula><p>The first term of the scoring function is the same as that of the BILINEAR model, and the second term takes into account the similarity of the weighted path representations for (s, t) and the predicted relation r. Here we use element-wise product of the two matrices as the similarity metric. Training and scoring under this model requires explicit con- structions of paths, and computing and storing the random walk probabilities P (t|s, π), which makes it expensive to scale. In the next section, we in- troduce an algorithm that can efficiently take into account all paths connecting two nodes, and natu- rally extend it to model the impact of intermediate nodes on the informativeness of the paths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">ALL-PATHS: A Novel Representation and Algorithm</head><p>We now introduce a novel algorithm for efficiently computing and learning the scoring function from (eq. 4), while summing over the set of all paths π up to a certain length L, and additionally modeling the impact of intermediate nodes on the represen- tations Φ(π) of paths. Depending on the character- istics of the knowledge graph and the dimensional- ity of the learned embedding representations, this method in addition to being more exact, can be faster and take less memory than a method that explicitly generates and prunes full relation paths. We first define a new representation function for paths of the form (s, π, t) = (s, r 1 , e 1 , r 2 , e 2 . . . , e n−1 , r n , t), which has as a special case the relation-type based function used in prior work, but can additionally model the impact of interme- diate nodes e j on the path representation.</p><p>We introduce new parameters w e i which can impact the representations of paths passing through nodes e i . w e i is a scalar weight in our implementation, but vectors of dimensionality d could also be implemented using the same general approach. The embedding of the sample path π under a BILINEAR model is defined as:</p><formula xml:id="formula_5">Φ π = W r 1 tanh(w e 1 ) · · · W rn tanh(w en ).</formula><p>Here the weight of each node is first trans- formed into the range [−1, 1] using a non-linear tanh function. To derive our exact algorithm, we use quantities F l (s, t) denoting the weighted sum of path representations for all paths of length l be- tween nodes s and t. The weighted sum of path representations F (s, t) can be written as:</p><formula xml:id="formula_6">l=1...L w l F l (s, t),<label>(5)</label></formula><p>where F l (s, t) = π∈P l (s,t) p(s|t, π)Φ π and P l (s, t) denotes all paths of length l between s, t.</p><p>Computing F (s, t) in the naive way by enumer- ating all possible (s, t) paths is impractical since the number of possible paths can be very large, es- pecially in graphs containing text. Therefore prior work selected a subset of paths through sampling and pruning <ref type="bibr" target="#b13">(Neelakantan et al., 2015;</ref><ref type="bibr" target="#b11">Lin et al., 2015)</ref>. However, the properties of the BILINEAR composition function for path representation en- able us to incrementally build the sums of all path representations exactly, using dynamic program- ming. Algorithm 1 shows how to compute the nec- essary quantities F l (s, t) for all entity pairs in G. 3</p><p>Algorithm 1 Compute the sum of path represen- tations (up to length L) for every entity pair (s, t).</p><formula xml:id="formula_7">Input : G = (E, R), {Wr|r ∈ R}, list of all entity pairs EP = {(s, t)}. Output : FL(s, t) for every (s, t) ∈ EP . Initialize: for (s, t) ∈ EP do if R(s, t) = ∅ then F1(s, t) ← r∈R(s,t) tanh(wt)p(t|s, r)W r(s,t) else F1(s, t) ← 0 end end for l = 2, . . . , L do for (s, t) ∈ EP do F l (s, t) ← 0 for e ∈ ν(s) do F l (s, t) ← F l (s, t) + F1(s, e)F l−1 (e, t) end end end</formula><p>The key to this solution is that the representa- tions of the longer paths are composed of those of the shorter paths and that the random walk proba- bilities of relation paths are products of transition probabilities over individual relation edges. The sub-components of paths frequently overlap. For example, all paths π depicted in <ref type="figure" target="#fig_1">Figure 2</ref> share the same tail edge family. The algorithms from prior work perform a sum over product representations of paths, but it is more efficient to regroup these into a product of sums. This regrouping allows taking into account exponentially many possible paths without explicitly enumerating them and in- dividually computing their path-constrained ran- dom walk probabilities.</p><p>After all quantities F l (s, t) are computed, their weighted sum F (s, t) can be computed using O dL operations for each entity pair. These can directly be used to compute the scores of all posi- tive and negative triples for training, using (eq. 4).</p><p>As we can see, no explicit enumeration or storage of multi-step relation paths between pairs of nodes is needed. To compute gradients of the loss func- tion with respect to individual model parameters, we use a similar algorithm to compute the error for each intermediate edge (e i , r , e j ) for each group of length l paths between s and t. The algorithm is a variant of forward-backward, corresponding to the forward Algorithm 1. Notice that directly adding node representations for the paths in the PRUNED-PATHS approach would be infeasible since it would lead to an ex- position in the possible path types and therefore the memory and running time requirements of the model. Thus the use of an adaptation of our dy- namic programming algorithm to the task of sum- ming over node sequences for a given path type would become necessary to augment the paramet- ric family of the PRUNED-PATHS approach with node representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Efficiency Analysis</head><p>We perform a worst-case analysis of the time and memory required for training and evaluation for each of the introduced methods. For the models taking into account relation paths, we assume that all paths up to a certain length L will be modeled <ref type="bibr">4</ref> . For the basic model text is not taken into account in training, since modeling text and KB relations uniformly in this model did not help performance as seen in the Experiments section. We only take text into account as a source of relation path fea- tures or auxiliary path facts.</p><p>Notation Let N e denote the number of nodes in the knowledge graph, E kb the number of knowl-edge graph links/triples, E txt the number of tex- tual links/triples, a the average number of outgo- ing links for a node in the graph given the outgo- ing relation type, N r the number of distinct rela- tions in the graph, η the number of negative triples for each training triple, and d the dimensionality of entity embeddings and (diagional) matrix rep- resentations of relations.  The memory requirements of this method are the same as these of ( <ref type="bibr" target="#b8">Guu et al., 2015</ref>), up to a con- stant to store random-walk probabilities for paths. The time requirements are different, however. At training time, we compute scores and up- date gradients for triples corresponding to direct <ref type="bibr">5</ref> The computation uses the fact that the number of path type sequences of length l is N l r . We use a, the average branching factor of nodes given relation types, to derive the estimated number of triples of a given relation type for a path of length l. knowledge base edges, whose number is E kb . For each considered triple, however, we need to compute the sum of representations of path features that are active for the triple. We es- timate the average number of active paths per node pair as T Ne 2 . Therefore the overall time for this method per training iteration is O 2d(η + 1)E kb</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BILINEAR-DIAG</head><formula xml:id="formula_8">T Ne 2 +O d l=2...L lN r l .</formula><p>We should note that whether this method or the one of <ref type="bibr" target="#b8">Guu et al. (2015)</ref> will be faster in training depends on whether the average number of paths per node pair multiplied by E kb is big- ger or smaller than the total number of triples T . Unlike the method of <ref type="bibr" target="#b8">Guu et al. (2015)</ref>, the evaluation-time memory requirements of this ap- proach are the same as its training memory re- quirements, or they could be reduced slightly to match the evaluation-time memory requirements of ALL-PATHS, if these are lower as determined by the specific problem instance.</p><p>ALL-PATHS This method does not explicitly construct or store fully constructed paths (s, π, t). Instead, memory and time is determined by the dynamic program in Algorithm 1, as well as the forward-backward algorithm for computation of gradients. The memory required to store path representation sums</p><formula xml:id="formula_9">F l (s, t) is O dLN e 2</formula><p>in the worst case. Denote E = E kb + E txt . The time to compute these sums is</p><formula xml:id="formula_10">O dE(1 + l=2...L (l − 1)N e )</formula><p>. After this computation, the time to com- pute the scores of training positive and negative triples is O d2(η + 1)E kb L . The time to in- crement gradients using each triple considered in training is O dEL 2 . The evaluation time mem- ory is reduced relative to training time memory by a factor of L and the evaluation time per triple can also be reduced by a factor of L using pre- computation.</p><p>Based on this analysis, we computed train- ing time and memory estimates for our NCI+Txt knowledge base. Given the values of the quanti- ties from our knowledge graph and d = 50, η = 50, and maximum path length of 5, the estimated memory for ( <ref type="bibr" target="#b8">Guu et al., 2015)</ref> and PRUNED- PATHS is 4.0 × 10 18 and for ALL-PATHS the mem- ory is 1.9×10 9 . The time estimates are 2.4×10 21 , 2.6 × 10 25 , and 7.3 × 10 15 for ( <ref type="bibr" target="#b8">Guu et al., 2015)</ref>, PRUNED-PATHS, and ALL-PATHS, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KB</head><p>KB and Text MAP HITS@10 MAP HITS@10 BILINEAR-DIAG ( <ref type="bibr" target="#b8">Guu et al., 2015</ref>  <ref type="table">Table 1</ref>: KB completion results on NCI-PID test: comparison of our compositional learning approach (ALL-PATHS+NODES) with baseline systems. d is the embedding dimension; sampled paths occurring less than c times were pruned in PRUNED-PATHS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Our experiments are designed to study three re- search questions: (i) What is the impact of using path representations as a source of compositional regularization as in ( <ref type="bibr" target="#b8">Guu et al., 2015</ref>) versus using them as features for scoring as in PRUNED-PATHS and ALL-PATHS? (ii) What is the impact of us- ing textual mentions for KB completion in differ- ent models? (iii) Does modeling intermediate path nodes improve the accuracy of KB completion?</p><p>Datasets We used two datasets for evaluation: NCI-PID and WordNet. For the first set of experiments, we used the Pathway Interaction Database (NCI-PID) (Schae- fer et al., 2009) as our knowledge base, which was created by editors from the Nature Publish- ing Groups, in collaboration with the National Cancer Institute. It contains a collection of high- quality gene regulatory networks (also referred to as pathways). The original networks are in the form of hypergraphs, where nodes could be com- plex gene products (e.g., "protein complex" with multiple proteins bound together) and regulations could have multiple inputs and outputs. Follow- ing the convention of most network modeling ap- proaches, we simplified the hypergraphs into bi- nary regulations between genes (e.g., GRB2 posi- tive reg MAPK3), which yields a graph with 2774 genes and 14323 triples. The triples are then split into train, dev, and test sets, of size 10224, 1315, 2784, respectively. We identified genes belonging to the same family via the common letter prefix in their names, which adds 1936 triples to training.</p><p>As a second dataset, we used a WordNet KB with the same train, dev, and test splits as <ref type="bibr" target="#b8">Guu et al. (2015)</ref>. There are 38,696 entities and 11 types of knowledge base relations. The KB includes 112,581 triples for training, 2,606 triples for val- idation, and 10,544 triples for testing. WordNet does not contain textual relations and is used for a more direct comparison with recent works.</p><p>Textual Relations We used PubMed abstracts for text for NCI-PID. We used the gene men- tions identified by Literome ( <ref type="bibr" target="#b15">Poon et al., 2014)</ref>, and considered sentences with co-occurring gene pairs from NCI-PID. We defined textual relations using the fully lexicalized dependency paths be- tween two gene mentions, as proposed in <ref type="bibr" target="#b17">Riedel et al. (2013)</ref>. Additionally, we define trigger-mapped dependency paths, where only important "trigger" words are lexicalized and the rest of the words are replaced with a wild-card character X. A set of 333 words often associated with regulation events in Literome (e.g. induce, inhibit, reduce, sup- press) were used as trigger words. To avoid in- troducing too much noise, we only included tex- tual relations that occur at least 5 times between mentions of two genes that have a KB relation. This resulted in 3,827 distinct textual relations and 1,244,186 mentions. <ref type="bibr">6</ref> The number of textual rela- tions is much larger than that of KB relations, and it helped induce much larger connectivity among genes (390,338 pairs of genes are directly con- nected in text versus 12,100 pairs in KB).</p><p>Systems ALL-PATHS denotes our compositional learning approach that sums over all paths using dynamic programming; ALL-PATHS+NODES ad- ditionally models nodes in the paths. PRUNED- PATHS denotes the traditional approach that learns from sampled paths detailed in §3.1.2; paths with occurrence less than a cutoff are pruned (c = 1 in <ref type="table">Table 1</ref> means that all sampled paths are used). The most relevant prior approach is <ref type="bibr" target="#b8">Guu et al. (2015)</ref>. We ran experiments using both their pub- licly available code and our re-implementation. We also included the BILINEAR-DIAG baseline.</p><p>Implementation Details We used batch training with RProp ( <ref type="bibr" target="#b18">Riedmiller and Braun, 1993)</ref>. The L 2 penalty λ was set to 0.1 for all models, and the entity vectors x e were normalized to unit vectors. For each positive example we sample 500 nega- tive examples. For our implementation of ( <ref type="bibr" target="#b8">Guu et al., 2015)</ref>, we run 5 random walks of each length starting from each node and we found that adding a weight β to the multi-step path triples improves the results. After preliminary experimentation, we fixed β to 0.1. Models using KB and textual rela- tions were initialized from models using KB rela- tions only <ref type="bibr">7</ref> . Model training was stopped when the development set MAP did not improve for 40 it- erations; the parameters with the best MAP on the development set were selected as output. Finally, we used only paths of length up to 3 for NCI-PID and up to length 5 for WordNet. 8</p><p>Evaluation metrics We evaluate our models on their ability to predict the subjects/objects of knowledge base triples in the test set. Since the re- lationships in the gene regulation network are fre- quently not one-to-one, we use the mean average precision (MAP) measure instead of the mean re- ciprocal rank often used in knowledge base com- pletion works in other domains. In addition to MAP, we use Hits@10, which is the percentage of correct arguments ranked among the top 10 predictions <ref type="bibr">9</ref> . We compute measures for ranking both the object entities (s, r, * ) and the subject entities( * , r, t). We report evaluation metrics com- puted on the union of the two query set. <ref type="table">Table 1</ref> summarizes the knowledge base (KB) completion results on the NCI-PID test. The rows compare our compo- sitional learning approach ALL-PATHS+NODES with prior approaches. The comparison of the two columns demonstrates the impact when text is jointly embedded with KB. Our compositional learning approach significantly outperforms all other approaches in both evaluation metrics (MAP and HITS@10). Moreover, jointly embedding text and KB led to substantial improvement, com- pared to embedding KB only. <ref type="bibr">10</ref> Finally, modeling nodes in the paths offers significant gains (ALL- PATHS+NODE gains 3 points in MAP over ALL- PATHS), with statistical significance (p &lt; .001) according to a McNemar test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NCI-PID Results</head><p>Evaluating the effect of path pruning on the tra- ditional approach (PRUNED-PATHS) is quite illu- minating. As the number of KB relations is rela- tively small in this domain, when only KB is em- bedded, most paths occur frequently. So there is little difference between the heaviest pruned ver- sion (c=1000) and the lightest (c=1). When textual relations are included, the cutoff matters more, al- though the difference was small as many rarer tex- tual relations were already filtered beforehand. In either case, the accuracy difference between ALL- PATHS and PRUNED-PATHS is small, and ALL- PATHS mainly gains in efficiency. However, when nodes are modeled, the compositional learning ap- proach gains in accuracy as well, especially when text is jointly embedded.</p><p>Comparison among the baselines also offers valuable insights. The implementation of <ref type="bibr" target="#b8">Guu et al. (2015)</ref> with default parameters performed significantly worse than our re-implementation. Also, our re-implementation achieves only a slight gain over the BILINEAR-DIAG baseline, whereas the original implementation obtains substantial improvement over its own version of BILINEAR- DIAG. These results underscore the importance of hyper-parameters and optimization, and invite future systematic research on the impact of such modeling choices. <ref type="bibr">11</ref> Model MAP HITS@10 BILINEAR-DIAG ( <ref type="bibr" target="#b8">Guu et al., 2015)</ref> N/A 12.9 BILINEAR-DIAG 8.0 12.2 +Guu et al. <ref type="formula" target="#formula_0">(2015)</ref> N/A 14.4 PRUNED-PATHS l = 3 c=10 9.5 14.8 PRUNED-PATHS l = 3 c=1 9.5 14.9 PRUNED-PATHS l = 5 c=10 8.9 14.4 ALL-PATHS l = 3 9.4 14.7 ALL-PATHS+NODES l=3 9.4 15.2 ALL-PATHS l = 5 9.6 16.6 ALL-PATHS+NODES l=5 9.8 16.7 <ref type="table" target="#tab_2">Table 2</ref>: KB completion results on the WordNet test set: comparison of our compositional learn- ing approach (ALL-PATHS) with baseline systems.</p><p>The maximum length of paths is denoted by l. Sampled paths occurring less than c times were pruned in PRUNED-PATHS.  <ref type="bibr" target="#b8">Guu et al. (2015)</ref>, and we find that using PRUNED-PATHS as features gives similar, but a bit higher performance gains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WordNet Results</head><p>The PRUNED-PATHS method is evaluated using count cutoffs of 1 and 10, and maximum path lengths of 3 and 5. As can be seen, lower count cutoff performed better for paths up to length 3, but we could not run the method with path lengths up to 5 and count cutoff of 1, due to excessive memory requirements (more than 248GB). When using count cutoff of 10, paths up to length 5 per- formed worse than paths up to length 3. This performance degradation could be avoided with <ref type="bibr">12</ref> We ran the trained model distributed by <ref type="bibr" target="#b8">Guu et al. (2015)</ref> and obtained a much lower Hits@10 value of 6.4 and MAP of of 3.5. Due to the discrepancy, we report the original results from the authors' paper which lack MAP values instead. a staged training regiment where models with shorter paths are first trained and used to initial- ize models using longer paths.</p><p>The performance of the ALL-PATHS method can be seen for maximum paths up to lengths 3 and 5, and with or without using features on inter- mediate path nodes. <ref type="bibr">13</ref> As shown in <ref type="table" target="#tab_2">Table 2</ref>, longer paths were useful, and features on intermediate nodes were also beneficial. We tested the signif- icance of the differences between several pairs of models and found that nodes led to significant im- provement (p &lt; .002) for paths of length up to 3, but not for the setting with longer paths. All mod- els using path features are significantly better than the baseline BILINEAR-DIAG model.</p><p>To summarize both sets of experiments, the ALL-PATHS approach allows us to efficiently in- clude information from long KB relation paths as in WordNet, or paths including both text and KB relations as in NCI-PID. Our dynamic pro- gramming algorithm considers relation paths ef- ficiently, and is also straightforwardly general- izable to include modeling of intermediate path nodes, which would not be directly possible for the PRUNED-PATHS approach. Using intermedi- ate nodes was beneficial on both datasets, and es- pecially when paths could include textual relations as in the NCI-PID dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this work, we propose the first approach to ef- ficiently incorporate all relation paths of bounded length in a knowledge base, while modeling both relations and intermediate nodes in the composi- tional path representations. Experimental results on two datasets show that it outperforms prior ap- proaches by modeling intermediate path nodes. In the future, we would like to study the impact of relation paths for additional basic KB embedding models and knowledge domains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A snapshot depicting the knowledge graph of a gene regulatory network, augmented with gene family and dependency path relations.</figDesc><graphic url="image-1.png" coords="2,315.61,62.81,201.60,138.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An illustration of the weighted sum of path representations for paths connecting GRB2 and MAPK3 from Figure 1.The prefix " " of a relation type indicates an inverse relation. The path-constrained random walk probabilities for P 1, P 2 and P 3 are 0.3, 0.4 and 0.05 respectively, and 0.0 for the rest. The path length weights are omitted.</figDesc><graphic url="image-2.png" coords="4,308.41,62.81,216.00,120.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Guu et al. ( 2015 )</head><label>2015</label><figDesc>This method generates train- ing path triples {(x, π, y)} for all entity pairs con- nected by paths π up to length L. The number of such triples 5 is T = E kb + E txt + l=2...L N r l × N e × a l . The memory required is the memory to store all triples and the set of relation paths, which is O T + l=2...L lN r l . The time required in- cludes the time to compute the scores and gradi- ents for all triples and their negative examples (for subject and object position), as well as to com- pute the compositional path representations of all path types. The time to compute compositional path representations is O d l=2...L lN r l and the time spent per triple is 2d(η + 1) as in the ba- sic model. Therefore the overall time per itera- tion is O 2d(η + 1)T +O d l=2...L lN r l . The test time and memory requirements of this method are the same as these of BILINEAR-DIAG, which is a substantial advantage over other methods, if evaluation-time efficiency is important. PRUNED-PATHS This method computes and stores the values of the random walk probabili- ties for all pairs of nodes and relation paths, for which these probabilities are non-zero. This can be done in time O T where Triples is the same quantity used in the analysis of Guu et al. (2015).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 presents</head><label>2</label><figDesc></figDesc><table>a compar-
ative evaluation on the WordNet dataset. This 
dataset has a larger number of knowledge base 
relation types compared to NCI-PID, and longer 
relation paths in this KB are expected to be ben-
eficial. Guu et al. (2015) evaluated their com-
positional regularization approach on this dataset 
and we can directly compare to their results. The 
first two rows in the Table show the baseline 
BILINEAR-DIAG model results according to the 
results reported in (Guu et al., 2015) and our im-
plementation. The MAP results were not reported 
in Guu et al. (2015); hence the NA value for MAP 
in row one. 12 On this dataset, our implementation 
of the baseline model does not have substantially 
different results than Guu et al. (2015) and we use 
their reported results for the baseline and compo-
sitionally trained model. 
Compositional training improved performance 
in Hits@10 from 12.9 to 14.4 in </table></figure>

			<note place="foot">* This research was conducted during the author&apos;s internship at Microsoft Research.</note>

			<note place="foot" n="1"> The downside of this approximation is that it enforces symmetry in every relation, i.e., f (s, r, t) = (xs • xt) wr = (xt • xs) wr = f (t, r, s). However, empirically it has been shown to outperform the Bilinear model when the number of training examples is small (Yang et al., 2015).</note>

			<note place="foot" n="2"> Unlike this illustration, the representations of dependency paths are not decomposed into representations of individual edges in our implementation.</note>

			<note place="foot" n="3"> We further speed up the computation by pre-computing, for each node s, the set of notes t reachable via paths of length l. Then we iterate over non-zero entries only, instead of all pairs in the loops.</note>

			<note place="foot" n="4"> If only a subset of paths is used, the complexity of the methods other than ALL-PATHS will be reduced. However, in order to compare the methods in a similar setting, we analyze performance in the case of modeling all multi-step paths.</note>

			<note place="foot" n="6"> Modeling such a large number of textual relations introduces sparsity, which necessitates models such as (Toutanova et al., 2015; Verga et al., 2015) to derive composed representations of text. We leave integration with such methods for future work.</note>

			<note place="foot" n="7"> In addition, models using path training were initialized from models trained on direct relation links only. 8 Using paths up to length 4 on NCI-PID did not perform better. 9 As a common practice in KB completion evaluation, for both MAP and Hits@10, we filtered out the other correct answers when ranking a particular triple to eliminate ranking penalization induced by other correct predictions.</note>

			<note place="foot" n="10"> Text did not help for the models in the first four rows in the Table, possibly because in these approaches text and KB information are equally weighted in the loss function and the more numerous textual triples dominate the KB ones. 11 The differences between our two implementations are: max-margin loss versus softmax loss and stochastic gradient training versus batch training.</note>

			<note place="foot" n="13"> To handle the larger scale of the WordNet dataset in terms of the number of nodes in the knowledge base, we enforced a maximum limit (of 100) on the degree of a node that can be an intermediate node in a multi-step relation path.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank the anonymous reviewers and meta-reviewer for suggestions on making the paper stronger, as well as Kenton Lee and Aria Haghighi for their helpful comments on the draft.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Dbpedia: A nucleus for a web of open data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sören</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgi</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Ives</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic parsing on Freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1533" to="1544" />
		</imprint>
	</monogr>
	<note>, October. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD international conference on Management of data</title>
		<meeting>the 2008 ACM SIGMOD international conference on Management of data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garciaduran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Typed tensor decomposition of knowledge bases for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Composing relationships with translations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="286" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improving learning and inference in a large knowledge-base using latent syntactic cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><forename type="middle">Pratim</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Incorporating vector space similarity in random walk inference over knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Traversing knowledge graphs in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="318" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Relational retrieval using a combination of path-constrained random walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>William W Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reading the web with learned syntactic-semantic inference rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amarnag</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Modeling relation paths for representation learning of knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="705" to="714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACLIJCNLP)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Compositional vector space models for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning (ICML-11)</title>
		<meeting>the 28th international conference on machine learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Literome: Pubmed-scale genomic knowledge base in the cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charlie</forename><surname>Deziel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Heckerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page" from="2840" to="2842" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distant supervision for cancer pathway extraction from text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PSB</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Relation extraction with matrix factorization and universal schemas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">M</forename><surname>Marlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A direct adaptive method for faster backpropagation learning: The rprop algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heinrich</forename><surname>Braun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page" from="586" to="591" />
		</imprint>
	</monogr>
	<note>Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">PID: the pathway interaction database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Carl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Schaefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiva</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Krupa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Buchoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Day</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth H</forename><surname>Hannay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Buetow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="674" to="679" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>suppl</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Open domain question answering via semantic enrichment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Tse</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
		<meeting>the 24th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1045" to="1055" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multiinstance multi-label learning for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Observed versus latent features for knowledge base and text inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting>the 3rd Workshop on Continuous Vector Space Models and their Compositionality</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Representing text for joint embedding of text and knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pallavi</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Multilingual relation extraction using compositional universal schema</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno>arxiv, abs/1511.06396</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semantic parsing via staged query graph generation: Question answering with knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1321" to="1331" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
