<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:05+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GTR-LSTM: A Triple Encoder for Sentence Generation from RDF Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bayu</forename><surname>Distiawan Trisedya</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Melbourne</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhong</forename><surname>Qi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Melbourne</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Melbourne</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of New</orgName>
								<address>
									<country>South Wales</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GTR-LSTM: A Triple Encoder for Sentence Generation from RDF Data</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1627" to="1637"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1627</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>A knowledge base is a large repository of facts that are mainly represented as RDF triples, each of which consists of a subject , a predicate (relationship), and an object. The RDF triple representation offers a simple interface for applications to access the facts. However, this representation is not in a natural language form, which is difficult for humans to understand. We address this problem by proposing a system to translate a set of RDF triples into natural sentences based on an encoder-decoder framework. To preserve as much information from RDF triples as possible, we propose a novel graph-based triple encoder. The proposed encoder encodes not only the elements of the triples but also the relationships both within a triple and between the triples. Experimental results show that the proposed en-coder achieves a consistent improvement over the baseline models by up to 17.6%, 6.0%, and 16.4% in three common metrics BLEU, METEOR, and TER, respectively.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge bases (KBs) are becoming an en- abling resource for many applications includ- ing Q&amp;A systems, recommender systems, and summarization tools. KBs are designed based on a W3C standard called the Resource De- scription Framework (RDF) <ref type="bibr">1</ref> . An RDF triple consists of three elements in the form of subject, predicate (relationship), object. It describes a relationship between an entity (the subject) and another entity or literal (the object)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target sentence</head><p>John Doe was born on 1967-01-10 in London, the capital of England. via the predicate. This representation allows easy data share between KBs. However, usually the el- ements of a triple are stored as Uniform Resource Identifiers (URIs), and many predicates (words or phrases) are not intuitive; this representation is dif- ficult to comprehend by humans.</p><p>Translating RDF triples into natural sentences helps humans to comprehend the knowledge embedded in the triples, and building a natural language based user interface is an important task in user interaction studies <ref type="bibr" target="#b7">(Damljanovic et al., 2010)</ref>. This task has many applications, such as question answering <ref type="bibr" target="#b2">(Bordes et al., 2014;</ref><ref type="bibr" target="#b10">Fader et al., 2014</ref>), profile summariz- ing ( <ref type="bibr" target="#b16">Lebret et al., 2016;</ref><ref type="bibr" target="#b3">Chisholm et al., 2017)</ref>, and automatic weather forecasting ( <ref type="bibr" target="#b20">Mei et al., 2016)</ref>. For example, the SPARQL inference of a Q&amp;A system ( <ref type="bibr" target="#b26">Unger et al., 2012</ref>) returns a set of RDF triples which need to be translated into natural sentences to provide a more read- able answer for the users. <ref type="table" target="#tab_0">Table 1</ref> illustrates such an example. Suppose a user is asking a question about "John Doe". By querying a KB, a Q&amp;A system retrieves three triples "John Doe,birth place,London", "John Doe,birth date,1967-01-10", and "London,capital of,England." We aim to generate a natural sentence that incorporates the information of the triples and is easier to be understood by the user. In this example, the generated sentence is "John Doe was born on 1967-01-10 in London, the capital of England."</p><p>Most existing studies for this task use domain specific rules. <ref type="bibr" target="#b1">Bontcheva and Wilks (2004)</ref> create rules to generate sentences in the medical domain, while <ref type="bibr" target="#b5">Cimiano et al. (2013)</ref> create rules to gener- ate step by step cooking instructions. The prob- lem of rule-based methods is that they need a lot of human efforts to create the rules, which mostly cannot deal with complex or novel cases.</p><p>Recent studies propose neural language gener- ation systems. <ref type="bibr" target="#b16">Lebret et al. (2016)</ref> generate the first sentence of a biography by a conditional neu- ral language model. <ref type="bibr" target="#b20">Mei et al. (2016)</ref> propose an encoder-aligner-decoder architecture to gener- ate weather forecasts. The model does not need predefined rules and hence generalizes better to open domain data.</p><p>A straightforward adaptation of neural language generation system is to use the encoder-decoder model by first concatenating the elements of the RDF triples into a linear sequence and then feed- ing the sequence as the model input to learn the corresponding natural sentence. We implemented such a model (detailed in Section 3.2) that ranked top in the WebNLG Challenge 2017 2 . This Chal- lenge has a primary objective of generating syntac- tically correct natural sentences from a set of RDF triples. Our model achieves the highest global scores on the automatic evaluation, outperforming competitors that use rule-based methods, statisti- cal machine translation, and neural machine trans- lation ( <ref type="bibr" target="#b13">Gardent et al., 2017b)</ref>. While our previous model achieves a good re- sult, simply concatenating the elements in the RDF triples may lose the relationship between entities that affects the semantics of the result- ing sentence (cf. <ref type="table">Table 3</ref>). To address this is- sue, in this paper, we propose a novel graph-based triple encoder model that maintain the structure of RDF triples as a small knowledge graph named the GTR-LSTM model. This model computes the hidden state of each entity in a graph to pre- serve the relationships between entities in a triple (intra-triple relationships) and the relationships between entities in related triples (inter-triple re- lationships) that helps to achieve even more ac- curate sentences. This leads to two problems of preserving the relationships in a knowledge graph: (1) how to deal with a cycle in a knowledge graph; (2) how to deal with multiple non-predefined re- 2 http://talc1.loria.fr/webnlg/stories/challenge.html lationships between two entities in a knowledge graph. The proposed model differs from existing non-linear LSTM models such as Tree LSTM <ref type="bibr" target="#b25">(Tai et al., 2015)</ref> and Graph LSTM ( <ref type="bibr" target="#b17">Liang et al., 2016</ref>) in addressing the mentioned problem. In particu- lar, Tree LSTM does not allow cycles, while the proposed model handles cycles by first using a combination of topological sort and breadth-first traversal over a graph, and then using an atten- tion model to capture the global information of the knowledge graph. Meanwhile, Graph LSTM only allows a predefined set of relationships between entities, while the proposed model allows any re- lationships by treating them as part of the input for the hidden state computation.</p><p>To further enhance the capability of our model to handle unseen entities, we propose to use en- tity masking, which maps the entities in the model training pairs to their types, e.g., we map an en- tity (literal) "1967-01-10" to a type symbol "DATE" in the training pairs. This way, our model can learn to handle any date entities rather than just "1967-01-10". This is particularly helpful when there is a limited training dataset.</p><p>Our contributions are:</p><p>• We propose an end-to-end encoder-decoder based framework for the problem of translat- ing RDF triples into natural sentences.</p><p>• We further propose a graph-based triple en- coder to optimize the amount of information preserved in the input of the framework. The proposed model can handle cycles to cap- ture the global information of a knowledge graph. The proposed model also handles non- predefined relationships between entities.</p><p>• We evaluate the proposed framework and model over two real datasets. The results show that our model outperforms the state- of-the-art models consistently.</p><p>The rest of this paper is organized as follows. Section 2 summarizes previous studies on sen- tence generation. Section 3 details the proposed model. Section 4 presents the experimental re- sults. Section 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The studied problem falls in the area of Natu- ral Language Generation (NLG) <ref type="bibr" target="#b23">(Reiter and Dale, 2000</ref>). <ref type="bibr" target="#b1">Bontcheva and Wilks (2004)</ref> follow a traditional NLG approach to generate sentences from RDF data in the medical domain. They start with filtering repetitive RDF data (document planning) and then group coherent triples (micro- planning). After that, they aggregate the sentences generated for coherent triples to produce the fi- nal sentences (aggregation and realization). <ref type="bibr" target="#b5">Cimiano et al. (2013)</ref> generate cooking recipes from semantic web data. They focus on using a large corpus to extract lexicon in the cooking domain. The lexicon is then used with a traditional NLG approach to generate cooking recipes. <ref type="bibr" target="#b9">Duma and Klein (2013)</ref> learn a sentence template from a par- allel RDF data and text corpora. They first align entities in RDF triples with entities mentioned in sentences. Then, they extract templates from the aligned sentences by replacing the entity mention with a unique token. This method works well on RDF triples in a seen domain but fails on RDF triples in a previously unseen domain.</p><p>Recently, several methods using neural net- works are proposed. <ref type="bibr" target="#b16">Lebret et al. (2016)</ref> gener- ate the first sentence of a biography using a con- ditional neural language model. This model is trained to predict the next word of a sentence not only based on previous words, but also by using features captured from Wikipedia infoboxes. <ref type="bibr" target="#b20">Mei et al. (2016)</ref> propose an encoder-aligner-decoder model to generate weather forecasts. The aligner is used to filter the most relevant data to be used to predict the weather forecast. Both studies experi- ment on cross-domain datasets. The result shows that the neural language generation approach is more flexible to work in an open domain since it is not limited to handcrafted rules. This motivates us to use a neural network based framework. The most similar system to ours is Neural Wikipedian ( <ref type="bibr" target="#b27">Vougiouklis et al., 2017)</ref>, which gen- erates a summary from RDF triples. It uses feed- forward neural networks to encode RDF triples and concatenate them as the input of the decoder. The decoder uses LSTM to predict a sequence of words as a summary. There are differences from our work. First, Neural Wikipedian only works with a set of RDF triples with a single entity point of view (i.e., the entity of interest must be in either the subject or the object of every triple). Our sys- tem does not have this constraint. Second, Neu- ral Wikipedian uses standard feed-forward neural networks in the encoder. We design new triple en- coder models to accommodate specific features of </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Model</head><p>We start with the problem definition. We con- sider a set of RDF triples as the input, which is denoted by T = [t 1 , t 2 , ..., t n ] where a triple t i consists of three elements (subject s i , predicate p i , and object o i ), t i = s i , p i , o i . Every element can contain multiple words. We aim to generate a set of sentences that consist of a sequence of words S = w 1 , w 2 , ..., w m , such that the relationships in the input triples are correctly represented in S while the sentences have a high quality. We use BLEU, METEOR, and TER to assess the quality of the sentence (detailed in Section 4). <ref type="table" target="#tab_0">Table 1</ref> il- lustrates our problem input and the target output. This section is organized as follows. First we describe the overall framework (Section 3.1). Next, we describe three triple encoder models in- cluding the adapted standard BLSTM model (Sec- tion 3.2), the adapted standard triple encoder model (Section 3.3), and the proposed GTR-LSTM model (Section 3.4). The decoder which is used for all encoder models is described in Section 3.5. The entity masking is described in Section 3.6</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Solution Framework</head><p>Our solution framework uses an encoder-decoder architecture as illustrated in <ref type="figure">Fig. 1</ref>. The framework consists of three components including an RDF pre-processor, a target text pre-processor, and an encoder-decoder module.</p><p>The RDF pre-processor consists of an entity type mapper and a masking module. The entity type mapper maps the subjects and objects in the triples to their types, such that the sentence pat- terns learned are based on entity types rather than entities. For example, the input entities in <ref type="table" target="#tab_0">Table 1</ref>, "John Doe", "London", "England", and "1967-01-10" can be mapped to "PERSON", "CITY", "COUNTRY", and "DATE", respectively. The mapping has been shown in our experiments to be highly effective in improving the model output quality. The masking module converts each entity into an entity identifier (eid). The target text pre-processor consists of a text nor- malizer and a de-lexicalizer. The text normal- izer converts abbreviations and dates into the same format as the corresponding entities in the triples. The de-lexicalizer replaces all entities in the target sentences by their eids. The RDF and target text pre-processors are detailed in Sec- tion 3.6. The replaced target sentences are com- bined with the original target sentences and the English Wikipedia articles is used as a corpus to learn the word embeddings of the vocabulary.</p><p>To accommodate the RDF data, in the encoder side, we consider three triple encoder models: (1) the adapted standard BLSTM encoder; (2) the adapted standard triple encoder; and (3) the pro- posed GTR-LSTM triple encoder. The adapted standard BLSTM encoder concatenates the tokens in RDF triples as an input sequence, while the standard triple encoder first encodes each RDF triple into a vector representation and then con- catenates the vectors of different triples. The latter model better captures intra-triple relationships but suffers in capturing inter-triple relationships. Con- sidering the native representation of RDF triples as a small knowledge graph, our graph-based GTR- LSTM triple encoder captures both intra-triple and inter-triple entity relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Adapted Standard BLSTM Encoder</head><p>The standard encoder-decoder model with a BLSTM encoder is a sequence to sequence learn- ing model ( <ref type="bibr" target="#b4">Cho et al., 2014</ref>). To adapt such a model for our problem, we transform a set of RDF triples input T into a sequence of elements (i.e., T = [w 1,1 , w 1,2 , ..., w 1,j , ..., w n,j ]), where w n,j is</p><formula xml:id="formula_0">John → w 1,1 Doe → w 1,2 birth → w 1,3 place → w 1,4 London → w 1,5 London → w 2,1 capital → w 2,2 of → w 2,3 England → w 2,4 &lt;pad&gt; → w 2,5 … → w n,1 … → w n,2 … → w n,3 … → w n,4 … → w n,5</formula><p>w n,5</p><p>. . . the word embedding of a word in the n-th triple. For example, following the triples in <ref type="table" target="#tab_0">Table 1</ref>, w 1,1 is the word embedding of "John", w 1,2 is the word embedding of "Doe", etc. This sequence forms an input for the encoder. We use zero padding to ensure that each input has the same representa- tion size. The rest of the model is the same as the standard encoder-decoder model with an attention mechanism ( <ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref>. We call this model the adapted standard BLSTM encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Adapted Standard Triple Encoder</head><p>The standard BLSTM encoder suffers in captur- ing the element relationships as the elements are simply concatenated together. Next, we adapt the standard BLSTM encoder to aggregate the word embeddings of the elements of the same triple to retain the intra-triple relationship. We call this the adapted standard triple encoder. The adaptation is done by grouping the ele- ments of each triple, so the input is represented as T = [w 1,1 , ..., w 1,j , ..., w n,1 , ...w n,j ], where w n,j is the word embedding of a word in the n-th triple. We use zero padding to ensure that each triple has the same representation size. An LSTM network of the encoder computes a hidden state of each triple and concatenates them together to be the input for the decoder:</p><formula xml:id="formula_1">h T = [f (t 1 ); f (t 2 ); ...; f (t n )]<label>(1)</label></formula><p>where h T is the input vector representation for the decoder and f is an LSTM network (cf. <ref type="figure" target="#fig_0">Fig. 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">GTR-LSTM Triple Encoder</head><p>The adapted standard triple encoder has an ad- vantage in preserving the intra-triple relationship. However, it has not considered the structural rela- To overcome this limitation, we propose a graph- based triple encoder. We call it the GTR-LSTM triple encoder. This encoder takes the input triples in the form of a graph, which preserves the natural structure of the triples (cf. <ref type="figure" target="#fig_1">Fig. 3)</ref>.</p><p>GTR-LSTM differs from existing Graph LSTM ( <ref type="bibr" target="#b17">Liang et al., 2016)</ref> and Tree LSTM <ref type="bibr" target="#b25">(Tai et al., 2015</ref>) models in the following aspects. Graph LSTM is proposed for image data. It con- structs the graph based on the spatial relationships among super-pixels of an image. Tree LSTM uses the dependency tree as the structure of a sentence. Both models have a predefined relationship between the vertices (Graph LSTM uses spatial relationships: top, bottom, left, or right between super-pixels; Tree LSTM uses dependencies between words in a sentence as the relationship). In contrast, a KB has an open set of relationships between the vertices (i.e., the predicate defines the relationship between entities/vertices) which make our problem more difficult to model.</p><p>Our GTR-LSTM triple encoder overcomes the difficulty as follows. It receives a directed graph G = V, E as the input, where V is a set of vertices that represent entities or literals, and E is a set of directed edges that represent predi- cates. Since the graph can contain cycles, we use a combination of topological sort and breadth-first traversal algorithms to traverse the graph. The traversal is used to create an ordering of feeding the vertices into a GTR-LSTM unit to compute their hidden states. We start with running a topo- logical sort to establish an order of the vertices until no further vertex has a zero in-degree. For the remaining vertices, they must be in strongly connected component(s). Then, we run a breadth- first traversal over the remaining vertices with a random starting vertex, since every vertex can be reached from all vertices of a strongly connected component. When a vertex v i is visited, the hid- den states of all adjacent vertices of v i are com- puted (or updated if the hidden state of the vertex  is already computed in the previous step). Following the graph in <ref type="figure" target="#fig_1">Fig. 3</ref>, the order of hid- den state computation is as follows. The pro- cess starts with a vertex with zero in-degree. Be- cause there is no such vertex, a vertex is ran- domly selected as the starting vertex. Assume we pick "John" as the starting vertex, then we com- pute h john using h 0 as the previous hidden state. Next, following the breadth-first traversal algo- rithm, we visit vertex "John" and compute h mary and h london by passing h john as the previous hid- den state. Next step, vertex "Mary" is visited, but no hidden states are computed or updated since it does not have any adjacent vertices. In the last step, vertex "England" is visited and h john is updated. <ref type="figure" target="#fig_2">Fig. 4</ref> illustrates the overall process.</p><p>Different from the Graph LSTM, our GTR- LSTM model computes a hidden state by taking into account the processed entity and its edge (the edge pointing to the current entity from the previ- ous entity) to handle non-predefined relationships (any relationships between entities in a knowledge graph). Thus, our GTR-LSTM unit (cf. <ref type="figure" target="#fig_2">Fig. 4</ref>) receives two inputs, i.e., the entity and its relation- ship. We propose the following model to compute the hidden state of each GTR-LSTM unit.</p><formula xml:id="formula_2">it = σ e U ie xte + W ie ht−1 (2) fte = σ U f xte + W f ht−1 (3) ot = σ e (U oe xte + W oe ht−1) (4) gt = tanh e (U ge xte + W ge ht−1) (5) ct = ct−1 * e fte + (gt * it)<label>(6)</label></formula><p>ht = tanh(ct) * ot</p><p>Here, U and W are learned parameter matrices, σ denotes the sigmoid function, * denotes element-  <ref type="figure">Figure 5</ref>: Attention model of GTR-LSTM.</p><p>wise multiplication, and x is the input at the cur- rent time-step. The input gate i determines the weight of the current input. The forget gate f de- termines the weight of the previous state. The out- put gate o determines the weight of the cell state forwarded to the next time-step. The state g is the candidate hidden state used to compute the inter- nal memory unit c based on the current input and the previous state. The subscript t is the time- step. The subscript/superscript e is the input el- ement (an entity or a predicate). Following Tree LSTM ( <ref type="bibr" target="#b25">Tai et al., 2015</ref>) and Graph LSTM ( <ref type="bibr" target="#b17">Liang et al., 2016)</ref>, we also use a separate forget gate for each input that allows the GTR-LSTM unit to in- corporate information from each input selectively. <ref type="figure" target="#fig_2">From Fig. 4</ref>, we can see that the traversal cre- ates two branches, one ended in h mary and the other ended in h john . After the encoder computes the hidden states of each vertex, h john does not include the information of h mary and vice versa. Moreover, the graph can contain cycles that cause difficulty in determining the starting and ending vertices. Our traversal procedure ensures that the hidden states of all vertices are updated based on their adjacent vertices (local neighbors). To fur- ther capture the global information of the graph, we apply an attention model on the GTR-LSTM triple encoder. The attention model takes the hid- den states of all vertices computed by the encoder and the previous hidden state of the decoder to compute the final input vector of each decoder time-step. <ref type="figure">Figure 5</ref> illustrates the attention model of GTR-LSTM. Inspired by <ref type="bibr" target="#b19">Luong et al. (2015)</ref>, we adapt the following equation to compute the weight of each vertex.</p><formula xml:id="formula_4">α n = exp(h d t T W x n ) |X| j=1 exp(h d t T W x j )<label>(8)</label></formula><p>Here, h d t is the previous hidden state of the decoder, |X| is the total number of entities in the triples, W is a learned parameter matrix, x n and x j are hidden states of vertices, and α = {α 1 , α 2 , ..., α n } is the weight vector of all ver- tices. Then the input of the decoder for each time- step can be computed as follows.</p><formula xml:id="formula_5">h T = |X| n=1 α n x n (9)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Decoder</head><p>The decoder of the proposed framework is a stan- dard LSTM. It is trained to generate the output sequence by predicting the next output word w t conditioned on the hidden state h d t . The current hidden state h d t is conditioned on the hidden state of the previous time-step h d t−1 , the output of the previous time-step w t−1 , and input vector repre- sentation h T . The hidden state and the output of the decoder at time-step t are computed as:</p><formula xml:id="formula_6">h d t = f (h d t−1 , w t−1 , h T ) (10) w t = sof tmax(V h t )<label>(11)</label></formula><p>Here, f is a single LSTM unit, and V is the hidden-to-output weight matrix. The encoder and the decoder are trained to maximize the condi- tional log-likelihood:</p><formula xml:id="formula_7">p(S n | T n ) = |Sn| t=1 log w t<label>(12)</label></formula><p>Hence, the training objective is to minimize the negative conditional log-likelihood:</p><formula xml:id="formula_8">J = N n=1 − log p(S n | T n )<label>(13)</label></formula><p>where (S n , T n ) is a pair of output word sequence and input RDF triple set given for the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Entity Masking</head><p>Entity masking makes our framework generalizes better to unseen entities. This technique addresses the problem of a limited training set which is faced by many NLG problems. Entity masking replaces entity mentions with eids and entity types in both the input triples and the target sentences. However, we do not want our model to be overly generalized either. Thus, we need to have general and specific entity types. For example, the entity "John Doe" is replaced by "ENT-1 PERSON GOVERNOR". To add the en- tity types, we use the DBpedia lookup API. The API returns several entity types. The general and specific entity types are defined by the level of the word in the WordNet <ref type="bibr" target="#b11">(Fellbaum, 1998)</ref> hierarchy.</p><p>In the encoder side, each element of the triple t n = s n , p n , o n is transformed into s n = l sn , g sn , d sn , p n = l pn , and o n = l on , g on , d on , where l is the label of an element, g is the general entity type, and d is the specific entity type. The labels of the subject and the ob- ject are latter replaced by eids, while the label of the predicate is preserved, since it indicates the re- lationship between the subject and the object.</p><p>On the decoder side, the entities in the tar- get text are also replaced by their corresponding eids. Entity matching is beyond the scope of our study. We simply use a combination of three string matching methods to find entity mentions in the sentence: exact matching, n-gram matching, and parse tree matching. The exact matching is used to find the exact mention; the n-gram matching is used to handle partial matching with the same to- ken length; and parse tree matching is used to find a partial matching with different token length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate our framework on two datasets. The first is the dataset from <ref type="bibr" target="#b12">Gardent et al. (2017a)</ref>. We call it the WebNLG dataset. This dataset con- tains 25,298 RDF triple set-text pairs, with 9,674 unique sets of RDF triples. The dataset con- sists of a Train+Dev dataset and a Test Unseen dataset. We split Train+Dev into a training set (80%), a development set (10%), and a Seen test- ing set (10%). The Train+Dev dataset contains RDF triples in ten categories (topics, e.g., astro- naut, monument, food, etc.), while the Test Un- seen dataset has five other unseen categories. The maximum number of triples in each RDF triple set is seven. For the second dataset, we collected data from Wikipedia pages regarding landmarks. We call it the GKB dataset. We first extract RDF triples from Wikipedia infoboxes and sentences from the Wikipedia text that contain entities men- tioned in the RDF triples. Human annotators then filter out false matches to obtain 1,000 RDF triple set-text pairs. This dataset is split into the train- ing and development set (80%) and the testing set (20%). <ref type="table" target="#tab_0">Table 1</ref> illustrates an example of the data pairs of WebNLG and GKB dataset.</p><p>We implement the existing models, the adapted model, and the proposed model using Keras <ref type="bibr">3</ref> . We use three common evaluation metrics in- cluding BLEU ( <ref type="bibr" target="#b21">Papineni et al., 2002</ref>), ME- TEOR ( <ref type="bibr" target="#b8">Denkowski and Lavie, 2011)</ref>, and TER ( <ref type="bibr" target="#b24">Snover et al., 2006</ref>). For the metric com- putation and significance testing, we use MultE- val <ref type="bibr" target="#b6">(Clark et al., 2011</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Tested Models</head><p>We compare our proposed graph-based triple encoder (GTR-LSTM, Section 3.4) with three existing model including the adapted standard BLSTM encoder (BLSTM, Section 3.2), Neural Wikipedian (Vougiouklis et al., 2017) (TFF), and statistical machine translation <ref type="bibr" target="#b14">(Hoang and Koehn, 2008</ref>) (SMT) trained on a 6-gram language model. We also compare with the adapted standard triple encoder (TLSTM, Section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Hyperparameters</head><p>We use grid search to find the best hyperparame- ters for the neural networks. We use GloVe <ref type="bibr" target="#b22">(Pennington et al., 2014</ref>) trained on the GKB and WebNLG training data and full English Wikipedia data dump to get 300-dimension word embed- dings. We use 512 hidden units for both en- coder and decoder. We use a 0.5 dropout rate for regularization on both encoder and decoder to avoid overfitting. We train our model on NVIDIA Tesla K40c. We find that using adaptive learn- ing rates for the optimization is efficient and leads the model to converge faster. Thus, we use Adam ( <ref type="bibr" target="#b15">Kingma and Ba, 2015</ref>) with a learning rate of 0.0002 instead of stochastic gradient descent. The update of parameters in training is computed using a mini batch of 64 instances. We further ap- ply early stopping to detect the convergence. <ref type="table" target="#tab_5">Table 2</ref> shows the overall comparison of model performance. It shows that entity masking gives a consistent performance improvement for all mod- els. Generalizing the input triples and target sen- tences helps the models to learn the relationships between entities from their types. This is partic- ularly helpful when there is limited training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Effect of Entity Masking</head><p>We use a combination of exact matching, n-gram matching and parse tree matching to find the entity mentions in the sentence.   SMT wembley stadium is located in london , elizabeth tower . theresa may is the leader of england , england.</p><p>TFF the elizabeth tower is located in london , england , where wembley stadium is the leader and theresa may is the leader.</p><p>TLSTM the wembley stadium is located in london , england . the country is the location of elizabeth tower . theresa may is the leader of london.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GTR-LSTM</head><p>the wembley stadium and elizabeth tower are both located in london , england . theresa may is the leader of england. <ref type="table">Table 3</ref>: Sample output of the system. The error is highlighted in bold.</p><p>the GKB dataset is 82.45%. Entity masking improves the BLEU score of the proposed GTR-LSTM model by 8.5% (from 54.0 on the Entity Unmasking model to 58.6 on the Entity Masking model), 16.7%, and 8.0% on the WebNLG seen testing data (denoted by "Seen"), WebNLG unseen testing data (denoted by "Unseen"), and the GKB testing data (denoted by "GKB"). Using the entity masking not only improves the performance by recognizing the un- known vocabulary via eid masking but also im- proves the running time performance by requiring a smaller training vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Effect of Models</head><p>Table 2 also shows that the proposed GTR-LSTM triple encoder achieves a consistent improvement over the baseline models, and the improvement is statistically significant, with p &lt; 0.01 based on the t-test of all metrics. We use MultEval to com- pute the p value based on an approximate random- ization <ref type="bibr" target="#b6">(Clark et al., 2011</ref>). The improvement on the BLEU score indicates that the model reduces the errors in the generated sentence. Our manual inspection confirms this result. The better (lower) TER score suggests that the model generates a more compact output (i.e., better aggregation). <ref type="table">Table 3</ref> shows a sample output of all models. From this table, we can see that all baseline models produce sentences that contain wrong relationships between entities (e.g., the BLSTM output contains a wrong relationship "the elizabeth tower is located in the city of england"). Moreover, the base- line models generate sentences with a weak aggregation (e.g., "Elizabeth Tower" and "Wembley Stadium" are in separate sentences for TLSTM). The proposed GTR-LSTM model successfully avoids these problems.</p><p>Model training time. GTR-LSM is slower in training than the baseline models, which is ex- pected as it needs to encode more information. However, its training time is no more than twice as that of any baseline models tested, and the train- ing can complete within one day which seems reasonable. Meanwhile, the number of parame- ters trained for GTR-LSTM is up to 59% smaller than those of the baseline models, which saves the space cost for model storage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Human Evaluation</head><p>To complement the automatic evaluation, we con- duct human evaluations for all of the masked mod- els. We ask five human annotators. Each of them   has studied English for at least ten years and com- pleted education in a full English environment for at least two years. We provide a website 4 that shows them the RDF triples and the generated text. The annotators are given training on the scoring criteria. We also provide scoring examples. We randomly selected 100 sets of triples along with the output of each model. We only select sets of triples that contain more than two triples. Follow- ing ( <ref type="bibr" target="#b13">Gardent et al., 2017b</ref>), we use three evalua- tion metrics including correctness, grammatical- ity, and fluency. For each pair of triple set and generated sentences, the annotators are asked to give a score between one to three for each metric.</p><p>Correctness is used to measure the semantics of the output sentence. A score of 3 is given to gen- erated sentences that contain no errors in the rela- tionships between entities; a score of 2 is given to generated sentences that contain one error in the relationship; and a score of 1 is given to gener- ated sentences that contain more than one errors in the relationships. Grammaticality is used to rate the grammatical and spelling errors of the gener- ated sentences. Similar to the correctness metric, a score of 3 is given to generated sentences with no grammatical and spelling errors; a score of 2 is given to generated sentences with one error; and a score of 1 for the others. The last metric, fluency, is used to measure the fluency of the sentence out- put. We ask the annotators to give a score based on the aggregation of the sentences and the existence of sentence repetition. <ref type="table" target="#tab_7">Table 4</ref> shows the results of the human evaluations. The results confirm the automatic evaluation in which our proposed model achieves the best scores.</p><p>Error analysis. We further perform a manual inspection of 100 randomly selected output sentences of GTR-LSTM and BLSTM on the Seen and Unseen test data. We find that 32% of BLSTM output contains wrong relationships between entities.</p><p>In comparison, only 8% of GTR-LSTM output contains such errors. Besides, we find duplicate sub-sentences in <ref type="bibr">4</ref> http://bit.ly/gkb-mappings the output of GTR-LSTM (15%). The fol- lowing output is an example: "beef kway teow is a dish from singapore, where english language is spoken and the leader is tony tan. the leader of singapore is tony tan." While the duplicate sentence is not wrong, it affects the reading experience. We conjecture that the LSTM in the decoder caused such an issue. We aim to solve this problem in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We proposed a novel graph-based triple encoder GTR-LSTM for sentence generation from RDF data. The proposed model maintains the struc- ture of input RDF triples as a small knowledge graph to optimize the amount of information pre- served in the input of the model. The proposed model can handle cycles to capture the global in- formation of a knowledge graph and also handle non-predefined relationships between entities of a knowledge graph.</p><p>Our experiments show that GTR-LSTM offers a better performance than all the competitors. On the WebNLG dataset, our model outperforms the best existing model, the standard BLSTM model, by up to 17.6%, 6.0%, and 16.4% in terms of BLEU, METEOR, and TER scores, respectively. On the GKB dataset, our model outperforms the standard BLSTM model by up to 15.2%, 20.9%, and 23.1% in these three metrics, respectively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: LSTM-based standard triple encoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A small knowledge graph formed by a set of RDF triples. tionships between the entities in different triples. To overcome this limitation, we propose a graphbased triple encoder. We call it the GTR-LSTM triple encoder. This encoder takes the input triples in the form of a graph, which preserves the natural structure of the triples (cf. Fig. 3). GTR-LSTM differs from existing Graph LSTM (Liang et al., 2016) and Tree LSTM (Tai et al., 2015) models in the following aspects. Graph LSTM is proposed for image data. It constructs the graph based on the spatial relationships among super-pixels of an image. Tree LSTM uses the dependency tree as the structure of a sentence. Both models have a predefined relationship between the vertices (Graph LSTM uses spatial relationships: top, bottom, left, or right between super-pixels; Tree LSTM uses dependencies between words in a sentence as the relationship). In contrast, a KB has an open set of relationships between the vertices (i.e., the predicate defines the relationship between entities/vertices) which make our problem more difficult to model. Our GTR-LSTM triple encoder overcomes the difficulty as follows. It receives a directed graph G = V, E as the input, where V is a set of vertices that represent entities or literals, and E is a set of directed edges that represent predicates. Since the graph can contain cycles, we use a combination of topological sort and breadth-first traversal algorithms to traverse the graph. The traversal is used to create an ordering of feeding the vertices into a GTR-LSTM unit to compute their hidden states. We start with running a topological sort to establish an order of the vertices until no further vertex has a zero in-degree. For the remaining vertices, they must be in strongly connected component(s). Then, we run a breadthfirst traversal over the remaining vertices with a random starting vertex, since every vertex can be reached from all vertices of a strongly connected component. When a vertex v i is visited, the hidden states of all adjacent vertices of v i are computed (or updated if the hidden state of the vertex</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: GTR-LSTM triple encoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>BLSTM england is lead by theresa may and is located in the city of london . the elizabeth tower is located in the city of england and is located in the wembley stadium.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Model</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : RDF based sentence generation.</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Mary h mary England capital_of h england London h london John lead_by h' john spouse birth_place</head><label></label><figDesc></figDesc><table>&lt; 
&gt; 

John 

null 

h john 

h 0 

Attention model 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Comparison of model performance. 

RDF inputs 
Elizabeth Tower, location, London, Wembley Stadium, location, London, 
London, capital of, England, Theresa May, prime minister, England 

Reference 
london , england is home to wembley stadium and the elizabeth tower. 
the name of the leader in england is theresa may. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Human evaluation results. 

</table></figure>

			<note place="foot">t 1 word embedding Input representation LSTM t 2 t n h n,1 h n,2 h n,3 h n,4 h n,5 h 1,5 h 2,5 h n,5 w n,1 w n,2 w n,3 w n,4</note>

			<note place="foot" n="3"> https://nmt-keras.readthedocs.io/en/latest/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Bayu Distiawan Trisedya is supported by the In-donesian Endowment Fund for Education (LPDP). This work is supported by Australian Research Council (ARC) Discovery Project DP180102050 and Future Fellowships Project FT120100832, and Google Faculty Research Award. This work is partly done while Jianzhong Qi is visiting the University of New South Wales. Wei Wang was partially supported by D2DCRC DC25002, DC25003, ARC DP 170103710 and 180103411.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1409.0473" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Automatic Report Generation from Ontologies: The MIAKT Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalina</forename><surname>Bontcheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yorick</forename><surname>Wilks</surname></persName>
		</author>
		<idno type="doi">10.1007/978-3-540-27779-828</idno>
		<ptr target="https://doi.org/10.1007/978-3-540-27779-828" />
		<imprint>
			<date type="published" when="2004" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="324" to="335" />
			<pubPlace>Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Question answering with subgraph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D/D14/D14-1067.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="615" to="620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to generate one-sentence biographies from wikidata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chisholm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Hachey</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/E17-1060" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics (EACL)</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics (EACL)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="633" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D14-1179" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Exploiting ontology lexica for generating natural language texts from rdf data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Cimiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janna</forename><surname>Lüker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christina</forename><surname>Unger</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W13-2102" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th European Workshop on Natural Language Generation (ENLG)</title>
		<meeting>the 14th European Workshop on Natural Language Generation (ENLG)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="10" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Better hypothesis testing for statistical machine translation: Controlling for optimizer instability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P11-2031" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT)</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="176" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Natural language interfaces to ontologies: combining syntactic analysis and ontology-based lookup through the user interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danica</forename><surname>Damljanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Agatonovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamish</forename><surname>Cunningham</surname></persName>
		</author>
		<idno type="doi">10.1007/978-3-642-13486-98</idno>
		<ptr target="https://doi.org/10.1007/978-3-642-13486-98" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on The Semantic Web (ISWC)</title>
		<meeting>the 7th International Conference on The Semantic Web (ISWC)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="106" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Meteor 1.3: Automatic metric for reliable optimization and evaluation of machine translation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lavie</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/W11-2107" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Workshop on Statistical Machine Translation (WMT)</title>
		<meeting>the Sixth Workshop on Statistical Machine Translation (WMT)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="85" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generating natural language from linked data: Unsupervised template extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Duma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ewan</forename><surname>Klein</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W13-0108" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Computational Semantics (IWCS)</title>
		<meeting>the 10th International Conference on Computational Semantics (IWCS)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="83" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Open question answering over curated and extracted knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<idno type="doi">10.1145/2623330.2623677</idno>
		<ptr target="https://doi.org/10.1145/2623330.2623677" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<meeting>the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1156" to="1165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">WordNet: An Electronic Lexical Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Creating training corpora for nlg micro-planners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasia</forename><surname>Shimorina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Perez-Beltrachini</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P17-1017" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="179" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The webnlg challenge: Generating text from rdf data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasia</forename><surname>Shimorina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Perez-Beltrachini</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W17-3518" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Natural Language Generation (INLG)</title>
		<meeting>the 10th International Conference on Natural Language Generation (INLG)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="124" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Design of the moses decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W08-0510" />
	</analytic>
	<monogr>
		<title level="m">Software Engineering, Testing, and Quality Assurance for Natural Language Processing (SETQA-NLP)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="58" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1412.6980" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural text generation from structured data with application to the biography domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Lebret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<ptr target="https://aclweb.org/anthology/D16-" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1203" to="1213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semantic object parsing with graph lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th</title>
		<meeting>the 14th</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<idno type="doi">10.1007/978-3-319-46448-08</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46448-08" />
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page" from="125" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D15-1166" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">What to talk about and how? selective generation using lstms with coarse-to-fine alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Walter</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N16-1086" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="720" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P02-1040.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of 40th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>40th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="https://aclweb.org/anthology/D14-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Building natural language generation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A study of translation edit rate with targeted human annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Snover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linnea</forename><surname>Micciulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
		<ptr target="http://mt-archive.info/AMTA-2006-Snover.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association for Machine Translation in the Americas (AMTA)</title>
		<meeting>Association for Machine Translation in the Americas (AMTA)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="223" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P15-1150" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL) and the 7th International Joint Conference on Natural Language Processing (IJCNLP)</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics (ACL) and the 7th International Joint Conference on Natural Language Processing (IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Template-based question answering over rdf data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christina</forename><surname>Unger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenz</forename><surname>Bhmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel-Cyrille Ngonga</forename><surname>Ngomo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gerber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Cimiano</surname></persName>
		</author>
		<idno type="doi">10.1145/2187836.2187923</idno>
		<ptr target="https://doi.org/10.1145/2187836.2187923" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st international conference on World Wide Web (WWW)</title>
		<meeting>the 21st international conference on World Wide Web (WWW)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="639" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlos</forename><surname>Vougiouklis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hady</forename><surname>Elsahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucie-Aime</forename><surname>Kaffee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Gravier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederique</forename><surname>Laforest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Simperl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00155</idno>
		<title level="m">Neural wikipedian: Generating textual summaries from knowledge base triples</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
