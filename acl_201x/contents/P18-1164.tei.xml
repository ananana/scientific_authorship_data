<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:59+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attention Focusing for Neural Machine Translation by Bridging Source and Target Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohui</forename><surname>Kuang</surname></persName>
							<email>shaohuikuang@foxmail.com, {lijunhui, dyxiong}@suda.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">António</forename><surname>Branco</surname></persName>
							<email>antonio.branco@di.fc.ul.pt</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Informatics Faculdade de Ciências</orgName>
								<orgName type="laboratory">NLX-Natural Language and Speech Group</orgName>
								<orgName type="institution">University of Lisbon</orgName>
								<address>
									<addrLine>Campo Grande</addrLine>
									<postCode>1749-016</postCode>
									<settlement>Lisboa, Portuga</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Luo</surname></persName>
							<email>weihua.luowh@alibaba-inc.com</email>
							<affiliation key="aff2">
								<orgName type="department">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Attention Focusing for Neural Machine Translation by Bridging Source and Target Embeddings</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1767" to="1776"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1767</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In neural machine translation, a source sequence of words is encoded into a vector from which a target sequence is generated in the decoding phase. Differently from statistical machine translation, the associations between source words and their possible target counterparts are not explicitly stored. Source and target words are at the two ends of a long information processing procedure, mediated by hidden states at both the source encoding and the target decoding phases. This makes it possible that a source word is incorrectly translated into a target word that is not any of its admissible equivalent counterparts in the target language. In this paper, we seek to somewhat shorten the distance between source and target words in that procedure, and thus strengthen their association, by means of a method we term bridging source and target word embeddings. We experiment with three strategies: (1) a source-side bridging model, where source word embeddings are moved one step closer to the output target sequence; (2) a target-side bridging model, which explores the more relevant source word embeddings for the prediction of the target sequence; and (3) a direct bridging model, which directly connects source and target word embeddings seeking to minimize errors in the translation of ones by the others. Experiments and analysis presented in this paper demonstrate that the proposed bridging models are able to significantly * Corresponding author … … … … source target Figure 1: Schematic representation of seq2seq NMT, where x 1 ,. .. , x T and h 1 ,. .. , h T represent source-side word embeddings and hidden states respectively, c t represents a source-side context vector, s t a target-side decoder RNN hidden state, and y t a predicted word. Seeking to shorten the distance between source and target word embed-dings, in what we term bridging, is the key insight for the advances presented in this paper. improve quality of both sentence translation , in general, and alignment and translation of individual source words with target words, in particular.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural machine translation (NMT) is an end- to-end approach to machine translation that has achieved competitive results vis-a-vis statisti- cal machine translation (SMT) on various lan- guage pairs ( <ref type="bibr" target="#b1">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b19">Sutskever et al., 2014;</ref>. In NMT, the sequence-to-sequence (seq2seq) model learns word embeddings for both source and target words synchronously. However, as illustrated in <ref type="figure">Figure 1</ref>, source and target word embeddings are at the two ends of a long informa- tion processing procedure. The individual associ- ations between them will gradually become loose due to the separation of source-side hidden states (represented by h 1 , . . . , h T in <ref type="figure">Fig. 1</ref>) and a target-(winter olympics) (honors) eos the french athletes , who have participated in the disabled , have returned to paris . eos french athletes participating in special winter olympics returned to paris with honors Source Reference Baseline (this) (month) (late) eos sir lanka UNK to hold talks in geneva eos Reference two warring sides in sri lanka agreed to hold talks in geneva late this month Figure 2: Examples of NMT output with incorrect alignments of source and target words that can- not be the translation of each other in any possible context. side hidden state (represented by s t in <ref type="figure">Fig. 1)</ref>. As a result, in the absence of a more tight interaction between source and target word pairs, the seq2seq model in NMT produces tentative translations that contain incorrect alignments of source words with target counterparts that are non-admissible equiv- alents in any possible translation context.</p><p>Differently from SMT, in NMT an attention model is adopted to help align output with input words. The attention model is based on the es- timation of a probability distribution over all in- put words for each target word. Word alignments with attention weights can then be easily deduced from such distributions and support the transla- tion. Nevertheless, sometimes one finds trans- lations by NMT that contain surprisingly wrong word alignments, that would unlikely occur in SMT.</p><p>For instance, <ref type="figure">Figure 2</ref> shows two Chinese- to-English translation examples by NMT. In the top example, the NMT seq2seq model incorrectly aligns the target side end of sentence mark eos to /late with a high attention weight (0.80 in this example) due to the failure of appropriately cap- turing the similarity, or the lack of it, between the source word /late and the target eos. It is also worth noting that, as /this and /month end up not being translated in this example, inappropriate alignment of target side eos is likely the respon- sible factor for under translation in NMT as the decoding process ends once a target eos is gener- ated. Statistics on our development data show that as much as 50% of target side eos do not properly align to source side eos.</p><p>The second example in <ref type="figure">Figure 2</ref> shows another case where source words are translated into tar- get items that are not their possible translations in that or in any other context. In particular, /winter olympics is incorrectly translated into a target comma "," and /honors into have.</p><p>In this paper, to address the problem illustrated above, we seek to shorten the distance within the seq2seq NMT information processing procedure between source and target word embeddings. This is a method we term as bridging, and can be con- ceived as strengthening the focus of the attention mechanism into more translation-plausible source and target word alignments. In doing so, we hope that the seq2seq model is able to learn more appro- priate word alignments between source and target words.</p><p>We propose three simple yet effective strategies to bridge between word embeddings. The inspir- ing insight in all these three models is to move source word embeddings closer to target word em- beddings along the seq2seq NMT information pro- cessing procedure. We categorize these strategies in terms of how close the source and target word embeddings are along that procedure, schemati- cally depicted in <ref type="figure">Fig. 1</ref>.</p><p>(1) Source-side bridging model: Our first strat- egy for bridging, which we call source-side bridging, is to move source word embeddings just one step closer to the target end. Each source word embedding is concatenated with the respective source hidden state at the same position so that the attention model can more closely benefit from source word embeddings to produce word alignments.</p><p>(2) Target-side bridging model: In a second more bold strategy, we seek to incorporate rel- evant source word embeddings more closely into the prediction of the next target hid- den state. In particular, the most appropriate source words are selected according to their attention weights and they are made to more closely interact with target hidden states.</p><p>(3) Direct bridging model: The third model con- sists of directly bridging between source and target word embeddings. The training objec- tive is optimized towards minimizing the dis- tance between target word embeddings and their most relevant source word embeddings, selected according to the attention model.</p><p>Experiments on Chinese-English translation with extensive analysis demonstrate that directly bridging word embeddings at the two ends can produce better word alignments and thus achieve better translation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Bridging Models</head><p>As suggested by <ref type="figure">Figure 1</ref>, there may exist different ways to bridge between x and y t . We concentrate on the folowing three bridging models. <ref type="figure">Figure 3</ref> illustrates the source-side bridging model. The encoder reads a word sequence equipped with word embeddings and generates a word annotation vector for each position. Then we simply concatenate the word annotation vec- tor with its corresponding word embedding as the final annotation vector. For example, the fi- nal annotation vector h j for the word</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Source-side Bridging Model</head><formula xml:id="formula_0">x j in Fig- ure 3 is [ − → h j ; ← − h j ; x j ], where the first two sub-items [ − → h j ; ← − h j ]</formula><p>are the source-side forward and back- ward hidden states and x j is the corresponding word embedding. In this way, the word embed- dings will not only have a more strong contribu- tion in the computation of attention weights, but also be part of the annotation vector to form the weighted source context vector and consequently have a more strong impact in the prediction of tar- get words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Target-side Bridging Model</head><p>While the above source-side bridging method uses the embeddings of all words for every target word, in the target-side method only more rel- evant source word embeddings for bridging are explored, rather than all of them. This is par- tially inspired by the word alignments from SMT, where words from the two ends are paired as they are possible translational equivalents of each other and those pairs are explicitly recorded and enter into the system inner workings. In particular, for a given target word, we explicitly determine the most likely source word aligned to it and use the word embedding of this source word to support the prediction of the target hidden state of the next tar- get word to be generated. <ref type="figure" target="#fig_1">Figure 4</ref> schematically illustrates the target-side bridging method, where the input for computing the hidden state s t of the decoder is augmented by x t * , as follows:</p><formula xml:id="formula_1">s t = f (s t−1 , y t−1 , c t , x t * ) (1)</formula><p>where x t * is the word embedding of the se- lected source word which has the highest attention weight:</p><formula xml:id="formula_2">t * = arg max j (α tj ) (2)</formula><p>where α tj is the attention weight of each hidden state h j computed by the attention model</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Direct Bridging Model</head><p>Further to the above two bridging methods, which use source word embeddings to predict target words, we seek to bridge the word embeddings of the two ends in a more direct way. This is done by resorting to an auxiliary objective function to nar- row the discrepancy between word embeddings of the two sides. <ref type="figure" target="#fig_2">Figure 5</ref> is a schematic representation of our direct bridging method, with an auxiliary objec- tive function. More specifically, the goal is to let the learned word embeddings on the two ends be transformable, i.e. if a target word e i aligns with a source word f j , a transformation matrix W is learned with the hope that the discrepancy of W x i and y j tends to be zero. Accordingly, we update the objective function of training for a single sen- tence with its following extended formulation:</p><formula xml:id="formula_3">L(θ) = − Ty t=1 (log p(y t |y &lt;t , x) − W x t * − y t 2 ) (3)</formula><p>where log p(y t |y &lt;t , x) is the original objective function of the NMT model, and the term W x t * − y t 2 measures and penalizes the differ- ence between target word y t and its aligned source word x t * , i.e. the one with the highest attention weight, as computed in Equation 2. Similar to <ref type="bibr" target="#b14">Mi et al. (2016)</ref>, we view the two parts of the loss in Equation 3 as equally important.</p><p>At this juncture, it is worth noting the following:</p><p>• Our direct bridging model is an extension of the source-side bridging model, where the source word embeddings are part of the fi- nal annotation vector of the encoder. We have also tried to place the auxiliary object function directly on the NMT baseline model. However, our empirical study showed that the combined objective consistently worsens the translation quality. We blame this on that the learned word embeddings on two sides by the baseline model are too heterogeneous to be constrained.</p><p>• Rather than using a concrete source word em- bedding x t * in Equation 3, we could also use a weighted sum of source word embeddings, i.e. j α tj h j . However, our preliminary ex- periments showed that the performance gap between these two methods is very small. Therefore, we use x t * to calculate the new training objective as shown in Equation 3 in all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>As we have presented above three different meth- ods to bridge between source and target word em- beddings, in the present section we report on a se- ries of experiments on Chinese to English transla- tion that are undertaken to assess the effectiveness of those bridging methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Settings</head><p>We resorted to Chinese-English bilingual corpora that contain 1.25M sentence pairs extracted from LDC corpora, with 27.9M Chinese words and 34.5M English words respectively. <ref type="bibr">1</ref> We chose the NIST06 dataset as our development set, and the NIST02, NIST03, NIST04, NIST08 datasets as our test sets.</p><p>We used the case-insensitive 4-gram NIST BLEU score as our evaluation metric ( <ref type="bibr" target="#b17">Papineni et al., 2002</ref>) and the script 'mteval-v11b.pl' to compute BLEU scores. We also report TER scores on our dataset ( <ref type="bibr" target="#b18">Snover et al., 2006</ref>).</p><p>For the efficient training of the neural net- works, we limited the source (Chinese) and target (English) vocabularies to the most frequent 30k words, covering approximately 97.7% and 99.3% of the two corpora respectively. All the out-of- vocabulary words were mapped to the special to- ken UNK. The dimension of word embedding was 620 and the size of the hidden layer was 1000. All other settings were the same as in <ref type="bibr" target="#b1">Bahdanau et al. (2015)</ref>. The maximum length of sentences that we used to train the NMT model in our experiments was set to 50, for both the Chinese and English sides. Additionally, during decoding, we used the beam-search algorithm and set the beam size to 10. The model parameters were selected according to the maximum BLEU points on the development set.</p><p>We compared our proposed models against the following two systems:</p><p>• cdec ( <ref type="bibr" target="#b6">Dyer et al., 2010)</ref>: this is an open source hierarchical phrase-based SMT sys- tem <ref type="bibr" target="#b2">(Chiang, 2007)</ref> with default configura- tion and a 4-gram language model trained on the target side of the training data.</p><p>• RNNSearch*: this is an attention-based NMT system, taken from the dl4mt tutorial with slight changes. It improves the atten- tion model by feeding the lastly generated word. For the activation function f of an RNN, we use the gated recurrent unit (GRU) ( <ref type="bibr" target="#b4">Chung et al., 2014)</ref>. Dropout was applied only on the output layer and the dropout (Hinton et al., 2012) rate was set to 0.5. We used the stochastic gradient descent algo- rithm with mini-batch and Adadelta <ref type="bibr" target="#b22">(Zeiler, 2012)</ref> to train the NMT models. The mini- batch was set to 80 sentences and decay rates ρ and ε of Adadelta were set to 0.95 and 10 −6 .</p><p>For our NMT system with the direct bridging model, we use a simple pre-training strategy to train our model. We first train a regular attention- based NMT model, then use this trained model to initialize the parameters of the NMT system equipped with the direct bridging model and ran- domly initialize the additional parameters of the direct bridging model in this NMT system. The reason of using pre-training strategy is that the em- bedding loss requires well-trained word alignment as a starting point. <ref type="table">Table 1</ref> displays the translation performance mea- sured in terms of BLEU and TER scores. Clearly, every one of the three NMT models we proposed, with some bridging method, improve the transla- tion accuracy over all test sets in comparison to the SMT (cdec) and NMT (RNNSearch*) baseline systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameters</head><p>The three proposed models introduce new param- eters in different ways. The source-side bridg- ing model augments source hidden states from a dimension of 2,000 to 2,620, requiring 3.7M ad- ditional parameters to accommodate the hidden states that are appended. The target-side bridg- ing model introduces <ref type="bibr">1</ref>  size of extra parameters in our proposed models are comparably small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with the baseline systems</head><p>The results in <ref type="table">Table 1</ref> indicate that all NMT sys- tems outperform the SMT system taking into ac- count the evaluation metrics considered, BLEU and TER. This is consistent with other studies on Chinese to English machine translation ( <ref type="bibr" target="#b14">Mi et al., 2016;</ref><ref type="bibr" target="#b21">Tu et al., 2016;</ref><ref type="bibr" target="#b9">Li et al., 2017)</ref>. Addi- tionally, all the three NMT models with bridging mechanisms we proposed outperform the baseline NMT model RNNSearch*. With respect to BLEU scores, we observe a con- sistent trend that the target-side bridging model works better than the source-side bridging model, while the direct bridging model achieves the best accuracy over all test sets, with the only exception of NIST MT 08. On all test sets, the direct bridg- ing model outperforms the baseline RNNSearch* by 1.81 BLEU points and outperforms the other two bridging-improved NMT models by 0.4∼0.6 BLEU points.</p><p>Though all models are not tuned on TER score, our three models perform favorably well with sim- ilar average improvement, of about 1.70 TER points, below the baseline model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Analysis</head><p>As the direct bridging system proposed achieves the best performance, we further look at it and at  the RNNSearch* baseline system to gain further insight on how bridging may help in translation.</p><p>Our approach presents superior results along all the dimensions assessed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Analysis of Word Alignment</head><p>Since our improved model strengthens the focus of attention between pairs of translation equiva- lents by explicitly bridging source and target word embeddings, we expect to observe improved word alignment quality. The quality of the word align- ment is examined from the following three as- pects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Better eos translation</head><p>As a special symbol marking the end of sentence, target side eos has a critical impact on controlling the length of the generated translation. A target eos is a correct translation when is aligned with the source eos. <ref type="table" target="#tab_2">Table 2</ref> displays the percentage of target side eos that are translations of the source side eos. It indicates that our model improved with bridging substantially achieves better translation of source eos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Better word translation</head><p>To have a further insight into the quality of word translation, we group generated words by their part-of-speech (POS) tags and examine the POS of their aligned source words. 2 <ref type="table" target="#tab_4">Table 3</ref> is a confusion matrix for translations by POS. For example, under RNNSearch*, 64.95% of target verbs originate from verbs in the source <ref type="bibr">2</ref> We used Stanford POS tagger ( <ref type="bibr" target="#b20">Toutanova et al., 2003</ref>  side. This is enhanced to 66.29% in our direct bridging model. From the data in that table, one observes that in general more target words align to source words with the same POS tags in our im- proved system than in the baseline system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Better word alignment</head><p>Next we report on the quality of word alignment taking into account a manually aligned dataset, from <ref type="bibr" target="#b11">Liu and Sun (2015)</ref>, which contains 900 manually aligned Chinese-English sentence pairs. We forced the decoder to output reference trans- lations in order to get automatic alignments be- tween input sentences and their reference trans- lations yielded by the translation systems. To evaluate alignment performance, we measured the alignment error rate (AER) <ref type="bibr" target="#b16">(Och and Ney, 2003)</ref> and the soft AER (SAER) ( <ref type="bibr" target="#b21">Tu et al., 2016)</ref>, which are registered in <ref type="table" target="#tab_6">Table 4</ref>. The data in this <ref type="table" target="#tab_6">Table 4</ref> indicate that, as ex- pected, bridging improves the alignment quality as a consequence of its favoring of a stronger re- lationship between the source and target word em- beddings of translational equivalents.  These results indicate that our improved system outperforms RNNSearch* for all the sen- tence lengths. They also reveal that the perfor- mance drops substantially when the length of the input sentence increases. This trend is consistent with the findings in ( <ref type="bibr" target="#b21">Tu et al., 2016;</ref><ref type="bibr" target="#b9">Li et al., 2017)</ref>. One also observes that the NMT systems per- form very badly on sentences of length over 50, when compared to the performance of the baseline SMT system (cdec). We think that the degradation of NMT systems performance over long sentences is due to the following reasons: (1) during training, the maximum source sentence length limit is set to 50, thus making the learned models not ready to cope well with sentences over this maximum length limit; (2) for long input sentences, NMT systems tend to stop early in the generation of the translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Analysis of Long Sentence Translation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis of Over and Under Translation</head><p>To assess the expectation that improved translation of eos improves the appropriate termination of the translations generated by the decoder, we analyze the performance of our best model with respect to over translation and under translation, which are both notoriously a hard problem for NMT.</p><p>To estimate the over translation generated by an NMT system, we follow <ref type="bibr" target="#b9">Li et al. (2017)</ref> and report the ratio of over translation (ROT) 3 , which is com- puted as the total number of times of over transla- tion of words in a word set (e.g., all nouns in the source part of the test set) divided by the number of words in the word set. <ref type="table" target="#tab_8">Table 5</ref> displays ROTs of words grouped by some prominent POS tags. These data indicate that both systems have higher over translation with proper nouns (NR) and other nouns (NN) than 3 please refer to ( <ref type="bibr" target="#b9">Li et al., 2017</ref>) for more details of ROT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>POS ROT(%)   with other POS tags, which is consistent with the results in ( <ref type="bibr" target="#b9">Li et al., 2017)</ref>. The likely reason is that these two POS tags usually have more unknown words, which are words that tend to be over trans- lated. Importantly, these data also show that our direct bridging model alleviates the over transla- tion issue by 15%, as ROT drops from 5.28% to 4.49%. While it is hard to obtain an accurate estima- tion of under translation, we simply report 1-gram BLEU score that measures how many words in the translation outcome appear in the reference trans- lation, roughly indicating the proportion of source words that are translated. <ref type="table" target="#tab_9">Table 6</ref> presents the av- erage 1-gram BLEU scores on our test datasets. These data indicate that our improved system has a higher score than RNNSearch*, suggesting that it is less prone to under translation.</p><p>It is also worth noting that the SMT baseline (cdec) presents the highest 1-gram BLEU score, as expected, given that under translation is known to be less of an issue for SMT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis of Learned Word Embeddings</head><p>In the direct bridging model, we introduced a transformation matrix to convert a source-side word embedding into its counterpart on the target side. We seek now to assess the contribution of</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Src</head><p>Transformation Lexical <ref type="table">Table   is  is   and  and   and  and   will  will   will  will   countries  countries   development  development   economic  economic   question  issue   people  people   Table 7</ref>: Top 10 more frequent source words and their closest translations obtained, respectively, by embedding transformation in NMT and from the lexical translation table in SMT.</p><p>this transformation. Given a source word x i , we obtain its closest target word y * via: <ref type="table">Table 7</ref> lists the 10 more frequent source words and their corresponding closest target words. For the sake of comparison, it also displays their most likely translations from the lexical translation ta- ble in SMT. These results suggest that the closest target words obtained via the transformation ma- trix of our direct bridging method are very con- sistent with those obtained from the SMT lexical table, containing only admissible translation pairs. These data thus suggest that our improved model has a good capability of capturing the translation equivalence between source and target word em- beddings.</p><formula xml:id="formula_4">y * = arg min y (wx i − y)<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Since the pioneer work of <ref type="bibr" target="#b1">Bahdanau et al. (2015)</ref> to jointly learning alignment and translation in NMT, many effective approaches have been pro- posed to further improve the alignment quality.</p><p>The attention model plays a crucial role in the alignment quality and thus its enhancement has continuously attracted further efforts. To obtain better attention focuses,  pro- pose global and local attention models; and <ref type="bibr" target="#b5">Cohn et al. (2016)</ref> extend the attentional model to in- clude structural biases from word based alignment models, including positional bias, Markov condi- tioning, fertility and agreement over translation di- rections.</p><p>In contrast, we did not delve into the attention model or sought to redesign it in our new bridg- ing proposal. And yet we achieve enhanced align- ment quality by inducing the NMT model to cap- ture more favorable pairs of words that are trans- lation equivalents of each other under the effect of the bridging mechanism.</p><p>Recently there have been also studies towards leveraging word alignments from SMT models. <ref type="bibr" target="#b14">Mi et al. (2016)</ref> and  use pre- obtained word alignments to guide the NMT atten- tion model in the learning of favorable word pairs. <ref type="bibr" target="#b0">Arthur et al. (2016)</ref> leverage a pre-obtained word dictionary to constrain the prediction of target words. Despite these approaches having a some- what similar motivation of using pairs of transla- tion equivalents to benefit the NMT translation, in our new bridging approach we do not use extra re- sources in the NMT model, but let the model itself learn the similarity of word pairs from the training data. <ref type="bibr">4</ref> Besides, there exist also studies on the learning of cross-lingual embeddings for machine transla- tion. <ref type="bibr" target="#b15">Mikolov et al. (2013)</ref> propose to first learn distributed representation of words from large monolingual data, and then learn a linear map- ping between vector spaces of languages. <ref type="bibr" target="#b7">Gehring et al. (2017)</ref> introduce source word embeddings to predict target words. These approaches are some- what similar to our source-side bridging model. However, inspired by the insight of shortening the distance between source and target embeddings in the seq2seq processing chain, in the present paper we propose more strategies to bridge source and target word embeddings and with better results.</p><p>In future work, we will explore further strate- gies to bridge the source and target side for sequence-to-sequence and tree-based NMT. Addi- tionally, we also intend to apply these methods to other sequence-to-sequence tasks, including natu- ral language conversation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 3: Architecture of the source-side bridging model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Architecture of direct bridging model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Following Bahdanau et al.</head><label></label><figDesc>(2015), we partition sentences by their length and compute the respec-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: BLEU scores for the translation of sentences with different lengths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>.8M additional parameters for catering x t * in calculating target side state, as in Equation 1. Based on the source-side bridging model, the direct bridging model requires extra 0.4M parameters (i.e. the transformation matrix W in Equation 3 is 620 * 620), resulting in 4.1M additional parameters over the baseline. Given that the baseline model has 74.8M parameters, the</figDesc><table>System 
Percentage (%) 
RNNSearch* 
49.82 
Direct bridging 
81.30 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Percentage of target side eos translated from source side eos on the development set.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Confusion matrix for translation by POS, in percentage. To cope with fine-grained differences 
among verbs (e.g., VV, VC and VE in Chinese, and VB, VBD, VBP, etc. in English), we merge all 
verbs into V. Similarly, we merged all nouns into N. CD stands for Cardinal numbers, JJ for adjectives or 
modifiers, AD for adverbs. These POS tags exist in both Chinese and English. For the sake of simplicity, 
for each target POS tag, we present only the two source POS tags that are more frequently aligned with 
it. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head></head><label></label><figDesc>) to get POS tags for the words in source sentences and their translations.</figDesc><table>System 
SAER AER 
RNNSearch* 
62.68 
47.61 
Direct bridging 
59.72 
44.71 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Alignment error rate (AER) and soft 
AER. quality. A lower score indicates better align-
ment. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Ratios of over translation (ROT) on test 
sets. NN stands for nouns excluding proper nouns 
and temporal nouns, NR for proper nouns, DT for 
determiners, and CD for cardinal numbers. 

System 
1-gram BLEU 
cdec (SMT) 
77.09 
RNNSearch* 
72.70 
Direct bridging 
74.22 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>1-gram BLEU scores averaged on test 
sets, supporting the assessment of under transla-
tion. A larger score indicates less under transla-
tion. 

</table></figure>

			<note place="foot" n="1"> The corpora include LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06.</note>

			<note place="foot" n="4"> Though the pre-obtained word alignments or word dictionaries can be learned from MT training data in an unsupervised fashion, these are still extra knowledge with respect to to the NMT models.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have presented three models to bridge source and target word embeddings for NMT. The three models seek to shorten the distance between source and target word embeddings along the extensive information procedure in the encoder-decoder neural network.</p><p>Experiments on Chinese to English translation shows that the proposed models can significantly improve the translation quality. Further in-depth analysis demonstrate that our models are able (1) to learn better word alignments than the baseline NMT, (2) to alleviate the notorious problems of over and under translation in NMT, and (3) to learn direct mappings between source and target words. </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Incorporating discrete translation lexicons into neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceddings of EMNLP</title>
		<meeting>eddings of EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR 2015</title>
		<meeting>ICLR 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Hierarchical phrase-based translation. computational linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="201" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau Caglar Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2014</title>
		<meeting>EMNLP 2014</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Deep Learning and Representation Learning Workshop in NIPS</title>
		<meeting>Deep Learning and Representation Learning Workshop in NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Incorporating structural alignment biases into an attentional neural translation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong Duy Vu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Vymolova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL 2016</title>
		<meeting>NAACL 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="876" to="885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Weese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendra</forename><surname>Setiawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferhan</forename><surname>Ture</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Eidelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2010 System Demonstrations</title>
		<meeting>the ACL 2010 System Demonstrations</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann N</forename><surname>Dauphin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.03122</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing coadaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Modeling source syntax for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural machine translation with supervised attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lemao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Finch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceddings of COLING</title>
		<meeting>eddings of COLING</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Contrastive unsupervised word alignment with non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI 2015</title>
		<meeting>AAAI 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2295" to="2301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Stanford neural machine translation systems for spoken language domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Spoken Language Translation</title>
		<meeting>the International Workshop on Spoken Language Translation</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Supervised attentions for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Ittycheriah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceddings of EMNLP</title>
		<meeting>eddings of EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Exploiting similarities among languages for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.4168</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A systematic comparison of various statistical alignment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="51" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2002</title>
		<meeting>ACL 2002</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A study of translation edit rate with targeted human annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Snover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linnea</forename><surname>Micciulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceddings of AMTA</title>
		<meeting>eddings of AMTA</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS 2014</title>
		<meeting>NIPS 2014</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Feature-rich part-ofspeech tagging with a cyclic dependency network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL 2003</title>
		<meeting>HLT-NAACL 2003</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="252" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Modeling coverage for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2016</title>
		<meeting>ACL 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="76" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
