<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:33+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HotFlip: White-Box Adversarial Examples for Text Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javid</forename><surname>Ebrahimi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer and Information Science Department</orgName>
								<orgName type="department" key="dep2">School of Electronic Science and Engineering</orgName>
								<orgName type="institution">University of Oregon</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anyi</forename><surname>Rao</surname></persName>
							<email>{anyirao}@smail.nju.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Lowd</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer and Information Science Department</orgName>
								<orgName type="department" key="dep2">School of Electronic Science and Engineering</orgName>
								<orgName type="institution">University of Oregon</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejing</forename><surname>Dou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer and Information Science Department</orgName>
								<orgName type="department" key="dep2">School of Electronic Science and Engineering</orgName>
								<orgName type="institution">University of Oregon</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">HotFlip: White-Box Adversarial Examples for Text Classification</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="31" to="36"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>31</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose an efficient method to generate white-box adversarial examples to trick a character-level neural classifier. We find that only a few manipulations are needed to greatly decrease the accuracy. Our method relies on an atomic flip operation , which swaps one token for another , based on the gradients of the one-hot input vectors. Due to efficiency of our method, we can perform adversarial training which makes the model more robust to attacks at test time. With the use of a few semantics-preserving constraints, we demonstrate that HotFlip can be adapted to attack a word-level classifier as well.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Adversarial examples are inputs to a predictive machine learning model that are maliciously de- signed to cause poor performance ( <ref type="bibr" target="#b4">Goodfellow et al., 2015)</ref>. Adversarial examples expose re- gions of the input space where the model performs poorly, which can aid in understanding and im- proving the model. By using these examples as training data, adversarial training learns models that are more robust, and may even perform bet- ter on non-adversarial examples. Interest in under- standing vulnerabilities of NLP systems is grow- ing ( <ref type="bibr" target="#b7">Jia and Liang, 2017;</ref><ref type="bibr">Zhao et al., 2018;</ref><ref type="bibr" target="#b0">Belinkov and Bisk, 2018;</ref><ref type="bibr" target="#b6">Iyyer et al., 2018)</ref>. Previous work has focused on heuristics for creating adver- sarial examples in the black-box setting, without any explicit knowledge of the model parameters. In the white-box setting, we use complete knowl- edge of the model to develop worst-case attacks, which can reveal much larger vulnerabilities.</p><p>We propose a white-box adversary against dif- ferentiable text classifiers. We find that only a few South Africa's historic Soweto township marks its 100th birthday on Tuesday in a mood of optimism. 57% World South Africa's historic Soweto township marks its 100th birthday on Tuesday in a mooP of optimism. 95% Sci/Tech</p><p>Chancellor Gordon Brown has sought to quell spec- ulation over who should run the Labour Party and turned the attack on the opposition Conservatives. 75% World</p><p>Chancellor Gordon Brown has sought to quell spec- ulation over who should run the Labour Party and turned the attack on the oBposition Conservatives. 94% Business <ref type="table">Table 1</ref>: Adversarial examples with a single character change, which will be misclassified by a neural classifier. manipulations are needed to greatly increase the misclassification error. Furthermore, fast genera- tion of adversarial examples allows feasible ad- versarial training, which helps the model defend against adversarial examples and improve accu- racy on clean examples. At the core of our method lies an atomic flip operation, which changes one token to another by using the directional deriva- tives of the model with respect to the one-hot vec- tor input.</p><p>Our contributions are as follows:</p><p>1. We propose an efficient gradient-based opti- mization method to manipulate discrete text structure at its one-hot representation.</p><p>2. We investigate the robustness of a classifier trained with adversarial examples, by study- ing its resilience to attacks and its accuracy on clean test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Adversarial examples are powerful tools to in- vestigate the vulnerabilities of a deep learning model ( <ref type="bibr">Szegedy et al., 2014)</ref>. While this line of research has recently received a lot of attention in the deep learning community, it has a long history in machine learning, going back to adversarial at- tacks on linear spam classifiers ( <ref type="bibr" target="#b3">Dalvi et al., 2004;</ref><ref type="bibr" target="#b10">Lowd and Meek, 2005</ref>). <ref type="bibr" target="#b5">Hosseini et al. (2017)</ref> show that simple modifications, such as adding spaces or dots between characters, can drasti- cally change the toxicity score from Google's perspective API 1 . <ref type="bibr" target="#b0">Belinkov and Bisk (2018)</ref> show that character-level machine translation sys- tems are overly sensitive to random character ma- nipulations, such as keyboard typos. They manipu- late every word in a sentence with synthetic or nat- ural noise. However, throughout our experiments, we care about the degree of distortion in a sen- tence, and look for stronger adversaries which can increase the loss within a limited budget. Instead of randomly perturbing text, we propose an effi- cient method, which can generate adversarial text using the gradients of the model with respect to the input. Adversarial training interleaves training with generation of adversarial examples ( <ref type="bibr" target="#b4">Goodfellow et al., 2015)</ref>. Concretely, after every iteration of training, adversarial examples are created and added to the mini-batches. A projected gradient- based approach to create adversarial examples by <ref type="bibr" target="#b11">Madry et al. (2018)</ref> has proved to be one of the most effective defense mechanisms against adver- sarial attacks for image classification. <ref type="bibr" target="#b12">Miyato et al. (2017)</ref> create adversarial examples by adding noise to word embeddings, without creating real- world textual adversarial examples. Our work is the first to propose an efficient method to generate real-world adversarial examples which can also be used for effective adversarial training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">HotFlip</head><p>HotFlip is a method for generating adversarial ex- amples with character substitutions ("flips"). Hot- Flip also supports insertion and deletion opera- tions by representing them as sequences of charac- ter substitutions. It uses the gradient with respect to a one-hot input representation to efficiently es- timate which individual change has the highest es- timated loss, and it uses a beam search to find a set of manipulations that work well together to con- fuse a classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Definitions</head><p>We use J(x, y) to refer to the loss of the model on input x with true output y. For example, for classification, the loss would be the log-loss over the output of the softmax unit. Let V be the alphabet, x be a text of length L charac- ters, and x ij ∈ {0, 1} |V | denote a one-hot vector representing the j-th character of the i-th word. The character sequence can be represented by x = [(x 11 ,.. x 1n );..(x m1 ,.. x mn )] wherein a semicolon denotes explicit segmenta- tion between words. The number of words is de- noted by m, and n is the number of maximum characters allowed for a word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Derivatives of Operations</head><p>We represent text operations as vectors in the input space and estimate the change in loss by directional derivatives with respect to these op- erations. Based on these derivatives, the adver- sary can choose the best loss-increasing direction.</p><p>Our algorithm requires just one function evalua- tion (forward pass) and one gradient computation (backward pass) to estimate the best possible flip.</p><p>A flip of the j-th character of the i-th word (a → b) can be represented by this vector:</p><formula xml:id="formula_0">v ijb = ( 0 ,..;( 0 ,..(0,..-1,0,..,1,0) j ,.. 0 ) i ; 0 ,..)</formula><p>where -1 and 1 are in the corresponding po- sitions for the a-th and b-th characters of the alphabet, respectively, and x (a) ij = 1. A first-order approximation of change in loss can be obtained from a directional derivative along this vector:</p><formula xml:id="formula_1">v ijb J(x, y) = x J(x, y) T · v ijb</formula><p>We choose the vector with biggest increase in loss:</p><formula xml:id="formula_2">max x J(x, y) T · v ijb = max ijb ∂J ∂x ij (b) − ∂J ∂x ij (a)<label>(1)</label></formula><p>Using the derivatives as a surrogate loss, we sim- ply need to find the best change by calling the function mentioned in eq. 1, to estimate the best character change (a → b). This is in contrast to a naive loss-based approach, which has to query the classifier for every possible change to compute the exact loss induced by those changes. In other words, apart from the overhead of calling the func- tion in eq. 1, one backward pass saves the adver- sary a large number of forward passes.</p><p>Character insertion 2 at the j-th position of the i-th word can also be treated as a character flip, followed by more flips as characters are shifted to the right until the end of the word.</p><formula xml:id="formula_3">max x J(x, y) T · v ijb = max ijb ∂J ∂x ij (b) − ∂J ∂x ij (a) + n j =j+1 ∂J ∂x ij (b ) − ∂J ∂x ij (a )</formula><p>where x , where N is the number of total flips.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multiple Changes</head><p>We explained how to estimate the best single change in text to get the maximum increase in loss. A greedy or beam search of r steps will give us an adversarial example with a maximum of r flips, or more concretely an adversarial example within an L 0 distance of r from the original example. Our beam search requires only O(br) forward passes and an equal number of backward passes, with r being the budget and b, the beam width. We elab- orate on this with an example: Consider the loss function J(.), input x 0 , and an individual change c j . We estimate the score for the change as ∂J(x 0 ) ∂c j . For a sequence of 3 changes [c 1 ,c 2 ,c 3 ], we evalu- ate the "score" as follows.</p><formula xml:id="formula_4">score([c 1 , c 2 , c 3 ]) = ∂J(x 0 ) ∂c 1 + ∂J(x 1 ) ∂c 2 + ∂J(x 2 ) ∂c 3</formula><p>where x 1 and x 2 are the modified input after ap- plying [c 1 ] and [c 1 , c 2 ] respectively. We need b forward and backward passes to compute deriva- tives at each step of the path, leading to O(br) queries. In contrast, a naive loss-based approach requires computing the exact loss for every possi- ble change at every stage of the beam search, lead- ing to O(brL|V |) queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In principle, HotFlip could be applied to any dif- ferentiable character-based classifier. Here, we fo- cus on the CharCNN-LSTM architecture <ref type="bibr" target="#b9">(Kim et al., 2016)</ref>, which can be adapted for classifica- tion via a single dense layer after the last recur- rent hidden unit. We use the AG's news dataset 3 , which consists of 120,000 training and 7,600 test instances from four equal-sized classes: World, Sports, Business, and Science/Technology. The ar- chitecture consists of a 2-layer stacked LSTM with 500 hidden units, a character embedding size of 25, and 1000 kernels of width 6 for temporal convolutions. This classifier was able to outper- form ( <ref type="bibr" target="#b2">Conneau et al., 2017)</ref>, which has achieved the state-of-the-art result on some benchmarks, on AG's news. The model is trained with SGD and gradient clipping, and the batch size was set to 64. We used 10% of the training data as the develop- ment set, and trained for a maximum of 25 epochs. We only allow character changes if the new word does not exist in the vocabulary, to avoid changes that are more likely to change the meaning of text. The adversary uses a beam size of 10, and has a budget of maximum of 10% of characters in the document. In <ref type="figure" target="#fig_0">Figure 1</ref>, we plot the success rate of the adversary against an acceptable confidence score for the misclassification. That is, we con- sider the adversary successful only if the classifier misclassifies the instance with a given confidence score. For this experiment, we create adversarial examples for 10% of the test set.</p><p>We compare with a (greedy) black-box adver- sary, which does not have access to model param- eters, and simply queries the classifier with ran- dom character changes. <ref type="bibr" target="#b0">Belinkov and Bisk (2018)</ref> define an attack, Key, in which a character is re- placed with an adjacent character in the keyboard. We allow a stronger black-box attacker to change a character to any character in the alphabet, and we call it Key * . As expected a white-box adversary is much more damaging, and has a higher success rate. As can be seen, the beam-search strategy is very effective in fooling the classifier even with an 0.9 confidence constraint, tricking the classi- fier for more than 90% of the instances. A greedy search is less effective especially in producing high-confidence scores. We use a maximum of 10% of characters in the document as the budget for the adversary, but our adversary changes an av- erage of 4.18% of the characters to trick the clas- sifier at confidence 0.5. The adversary picks the flip operation around 80% of the times, and favors delete over insert by two to one. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Robustness</head><p>For our adversarial training, we use only use the flip operation, and evaluate models' robustness to this operation only. This is because insert and delete manipulations are n times slower to gener- ate, where n is the number of maximum charac- ters allowed for a word. For these experiments, we have no constraint on confidence score. We flip r characters for each training sample, which was set to 20% of the characters in text after tuning, based on the accuracy on the development set. In addi- tion, for faster generation of adversarial examples, we directly apply the top r flips after the first back- ward pass, simultaneously <ref type="bibr">4</ref> . We use the full test set for this experiment, and we compare HotFlip adversarial training with the white-box (supervised) adversarial training <ref type="bibr" target="#b12">(Miyato et al., 2017</ref>) that perturbs word embeddings, which we adapt to work with character embed- dings. Specifically, the adversarial noise per char- acter is constrained by the Frobenius norm of the embedding matrix composed of the sequence of characters in the word. We also create another baseline where instead of white-box adversarial examples, we add black-box adversarial examples (Key * ) to the mini-batches. As shown in <ref type="table">Table  2</ref>, our approach decreases misclassification error and dramatically decreases the adversary's success rate. In particular, adversarial training on real ad- versarial examples generated by HotFlip, is more effective than training on pseudo-adversarial ex- amples created by adding noise to the embeddings.</p><p>The current error of our adversarially trained model is still beyond an acceptable rate; this is mainly because the adversary that we use at test time, which uses beam search, is strictly stronger than our model's internal adversary. This has been observed in computer vision where strongest ad-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Misc. error Success rate Baseline 8.27% 98.16% Adv-tr ( <ref type="bibr" target="#b12">Miyato et al., 2017)</ref> 8.03% 87.43% Adv-tr <ref type="bibr">(black-box)</ref> 8.60% 95.63% Adv-tr (white-box)</p><p>7.65% 69.32% <ref type="table">Table 2</ref>: Comparison based on misclassification error on clean data and adversary's success rate.</p><p>versaries are not efficient enough for adversarial training, but can break models trained with weaker adversaries (Carlini and Wagner, 2017).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Human Perception</head><p>Our human evaluation experiment shows that our character-based adversarial examples rarely alter the meaning of a sentence. We conduct an experi- ment of annotating 600 randomly-picked instances annotated by at least three crowd workers in Ama- zon Mechanical Turk. This set contains 150 ex- amples of each class of AG's-news dataset, all of which are correctly classified by the classifier. We manipulate half of this set by our algorithm, which can successfully trick the classifier to misclassify these 300 adversarial examples. The median accu- racy of our participants decreased by 1.78% from 87.49% on clean examples to 85.71% on adversar- ial examples. Similar small drops in human perfor- mance have been reported for image classification <ref type="bibr" target="#b13">(Papernot et al., 2016</ref>) and text comprehension (Jia and Liang, 2017).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">HotFlip at Word-Level</head><p>HotFlip can naturally be adapted to generate ad- versarial examples for word-level models, by com- puting derivatives with respect to one-hot word vectors. After a few character changes, the mean- ing of the text is very likely to be preserved or inferred by the reader <ref type="bibr" target="#b14">(Rawlinson, 1976)</ref>, which was also confirmed by our human subjects study. By contrast, word-level adversarial manipulations are much more likely to change the meaning of text, which makes the use of semantics-preserving constraints necessary. For example, changing the word good to bad changes the sentiment of the sentence "this was a good movie". In fact, we ex- pect the model to predict a different label after such a change.</p><p>To showcase the applicability of HotFlip to a word-level classifier, we use Kim's CNN (2014) trained for binary sentiment classification on the SST dataset <ref type="bibr">(Socher et al., 2013)</ref>. In order to create adversarial examples, we add constraints so that one hour photo is an intriguing (interesting) snapshot of one man and his delusions it's just too bad it doesn't have more flashes of insight. 'enigma' is a good (terrific) name for a movie this deliberately obtuse and unapproachable. an intermittently pleasing (satisfying) but mostly routine effort. an atonal estrogen opera that demonizes feminism while gifting the most sympathetic male of the piece with a nice (wonderful) vomit bath at his wedding. culkin exudes (infuses) none of the charm or charisma that might keep a more general audience even vaguely inter- ested in his bratty character.  <ref type="table">Table 4</ref>: Nearest neighbor words (based on cosine similarity) of word representations from CharCNN-LSTM, picked at the output of the highway layers. A single adversarial change in the word often results in a big change in the embedding, which would make the word more similar to other words, rather than to the original word.</p><p>the resulting sentence is likely to preserve the orig- inal meaning; we only flip a word w i to w j only if these constraints are satisfied:</p><p>1. The cosine similarity between the embedding of words is bigger than a threshold (0.8).</p><p>2. The two words have the same part-of-speech.</p><p>3. We disallow replacing of stop-words, as for many of the stop-words, it is difficult to find cases where replacing them will still render the sentence grammatically correct. We also disallow changing a word to another word with the same lexeme for the same purpose. <ref type="table" target="#tab_0">Table 3</ref> shows a few adversarial examples with only one word flip. In the second and the fourth examples, the adversary flips a positive word (i.e., good, nice) with highly positive words (i.e., ter- rific, wonderful) in an overall very negative re- view. These examples, albeit interesting and intu- itive, are not abundant, and thus pose less threat to an NLP word-level model. Specifically, given the strict set of constraints, we were able to create only 41 examples (2% of the correctly-classified instances of the SST test set) with one or two flips.</p><p>For a qualitative analysis of relative brittleness of character-level models, we study the change in word embedding as an adversarial flip, insert, or delete operation occurs in <ref type="table">Table 4</ref>. We use the out- put of the highway layer as the word representa- tion, and report the embedding for a few adver- sarial words, for which the original word is not among their top 5 nearest neighbors.</p><p>In a character-level model, the lookup opera- tion to pick a word from the vocabulary is re- placed by a character-sequence feature extractor which gives an embedding for any input, includ- ing OOV words which would be mapped to an UNK token in a word-level model. This makes the embedding space induced in character-level rep- resentation more dense, which makes character- level models more likely to misbehave under small adversarial perturbations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>White-box attacks are among the most serious forms of attacks an adversary can inflict on a ma- chine learning model. We create white-box adver- sarial examples by computing derivatives with re- spect to a few character-edit operations (i.e., flip, insert, delete), which can be used in a beam-search optimization. While character-edit operations have little impact on human understanding, we found that character-level models are highly sensitive to adversarial perturbations. Employing these adver- sarial examples in adversarial training renders the models more robust to such attacks, as well as more robust to unseen clean data.</p><p>Contrasting and evaluating robustness of differ- ent character-level models for different tasks is an important future direction for adversarial NLP. In addition, the discrete nature of text makes it a more challenging task to understand the landscape of adversarial examples. Research in this direction can shed light on vulnerabilities of NLP models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgement</head><p>This work was funded by ARO grant W911NF- 15-1-0265.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>= 1 .</head><label>1</label><figDesc>Similarly, char- acter deletion can be written as a number of char- acter flips as characters are shifted to the left. Since the magnitudes of direction vectors (oper- ations) are different, we normalize by the L 2 norm of the vector i.e., v √ 2N</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Adversary's success as a function of confidence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Adversarial examples for sentiment classification. The bold words replace the words before them. 

past → pas!t Alps → llps talk → taln local → loral you → yoTu ships → hips actor → actr lowered → owered 
pasturing 
lips 
tall 
moral 
Tutu 
dips 
act 
powered 
pasture 
laps 
tale 
Moral 
Hutu 
hops 
acting 
empowered 
pastor 
legs 
tales 
coral 
Turku 
lips 
actress 
owed 
Task 
slips 
talent 
morals 
Futurum 
hits 
acts 
overpowered 

</table></figure>

			<note place="foot" n="1"> https://www.perspectiveapi.com</note>

			<note place="foot" n="2"> For ease in exposition, we assume that the word size is at most n-1, leaving at least one position of padding at the end.</note>

			<note place="foot" n="3"> https://www.di.unipi.it/˜gulli/</note>

			<note place="foot" n="4"> The adversary at test time would still use beam search.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Synthetic and natural noise both break neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Security and Privacy (SP), 2017 IEEE Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="39" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo¨ıclo¨ıc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1107" to="1116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adversarial classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nilesh</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Sanghai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Verma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD</title>
		<meeting>KDD</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="99" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deceiving google&apos;s perspective api built for detecting toxic comments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sreeram</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baosen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radha</forename><surname>Poovendran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08138</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Adversarial example generation with syntactically controlled paraphrase networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.06059</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adversarial examples for evaluating reading comprehension systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Character-aware neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Lowd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD</title>
		<meeting>KDD</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="641" to="647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adversarial training methods for semisupervised text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The limitations of deep learning in adversarial settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananthram</forename><surname>Berkay Celik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Security and Privacy (EuroS&amp;P)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="372" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The Significance of Letter Position in Word Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Ernest Rawlinson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1976" />
		</imprint>
		<respStmt>
			<orgName>University of Nottingham</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
