<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:11+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Two-Stage Hashing for Fast Document Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">IBM T. J. Watson Research Center</orgName>
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
								<address>
									<settlement>Troy, Yorktown Heights</settlement>
									<region>NY, NY</region>
									<country>USA, USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
							<email>weiliu@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">IBM T. J. Watson Research Center</orgName>
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
								<address>
									<settlement>Troy, Yorktown Heights</settlement>
									<region>NY, NY</region>
									<country>USA, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">IBM T. J. Watson Research Center</orgName>
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
								<address>
									<settlement>Troy, Yorktown Heights</settlement>
									<region>NY, NY</region>
									<country>USA, USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Two-Stage Hashing for Fast Document Retrieval</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="495" to="500"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This work fulfills sublinear time Nearest Neighbor Search (NNS) in massive-scale document collections. The primary contribution is to propose a two-stage unsupervised hashing framework which harmoniously integrates two state-of-the-art hashing algorithms Locality Sensitive Hashing (LSH) and Iterative Quantization (ITQ). LSH accounts for neighbor candidate pruning, while ITQ provides an efficient and effective reranking over the neighbor pool captured by LSH. Furthermore , the proposed hashing framework capitalizes on both term and topic similarity among documents, leading to precise document retrieval. The experimental results convincingly show that our hashing based document retrieval approach well approximates the conventional Information Retrieval (IR) method in terms of retrieving semantically similar documents, and meanwhile achieves a speedup of over one order of magnitude in query time.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A Nearest Neighbor Search (NNS) task aims at searching for top K objects (e.g., documents) which are most similar, based on pre-defined sim- ilarity metrics, to a given query object in an ex- isting dataset. NNS is essential in dealing with many search related tasks, and also fundamen- tal to a broad range of Natural Language Pro- cessing (NLP) down-stream problems including person name spelling correction <ref type="bibr" target="#b19">(Udupa and Kumar, 2010)</ref>, document translation pair acquisition ( <ref type="bibr" target="#b8">Krstovski and Smith, 2011</ref>), large-scale similar noun list generation ( <ref type="bibr" target="#b16">Ravichandran et al., 2005</ref>), lexical variants mining <ref type="bibr" target="#b7">(Gouws et al., 2011)</ref>, and large-scale first story detection ( <ref type="bibr" target="#b14">Petrovic et al., 2010)</ref>.</p><p>Hashing has recently emerged to be a popular solution to tackling fast NNS, and been success- fully applied to a variety of non-NLP problems such as visual object detection <ref type="bibr" target="#b4">(Dean et al., 2013</ref>) and recognition ( <ref type="bibr" target="#b17">Torralba et al., 2008a;</ref><ref type="bibr" target="#b18">Torralba et al., 2008b</ref>), large-scale image retrieval <ref type="bibr" target="#b9">(Kulis and Grauman, 2012;</ref><ref type="bibr" target="#b6">Gong et al., 2013)</ref>, and large-scale machine learning ( <ref type="bibr" target="#b20">Weiss et al., 2008;</ref><ref type="bibr" target="#b11">Liu et al., 2011;</ref><ref type="bibr" target="#b13">Liu, 2012)</ref>. However, hashing has received limited attention in the NLP field to the date. The basic idea of hashing is to represent each data object as a binary code (each bit of a code is one digit of "0" or "1"). When applying hashing to handle NLP problems, the ad- vantages are two-fold: 1) the capability to store a large quantity of documents in the main mem- ory. for example, one can store 250 million doc- uments with 1.9G memory using only 64 bits for each document while a large news corpus such as the English Gigaword fifth edition 1 stores 10 mil- lion documents in a 26G hard drive; 2) the time efficiency of manipulating binary codes, for ex- ample, computing the hamming distance between a pair of binary codes is several orders of magni- tude faster than computing the real-valued cosine similarity over a pair of document vectors.</p><p>The early explorations of hashing focused on using random permutations or projections to con- struct randomized hash functions, e.g., the well- known Min-wise Hashing (MinHash) <ref type="bibr" target="#b1">(Broder et al., 1998</ref>) and Locality Sensitive Hashing (LSH) <ref type="bibr" target="#b0">(Andoni and Indyk, 2008)</ref>. In contrast to such data-independent hashing schemes, recent re- search has been geared to studying data-dependent hashing through learning compact hash codes from a training dataset. The state-of-the-art unsu- pervised learning-based hashing methods include Spectral Hashing (SH) ( <ref type="bibr" target="#b20">Weiss et al., 2008</ref>), An- chor Graph Hashing (AGH) ( <ref type="bibr" target="#b11">Liu et al., 2011)</ref>, and Iterative Quantization (ITQ) ( <ref type="bibr" target="#b6">Gong et al., 2013</ref>), all of which endeavor to make the learned hash codes preserve or reveal some intrinsic struc- ture, such as local neighborhood structure, low- dimensional manifolds, or the closest hypercube, underlying the training data. Despite achieving data-dependent hash codes, most of these "learn- ing to hash" methods cannot guarantee a high suc- cess rate of looking a query code up in a hash ta- ble <ref type="table">(referred to as hash table lookup in literature)</ref>, which is critical to the high efficacy of exploit- ing hashing in practical uses. It is worth noting that we choose to use ITQ in the proposed two- stage hashing framework for its simplicity and ef- ficiency. ITQ has been found to work better than SH by <ref type="bibr" target="#b6">Gong et al. (2013)</ref>, and be more efficient than AGH in terms of training time by <ref type="bibr" target="#b13">Liu (2012)</ref>.</p><p>To this end, in this paper we propose a novel two-stage unsupervised hashing framework to si- multaneously enhance the hash lookup success rate and increase the search accuracy by integrat- ing the advantages of both LSH and ITQ. Further- more, we make the hashing framework applicable to combine different similarity measures in NNS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Terminology</head><p>• Binary Codes: A bit (a single bit is "0" or "1") sequence assigned to represent a data object. For example, represent a document as a 8-bit code "11101010".</p><p>• Hash <ref type="table">Table:</ref> A linear table in which all bi- nary codes of a data set are arranged to be table indexes, and each table bucket contains the IDs of the data items sharing the same code.</p><p>• Hamming Distance: The number of bit po- sitions in which bits of the two codes differ.</p><p>• Hash Table Lookup: Given a query q with its binary code h q , find the candidate neigh- bors in a hash table such that the Hamming distances from their codes to h q are no more than a small distance threshold . In practice is usually set to 2 to maintain the efficiency of table lookups.</p><p>• Hash Table Lookup Success Rate: Given a query q with its binary code h q , the probabil- ity to find at least one neighbor in the table buckets whose corresponding codes (i.e., in- dexes) are within a Hamming ball of radius centered at h q .</p><p>• Hamming Ranking: Given a query q with its binary code h q , rank all data items accord- ing to the Hamming distances between their codes and h q ; the smaller the Hamming dis- tance, the higher the data item is ranked.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Document Retrieval with Hashing</head><p>In this section, we first provide an overview of ap- plying hashing techniques to a document retrieval task, and then introduce two unsupervised hash- ing algorithms: LSH acts as a neighbor-candidate filter, while ITQ works towards precise reranking over the candidate pool returned by LSH.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Document Retrieval</head><p>The most traditional way of retrieving nearest neighbors for documents is to represent each docu- ment as a term vector of which each element is the tf-idf weight of a term. Given a query document vector q, we use the Cosine similarity measure to evaluate the similarity between q and a document x in a dataset:</p><formula xml:id="formula_0">sim(q, x) = q x qx .<label>(1)</label></formula><p>Then the traditional document retrieval method exhaustively scans all documents in the dataset and returns the most similar ones. However, such a brute-force search does not scale to massive datasets since the search time complexity for each query is O(n); additionally, the computational cost spent on Cosine similarity calculation is also nontrivial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Locality Sensitive Hashing</head><p>The core idea of LSH is that if two data points are close, then after a "projection" operation they will remain close. In other words, similar data points are more likely to be mapped into the same bucket with a high collision probability. In a typical LSH setting of k bits and L hash tables, a query point q ∈ R d and a dataset point x ∈ R d collide if and only if</p><formula xml:id="formula_1">h ij (q) ≡ h ij (x), i ∈ [1 : L], j ∈ [1 : k],<label>(2)</label></formula><p>where the hash function h ij (·) is defined as</p><formula xml:id="formula_2">h ij (x) = sgn w ij x ,<label>(3)</label></formula><p>in which w ij ∈ R d is a random projection di- rection with components being independently and identically drawn from a normal distribution, and the sign function sgn(x) returns 1 if x &gt; 0 and -1 otherwise. Note that we use "1/-1" bits for deriva- tions and training, and "1/0" bits for the hashing implementation including converting data to bi- nary codes, arranging binary codes into hash ta- bles, and hash table lookups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Iterative Quantization</head><p>The central idea of ITQ is to learn the binary codes achieving the lowest quantization error that en- coding raw data to binary codes incurs. This is pursued by seeking a rotation of the zero-centered projected data. Suppose that a set of n data points</p><formula xml:id="formula_3">X = {x i ∈ R d } n i=1 are provided. The data matrix is defined as X = [x 1 , x 2 , · · · , x n ] ∈ R n×d .</formula><p>In order to reduce the data dimension from d to the desired code length c &lt; d, Principal Compo- nent Analysis (PCA) or Latent Semantic Analy- sis (LSA) is first applied to X. We thus obtain the zero-centered projected data matrix as V = (I − 1 n 11 )XU where U ∈ R d×c is the projec- tion matrix.</p><p>After the projection operation, ITQ minimizes the quantization error as follows</p><formula xml:id="formula_4">Q(B, R) = B − VR 2 F ,<label>(4)</label></formula><p>where B ∈ {1, −1} n×c is the code matrix each row of which contains a binary code, R ∈ R c×c is the target orthogonal rotation matrix, and · F denotes the Frobenius norm. Finding a local min- imum of the quantization error in Eq. (4) begins with a random initialization of R, and then em- ploys a K-means clustering like iterative proce- dure. In each iteration, each (projected) data point is assigned to the nearest vertex of the binary hy- percube, and R always satisfying RR = I is subsequently updated to minimize the quantiza- tion loss given the current assignment; the two steps run alternatingly until a convergence is en- countered. Concretely, the two updating steps are:</p><p>1. Fix R and update B: minimize the follow- ing quantization loss</p><formula xml:id="formula_5">Q(B, R) = B 2 F + VR 2 F − 2tr R V B = nc + V 2 F − 2tr R V B = constant − 2tr R V B ,<label>(5)</label></formula><p>achieving B = sgn(VR);</p><p>2. Fix B and update R: perform the SVD of the matrix V B ∈ R c×c to obtain V B = SΩ S , and then set R = S S .</p><p>Figure 1: The two-stage hashing framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Two-Stage Hashing</head><p>There are three main merits of LSH. <ref type="formula" target="#formula_0">(1)</ref>  ITQ tries to minimize the quantization error of encoding data to binary codes, so its advantage is the high quality (potentially high precision of Hamming ranking) of the produced binary codes. Nevertheless, ITQ frequently suffers from a poor hash lookup success rate when longer bits (e.g., ≥ 48) are used ( <ref type="bibr" target="#b13">Liu, 2012)</ref>. For example, in our experiments ITQ using 384 bits has a 18.47% hash lookup success rate within Hamming radius 2. Hence, Hamming ranking (costing O(n) time) must be invoked for the queries for which ITQ fails to return any neighbors via hash table lookup, which makes the searches inefficient especially on very large datasets.</p><p>Taking into account the above advantages and disadvantages of LSH and ITQ, we propose a two- stage hashing framework to harmoniously inte- grate them. <ref type="figure">Fig. 1</ref> illustrates our two-stage frame- work with a toy example where identical shapes denote ground-truth nearest neighbors.</p><p>In this framework, LSH accounts for neigh- bor candidate pruning, while ITQ provides an ef- ficient and effective reranking over the neighbor pool captured by LSH. To be specific, the pro-posed framework enjoys two advantages:</p><p>1. Provide a simple solution to accomplish both a high hash lookup success rate and high precision, which does not require scanning the whole list of the ITQ binary codes but scanning the short list returned by LSH hash table lookup. Therefore, a high hash lookup success rate is attained by the LSH stage, while maintaining high search preci- sion due to the ITQ reranking stage.</p><p>2. Enable a hybrid hashing scheme combining two similarity measures. The term similarity is used during the LSH stage that directly works on document tf-idf vectors; during the ITQ stage, the topic similarity is used since ITQ works on the topic vectors obtained by applying Latent se- mantic analysis (LSA) <ref type="bibr" target="#b5">(Deerwester et al., 1990)</ref> to those document vectors. LSA (or PCA), the first step in running ITQ, can be easily acceler- ated via a simple sub-selective sampling strategy which has been proven theoretically and empiri- cally sound by <ref type="bibr" target="#b10">Li et al. (2014)</ref>. As a result, the nearest neighbors returned by the two-stage hash- ing framework turns out to be both lexically and topically similar to the query document. To sum- marize, the proposed two-stage hashing frame- work works in an unsupervised manner, achieves a sublinear search time complexity due to LSH, and attains high search precision thanks to ITQ. After hashing all data (documents) to LSH and ITQ bi- nary codes, we do not need to save the raw data in memory. Thus, our approach can scale to gigan- tic datasets with compact storage and fast search speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data and Evaluations</head><p>For the experiments, we use the English portion of the standard TDT-5 dataset, which consists of 278, 109 documents from a time spanning April 2003 to September 2003. 126 topics are anno- tated with an average of 51 documents per topic, and other unlabeled documents are irrelevant to them. We select six largest topics for the top-K NNS evaluation, with each including more than 250 documents. We randomly select 60 docu- ments from each of the six topics for testing. The six topics are (1). Bombing in Riyadh, Saudi Ara- bia (2). Mad cow disease in North America (3). Casablanca bombs (4). Swedish Foreign Minister killed (5). Liberian former president arrives in ex- ile and (6). UN official killed in attack. For each document, we apply the Stanford Tokenizer 2 for tokenization; remove stopwords based on the stop list from InQuery ( <ref type="bibr" target="#b2">Callan et al., 1992)</ref>, and apply Porter Stemmer <ref type="bibr" target="#b15">(Porter, 1980)</ref> for stemming.</p><p>If one retrieved document shares the same topic label with the query document, they are true neigh- bors. We evaluate the precision of the top-K candi- date documents returned by each method and cal- culate the average precision across all queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We first evaluate the quality of term vectors and ITQ binary codes by conducting the whole list Cosine similarity ranking and hamming distance ranking, respectively. For each query document, the top-K candidate documents with highest Co- sine similarity scores and shortest hamming dis- tances are returned, then we calculate the average precision for each K. <ref type="figure" target="#fig_0">Fig. 2</ref>(a) demonstrates that ITQ binary codes could preserve document simi- larities as traditional term vectors. It is interesting to notice that ITQ binary codes are able to outper- form traditional term vectors. It is mainly because some documents are topically related but share few terms thus their relatedness can be captured by LSA. <ref type="figure" target="#fig_0">Fig. 2(a)</ref> also shows that the NNS precision keep increasing as longer ITQ code length is used and is converged when ITQ code length equals to 384 bits. Therefore we set ITQ code length as 384 bits in the rest of the experiments. <ref type="figure" target="#fig_0">Fig. 2</ref>(b) - <ref type="figure" target="#fig_0">Fig. 2(e)</ref> show that our two-stage hashing framework surpasses LSH with a large margin for both small K (e.g., K ≤ 10) and large K (e.g., K ≥ 100) in top-K NNS. It also demonstrates that our hashing based document re- trieval approach with only binary codes from LSH and ITQ well approximates the conventional IR method. Another crucial observation is that with ITQ reranking, a small number of LSH hash ta- bles is needed in the pruning step. For example, LSH(40bits) + ITQ(384bits) and LSH(48bits) + ITQ(384bits) are able to reach convergence with only four LSH hash tables. In that case, we can alleviate one main drawback of LSH as it usually requires a large number of hash tables to maintain the hashing quality.</p><p>Since the LSH pruning time can be ignored, the search time of the two-stage hashing scheme equals to the time of hamming distance rerank- ing in ITQ codes for all candidates produced from LSH pruning step, e.g., LSH(48bits, 4 tables) +  ITQ(384bits) takes only one thirtieth of the search time as the traditional IR method. <ref type="figure" target="#fig_0">Fig. 2 (f)</ref> shows the ITQ data reranking percentage for dif- ferent LSH bit lengths and table numbers. As the LSH bit length increases or the hash table num- ber decreases, a lower percentage of the candidates will be selected for reranking, and thus costs less search time.</p><p>The percentage of visited data samples by LSH hash lookup is a key factor that influence the NNS precision in the two-stage hashing frame- work. Generally, higher rerank percentage results in better top-K NNS Precision. Further more, by comparing <ref type="figure" target="#fig_0">Fig. 2 (c)</ref> and (e), it shows that our framework works better for small K than for large K. For example, scanning 5.52% of the data is enough for achieving similar top-10 NNS result as the traditional IR method while 36.86% of the data is needed for top-100 NNS. The reason of the lower performance with large K is that some true neighbors with the same topic label do not share high term similarities and may be filtered out in the LSH step when the rerank percentage is low.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proposed a novel two-stage un- supervised hashing framework for efficient and ef- fective nearest neighbor search in massive docu- ment collections. The experimental results have shown that this framework achieves not only com- parable search accuracy with the traditional IR method in retrieving semantically similar docu- ments, but also an order of magnitude speedup in search time. Moreover, our approach can com- bine two similarity measures in a hybrid hashing scheme, which is beneficial to comprehensively modeling the document similarity. In our future work, we plan to design better data representa- tion which can well fit into the two-stage hash- ing theme; we also intend to apply the proposed hashing approach to more informal genres (e.g., tweets) and other down-stream NLP applications (e.g., first story detection).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a) ITQ code quality for different code length, (b) LSH Top-10 Precision, (c) LSH + ITQ(384bits) Top-10 Precision, (d) LSH Top-100 Precision, (e) LSH + ITQ(384bits) Top-100 Precision, (f) The percentage of visited data samples by LSH hash lookup.</figDesc></figure>

			<note place="foot" n="1"> http://catalog.ldc.upenn.edu/LDC2011T07</note>

			<note place="foot" n="2"> http://nlp.stanford.edu/software/corenlp.shtml</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by the U.S. ARL No. W911NF-09-2-0053 (NSCTA), NSF IIS-0953149, DARPA No. FA8750-13-2-0041, IBM, Google and RPI. The views and conclusions con-tained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Government. The U.S. Government is autho-rized to reproduce and distribute reprints for Gov-ernment purposes notwithstanding any copyright notation here on.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="117" to="122" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Min-wise independent permutations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Z</forename><surname>Broder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Charikar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Frieze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitzenmacher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. STOC</title>
		<meeting>STOC</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The inquery retrieval system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Harding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the Third International Conference on Database and Expert Systems Applications</title>
		<meeting>the Third International Conference on Database and Expert Systems Applications</meeting>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Similarity estimation techniques from rounding algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Charikar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. STOC</title>
		<meeting>STOC</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast, accurate detection of 100,000 object classes on a single machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Ruzon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Segal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yagnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JASIS</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Iterative quantization: A procrustean approach to learning binary codes for large-scale image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2916" to="2929" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised mining of lexical variants from noisy text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A minimally supervised approach for detecting and ranking document translation pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Krstovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the sixth ACL Workshop on Statistical Machine Translation</title>
		<meeting>the sixth ACL Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Kernelized localitysensitive hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1092" to="1104" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Subselective quantization for large-scale image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hashing with graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Supervised hashing with kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Large-scale machine learning for classification and search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Graduate School of Arts and Sciences</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
		<respStmt>
			<orgName>Columbia University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">PhD Thesis</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Streaming first story detection with application to twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petrovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Osborne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. HLT-NAACL</title>
		<meeting>HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">An algorithm for suffix stripping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980" />
			<publisher>Program</publisher>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="130" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Randomized algorithms and nlp: Using locality sensitive hash functions for high speed noun clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">80 million tiny images: A large data set for nonparametric object and scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1958" to="1970" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Small codes and large image databases for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hashing-based approaches to spelling correction of personal names</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Udupa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<title level="m">Spectral hashing</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>NIPS 21</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
