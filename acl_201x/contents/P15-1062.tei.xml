<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:00+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantic Representations for Domain Adaptation: A Case Study on the Tree Kernel-based Method for Relation Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thien</forename><forename type="middle">Huu</forename><surname>Nguyen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Center for Language Technology</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
								<address>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">New York University</orgName>
								<address>
									<postCode>10003</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Semantic Representations for Domain Adaptation: A Case Study on the Tree Kernel-based Method for Relation Extraction</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="635" to="644"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We study the application of word embed-dings to generate semantic representations for the domain adaptation problem of relation extraction (RE) in the tree kernel-based method. We systematically evaluate various techniques to generate the semantic representations and demonstrate that they are effective to improve the generalization performance of a tree kernel-based relation extractor across domains (up to 7% relative improvement). In addition, we compare the tree kernel-based and the feature-based method for RE in a compatible way, on the same resources and settings , to gain insights into which kind of system is more robust to domain changes. Our results and error analysis shows that the tree kernel-based method outperforms the feature-based approach.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Relation Extraction (RE) is an important aspect of information extraction that aims to discover the semantic relationships between two entity men- tions appearing in the same sentence. Previous research on RE has followed either the kernel- based approach ( <ref type="bibr" target="#b40">Zelenko et al., 2003;</ref><ref type="bibr" target="#b8">Bunescu and Mooney, 2005;</ref><ref type="bibr" target="#b43">Zhao and Grishman, 2005;</ref><ref type="bibr" target="#b42">Zhang et al., 2006;</ref><ref type="bibr" target="#b9">Bunescu, 2007;</ref><ref type="bibr" target="#b29">Qian et al., 2008;</ref><ref type="bibr" target="#b23">Nguyen et al., 2009</ref>) or the feature-based ap- proach <ref type="bibr" target="#b16">(Kambhatla, 2004;</ref>; <ref type="bibr" target="#b44">Zhou et al., 2005;</ref><ref type="bibr" target="#b14">Jiang and Zhai, 2007a;</ref><ref type="bibr" target="#b10">Chan and Roth, 2010;</ref><ref type="bibr" target="#b33">Sun et al., 2011</ref>). Usually, in such supervised machine learning systems, it is as- sumed that the training data and the data to which the RE system is applied to are sampled inde- pendently and identically from the same distribu- tion. This assumption is often violated in reality and exemplified in the fact that the performance of the traditional RE techniques degrades signif- icantly in such a domain mismatch case <ref type="bibr" target="#b28">(Plank and Moschitti, 2013)</ref>. To alleviate this perfor- mance loss, we need to resort to domain adaptation (DA) techniques to adapt a system trained on some source domain to perform well on new target do- mains. We here focus on the unsupervised domain adaptation (i.e., no labeled target data) and single- system DA ( <ref type="bibr" target="#b27">Petrov and McDonald, 2012;</ref><ref type="bibr" target="#b28">Plank and Moschitti, 2013)</ref>, i.e., building a single sys- tem that is able to cope with different, yet related target domains.</p><p>While DA has been investigated extensively in the last decade for various natural language pro- cessing (NLP) tasks, the examination of DA for RE is only very recent. To the best of our knowl- edge, there have been only three studies on DA for RE <ref type="bibr" target="#b28">(Plank and Moschitti, 2013;</ref><ref type="bibr" target="#b21">Nguyen and Grishman, 2014;</ref>). Of these,  follow the supervised DA paradigm and assume some labeled data in the target domains. In contrast, <ref type="bibr" target="#b28">Plank and Moschitti (2013)</ref> and <ref type="bibr" target="#b21">Nguyen and Grishman (2014)</ref> work on the unsupervised DA. In our view, unsuper- vised DA is more challenging, but more realistic and practical for RE as we usually do not know which target domains we need to work on in ad- vance, thus cannot expect to possess labeled data of the target domains. Our current work therefore focuses on the single-system unsupervised DA. Besides, note that this setting tries to construct a single system that can work robustly with differ- ent but related domains (multiple target domains), thus being different from most previous studies on DA ( <ref type="bibr" target="#b4">Blitzer et al., 2006;</ref><ref type="bibr" target="#b5">Blitzer et al., 2007</ref>) which have attempted to design a specialized system for every specific target domain. <ref type="bibr" target="#b28">Plank and Moschitti (2013)</ref> propose to embed word clusters and latent semantic analysis (LSA) of words into tree kernels for DA of RE, while Nguyen and Grishman (2014) studies the appli-cation of word clusters and word embeddings for DA of RE on the feature-based method. Although word clusters <ref type="bibr" target="#b7">(Brown et al., 1992</ref>) have been em- ployed by both studies to improve the performance of relation extractors across domains, the appli- cation of word embeddings ( <ref type="bibr" target="#b2">Bengio et al., 2003;</ref><ref type="bibr" target="#b18">Mnih and Hinton, 2008;</ref><ref type="bibr" target="#b34">Turian et al., 2010)</ref> for DA of RE is only examined in the feature-based method and never explored in the tree kernel- based method so far, giving rise to the first ques- tion we want to address in this paper:</p><p>(i) Can word embeddings help the tree kernel- based methods on DA for RE and more impor- tantly, in which way can we do it effectively?</p><p>This question is important as word embeddings are real valued vectors, while the tree kernel-based methods rely on the symbolic matches or mis- matches of concrete labels in the parse trees to compute the kernels. It is unclear at the first glance how to encode word embeddings into the tree ker- nels effectively so that word embeddings could help to improve the generalization performance of RE. One way is to use word embeddings to com- pute similarities between words and embed these similarity scores into the kernel functions, e.g., by resembling the method of <ref type="bibr" target="#b28">Plank and Moschitti (2013)</ref> that exploited LSA (in the semantic syntac- tic tree kernel (SSTK), cf. §2.1). We explore vari- ous methods to apply word embeddings to gener- ate the semantic representations for DA of RE and demonstrate that semantic representations are very effective to significantly improve the portability of the relation extractors based on the tree kernels, bringing us to the second question:</p><p>(ii) Between the feature-based method in <ref type="bibr" target="#b21">Nguyen and Grishman (2014)</ref> and the SSTK method in <ref type="bibr" target="#b28">Plank and Moschitti (2013)</ref>, which method is better for DA of RE, given the recent discovery of word embeddings for both methods?</p><p>It is worth noting that besides the approach dif- ference, these two works employ rather different resources and settings in their evaluation, mak- ing it impossible to directly compare their perfor- mance. In particular, while <ref type="bibr" target="#b28">Plank and Moschitti (2013)</ref> only use the path-enclosed trees induced from the constituent parse trees as the represen- tation for relation mentions, Nguyen and Grish- man (2014) include a rich set of features extracted from multiple resources such as constituent trees, dependency trees, gazetteers, semantic resources in the representation. <ref type="bibr">Besides, Plank and Moschitti (2013)</ref> consider the direction of relations in their evaluation (i.e, distinguishing between rela- tion classes and their inverses) but <ref type="bibr" target="#b21">Nguyen and Grishman (2014)</ref> disregard this relation direction. Finally, we note that although both studies evalu- ate their systems on the ACE 2005 dataset, they actually have different dataset partitions. In order to overcome this limitation, we conduct an eval- uation in which the two methods are directed to use the same resources and settings, and are thus compared in a compatible manner to achieve an in- sight on their effectiveness for DA of RE. In fact, the problem of incompatible comparison is unfor- tunately very common in the RE literature <ref type="bibr" target="#b35">(Wang, 2008;</ref><ref type="bibr" target="#b28">Plank and Moschitti, 2013)</ref> and we believe there is a need to tackle this increasing confusion in this line of research. Therefore, this is actu- ally the first attempt to compare the two methods (tree kernel-based and feature-based) on the same settings. To ease the comparison for future work and circumvent the Zigglebottom pitfall <ref type="bibr" target="#b26">(Pedersen, 2008)</ref>, the entire setup and package is available. <ref type="bibr">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Relation Extraction Approaches</head><p>In the following, we introduce the two relation ex- traction systems further examined in this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Tree kernel-based Method</head><p>In the tree kernel-based method <ref type="bibr" target="#b19">(Moschitti, 2006;</ref><ref type="bibr" target="#b20">Moschitti, 2008;</ref><ref type="bibr" target="#b28">Plank and Moschitti, 2013</ref>), a relation mention (the two entity mentions and the sentence containing them) is represented by the path-enclosed tree (PET), the smallest constituency-based subtree including the two tar- get entity mentions ( <ref type="bibr" target="#b42">Zhang et al., 2006</ref>). The syn- tactic tree kernel (STK) is then defined to compute the similarity between two PET trees (where tar- get entities are marked) by counting the common sub-trees, without enumerating the whole frag- ment space <ref type="bibr" target="#b19">(Moschitti, 2006;</ref><ref type="bibr" target="#b20">Moschitti, 2008)</ref>. STK is then applied in the support vector ma- chines (SVMs) for RE. The major limitation of STK is its inability to match two trees that share the same substructure, but involve different though semantically related terminal nodes (words). This is caused by the hard matches between words, and consequently between sequences containing them. For instance, in the following example taken from <ref type="bibr" target="#b28">Plank and Moschitti (2013)</ref>, the two frag- ments "governor from Texas" and "head of Mary-land" would not match in STK although they have very similar syntactic structures and basically con- vey the same relationship. <ref type="bibr" target="#b28">Plank and Moschitti (2013)</ref> propose to resolve this issue for STK using the semantic syntac- tic tree kernel (SSTK) <ref type="bibr" target="#b6">(Bloehdorn and Moschitti, 2007)</ref> and apply it to the domain adaptation prob- lem of RE. The two following techniques are uti- lized to activate the SSTK: (i) replace the part-of- speech nodes in the PET trees by the new ones labeled by the word clusters of the corresponding terminals (words); (ii) replace the binary similar- ity scores between words (i.e, either 1 or 0) by the similarities induced from the latent semantic analysis (LSA) of large corpus. The former gener- alizes the part-of-speech similarity to the seman- tic similarity on word clusters; the latter, on the other hand, allows soft matches between words that have the same latent semantic but differ in symbolic representation. Both techniques empha- size the invariants of word semantics in different domains, thus being helpful to alleviate the vocab- ulary difference across domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Feature-based Method</head><p>In the feature-based method ( <ref type="bibr" target="#b44">Zhou et al., 2005;</ref><ref type="bibr" target="#b33">Sun et al., 2011;</ref><ref type="bibr" target="#b21">Nguyen and Grishman, 2014</ref>), re- lation mentions are first transformed into rich fea- ture vectors that capture various characteristics of the relation mentions (i.e, lexicon, syntax, seman- tics etc). The resulting vectors are then fed into the statistical classifiers such as Maximum Entropy (MaxEnt) to perform classification for RE.</p><p>The main reason for the performance loss of the feature-based systems on new domains is the behavioral changes of the features when domains shift. Some features might be very informative in the source domain but become less relevant in the target domains. For instance, some words, that are very indicative in the source domain might not appear in the target domains (lexical sparsity). Consequently, the models putting high weights on such words (features) in the source domain will fail to perform well on the target domains. <ref type="bibr" target="#b21">Nguyen and Grishman (2014)</ref> address this problem for the feature-based method in DA of RE by introduc- ing word embeddings as additional features. The rationale is based on the fact that word embed- dings are low dimensional and real valued vec- tors, capturing latent syntactic and semantic prop- erties of words ( <ref type="bibr" target="#b2">Bengio et al., 2003;</ref><ref type="bibr" target="#b18">Mnih and Hinton, 2008;</ref><ref type="bibr" target="#b34">Turian et al., 2010)</ref>. The embed- dings of symbolically different words are often close to each other if they have similar semantic and syntactic functions. This again helps to mit- igate the lexical sparsity or the vocabulary differ- ence between the domains and has proven helpful for, amongst others, the feature-based method in DA of RE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Tree Kernel-based vs Feature-based</head><p>The feature-based method explicitly encapsulates the linguistic intuition and domain expertise for RE into the features, while the tree kernel-based method avoids the complicated feature engineer- ing and implicitly encode the features into the computation of the tree kernels. Which method is better for DA of RE?</p><p>In order to ensure the two methods <ref type="bibr" target="#b28">(Plank and Moschitti, 2013;</ref><ref type="bibr" target="#b21">Nguyen and Grishman, 2014</ref>) are compared compatibly on the same resources, we make sure the two systems have access to the same amount of information. Thus, we follow <ref type="bibr" target="#b28">Plank and Moschitti (2013)</ref> and use the PET trees (be- side word clusters and word embeddings) as the only resource the two methods can exploit.</p><p>For the feature-based method, we utilize all the features extractable from the PET trees that are standardly used in the state-of-the-art feature- based systems for DA of RE <ref type="bibr" target="#b21">(Nguyen and Grishman, 2014</ref>). Specifically, the feature set em- ployed in this paper (denoted by FET) includes: the lexical features, i.e., the context words, the head words, the bigrams, the number of words, the lexical path, the order of mention ( <ref type="bibr" target="#b44">Zhou et al., 2005;</ref><ref type="bibr" target="#b33">Sun et al., 2011)</ref>; and the syntactic features, i.e., the path connecting the two mentions in PET and the unigrams, bigrams, trigrams along this path ( <ref type="bibr" target="#b44">Zhou et al., 2005;</ref><ref type="bibr" target="#b14">Jiang and Zhai, 2007a)</ref>.</p><p>Hypothesis: Assuming identical settings and resources, we hypothesize that the tree kernel- based method is better than the feature-based method for DA of RE. This is motivated because of at least two reasons: (i) the tree kernel-based method implicitly encodes a more comprehen- sive feature set (involving all the sub-trees in the PETs), thus potentially captures more domain- independent features to be useful for DA of RE; (ii) the tree kernel-based method avoids the in- clusion of fine-tuned and domain-specific features originated from the excessive feature engineer- ing (i.e., hand-designing feature sets based on the linguistic intuition for specific domains) of the feature-based method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Word Embeddings &amp; Tree Kernels</head><p>In this section, we first give the intuition that guides us in designing the proposed methods. In particular, one limitation of the syntactic seman- tic tree kernel presented in Plank and Moschitti (2013) ( §2.1) is that semantics is highly tied to syntax (the PET trees) in the kernel computation, limiting the generalization capacity of semantics to the extent of syntactic matches. If two rela- tion mentions have different syntactic structures, the two relation mentions will not match, although they share the same semantic representation and express the same relation class. For instance, the two fragments "Tom is the CEO of the company" and "the company, headed by Tom" express the same relationship between "Tom" and "company" based on the semantics of their context words, but cannot be matched in SSTK as their syntac- tic structures are different. In such a case, it is desirable to have a representation of relation men- tions that is grounded on the semantics of the con- text words and reflects the latent semantics of the whole relation mentions. This representation is expected to be general enough to be effective on different domains. Once the semantic representa- tion of relation mentions is established, we can use it in conjunction with the traditional tree kernels to extend their coverage. The benefit is mutual as both semantics and syntax help to generalize rela- tion mentions to improve the recall, but also con- strain each other to support precision. This is the basic idea of our approach, which we compare to the previous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Methods</head><p>We propose to utilize word embeddings of the con- text words as the principal components to obtain semantic representations for relation mentions in the tree kernel-based methods. Besides more tra- ditional approaches to exploit word embeddings, we investigate representations that go beyond the word level and use compositionality embeddings for domain adaptation for the first time.</p><p>In general, suppose we are able to acquire an additional real-valued vector V i from word embed- dings to semantically represent a relation mention R i (along with the PET tree T i ), leading to the new representation of R i = (T i , V i ). The new kernel function in this case is then defined by:</p><formula xml:id="formula_0">K new (R i , R j ) = (1 − α)SSTK(T i , T j ) + αK vec (V i , V j )</formula><p>where K vec (V i , V j ) is some standard vector ker- nel like the polynomial kernels. α is a trade-off parameter and indicates whether the system at- tributes more weight to the traditional SSTK or the new semantic kernel K vec .</p><p>In this work, we consider the following meth- ods to obtain the semantic representation V i from the word embeddings of the context words of R i (assuming d is the dimensionality of the word em- beddings):</p><p>HEAD: V i = the concatenation of the word em- beddings of the two entity mention heads of R i . This representation is inherited from Nguyen and Grishman (2014) that only examine embeddings at the word level separately for the feature-based method without considering the compositionality embeddings of relation mentions. The dimension- ality of HEAD is 2d.</p><p>According to the principle of compositional- ity ( <ref type="bibr" target="#b36">Werning et al., 2006</ref>; <ref type="bibr" target="#b0">Baroni and Zamparelli, 2010;</ref><ref type="bibr" target="#b25">Paperno et al., 2014)</ref>, the meaning of a com- plex expression is determined by the meanings of its components and the rules to combine them. We study the following two compositionality embed- dings for relation mentions that can be generated from the embeddings of the context words:</p><p>PHRASE: V i = the mean of the embeddings of the words contained in the PET tree T i of R i . Although this composition is simple, it is in fact competitive to the more complicated methods based on recursive neural networks <ref type="bibr" target="#b31">(Socher et al., 2012b;</ref><ref type="bibr" target="#b3">Blacoe and Lapata, 2012;</ref><ref type="bibr" target="#b32">Sterckx et al., 2014</ref>) on representing phrase semantics.</p><p>TREE: This is motivated by the training of re- cursive neural networks (Socher et al., 2012a) for semantic compositionality and attempts to aggre- gate the context words embeddings syntactically. In particular, we compute an embedding for every node in the PET tree in a bottom-up manner. The embeddings of the leaves are the embeddings of the words associated with them while the embed- dings of the internal nodes are the means of the embeddings of their children nodes. We use the embeddings of the root of the PET tree to represent the relation mention in this case. Both PHRASE and TREE have d dimensions.</p><p>It is also interesting to examine combinations of these three representations (cf., <ref type="table">Table 1</ref>). SIM: Finally, for completeness, we experi- ment with a more obvious way to introduce word embeddings into tree kernels, resembling more closely the approach of <ref type="bibr" target="#b28">Plank and Moschitti (2013)</ref>. In particularly, the SIM method simply replaces the similarity scores between word pairs obtained from LSA by the cosine similarities be- tween the word embeddings to be used in the SSTK kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset, Resources and Parameters</head><p>We use the word clusters trained by <ref type="bibr" target="#b28">Plank and Moschitti (2013)</ref> on the ukWaC corpus ( <ref type="bibr" target="#b1">Baroni et al., 2009</ref>) with 2 billion words, and the C&amp;W word embeddings from Turian el al. (2010) 2 with 50 dimensions following <ref type="bibr" target="#b21">Nguyen and Grishman (2014)</ref>. In order to make the comparisons com- patible, we introduce word embeddings into the tree kernel by extending the package provided by <ref type="bibr" target="#b28">Plank and Moschitti (2013)</ref>, which uses the Char- niak parser to obtain the constituent trees, the SVM-light-TK for the SSTK kernel in SVM, the directional relation classes, etc. We utilize the de- fault vector kernel in the SVM-light-TK package (d=3). For the feature-based method, we apply the MaxEnt classifier in the MALLET 3 package with the L2 regularizer on the hierarchical architecture for relation extraction as in <ref type="bibr" target="#b21">Nguyen and Grishman (2014)</ref>.</p><p>Following prior work, we evaluate the sys- tems on the ACE 2005 dataset which involves 6 domains: broadcast news (bn), newswire (nw), broadcast conversation (bc), telephone conversa- tion (cts), weblogs (wl) and usenet (un). The union of bn and nw (news) is used as the source domain while bc, cts and wl play the role of the target do- mains. We take half of bc as the only target de- velopment set, and use the remaining data and do- mains for testing. The dataset partition is exactly the same as in <ref type="bibr" target="#b28">Plank and Moschitti (2013)</ref>. As described in their paper, the target domains quite differ from the source domain in the relation dis- tributions and vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Word Embeddings for Tree Kernel</head><p>We investigate the effectiveness of different se- mantic representations ( §3.1) in tree kernels by   <ref type="table" target="#tab_2">Table 2   Table 1</ref> shows the results. The main conclusions include:</p><p>(i) The substitution of LSA similarity scores with the word embedding cosine similarities (SIM) does not help to improve the performance of the tree kernel method.</p><p>(ii) When employed independently, both the word level embeddings (HEAD) and the compo- sitionality embeddings (PHRASE, TREE) are ef- fective for the tree kernel-based method on DA for RE, showing a slight advantage for HEAD.</p><p>(iii) Thus, the compositionality embeddings PHRASE and TREE seem to capture different information with respect to the word level em- beddings HEAD. We expect the combination of HEAD with either PHRASE or TREE to further improve performance. This is the case when adding one of them at a time. PHRASE and TREE seem to capture similar information, combining all (last row in <ref type="table">Table 1)</ref> is not the overall best sys- tem. The best performance is achieved when the HEAD and PHRASE embeddings are utilized at nw+bn (in-dom.) bc cts wl <ref type="table">#  System:  P:  R:  F1:  P:  R:  F1:  P:  R:  F1:  P:  R:  F1:  1</ref> PET ( <ref type="bibr" target="#b28">Plank and Moschitti, 2013)</ref>   the same time, reaching an F1 of 53.4% (compared to 46.4% of the baseline) on the development set.</p><p>The results in <ref type="table">Table 1</ref> are obtained using the trade-off parameter α = 0.7. <ref type="figure" target="#fig_0">Figure 1</ref> addi- tionally shows the variation of the performance with changing α (for the best system on dev, i.e., for the representation PET+HEAD+PHRASE). As we can see, the performance for α &gt; 0.5 is in general better, suggesting a preference for the semantic representation over the syntactic repre- sentation in DA for RE. The performance reaches its peak when the suitable amounts of semantics and syntax are combined (i.e, α = 0.7).</p><p>In the following experiments, we use the embedding combination (HEAD+PHRASE) with α = 0.7 for the tree kernels, denoted WED.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Domain Adaptation Experiments</head><p>In this section, we examine the semantic rep- resentation for DA of RE in the tree kernel- based method. In particular, we take the sys- tems using the PET trees, word clusters and LSA in <ref type="bibr" target="#b28">Plank and Moschitti (2013)</ref> as the baselines and augment them with the embeddings WED = HEAD+PHRASE. We report the performance of these augmented systems in <ref type="table" target="#tab_2">Table 2</ref> for the two scenarios: (i) in-domain: both training and test- ing are performed on the source domain via 5-fold cross validation and (ii) out-of-domain: models are trained on the source domain but evaluated on the three target domains. To summarize, we find:</p><p>First, word embeddings seem to subsume word clusters in the tree kernel-based method (compar- ing rows 2 and 4, and except domain cts) while word embeddings and LSA actually encode dif- ferent information (comparing rows 2 and 6 for the out-of-domain experiments) and their combi- nation would be helpful for DA of RE.</p><p>Second, regarding composite kernels, given word embeddings, the addition of the baseline ker- nel (PET) is in general useful for the augmented kernels PET WC and PET LSA (comparing rows 4 and 8, rows 6 and 10) although it is less pro- nounced for PET LSA.</p><p>Third and most importantly, for all the systems in Plank and Moschitti (2013) (the baselines) and for all the target domains, whether word clusters and LSA are utilized or not, we consistently wit- ness the performance improvement of the base- lines when combined with word embedding (com- paring systems X and X+WED where X is some baseline system). The best out-of-domain perfor- mance is achieved when word embeddings are em- ployed in conjunction with the composite kernels (PET+PET WC+PET LSA for the target domains bc and wl, and PET+PET WC for the target do- main cts). To be more concrete, the best system with word embeddings (row 12 in <ref type="table" target="#tab_2">Table 2</ref>) signif- icantly outperforms the best system in Plank and Moschitti (2013) with p &lt; 0.05, an improvement of 3.7%, 1.1% and 2.7% on the target domains bc, cts and wl respectively, demonstrating the bene- fit of word embeddings for DA of RE in the tree kernel-based method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Tree Kernel-based vs Feature-based DA of RE</head><p>This section aims to compare the tree kernel-based method in <ref type="bibr" target="#b28">Plank and Moschitti (2013)</ref> and the feature-based method in <ref type="bibr" target="#b21">Nguyen and Grishman (2014)</ref> for DA of RE on the same settings (i.e, same dataset partition, the same pre-processing nw+bn (in-dom.) bc cts wl System: P: R: F1: P: R: F1: P: R: F1: P: R: F1: Tree kernel-based: PET+PET WC+HEAD+PHRASE 56.3 50.3 53.1 57.5 46.6 51.5 55. <ref type="bibr">6</ref>    <ref type="bibr" target="#b21">Nguyen and Grishman (2014)</ref>. All the compar- isons between the tree kernel-based method and the feature-based method in this table are significant with p &lt; 0.05.</p><p>procedure, the same model of directional relation classes, the same PET trees for tree kernels and feature extraction, the same word clusters and the same word embeddings). We first evaluate the feature-based system with different combinations of embeddings (i.e, HEAD, PHRASE and TREE) on the bc development set. Based on the evalua- tion results, we then discuss the effect of the se- mantic representations on the feature-based sys- tem and the tree kernel-based system, and then compare the performance of the two methods when they are augmented with their best corre- sponding embedding combinations.   <ref type="table" target="#tab_6">Table 4</ref> presents the evaluation results on the bc development for the feature-based system where B is the baseline feature set consisting of FET and word clusters (WC) <ref type="bibr" target="#b21">(Nguyen and Grishman, 2014</ref>).</p><p>The Role of Semantic Representations Con- sidering <ref type="table" target="#tab_6">Table 4</ref> for the feature-based method and <ref type="table">Table 1</ref> for the tree kernel-based method, we see that when combined with the HEAD embeddings, the compositionality embedding TREE is more ef- fective for the feature-based method, in contrast to the tree kernel-based method, where the PHRASE embeddings are better. This can be partly ex- plained by the fact that the tree kernel-based method emphasizes the syntactic structure of the relation mentions, while the feature-based method exploits the sequential structure more. Conse- quently, the syntactic semantics of TREE are more helpful for the feature-based method, whereas the sequential semantics of PHRASE are more useful for the tree kernel-based method.</p><p>Performance Comparison The three best em- bedding combinations for the feature-based sys- tem in <ref type="table" target="#tab_6">Table 4</ref> are (listed by performance order): (HEAD), (HEAD+TREE) and (TREE), where (HEAD) is also the best word level method em- ployed in <ref type="bibr" target="#b21">Nguyen and Grishman (2014)</ref>. In order to enable a fairer and clearer evaluation, when doing comparison, we use both the three best embedding combinations in the feature- based method and the best embedding combina- tion (HEAD+PHRASE) in the tree kernel-based method. In the tree kernel-based method, we do not employ the LSA information as it comes in the form of similarity scores between pairs of words, and it is not clear how to encode this information into the feature-based method effectively. Finally, we utilize the composite kernel for its demon- strated effectiveness in Section 4.3.</p><p>The most important observation from the ex- perimental results (shown in <ref type="table" target="#tab_4">Table 3</ref>) is that over all the target domains, the tree kernel-based sys- tem is significantly better than the feature-based systems with p &lt; 0.05 (assuming the same re- sources and settings mentioned above). In fact, there are large margins between the tree kernel- based and the feature-based methods in this case (i.e, about 3.7% for bc, 3.1% for cts and 2.3% for wl), clearly confirming the hypothesis about the advantage of the tree kernel-based method over the feature-based method on DA for RE in Section 2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>This section analyzes the output of the systems to gain more insights into their operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word Embeddings for the Tree-kernel based</head><p>Method We focus on the comparison of the best model in <ref type="bibr" target="#b28">Plank and Moschitti (2013)</ref>  <ref type="table" target="#tab_2">(row 11  in Table 2</ref>) (called P) with the same model but augmented with the embedding WED (row 12 in Tabel 2) (called P+WED). One of the most inter- esting insights is that the embedding WED helps to semantically generalize the phrases connecting the two target entity mentions beyond the syntactic constraints. For instance, model P fails to discover the relation between "Chuck Hagel" and "Viet- nam" in the sentence (of the target domain bc): "Sergeant Chuck Hagel was seriously wounded twice in Vietnam." (i.e, it returns the NONE re- lation as the prediction) as the substructure asso- ciated with "seriously wounded twice" does not appear with any relation in the source domain. Model P+WED, on the other hand, correctly pre- dicts the PHYS (Located) relation between the two entities as the PHRASE embedding of "Chuck Hagel was seriously wounded twice in Vietnam." (phrase X1) is very close to the embedding of the source domain phrase: "Stewart faces up to 30 years in prison" (phrase X2) (annotated with the PHYS relation between "Stewart" and "prison").</p><p>In fact, X2 is only the 9th closest phrase in the source domain of X1. The closest phrase of X1 in the source domain is X3: the phrase be- tween "Iraqi soldiers" and "herself" in the sen- tence "The Washington Post is reporting she shot several Iraqi soldiers before she was captured and she was shot herself , too.". However, as the syntactical structure of X1 is more similar to X2's, and is remarkably different from X3 as well as the other closest phrases (ranked from 2nd to 8th), the new kernel function K new would still prefer X2 due to its trade-off between syntax and semantics.</p><p>Tree Kernel-based vs Feature-based From the analysis of the systems in <ref type="table" target="#tab_4">Table 3</ref>, we find that, among others, the tree kernel-based method im- proves the precision significantly via the seman- tic and syntactic refinement it maintains. Let us consider the following phrase of the target domain bc: "troops have dislodged stubborn Iraqi sol- diers" (called Y1). The feature-based systems in <ref type="table" target="#tab_4">Table 3</ref> incorrectly predict the ORG-AFF relation (Employment or Membership) between "Iraqi sol- diers" and "troops". This is mainly due to the high weights of the features linking the words "troop" and "soldiers" with the relation type ORG-AFF in the feature-based models, which is, in turn, orig- inated from the high correlation of these words and the relation type in the training data of the source domain (domain bias). The tree kernel- based model in <ref type="table" target="#tab_4">Table 3</ref> successfully recognizes the NONE relation in this case. A closer examination shows that the phrase with the closest embedding to Y1 in the source domain is Y2: "Iraqi soldiers abandoned their posts", 5 which is annotated with the NONE relation between "Iraqi soldiers" and "their posts". As the syntactic structure of Y2 is also very similar to Y1, it is not surprising that Y1 is closest to Y2 in the new kernel function, conse- quently helping the tree kernel-based method work correctly in this case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related work</head><p>Word embeddings are only applied to RE recently. <ref type="bibr" target="#b31">Socher et al. (2012b)</ref> use word embeddings as in- put for matrix-vector recursive neural networks in relation classification while <ref type="bibr" target="#b41">Zeng et al. (2014)</ref>, and <ref type="bibr" target="#b22">Nguyen and Grishman (2015)</ref> employ word embeddings in the framework of convolutional neural networks for relation classification and ex- traction, respectively. <ref type="bibr" target="#b32">Sterckx et al. (2014)</ref> uti- lize word embeddings to reduce noise of training data in distant supervision. <ref type="bibr" target="#b17">Kuksa et al. (2010)</ref> present a string kernel for bio-relation extraction with word embeddings, and <ref type="bibr" target="#b38">Yu et al. (2014;</ref>) study the factor-based compositional embedding models. However, none of this work examines word embeddings for tree kernels as well as do- main adaptation as we do.</p><p>Regarding DA, in the unsupervised DA setting, <ref type="bibr" target="#b13">Huang and Yates (2010)</ref> attempt to learn multi- dimensional feature representations while <ref type="bibr" target="#b4">Blitzer et al. (2006)</ref> introduce structural correspondence learning. <ref type="bibr">Daumé (2007)</ref> proposes an easy adapta- tion framework (EA) while <ref type="bibr" target="#b37">Xiao and Guo (2013)</ref> present a log-bilinear language adaptation tech- nique in the supervised DA setting. Unfortunately, all of this work assumes some prior (in the form of either labeled or unlabeled data) on the target do- mains for the sequential labeling tasks, in contrast to our single-system unsupervised DA setting for relation extraction. An alternative method that is also popular to DA is instance weighting <ref type="bibr" target="#b15">(Jiang and Zhai, 2007b)</ref>. However, as shown by <ref type="bibr" target="#b28">Plank and Moschitti (2013)</ref>, instance weighting is not very useful for DA of RE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In order to improve the generalization of rela- tion extractors, we propose to augment the seman- tic syntactic tree kernels with the semantic rep- resentation of relation mentions, generated from the word embeddings of the context words. The method demonstrates strong promise for the DA of RE, i.e, it significantly improves the best sys- tem of <ref type="bibr" target="#b28">Plank and Moschitti (2013)</ref> (up to 7% rela- tive improvement). Moreover, we perform a com- patible comparison between the tree kernel-based method and the feature-based method on the same settings and resources, which suggests that the tree kernel-based method <ref type="bibr" target="#b28">(Plank and Moschitti, 2013)</ref> is better than the feature-based method <ref type="bibr" target="#b21">(Nguyen and Grishman, 2014</ref>) for DA of RE. An error anal- ysis is conducted to get a deeper comprehension of the systems. Our future plan is to investigate other syntactic and semantic structures (such as depen- dency trees, abstract meaning representation etc) for DA of RE, as well as continue the comparison between the kernel-based method and the feature- based method when they are allowed to exploit more resources.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2Figure 1 :</head><label>1</label><figDesc>Figure 1: α vs F-measure on PET+HEAD+PHRASE</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : In-domain (first column) and out-of-domain performance (columns two to four) on ACE 2005. Systems of the rows not in gray come from Plank and Moschitti (2013) (the baselines). WED means HEAD+PHRASE.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Tree kernel-based in Plank and Moschitti (2013) vs feature-based in </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Performance of the feature-based method (dev). 

</table></figure>

			<note place="foot" n="1"> https://bitbucket.org/nycphre/limo-re</note>

			<note place="foot" n="4"> By using their system we obtained the same results.</note>

			<note place="foot" n="5"> The full sentence is: &quot;After today&apos;s air strikes, Iraqi soldiers abandoned their posts and surrendered to Kurdish fighters.&quot;.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The WaCky wide web: a collection of very large linguistically processed web-crawled corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Bernardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriano</forename><surname>Ferraresi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eros</forename><surname>Zanchetta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Language Resources and Evaluation</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="209" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A comparison of vector-based representations for semantic composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Blacoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Domain adaptation with structural correspondence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploiting Structure and Semantics for Expressive Text Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Bloehdorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Class-based n-gram models of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenifer</forename><forename type="middle">C</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Linguistics</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="467" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A shortest path dependency kernel for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Razvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>In EMNLP</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to extract relations from the web using minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Razvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bunescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Exploiting background knowledge for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><surname>Seng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daume</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Nyu&apos;s english ace 2005 system description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Westbrook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Meyers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The ACE 2005 Evaluation Workshop</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Exploring representation-learning approaches to domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The ACL Workshop on Domain Adaptation for Natural Language Processing</title>
		<imprint>
			<publisher>DANLP</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A systematic exploration of the feature space for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Instance weighting for domain adaptation in nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Combining lexical, syntactic, and semantic features with maximum entropy models for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanda</forename><surname>Kambhatla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semi-supervised abstractionaugmented string kernel for multi-level bio-relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Pavlovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Ning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML PKDD</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A scalable hierarchical distributed language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient convolution kernels for dependency and constituent syntactic trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Kernel methods, syntax and semantics for relational text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Employing word representations and regularization for domain adaptation of relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huu</forename><surname>Thien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Relation extraction: Perspective from convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huu</forename><surname>Thien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The NAACL Workshop on Vector Space Modeling for NLP (VSM)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Convolution kernels on constituent, dependency and sequential structures for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Truc-Vien Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Riccardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Robust domain adaptation for relation extraction via clustering consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Ivor</forename><surname>Luan Minh Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kian Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leong Hai</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A practical and linguistically-motivated approach to compositional distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Paperno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">The</forename><forename type="middle">Nghia</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Empiricism is not a matter of faith</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Linguistics 3</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="465" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Overview of the 2012 shared task on parsing the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Embedding semantic similarity in tree kernels for domain adaptation of relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Exploiting constituent dependencies for tree kernel-based semantic relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhua</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peide</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Using active learning and semantic clustering for noise reduction in distant supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Sterckx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Develder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AKBC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Semi-supervised relation extraction with large-scale word clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Sekine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Word representations: A simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev-Arie</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A re-examination of dependency path kernels for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqiu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNLP</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Compositionality of meaning and content: Foundational issues (linguistics &amp; philosophy)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Werning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Machery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Schurz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Linguistics &amp; philosophy</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Domain adaptation for sequence labeling tasks with a probabilistic language adaptation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Factor-based compositional embedding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Gormley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The NIPS workshop on Learning Semantics</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Combining word embeddings and feature embeddings for fine-grained relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Gormley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Kernel methods for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Zelenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chinatsu</forename><surname>Aone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Richardella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1083" to="1106" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A composite kernel to extract relations between entities with both flat and structured features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING-ACL</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Extracting relations with integrated information using kernel methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Exploring various knowledge in relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
