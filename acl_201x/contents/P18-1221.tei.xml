<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Building Language Models for Text with Named Entities</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018. 2373</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Md</roleName><forename type="first">Rizwan</forename><surname>Parvez</surname></persName>
							<email>rizwan@cs.ucla.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of California Los Angeles</orgName>
								<orgName type="institution" key="instit2">Columbia University</orgName>
								<orgName type="institution" key="instit3">University of Virginia</orgName>
								<orgName type="institution" key="instit4">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baishakhi</forename><surname>Ray</surname></persName>
							<email>rayb@cs.columbia.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of California Los Angeles</orgName>
								<orgName type="institution" key="instit2">Columbia University</orgName>
								<orgName type="institution" key="instit3">University of Virginia</orgName>
								<orgName type="institution" key="instit4">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saikat</forename><surname>Chakraborty</surname></persName>
							<email>saikatc@virginia.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of California Los Angeles</orgName>
								<orgName type="institution" key="instit2">Columbia University</orgName>
								<orgName type="institution" key="instit3">University of Virginia</orgName>
								<orgName type="institution" key="instit4">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
							<email>kwchang@cs.ucla.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of California Los Angeles</orgName>
								<orgName type="institution" key="instit2">Columbia University</orgName>
								<orgName type="institution" key="instit3">University of Virginia</orgName>
								<orgName type="institution" key="instit4">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Building Language Models for Text with Named Entities</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2373" to="2383"/>
							<date type="published">July 15-20, 2018. 2018. 2373</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Text in many domains involves a significant amount of named entities. Predicting the entity names is often challenging for a language model as they appear less frequent on the training corpus. In this paper, we propose a novel and effective approach to building a discriminative language model which can learn the entity names by leveraging their entity type information. We also introduce two benchmark datasets based on recipes and Java programming codes, on which we evaluate the proposed model. Experimental results show that our model achieves 52.2% better perplexity in recipe generation and 22.06% on code generation than the state-of-the-art language models.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Language model is a fundamental component in Natural Language Processing (NLP) and it sup- ports various applications, including document generation ( <ref type="bibr" target="#b28">Wiseman et al., 2017</ref>), text auto- completion ( <ref type="bibr" target="#b1">Arnold et al., 2017)</ref>, spelling correc- tion <ref type="bibr" target="#b3">(Brill and Moore, 2000</ref>), and many others. Recently, language models are also successfully used to generate software source code written in programming languages like Java, C, etc. ( <ref type="bibr" target="#b13">Hindle et al., 2016;</ref><ref type="bibr" target="#b29">Yin and Neubig, 2017;</ref><ref type="bibr" target="#b12">Hellendoorn and Devanbu, 2017;</ref><ref type="bibr" target="#b22">Rabinovich et al., 2017</ref>). These models have improved the language generation tasks to a great extent, e.g., <ref type="bibr">(Mikolov et al., 2010;</ref><ref type="bibr">Galley et al., 2015)</ref>. However, while generating text or code with a large number of named entities (e.g., different variable names in source code), these models often fail to predict the entity names properly due to their wide variations. For instance, consider building a language model for generating recipes. There are numerous simi- lar, yet slightly different cooking ingredients (e.g., olive oil, canola oil, grape oil, etc.-all are dif- ferent varieties of oil). Such diverse vocabularies of the ingredient names hinder the language model from predicting them properly.</p><p>To address this problem, we propose a novel language model for texts with many entity names. Our model learns the probability distribution over all the candidate words by leveraging the en- tity type information. For example, oil is the type for named entities like olive oil, canola oil, grape oil, etc. 1 Such type information is even more prevalent for source code corpus written in statically typed programming languages <ref type="bibr" target="#b5">(Bruce, 1993)</ref>, since all the variables are by construct as- sociated with types like integer, float, string, etc.</p><p>Our model exploits such deterministic type in- formation of the named entities and learns the probability distribution over the candidate words by decomposing it into two sub-components: (i) Type Model. Instead of distinguishing the individ- ual names of the same type of entities, we first con- sider all of them equal and represent them by their type information. This reduces the vocab size to a great extent and enables to predict the type of each entity more accurately. (ii) Entity Composite Model. Using the entity type as a prior, we learn the conditional probability distribution of the ac- tual entity names at inference time. We depict our model in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>To evaluate our model, we create two bench- mark datasets that involve many named entities. One is a cooking recipe corpus 2 where each recipe contains a number of ingredients which are cate-place proteins in center of a dish with vegetables on each side . place chicken in center of a dish with broccoli on each side .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>entity name w P(w|proteins) P(w)</head><p>q chicken 0.43 0. <ref type="bibr">35</ref>    gorized into 8 super-ingredients (i.e., type); e.g., "proteins", "vegetables", "fruits", "seasonings", "grains", etc. Our second dataset comprises a source code corpus of 500 open-source Android projects collected from GitHub. We use an Ab- stract Syntax Tree (AST) <ref type="bibr">(Parsons, 1992)</ref> based approach to collect the type information of the code identifiers. Our experiments show that although state-of- the-art language models are, in general, good to learn the frequent words with enough training in- stances, they perform poorly on the entity names. A simple addition of type information as an ex- tra feature to a neural network does not guarantee to improve the performance because more features may overfit or need more model parameters on the same data. In contrast, our proposed method sig- nificantly outperforms state-of-the-art neural net- work based language models and also the models with type information added as an extra feature.</p><p>Overall, followings are our contributions:</p><p>• We analyze two benchmark language corpora where each consists of a reasonable number of entity names. While we leverage an ex- isting corpus for recipe, we curated the code corpus. For both datasets, we created auxil- iary corpora with entity type information. All the code and datasets are released. 3 • We design a language model for text consist- ing of many entity names. The model learns to mention entities names by leveraging the entity type information.</p><p>• We evaluate our model on our benchmark datasets and establish a new baseline perfor- mance which significantly outperforms state- of-the-art language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work and Background</head><p>Class Based Language Models. Building lan- guage models by leveraging the deterministic or probabilistic class properties of the words (a.k.a, class-based language models) is an old idea ( <ref type="bibr" target="#b4">Brown et al., 1992;</ref><ref type="bibr" target="#b10">Goodman, 2001</ref>). How- ever, the objective of our model is different from the existing class-based language models. The key differences are two-folds: 1) Most existing class-based language models ( <ref type="bibr" target="#b4">Brown et al., 1992;</ref><ref type="bibr" target="#b21">Pereira et al., 1993;</ref><ref type="bibr">Niesler et al., 1998;</ref><ref type="bibr" target="#b2">Baker and McCallum, 1998;</ref><ref type="bibr" target="#b10">Goodman, 2001;</ref><ref type="bibr">Maltese et al., 2001</ref>) are generative n-gram models whereas ours is a discriminative language model based on neu- ral networks. The modeling principle and assump- tions are very different. For example, we can- not calculate the conditional probability by statis- tical occurrence counting as these papers did. 2) Our approaches consider building two models and perform joint inference which makes our frame- work general and easy to extend. In Section 4, we demonstrate that our model can be easily in- corporated with the state-of-art language model. The closest work in this line is hierarchical neu- ral language models <ref type="bibr">(Morin and Bengio, 2005)</ref>, which model language with word clusters. How- ever, their approaches do not focus on dealing with named entities as our model does. A recent work <ref type="bibr" target="#b15">(Ji et al., 2017</ref>) studied the problem of build- ing up a dynamic representation of named entity by updating the representation for every contextu- alized mention of that entity. Nonetheless, their approach does not deal with the sparsity issue and their goal is different from ours.</p><p>Language Models for Named Entities. In some generation tasks, recently developed lan- guage models address the problem of predict-ing entity names by copying/matching the entity names from the reference corpus. For example, <ref type="bibr" target="#b27">Vinyals et al. (2015)</ref> calculates the conditional probability of discrete output token sequence cor- responding to positions in an input sequence. <ref type="bibr" target="#b11">Gu et al. (2016)</ref> develops a seq2seq alignment mech- anism which directly copies entity names or long phrases from the input sequence. <ref type="bibr" target="#b28">Wiseman et al. (2017)</ref> generates document from structured table like basketball statistics using copy and recon- struction method as well. Another related code generation model <ref type="bibr" target="#b29">(Yin and Neubig, 2017</ref>) parses natural language descriptions into source code considering the grammar and syntax in the tar- get programming language (e.g., Python). <ref type="bibr" target="#b17">Kiddon et al. (2016)</ref> generates recipe for a given goal, and agenda by making use of items on the agenda. While generating the recipe it continuously moni- tors the agenda coverage and focus on increasing it. All of them are sequence-to-sequence learning or end-to-end systems which differ from our gen- eral purpose (free form) language generation task (e.g., text auto-completion, spelling correction).</p><p>Code Generation. The way developers write codes is not only just writing a bunch of instruc- tions to run a machine, but also a form of com- munication to convey their thought. As observed by Donald E. Knuth <ref type="bibr" target="#b20">(Knuth, 1992)</ref>, "The prac- titioner of literate programming can be regarded as an essayist, whose main concern is exposition and excellence of style. Such an author, with the- saurus in hand, chooses the names of variables carefully and explains what such variable means." Such comprehensible software corpora show sur- prising regularity ( <ref type="bibr" target="#b23">Ray et al., 2015;</ref><ref type="bibr" target="#b7">Gabel and Su, 2010</ref>) that is quite similar to the statistical properties of natural language corpora and thus, amenable to large-scale statistical analysis ( <ref type="bibr" target="#b14">Hindle et al., 2012</ref> Although similar, source code has some unique properties that differentiate it from natural lan- guage. For example, source code often shows more regularities in local context due to common development practices like copy-pasting <ref type="bibr" target="#b9">(Gharehyazie et al., 2017;</ref><ref type="bibr" target="#b18">Kim et al., 2005</ref>). This prop- erty is successfully captured by cache based lan- guage models <ref type="bibr" target="#b12">(Hellendoorn and Devanbu, 2017;</ref><ref type="bibr" target="#b26">Tu et al., 2014</ref>). Code is also less ambiguous than natural language so that it can be interpreted by a compiler. The constraints for generating cor- rect code is implemented by combining language model and program analysis technique <ref type="bibr" target="#b25">(Raychev et al., 2014</ref>). Moreover, code contains open vocab- ulary-developers can coin new variable names without changing the semantics of the programs. Our model aims to addresses this property by leveraging variable types and scope.</p><p>LSTM Language Model. In this paper, we use LSTM language model as a running example to describe our approach. Our language model uses the LSTM cells to generate latent states for a given context which captures the necessary fea- tures from the text. At the output layer of our model, we use Softmax probability distribution to predict the next word based on the latent state. <ref type="bibr">Merity et al. (2017)</ref> is a LSTM-based language model which achieves the state-of-the-art perfor- mance on Penn Treebank (PTB) and WikiText- 2 (WT2) datasets. To build our recipe language model we use this as a blackbox and for our code generation task we use the simple LSTM model both in forward and backward direction. A for- ward directional LSTM starts from the beginning of a sentence and goes from left to right sequen- tially until the sentence ends, and vice versa. How- ever, our approach is general and can be applied with other types of language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A Probabilistic Model for Text with Named Entities</head><p>In this section, we present our approach to build a language model for text with name entities. Given previous context ¯ w = {w 1 , w 2 , .., w t−1 }, the goal of a language model is to predict the probabil- ity of next word P (w t | ¯ w) at time step t, where w t ∈ V text and V text is a fixed vocabulary set. Because the size of vocabulary for named entities is large and named entities often occur less fre- quently in the training corpus, the language model cannot generate these named entities accurately. For example, in our recipe test corpus the word "apple" occurs only 720 times whereas any kind of "fruits" occur 27,726 times. Existing approaches often either only generate common named entities or omit entities when generating text ( <ref type="bibr" target="#b16">Jozefowicz et al., 2016)</ref>.</p><p>To overcome this challenge, we propose to leverage the entity type information when model- ing text with many entities. We assume each en- tity is associated with an entity type in a finite set of categories S = {s 1 , s 2 , .., s i , .., s k }. Given a word w, s(w) reflects its entity type. If the word is a named entity, then we denote s(w) ∈ S; oth- erwise the type function returns the words itself (i.e, s(w) = w). To simplify the notations, we use s(w) ∈ S to represent the case where the word is not an entity. The entity type information given by s(w) is an auxiliary information that we can use to improve the language model. We use s( ¯ w) to represent the entity type information of all the words in context ¯ w and use w to represent the cur- rent word w t . Below, we show that a language model for text with typed information can be de- composed into the following two models: 1) a type model θ t that predicts the entity type of the next word and 2) an entity composite model θ v that pre- dicts the next word based on a given entity type.</p><p>Our goal is to model the probability of next word w given previous context ¯ w:</p><formula xml:id="formula_0">P (w| ¯ w; θ t , θ v ) ,<label>(1)</label></formula><p>where θ t and θ v are the parameters of the two aforementioned models. As we assume the typed information is given on the data, Eq. <ref type="formula" target="#formula_0">(1)</ref> is equiv- alent to</p><formula xml:id="formula_1">P (w, s(w)| ¯ w, s( ¯ w); θ t , θ v ) .<label>(2)</label></formula><p>A word can be either a named entity or not; therefore, we consider the following two cases.</p><p>Case 1: next word is a named entity. In this case, Eq. (2) can be rewritten as</p><formula xml:id="formula_2">P (s(w) = s| ¯ w, s( ¯ w); θ t , θ v ) × P (w| ¯ w, s( ¯ w), s(w) = s; θ v , θ t )<label>(3)</label></formula><p>based on the rules of conditional probability. We assume the type of the next token s(w) can be predicted by a model θ t using information of s( ¯ w), and we can approximate the first term in Eq.</p><formula xml:id="formula_3">(3) P (s(w)| ¯ w, s( ¯ w); θ t , θ v ) ≈ P (s(w)|s( ¯ w), θ t )<label>(4</label></formula><p>) Similarly, we can make a modeling assumption to simplify the second term as</p><formula xml:id="formula_4">P (w| ¯ w, s( ¯ w), s(w), θ v , θ t ) ≈ P (w| ¯ w, s( ¯ w), s(w), θ v ).<label>(5)</label></formula><p>Case 2: next word is not a named entity. In this case, we can rewrite Eq. (2) to be</p><formula xml:id="formula_5">P (s(w) ∈ S| ¯ w, s( ¯ w), θ t ) × P (w| ¯ w, s( ¯ w), s(w) ∈ S, θ v ) .<label>(6)</label></formula><p>The first term in Eq. (6) can be modeled by</p><formula xml:id="formula_6">1 − s∈S P (s(w) = s|s( ¯ w), θ t ),</formula><p>which can be computed by the type model 4 . The second term can be again approximated by (5) and further estimated by an entity composition model.</p><p>Typed Language Model. Combine the afore- mentioned equations, the proposed language model estimates P (w| ¯ w; θ t , θ v ) by</p><formula xml:id="formula_7">P (w| ¯ w, s( ¯ w), s(w), θ v )× P (s(w)|s( ¯ w), θ t ) if s(w) ∈ S (1− s∈S P (s(w) = s|s( ¯ w), θ t )) if s(w) ∈ S<label>(7)</label></formula><p>The first term can be estimated by an entity com- posite model and the second term can be estimated by a type model as discussed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Type model</head><p>The type model θ t estimates the probability of P (s(w)|s( ¯ w), θ t ). It can be viewed as a lan- guage model builds on a corpus with all entities replaced by their type. That is, assume the train- ing corpus consists of x = {w 1 , w 2 , .., w n }. Us- ing the type information provided in the auxiliary source, we can replace each word w with their corresponding type s(w) and generate a corpus of T = {s(w i ), s(w 2 ), .., s(w n )}. Note that if w i is not an named entity (i.e., s(w) ∈ S), s(w) = w and the vocabulary on T is V text ∪ S. 5 Any lan- guage modeling technique can be used in model- ing the type model on the modified corpus T . In this paper, we use the state-of-the-art model for each individual task. The details will be discussed in the experiment section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Entity Composite Model</head><p>The entity composite model predicts the next word based on modeling the conditional probability P (w| ¯ w, s( ¯ w), s(w), θ v ), which can be derived by</p><formula xml:id="formula_8">P (w| ¯ w, s( ¯ w); θ v ) ws∈Ω(s(w)) P (w s | ¯ w, s( ¯ w); θ v ) ,<label>(8)</label></formula><p>where Ω(s(w)) is the set of words of the same type with w.</p><p>To model the types of context word s( ¯ w) in P (w| ¯ w, s( ¯ w); θ v ), we consider learning a type embedding along with the word embedding by augmenting each word vector with a type vec- <ref type="figure">(s(w)</ref>). In this way, the condi- tional probability P (w| ¯ w, s( ¯ w), s(w), θ v ) can be derived.</p><note type="other">tor when learning the underlying word representa- tion. Specifically, we represent each word w as a vector of [v w (w) T ; v t (s(w)) T ] T , where v w (·) and v t (·) are the word vectors and type vectors learned by the model from the training corpus, respec- tively. Finally, to estimate Eq. (8) using θ v , when computing the Softmax layer, we normalize over only words in Ω</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training and Inference Strategies</head><p>We learn model parameters θ t and θ v indepen- dently by training two language models type model and entity composite model respectively. Given the context of type, type model predicts the type of the next word. Given the context and the type information of the all candidate words, en- tity composite model predicts the conditional ac- tual word (e.g., entity name) as depicted in <ref type="figure" target="#fig_0">Fig  1.</ref> At inference time the generated probabilities from these two models are combined according to conditional probability (i.e., Eq. <ref type="formula" target="#formula_7">(7)</ref>) which gives the final probability distribution over all candidate words <ref type="bibr">6</ref> .</p><p>Our proposed model is flexible to any language model, training strategy, and optimization. As per our experiments, we use ADAM stochastic mini- batch optimization <ref type="bibr" target="#b19">(Kingma and Ba, 2014</ref>). In Al- gorithm 1, we summarize the language generation procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate our proposed model on two different language generation tasks where there exist a lot of entity names in the text. In this paper, we release all the codes and datasets. The first task is recipe generation. For this task, we analyze a cooking recipe corpus. Each instance in this corpus is an individual recipe and consists of many ingredi- 6 While calculating the final probability distribution over all candidate words, with our joint inference schema, a strong state-of-art language model, without the type information, it- self can work sufficiently well and replace the entity com- posite model. Our experiments using ( <ref type="bibr">Merity et al., 2017</ref>) in Section 4.1 validate this claim.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Language Generation</head><p>Input: Language corpus X = {w 1 , w 2 , .., w n }, type s(w) of the words, integer number m. Output: θ t , θ v , {W 1 , W 2 , .., W m } Compute P (w| ¯ w, s( ¯ w), s(w), θ v )</p><note type="other">1 Training Phase: 2 Generate T = { s(w 1 ), s(w 2 ), .., s(w n )} 3 Train type model θ t on T 4 Train entity composite model θ v on X using [w</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>10</head><p>Compute P (w| ¯ w; θ t , θ v ) using Eq. <ref type="formula" target="#formula_7">(7)</ref> 11 end 12 W i ←argmax w P (w| ¯ w; θ t , θ v )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13">end</head><p>ents'. Our second task is code generation. We construct a Java code corpus where each instance is a Java method (i.e., function). These tasks are challenging because they have the abundance of entity names and state-of-the-art language models fail to predict them properly as a result of insuffi- cient training observations. Although in this paper, we manually annotate the types of the recipe in- gredients, in other applications it can be acquired automatically. For example: in our second task of code generation, the types are found using Eclipse JDT framework. In general, using DBpedia ontol- ogy (e.g., "Berlin" has an ontology "Location"), Wordnet hierarchy (e.g., "Dog" is an "Animal"), role in sports (e.g., "Messi" plays in "Forward"; also available in DBpedia 7 ), Thesaurus (e.g., "re- nal cortex", "renal pelvis", "renal vein", all are related to "kidney"), Medscape (e.g., "Advil" and "Motrin" are actually "Ibuprofen"), we can get the necessary type information. As for the applica- tions where the entity types cannot be extracted automatically by these frameworks (e.g., recipe in- gredients), although there is no exact strategy, any reasonable design can work. Heuristically, while annotating manually in our first task, we choose the total number of types in such a way that each type has somewhat balanced (similar) size. We use the same dimensional word embedding (400 for recipe corpus, 300 for code corpus) to represent both of the entity name (e.g., "apple") and their entity type (e.g., "fruits") in all the mod- els. Note that in our approach, the type model only replaces named entities with entity type when it generates next word. If next word is not a named entity, it will behave like a regular language model. Therefore, we set both models with the same dimensionality. Accordingly, for the entity composite model which takes the concatenation of the entity name and the entity type, the concate- nated input dimension is 800 and 600 respectively for recipe and code corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Recipe Generation</head><p>Recipe Corpus Pre-processing: Our recipe cor- pus collection is inspired by <ref type="bibr" target="#b17">(Kiddon et al., 2016)</ref>. We crawl the recipes from "Now Youre Cooking! Recipe Software" 8 . Among more than 150,000 recipes in this dataset, we select similarly struc- tured/formatted (e.g, title, blank line then ingre- dient lists followed by a recipe) 95,786 recipes. We remove all the irrelevant information (e.g., au- thor's name, data source) and keep only two in- formation: ingredients and recipes. We set aside the randomly selected 20% of the recipes for test- ing and from the rest, we keep randomly selected 80% for the training and 20% for the develop- ment. Similar to ( <ref type="bibr" target="#b17">Kiddon et al., 2016)</ref>, we pre- process the dataset and filter out the numerical values, special tokens, punctuation, and symbols. <ref type="bibr">9</ref> Quantitatively, the data we filter out is negligible; in terms of words, we keep 9,994,365 words out of 10,231,106 and the number of filter out words is around ∼2%. We release both of the raw and cleaned data for future challenges. As the ingredi- ents are the entity names in our dataset, we process it separately to get the type information. Retrieving Ingredient Type: As per our type model, for each word w, we require its type s(w). We only consider ingredient type for our experi- ment. First, we tokenize the ingredients and con- sider each word as an ingredient. We manually classify the ingredients into 8 super-ingredients: "fruits", "proteins", "sides", "seasonings", "veg- etables", "dairy", "drinks", and "grains". Some-times, ingredients are expressed using multiple words; for such ingredient phrase, we classify each word in the same group (e.g., for "boneless beef" both "boneless" and "beef" are classified as "proteins"). We classify the most frequent 1,224 unique ingredients, 10 which cover 944,753 out of 1,241,195 mentions (top 76%) in terms of fre- quency of the ingredients. In our experiments, we omit the remainder 14,881 unique ingredients which are less frequent and include some mis- spelled words. The number of unique ingredients in the 8 super ingredients is <ref type="bibr">110,</ref><ref type="bibr">316,</ref><ref type="bibr">140,</ref><ref type="bibr">180,</ref><ref type="bibr">156,</ref><ref type="bibr">80,</ref><ref type="bibr">84</ref>, and 158 respectively. We prepare the modified type corpus by replacing each actual in- gredient's name w in the original recipe corpus by the type (i.e., super ingredients s(w)) to train the type model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recipe Statistics: In our corpus, the total num- ber of distinct words in vocabulary is 52,468; number of unique ingredients (considering split- ting phrasal ingredients also) is 16,105; number of tokens is 8,716,664. In number of instances train/dev/test splits are 61,302/15,326/19,158. The average instance size of a meaningful recipe is 91 on the corpus.</head><p>Configuration: We consider the state-of-the art LSTM-based language model proposed in <ref type="bibr">(Merity et al., 2017)</ref> as the basic component for build- ing the type model, and entity composite model. We use 400 dimensional word embedding as de- scribed in Section 4. We train the embedding for our dataset. We use a minibatch of 20 instances while training and back-propagation through time value is set to 70. Inside of this ( <ref type="bibr">Merity et al., 2017)</ref> language model, it uses 3 layered LSTM architecture where the hidden layers are 1150 di- mensional and has its own optimization and reg- ularization mechanism. All the experiments are done using PyTorch and Python 3.5.</p><p>Baselines: Our first baseline is ASGD Weight- Dropped LSTM (AWD LSTM) ( <ref type="bibr">Merity et al., 2017)</ref>, which we also use to train our models (see 'Configuration' in 4.1). This model achieves the state-of-the-art performance on benchmark Penn Treebank (PTB), and WikiText-2 (WT2) language corpus. Our second baseline is the same language model (AWD LSTM) with the type information added as an additional feature (i.e., same as entity composite model).  Results of Recipe Generation. We compare our model with the baselines using perplexity met- ric-lower perplexity means the better prediction. <ref type="table" target="#tab_4">Table 1</ref> summarizes the result. The 3 rd row shows that adding type as a simple feature does not guarantee a significant performance improvement while our proposed method significantly outper- forms both baselines and achieves 52.2% improve- ment with respect to baseline in terms of perplex- ity. To illustrate more, we provide an example snippet of our test corpus: "place onion and gin- ger inside chicken . allow chicken to marinate for hour .". Here, for the last mention of the word "chicken", the standard language model assigns probability 0.23 to this word, while ours assigns probability 0.81.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Code Generation</head><p>Code Corpus Pre-processing. We crawl 500 Android open source projects from GitHub 11 . GitHub is the largest open source software forge where anyone can contribute ( <ref type="bibr" target="#b24">Ray et al., 2014</ref>). Thus, GitHub also contains trivial projects like student projects, etc. In our case, we want to study the coding practices of practitioners so that our model can learn to generate quality code. To en- sure this, we choose only those Android projects from GitHub that are also present in Google Play Store <ref type="bibr">12</ref> . We download the source code of these projects from GitHub using an off the shelf tool <ref type="bibr">GitcProc (Casalnuovo et al., 2017)</ref>.</p><p>Since real software continuously evolves to cater new requirements or bug fixes, to make our modeling task more realistic, we further study dif-ferent project versions. We partition the codebase of a project into multiple versions based on the code commit history retrieved from GitHub; each version is taken at an interval of 6 months. For example, anything committed within the first six months of a project will be in the first version, and so on. We then build our code suggestion task mimicking how a developer develops code in an evolving software-based on the past project history, developers add new code. To implement that we train our language model on past project versions and test it on the most recent version, at method granularity. However, it is quite difficult for any language model to generate a method from the scratch if the method is so new that even the method signature (i.e., method declaration state- ment consisting of method name and parameters) is not known. Thus, during testing, we only fo- cus on the methods that the model has seen before but some new tokens are added to it. This is simi- lar to the task when a developer edits a method to implement a new feature or bug-fix.</p><p>Since we focus on generating the code for ev- ery method, we train/test the code prediction task at method level-each method is similar to a sen- tence and each token in the method is equivalent to a word. Thus, we ignore the code outside the method scope like global variables, class decla- rations, etc. We further clean our dataset by re- moving user-defined "String" tokens as they in- crease the diversity of the vocabularies signifi- cantly, although having the same type. For ex- ample, the word sequences "Hello World!" and "Good wishes for ACL2018!!" have the same type java.lang.String.VAR.</p><p>Retrieving Token Type: For every token w in a method, we extract its type information s(w). A token type can be Java built-in data types (e.g., int, double, float, boolean etc.,) or user or framework defined classes (e.g., java.lang.String, io.segment.android.flush.FlushThread etc.). We extract such type information for each token by parsing the Abstract Syntax Tree (AST) of the source code <ref type="bibr">13</ref> . We extract the AST type infor- mation of each token using Eclipse JDT frame- work <ref type="bibr">14</ref> . Note that, language keywords like for, if, etc. are not associated with any type. Next, we prepare the type corpus by replacing the variable names with corresponding type informa- tion. For instance, if variable var is of type java.lang.Integer, in the type corpus we replace var by java.lang.Integer. Since multiple packages might contain classes of the same name, we retain the fully qualified name for each type <ref type="bibr">15</ref> .</p><p>Code Corpus Statistics: In our corpus, the total number of distinct words in vocabulary is 38,297; the number of unique AST type (including all user-defined classes) is 14,177; the number of tokens is 1,440,993. The number of instances used for train and testing is 26,600 and 3,546. Among these 38,297 vocabulary words, 37,411 are seen at training time while the rests are new.</p><p>Configuration: To train both type model and entity composite model, we use forward and back- ward LSTM (See Section 2) and combine them at the inference/generation time. We train 300- dimensional word embedding for each token as described in Section 4 initialized by GLOVE <ref type="bibr">(Pennington et al., 2014</ref>). Our LSTM is single lay- ered and the hidden size is 300. We implement our model on using PyTorch and Python 3.5. Our training corpus size 26,600 and we do not split it further into smaller train and development set; rather we use them all to train for one single epoch and record the result on the test set.</p><p>Baselines: Our first baseline is standard LSTM language model which we also use to train our modules (see 'Configuration' in 4.2). Similar to our second baseline for recipe generation we also consider LSTM with the type information added as more features <ref type="bibr">16</ref> as our another baseline. We further compare our model with state-of-the-art token-based language model for source code SLP- Core ( <ref type="bibr" target="#b12">Hellendoorn and Devanbu, 2017)</ref>.</p><p>Results of Code Generation: <ref type="table" target="#tab_6">Table 2</ref> shows that adding type as simple features does not guarantee a significant performance improvement while our proposed method significantly outper- forms both forward and backward LSTM base- lines. Our approach with backward LSTM has 40.3% better perplexity than original backward LSTM and forward has 63.14% lower (i.e., bet- ter) perplexity than original forward LSTM. With respect to SLP-Core performance, our model is 22.06% better in perplexity. We compare our model with SLP-Core details in case study-2. <ref type="bibr">15</ref> Also the AST type of a very same variable may differ in two different methods. Hence, the context is limited to each method. <ref type="bibr">16</ref> LSTM with type is same as entity composite model.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Quantitative Error Analysis</head><p>To understand the generation performance of our model and interpret the meaning of the numbers in <ref type="table" target="#tab_4">Table 1</ref> and 2, we further perform the following case studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Case Study-1: Recipe Generation</head><p>As the reduction of the perplexity does not neces- sarily mean the improvement of the accuracy, we design a "fill in the blank task" task to evaluate our model. A blank place in this task will contain an ingredient and we check whether our model can predict it correctly. In particular, we choose six ingredients from different frequency range (low, mid, high) based on how many times they have appeared in the training corpus. Following <ref type="table">Table  shows</ref> two examples with four blanks (underlined with the true answer).</p><p>Example fill in the blank task</p><p>1. Sprinkle chicken pieces lightly with salt.</p><p>2. Mix egg and milk and pour over bread.</p><p>We further evaluate our model on a multiple choice questioning (MCQ) strategy where the fill in the blank problem remains same but the options for the correct answers are restricted to the six in- gredients. Our intuition behind this case-study is to check when there is an ingredient whether our model can learn it. If yes, we then quantify the learning using standard accuracy metric and com- pare with the state-of-the-art model to evaluate how much it improves the performance. We also measure how much the accuracy improvement de- pends on the training frequency.  i.e., without any options (free-form) and MCQ. Note that, the percentage of improvement is in- versely proportional to the training frequencies of the ingredients-less-frequent ingredients achieve a higher accuracy improvement (e.g., "Apple" and "Tomato"). This validates our intuition of learning to predict the type first more accurately with lower vocabulary set and then use conditional probabil- ity to predict the actual entity considering the type as a prior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Case Study-2: Code Generation</head><p>Programming language source code shows regu- larities both in local and global context (e.g., vari- ables or methods used in one source file can also be created or referenced from another library file). SLP-Core ( <ref type="bibr" target="#b12">Hellendoorn and Devanbu, 2017</ref>) is a state-of-the-art code generation model that cap- tures this global and local information using a nested cache based n-gram language model. They further show that considering such code structure into account, a simple n-gram based SLP-Core outperforms vanilla deep learning based models like RNN, LSTM, etc. In our case, as our example instance is a Java method, we only have the local context. There- fore, to evaluate the efficiency of our proposed model, we further analyze that exploiting only the type information are we even learning any global code pattern? If yes, then how much in compar- ison to the baseline (SLP-Core)? To investigate these questions, we provide all the full project information to SLP-Core ( <ref type="bibr" target="#b12">Hellendoorn and Devanbu, 2017</ref>) corresponding to our train set. How- ever, at test-time, to establish a fair comparison, we consider the perplexity metric for the same methods. SLP-Core achieves a perplexity 3.40 where our backward LSTM achieves 2.65. This result shows that appropriate type information can actually capture many inherent attributes which can be exploited to build a good language model for programming language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Language model often lacks in performance to predict entity names correctly. Applications with lots of named entities, thus, obviously suffer. In this work, we propose to leverage the type infor- mation of such named entities to build an effective language model. Since similar entities have the same type, the vocabulary size of a type based lan- guage model reduces significantly. The prediction accuracy of the type model increases significantly with such reduced vocabulary size. Then, using the entity type information as prior we build an- other language model which predicts the true en- tity name according to the conditional probability distribution. Our evaluation and case studies con- firm that the type information of the named entities captures inherent text features too which leads to learn intrinsic text pattern and improve the perfor- mance of overall language model. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example illustrates the proposed model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>For</head><label></label><figDesc>a given context (i.e., types of context words as input), the type model (in bottom red block) generates the type of the next word (i.e., the probability of the type of the next word as output). Further, for a given context and type of each candidate (i.e., context words, correspond- ing types of the context words, and type of the next word generated by the type model as input), the entity compos- ite model (in upper green block) predicts the next word (actual entity name) by estimating the conditional proba- bility of the next word as output. The proposed approach conducts joint inference over both models to leverage type information for generating text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Comparing the performance of recipe gen-

eration task. All the results are on the test set of the 

corresponding corpus. AWD LSTM (type model) is our 

type model implemented with the baseline language model 

AWD LSTM (Merity et al., 2017). Our second baseline is 

the same language model (AWD LSTM) with the type in-

formation added as an additional feature for each word. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Comparing the performance of code genera-

tion task. All the results are on the test set of the corre-

sponding corpus. fLSTM, bLSTM denotes forward and 

backward LSTM respectively. SLP-Core refers to (Hel-

lendoorn and Devanbu, 2017). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 3 shows the result. Our model outper- forms the fill in the blank task for both cases,</head><label>3</label><figDesc></figDesc><table>Accuracy 
Ingredient Train Freq. #Blanks 
Free-Form 
MCQ 
AWD LSTM Our AWD LSTM Our 

Milk 
14, 136 
4,001 
26.94 
59.34 80.83 
94.90 
Salt 
33,906 
9,888 
37.12 
62.47 89.29 
95.75 
Apple 
7,205 
720 
1.94 
30.28 37.65 
89.86 
Bread 
11,673 
3,074 
32.43 
52.64 78.85 
94.53 
Tomato 12,866 
1,815 
2.20 
35.76 43.53 
88.76 
Chicken 19,875 
6,072 
22.50 
45.24 77.70 
94.63 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Performance of fill in the blank task. 

</table></figure>

			<note place="foot" n="1"> Entity type information is often referred as category information or group information. In many applications, such information can be easily obtained by an ontology or by a pre-constructed entity table. 2 Data is crawled from http://www.ffts.com/ recipes.htm.</note>

			<note place="foot" n="3"> https://github.com/uclanlp/NamedEntityLanguageModel</note>

			<note place="foot" n="4"> Empirically for the non-entity words, s∈S P (s(w) = s|s( ¯ w) ≈ 0 5 In a preliminary experiment, we consider putting all words with s(w) ∈ S in a category &quot;N/A&quot;. However, because most words on the training corpus are not named entities, the type &quot;N/A&quot; dominates others and hinder the type model to make accurate predictions.</note>

			<note place="foot" n="7"> http://dbpedia.org/page/Lionel Messi</note>

			<note place="foot" n="8"> http://www.ffts.com/recipes.htm 9 For example, in our crawled raw dataset, we find that some recipes have lines like &quot;===MMMMM===&quot; which are totally irrelevant to our task. For the words with numerical values like &quot;100 ml&quot;, we only remove the &quot;100&quot; and keep the &quot;ml&quot; since our focus is not to predict the exact number.</note>

			<note place="foot" n="10"> We consider both singular and plural forms. The number of singular formed annotated ingredients are 797.</note>

			<note place="foot" n="11"> https://github.com 12 https://play.google.com/store?hl=en</note>

			<note place="foot" n="13"> AST represents source code as a tree by capturing its abstract syntactic structure, where each node represents a construct in the source code. 14 https://www.eclipse.org/jdt/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their in-sightful comments. We also thank Wasi Ud-din Ahmad, Peter Kim, Shou-De Lin, and Paul Mineiro for helping us implement, annotate, and design the experiments. This work was supported in part by National Science Foundation <ref type="bibr">Grants IIS1760523, CCF-16-19123, CNS-16-18771</ref> and an NVIDIA hardware grant.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A survey of machine learning for big code and naturalness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Earl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Premkumar</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Devanbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.06182</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Counterfactual language model adaptation for suggesting phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><forename type="middle">C</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kalai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="49" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Distributional clustering of words for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Kachites</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 21st annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="96" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An improved error model for noisy channel spelling correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert C</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 38th Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="286" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Class-based n-gram models of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter F Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent J Della</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenifer C</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="479" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Safe type checking in a statically-typed object-oriented programming language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim B Bruce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGPLANSIGACT symposium on Principles of programming languages</title>
		<meeting>the 20th ACM SIGPLANSIGACT symposium on Principles of programming languages</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page" from="285" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Gitcproc: a tool for processing and classifying github commits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Casey</forename><surname>Casalnuovo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yagnik</forename><surname>Suchak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baishakhi</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cindy</forename><surname>Rubio-González</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ACM</publisher>
			<biblScope unit="page" from="396" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A study of the uniqueness of source code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Gabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering</title>
		<meeting>the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="147" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.06863</idno>
		<title level="m">Chris Quirk, Margaret Mitchell, Jianfeng Gao, and Bill Dolan. 2015. deltableu: A discriminative metric for generation tasks with intrinsically diverse targets</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Some from here, some from there: cross-project code reuse in github</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Gharehyazie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baishakhi</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Filkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Conference on Mining Software Repositories</title>
		<meeting>the 14th International Conference on Mining Software Repositories</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="291" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Classes for fast maximum entropy training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Goodman</surname></persName>
		</author>
		<idno>cs.CL/0108006</idno>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">O K</forename><surname>Li</surname></persName>
		</author>
		<idno>CoRR abs/1603.06393</idno>
		<ptr target="http://arxiv.org/abs/1603.06393" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Are deep neural networks the best choice for modeling source code?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Premkumar</forename><surname>Hellendoorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Devanbu</surname></persName>
		</author>
		<idno type="doi">10.1145/3106237.3106290</idno>
		<ptr target="https://doi.org/10.1145/3106237.3106290" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering</title>
		<meeting>the 2017 11th Joint Meeting on Foundations of Software Engineering<address><addrLine>New York, NY, USA, ESEC</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="763" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On the naturalness of software</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abram</forename><surname>Hindle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Earl</forename><forename type="middle">T</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Gabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Premkumar</forename><surname>Devanbu</surname></persName>
		</author>
		<idno type="doi">10.1145/2902362</idno>
		<ptr target="https://doi.org/10.1145/2902362" />
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="122" to="131" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On the naturalness of software</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abram</forename><surname>Hindle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Earl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Premkumar</forename><surname>Gabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Devanbu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Software Engineering (ICSE), 2012 34th International Conference on. IEEE</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="837" to="847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Martschat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.00781</idno>
		<title level="m">Dynamic entity representations in neural language models</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Exploring the limits of language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02410</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Globally coherent text generation with neural checklist models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloé</forename><surname>Kiddon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="329" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An empirical study of code clone genealogies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miryung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibha</forename><surname>Sazawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Notkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gail</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGSOFT Software Engineering Notes</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="187" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>CoRR abs/1412.6980</idno>
		<ptr target="http://arxiv.org/abs/1412.6980" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Literate programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Donald E Knuth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Center for the Study of Language and Information (CSLI), 1992. Conference on Empirical Methods in Natural Language Processing</title>
		<meeting><address><addrLine>Stanford, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distributional clustering of english words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st annual meeting on Association for Computational Linguistics</title>
		<meeting>the 31st annual meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page" from="183" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Abstract syntax networks for code generation and semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno>CoRR abs/1704.07535</idno>
		<ptr target="http://arxiv.org/abs/1704.07535" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The uniqueness of changes: Characteristics and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baishakhi</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meiyappan</forename><surname>Nagappan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nachiappan</forename><surname>Nagappan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Zimmermann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ACM</publisher>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A large scale study of programming languages and code quality in github</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baishakhi</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daryl</forename><surname>Posnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Filkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Premkumar</forename><surname>Devanbu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering</title>
		<meeting>the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="155" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Code completion with statistical language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Raychev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Vechev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acm Sigplan Notices</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="419" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On the localness of software</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Premkumar</forename><surname>Devanbu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering</title>
		<meeting>the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="269" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5866-pointer-networks.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Challenges in data-todocument generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><forename type="middle">M</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno>CoRR abs/1707.08052</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A syntactic neural model for general-purpose code generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno>CoRR abs/1704.01696</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
