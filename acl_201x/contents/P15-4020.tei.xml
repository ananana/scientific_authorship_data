<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:07+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-level Translation Quality Prediction with QUEST++</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 26-31, 2015. c 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><forename type="middle">Henrique</forename><surname>Paetzold</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolina</forename><surname>Scarton</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-level Translation Quality Prediction with QUEST++</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of ACL-IJCNLP 2015 System Demonstrations</title>
						<meeting>ACL-IJCNLP 2015 System Demonstrations <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="115" to="120"/>
							<date type="published">July 26-31, 2015. c 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper presents QUEST++ , an open source tool for quality estimation which can predict quality for texts at word, sentence and document level. It also provides pipelined processing, whereby predictions made at a lower level (e.g. for words) can be used as input to build models for predictions at a higher level (e.g. sentences). QUEST++ allows the extraction of a variety of features, and provides machine learning algorithms to build and test quality estimation models. Results on recent datasets show that QUEST++ achieves state-of-the-art performance.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Quality Estimation (QE) of Machine Translation (MT) have become increasingly popular over the last decade. With the goal of providing a predic- tion on the quality of a machine translated text, QE systems have the potential to make MT more use- ful in a number of scenarios, for example, improv- ing post-editing efficiency <ref type="bibr" target="#b16">(Specia, 2011)</ref>, select- ing high quality segments <ref type="bibr" target="#b14">(Soricut and Echihabi, 2010)</ref>, selecting the best translation <ref type="bibr" target="#b13">(Shah and Specia, 2014</ref>), and highlighting words or phrases that need revision ( <ref type="bibr" target="#b0">Bach et al., 2011</ref>).</p><p>Most recent work focuses on sentence-level QE. This variant is addressed as a supervised machine learning task using a variety of algorithms to in- duce models from examples of sentence transla- tions annotated with quality labels (e.g. 1-5 likert scores). Sentence-level QE has been covered in shared tasks organised by the Workshop on Statis- tical Machine Translation (WMT) annually since 2012. While standard algorithms can be used to build prediction models, key to this task is work of feature engineering. Two open source feature extraction toolkits are available for that: ASIYA <ref type="bibr">1</ref> and QUEST 2 ( ). The latter has been used as the official baseline for the WMT shared tasks and extended by a number of partic- ipants, leading to improved results over the years <ref type="bibr" target="#b5">(Callison-Burch et al., 2012;</ref><ref type="bibr" target="#b3">Bojar et al., 2013;</ref><ref type="bibr" target="#b4">Bojar et al., 2014</ref>).</p><p>QE at other textual levels have received much less attention. Word-level QE ( <ref type="bibr" target="#b2">Blatz et al., 2004;</ref><ref type="bibr" target="#b7">Luong et al., 2014</ref>) is seemingly a more challeng- ing task where a quality label is to be produced for each target word. An additional challenge is the acquisition of sizable training sets. Although significant efforts have been made, there is con- siderable room for improvement. In fact, most WMT13-14 QE shared task submissions were un- able to beat a trivial baseline.</p><p>Document-level QE consists in predicting a sin- gle label for entire documents, be it an absolute score <ref type="bibr" target="#b11">(Scarton and Specia, 2014</ref>) or a relative ranking of translations by one or more MT sys- tems <ref type="bibr" target="#b14">(Soricut and Echihabi, 2010)</ref>. While certain sentences are perfect in isolation, their combina- tion in context may lead to an incoherent docu- ment. Conversely, while a sentence can be poor in isolation, when put in context, it may benefit from information in surrounding sentences, leading to a good quality document. Feature engineering is a challenge given the little availability of tools to extract discourse-wide information. In addition, no datasets with human-created labels are avail- able and thus scores produced by automatic met- rics have to be used as approximation <ref type="bibr" target="#b12">(Scarton et al., 2015)</ref>.</p><p>Some applications require fine-grained, word- level information on quality. For example, one may want to highlight words that need fixing. Document-level QE is needed particularly for gist- ing purposes where post-editing is not an option.</p><p>For example, for predictions on translations of product reviews in order to decide whether or not they are understandable by readers. We believe that the limited progress in word and document- level QE research is partially due to lack of a basic framework that one can be build upon and extend.</p><p>QUEST++ is a significantly refactored and expanded version of an existing open source sentence-level toolkit, QUEST. Feature extrac- tion modules for both word and document-level QE were added and the three levels of prediction were unified into a single pipeline, allowing for in- teractions between word, sentence and document- level QE. For example, word-level predictions can be used as features for sentence-level QE. Finally, sequence-labelling learning algorithms for word- level QE were added. QUEST++ can be easily ex- tended with new features at any textual level. The architecture of the system is described in Section 2. Its main component, the feature extractor, is presented in Section 3. Section 4 presents experi- ments using the framework with various datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Architecture</head><p>QUEST++ has two main modules: a feature ex- traction module and a machine learning module. The first module is implemented in Java and pro- vides a number of feature extractors, as well as abstract classes for features, resources and pre- processing steps so that extractors for new fea- tures can be easily added. The basic functioning of the feature extraction module requires raw text files with the source and translation texts, and a few resources (where available) such as the MT source training corpus and source and target lan- guage models (LMs). Configuration files are used to indicate paths for resources and the features that should be extracted. For its main resources (e.g. LMs), if a resource is missing, QUEST++ can gen- erate it automatically. <ref type="figure">Figure 1</ref> depicts the architecture of QUEST++ . Document and Paragraph classes are used for document-level feature extraction. A Document is a group of Paragraphs, which in turn is a group of Sentences. Sentence is used for both word-and sentence-level feature extraction. A Feature Pro- cessing Module was created for each level. Each processing level is independent and can deal with the peculiarities of its type of feature.</p><p>Machine learning QUEST++ provides scripts to interface the Python toolkit scikit-learn 3 (Pedregosa et al., ). This module is indepen- dent from the feature extraction code and uses the extracted feature sets to build and test QE models. The module can be configured to run different regression and classification algorithms, feature selection methods and grid search for hyper-parameter optimisation. Algorithms from scikit-learn can be easily integrated by modifying existing scripts.</p><p>For word-level prediction, QUEST++ provides an interface for CRFSuite <ref type="bibr" target="#b8">(Okazaki, 2007)</ref>, a se- quence labelling C++ library for Conditional Ran- dom Fields (CRF). One can configure CRFSuite training settings, produce models and test them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Features</head><p>Features in QUEST++ can be extracted from either source or target (or both) sides of the corpus at a given textual level. In order describe the features supported, we denote:</p><p>• S and T the source and target documents, • s and t for source and target sentences, and • s and t for source and target words.</p><p>We concentrate on MT system-independent (black-box) features, which are extracted based on the output of the MT system rather than any of its internal representations. These allow for more flexible experiments and comparisons across MT systems. System-dependent features can be ex- tracted as long they are represented using a pre- defined XML scheme. Most of the existing fea- tures are either language-independent or depend on linguistic resources such as POS taggers. The latter can be extracted for any language, as long as the resource is available. For a pipelined ap- proach, predictions at a given level can become features for higher level model, e.g. features based on word-level predictions for sentence-level QE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Word level</head><p>We explore a range of features from recent work ( <ref type="bibr" target="#b1">Bicici and Way, 2014;</ref><ref type="bibr" target="#b6">Camargo de Souza et al., 2014;</ref><ref type="bibr" target="#b7">Luong et al., 2014;</ref><ref type="bibr" target="#b17">Wisniewski et al., 2014</ref>), totalling 40 features of seven types:</p><p>Target context These are features that explore the context of the target word. Given a word t i in position i of a target sentence, we extract: t i , <ref type="figure">Figure 1</ref>: Architecture of QUEST++ i.e., the word itself, bigrams t i−1 t i and t i t i+1 , and trigrams t i−2 t i−1 t i , t i−1 t i t i+1 and t i t i+1 t i+2 .</p><p>Alignment context These features explore the word alignment between source and target sen- tences. They require the 1-to-N alignment be- tween source and target sentences to be provided. Given a word t i in position i of a target sentence and a word s j aligned to it in position j of a source sentence, the features are: the aligned word s j it- self, target-source bigrams s j−1 t i and t i s j+1 , and source-target bigrams t i−2 s j , t i−1 s j , s j t i+1 and s j t i+2 .</p><p>Lexical These features explore POS informa- tion on the source and target words. Given the POS tag P t i of word t i in position i of a target sentence and the POS tag P s j of word s j aligned to it in position j of a source sen- tence, we extract: the POS tags P t i and P s j themselves, the bigrams P t i−1 P t i and P t i P t i+1 and trigrams P t i−2 P t i−1 P t i , P t i−1 P t i P t i+1 and P t i P t i+1 P t i+2 . Four binary features are also ex- tracted with value 1 if t i is a stop word, punctua- tion symbol, proper noun or numeral.</p><p>LM These features are related to the n-gram fre- quencies of a word's context with respect to an LM (Raybaud et al., 2011). Six features are extracted: lexical and syntactic backoff behavior, as well as lexical and syntactic longest preceding n-gram for both a target word and an aligned source word. Given a word t i in position i of a target sentence, the lexical backoff behavior is calculated as:</p><formula xml:id="formula_0">f (t i ) =                         </formula><p>7 if t i−2 , t i−1 , t i exists 6 if t i−2 , t i−1 and t i−1 , t i exist 5 if only t i−1 , t i exists 4 if t i−2 , t i−1 and t i exist 3 if t i−1 and t i exist 2 if t i exists 1 if t i is out of the vocabulary</p><p>The syntactic backoff behavior is calculated in an analogous fashion: it verifies for the existence of n-grams of POS tags in a POS-tagged LM. The POS tags of target sentence are produced by the Stanford Parser 4 (integrated in QUEST++ ).</p><p>Syntactic QUEST++ provides one syntactic fea- ture that proved very promising in previous work: the Null Link ( <ref type="bibr" target="#b19">Xiong et al., 2010)</ref>. It is a binary feature that receives value 1 if a given word t i in a target sentence has at least one dependency link with another word t j , and 0 otherwise. The Stan- ford Parser is used for dependency parsing.</p><p>Semantic These features explore the polysemy of target and source words, i.e. the number of senses existing as entries in a WordNet for a given target word t i or a source word s i . We employ the Universal WordNet, <ref type="bibr">5</ref> which provides access to WordNets of various languages.</p><p>Pseudo-reference This binary feature explores the similarity between the target sentence and a translation for the source sentence produced by an- other MT system. The feature is 1 if the given word t i in position i of a target sentence S is also present in a pseudo-reference translation R. In our experiments, the pseudo-reference is produced by Moses systems trained over parallel corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sentence level</head><p>Sentence-level QE features have been extensively explored and described in previous work. The number of QUEST++ features varies from 80 to 123 depending on the language pair. The complete list is given as part of QUEST++ 's documentation. Some examples are:</p><p>• In our experiments, we use the set of 80 fea- tures, as these can be extracted for all language pairs of our datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Document level</head><p>Our document-level features follow from those in the work of <ref type="bibr" target="#b18">(Wong and Kit, 2012)</ref> on MT evalua- tion and ( <ref type="bibr" target="#b11">Scarton and Specia, 2014</ref>) for document- level QE. Nine features are extracted, in addition to aggregated values of sentence-level features for the entire document:</p><p>• content words/lemmas/nouns repetition in S/T , • ratio of content words/lemmas/nouns in S/T ,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In what follows, we evaluate QUEST++'s perfor- mance for the three prediction levels and various datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Word-level QE</head><p>Datasets We use five word-level QE datasets: the WMT14 English-Spanish, Spanish-English, English-German and German-English datasets, and the WMT15 English-Spanish dataset.</p><p>Metrics For the WMT14 data, we evaluate per- formance in the three official classification tasks:</p><p>• Binary: A Good/Bad label, where Bad indi- cates the need for editing the token.</p><p>• Level 1: A Good/Accuracy/Fluency label, specifying the coarser level categories of er- rors for each token, or Good for tokens with no error.</p><p>• Multi-Class: One of 16 labels specifying the error type for the token (mistranslation, miss- ing word, etc.). The evaluation metric is the average F-1 of all but the Good class. For the WMT15 dataset, we consider only the Binary classification task, since the dataset does not provide other annotations.</p><p>Settings For all datasets, the models were trained with the CRF module in QUEST++ . While for the WMT14 German-English dataset we use the Passive Aggressive learning algorithm, for the remaining datasets, we use the Adaptive Reg- ularization of Weight Vector (AROW) learning. Through experimentation, we found that this setup to be the most effective. The hyper-parameters for each model were optimised through 10-fold cross validation. The baseline is the majority class in the training data, i.e. a system that always pre- dicts "Unintelligible" for Multi-Class, "Fluency" for Level 1, and "Bad" for the Binary setup.</p><p>Results The F-1 scores for the WMT14 datasets are given in <ref type="table" target="#tab_1">Tables 1-4</ref>, for QUEST++ and sys- tems that oficially participated in the task. The re- sults show that QUEST++ was able to outperform all participating systems in WMT14 except for the English-Spanish baseline in the Binary and Level 1 tasks. The results in <ref type="table" target="#tab_6">Table 5</ref> also highlight the importance of selecting an adequate learning al- gorithm in CRF models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>Binary Level    Dataset We use the WMT15 dataset for word- level QE. The split between training and test sets was modified to allow for more sentences for train- ing the sentence-level QE model. The 2000 last sentences of the original training set were used as test along with the original 1000 dev set sen- tences. Therefore, word predictions were gener- ated for 3000 sentences, which were later split in 2000 sentences for training and 1000 sentences for testing the sentence-level model. Oracle word level labels, as given in the original dataset, are also used in a separate experiment to study the potential of this pipelined approach.</p><p>Settings For learning sentence-level models, the SVR algorithm with RBF kernel and hyperparam- eters optimised via grid search in QUEST++ is used. Evaluation is done using MAE (Mean Ab- solute Error) as metric.</p><p>Results As shown in <ref type="table">Table 6</ref>, the use of word- level predictions as features led to no improve- ment. However, the use of the oracle word-level labels as features substantially improved the re- sults, lowering the baseline error by half. We note that the method used in this experiments is the same as that in Section 4.1, but with fewer in- stances for training the word-level models. Im-    <ref type="table">Table 6</ref>: MAE values for sentence-level QE</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Pipeline for document-level QE</head><p>Here we evaluate the pipeline of using sentence- level predictions as features for QE of documents.</p><p>Dataset For training the sentence-level model, we use the English-Spanish WMT13 training set for sentence-level QE. For the document-level model, we use English-Spanish WMT13 data from the translation shared task. We mixed the outputs of all MT systems, leading to 934 trans- lated documents. 560 randomly selected docu- ments were used for training and 374 for test- ing. As quality labels, for sentence-level training we consider both the HTER and the Likert labels available. For document-level prediction, BLEU, TER and METEOR are used as quality labels (not as features), given the lack of human-target quality labels for document-level prediction.</p><p>Features The 17 QUEST++ baseline features are aggregated to produce document-level fea- tures (Baseline). These are then combined with document-level features (Section 3.3) and finally with features from sentence-level predictions:</p><p>• maximum/minimum predicted HTER or Lik- ert score, • average predicted HTER or Likert score, • Median, first quartile and third quartile pre- dicted HTER or Likert score. Oracle sentence labels are not possible as they do not exist for the test set documents.</p><p>Settings For training and evaluation, we use the same settings as for sentence-level.</p><p>Results <ref type="table" target="#tab_8">Table 7</ref> shows the results in terms of MAE. The best result was achieved with the baseline plus HTER features, but no significant improvements over the baseline were observed. Document-level prediction is a very challenging task: automatic metric scores used as labels do not seem to reliably distinguish translations of dif- ferent source documents, since they were primar- ily designed to compare alternative translations for the same source document.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BLEU</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Remarks</head><p>The source code for the framework, the datasets and extra resources can be downloaded from https://github.com/ghpaetzold/ questplusplus. The license for the Java code, Python and shell scripts is BSD, a permissive license with no restrictions on the use or extensions of the software for any purposes, including commer- cial. For pre-existing code and resources, e.g., scikit-learn, their licenses apply.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>number of tokens in s &amp; t and their ratio, • LM probability of s &amp; t, • ratio of punctuation symbols in s &amp; t, • ratio of percentage of numbers, content-/non- content words, nouns/verbs/etc in s &amp; t, • proportion of dependency relations between (aligned) constituents in s &amp; t, • difference in depth of syntactic trees of s &amp; t.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Features</head><label></label><figDesc>The 17 QUEST++ baseline features are used alone (Baseline) and in combination with four word-level prediction features: • count &amp; proportion of Good words, • count &amp; proportion of Bad words.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>F-1 for the WMT14 English-Spanish task 

4.2 Pipeline for sentence-level QE 

Here we evaluate the pipeline of using word-level 
predictions as features for sentence-level QE. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 : F-1 for the WMT14 Spanish-English task</head><label>2</label><figDesc></figDesc><table>System 
Binary Level 1 Multiclass 
QUEST++ 0.507 
0.287 
0.161 
Baseline 
0.445 
0.117 
0.086 
RTM-1 
0.452 
0.211 
0.150 
RTM-2 
0.369 
0.219 
0.124 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>F-1 for the WMT14 English-German task 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 : F-1 for the WMT14 German-English task</head><label>4</label><figDesc></figDesc><table>Algorithm Binary 
AROW 
0.379 
PA 
0.352 
LBFGS 
0.001 
L2SGD 
0.000 
AP 
0.000 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><head>Table 5 : F-1 for the WMT15 English-Spanish task</head><label>5</label><figDesc></figDesc><table>proving word-level prediction could thus lead to 
better results in the pipeline for sentence-level QE. 

MAE 
Baseline 
0.159 
Baseline+Predicted 0.158 
Baseline+Oracle 
0.07 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 7 : MAE values for document-level QE</head><label>7</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> http://nlp.lsi.upc.edu/asiya/ 2 http://www.quest.dcs.shef.ac.uk/</note>

			<note place="foot" n="3"> http://scikit-learn.org/</note>

			<note place="foot" n="4"> http://nlp.stanford.edu/software/ lex-parser.shtml 5 http://www.lexvo.org/uwn/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the European Associ-ation for Machine Translation, the QT21 project (H2020 No. 645452) and the EXPERT project (EU Marie Curie ITN No. 317471).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Goodness: a method for measuring MT confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Al-Onaizan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Referential Translation Machines for Predicting Translation Quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bicici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Way</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WMT14</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Confidence Estimation for Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blatz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gandrabur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Goutte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanchis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ueffing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING04</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Specia</surname></persName>
		</author>
		<title level="m">Findings of the 2013 Workshop on SMT</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>WMT13</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leveling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pecina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Saint-Amand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tamchyna</surname></persName>
		</author>
		<title level="m">Findings of the 2014 Workshop on SMT</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>WMT14</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Specia</surname></persName>
		</author>
		<title level="m">Findings of the 2012 Workshop on SMT</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>WMT12</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">FBK-UPVUEdin participation in the WMT14 Quality Estimation shared-task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Camargo De Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>González-Rubio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Turchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Negri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WMT14</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">LIG System for Word Level QE task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">Q</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Besacier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lecouteux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WMT14</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">CRFsuite: a fast implementation of Conditional Random Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Okazaki</surname></persName>
		</author>
		<ptr target="http://www.chokkan.org/software/crfsuite/" />
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">This sentence is wrong. Detecting errors in machinetranslated sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Raybaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Langlois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Smali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Translation</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Document-level translation quality estimation: exploring discourse and pseudo-references</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scarton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Specia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>In EAMT14</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Searching for Context: a Study on Document-Level Labels for Translation Quality Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scarton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zampieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Genabith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EAMT15</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Quality estimation for translation selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EAMT14</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Trustrank: Inducing trust in automatic translations via ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Echihabi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Quest-a translation quality estimation framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G C</forename><surname>De Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL13</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Exploiting objective annotations for measuring translation post-editing effort</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Specia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">LIMSI Submission for WMT&apos;14 QE Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wisniewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pcheux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yvon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WMT14</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Extending machine translation evaluation metrics with lexical cohesion to document level</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T M</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kit</surname></persName>
		</author>
		<editor>EMNLP/CONLL</editor>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Error detection for SMT using linguistic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
