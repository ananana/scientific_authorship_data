<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:03+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Relational Question Answering from Narratives: Machine Reading and Reasoning in Simulated Worlds</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Labutov</surname></persName>
							<email>ilabutov@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Machine Learning Dept</orgName>
								<orgName type="department" key="dep2">Computer Science Dept</orgName>
								<orgName type="laboratory">Language Technologies Inst. Carnegie Mellon University Pittsburgh</orgName>
								<orgName type="institution" key="instit1">Carnegie Mellon University Pittsburgh</orgName>
								<orgName type="institution" key="instit2">Ariel University</orgName>
								<address>
									<postCode>15213, 15213</postCode>
									<region>PA, PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
							<email>bishan@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Machine Learning Dept</orgName>
								<orgName type="department" key="dep2">Computer Science Dept</orgName>
								<orgName type="laboratory">Language Technologies Inst. Carnegie Mellon University Pittsburgh</orgName>
								<orgName type="institution" key="instit1">Carnegie Mellon University Pittsburgh</orgName>
								<orgName type="institution" key="instit2">Ariel University</orgName>
								<address>
									<postCode>15213, 15213</postCode>
									<region>PA, PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anusha</forename><surname>Prakash</surname></persName>
							<email>anushap@andrew.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Machine Learning Dept</orgName>
								<orgName type="department" key="dep2">Computer Science Dept</orgName>
								<orgName type="laboratory">Language Technologies Inst. Carnegie Mellon University Pittsburgh</orgName>
								<orgName type="institution" key="instit1">Carnegie Mellon University Pittsburgh</orgName>
								<orgName type="institution" key="instit2">Ariel University</orgName>
								<address>
									<postCode>15213, 15213</postCode>
									<region>PA, PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Azaria</surname></persName>
							<email>Israel amos.azaria@ariel.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Machine Learning Dept</orgName>
								<orgName type="department" key="dep2">Computer Science Dept</orgName>
								<orgName type="laboratory">Language Technologies Inst. Carnegie Mellon University Pittsburgh</orgName>
								<orgName type="institution" key="instit1">Carnegie Mellon University Pittsburgh</orgName>
								<orgName type="institution" key="instit2">Ariel University</orgName>
								<address>
									<postCode>15213, 15213</postCode>
									<region>PA, PA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Relational Question Answering from Narratives: Machine Reading and Reasoning in Simulated Worlds</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="833" to="844"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>833</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Question Answering (QA), as a research field, has primarily focused on either knowledge bases (KBs) or free text as a source of knowledge. These two sources have historically shaped the kinds of questions that are asked over these sources, and the methods developed to answer them. In this work, we look towards a practical use-case of QA over user-instructed knowledge that uniquely combines elements of both structured QA over knowledge bases, and unstructured QA over narrative, introducing the task of multi-relational QA over personal narrative. As a first step towards this goal, we make three key contributions: (i) we generate and release TEXTWORLDSQA, a set of five diverse datasets, where each dataset contains dynamic narrative that describes entities and relations in a simulated world, paired with variably compositional questions over that knowledge, (ii) we perform a thorough evaluation and analysis of several state-of-the-art QA models and their variants at this task, and (iii) we release a lightweight Python-based framework we call TEXTWORLDS for easily generating arbitrary additional worlds and narrative, with the goal of allowing the community to create and share a growing collection of diverse worlds as a test-bed for this task.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Personal devices that interact with users via nat- ural language conversation are becoming ubiqui- tous (e.g., Siri, Alexa), however, very little of that conversation today allows the user to teach, and then query, new knowledge. Most of the focus in Figure 1: Illustration of our task: relational question answering from dynamic knowledge ex- pressed via personal narrative these personal devices has been on Question An- swering (QA) over general world-knowledge (e.g., "who was the president in 1980" or "how many ounces are in a cup"). These devices open a new and exciting possibility of enabling end-users to teach machines in natural language, e.g., by ex- pressing the state of their personal world to its vir- tual assistant (e.g., via narrative about people and events in that user's life) and enabling the user to ask questions over that personal knowledge (e.g., "which engineers in the QC team were involved in the last meeting with the director?").</p><p>This type of questions highlight a unique blend of two conventional streams of research in Question Answering (QA) -QA over struc- tured sources such as knowledge bases <ref type="bibr">(KBs)</ref>, and QA over unstructured sources such as free text. This blend is a natural consequence of our problem setting: (i) users may choose to express rich relational knowledge about their world, in turn enabling them to pose complex composi-1. There is an associate professor named Andy 2. He returned from a sabbatical 3. This professor currently has funding 4. There is a masters level course called G301 5. That course is taught by him 6. That class is part of the mechanical engineering department 7. Roslyn is a student in this course 8. U203 is a undergraduate level course 9. Peggy and that student are TAs for this course … What students are advised by a professor with funding?</p><p>[Albertha, Roslyn, Peggy, Lucy, Racquel]</p><p>What assistant professors advise students who passed their thesis proposal? <ref type="bibr">[Andy]</ref> Which courses have masters student TAs?</p><formula xml:id="formula_0">[G301, U101 ]</formula><p>Who are the professors working on unsupervised machine learning? <ref type="bibr">[Andy, Hanna]</ref> 1 <ref type="table" target="#tab_1">. There is a new important mobile project  2. That project is in the implementation stage  3. Hiram is a tester on mobile project  4. Mobile project has moved to the deployment  stage  5. Andrew created a new issue for mobile</ref> project: fails with apache stack 6. Andrew is no longer assigned to that project 7. That developer resolved the changelog needs to be added issue … Are there any developers assigned to projects in the evaluation stage? <ref type="bibr">[Tawnya, Charlott, Hiram]</ref> Who is the null pointer exception during parsing issue assigned to? Hiram Are there any issues that are resolved for experimental projects?</p><p>[saving data throws exception, wrong pos tag on consecutive words]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Academic Department World Software Engineering World</head><p>Figure 2: Illustrative snippets from two sample worlds. We aim to generate natural-sounding first-person narratives from five diverse worlds, covering a range of different events, entities and relations.</p><p>tional queries (e.g., "all CS undergrads who took my class last semester"), while at the same time (ii) personal knowledge generally evolves through time and has an open and growing set of relations, making natural language the only practical inter- face for creating and maintaining that knowledge by non-expert users. In short, the task that we ad- dress in this work is: multi-relational question answering from dynamic knowledge expressed via narrative.</p><p>Although we hypothesize that question- answering over personal knowledge of this sort is ubiquitous (e.g., between a professor and their administrative assistant, or even if just in the user's head), such interactions are rarely recorded, presenting a significant practical challenge to collecting a sufficiently large real-world dataset of this type. At the same time, we hypothesize that the technical challenges involved in developing models for relational question answering from narrative would not be fundamentally impacted if addressed via sufficiently rich, but controlled simulated narratives.</p><p>Such simulations also offer the advantage of enabling us to directly experiment with stories and queries of different complexity, potentially offering additional insight into the fundamental challenges of this task.</p><p>While our problem setting blends the problems of relational question answering over knowledge bases and question answering over text, our hy- pothesis is that end-to-end QA models may learn to answer such multisentential relational queries, without relying on an intermediate knowledge base representation. In this work, we conduct an extensive evaluation of a set of state-of-the-art end-to-end QA models on our task and analyze their results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Question answering has been mainly studied in two different settings: KB-based and text-based. KB-based QA mostly focuses on parsing ques- tions to logical forms ( <ref type="bibr" target="#b18">Zelle and Mooney, 1996;</ref><ref type="bibr" target="#b19">Zettlemoyer and Collins, 2012;</ref><ref type="bibr">Berant et al., 2013;</ref><ref type="bibr">Kwiatkowski et al., 2013;</ref>) in order to better retrieve answer candidates from a knowl- edge base. Text-based QA aims to directly an- swer questions from the input text. This includes works on early information retrieval-based meth- ods ( <ref type="bibr">Banko et al., 2002;</ref><ref type="bibr">Ahn et al., 2004</ref>) and methods that build on extracted structured repre- sentations from both the question and the input text ( <ref type="bibr" target="#b9">Sachan et al., 2015;</ref><ref type="bibr" target="#b10">Sachan and Xing, 2016;</ref><ref type="bibr">Khot et al., 2017;</ref><ref type="bibr">Khashabi et al., 2018b</ref>). Al- though these structured presentations make rea- soning more effective, they rely on sophisticated NLP pipelines and suffer from error propaga- tion. More recently, end-to-end neural archi- tectures have been successfully applied to text- based QA, including Memory-augmented neural networks ( <ref type="bibr" target="#b12">Sukhbaatar et al., 2015;</ref><ref type="bibr">Miller et al., 2016;</ref><ref type="bibr">Kumar et al., 2016</ref>) and attention-based neu- ral networks ( <ref type="bibr">Hermann et al., 2015;</ref><ref type="bibr">Chen et al., 2016;</ref><ref type="bibr">Kadlec et al., 2016;</ref><ref type="bibr">Dhingra et al., 2017;</ref><ref type="bibr" target="#b16">Xiong et al., 2017;</ref><ref type="bibr" target="#b11">Seo et al., 2017;</ref><ref type="bibr">Chen et al., 2017)</ref>. In this work, we focus on QA over text (where the text is generated from a supporting KB) and evaluate several state-of-the-art memory- augmented and attention-based neural architec- tures on our QA task. In addition, we consider a sequence-to-sequence model baseline ( <ref type="bibr">Bahdanau et al., 2015)</ref>, which has been widely used in dia- log ( <ref type="bibr" target="#b14">Vinyals and Le, 2015;</ref><ref type="bibr">Ghazvininejad et al., 2017)</ref> and recently been applied to generating an- swer values from Wikidata ( <ref type="bibr">Hewlett et al., 2016</ref>).</p><p>There are numerous datasets available for evalu- ating the capabilities of QA systems. For example, MCTest ( <ref type="bibr" target="#b8">Richardson et al., 2013</ref>) contains com- prehension questions for fictional stories. Allen AI Science Challenge <ref type="bibr">(Clark, 2015)</ref> contains sci- ence questions that can be answered with knowl- edge from text books. RACE ( <ref type="bibr" target="#b0">Lai et al., 2017</ref>) is an English exam dataset for middle and high school Chinese students. MULTIRC ( <ref type="bibr">Khashabi et al., 2018a</ref>) is a dataset that focuses on evaluating multi-sentence reasoning skills. These datasets all require humans to carefully design multiple- choice questions and answers, so that certain as- pects of the comprehension and reasoning capa- bilities are properly evaluated. As a result, it is difficult to collect them at scale. Furthermore, as the knowledge required for answering each ques- tion is not clearly specified in these datasets, it can be hard to identify the limitations of QA systems and propose improvements.  proposes to use synthetic QA tasks (the BABI dataset) to better under- stand the limitations of QA systems. BABI builds on a simulated physical world similar to interac- tive fiction <ref type="bibr">(Montfort, 2005</ref>) with simple objects and relations and includes 20 different reasoning tasks. Various types of end-to-end neural net- works ( <ref type="bibr" target="#b12">Sukhbaatar et al., 2015;</ref><ref type="bibr" target="#b1">Lee et al., 2015;</ref><ref type="bibr" target="#b6">Peng et al., 2015</ref>) have demonstrated promising accuracies on this dataset. However, the per- formance can hardly translate to real-world QA datasets, as BABI uses a small vocabulary (150 words) and short sentences with limited language variations (e.g., nesting sentences, coreference). A more sophisticated QA dataset with a support- ing KB is WIKIMOVIES ( <ref type="bibr">Miller et al., 2016)</ref>, which contains 100k questions about movies, each of them is answerable by using either a KB or a Wikipedia article. However, WIKIMOVIES is highly domain-specific, and similar to BABI, the questions are designed to be in simple forms with little compositionality and hence limit the diffi- culty level of the tasks.</p><p>Our dataset differs in the above datasets in that (i) it contains five different realistic domains per- mitting cross-domain evaluation to test the abil- ity of models to generalize beyond a fixed set of KB relations, (ii) it exhibits rich referring expres- sions and linguistic variations (vocabulary much larger than the BABI dataset), (iii) questions in our dataset are designed to be deeply compositional and can cover multiple relations mentioned across multiple sentences.</p><p>Other large-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>scale QA datasets include Cloze- style datasets such as CNN/Daily Mail (Her- mann et al., 2015), Children's Book Test (Hill et al., 2015), and Who Did What (Onishi et al., 2016); datasets with answers being spans in the document, such as SQuAD (Ra- jpurkar et al., 2016), NewsQA (Trischler et al., 2016), and TriviaQA (Joshi et al., 2017); and datasets with human generated answers, for in- stance, MS MARCO (Nguyen et al., 2016) and</head><p>SearchQA ( <ref type="bibr">Dunn et al., 2017)</ref>. One common drawback of these datasets is the difficulty in ac- cessing a system's capability of integrating infor- mation across a document context. Kočisk` <ref type="bibr">Kočisk`y et al. (2017)</ref> recently emphasized this issue and pro- posed NarrativeQA, a dataset of fictional stories with questions that reflect the complexity of nar- ratives: characters, events, and evolving relations. Our dataset contains similar narrative elements, but it is created with a supporting KB and hence it is easier to analyze and interpret results in a con- trolled setting. lated user may introduce new knowledge, update existing knowledge or express a state change (e.g., "Homework 3 is now due on Friday" or "Saman- tha passed her thesis defense"). Each narrative is interleaved with questions about the current state of the world, and questions range in complexity depending on the amount of knowledge that needs to be integrated to answer them. This allows us to benchmark a range of QA models at their ability to answer questions that require different extents of relational reasoning to be answered.</p><p>The set of worlds that we simulate as part of this work are as follows:</p><p>1. MEETING WORLD: This world describes sit- uations related to professional meetings, e.g., meetings being set/cancelled, people attending meetings, topics of meetings.</p><p>2. HOMEWORK WORLD: This world describes situations from the first-person perspective of a student, e.g., courses taken, assignments in different courses, deadlines of assignments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SOFTWARE ENGINEERING WORLD: This</head><p>world describes situations from the first-person perspective of a software development man- ager, e.g., task assignment to different project team members, stages of software develop- ment, bug tickets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">ACADEMIC DEPARTMENT WORLD: This</head><p>world describes situations from the first-person perspective of a professor, e.g., teaching assign- ments, faculty going/returning from sabbati- cals, students from different departments tak- ing/dropping courses.</p><p>5. SHOPPING WORLD: This world describes sit- uations about a person shopping for various occasions, e.g., adding items to a shopping list, purchasing items at different stores, noting where items are on sale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Narrative</head><p>Each world is represented by a set of entities E and a set of unary, binary or ternary relations R. For- mally, a single step in one simulation of a world involves a combination of instantiating new enti- ties and defining new (or mutating existing) re- lations between entities. Practically, we imple- ment each world as a collection of classes and Statistics Value # of total stories 5,000 # of total questions 1,207,022 Avg. # of entity mentions <ref type="table">(per story)</ref> 217.4 Avg. # of correct answers (per question) 8.7 Avg. # of statements in stories 100 Avg. # of tok. in stories 837.5 Avg. # of tok. in questions 8.9 Avg. # of tok. in answers 1.5 Vocabulary size (tok.) 1,994 Vocabulary size (entity) 10,793 <ref type="table">Table 1</ref>: TEXTWORLDSQA dataset statistics methods, with each step of the simulation creat- ing or mutating class instances by sampling en- tities and methods on those entities. By design, these classes and methods are easy to extend, to either enrich existing worlds or create new ones. Each simulation step is then expressed as a natural language statement, which is added to the narra- tive. In the process of generating a natural lan- guage expression, we employ a rich mechanism for generating anaphora, such as "meeting with John about the performance review" and "meeting that I last added", in addition to simple pronoun references. This allows us to generate more nat- ural and flowing narratives. These references are generated and composed automatically by the un- derlying TEXTWORLDS framework, significantly reducing the effort needed to build new worlds. Furthermore, all generated stories also provide ad- ditional annotation that maps all entities to under- lying gold-standard KB ids, allowing to perform experiments that provide models with different de- grees of access to the "simulation oracle". We generate 1,000 narratives within each world, where each narrative consists of 100 sentences, plus up to 300 questions interleaved randomly within the narrative. See <ref type="figure">Figure 1</ref> for two example narratives. Each story in a given world samples its entities from a large general pool of entity names collected from the web (e.g., people names, uni- versity names). Although some entities do overlap between stories, each story in a given world con- tains a unique flow of events and entities involved in those events. See <ref type="table">Table 1</ref> for the data statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Questions</head><p>Formally, questions are queries over the knowledge-base in the state defined up to the point when the question is asked in the narra- tive. In the narrative, the questions are expressed  <ref type="table" target="#tab_5">Single Entity/Relation  Multiple entities  Single relation  Two relations  Three relations  MEETING  57,590 (41.16%</ref>  in natural language, employing the same anaphora mechanism used in generating the narrative (e.g., "who is attending the last meeting I added?").</p><p>We categorize generated questions into four types, reflecting the number and types of facts re- quired to answer them; questions that require more facts to answer are typically more compositional in nature. We categorize each question in our dataset into one of the following four categories:</p><p>Single Entity/Single Relation Answers to these questions are a single entity, e.g. "what is John's email address?", or expressed in lambda-calculus notation:</p><formula xml:id="formula_1">λx.EmailAddress(John, x)</formula><p>The answers to these questions are found in a sin- gle sentence in the narrative, although it is pos- sible that the answer may change through the course of the narrative (e.g., "John's new office is GHC122").</p><p>Multi-Entity/Single Relation Answers to these questions can be multiple entities but involve a single relation, e.g., "Who is enrolled in the Math class?", or expressed in lambda calculus notation:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>λx.TakingClass(x, Math)</head><p>Unlike the previous category, answers to these questions can be sets of entities.</p><p>Multi-Entity/Two Relations Answers to these questions can be multiple entities and involve two relations, e.g., "Who is enrolled in courses that I am teaching?", or expressed in lambda calculus:</p><p>λx.∃y.EnrolledInClass(x, y) ∧ CourseTaughtByMe(y)</p><p>Multi-Entity/Three Relations Answers to these questions can be multiple entities and involve three relations, e.g., "Which undergraduates are enrolled in courses that I am teaching?", or ex- pressed in lambda calculus notation:</p><formula xml:id="formula_2">λx.∃y.EnrolledInClass(x, y) ∧ CourseTaughtByMe(y) ∧ Undergrad(x)</formula><p>In the data that we generate, answers to questions are always sets of spans in the narrative (the rea- son for this constraint is for easier evaluation of several existing machine-reading models; this as- sumption can easily be relaxed in the simulation). In all of our evaluations, we will partition our re- sults by one of the four question categories listed above, which we hypothesize correlates with the difficulty of a question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methods</head><p>We develop several baselines for our QA task, in- cluding a logistic regression model and four differ- ent neural network models: Seq2Seq ( <ref type="bibr">Bahdanau et al., 2015</ref>), <ref type="bibr">MemN2N (Sukhbaatar et al., 2015</ref>), BiDAF ( <ref type="bibr" target="#b11">Seo et al., 2017)</ref>, and DrQA ( <ref type="bibr">Chen et al., 2017</ref>). These models generate answers in differ- ent ways, e.g., predicting a single entity, predict- ing spans of text, or generating answer sequences. Therefore, we implement two experimental set- tings: ENTITY and RAW. In the ENTITY setting, given a question and a story, we treat all the en- tity spans in the story as candidate answers, and the prediction task becomes a classification prob- lem. In the RAW setting, a model needs to pre- dict the answer spans. For logistic regression and MemN2N, we adopt the ENTITY setting as they are naturally classification models. This ideally provides an upper bound on the performance when considering answer candidate generation. For all the other models, we can apply the RAW setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Logistic Regression</head><p>The logistic regression baseline predicts the likeli- hood of an answer candidate being a true answer.</p><p>For each answer candidate e and a given ques- tion, we extract the following features: (1) The fre- quency of e in the story; (2) The number of words within e; (3) Unigrams and bigrams within e; (4) Each non-stop question word combined with each non-stop word within e; (5) The average minimum distance between each non-stop question word and e in the story; (6) The common words (excluding stop words) between the question and the text sur- rounding of e (within a window of 10 words); (7) Sum of the frequencies of the common words to the left of e, to the right e, and both. These features are designed to help the model pick the correct an- swer spans. They have shown to be effective for answer prediction in previous work <ref type="bibr">(Chen et al., 2016;</ref><ref type="bibr" target="#b7">Rajpurkar et al., 2016</ref>).</p><p>We associate each answer candidate with a bi- nary label indicating whether it is a true answer. We train a logistic regression classifier to pro- duce a probability score for each answer candi- date. During test, we search for an optimal thresh- old that maximizes the F1 performance on the val- idation data. During training, we optimize the cross-entropy loss using Adam ( <ref type="bibr">Kingma and Ba, 2014</ref>) with an initial learning rate of 0.01. We use a batch size of 10, 000 and train with 5 epochs. Training takes roughly 10 minutes for each do- main on a Titan X GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Seq2Seq</head><p>The seq2seq model is based on the sequence to sequence model presented in ( <ref type="bibr">Bahdanau et al., 2015)</ref>, which includes an attention model. Bah- danau et al. ( <ref type="bibr">Bahdanau et al., 2015</ref>) have used this model to build a neural based machine translation performing at the state-of-the-art. We adopt this model to fit our own domain by including a pre- processing step in which all statements are con- catenated with a dedicated token, while eliminat- ing all previously asked questions, and the current question is added at the end of the list of state- ments. The answers are treated as a sequence of words. We use word embeddings ( <ref type="bibr" target="#b20">Zou et al., 2013)</ref>, as it was shown to improve accuracy. We use 3 GRU ( <ref type="bibr">Cho et al., 2014</ref>) connected layers, each with a capacity of 256. Our batch size was set to 16. We use gradient descent with an ini- tial learning rate of 0.5 and a decay factor of 0.99, iterating on the data for 50, 000 steps (5 epochs). The training process for each domain took approx- imately 48 hours on a Titan X GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">MemN2N</head><p>End-To-End Memory Network (MemN2N) is a neural architecture that encodes both long-term and short-term context into a memory and it- eratively reads from the memory (i.e., multi- ple hops) relevant information to answer a ques- tion ( <ref type="bibr" target="#b12">Sukhbaatar et al., 2015)</ref>. It has been shown to be effective for a variety of question answering tasks ( <ref type="bibr" target="#b12">Sukhbaatar et al., 2015;</ref><ref type="bibr">Hill et al., 2015)</ref>.</p><p>In this work, we directly apply MemN2N to our task with a small modification. Originally, MemN2N was designed to produce a single an- swer for a question, so at the prediction layer, it uses softmax to select the best answer from the answer candidates. In order to account for multi- ple answers for a given question, we modify the prediction layer to apply the logistic function and optimize the cross entropy loss instead. For train- ing, we use the parameter setting as in a publicly available MemN2N 1 except that we set the em- bedding size to 300 instead of 20. We train the model for 100 epochs and it takes about 2 hours for each domain on a Titan X GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">BiDAF-M</head><p>BiDAF (Bidirectional Attention Flow Net- works) ( <ref type="bibr" target="#b11">Seo et al., 2017</ref>) is one of the top- performing models on the span-based question answering dataset SQuAD ( <ref type="bibr" target="#b7">Rajpurkar et al., 2016)</ref>. We reimplement BiDAF with simplified parameterizations and change the prediction layer so that it can predict multiple answer spans.</p><p>Specifically, we encode the input story {x 1 , ..., x T } and a given question {q 1 , ..., q J } at the character level and the word level, where the character level uses CNNs and the word level uses pre-trained word vectors. The concatenation of the character and word embeddings are passed to a bidirectional LSTM to produce a contextual embedding for each word in the story context and in the question. Then, we apply the same bidirectional attention flow layer to model the interactions between the context and question embeddings, producing question-aware feature vectors for each word in the context, denoted as G ∈ R dg×T . G is then fed into a bidirec- tional LSTM layer to obtain a feature matrix M 1 ∈ R d 1 ×T for predicting the start offset of the answer span, and M 1 is then passed into  another bidirectional LSTM layer to obtain a feature matrix M 2 ∈ R d 2 ×T for predicting the end offset of the answer span. We then compute two probability scores for each word i in the narrative:</p><formula xml:id="formula_3">p start = sigmoid(w T 1 [G; M 1 ]) and p end = sigmoid(w T 2 [G; M 1 ; M 2 ])</formula><p>, where w 1 and w 2 are trainable weights. The training objec- tive is simply the sum of cross-entropy losses for predicting the start and end indices.</p><p>We use 50 1D filters for CNN character embed- ding, each with a width of 5. The word embedding size is 300 and the hidden dimension for LSTMs is 128. For optimization, we use Adam ( <ref type="bibr">Kingma and Ba, 2014</ref>) with an initial learning rate of 0.001, and use a minibatch size of 32 for 15 epochs. The training process takes roughly 20 hours for each domain on a Titan X GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">DrQA-M</head><p>DrQA ( <ref type="bibr">Chen et al., 2017</ref>) is an open-domain QA system that has demonstrated strong performance on multiple QA datasets. We modify the Doc- ument Reader component of DrQA and imple- ment it in a similar framework as BiDAF-M for fair comparisons. First, we employ the same character-level and word-level encoding layers to both the input story and a given question. We then use the concatenation of the character and word embeddings as the final embeddings for words in the story and in the question. We compute the aligned question embedding <ref type="bibr">(Chen et al., 2017</ref>) as a feature vector for each word in the story and con- catenate it with the story word embedding and pass it into a bidirectional LSTM to obtain the contex- tual embeddings E ∈ R d×T for words in the story. Another bidirectional LSTM is used to obtain the contextual embeddings for the question, and self- attention is used to compress them into one single vector q ∈ R d . The final prediction layer uses a bilinear term to compute scores for predicting the start offset: p start = sigmoid(q T W 1 E) and an- other bilinear term for predicting the end offset: p end = sigmoid(q T W 2 E), where W 1 and W 2 are trainable weights. The training loss is the same as in BiDAF-M, and we use the same parameter setting. Training takes roughly 10 hours for each domain on a Titan X GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We use two evaluation settings for measuring per- formance at this task: within-world and across- world. In the within-world evaluation setting, we test on the same world that the model was trained on. We then compute the precision, recall and F 1 for each question and report the macro-average F1 score for questions in each world. In the across- world evaluation setting, the model is trained on four out of the five worlds, and tested on the re- maining world. The across-world regime is obvi- ously more challenging, as it requires the model to be able to learn to generalize to unseen relations and vocabulary. We consider the across-world evaluation setting to be the main evaluation crite- ria for any future models used on this dataset, as it mimics the practical requirement of any QA sys- tem used in personal assistants: it has to be able to answer questions on any new domain the user introduces to the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Results</head><p>We draw several important observations from our results. First, we observe that more compositional questions (i.e., those that integrate multiple rela- tions) are more challenging for most models -as Cross-world</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DrQA-M Bidaf-M Logistic regression Memory networks Seq2seq</head><p>Figure 3: F 1 score breakdown based on the num- ber of relations involved in the questions.</p><p>all models (except Seq2seq) decrease in perfor- mance with the number of relations composed in a question ( <ref type="figure">Figure 5.1</ref>). This can be in part ex- plained by the fact that more composition ques- tions are typically longer, and also require the model to integrate more sources of information in the narrative in order to answer them. One surpris- ing observation from our results is that the perfor- mance on questions that ask about a single relation and have only a single answer is lower than ques- tions that ask about a single relation but that can have multiple answers (see detailed results in the Appendix). This is in part because questions that can have multiple answers typically have canoni- cal entities as answers (e.g., person's name), and these entities generally repeat in the text, making it easier for the model to find the correct answer. <ref type="table" target="#tab_3">Table 3</ref> reports the overall (macro-average) F1 scores for different baselines. We can see that BiDAF-M and DrQA-M perform surprisingly well in the within-world evaluation even though they do not use any entity span information. In partic- ular, DrQA-M outperforms BiDAF-M which sug- gests that modeling question-context interactions using simple bilinear terms have advantages over using more complex bidirectional attention flows. The lower performance of MemN2N suggests that its effectiveness on the BABI dataset does not di- rectly transfer to our dataset. Note that the original MemN2N architecture uses simple bag-of-words and position encoding for sentences. This may work well on dataset with a simple vocabulary, for example, MemN2N performs the best in the SOFTWARE world as the SOFTWARE world has a smaller vocabulary compared to other worlds. In general, we believe that better text representa- tions for questions and narratives can lead to im- proved performance. Seq2Seq model also did not perform as well. This is due to the inherent diffi- culty of generation and encoding long sequences. We found that it performs better when training and testing on shorter stories (limited to 30 state- ments). Interestingly, the logistic regression base- line performs on a par with MemN2N, but there is still a large performance gap to BiDAF-M and DrQA-M, and the gap is greater for questions that compose multiple relations.</p><p>In the across-world setting, the performance of all methods dramatically decreases. <ref type="bibr">2</ref> This sug- gests the limitations of these methods in gener- alizing to unseen relations and vocabulary. The span-based models BiDAF-M and DrQA-M have an advantage in this setting as they can learn to answer questions based on the alignment be- tween the question and the narrative. However, the low performance still suggests their limitations in transferring question answering capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we have taken the first steps towards the task of multi-relational question answering ex- pressed through personal narrative. Our hypoth- esis is that this task will become increasingly im- portant as users begin to teach personal knowledge about their world to the personal assistants em- bedded in their devices. This task naturally syn- thesizes two main branches of question answer- ing research: QA over KBs and QA over free text. One of our main contributions is a collec- tion of diverse datasets that feature rich composi- tional questions over a dynamic knowledge graph expressed through simulated narrative. Another contribution of our work is a thorough set of ex- periments and analysis of different types of end- to-end architectures for QA at their ability to an- swer multi-relational questions of varying degrees of compositionality. Our long-term goal is that both the data and the simulation code we release will inspire and motivate the community to look towards the vision of letting end-users teach our personal assistants about the world around us.   <ref type="table">Table 5</ref>: Test performance (F 1 score) at the task of question answering by question type using the across-world evaluation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Dataset statistics by question type. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 : F 1 scores for different baselines evaluated on both within-world and across-world settings.</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Test performance at the task of question answering by question type using the within-world 
evaluation. </table></figure>

			<note place="foot" n="3"> TEXTWORLDS: Simulated Worlds for Multi-Relational QA from Narratives In this work, we synthesize narratives in five diverse worlds, each containing a thousand narratives and where each narrative describes the evolution of a simulated user&apos;s world from a firstperson perspective. In each narrative, the simu</note>

			<note place="foot" n="1"> https://github.com/domluna/memn2n</note>

			<note place="foot" n="2"> In order to allow generalization across different domains for the Seq2Seq model, we replace entities appearing in each story with an id that correlates to their appearance order. After the model outputs its prediction, the entity ids are converted back to the entity phrase.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Race: Large-scale reading comprehension dataset from examinations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moontae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smolensky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06426</idno>
		<title level="m">Reasoning in vector space: An exploratory study of question answering</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirhossein</forename><surname>Karimi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03126</idno>
		<title level="m">Antoine Bordes, and Jason Weston. 2016. Key-value memory networks for directly reading documents</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Twisty Little Passages: an approach to interactive fiction</title>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Mit Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Ms marco: A human generated machine reading comprehension dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mir</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09268</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Who did what: A large-scale person-centered cloze dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Onishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.05457</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>Kevin Gimpel, and David McAllester</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.05508</idno>
		<title level="m">Towards neural network-based reasoning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mctest: A challenge dataset for the open-domain machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Renshaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="193" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning answerentailing structures for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrinmaya</forename><surname>Sachan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumar</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="239" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Machine comprehension using rich semantic representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrinmaya</forename><surname>Sachan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="486" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09830</idno>
		<title level="m">Newsqa: A machine comprehension dataset</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05869</idno>
		<title level="m">A neural conversational model</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Towards ai-complete question answering: A set of prerequisite toy tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.05698</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dynamic coattention networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semantic parsing via staged query graph generation: Question answering with knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to parse database queries using inductive logic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond J</forename><surname>Zelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national conference on artificial intelligence</title>
		<meeting>the national conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="1050" to="1055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collins</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.1420</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bilingual word embeddings for phrase-based machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Will</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1393" to="1398" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
