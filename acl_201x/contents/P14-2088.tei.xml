<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fast Easy Unsupervised Domain Adaptation with Marginalized Structured Dropout</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Interactive Computing</orgName>
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Interactive Computing</orgName>
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Fast Easy Unsupervised Domain Adaptation with Marginalized Structured Dropout</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="538" to="544"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Unsupervised domain adaptation often relies on transforming the instance representation. However, most such approaches are designed for bag-of-words models, and ignore the structured features present in many problems in NLP. We propose a new technique called marginalized struc-tured dropout, which exploits feature structure to obtain a remarkably simple and efficient feature projection. Applied to the task of fine-grained part-of-speech tagging on a dataset of historical Por-tuguese, marginalized structured dropout yields state-of-the-art accuracy while increasing speed by more than an order-of-magnitude over previous work.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Unsupervised domain adaptation is a fundamen- tal problem for natural language processing, as we hope to apply our systems to datasets unlike those for which we have annotations. This is par- ticularly relevant as labeled datasets become stale in comparison with rapidly evolving social media writing styles <ref type="bibr" target="#b7">(Eisenstein, 2013)</ref>, and as there is increasing interest in natural language processing for historical texts <ref type="bibr" target="#b22">(Piotrowski, 2012</ref>). While a number of different approaches for domain adap- tation have been proposed <ref type="bibr" target="#b20">(Pan and Yang, 2010;</ref><ref type="bibr" target="#b25">Søgaard, 2013)</ref>, they tend to emphasize bag-of- words features for classification tasks such as sen- timent analysis. Consequently, many approaches rely on each instance having a relatively large number of active features, and fail to exploit the structured feature spaces that characterize syn- tactic tasks such as sequence labeling and pars- ing <ref type="bibr" target="#b24">(Smith, 2011)</ref>.</p><p>As we will show, substantial efficiency im- provements can be obtained by designing domain adaptation methods for learning in structured fea- ture spaces. We build on work from the deep learning community, in which denoising autoen- coders are trained to remove synthetic noise from the observed instances ( <ref type="bibr" target="#b10">Glorot et al., 2011a</ref>). By using the autoencoder to transform the original feature space, one may obtain a representation that is less dependent on any individual feature, and therefore more robust across domains. <ref type="bibr" target="#b3">Chen et al. (2012)</ref> showed that such autoencoders can be learned even as the noising process is analyt- ically marginalized; the idea is similar in spirit to feature noising ( <ref type="bibr" target="#b28">Wang et al., 2013)</ref>. While the marginalized denoising autoencoder (mDA) is considerably faster than the original denoising au- toencoder, it requires solving a system of equa- tions that can grow very large, as realistic NLP tasks can involve 10 5 or more features.</p><p>In this paper we investigate noising functions that are explicitly designed for structured feature spaces, which are common in NLP. For example, in part-of-speech tagging, <ref type="bibr" target="#b26">Toutanova et al. (2003)</ref> define several feature "templates": the current word, the previous word, the suffix of the current word, and so on. For each feature template, there are thousands of binary features. To exploit this structure, we propose two alternative noising tech- niques: (1) feature scrambling, which randomly chooses a feature template and randomly selects an alternative value within the template, and (2) structured dropout, which randomly eliminates all but a single feature template. We show how it is possible to marginalize over both types of noise, and find that the solution for structured dropout is substantially simpler and more efficient than the mDA approach of <ref type="bibr" target="#b3">Chen et al. (2012)</ref>, which does not consider feature structure.</p><p>We apply these ideas to fine-grained part-of- speech tagging on a dataset of Portuguese texts from the years 1502 to 1836 <ref type="bibr" target="#b9">(Galves and Faria, 2010)</ref>, training on recent texts and evaluating on older documents. Both structure-aware do- main adaptation algorithms perform as well as standard dropout -and better than the well- known structural correspondence learning (SCL) algorithm <ref type="bibr" target="#b1">(Blitzer et al., 2007)</ref> -but structured dropout is more than an order-of-magnitude faster. As a secondary contribution of this paper, we demonstrate the applicability of unsupervised do- main adaptation to the syntactic analysis of histor- ical texts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>In this section we first briefly describe the de- noising autoencoder ( <ref type="bibr" target="#b11">Glorot et al., 2011b</ref>), its ap- plication to domain adaptation, and the analytic marginalization of noise ( <ref type="bibr" target="#b3">Chen et al., 2012</ref>). Then we present three versions of marginalized denois- ing autoencoders (mDA) by incorporating differ- ent types of noise, including two new noising pro- cesses that are designed for structured features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Denoising Autoencoders</head><p>Assume instances x 1 , . . . , x n , which are drawn from both the source and target domains. We will "corrupt" these instances by adding different types of noise, and denote the corrupted version of x i by˜xby˜ by˜x i . Single-layer denoising autoencoders recon- struct the corrupted inputs with a projection matrix W : R d → R d , which is estimated by minimizing the squared reconstruction loss</p><formula xml:id="formula_0">L = 1 2 n i=1 ||x i − W˜xW˜x i || 2 .<label>(1)</label></formula><p>If we write X = [x 1 , . . . , x n ] ∈ R d×n , and we write its corrupted versioñ X, then the loss in (1) can be written as</p><formula xml:id="formula_1">L(W) = 1 2n tr X − W ˜ X X − W ˜ X .</formula><p>(2) In this case, we have the well-known closed- form solution for this ordinary least square prob- lem:</p><formula xml:id="formula_2">W = PQ −1 ,<label>(3)</label></formula><p>where Q = ˜ X ˜ X and P = X ˜ X . After ob- taining the weight matrix W, we can insert non- linearity into the output of the denoiser, such as tanh(WX). It is also possible to apply stack- ing, by passing this vector through another autoen- coder ( <ref type="bibr" target="#b3">Chen et al., 2012</ref>). In pilot experiments, this slowed down estimation and had little effect on accuracy, so we did not include it.</p><p>High-dimensional setting Structured predic- tion tasks often have much more features than simple bag-of-words representation, and perfor- mance relies on the rare features. In a naive im- plementation of the denoising approach, both P and Q will be dense matrices with dimension- ality d × d, which would be roughly 10 11 ele- ments in our experiments. To solve this problem, <ref type="bibr" target="#b3">Chen et al. (2012)</ref> propose to use a set of pivot features, and train the autoencoder to reconstruct the pivots from the full set of features. Specifi- cally, the corrupted input is divided to S subsets˜x</p><formula xml:id="formula_3">subsets˜ subsets˜x i = (˜ x) 1 i , . . . , (˜ x) S i</formula><p>. We obtain a projec- tion matrix W s for each subset by reconstructing the pivot features from the features in this subset; we can then use the sum of all reconstructions as the new features, tanh(</p><formula xml:id="formula_4">S s=1 W s X s ).</formula><p>Marginalized Denoising Autoencoders In the standard denoising autoencoder, we need to gen- erate multiple versions of the corrupted datã X to reduce the variance of the solution (Glorot et al., 2011b). But <ref type="bibr" target="#b3">Chen et al. (2012)</ref> show that it is possible to marginalize over the noise, analyt- ically computing expectations of both P and Q, and computing</p><formula xml:id="formula_5">W = E[P]E[Q] −1 ,<label>(4)</label></formula><p>where</p><formula xml:id="formula_6">E[P] = n i=1 E[x i ˜ x i ] and E[Q] = n i=1 E[˜ x i ˜ x i ]</formula><p>. This is equivalent to corrupting the data m → ∞ times. The computation of these expectations depends on the type of noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Noise distributions</head><p>Chen et al. (2012) used dropout noise for domain adaptation, which we briefly review. We then de- scribe two novel types of noise that are designed for structured feature spaces, and explain how they can be marginalized to efficiently compute W.</p><p>Dropout noise In dropout noise, each feature is set to zero with probability p &gt; 0. If we define the scatter matrix of the uncorrupted input as S = XX , the solutions under dropout noise are</p><formula xml:id="formula_7">E[Q] α,β = (1 − p) 2 S α,β if α = β (1 − p)S α,β if α = β ,<label>(5)</label></formula><p>and</p><formula xml:id="formula_8">E[P] α,β = (1 − p)S α,β ,<label>(6)</label></formula><p>where α and β index two features. The form of these solutions means that computing W requires solving a system of equations equal to the num- ber of features (in the naive implementation), or several smaller systems of equations (in the high- dimensional version). Note also that p is a tunable parameter for this type of noise.</p><p>Structured dropout noise In many NLP set- tings, we have several feature templates, such as previous-word, middle-word, next-word, etc, with only one feature per template firing on any token. We can exploit this structure by using an alterna- tive dropout scheme: for each token, choose ex- actly one feature template to keep, and zero out all other features that consider this token (transition feature templates such as y t , y t−1 are not con- sidered for dropout). Assuming we have K feature templates, this noise leads to very simple solutions for the marginalized matrices E <ref type="bibr">[P]</ref> and</p><formula xml:id="formula_9">E[Q], E[Q] α,β = 0 if α = β 1 K S α,β if α = β<label>(7)</label></formula><formula xml:id="formula_10">E[P] α,β = 1 K S α,β ,<label>(8)</label></formula><p>For E[P], we obtain a scaled version of the scat- ter matrix, because in each instance˜xinstance˜ instance˜x, there is ex- actly a 1/K chance that each individual feature survives dropout. E[Q] is diagonal, because for any off-diagonal entry E[Q] α,β , at least one of α and β will drop out for every instance. We can therefore view the projection matrix W as a row- normalized version of the scatter matrix S. Put another way, the contribution of β to the recon- struction for α is equal to the co-occurence count of α and β, divided by the count of β.</p><p>Unlike standard dropout, there are no free hyper-parameters to tune for structured dropout. Since E[Q] is a diagonal matrix, we eliminate the cost of matrix inversion (or of solving a system of linear equations). Moreover, to extend mDA for high dimensional data, we no longer need to di- vide the corrupted input˜xinput˜ input˜x to several subsets. <ref type="bibr">1</ref> For intuition, consider standard feature dropout with p = K−1 K . This will look very similar to structured dropout: the matrix E[P] is identical, and E[Q] has off-diagonal elements which are scaled by (1 − p) 2 , which goes to zero as K is 1 E[P] is an r by d matrix, where r is the number of pivots. large. However, by including these elements, stan- dard dropout is considerably slower, as we show in our experiments.</p><p>Scrambling noise A third alternative is to "scramble" the features by randomly selecting al- ternative features within each template. For a fea- ture α belonging to a template F , with probability p we will draw a noise feature β also belonging to F , according to some distribution q. In this work, we use an uniform distribution, in which q β = 1 |F | . However, the below solutions will also hold for other scrambling distributions, such as mean-preserving distributions.</p><p>Again, it is possible to analytically marginal- ize over this noise.</p><p>Recall</p><formula xml:id="formula_11">that E[Q] = n i=1 E[˜ x i ˜ x i</formula><p>]. An off-diagonal entry in the ma- trix˜x˜xtrix˜ trix˜x˜trix˜x˜x which involves features α and β belong- ing to different templates (F α = F β ) can take four different values (x i,α denotes feature α in x i ):</p><p>• x i,α x i,β if both features are unchanged, which happens with probability (1 − p) 2 .</p><p>• 1 if both features are chosen as noise features, which happens with probability p 2 q α q β .</p><p>• x i,α or x i,β if one feature is unchanged and the other one is chosen as the noise feature, which happens with probability p(1 − p)q β or p(1 − p)q α .</p><p>The diagonal entries take the first two values above, with probability 1 − p and pq α respec- tively. Other entries will be all zero (only one feature belonging to the same template will fire in x i ). We can use similar reasoning to compute the expectation of P. With probability (1 − p), the original features are preserved, and we add the outer-product x i x i ; with probability p, we add the outer-product x i q . Therefore E[P] can be com- puted as the sum of these terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We compare these methods on historical Por- tuguese part-of-speech tagging, creating domains over historical epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experiment setup</head><p>Datasets We use the Tycho Brahe corpus to evaluate our methods. The corpus contains a total of 1,480,528 manually tagged words. It uses a set of 383 tags and is composed of various texts from historical Portuguese, from 1502 to 1836. We di- vide the texts into fifty-year periods to create dif- ferent domains. <ref type="table" target="#tab_1">Table 1</ref> presents some statistics of the datasets. We hold out 5% of data as develop- ment data to tune parameters. The two most recent domains (1800-1849 and 1750-1849) are treated as source domains, and the other domains are tar- get domains. This scenario is motivated by train- ing a tagger on a modern newstext corpus and ap- plying it to historical documents.  , we consider pivot features that appear more than 50 times in all the domains. This leads to a total of 1572 pivot features in our experiments.</p><p>Methods We compare mDA with three alterna- tive approaches. We refer to baseline as training a CRF tagger on the source domain and testing on the target domain with only base features. We also include PCA to project the entire dataset onto a low-dimensional sub-space (while still including the original features). Finally, we compare against Structural Correspondence Learning (SCL; <ref type="bibr" target="#b0">Blitzer et al., 2006</ref>), another feature learning algorithm. In all cases, we include the entire dataset to com- pute the feature projections; we also conducted ex- periments using only the test and training data for feature projections, with very similar results.</p><p>Parameters All the hyper-parameters are de- cided with our development data on the training set. We try different low dimension K from 10 to 2000 for PCA. Following Blitzer (2008) we per- form feature centering/normalization, as well as rescaling for SCL. The best parameters for SCL are dimensionality K = 25 and rescale factor α = 5, which are the same as in the original pa- per. For mDA, the best corruption level is p = 0.9 for dropout noise, and p = 0.1 for scrambling noise. Structured dropout noise has no free hyper- parameters. <ref type="table">Table 2</ref> presents results for different domain adap- tation tasks. We also compute the transfer ra- tio, which is defined as adaptation accuracy baseline accuracy , shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The generally positive trend of these graphs indicates that adaptation becomes progres- sively more important as we select test sets that are more temporally remote from the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>In general, mDA outperforms SCL and PCA, the latter of which shows little improvement over the base features. The various noising approaches for mDA give very similar results. However, struc- tured dropout is orders of magnitude faster than the alternatives, as shown in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Domain adaptation Most previous work on do- main adaptation focused on the supervised setting, in which some labeled data is available in the tar- get domain <ref type="bibr" target="#b15">(Jiang and Zhai, 2007;</ref><ref type="bibr" target="#b4">Daumé III, 2007;</ref><ref type="bibr" target="#b8">Finkel and Manning, 2009)</ref>. Our work fo- cuses on unsupervised domain adaptation, where no labeled data is available in the target domain. Several representation learning methods have been proposed to solve this problem. In structural corre- spondence learning (SCL), the induced represen- tation is based on the task of predicting the pres- ence of pivot features. Autoencoders apply a sim- ilar idea, but use the denoised instances as the la- tent representation <ref type="bibr" target="#b27">(Vincent et al., 2008;</ref><ref type="bibr" target="#b11">Glorot et al., 2011b;</ref><ref type="bibr" target="#b3">Chen et al., 2012</ref>  <ref type="table">Table 2</ref>: Accuracy results for adaptation from labeled data in 1800-1849, and in 1750-1849. On the specific problem of sequence labeling, <ref type="bibr" target="#b29">Xiao and Guo (2013)</ref> proposed a supervised do- main adaptation method by using a log-bilinear language adaptation model. <ref type="bibr" target="#b5">Dhillon et al. (2011)</ref> presented a spectral method to estimate low di- mensional context-specific word representations for sequence labeling. <ref type="bibr" target="#b13">Huang and Yates (2009;</ref><ref type="bibr" target="#b22">2012</ref>) used an HMM model to learn latent rep- resentations, and then leverage the Posterior Reg- ularization framework to incorporate specific bi- ases. Unlike these methods, our approach uses a standard CRF, but with transformed features.</p><p>Historical text Our evaluation concerns syntac- tic analysis of historical text, which is a topic of in- creasing interest for NLP <ref type="bibr" target="#b22">(Piotrowski, 2012)</ref>. Pen- nacchiotti and <ref type="bibr" target="#b21">Zanzotto (2008)</ref> find that part-of- speech tagging degrades considerably when ap- plied to a corpus of historical Italian. <ref type="bibr" target="#b17">Moon and Baldridge (2007)</ref> tackle the challenging problem of tagging Middle English, using techniques for projecting syntactic annotations across languages. Prior work on the Tycho Brahe corpus applied su- pervised learning to a random split of test and training data ( <ref type="bibr" target="#b16">Kepler and Finger, 2006</ref>; Dos San- tos et al., 2008); they did not consider the domain adaptation problem of training on recent data and testing on older historical text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>Denoising autoencoders provide an intuitive so- lution for domain adaptation: transform the fea- tures into a representation that is resistant to the noise that may characterize the domain adaptation process. The original implementation of this idea produced this noise directly ( <ref type="bibr" target="#b11">Glorot et al., 2011b)</ref>; later work showed that dropout noise could be an- alytically marginalized <ref type="bibr" target="#b3">(Chen et al., 2012</ref>). We take another step towards simplicity by showing that structured dropout can make marginalization even easier, obtaining dramatic speedups without sacrificing accuracy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Transfer ratio for adaptation to historical text on dropout noise, which has also been applied as a general technique for improving the robustness of machine learning, particularly in neural networks (Hinton et al., 2012; Wang et al., 2013). On the specific problem of sequence labeling, Xiao and Guo (2013) proposed a supervised domain adaptation method by using a log-bilinear language adaptation model. Dhillon et al. (2011) presented a spectral method to estimate low dimensional context-specific word representations for sequence labeling. Huang and Yates (2009; 2012) used an HMM model to learn latent representations, and then leverage the Posterior Regularization framework to incorporate specific biases. Unlike these methods, our approach uses a standard CRF, but with transformed features.</figDesc><graphic url="image-1.png" coords="5,118.80,262.37,359.96,119.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 : Statistics of the Tycho Brahe Corpus</head><label>1</label><figDesc></figDesc><table>CRF tagger We use a conditional random field 
tagger, choosing CRFsuite because it supports 
arbitrary real valued features (Okazaki, 2007), 
with SGD optimization. Following the work of 
Nogueira Dos Santos et al. (2008) on this dataset, 
we apply the feature set of Ratnaparkhi (1996). 
There are 16 feature templates and 372, 902 fea-
tures in total. Following Blitzer et al. (2006)</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 .</head><label>3</label><figDesc>The scram- bling noise is most time-consuming, with cost dominated by a matrix multiplication.</figDesc><table>Method PCA SCL 
mDA 

dropout structured scambling 

Time 
7,779 38,849 8,939 
339 
327,075 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Time, in seconds, to compute the feature 
transformation 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Domain adaptation with structural correspondence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;06</title>
		<meeting>the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;06<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="120" to="128" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<meeting><address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Domain Adaptation of Natural Language Processing Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Marginalized denoising autoencoders for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine Learning (ICML12), ICML &apos;12</title>
		<editor>John Langford and Joelle Pineau</editor>
		<meeting>the 29th International Conference on Machine Learning (ICML12), ICML &apos;12<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012-07" />
			<biblScope unit="page" from="767" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">1785</biblScope>
			<biblScope unit="page">1787</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-view learning of word embeddings via cca</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paramveer S Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyle H</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="199" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Portuguese part-of-speech tagging using entropy guided transformation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cícero Nogueira Dos</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Milidiú</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rentería</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Processing of the Portuguese Language</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="143" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">What to do about bad language on the internet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL<address><addrLine>Atlanta, GA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hierarchical bayesian domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="602" to="610" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Tycho Brahe Parsed Corpus of Historical Portuguese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charlotte</forename><surname>Galves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Faria</surname></persName>
		</author>
		<ptr target="http://www.tycho.iel.unicamp.br/ty-cho/corpus/en/index.html" />
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Conference on Artificial Intelligence and Statistics. JMLR W&amp;CP</title>
		<meeting>the 14th International Conference on Artificial Intelligence and Statistics. JMLR W&amp;CP</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Domain adaptation for large-scale sentiment classification: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing coadaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distributional representations for handling sparsity in supervised sequence-labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="495" to="503" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Biased representation learning for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1313" to="1323" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Instance weighting for domain adaptation in nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Comparing two markov methods for part-of-speech tagging of portuguese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Fábio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcelo</forename><surname>Kepler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Finger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Artificial IntelligenceIBERAMIA-SBIA 2006</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="482" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Part-ofspeech tagging for middle english through alignment and projection of parallel diachronic texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesun</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="390" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Portuguese part-of-speech tagging using entropy guided transformation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cícero Nogueira Dos</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruy</forename><forename type="middle">L</forename><surname>Milidiú</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raúl</forename><forename type="middle">P</forename><surname>Rentería</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th international conference on Computational Processing of the Portuguese Language, PROPOR &apos;08</title>
		<meeting>the 8th international conference on Computational Processing of the Portuguese Language, PROPOR &apos;08<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="143" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Crfsuite: a fast implementation of conditional random fields (crfs)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoaki</forename><surname>Okazaki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A survey on transfer learning. Knowledge and Data Engineering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Natural language processing across time: An empirical investigation on italian</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">Massimo</forename><surname>Zanzotto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Natural Language Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="371" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Natural language processing for historical texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Piotrowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Human Language Technologies</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="157" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A maximum entropy model for part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adwait</forename><surname>Ratnaparkhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="1996-04-16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Linguistic structure prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Human Language Technologies</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="274" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semi-supervised learning and domain adaptation in natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Human Language Technologies</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="103" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Feature-rich part-ofspeech tagging with a cyclic dependency network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Feature noising for log-linear structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Wager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Domain adaptation for sequence labeling tasks with a probabilistic language adaptation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning (ICML-13)</title>
		<editor>Sanjoy Dasgupta and David Mcallester</editor>
		<meeting>the 30th International Conference on Machine Learning (ICML-13)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="293" to="301" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
