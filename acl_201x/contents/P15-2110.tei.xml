<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:07+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">One Tense per Scene: Predicting Tense in Chinese Conversations</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ge</surname></persName>
							<email>getao@pku.edu.cn, jih@rpi.edu,</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Ministry of Education</orgName>
								<orgName type="department" key="dep2">School of EECS</orgName>
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Collaborative Innovation Center for Language Ability</orgName>
								<address>
									<postCode>221009</postCode>
									<settlement>Xuzhou, Jiangsu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
								<address>
									<postCode>12180</postCode>
									<settlement>Troy</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Ministry of Education</orgName>
								<orgName type="department" key="dep2">School of EECS</orgName>
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Collaborative Innovation Center for Language Ability</orgName>
								<address>
									<postCode>221009</postCode>
									<settlement>Xuzhou, Jiangsu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Ministry of Education</orgName>
								<orgName type="department" key="dep2">School of EECS</orgName>
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Collaborative Innovation Center for Language Ability</orgName>
								<address>
									<postCode>221009</postCode>
									<settlement>Xuzhou, Jiangsu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">One Tense per Scene: Predicting Tense in Chinese Conversations</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="668" to="673"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We study the problem of predicting tense in Chinese conversations. The unique challenges include: (1) Chinese verbs do not have explicit lexical or grammatical forms to indicate tense; (2) Tense information is often implicitly hidden outside of the target sentence. To tackle these challenges, we first propose a set of novel sentence-level (local) features using rich linguistic resources and then propose a new hypothesis of &quot;One tense per scene&quot; to incorporate scene-level (global) evidence to enhance the performance. Experimental results demonstrate the power of this hybrid approach, which can serve as a new and promising benchmark.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In natural languages, tense is important to indicate the time at which an action or event takes place. In some languages such as Chinese, verbs do not have explicit morphological or grammatical forms to indicate their tense information. Therefore, au- tomatic tense prediction is important for both hu- man's deep understanding of these languages as well as downstream natural language processing tasks (e.g., machine translation ( <ref type="bibr" target="#b6">Liu et al., 2011))</ref>.</p><p>In this paper, we concern "semantic" tense (time of the event relative to speech time) as opposed to morphosyntactic tense systems found in many languages. Our goal is to predict the tense (past, present or future) of the main predicate 1 of each sentence in a Chinese conversation, which has never been thoroughly studied before but is ex- tremely important for conversation understanding.</p><p>Some recent work ( <ref type="bibr" target="#b10">Ye et al., 2006;</ref>) on Chinese <ref type="bibr">1</ref> The main predicate of a sentence can be considered equal to the root of a dependency parse tense prediction found that tense in written lan- guage can be effectively predicted by some fea- tures in local contexts such as aspectual markers (e.g. 着 (zhe), 了 (le), 过 (guo)) and time ex- pressions (e.g., 昨天 (yesterday)). However, it is much more challenging to predict tense in Chinese conversations and there has not been an effective set of rules to predict Chinese tense so far due to the complexity of language-specific phenomena. Let's look at the examples shown in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>In general, there are three unique challenges for tense prediction in Chinese conversations: (1) Informal verbal expressions: sentences in a conversation are often grammatically incorrect, which makes aspectual marker based evidence un- reliable. Moreover, sentences in a conversation often omit important sentence components. For example, in conversation 1 in <ref type="table" target="#tab_0">Table 1</ref>, "如果(if)" which is a very important cue to predict tense of verb "废(destroy)" is omitted.</p><p>(2) Effects of interactions on tense: In contrast to other genres, conversations are interactive, which may have an effect on tense: in some cases, tense can only be inferred by understanding the interac- tions. For example, we can see from conversations 2, 3 and 4 in <ref type="table" target="#tab_0">Table 1</ref> that when the second person (你(you)) is used as the object of the predicate "告 诉(tell)", the predicate describes the action during the conversation and thus its tense is present. In contrast, when the third person is used in a sen- tence, it is unlikely that the tense of the predicate is present because it does not describe an action during the conversation. This challenge is unique to Chinese conversations. (3) Tense ambiguity in a single sentence: Sentence-level analysis is often inadequate to dis- ambiguate tense. For example, it is impossible to determine whether "告诉(tell)" in conversations 3 and 4 in <ref type="table" target="#tab_0">Table 1</ref> is a past action (the speaker al- ready told) or a future action (the speaker hasn't told yet) only based on sentence-level contexts. [如果(if)]你(you)动(touch)我(my)儿子(son)一下(once)，我(I)先(first)废(destroy)了你(you)。 (If you touch my son, I'll destroy you.) 2 b: 我(I)告诉(tell)你(you)一声，航班(flight)取消(cancel)了。(I'm telling you: the flight is canceled.) c:你 (you)刚刚 (just now)和他(to him)说(say)什么(what)了？ (What did you say to him just now?) d: 我(I)[刚才(just now)]告诉(tell)他(him)一声 ，航班(flight)取消(cancel)了。(I told him the flight is canceled.)  In fact, the sentence in conversation 3 omits "刚 才(just now)" which indicates past tense and the sentence in the conversation 4 omits "要(will)" which indicates future tense. If we add the omitted word back to the original sentence, there will not be tense ambiguity. To tackle the above challenges, we propose to predict tense in Chinese conversations from two views -sentence-level (local) and scene- level (global). We first develop a local classifier with linguistic knowledge and new conversation- specific features (Section 2.1). Then we propose a novel framework to exploit the global contexts of the entire scene to infer tense, based on a new "One tense per scene" hypothesis (Section 2.2). We created a new a benchmark data set 2 , which contains 294 conversations (1,857 sentences) and demonstrated the effectiveness of our approach. ble</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Local Predictor</head><p>We develop a Maximum Entropy (MaxEnt) clas- sifier <ref type="bibr" target="#b14">(Zhang, 2004</ref>) as the local predictor. Basic features: The unigrams, bigrams and tri- grams of a sentence. Dependency parsing features: We use the Stan- ford parser <ref type="bibr" target="#b0">(Chen and Manning, 2014</ref>) to conduct dependency parsing 3 on the target sentences and use dependency paths associated with the main predicate of a sentence as well as their dependency types as features. By using the parsing features, we can not only find aspectual markers (e.g., "了") but also capture the effect of sentence structures on the tense. Linguistic knowledge features: We also ex- ploit the following linguistic knowledge from the Grammatical Knowledge-base of Contemporary Chinese ( <ref type="bibr" target="#b11">Yu et al., 1998</ref>) (also known as GKB):</p><p>• Tense of time expressions: GKB lists all common time expressions and their associ- ated tense. For example, GKB can tell us "往 年 (previous years)" and "中 世 纪 (Middle Ages)" can only be associated with the past tense.</p><p>• Function of conjunction words: Some con- junction words may have an effect on tense. For example, the conjunction word "如 果(if)" indicates a conditional clause and the main predicate of this sentence is likely to be future tense. GKB can tell us the function of common Chinese conjunction words.</p><p>Conversation-specific features: As mentioned in Section 1, different person roles being the subject or the object of a predicate may have an effect on the tense in a conversation. We analyze the person roles of the subject and the object of the main pred- icate and encode them as features, which helps our model understand effects of interactions on tense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Global Predictor</head><p>As we discussed before, tense ambiguity in a sen- tence arises from the omissions of sentence com- ponents. According to the principle of efficient information transmission (Jaeger and <ref type="bibr" target="#b2">Levy, 2006;</ref><ref type="bibr" target="#b3">Jaeger, 2010)</ref> and Gricean Maxims ( <ref type="bibr" target="#b1">Grice et al., 1975</ref>) in cooperative theory, the omitted elements can be predicted by considering contextual infor- mation and the tense can be further disambiguated. In order to better predict tense, we propose a new hypothesis:</p><p>One tense per scene: Within a scene, tense in sen- tences tends to be consistent and coherent. <ref type="table" target="#tab_0">Table 1</ref>, we can learn the scene is about the past from the word "刚刚 (just now)" in the first sentence. Therefore, we can exploit this clue to determine the tense of "告诉(tell)" as past.</p><note type="other">During a conversation, a speaker/listener can know the tense of a predicate by either a tense in- dicator in the target sentence or scene-level tense analysis. A scene is a subdivision of a conversa- tion in which the time is continuous and the topic is highly coherent and which does not usually in- volve a change of tense. For example, for the con- versation 3 in</note><p>Therefore, when we are not sure which tense of the main predicate in a sentence should be, we can consider the tense of the entire scene. For example, the conversation 5 in <ref type="table" target="#tab_0">Table 1</ref> is about a past scene because the whole conver- sation is about a past event.</p><p>For the sen- tence "我们(We)在(keep)监视(surveillance)一批 货(a cargo)" where the tense of the predicate is ambiguous (past tense and present tense are both reasonable), we can exploit the tense of the scene (past) to determine its tense as past.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global tense prediction</head><p>Inspired by the burst detection algorithm proposed by <ref type="bibr" target="#b4">Kleinberg (2003)</ref>, we use a 3-state automaton sequence model to globally predict tense based on the above hypothesis. In a conversation with n sentences, each sentence is one element in the se- quence. The sentence's tense can be seen as the hidden state and the sentence's features are the ob- servation. Formally, we define the tense in the i th sentence as t i and the observations (i.e., features) in the sentence as o i . The goal of this model is to output an optimal sequence t * = {t * 1 , t * 2 , ..., t * n } that minimizes the cost function defined as fol- lows:</p><formula xml:id="formula_0">Cost(t, o) = λ n i=1 −lnP (ti|oi)+(1−λ) n−1 i=1 1(ti+1 = ti)<label>(1)</label></formula><p>where 1(·) is an indicator function.</p><p>As we can see in (1), the cost function consists of two parts. The first part is the negative log like- lihood of the local prediction, allowing the model to incorporate the results from the local predic- tor. The second part is the cost of tense inconsis- tency between adjacent sentences, which enables the model to take into account tense consistency in a scene. Finding the optimal sequence is a de- coding process, which can be done using Viterbi algorithm in O(n) time. The parameter λ is used for adjusting weights of these two parts. If λ = 1, the predictor will not consider global tense consis- tency and thus the optimal sequence t * will be the same as the output of the local predictor. <ref type="figure" target="#fig_3">Figure 1</ref> shows how the global predictor works for predicting the tense in the conversation 5 in <ref type="table" target="#tab_0">Table 1</ref>. The global predictor can correct wrong local predictions, especially less confident ones.  <ref type="table" target="#tab_0">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data and Scoring Metric</head><p>To the best of our knowledge, tense prediction in Chinese conversations has never been studied be- fore and there is no existing benchmark for evalu- ation. We collected 294 conversations (including 1,857 sentences) from 25 popular Chinese movies, dramas and TV shows. Each conversation con- tains 2-18 sentences. We manually annotate the main predicate and its tense in each sentence. We use ICTCLAS ( <ref type="bibr" target="#b13">Zhang et al., 2003</ref>) to do word seg- mentation as preprocessing.</p><p>Since tense prediction can be seen as a multi- class classification problem, we use accuracy as the metric to evaluate the performance. We ran- domly split our dataset into three sets: training set (244 conversations), development set (25 conver- sations) and test set (25 conversations) for eval- uation. In evaluation, we ignore imperative sen- tences and sentences without predicates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Results</head><p>We compare our approach with the following baselines:</p><p>• Majority: We label every instance with the majority tense (present tense).</p><p>• Local predictor with basic features (Local(b))</p><p>• Local predictor with basic features + depen- dency parsing features (Local(b+p))</p><p>• Local predictor with basic features + depen- dency parsing features + linguistic knowl- edge features (Local(b+p+l))</p><p>• Local predictor + all features introduced in Section 2.1 (Local(all))</p><p>• Conditional Random Fields (CRFs): We model a conversation as a sequence of sen- tences and predict tense using CRFs <ref type="bibr" target="#b5">(Lafferty et al., 2001</ref>). We implement CRFs using CRFsuite <ref type="bibr" target="#b7">(Okazaki, 2007</ref>) with all features introduced in Section 2.1.</p><p>Among the baselines, Local(b+p) is the most similar model to the approaches in previous work on Chinese tense prediction in written languages ( <ref type="bibr" target="#b10">Ye et al., 2006;</ref><ref type="bibr" target="#b9">Xue, 2008;</ref><ref type="bibr" target="#b6">Liu et al., 2011</ref>). Re- cent work ) used eventuality and modality labels as features that derived from a classifier trained on an annotated corpus. How- ever, the annotated corpus for training the eventu- ality and modality classifier is not publicly avail- able, we cannot duplicate their approaches.   <ref type="table" target="#tab_2">Table 2</ref> shows the results of various models. For our global predictor, the optimal λ (0.4) is tuned on the development set and used on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dev</head><p>According to <ref type="table" target="#tab_2">Table 2</ref>, n-grams and depen- dency parsing features 4 are useful to predict tense, and linguistic knowledge can further im- prove the accuracy of tense prediction. However, adding conversation-specific features (interaction features) does not benefit Local(b+p+l). The first reason is that the subject and the object of the predicates in many sentences are omitted, which is common in Chinese conversations. The other reason, also the main reason, is that simply using the person roles of the subject and the object is not sufficient to depict the interaction. For exam- ple, the subject and the object of the following sen- tences have the same person role but have different tenses because "警告(warn)" is the current action of the speaker but "教(teach)" is not. Therefore, to exploit the interaction features of a conversa- tion, we must deeply understand the meanings of action verbs.</p><formula xml:id="formula_1">我(I)警 告(warn)你(you)。 (I'm warn- ing you.) 我(I)教(teach)你(you)。 (I'll teach you.)</formula><p>The global predictor significantly improves the local predictor's performance (at 95% confidence level according to Wilcoxon Signed-Rank Test), which verifies the effectiveness of "One tense per scene" hypothesis for tense prediction. It is no- table that CRFs do not work well on our dataset. The reason is that the transition pattern of tenses in a sequence of sentences is not easy to learn, es- pecially when the size of training data is not very large. In many cases, the tense of a verb in a sentence is determined by features within the sen- tence, which has nothing to do with tense tran- sition. In these cases, learning tense transition patterns will mislead the model and accordingly affect the performance. In contrast, our global model is more robust because it is based on our "One tense per scene" hypothesis which can be seen as prior linguistic knowledge, thus achieves good performance even when the training data is not sufficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Discussion</head><p>There are still many remaining challenges for tense prediction in Chinese conversations: Omission detection: The biggest challenge for this task is the omission of sentence components. As shown in <ref type="table" target="#tab_0">Table 1</ref>, if omitted words can be re- covered, it will be less likely to make a wrong pre- diction. Word Sense Disambiguation: Some function words which can indicate tense are ambiguous. For example, the function word "要" has many senses. It can mean 将要(will), 想要(want) and 需 要(need), and also it is sometimes used to present an option. It is difficult for a system to correctly predict tense unless it can disambiguate the sense of such function words:</p><p>• 一会儿(later)他(he)要 要 要 (will)过来 (come)。 (He'll come here later.)</p><p>• 我 (I)要 要 要 (want) 吃 (eat) 苹果 (apples)。 (I want to eat apples)</p><p>• 你(you)要 要 要(need)多 多(much)锻 炼(exercise) (You need to take more exercises.)</p><p>• 为什么(why)你(you)要 要 要(opt)救(save)我(me)？ (Why did you save me?)</p><p>Verb Tense Preference: Different verbs may have different tense preferences. For example, "以 为(think)" is often used in the past tense while "认 为(think)" is usually in the present tense:</p><formula xml:id="formula_2">• 我(I)以 以 以 为 为 为(think)他(he)不 会(won't)来(co- me) (I thought he would not come.) • 我(I)认 认 认 为 为 为(think)他(he)不 会(won't)来(co- me) (I think he won't come.)</formula><p>Generic and specific subject/object: Whether the subject/object is generic or specific has an ef- fect on tense. For example, in the sentence "那 场(that)战 争(war)太(very)残 酷(brutal)了", the predicate "残 酷(brutal)" is in the past tense while in the sentence "战 争(war)太(very)残 酷(brutal)了", the predicate "残酷(brutal)" is in the present tense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Early work on Chinese tense prediction ( <ref type="bibr" target="#b10">Ye et al., 2006;</ref><ref type="bibr" target="#b9">Xue, 2008)</ref> modeled this task as a multi-class classification problem and used ma- chine learning approaches to solve the problem. Recent work ( <ref type="bibr" target="#b6">Liu et al., 2011;</ref>) studied distant an- notation of tense from a bilingual parallel cor- pus. Among them,  and  improved tense prediction by using eventuality and modality labels. How- ever, none of the previous work focused on the specific challenge of the tense prediction in oral languages although the dataset used by <ref type="bibr" target="#b6">Liu et al. (2011)</ref> includes conversations. In contrast, this paper presents the unique challenges and corre- sponding solutions to tense prediction in conver- sations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>This paper presents the importance and challenges of tense prediction in Chinese conversations and proposes a novel solution to the challenges. In the future, we plan to further study this problem by focusing on omission detection, verb tense preference from the view of pragmatics, and jointly learning the local and global predictors. In addition, we will study predicting the tense of mul- tiple predicates in a sentence and identifying im- perative sentences in a conversation, which is also a challenge of tense prediction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>3</head><label>3</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>你(you)要(will)干(do)吗(what)去(go)？ (What are you going to do?) f: 我(I)[要(will)]告诉(tell)他(him)一声 ，航班(fight)取消(cancel)了。(I'll tell him the flight is canceled.) 5 a: 发生(happen)了什么(what)事情(event)？ (What happened?) b: 我(I)跟吴清(Wu Qing)一起(with) (I was with Wu Qing) b: 我们(We)在(keep)监视(surveilance)一批货(a cargo) (We were keeping surveillance on a cargo...) b: 我 们(We)怀疑(suspect)那 些(thoses)是(are)偷 来 的(stolen)文 物(antiques) (We suspected those were stolen an- tiques) b: 那些人(those guys)，突然(suddenly)就走(walk)出来(out)打(beat)我们(us) (Suddenly, all those guys walked out to beat us up!) b: 我(I)要(want)报警(call the police)他们(they)才停手(stop) (They stopped only when I tried to call the police)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Global tense prediction for the conversation 5 in Table 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Five sample conversations that show the challenges in tense prediction in Chinese conversations. 
a,b,c,d at the beginning of each sentence denote various speakers. The words in square brackets are 
omitted content in the original sentences and the underlined words are main predicates. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Tense prediction accuracy.</head><label>2</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="2"> http://nlp.cs.rpi.edu/data/chinesetense.zip 3 We use CCProcessed dependencies.</note>

			<note place="foot" n="4"> We also tried adding POS tags to dependency paths but didn&apos;t see improvements because POS information has been implicitly indicated by dependency types and thus becomes redundant.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank all the anonymous reviewers for their constructive suggestions. We thank Prof. Shi-wen Yu and Xun Wang for providing insights from Chinese linguistics. This work is supported by National Key Basic Research Program of China 2014CB340504, NSFC project 61375074, U.S. DARPA Award No. FA8750-13-2-0045 and China Scholarship Council <ref type="bibr">(CSC, No. 201406010174)</ref>. The contact author of this paper is Zhifang Sui.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>In EMNLP</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Paul Grice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry L</forename><surname>Morgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Syntax and semantics. Logic and conversation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="41" to="58" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Speakers optimize information density through syntactic reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Redundancy and reduction: Speakers manage syntactic information density</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Jaeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive psychology</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="62" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bursty and hierarchical structure in streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="373" to="397" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando Cn</forename><surname>Pereira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning from Chinese-English parallel data for Chinese tense prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feifan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNLP</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">CRFsuite: a fast implementation of conditional random fields (CRFs)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoaki</forename><surname>Okazaki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Buy one get one free: Distant annotation of Chinese tense, event type, and modality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automatic inference of the temporal location of situations in Chinese text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Latent features in automatic tense translation between Chinese and English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><forename type="middle">Li</forename><surname>Fossum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Abney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGHAN workshop</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The grammatical knowledge-base of contemporary Chinese-a complete specification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuefeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunyun</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic inference of the tense of Chinese events using implicit information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yucheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hhmm-based Chinese lexical analyzer ictclas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua-Ping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Kui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De-Yi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGHAN workshop</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Maximum entropy modeling toolkit for Python and C++</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
