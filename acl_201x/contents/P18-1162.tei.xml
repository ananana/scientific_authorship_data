<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:31+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Question Condensing Networks for Answer Selection in Community Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Collaborative Innovation Center for Language Ability</orgName>
								<address>
									<postCode>221009</postCode>
									<settlement>Xuzhou</settlement>
									<region>Jiangsu</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Question Condensing Networks for Answer Selection in Community Question Answering</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1746" to="1755"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1746</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Answer selection is an important subtask of community question answering (CQA). In a real-world CQA forum, a question is often represented as two parts: a subject that summarizes the main points of the question, and a body that elaborates on the subject in detail. Previous researches on answer selection usually ignored the difference between these two parts and concatenated them as the question representation. In this paper, we propose the Question Condensing Networks (QCN) to make use of the subject-body relationship of community questions. In this model, the question subject is the primary part of the question representation, and the question body information is aggregated based on similarity and disparity with the question subject. Experimental results show that QCN outperforms all existing models on two CQA datasets.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Community question answering (CQA) has seen a spectacular increase in popularity in recent years. With the advent of sites like Stack Overflow 1 and Quora 2 , more and more people can freely ask any question and expect a variety of answers. With the influx of new questions and the varied qual- ity of provided answers, it is very time-consuming for a user to inspect them all. Therefore, develop- ing automated tools to identify good answers for a question is of practical importance.</p><p>A typical example for CQA is shown in <ref type="table">Table 1</ref>. In this example, Answer 1 is a good answer, be- cause it provides helpful information, e.g., "check it to the traffic dept". Although Answer 2 is rele- vant to the question, it does not contain any useful information so that it should be regarded as a bad answer.</p><p>From this example, we can observe two charac- teristics of CQA that ordinary QA does not pos- sess. First, a question includes both a subject that gives a brief summary of the question and a body that describes the question in detail. The question- ers usually convey their main concern and key in- formation in the question subject. Then, they pro- vide more extensive details about the subject, seek help, or express gratitude in the question body. Second, the problem of redundancy and noise is prevalent in CQA ( <ref type="bibr" target="#b23">Zhang et al., 2017)</ref>. Both ques- tions and answers contain auxiliary sentences that do not provide meaningful information.</p><p>Previous researches ( <ref type="bibr" target="#b17">Tran et al., 2015;</ref><ref type="bibr" target="#b8">Joty et al., 2016</ref>) usually treat each word equally in the question and answer representation. How- ever, due to the redundancy and noise problem, only part of text from questions and answers is useful to determine the answer quality. To make things worse, they ignored the difference between question subject and body, and simply concate- nated them as the question representation. Due to the subject-body relationship described above, this simple concatenation can aggravate the re- dundancy problem in the question. In this paper, we propose the Question Condensing Networks (QCN) to address these problems.</p><p>In order to utilize the subject-body relationship in community questions, we propose to treat the question subject as the primary part of the ques- tion, and aggregate the question body information based on similarity and disparity with the ques- tion subject. The similarity part corresponds to the information that exists in both question sub- ject and body, and the disparity part corresponds to the additional information provided by the ques-Question Subject Checking the history of the car.</p><p>Question body How can one check the history of the car like maintenance, accident or service history. In every advertisement of the car, people used to write "Accident Free", but in most cases, car have at least one or two accident, which is not easily detectable through Car Inspection Company. Share your opinion in this regard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer1</head><p>Depends on the owner of the car.. if she/he reported the accident/s i believe u can check it to the traffic dept.. but some owners are not doing that especially if its only a small accident.. try ur luck and go to the traffic dept..</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer2</head><p>How about those who claim a low mileage by tampering with the car fuse box? In my sense if you're not able to detect traces of an accident then it is probably not worth mentioning... For best results buy a new car :) <ref type="table">Table 1</ref>: An example question and its related answers in CQA. The text is shown in its original form, which may contain errors in typing.</p><p>tion body. Both information can be important for question representation. In our model, they are processed separately and the results are combined to form the final question representation. In order to reduce the impact of redundancy and noise in both questions and answers, we pro- pose to align the question-answer pairs using the multi-dimensional attention mechanism. Differ- ent from previous attention mechanisms that com- pute a scalar score for each token pair, multi- dimensional attention, first proposed in <ref type="bibr" target="#b15">Shen et al. (2018)</ref>, computes one attention score for each di- mension of the token embedding. Therefore, it can select the features that can best describe the word's specific meaning in the given context. Therefore, we can learn the interaction between questions and answers more accurately.</p><p>The main contributions of our work can be sum- marized as follows:</p><p>• We propose to treat the question subject and the question body separately in community question answering. We treat the question subject as the primary part of the question, and aggregate the question body information based on similarity and disparity with the question subject.</p><p>• We introduce a new method that uses the multi-dimensional attention mechanism to align question-answer pair. With this at- tention mechanism, the interaction between questions and answers can be learned more accurately.</p><p>• Our proposed Question Condensing Net- works (QCN) achieves the state-of-the-art performance on two SemEval CQA datasets, outperforming all exisiting SOTA models by a large margin, which demonstrates the effec- tiveness of our model. <ref type="bibr">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Description</head><p>A community question answering consists of four parts, which can be formally defined as a tuple of four elements (S, B, C, y). S = [s 1 , s 2 , ..., s l ] de- notes the subject of a question whose length is l, where each s i is a one-hot vector whose dimen- sion equals the size of the vocabulary. Similarly, B = [b 1 , b 2 , ..., b m ] denotes the body of a ques- tion whose length is m. C = [c 1 , c 2 , ..., c n ] de- notes an answer corresponding to that question whose length is n. y ∈ Y is the label represent- ing the degree to which it can answer that ques- tion. Y = {Good, PotentiallyUseful, Bad} where Good indicates the answer can answer that ques- tion well, PotentiallyUseful indicates the answer is potentially useful to the user, and Bad indi- cates the answer is just bad or useless. Given {S, B, C}, the task of CQA is to assign a label to each answer based on the conditional probability P r(y|S, B, C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Model</head><p>In this paper, we propose Question Condensing Networks (QCN) which is composed of the fol- lowing modules. The overall architecture of our model is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Word-Level Embedding</head><p>Word-level embeddings are composed of two components: GloVe ( <ref type="bibr" target="#b14">Pennington et al., 2014)</ref> word vectors trained on the domain-specific unan- notated corpus provided by the task 4 , and con- volutional neural network-based character embed- dings which are similar to <ref type="bibr" target="#b9">(Kim et al., 2016)</ref>. Web text in CQA forums differs largely from normal- ized text in terms of spelling and grammar, so specifically trained GloVe vectors can model word interactions more precisely. Character embedding has proven to be very useful for out-of-vocabulary (OOV) words, so it is especially suitable for noisy web text in CQA.</p><p>We concatenate these two embedding vectors for every word to generate word-level embeddings S emb ∈ R d×l , B emb ∈ R d×m , C emb ∈ R d×n , where d is the word-level embedding size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Question Condensing</head><p>In this section, we condense the question repre- sentation using subject-body relationship. In most cases, the question subject can be seen as a sum- mary containing key points of the question, the question body is relatively lengthy in that it needs to explain the key points and add more details about the posted question. We propose to cheat the question subject as the primary part of the question representation, and aggregate question body infor- mation from two perspectives: similarity and dis- parity with the question subject. To achieve this goal, we use an orthogonal decomposition strat- egy, which is first proposed by , to decompose each question body embedding into a parallel component and an orthogonal compo- 4 http://alt.qcri.org/semeval2015/ task3/index.php?id=data-and-tools nent based on every question subject embedding:</p><formula xml:id="formula_0">b i,j para = b j emb · s i emb s i emb · s i emb s i emb (1) b i,j orth = b j emb − b i,j para (2)</formula><p>All vectors in the above equations are of length d.</p><p>Next we describe the process of aggregating the question body information based on the parallel component in detail. The same process can be ap- plied to the orthogonal component, so at the end of the fusion gate we can obtain S orth and S orth respectively. The decomposed components are passed through a fully connected layer to compute the multi-dimensional attention weights. Here we use the scaled tanh activation, which is similar to <ref type="bibr" target="#b15">Shen et al. (2018)</ref>, to prevent large difference among scores while it still has a range large enough for output:</p><formula xml:id="formula_1">a i,j para = c · tanh W p1 b i,j para + b p1 /c<label>(3)</label></formula><p>where W p1 ∈ R d×d and b p1 ∈ R d are parame- ters to be learned, and c is a hyper-parameter to be tuned.</p><p>The obtained word-level alignment tensor A para ∈ R d×l×m is then normalized along the third dimension to produce the attention weights over the question body for each word in the ques- tion subject. The output of this attention mecha- nism is a weighted sum of the question body em- beddings for each word in the question subject:</p><formula xml:id="formula_2">w i,j para = exp a i,j para m j=1 exp a i,j para (4) s i ap = m j=1 w i,j para b j emb (5)</formula><p>where means point-wise product. This multi- dimensional attention mechanism has the advan- tage of selecting features of a word that can best describe the word's specific meaning in the given context. In order to determine the importance be- tween the original word in the question subject and the aggregated information from the question body with respect to this word, a fusion gate is utilized to combine these two representations:</p><formula xml:id="formula_3">F para = σ (W p2 S emb + W p3 S ap + b p2 ) (6) S para = F para S emb + (1 − F para ) S ap (7)</formula><p>where W p2 , W p3 ∈ R d×d , and b p2 ∈ R d are learnable parameters of the fusion gate, and F para , S emb , S ap , S para ∈ R d×l . The final ques- tion representation S rep ∈ R 2d×l is obtained by concatenating S para and S orth along the first di- mension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Answer Preprocessing</head><p>This module has two purposes. First, we try to map each answer word from embedding space C emb ∈ R d×n to the same interaction space C rep ∈ R 2d×n as the question. Second, similar to <ref type="bibr" target="#b18">Wang and Jiang (2017)</ref>, a gate is utilized to con- trol the importance of different answer words in determining the question-answer relation:</p><formula xml:id="formula_4">C rep =σ (W c1 C emb + b c1 ) tanh (W c2 C emb + b c2 )<label>(8)</label></formula><p>where W c1 , W c2 ∈ R d×2d and b c1 , b c2 ∈ R 2d are parameters to be learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Question Answer Alignment</head><p>We apply the multi-dimensional attention mech- anism to the question and answer representa- tion S rep and C rep to obtain word-level align- ment tensor A align ∈ R 2d×l×n . Similar to the multi-dimensional attention mechanism described above, we can compute attention weights and weighted sum for both the question representation and the answer representation :</p><formula xml:id="formula_5">˜ a i,j align = W a1 s i rep + W a2 c j rep + b a (9) a i,j align = c · tanhã tanh˜tanhã i,j align /c<label>(10)</label></formula><formula xml:id="formula_6">s i ai = n j=1 exp a i,j align n j=1 exp a i,j align c j rep (11) c j ai = l i=1 exp a i,j align l i=1 exp a i,j align s i rep (12)</formula><p>where W a1 , W a2 ∈ R 2d×2d and b a ∈ R 2d are parameters to be learned. To attenuate the effect of incorrect attendance, input and output of this attention mechanism are concatenated and fed to the subsequent layer. Finally, we obtain the ques- tion and answer representation</p><formula xml:id="formula_7">S att ∈ R 4d×l = [S rep ; S ai ], C att ∈ R 4d×n = [C rep ; C ai ].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Interaction Summarization</head><p>In this layer, the multi-dimensional self-attention mechanism is employed to summarize two se- quences of vectors (S att and C att ) into two fixed- length vectors s sum ∈ R 4d and c sum ∈ R 4d .</p><formula xml:id="formula_8">A s = W s2 tanh (W s1 S att + b s1 ) + b s2 (13) s sum = n i=1 exp a i s n i=1 exp (a i s ) s i att<label>(14)</label></formula><p>where W s1 , W s2 ∈ R 4d×4d and b s1 , b s2 ∈ R 4d are parameters to be learned. The same process can be applied to C att and obtain c sum .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Prediction</head><p>In this component, s sum and c sum are concate- nated and fed into a two-layer feed-forward neural network. At the end of the last layer, the sof tmax function is applied to obtain the conditional prob- ability distribution P r(y|S, B, C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We use two community question answering datasets from SemEval ( <ref type="bibr" target="#b11">Nakov et al., , 2017</ref>) to evaluate our model. The statistics of these datasets are listed in  <ref type="table" target="#tab_1">Table 2</ref>: Statistics of two CQA datasets. We can see from the statistics that the question body is much lengthier than the question subject. Thus, it is necessary to condense the question representation.</p><p>consists of questions and a list of answers for each question, and each question consists of a short ti- tle and a more detailed description. There are also some metadata associated with them, e.g., user ID, date of posting, and the question category. We do not use the metadata because they failed to boost performance in our model. Since the SemEval 2017 dataset is an updated version of SemEval 2016 6 , and shares the same evaluation metrics with SemEval 2016, we choose to use the SemEval 2017 dataset for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Metrics</head><p>In order to facilitate comparison, we adopt the evaluation metrics used in the official task or prior work. For the SemEval 2015 dataset, the offi- cial scores are macro-averaged F1 and accuracy over three categories. However, many recent re- searches ( <ref type="bibr" target="#b8">Joty et al., , 2016</ref>) switched to a binary classification set- ting, i.e., identifying Good vs. Bad answers. Be- cause binary classification is much closer to a real- world CQA application. Besides, the Potential- lyUseful class is both the smallest and the noisiest class, making it the hardest to predict. To make it worse, its impact is magnified by the macro- averaged F1. Therefore, we adopt the F1 score and accuracy on two categories for evaluation.</p><p>SemEval 2017 regards answer selection as a ranking task, which is closer to the application sce- nario. As a result, mean average precision (MAP) is used as an evaluation measure. For a perfect ranking, a system has to place all Good answers above the PotentiallyUseful and Bad answers. The latter two are not actually distinguished and are considered Bad in terms of evaluation. Addition- <ref type="bibr">6</ref> The SemEval 2017 dataset provides all the data from 2016 for training , and fresh data for testing, but it does not include a development set. Following previous work <ref type="bibr" target="#b5">(Filice et al., 2017)</ref>, we use the 2016 official test set as the develop- ment set.</p><p>ally, standard classification measures like accuracy and F1 score are also reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details</head><p>We use the tokenizer from NLTK <ref type="bibr" target="#b1">(Bird, 2006</ref>) to preprocess each sentence. All word embed- dings in the sentence encoder layer are initial- ized with the 300-dimensional GloVe ( <ref type="bibr" target="#b14">Pennington et al., 2014</ref>) word vectors trained on the domain- specific unannotated corpus, and embeddings for out-of-vocabulary words are set to zero. We use the Adam Optimizer ( <ref type="bibr" target="#b10">Kingma and Ba, 2014</ref>) for optimization with a first momentum coefficient of 0.9 and a second momentum coefficient of 0.999. We perform a small grid search over combina- tions of initial learning rate [1 × 10 −6 , 3 × 10 −6 , 1 × 10 −5 ], L2 regularization parameter [1 × 10 −7 , 3 × 10 −7 , 1 × 10 −6 ], and batch size <ref type="bibr">[8,</ref><ref type="bibr">16,</ref><ref type="bibr">32]</ref>. We take the best configuration based on perfor- mance on the development set, and only evalu- ate that configuration on the test set. In order to mitigate the class imbalance problem, median fre- quency balancing <ref type="bibr" target="#b2">Eigen and Fergus (2015)</ref> is used to reweight each class in the cross-entropy loss. Therefore, the rarer a class is in the training set, the larger weight it will get in the cross entropy loss. Early stopping is applied to mitigate the problem of overfitting. For the SemEval 2017 dataset, the conditional probability over the Good class is used to rank all the candidate answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>In this section, we evaluate our QCN model on two community question answering datasets from Se- mEval shared tasks.  • HITSZ-ICRC (Hou et al., 2015): It pro- posed ensemble learning and hierarchical classification method to classify answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">SemEval 2015 Results</head><p>• Graph-cut (Joty et al., 2015): It modeled the relationship between pairs of answers at any distance in the same question thread, based on the idea that similar answers should have similar labels.</p><p>• FCCRF (Joty et al., 2016): It used locally learned classifiers to predict the label for each individual node, and applied fully connected CRF to make global inference.</p><p>• CNN-LSTM-CRF ( <ref type="bibr" target="#b22">Xiang et al., 2016)</ref>: The question and its answers are linearly con- nected in a sequence and encoded by CNN. An attention-based LSTM with a CRF layer is then applied on the encoded sequence.</p><p>• BGMN (Wu et al., 2017b): It used the mem- ory mechanism to iteratively aggregate more relevant information which is useful to iden- tify the relationship between questions and answers.</p><p>Baselines include top systems from SemEval 2015 (1, 2), systems relying on thread level infor- mation to make global inference <ref type="bibr">(3,</ref><ref type="bibr">4)</ref>, and neu- ral network based systems (5, 6). We observe that our proposed QCN can achieve the state-of-the-art performance on this dataset, outperforming previ- ous best model (6) by 1.7% in terms of F1 and 3.4% in terms of accuracy.  Notably, Systems (1, 2, 3, 4) have heavy feature engineering, while QCN only uses automatically- learned feature vectors, demonstrating that our QCN model is concise as well as effective. Fur- thermore, our model can outperform systems rely- ing on thread level information to make global in- ference <ref type="bibr">(3,</ref><ref type="bibr">4)</ref>, showing that modeling interaction between the question-answer pair is useful enough for answer selection task. Finally, neural network based systems (5, 6) used attention mechanism in sentence representation but ignored the subject- body relationship in community questions. QCN can outperform them by a large margin, showing that condensing question representation helps in the answer selection task. <ref type="table" target="#tab_5">Table 4</ref> compares our model with the following baselines:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MAP F1 Acc</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">SemEval 2017 Results</head><p>• KeLP ( <ref type="bibr" target="#b5">Filice et al., 2017)</ref>: It used syn- tactic tree kernels with relational links be- tween questions and answers, together with some standard text similarity measures lin- early combined with the tree kernel.</p><p>• Beihang-MSRA ( <ref type="bibr" target="#b3">Feng et al., 2017)</ref>: It used gradient boosted regression trees to combine traditional NLP features and neural network- based matching features.</p><p>• ECNU (Wu et al., 2017a): It combined a su- pervised model using traditional features and a convolutional neural network to represent the question-answer pair.</p><p>• LSTM: It is a simple neural network based baseline that we implemented. In this model, the question subject and the question body are concatenated, and an LSTM is used to ob- tain the question and answer representation.</p><p>• LSTM-subject-body: It is another neural network based baseline that we implemented. LSTM is applied on the question subject and body respectively, and the results are concate- nated to form question representation.</p><p>Baselines include top systems from the Se- mEval 2017 CQA task <ref type="figure" target="#fig_0">(1, 2, 3</ref>) and two neural net- work based baselines (4, 5) that we implemented. (5) can outperform (4), showing that treating ques- tion subject and body differently can indeed boot model performance. Comparing <ref type="formula">(6)</ref> with <ref type="formula">(5)</ref>, we can draw the conclusion that orthogonal decom- position is more effective than simple concatena- tion, because it can flexibly aggregate related in- formation from the question body with respect to the main subject. In the example listed in <ref type="table">Table 1</ref>, attention heatmap of A orth indicates that QCN can effectively find additional information like "main- tenance, accident or service history", while (5) fails to do so.</p><p>QCN has a great advantage in terms of accu- racy. We hypothesize that QCN focuses on mod- eling interaction between questions and answers, i.e., whether an answer can match the correspond- ing question. Many pieces of previous work fo- cus on modeling relationship between answers in a question thread, i.e., which answer is more suit- able in consideration of all other answers. As a consequence, their models have a greater advan- tage in ranking while QCN has a greater advan- tage in classification. Despite all this, QCN can still obtain better ranking performance. • w/o character embeddings where word- level embeddings are only composed of 600- dimensional GloVe word vectors trained on the domain-specific unannotated corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Study</head><p>• subject-body alignment where we use the same attention mechanism as Question An- swer Alignment to obtain weighted sum of Model Acc (1) w/o task-specific word embeddings 78.81 (2) w/o character embeddings 78.05 (3) subject-body alignment 77.38 (4) subject-body concatenation 76.06 (5) w/o multi-dimensional attention 78.33 (6) subject only 74.02 (7) body only 75.57 (8) similarity only 79.11 (9) disparity only 78.24 (10) QCN 80.71 the question body for each question subject word, and then the result is concatenated with S emb to obtain question representation S rep .</p><p>• subject-body concatenation where we con- catenate question subject and body text, and use the preprocessing step described in sec- tion 3.3 to obtain S rep .</p><p>• w/o multi-dimensional attention where the multi-dimensional attention mechanism is re- placed by vanilla attention in all modules, i.e., attention score for each token pair is a scalar instead of a vector.</p><p>• subject only where only question subject is used as question representation.</p><p>• body only where only question body is used as question representation.</p><p>• similarity only where the parallel component alone is used in subject-body interaction.</p><p>• disparity only where the orthogonal compo- nent alone is used in subject-body interaction.</p><p>The results are listed in <ref type="table" target="#tab_6">Table 5</ref>. We can see that using task-specific embeddings and character em- beddings both contribute to model performance. This is because CQA text is non-standard. There are quantities of informal language usage, such as abbreviations, typos, emoticons, and grammati- cal mistakes. Using task-specific embeddings and character embeddings can help to attenuate the OOV problem.</p><p>Using orthogonal decomposition (10) instead of subject-body alignment (3) can bring about signif- icant performance gain. This is because not only  <ref type="table">Which  airline  offers  the  best  fares  or  low  cost</ref> to Kuala Lumpur (c) A align <ref type="figure">Figure 2</ref>: Attention probabilities in A para , A orth and A align . In order to visualize the multi-dimensional attention vector, we use the L2 norm of the attenion vector for representation. the similar part of the question body to the ques- tion subject is useful for the question representa- tion, the disparity part can also provide additional information. In the example listed in <ref type="table">Table 1</ref>, ad- ditional information like "maintenance, accident or service history" is also important to determine answer quality.</p><p>QCN outperforms (4) by a great margin, demonstrating that subject-body relationship in community questions helps to condense question representation. Therefore, QCN can identify the meaningful part of the question representation that helps to determine answer quality.</p><p>Using the multi-dimensional attention can fur- ther boost model performance, showing that the multi-dimensional attention can model the inter- action between questions and answers more pre- cisely.</p><p>Comparing QCN with (6) and <ref type="formula">(7)</ref>, we can con- clude that both the subject and the body are indis- pensable for question representation. (8) outper- forms (9), demonstrating the parallel component is more useful in subject-body interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Qualitative Study</head><p>To gain a closer view of what dependencies are captured in the subject-body pair and the question- answer pair, we visualize the attention probabili- ties A para , A orth and A align by heatmap. A train- ing example from SemEval 2015 is selected for illustration.</p><p>In <ref type="figure">Figure 2</ref>, we can draw the following con- clusions. First, orthogonal decomposition helps to divide the labor of identifying similar parts in the parallel component and collecting related in- formation in the question body in the orthogonal component. For instance, for the word "Kuala" in the question subject, its parallel alignment score focuses more on "Doha" and "Travel", while its orthogonal alignment score focuses on "arrange" and "package", which is the purpose of the travel and therefore is also indispensable for sentence representation. Second, semantically important words such as "airline" and "fares" dominate the attention weights, showing that our QCN model can effectively select words that are most repre- sentative for the meaning of the whole sentence. Lastly, words that are useful to determine answer quality stand out in the question-answer interac- tion matrix, demonstrating that question-answer relationship can be well modeled. For example, "best" and "low" are the words that are more im- portant in the question-answer relationship, they are emphasized in the question-answer alignment matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>One main task in community question answering is answer selection, i.e., to rate the answers ac- cording to their quality. The SemEval CQA tasks ( <ref type="bibr" target="#b11">Nakov et al., , 2017</ref>) provide univer- sal benchmark datasets for evaluating researches on this problem.</p><p>Earlier work of answer selection in CQA relied heavily on feature engineering, linguistic tools, and external resource.  investi- gated a wide range of feature types including sim- ilarity features, content features, thread level/meta features, and automatically generated features for SemEval CQA models. <ref type="bibr" target="#b17">Tran et al. (2015)</ref> studied the use of topic model based features and word vector representation based features in the answer re-ranking task. <ref type="bibr" target="#b4">Filice et al. (2016)</ref> designed var- ious heuristic features and thread-based features that can signal a good answer. Although achiev- ing good performance, these methods rely heav- ily on feature engineering, which requires a large amount of manual work and domain expertise.</p><p>Since answer selection is inherently a ranking task, a few recent researches proposed to use local features to make global ranking decision. <ref type="bibr">BarrónCedeño et al. (2015)</ref> was the first work that ap- plies structured prediction model on CQA answer selection task. <ref type="bibr" target="#b8">Joty et al. (2016)</ref> approached the task with a global inference process to exploit the information of all answers in the question-thread in the form of a fully connected graph.</p><p>To avoid feature engineering, many deep learn- ing models have been proposed for answer selec- tion. Among them, <ref type="bibr" target="#b23">Zhang et al. (2017)</ref> proposed a novel interactive attention mechanism to address the problem of noise and redundancy prevalent in CQA. <ref type="bibr" target="#b16">Tay et al. (2017)</ref> introduced temporal gates for sequence pairs so that questions and answers are aware of what each other is remembering or forgetting. Simple as their model are, they did not consider the relationship between question subject and body, which is useful for question condensing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion and Future Work</head><p>We propose Question Condensing Networks (QCN), an attention-based model that can utilize the subject-body relationship in community ques- tions to condense question representation. By or- thogonal decomposition, the labor of identifying similar parts and collecting related information in the question body can be well divided in two dif- ferent alignment matrices. To better capture the interaction between the subject-body pair and the question-answer pair, the multi-dimensional atten- tion mechanism is adopted. Empirical results on two community question answering datasets in Se- mEval demonstrate the effectiveness of our model. In future work, we will try to incorporate more hand-crafted features in our model. Furthermore, since thread-level features have been explored in previous work <ref type="bibr" target="#b8">Joty et al., , 2016</ref>), we will verify their effective- ness in our architecture.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture for Question Condensing Network (QCN). Each block represents a vector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>For thorough comparison, besides the preceding models, we implement nine extra baselines on the SemEval 2017 dataset to analyze the improve- ments contributed by each part of our QCN model: • w/o task-specific word embeddings where word embeddings are initialized with the 300-dimensional GloVe word vectors trained on Wikipedia 2014 and Gigaword 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table>The corpora contain 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 compares our model with the following baselines:</head><label>3</label><figDesc></figDesc><table>Methods 
F1 
Acc 
(1) JAIST 
78.96 79.10 
(2) HITSZ-ICRC 
76.52 76.11 
(3) Graph-cut 
80.55 79.80 
(4) FCCRF 
81.50 80.50 
(5) BGMN 
77.23 78.40 
(6) CNN-LSTM-CRF 82.22 82.24 
(7) QCN 
83.91 85.65 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Comparisons on the SemEval 2015 
dataset. 

• JAIST (Tran et al., 2015): It used an SVM 
classifier to incorporate various kinds of fea-
tures , including topic model based features 
and word vector representations. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Comparisons on the SemEval 2017 
dataset. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Ablation studies on the SemEval 2017 
dataset. 

</table></figure>

			<note place="foot" n="1"> https://stackoverflow.com/ 2 https://www.quora.com/</note>

			<note place="foot" n="3"> An implementation of our model is available at https: //github.com/pku-wuwei/QCN.</note>

			<note place="foot" n="5"> http://www.qatarliving.com/forum</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Acknowledgments</head><p>We would like to thank anonymous reviewers for their insightful comments. Our work is supported by National Natural Science Foundation of China under Grant No.61433015 and the National Key Research and Development Program of China un-der Grant No.2017YFB1002101. The correspond-ing author of this paper is Houfeng Wang.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Thread-level information for comment classification in community question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Filice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Da San</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><forename type="middle">R</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluís</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2015-07-26" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="687" to="693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">NLTK: the natural language toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2006, 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-07" />
			<biblScope unit="page" from="17" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision, ICCV 2015</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="2650" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Beihang-msra at semeval-2017 task 3: A ranking system with neural matching features for community question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
		<meeting>the 11th International Workshop on Semantic Evaluation<address><addrLine>SemEval@ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-03" />
			<biblScope unit="page" from="280" to="286" />
		</imprint>
	</monogr>
<note type="report_type">Vancouver</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Kelp at semeval-2016 task 3: Learning semantic relations between questions and answers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Filice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Croce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Basili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2016</title>
		<meeting>the 10th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2016<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-16" />
			<biblScope unit="page" from="1116" to="1123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Kelp at semeval-2017 task 3: Learning pairwise patterns in community question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Filice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Da San</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
		<meeting>the 11th International Workshop on Semantic Evaluation<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-03" />
			<biblScope unit="page" from="326" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">HITSZICRC: exploiting classification approach for answer selection in community question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongshuai</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2015</title>
		<meeting>the 9th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2015<address><addrLine>Denver, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-04" />
			<biblScope unit="page" from="196" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Global thread-level inference for comment classification in community question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shafiq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Da San</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluís</forename><surname>Filice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-17" />
			<biblScope unit="page" from="573" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Joint learning with global inference for comment classification in community question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shafiq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluís</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>San Diego California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-12" />
			<biblScope unit="page" from="703" to="713" />
		</imprint>
	</monogr>
	<note>NAACL HLT 2016</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Character-aware neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence<address><addrLine>Phoenix, Arizona, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1217" />
			<biblScope unit="page" from="2741" to="2749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>CoRR abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semeval-2017 task 3: Community question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doris</forename><surname>Hoogeveen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamdy</forename><surname>Mubarak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karin</forename><surname>Verspoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
		<meeting>the 11th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2017-08-03" />
			<biblScope unit="page" from="27" to="48" />
		</imprint>
	</monogr>
<note type="report_type">Vancouver</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semeval-2015 task 3: Answer selection in community question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walid</forename><surname>Magdy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Randeree</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2015</title>
		<meeting>the 9th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2015<address><addrLine>Denver, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-04" />
			<biblScope unit="page" from="269" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Abed Alhakim Freihat, Jim Glass, and Bilal Randeree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walid</forename><surname>Magdy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamdy</forename><surname>Mubarak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation, SemEval@NAACLHLT 2016</title>
		<meeting>the 10th International Workshop on Semantic Evaluation, SemEval@NAACLHLT 2016<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-16" />
			<biblScope unit="page" from="525" to="545" />
		</imprint>
	</monogr>
	<note>Semeval-2016 task 3: Community question answering</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-25" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Disan: Directional self-attention network for rnn/cnn-free language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Cross temporal recurrent networks for ranking question answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu Cheung</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hui</surname></persName>
		</author>
		<idno>CoRR abs/1711.07656</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">JAIST: combining multiple features for answer selection in community question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vu</forename><surname>Quan Hung Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tu</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Son</forename><forename type="middle">Bao</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2015</title>
		<meeting>the 9th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2015<address><addrLine>Denver, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-04" />
			<biblScope unit="page" from="215" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A compareaggregate model for matching text sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sentence similarity learning by lexical decomposition and composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><surname>Ittycheriah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">ECNU at semeval-2017 task 3: Using traditional and deep learning methods to address community question answering task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoshun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
		<meeting>the 11th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2017-08-03" />
			<biblScope unit="page" from="365" to="369" />
		</imprint>
	</monogr>
<note type="report_type">Vancouver</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bidirectional gated memory networks for answer selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chinese Computational Linguistics and Natural Language Processing Based on Naturally Annotated Big Data. LNAI 10565</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="251" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Incorporating label dependency for answer quality tagging in community question answering via CNN-LSTM-CRF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buzhou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers</title>
		<meeting><address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-11" />
			<biblScope unit="page" from="1231" to="1241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attentive interactive neural networks for answer selection in community question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-02-04" />
			<biblScope unit="page" from="3525" to="3531" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
