<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:18+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scattertext: a Browser-Based Tool for Visualizing how Corpora Differ</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">S</forename><surname>Kessler</surname></persName>
							<email>jason.kessler@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">CDK Global</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Scattertext: a Browser-Based Tool for Visualizing how Corpora Differ</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of ACL 2017, System Demonstrations</title>
						<meeting>ACL 2017, System Demonstrations <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="85" to="90"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-4015</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Scattertext is an open source tool for visualizing linguistic variation between document categories in a language-independent way. The tool presents a scatterplot, where each axis corresponds to the rank-frequency a term occurs in a category of documents. Through a tie-breaking strategy, the tool is able to display thousands of visible term-representing points and find space to legibly label hundreds of them. Scattertext also lends itself to a query-based visualization of how the use of terms with similar embeddings differs between document categories, as well as a visualization for comparing the importance scores of bag-of-words features to univariate metrics.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Finding words and phrases that discriminate cat- egories of text is a common application of sta- tistical NLP. For example, finding words that are most characteristic of a political party in congressional speeches can help political scien- tists identify means of partisan framing <ref type="bibr" target="#b9">(Monroe et al., 2008;</ref><ref type="bibr" target="#b5">Grimmer, 2010)</ref>, while identify- ing differences in word usage between male and female characters in films can highlight narra- tive archetypes ( <ref type="bibr" target="#b12">Schofield and Mehr, 2016)</ref>. Lan- guage use in social media can inform understand- ing of personality types ( <ref type="bibr" target="#b13">Schwartz et al., 2013)</ref>, and provides insights into customers' evaluations of restaurants ( <ref type="bibr" target="#b7">Jurafsky et al., 2014)</ref>.</p><p>A wide range of visualizations have been used to highlight discriminating words-simple ranked lists of words, word clouds, word bubbles, and word-based scatter plots. These techniques have a number of limitations. For example, the difficulty in comparing the relative frequencies of two terms in a word cloud, or in legibly displaying term la- bels in scatterplots.</p><p>Scattertext <ref type="bibr">1</ref> is an interactive, scalable tool which overcomes many of these limitations. It is built around a scatterplot which displays a high number of words and phrases used in a corpus. Points representing terms are positioned to allow a high number of unobstructed labels and to in- dicate category association. The coordinates of a point indicate how frequently the word is used in each category. <ref type="figure" target="#fig_2">Figure 1</ref> shows an example of a Scattertext plot comparing Republican and Democratic political speeches. The higher up a point is on the y-axis, the more it was used by Democrats, and similarly, the further right on the x-axis a point appears, the more its corresponding word was used by Re- publicans. Highly associated terms fall closer to the upper left and lower right-hand corners of the chart, while stop words fall in the far upper right-hand corner. Words occurring infrequently in both classes fall closer to the lower left-hand corner. When used interactively, mousing-over a point shows statistics about a term's relative use in the two contrasting categories, and clicking on a term shows excerpts from convention speeches used.</p><p>The point placement, intelligent word-labeling, and auxiliary term-lists ensure a low-whitespace, legible plot. These are issues which have plagued other scatterplot visualizations showing discrimi- native language.</p><p>ยง2 discusses different views of term-category association that make up the basis of visualiza- tions. In ยง3, the objectives, strengths, and weak- nesses of existing visualization techniques. ยง4 presents the technical details behind Scattertext.  <ref type="table">Top Democratic  auto  auto industry  insurance companies  pell  last week  pell grants  affordable  grants  platform  reduce  access  fair  grandmother  clean   Top Republican  unemployment  liberty  olympics  reagan  ann  founding  constitution  church  free enterprise  federal government  enterprise  sons  boy  greatness   Characteristic  obama  romney  barack  mitt  obamacare  biden  romneys  hardworking  bain  grandkids  billionaires  millionaires  ledbetter  buenas  pell  noches  bless  dreamers  congresswoman  bipartisan  wealthiest  risked  trillion  republicans  recession  insurmountable  gentlemen  electing  pelosi  understands   obama   romney   barack   millionaires   pell   wealthiest   trillion   gentlemen   fought   president   americans</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">On text visualization</head><p>The simplest visualization, a list of words ranked by their scores, is easy to produce, interpret and is thus very common in the literature. There are numerous ways of producing word scores for ranking which are thoroughly covered in previ- ous work. The reader is directed to <ref type="bibr" target="#b9">Monroe et al. (2008)</ref> (subsequently referred to as MCQ) for an overview of model-based term scoring algorithms. Also of interest, <ref type="bibr" target="#b1">Bitvai and Cohn (2015)</ref> present a method for finding sparse words and phrase scores from a trained ANN (with bag-of-words features) and its training data.</p><p>Regardless of how complex the calculation, word scores capture a number of different mea- sures of word-association, which can be interest- ing when viewed independently instead of as part of a unitary score. These loosely defined measures include: Precision A word's discriminative power regard- less of its frequency. A term that appears once in the categorized corpus will have perfect precision. This (and subsequent metrics) presuppose a bal- anced class distribution. Words close to the x and y-axis in Scattertext have high precision.</p><p>Recall The frequency a word appears in a partic- ular class, or P (word|class). The variance of pre- cision tends to decrease as recall increases. Ex- tremely high recall words tend to be stop-words. High recall words occur close to the top and right sides of Scattertext plots.</p><p>Non-redundancy The level of a word's discrimi- native power given other words that co-occur with it. If a word w a always co-occurs with w b and word w b has a higher precision and recall, w a would have a high level of redundancy. Measur- ing redundancy is non-trivial, and has tradition- ally been approached through penalized logistic regression ( <ref type="bibr" target="#b6">Joshi et al., 2010)</ref>, as well as through other feature selection techniques. In configu- rations of Scattertext such as the one discussed at the end of ยง4, terms can be colored based on their regression coefficients that indicate non- redundancy.</p><p>Characteristicness How much more does a word occur in than the categories examined than in background in-domain text? For example, if com- paring positive and negative reviews of a single movie, a logical background corpus may be re- views of other movies. Highly associated terms tend to be characteristic because they frequently appear in one category and not the other. Some visualizations explicitly highlight these, ex. <ref type="bibr" target="#b4">(Coppersmith and Kelly, 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Past work and design motivation</head><p>Text visualizations manipulate the position and ap- pearance of words or points representing them to indicate their relative scores in these measures. For example, in <ref type="bibr" target="#b13">Schwartz et al. (2013)</ref>, two word clouds are given, one per each category of text being compared. Words (and selected n- grams) are sized by their linear regression coef- ficients (a composite metric of precision, recall, and redundancy) and colored by frequency. Only words occurring in โฅ1% of documents and hav- ing Bonferroni-corrected coefficient p-values of &lt;0.001 were shown. Given that these words are highly correlated to their class of interest, the fre- quency of use is likely a good proxy for recall.</p><p>Coppersmith and Kelly (2014) also describe a word-cloud based visualization for discriminating terms, but intend it for categories which are both small subsets of a much larger corpus. They in- clude a third, middle cloud for terms that appear characteristic.</p><p>Word clouds can be difficult to interpret. It is difficult to compare the sizes of two non- horizontally adjacent words, as well as the relative color intensities of any two words. Longer words unintentionally appear more important since they naturally occupy more space in the cloud. Sizing of words can be a source of confusion when used to represent precision, since a larger word may naturally be seen as more frequent. <ref type="bibr" target="#b2">Bostock et al. (2012)</ref> 2 features an interactive word-bubble visualization for exploring different word usage among Republicans and Democrats in the 2012 US presidential nominating conventions. Each term displayed is represented by a bubble, sized proportionate to their frequency. Each bub- ble is colored blue and red, s.t. the blue parti- tion's size corresponds to the term's relative use by Democrats. Terms were manually chosen, and arranged along the x-axis based on their dis- criminative power. When clicked, sentences from speeches containing the word used are listed be- low the visualization.</p><p>The dataset used in <ref type="bibr" target="#b2">Bostock et al. (2012)</ref> is used to demonstrate the capabilities of Scattertext in each of these figures. The dataset is available via the Scattertext Github page. is, the more shrinkage we will have. <ref type="figure" target="#fig_4">Figure 5</ref> illustrates results from the running example using this approach. We set a 0 to imply a ''prior sample'' of 500 words per party every day, roughly the average number of words per day used per party on each topic in the data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Scatterplot visualizations</head><p>As is shown in <ref type="figure" target="#fig_4">Figure 5</ref>, this has a desirable effect on the function-word problem noted above. For example, the Republican top 20 list has shuffled, with the, you, not, of, and be being replaced by the considerably more evocative aliv, infant, brutal, brain, and necessari.  MCQ present a visualization to illustrate the use of their proposed word score, log-odds-ratio with an informative Dirichlet prior (top of <ref type="figure">Figure 2</ref>). This visualization plots word-representing points along two axes. The axes are log 10 recall vs. the difference in word scores z-scores. Points with a z-score difference &lt;1.96 are grayed-out, while the top and bottom 20 are labeled, both by each point and on the right-hand side. The side-labeling is necessary because labels are permitted to overlap, hindering their on-plot readability. The sizes of points and labels are increased proportionally to the word score. This word score encompasses pre- cision, recall, and characteristicness since it penal- izes scores of terms used more frequently in the background corpus. MCQ used this type of plot to illustrate the different effects of various scoring techniques introduced in the paper. However, the small number of points which are possible to label limit its utility for in-depth corpus analysis. <ref type="bibr" target="#b12">Schofield and Mehr (2016)</ref> use essentially the same visualization, but plot over 100 correspond- ing n-grams next to an unlabeled frequency/z- score plot. While this is appropriate for publica- tion, displaying associated terms and the shape of the score distribution, it is impossible to align all but the highest scoring points to their labels. The tidytext R-package (Silge and Robinson, 2016) documentation includes a non-interactive ggplot2-based scatter plot that is very similar to Scattertext. The x and y-axes both, like in Scat- tertext, correspond to word frequencies in the two contrasting categories, with jitter added. <ref type="bibr">3</ref> In the example in <ref type="figure">Figure 2 (bottom)</ref>, the contrasting cate- gories are tweets from two different accounts. The red diagonal line separates words based on their odds-ratio. Importantly, compared to MCQ, less of this chart's area is occupied by whitespace.</p><p>While tidytext's labels do not overlap each other (in contrast to MCQ) they do overlap points. The points' semi-transparency makes labels in less- dense areas legible, the dense interior of the chart is nearly illegible, with both points and labels ob- scured. <ref type="figure" target="#fig_6">Figure 3</ref>  The next section presents Scattertext and how its approach to word ordering solves the problems discussed above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Scattertext</head><p>Scattertext builds on tinytext and Rudder <ref type="bibr">(2014)</ref>. It plots a set of unigrams and bigrams (referred to in this paper as "terms") found in a corpus of documents assigned to one of two categories on a two-dimensional scatterplot.</p><p>In the following notation, user-supplied param- eters are in bold typeface.</p><p>Consider a corpus of documents C with disjoint subsets A and B s.t. A โช B โก C. Let ฯ T (t, C) be the number of times term t occurs in C, ฯ T (t, A) be the the number of times t occurs in A. Let ฯ D (t, A) refer to the number of documents in A containing t. Let t ij be the jth word in term t i . In practice, j โ {1, 2}. The parameter ฯ may be ฯ T or ฯ D . <ref type="bibr">4</ref> Other feature representations (ex., tf.idf) may be used for ฯ.</p><formula xml:id="formula_0">P r[t i ] = ฯ(t i , C) tโCโง|t|โก|t i | ฯ(t, C)</formula><p>.</p><p>(1)</p><p>The construction of the set of terms included in the visualization V is a two-step process. Terms must occur geqm times, and if bigrams, appear to be phrases. In order to keep the approach lan- guage neutral, I follow <ref type="bibr">Schartz et al. (2013)</ref>, and use a pointwise mutual information score to filter out bigrams that do not occur far more frequently than would be expected. Let</p><formula xml:id="formula_1">P M I(t i ) = log P r[t i ] t ij โt i P r[T ij ]</formula><p>.</p><p>The minimum P M I accepted is p. Now, V can be defined as</p><formula xml:id="formula_3">{t|ฯ(t, C) โฅ m โง (|t| โก 1 โจ P M I(t) &gt; p)} (3)</formula><p>Let a term t's coordinates on the scatterplot be (x A t , x B t ), where A and B are the two docu- ment categories. Although x K t is proportional to ฯ(t, K), many terms will have identical ฯ(t, K) values. To break ties the word that appears last alphabetically will have a larger x K t . Let us define r K t s.t. t โ V and K โ {A, B} as the ranks of ฯ(t, K), sorted in ascending order, where ties are broken by terms' alphabetical order. This allows us to define</p><formula xml:id="formula_4">x K t = r K t argmax r K<label>(4)</label></formula><p>This limits x values to <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>, ensuring both axes are scaled identically. This keeps the chart from becoming lopsided toward the corpus that had a larger number of terms. <ref type="bibr">5</ref> The charts in <ref type="figure" target="#fig_2">Figures 1, 4</ref>, and 5, were made with parameters m=5, p=8, and ฯ=ฯ T .</p><p>Breaking ties alphabetically is a simple but important alternative to jitter. While jitter (i.e., randomly perturbing x A t and x B t ) breaks up the stacked points shown in <ref type="figure" target="#fig_6">Figure 3</ref>, it eliminates empty space to legibly label points. Jitter can make it seem like identically frequent points are closer to an upper left or lower right corner. Al- phabetic tie-breaking makes identical adjustments to both axes, leading to the horizontal (lower-left to upper-right) alignments of identically frequent points. This angle does not cause one point to be substantially closer to either of the category asso- ciated corners (the upper-left and lower-right).</p><p>These alignments provide two advantages. First, they open up point-free tracts in the center of the chart which allow for unobstructed interior labels. Second, they arrange points in a way that it is easy to hover a mouse over all of them, to indi- cate what term they correspond to, and be clicked to see excerpts of that term.</p><p>In the running example, 154 points were la- beled when a jitter of 10% of each axis and no tie-breaking was applied. 210 points (a 36% lift) were labeled when no jitter was applied. 140 were labeled if no tie breaking was used.</p><p>Rudder (2014) observed terms closer to the lower-right corner were used frequently in A and infrequently in B, indicating they have both high recall and precision wrt category A. Symmetri- cally, the same relationship exists for B and the upper-right corner. I can formalize this score be- tween a point's coordinates and it's respective cor- ner. This intuition is represented by a score func- tion s K (t) (K โ {A, B} and t โ V ) where</p><formula xml:id="formula_5">s K (t) = 1 โ x A t , x B t if K = A, x A t , 1 โ x B t if K = B .<label>(5)</label></formula><p>Other term scoring methods (e.g., regression weights or a weighted log-odd-ratio with a prior) may be used in place of Formula 5. Maximal non-overlapping labeling of scatter- plots is NP-hard ( <ref type="bibr" target="#b0">Been et al., 2007</ref>). Scattertext's heuristic is labeling points if space is available in one of many places around a point. This is per- formed iteratively, beginning with points having the highest score (regardless of category) and pro- ceeding downward in score. An optimized data structure automatically constructed using Cozy ( <ref type="bibr" target="#b8">Loncaric et al., 2016)</ref> holds the locations of drawn points and labels.</p><p>The top scoring terms in classes B and A (Democrats and Republicans in <ref type="figure" target="#fig_2">Figure 1</ref>) are listed to the right of the chart. Hovering over points and terms highlights the point and displays frequency statistics.</p><p>Point colors are determined by their scores on s. Those corresponding to terms with a high s B col- ored in progressively darker shades of blue, while those with a higher s A are colored in progressively darker shades of red. When both scores are about equal, the point colors become more yellow, which creates a visual divide between the two classes. The colors are provided by D3's "RdYlBu" di- verging color scheme from Colorbrewer 6 via d3 <ref type="bibr">7</ref> .</p><p>Other point colors (and scorings) can be used. For example, <ref type="figure">Figure 4</ref> shows coefficients of an 1 penalized log. reg. classifier on V features. Scat- tertext, in this example, is set to color 0-scoring coefficients light gray. Terms' univariate predic- tive power are still evident by their chart position. See below 8 for an interactive version. BARACK OBAMA They knew they were part of something larger - a nation that triumphed over fascism and depression, a nation where the most innovative businesses turn out the world's best products, and everyone shared in that pride and success from the corner office to the factory floor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MITT ROMNEY</head><p>That business we started with 10 people has now grown into a great American success story.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MARCO RUBIO</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Topical category discriminators</head><p>In 2012, how did Republicans and Democrats use language relating to "jobs", "healthcare", or "mil- itary" differently? <ref type="figure" target="#fig_4">Figure 5</ref> shows, in the running example, words similar to "jobs" that were char- acteristic of the political parties.  In this configuration of Scattertext, words are colored by their cosine similarity to a query phrase. This is done using spaCy 9 -provided GloVe ( <ref type="bibr" target="#b10">Pennington et al., 2014</ref>) word vectors (trained on the Common Crawl corpus). Mean vectors are used for phrases.</p><p>The calculation of the most similar terms as- sociated with each category is a simple heuristic. First, sets of terms closely associated with a cat- egory are found. Second, these terms are ranked based on their similarity to the query, and the top rank terms are displayed to the right of the scatter- plot ( <ref type="figure" target="#fig_4">Figure 5)</ref>.</p><p>A term is considered associated if its p-value is &lt;0.05. P-values are determined using MCQ's difference in the weighted log-odds-ratio with an uninformative Dirichlet prior. This is the only model-based method discussed in Monroe et al. that does not rely on a large in-domain background corpus. Since I am scoring bigrams in addition to the unigrams scored by MCQ, the size of the cor- pus would have to be larger to have high enough bigram counts for proper penalization.</p><p>This function relies the Dirichlet distribution's parameter ฮฑ โ R |V | + . Following MCQ, ฮฑ t = 0.01. Formulas 16, 18 and 22 are used to compute z- scores, which are then converted to p-values using the Normal CDF ofหฮถofห ofหฮถ AโB w , letting y (K) t = ฯ(t, K) st K โ {A, B} and t โ V .</p><p>As seen in <ref type="figure" target="#fig_4">Figure 5</ref>, the top Republican word related to "jobs" is "job creators", while "workers" is the top Democratic term. <ref type="bibr">9</ref> spacy.io 6 Conclusion and future work Scattertext, a tool to make legible, comprehen- sive visualizations of class-associated term fre- quencies, was introduced. Future work will in- volve rigorous human evaluation of the usefulness of the visualization strategies discussed.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>fighting cuts</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Scattertext visualization of words and phrases used in the 2012 Political Conventions. 2,202 points are colored red or blue based on the association of their corresponding terms with Democrats or Republicans, 215 of which were labeled. The corpus consists of 123 speeches by Democrats (76,864 words) and 66 by Republicans (58,138 words). The most associated terms are listed under "Top Democrat" and "Top Republican" headings. Interactive version: https://jasonkessler.github.io/st-main.html ยง5 discusses how Scattertext can be used to identify category-discriminating terms that are semantically similar to a query.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>a รฐiร kw 5 a รฐiร k0หpk0ห k0หp MLE 5 y ร a0 n รฐ23ร where a รฐiร k0 determines the implied amount of information in the prior. This prior shrinks the p รฐiร kw and X รฐiร kw to the global values, and shrinks the feature evaluation measures, the f รฐiร kw and the f รฐi 2 jร kw , toward zero. The greater a รฐiร k0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5</head><label>5</label><figDesc>Fig. 5 Feature evaluation and selection based onหfonห onหf รฐD 2 Rร kw . Plot size is proportional to evaluation</figDesc><graphic url="image-1.png" coords="3,308.52,114.77,216.08,222.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>388 Burt L.Figure 2 :</head><label>3882</label><figDesc>figure hotel 3rd</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A small cropping from an un-jittered version at the bottom of Figure 2. The dark, opaque points indicate stacks of points. plot, but with no jitter. Words appearing with the same frequency in both categories all become stacked atop each other, however, this provides more interior space for labeling. As a side note, many text visualizations plot words in a 2D space according to their similarity in a high dimensional space. For example, Cho et al. 2014 uses the Barnes-Hut-SNE to plot words in a 2D space s.t. those with similar representations are grouped close together. Class-association does not play a role in this line of research, and global position is essentially irrelevant. The next section presents Scattertext and how its approach to word ordering solves the problems discussed above.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Democratic frequency: 8 per 25, 000 terms Some of the 25 mentions: Republican frequency: 26 per 25, 000 terms Some of the 62 mentions:</head><label>800026000</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Democratic document count: 123อพ word count: 76 , 864 Republican document count: 66อพ word count: 58</head><label>7686458</label><figDesc>fighting</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :</head><label>5</label><figDesc>fighting</figDesc></figure>

			<note place="foot" n="1"> github.com/JasonKessler/scattertext</note>

			<note place="foot" n="2"> nytimes.com/interactive/2012/09/06/us/politics/conventionword-counts.html</note>

			<note place="foot" n="3"> This type of visualization may have first been introduced in Rudder (2014).</note>

			<note place="foot" n="4"> ฯ D is useful when documents contain unique, characteristic, highly frequent terms. For example, names of movies can have high ฯ T when finding differences in positive and negative film reviews. The may lead to them receiving higher scores than sentiment terms.</note>

			<note place="foot" n="5"> While both are available, ordinal ranks are preferable to log frequency since uninteresting stop-words often occupy disproportionate axis space.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Jay Powell, Kyle Lo, Ray Little-Herrick, Will Headden, Chuck Little, Nancy Kessler and Kam Woods helped proofread this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Dynamic map labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Been</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Daiches</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chee</forename><surname>Yap</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>IEEE-VCG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Non-linear text regression with a deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Bitvai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>In ACL</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">At the national conventions, the words they used</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Bostock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Ericson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The New York Times</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gรผlรงehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Dynamic wordclouds and vennclouds for exploratory data analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glen</forename><surname>Coppersmith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><surname>Kelly</surname></persName>
		</author>
		<editor>ACL-ILLVI</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Representational Style: The Central Role of Communication in Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">Ryan</forename><surname>Grimmer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>Harvard University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Movie reviews and revenues: An experiment in text regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahesh</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Narrative framing of consumer sentiment in online restaurant reviews. First Monday</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Chahuneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Routledge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast synthesis of fast collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Calvin</forename><surname>Loncaric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emina</forename><surname>Torlak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">D</forename><surname>Ernst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PLDI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fightin&apos; words: Lexical feature selection and evaluation for identifying the content of political conflict</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burt</forename><forename type="middle">L</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">P</forename><surname>Colaresi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">M</forename><surname>Quinn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political Analysis</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Dataclysm: Who We Are (When We Think No One&apos;s Looking)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rudder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Crown Publishing Group</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Genderdistinguishing features in film dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Schofield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Mehr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>NAACLCLfL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Personality, gender, and age in the language of social media: The openvocabulary approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><forename type="middle">C</forename><surname>Eichstaedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><forename type="middle">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">tidytext: Text mining and analysis using tidy data principles in r</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Silge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JOSS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
