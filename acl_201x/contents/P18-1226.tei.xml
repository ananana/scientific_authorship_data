<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Subword-level Word Vector Representations for Korean</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjoon</forename><surname>Park</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">KAIST</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeongmin</forename><surname>Byun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">KAIST</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sion</forename><surname>Baek</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Program in Cognitive Science</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongseok</forename><surname>Cho</surname></persName>
							<email>yongseok.cho84@gmail.com, alice.oh@kaist.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Natural Language Processing team</orgName>
								<address>
									<settlement>Adecco</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Oh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">KAIST</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Subword-level Word Vector Representations for Korean</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2429" to="2438"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2429</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Research on distributed word representations is focused on widely-used languages such as English. Although the same methods can be used for other languages, language-specific knowledge can enhance the accuracy and richness of word vector representations. In this paper, we look at improving distributed word representations for Korean using knowledge about the unique linguistic structure of Korean. Specifically, we decompose Korean words into the jamo level, beyond the character-level, allowing a systematic use of sub-word information. To evaluate the vectors, we develop Korean test sets for word similarity and analogy and make them publicly available. The results show that our simple method outperforms word2vec and character-level Skip-Grams on semantic and syntactic similarity and analogy tasks and contributes positively toward downstream NLP tasks such as sentiment analysis .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Word vector representations built from a large cor- pus embed useful semantic and syntactic knowl- edge. They can be used to measure the similar- ity between words and can be applied to various downstream tasks such as document classification <ref type="bibr">(Yang et al., 2016)</ref>, conversation modeling <ref type="bibr" target="#b21">(Serban et al., 2016)</ref>, and machine translation ( <ref type="bibr" target="#b18">Neishi et al., 2017</ref>). Most previous research for learning the vectors focuses on English <ref type="bibr" target="#b6">(Collobert and Weston, 2008;</ref><ref type="bibr">Mikolov et al., 2013b,a;</ref><ref type="bibr" target="#b20">Pennington et al., 2014;</ref><ref type="bibr" target="#b14">Liu et al., 2015;</ref><ref type="bibr" target="#b4">Cao and Lu, 2017)</ref> and thus leads to difficulties and limitations in directly applying those techniques to a language with a different internal structure from that of En- glish.</p><p>The mismatch is especially significant for mor- phologically rich languages such as Korean where the morphological richness could be captured by subword level embedding such as character em- bedding. It has been already shown that decom- posing a word into subword units and using them as inputs improves performance for downstream NLP such as text classification ( <ref type="bibr" target="#b30">Zhang et al., 2015)</ref>, language modeling ( , and machine translation ( <ref type="bibr" target="#b13">Ling et al., 2015;</ref>. Despite their effectiveness in capturing syntactic features of diverse languages, decom- posing a word into a set of n-grams and learning n-gram vectors does not consider the unique lin- guistic structures of various languages. Thus, re- searchers have integrated language-specific struc- tures to learn word vectors, for example subchar- acter components of Chinese characters (  and syntactic information (such as prefixes or post-fixes) derived from external sources for English <ref type="bibr" target="#b4">(Cao and Lu, 2017)</ref>.</p><p>For Korean, integrating Korean linguistic struc- ture at the level of jamo, the consonants and vow- els that are much more rigidly defined than En- glish, is shown to be effective for sentence parsing <ref type="bibr" target="#b23">(Stratos, 2017)</ref>. Previous work has looked at im- proving the vector representations of Korean us- ing the character-level decomposition ( <ref type="bibr" target="#b5">Choi et al., 2017</ref>), but there is room for further investigation because Korean characters can be decomposed to jamos which are smaller units than the characters.</p><p>In this paper, we propose a method to integrate Korean-specific subword information to learn Ko- rean word vectors and show improvements over previous baselines methods for word similarity, analogy, and sentiment analysis. Our first contri-bution is the method to decompose the words into both character-level units and jamo-level units and train the subword vectors through the Skip-Gram model. Our second major contribution is the Ko- rean evaluation datasets for word similarity and analogy tasks, a translation of the WS-353 with annotations by 14 Korean native speakers, and 10,000 items for semantic and syntactic analogies, developed with Korean linguistic expertise. Using those datasets, we show that our model improves performance over other baseline methods without relying on external resources for word decomposi- tion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Language-specific features for NLP</head><p>Recent studies in NLP field flourish with devel- opment of various word vector models. Although such studies aim for universal usage, distinct char- acteristics of individual languages still remain as a barrier for a unified model. The aforementioned issue is even more prominent when it comes to languages that have rich morphology but lack re- sources for research ( <ref type="bibr" target="#b1">Berardi et al., 2015)</ref>. Accord- ingly, various studies dealing with language spe- cific NLP technique proposed considering linguis- tics traits in models.</p><p>A large portion of these papers was dedicated to Chinese. Since Chinese is a logosyllabic language, ( ) relevant studies focused on in- corporation of different subword level features on word embedding, such as word internal structure ( <ref type="bibr" target="#b25">Wang et al., 2017)</ref>, subcharacter component,( ), syllable <ref type="bibr" target="#b0">(Assylbekov et al., 2017</ref>), radicals ( <ref type="bibr" target="#b27">Yin et al., 2016)</ref>, and sememe ( <ref type="bibr" target="#b19">Niu et al., 2017)</ref>.</p><p>The Korean language is a member of the agglu- tinative languages <ref type="bibr" target="#b22">(Song, 2006</ref>), so previous stud- ies have tried fusing the complex internal structure into the model. For example, a grammatical com- position called 'Josa' in combination with word embedding is utilized in semantic role labeling <ref type="bibr" target="#b17">(Nam and Kim, 2016)</ref> and exploiting jamo to han- dle morphological variation <ref type="bibr" target="#b23">(Stratos, 2017)</ref>. Also considered in prior work to obtain the word vec- tor presentations for Korean is the syllable ( <ref type="bibr" target="#b5">Choi et al., 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Subword features for NLP</head><p>Applying subword features to various NLP tasks has become popular in the NLP field. Typically, character-level information is useful when com- bined with the neural network based models. <ref type="bibr" target="#b24">(Vania and Lopez, 2017;</ref><ref type="bibr" target="#b0">Assylbekov et al., 2017;</ref><ref type="bibr" target="#b4">Cao and Lu, 2017</ref>) Previous papers showed per- formance enhancement in various tasks includ- ing language modeling ( <ref type="bibr" target="#b2">Bojanowski et al., 2017</ref><ref type="bibr" target="#b3">Bojanowski et al., , 2015</ref>, machine translation ( <ref type="bibr" target="#b13">Ling et al., 2015</ref>), text classification ( <ref type="bibr" target="#b30">Zhang et al., 2015;</ref><ref type="bibr" target="#b13">Ling et al., 2015)</ref> and parsing ( <ref type="bibr" target="#b29">Yu and Vu, 2017)</ref>. In addition, the character n-gram fused model was suggested as a solution for a small dataset due to its robust- ness against data sparsity <ref type="bibr" target="#b4">(Cao and Lu, 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>We introduce our model training Korean word vector representations based on a subword-level information Skip-Gram. First, we briefly explain the hierarchical composition structure of Korean words to show how we decompose a Korean word into a sequence of subword components (jamo). Then, we extract character and jamo n-grams from the decomposed sequence to compute word vec- tors as a mean of the extracted n-grams. We train the vectors by widely-used Skip-Gram model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Decomposition of Korean Words</head><p>Korean words are formed by an explicit hierar- chical structure which can be exploited for better modeling. Every word can be decomposed into a sequence of characters, which in turn can be de- composed into jamos, the smallest lexicographic units representing the consonants and vowels of the language. Unlike English which has a more flexible sequences of consonants and vowels mak- ing up syllables (e.g., "straight"), a Korean "char- acter" which is similar to a syllable in English has a rigid structure of three jamos. They have names that reflect the position in a character: 1) chosung (syllable onset), 2) joongsung (syllable nucleus), and 3) jongsung (syllable coda). The prefix cho in chosung means "first", joong in joongsung means "middle", and jong in jongsung means "end" of a character. Each component indicates how the char- acter should be pronounced. With the exception of empty consonants, chosung and jongsung are con- sonants while joongsung are vowels. The jamos are written with the chosung on top, with joong- sung on the right of or below chosung, and jong- sung on the bottom (see <ref type="figure" target="#fig_0">Fig. 1</ref>).</p><p>As shown in the top of <ref type="figure" target="#fig_0">Fig. 1</ref>, some characters such as 'Sun' lack jongsung. In this case, we add an empty jongsung symbol e such that a charac- ter always has three (jamos). Thus, the character 'Moon' is decomposed into {, , }, and 'Sun' into {, , e}.</p><p>When decomposing a word, we keep the order of the characters and the order of jamos (chosung, joongsung, and jongsung) within the character. By following this rule, we ensure that a Korean word with N characters will have 3N jamos in order. Lastly, the symbols for start of a word &lt; and end of a word &gt; are added to the sequence. For example, the word 'puppy' will be decomposed to a sequence of jamos: {&lt;, , , , , , e, , , e, &gt;}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Extracting N-grams from jamo Sequence</head><p>We extract the following jamo-level and character- level n-grams from the decomposed Korean words: 1) character-level n-grams, and 2) inter- character jamo-level n-grams. These two levels of subword features can be successfully integrated into jamo-level n-grams by ensuring a character has three jamos, adding empty jongsung symbol to the sequence. For better understanding, we start with the word 'ate'. Character-level n-grams. Since we add the empty jongsung symbol e when decomposing characters, we can find jamo-level trigrams repre- senting a single character in the decomposed jamo sequence of a word. For example, there are three character-level unigrams in the word 'ate': {, , }, {, , }, {, , e} Next, we find character-level n-grams by us- ing the extracted unigrams. Adjacent unigrams are attached to construct n-grams. There are two character-level bigrams, and one trigram in the ex- ample: {, , , , , }, {, , , , , e} {, , , , , , , , , , , e} Lastly, we add the total jamo sequence of a word including &lt; and &gt; to the set of extracted character- level n-grams. Inter-character jamo-level n-grams. Since Ko- rean is a member of the agglutinative language, a syntactic character is attached to the semantic part in the word, and this generates many vari- ations. These variations are often determined by jamo-level information. For example, usage of the subjective case '' or '' is determined by the existence of jongsung in the previous character. In order to learn these regularities, we consider jamo- level n-grams across adjacent characters as well. For instance, there are 6 inter-character jamo-level trigrams in the example:</p><formula xml:id="formula_0">{&lt;, , }, {, , }, {, , }, {, , }, {, , }, {, e, &gt;}</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Subword Information Skip-Gram</head><p>Suppose the training corpus contains a sequence of words {..., w t−2 , w t−1 , w t , w t+1 , w t+2 , ...}, the Skip-Gram model maximizes the log probability of context word w t+j under a target word w t :</p><formula xml:id="formula_1">1 T T t=1 2c −c≤j≤c,j =0 log p(w t+j |w t ) (1)</formula><p>where c is the size of context window, t is to- tal number of words in the corpus. The origi- nal Skip-Gram model use softmax function out- puts for log p(w t+j |w t ) in Eq. 1, however, it re- quires large computational cost. To avoid com- puting softmax precisely, we approximately max- imize the log probability by Noise Contrastive Es- timation, and it can be simplified to the negative sampling using the binary logistic loss:</p><formula xml:id="formula_2">log(1 + e −s(w t+j ,wt) ) + nc n=1 log(1 + e s(w t+j ,wn) ) (2)</formula><p>where n c is the number of negative samples, and s(w t+j , w t ) is a scoring function. The func- tion computes the dot product between the input of the target word vector w t and the output of the context word vector w t+j . In Skip-Gram ( <ref type="bibr" target="#b15">Mikolov et al., 2013a)</ref>, an input of a word w t is uniquely as- signed over the training corpus; however, the vec- tor in the Subword Information Skip-Gram model ( <ref type="bibr" target="#b2">Bojanowski et al., 2017</ref>) is the mean vector of the set of n-grams extracted from the word. Formally, the scoring function s(w t , w t+j ) is:</p><formula xml:id="formula_3">1 |G t | |Gt| gt∈Gt z T gt v t+j (3)</formula><p>where the decomposed set of n-grams of w t is G t and its elements are g t , |G t | is total number of elements of G t . In general, the n-grams for 3 ≤ n ≤ 6 is extracted from a word, regardless of the subword-level or compositionality of a word.</p><p>Similarly, we construct a vector representation of a Korean word by using the extracted two types of n-grams. We compute the sum of jamo-level n- grams, sum of character-level n-grams, and com- pute mean of the vectors. Let us denote character- level n-grams of w t to G ct , and inter-character jamo-level n-grams G jt , then we obtain the scor- ing function s(w t , w t+j ) as follows:</p><formula xml:id="formula_4">1 N ( |Gct| gct∈Gct z T gct v t+j + |G jt | g jt ∈G jt z T g jt v t+j )<label>(4)</label></formula><p>where z g jt is the vector representation of the jamo- level n-gram g jt , and z gct is that of the character- level n-gram g ct . N is sum of the number of character-level n-grams and the number of inter- character jamo-level n-grams |G ct | + |G jt |.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Corpus</head><p>We collect a corpus of Korean documents from various sources to cover a wide context of word usages. The corpus used to train the models in- Sejong Corpus. This data is a publicly available corpus 2 which is collected under a national re- search project named the "21st century Sejong Project". The corpus was developed from 1998 to 2007, and contains formal text (newpapers, dic- tionaries, novels, etc) and informal text (transcrip- tions of TV shows and radio programs, etc). Thus, the corpus covers topics and context of language usage which could not be dealt with Wikipedia or news articles. We exclude some documents con- taining unnatural sentences such as POS-tagged sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Tasks and Datasets</head><p>We evaluate the performance of word vectors through word similarity task and word analogy task. However, to best of our knowledge, there is no Korean evaluation dataset for either task. Thus we first develop the evaluation datasets. We also test the word vectors for sentiment analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Word Similarity Evaluation Dataset</head><p>Translating the test set. We develop a Korean version of the word similarity evaluation set. Two graduate students who speak Korean as native lan- guage translated the English word pairs in WS-353 ( <ref type="bibr" target="#b7">Finkelstein et al., 2001</ref>). Then, 14 Korean native speakers annotated the similarity between pairs by giving scores from 0 to 10 for the translated pairs, following written instructions. The original English instructions were translated into Korean as well. Among the 14 scores for each pair, we exclude the minimum and maximum scores and compute the mean of the rest of the scores. The correlation between the original scores and the an- notated scores of the translated pairs is .82, which indicates that the translations are sufficiently reli- able. We attribute the difference to the linguistic and cultural differences. We make the Korean ver- sion of WS-353 publicly available. <ref type="bibr">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Word Analogy Evaluation Dataset</head><p>We develop the word analogy test items to evalu- ate the performance of word vectors. The evalua- tion dataset consists of 10,000 items and includes 5,000 items for evaluating the semantic features and 5,000 for the syntactic features. We also re- lease our word analogy evaluation dataset for fu- ture research. • Capital-Country (Capt.) includes two word pairs representing the relation between the country name and its capital:</p><formula xml:id="formula_5">Athens : Greece = Baghdad :</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Iraq</head><p>• Male-Female (Gend.) evaluates the relation between male and female:</p><p>prince:princess = gentlemen:ladies</p><p>• Name-Nationality (Name) evaluates the rela- tion between the name of celebrities or stars and their nationality:</p><p>Gandhi : India = Lincoln : USA</p><p>• Country-Language (Lang.) evaluates the re- lation between the country name and its offi- cial language:</p><p>Argentina : Spanish = USA :</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>English</head><p>• Miscellaneous (Mics.) includes various se- mantic features, such as pairs of a young an- imals, sound of animals, and Korean-specific color-words or regions, etc.. Korean-specific test items, rather than trying to cover the existing categories in the original sets ( <ref type="bibr" target="#b15">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b8">Gladkova et al., 2016)</ref>. This is because most of the syntactic features in these sets are not available in Korean. We develop the test set with linguistic expert knowledge of Korean. The following case is a good example. In Korean, the subject marker is at- tached to the back of a word, and other case mark- ers are also explicit at the word level. Here, word level refers to 'a phrase delimited by two whites- paces around it'. Unlike Korean, in English, sub- jects are determined by the position in a sentence (i.e., subject comes before the verb), so the case is not explicitly marked in the word. Similarly, there are other important and unique syntactic features of the Korean language, of which we choose the following five categories to evaluate the word vec- tors:</p><p>• Case contains various case markers attached to common nouns. This evaluates a case in Korean which is represented within a word- level:</p><p>Professor : Professor+case </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Sentiment Analysis</head><p>We perform a binary sentiment classification task for evaluation of word vectors. Given a sequence of words, the trained classifier should predict the sentiment from the inputs while maintaining the input word vectors fixed. Dataset We choose Naver Sentiment Movie Cor- pus <ref type="bibr">4</ref> . Scraped from Korean portal site Naver, the dataset contains 200K movie reviews. Each review is no longer than 140 characters and contain bi- nary label according to its sentiment (1 for posi- tive and 0 for negative). The number of samples in both sentiments is equal with 100K of positives and 100K of negatives in sum. We sample from the dataset for training (100K), validation (25K), and test set (25K). Again, each set's ratio of sen- timent class is balanced. Although we apply sim- ple preprocessing of stripping out punctuation and emoticon, the dataset is still noisy with typos, seg- mentation errors and abnormal word usage since its original source is raw comments from portal site.</p><p>Classifier In order to build sentiment classifier, we adopt single layer LSTM with 300 hidden units and 0.5 dropout rates. Given the final state of LSTM unit, sigmoid activation function is ap- plied for output prediction. We use cross-entropy loss and optimize parameters through Adam opti- mizer ( <ref type="bibr" target="#b10">Kingma and Ba, 2014</ref>) with learning rate of 0.001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison Models</head><p>We compare performance of our model to compar- ison models including word-level, character-level, and jamo-level Skip-Gram models trained by neg- ative sampling. Hyperparameters of each models are tuned over word similarity task. We fix the number of training epochs 5. Skip-Gram (SG) We first compare the per- formance with word-level Skip-Gram model ( <ref type="bibr" target="#b15">Mikolov et al., 2013a</ref>) where a unique vector is assigned for every unique words in the corpus. We set the number of dimensions as 300, number of negative samples to 5, and window size to 5.</p><p>Character-level Skip-Gram (SISG(ch)) splits words to character-level n-grams based on sub- word information skip-gram. ( <ref type="bibr" target="#b2">Bojanowski et al., 2017)</ref>. We set the number of dimensions as 300, number of negative samples to 5, and window size to 5. The n was set to 2-4. Jamo-level Skip-Gram with Empty Jongsung Symbol (SISG(jm)) splits words to jamo-level n- grams based on subword information skip-gram. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word Similarity</head><p>Figure 2: Spearman's correlation coefficient of word similarity task for each models. The results show higher consistency to human word similarity judgment on our method.</p><p>( <ref type="bibr" target="#b2">Bojanowski et al., 2017</ref>). In addition, if a charac- ter lacks jongsung, the symbol e is added. We set the number of dimensions as 300, number of neg- ative samples to 5, and window size to 5. The n was set to 3-6. Note that setting n=3-6 and adding the jongsung symbol makes this model as a spe- cific case of our model, containing jamo-level n- grams (n=3-6) and character-level n-grams (n=1- 2) as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Optimization</head><p>In order to train our model, we apply stochastic gradient descent with linearly scheduled learning rate decay. Initial learning rate is set to .025. To speed up the training, we train the vectors in par- allel with shared parameters, and they are updated asynchronously.</p><p>For our model, we set n of character n-grams to 1-4 or 1-6, and n of inter-character jamo- level n-grams to 3-5. We name both model as SISG(ch4+jm) and SISG(ch6+jm), respectively. The number of dimension is set to 300, window size to 5, and negative samples to 5. We train our model 5 epochs over training corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>Word Similarity. We report Spearman's correla- tion coefficient between the human judgment and model's cosine similarity for the similarity of word pairs. <ref type="figure">Fig. 2</ref> presents the results. For word-level skip-gram, Spearman's correlation is .599. If we decompose words into characters n-grams in or- der to construct word vectors (SISG(Ch)), perfor- mance is highly improved to .658. It indicates that decomposing words itself is helpful to learn good Model Analogy Semantic Syntactic Capt Gend Name Lang Misc Case Tense Voice Form Honr SG 0.460 0.551 0.537 0.435 0.574 0.521 0.597 0.594 0.  <ref type="table">Table 2</ref>: Performance of our method and comparison models. Average cosine distance for each category in word analogy task are reported. Overall, our model outperforms comparison models, showing close distance between predicted vector a + b − c and the target vector d (a:b=c:d). Specifically, performance is improved more in syntactic analogies.</p><p>Korean word vectors, which is morphologically rich language. Moreover, if the words are decom- posed to deeper level (SISG(jm)), performance is further improved to .671.</p><p>Next, addition of an empty jongsung sym- bol e to jamo sequence, which reflects Korean- specific linguistic regularities, improves the qual- ity of word vectors. SISG(jm), specific case of our model, shows higher correlation coefficient than the other baselines. Lastly, when we extend num- ber of characters to learn in a word to 4 or 6, our models outperform others. Word Analogy. In general, given an item a:b=c:d and corresponding word vectors u a , u b , u c , u d , the vector u a + u b − u c is used to compute cosine dis- tances between the vector and the others. Then the vectors are ranked in terms of the distance by as- cending order and if the vector u d is found at the top, the item is counted as correct. Top 1 accuracy or error rate for each category is frequently used metric for this task, however, in this case these rank-based measures may not be an appropriate measure since the total number of unique n-grams (e.g., SISG) or unique words (e.g., SG) over the same corpus largely differ from each other. For fair comparison, we directly report cosine distances between the vector u a + u b − u c and u d of each category, rather than evaluating ranks of the vec- tors. Formally, given an item a:b=c:d, we compute 3COSADD based metric:</p><formula xml:id="formula_6">1 − cos(u a + u b − u c , u d ) (5)</formula><p>We report the average cosine distance between predicted vector u a + u b − u c and target vector u d of each category. In semantic analogies, decomposing word into character helps little for learning semantic fea- tures. However, jamo-level n-grams help repre- senting overall semantic features and our model show higher performance compared to baseline models. One exception is Name-Nationality cat- egory since it mainly consists of items including proper nouns, and decomposing these nouns does not help learning the semantic feature of the word. For example, it is obvious that the semantic fea- tures of both words 'Ghandi' and 'India' could not be derived from that of characters or jamo n-grams comprising those words.</p><p>On the other hand, decomposing words does help to learn syntactic features for all categories, and decomposing a word to even deeper levels makes learning those features more effectively. Our model outperforms all other baselines, and the amount of decreased cosine distances com- pared to that of word-level Skip-Gram is larger than semantic categories. Korean language is ag- glutinative language that character-level syntactic affixes are attached to the root of the word, and the combination of them determines final form the word. Also, the form can be reduced with jamo-level transformation. This is the main rea- son that we can learn syntactic feature of Korean words if we decompose a word into character-level and jamo-level simultanously. We observe similar tendency when using 3COSMUL distance metric. ( <ref type="bibr" target="#b12">Levy and Goldberg, 2014)</ref> Sentiment Analysis. We report accuracy, loss, precision, recall and f1 score for binary sentiment classification task over test set. Although overall performance is homogeneous, our method which decompose a word to 1-6 character n-grams and 3- 5 jamo n-grams show slightly higher performance over comparison models. In addition, our ap- proach show better results compared to character-  <ref type="table">Table 3</ref>: Performance of sentiment classification task. 3-5 jamo n-grams and 1-6 chracter n-grams show slightly higher performance in terms of ac- curacy and f1-score over comparison models. level SISG or jamo-level SISG. On the other hand, word-level Skip-Gram show comparable F1-score to our model, and is even higher than other com- parison models. This is because the dataset con- tains significant amount of proper nouns, such as movie or actor names, and these word's semantic representations are captured better by word-level representations, as shown in word analogy task.</p><p>Effect of Size n in both n-grams. <ref type="table">Table. 4</ref> shows performance of word similarity task for each number of inter-character jamo-level n-grams and character-level n-grams. For the n of jamo-level n- grams, including n=5,6 of n-grams and excluding bigrams show higher performance. Meanwhile, n of character-level n-grams, including all of the character n-grams while decomposing a word does not guarantee performance improvement. Since most of the Korean word consists of no more than 6 characters (97.2% of total corpus), it seems max- imum number of n=6 in character n-gram is large enough to learn word vectors. In addition, words with no more than 4 characters takes 82.6% of to- tal corpus, so that n=4 sufficient to learn character n-grams as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Discussions</head><p>In this paper, we present how to decompose a Korean character into a sequence of jamos with empty jongsung symbols, then extract character- level n-grams and intercharacter jamo-level n- grams from that sequence. Both n-grams construct a word vector representation by computing the av- erage of n-grams, and these vectors are trained by subword-level information Skip-Gram. Prior to evaluating the performance of the vectors, we de- veloped test set for word similarity and word anal- ogy tasks for Korean. We demonstrated the effectiveness of the learned word vectors in capturing the seman- tic and syntactic information by evaluating these vectors with word similarity and word analogy tasks. Specifically, the vectors using both jamo and character-level information can represent syntac- tic features more precisely even in an agglutina- tive language. Furthermore, sentiment classifica- tion results of our work indicate that the represen- tative power of the vectors positively contributes to downstream NLP task.</p><p>Decomposing Korean word into jamo-level or character unigram helps capturing syntactic infor- mation. For example, Korean words add a charac- ter to the root of the word (e.g., '-' subjective case, '-' for past tense '--' for honorific, '-- ' for voice, and '--' for verb ending form.) Then composed word can be reduced to have fewer characters by transforming jamos, such as ' ' to ''. Hence, the inter-character jamo- level n-grams also help capture these features. On the other hand, larger n-grams such as character- level trigram will learn unique meaning of that word since those larger component of the word will mostly occur with that word. By leveraging both features, our method produces word vectors reflecting linguistic features effectively, and thus, outperforms previous word-level approaches.</p><p>Since Korean words are divisible once more into grapheme level, resulting in longer sequence of jamos for a given word, we plan to explore potential applicability of deeper level of subword information in Korean. Meanwhile, we will fur- ther train our model over noisy data and investi- gate how it is dealing with noisy words. Generally, informal Korean text contains intentional typos (''delicious' with typo'), stand-alone jamo as a character, ('lol') and segmentation errors. (' 'go together' without space'). Since these errors occur frequently, it is important to apply the vec- tors in training NLP models over real-word data. We plan to apply these vectors for various neu- ral network based NLP models, such as conversa- tion modeling. Lastly, since our method can cap- ture Korean syntactic features through jamo and character n-grams, we can apply the same idea to other tasks such as POS tagging and parsing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of the composition of a Korean character. Each character is comprised of 3 parts as shown in example of 'Moon'. On the other hand, as in the top case 'Sun', some characters lack the last component, 'jongsung'.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Frog</head><label></label><figDesc>: tadpole = horse : pony chicken:cackling=tiger:growl blue:bluish=yellow:yellowish Busan : South Gyeongsang Province = Daegu : North Gyeongsang Province Syntactic Feature Evaluation We define five representative syntactic categories and develop 3 https://github.com/SungjoonPark/KoreanWordVectors</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>=</head><label></label><figDesc>fight : fought = come : came • Voice has a pair of verb voice, one for an active voice and a passive voice for the other. It evaluates the voice which is represented by a verbal suffix: sold : be sold = evaluated : was evaluated • Verb ending form includes various verb ending forms. The various forms are part of verbal inflection in Korean: go : go+form = write : write+form • Honorific (Honr.) evaluates a morphological variation for verbs in Korean. An honorific expression is one of the most distinctive feature in Korean compared to other languages. This test set introduces the honorific morpheme '-' which is used in verbs: helped : helped+honorific = done : done+honorific</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Table 4 :</head><label>4</label><figDesc>Spearman's correlation coefficient of Word similarity task by n-gram of jamos and char- acters. Performance are improved when the 3-5 gram of jamos and 1-4 or 1-6 gram of characters.</figDesc></figure>

			<note place="foot" n="2"> https://ithub.korean.go.kr/user/main.do</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Syllable-aware neural language models: A failure to beat character-aware ones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenisbek</forename><surname>Assylbekov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rustem</forename><surname>Takhanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bagdat</forename><surname>Myrzakhmetov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan N</forename><surname>Washington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Word embeddings go to italy: A comparison of models and training datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giacomo</forename><surname>Berardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IIR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06303</idno>
		<title level="m">Alternative structures for character-level rnns</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving word embeddings with convolutional feature learning and subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaosheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A syllable-based technique for word embeddings of korean words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeuk</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinseok</forename><surname>Seol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanggoo</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the First Workshop on Subword and Character Level Models in NLP</title>
		<meeting>of the First Workshop on Subword and Character Level Models in NLP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="36" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Placing search in context: The concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WWW</title>
		<meeting>of WWW</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn&apos;t</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Gladkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandr</forename><surname>Drozd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Matsuoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the NAACL Student Research Workshop</title>
		<meeting>of the NAACL Student Research Workshop</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="8" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Character-aware neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fully character-level neural machine translation without explicit segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Linguistic regularities in sparse and explicit word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth CoNLL</title>
		<meeting>the Eighteenth CoNLL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Character-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning context-sensitive word embeddings with neural tensor skip-gram model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCAI</title>
		<meeting>of IJCAI</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A word embedding and a josa vector for korean unsupervised semantic role induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Kyeong-</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Seop</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4240" to="4241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A bag of useful tricks for practical neural machine translation: Embedding layer initialization and large batch size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masato</forename><surname>Neishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Sakuma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Tohda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shonosuke</forename><surname>Ishiwatari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoki</forename><surname>Yoshinaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Toyoda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Workshop on Asian Translation (WAT2017)</title>
		<meeting>the 4th Workshop on Asian Translation (WAT2017)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improved word representation learning with sememes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilin</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Building end-to-end dialogue systems using generative hierarchical neural network models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Iulian Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The Korean language: Structure, use and context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Jung</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>Routledge</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A sub-character architecture for korean language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="721" to="726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">From characters to words to in between: Do we capture morphology?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Vania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Exploiting word internal structures for generic chinese sentence representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaonan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Xiaodong He, Alex Smola, and Eduard Hovy. 2016. Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-granularity chinese word embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongchao</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="981" to="986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Joint embeddings of chinese words, characters, and fine-grained subcharacter components</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="286" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Character composition model with convolutional neural networks for dependency parsing on morphologically rich languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><forename type="middle">Thang</forename><surname>Vu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
