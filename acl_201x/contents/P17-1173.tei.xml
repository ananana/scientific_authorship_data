<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Algebra for Feature Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Srikumar</surname></persName>
							<email>svivek@cs.utah.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">University of Utah</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">An Algebra for Feature Extraction</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1891" to="1900"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1173</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Though feature extraction is a necessary first step in statistical NLP, it is often seen as a mere preprocessing step. Yet, it can dominate computation time, both during training, and especially at deployment. In this paper, we formalize feature extraction from an algebraic perspective. Our for-malization allows us to define a message passing algorithm that can restructure feature templates to be more computationally efficient. We show via experiments on text chunking and relation extraction that this restructuring does indeed speed up feature extraction in practice by reducing redundant computation.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Often, the first step in building statistical NLP models involves feature extraction. It is well un- derstood that the right choice of features can sub- stantially improve classifier performance. How- ever, from the computational point of view, the process of feature extraction is typically treated, at best as the preprocessing step of caching fea- turized inputs over entire datasets, and at worst, as 'somebody else's problem'. While such ap- proaches work for training, when trained models are deployed, the computational cost of feature ex- traction cannot be ignored.</p><p>In this paper, we present the first (to our knowl- edge) algebraic characterization of the process of feature extraction. We formalize feature extrac- tors as arbitrary functions that map objects (words, sentences, etc) to a vector space and show that this set forms a commutative semiring with respect to feature addition and feature conjunction.</p><p>An immediate consequence of the semiring characterization is a computational one. Every semiring admits the Generalized Distributive Law (GDL) Algorithm <ref type="bibr" target="#b0">(Aji and McEliece, 2000</ref>) that exploits the distributive property to provide com- putational speedups. Perhaps the most common manifestation of this algorithm in NLP is in the form of inference algorithms for factor graphs and Bayesian networks like the max-product, max- sum and sum-product algorithms (e.g. <ref type="bibr" target="#b11">Goodman, 1999;</ref><ref type="bibr" target="#b14">Kschischang et al., 2001</ref>). When applied to feature extractors, the GDL algorithm can refactor a feature extractor into a faster one by reducing re- dundant computation. In this paper, we propose a junction tree construction to allow such refactor- ing. Since the refactoring is done at the feature template level, the actual computational savings grow as classifiers encounter more examples.</p><p>We demonstrate the practical utility of our ap- proach by factorizing existing feature sets for text chunking and relation extraction. We show that, by reducing the number of operations performed, we can obtain significant savings in the time taken to extract features.</p><p>To summarize, the main contribution of this pa- per is the recognition that feature extractors form a commutative semiring over addition and conjunc- tion. We demonstrate a practical consequence of this characterization in the form of a mechanism for automatically refactoring any feature extractor into a faster one. Finally, we show the empirical usefulness of our approach on relation extraction and text chunking tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Definition</head><p>Before formal definitions, let us first see a running example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Motivating Example</head><p>Consider the frequently used unigram, bigram and trigram features. Each of these is a template that specifies a feature representation for a word. In fact, the bigram and trigram templates them- selves are compositional by definition. A bigram is simply the conjunction of a word w and pre- vious word, which we will denote as w -1 ; i.e., bigram = w -1 &amp;w. Similarity, a trigram is the conjunction of w -2 and bigram.</p><p>These templates are a function that operate on inputs. Given a sentence, say John ate alone, and a target word, say alone, they will produce indi- cators for the strings w=alone, w -1 =ate&amp;w=alone and w -2 =John&amp;w -1 =ate&amp;w=alone respectively. Equivalently, each template maps an input to a vector. Here, the three vectors will be basis vec- tors associated with the feature strings.</p><p>Observe that the function that extracts the target word (i.e., w) has to be executed in all three feature templates. Similarly, w -1 has to be extracted to compute both the bigrams and the trigrams. Can we optimize feature computation by automatically detecting such repetitions?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Definitions and Preliminaries</head><p>Let X be a set of inputs to a classification prob- lem at hand; e.g., X could be words, sentences, etc. Let V be a possibly infinite dimensional vec- tor space that represents the feature space. Feature extractors are functions that map the input space X to the feature space V to produce feature vec- tors for inputs. Let F represent the set of feature functions, defined as the set {f : X → V}. We will use the typewriter font to denote feature func- tions like w and bigram.</p><p>To round up the definitions, we will name two special feature extractors in F. The feature extrac- tor 0 maps all inputs to the zero vector. The feature extractor 1 maps all inputs to a bias feature vector. Without loss of generality, we will designate the basis vector i 0 ∈ V as the bias feature vector.</p><p>In this paper, we are concerned about two gen- erally well understood operators on feature func- tions -addition and conjunction. However, let us see formal definitions for completeness. Feature Addition. Given two feature extractors f 1 , f 2 ∈ F, feature addition (denoted by +) pro- duces a feature extractor f 1 + f 2 that adds up the images of f 1 and f 2 . That is, for any example x ∈ X, we have</p><formula xml:id="formula_0">(f 1 + f 2 ) (x) = f 1 (x) + f 2 (x)<label>(1)</label></formula><p>For example, the feature extractor w + w -1 will map the word alone to a vector that is one for the basis elements w=alone and w -1 =went. This vec- tor is the sum of the indicator vectors produced by the two operands w and w -1 . Feature Conjunction. Given two feature extrac- tors f 1 , f 2 ∈ F, their conjunction (denoted by &amp;) can be interpreted as an extension of Boolean conjunction. Indicator features like bigram are predicates for certain observations. Conjoining in- dicator features for two predicates is equivalent to an indicator feature for the Boolean conjunc- tion of the predicates. More generally, with fea- ture extractors that produce real valued vectors, the conjunction will produce their tensor prod- uct. The equivalence of feature conjunctions to tensor products has been explored and exploited in recent literature for various NLP tasks ( <ref type="bibr" target="#b15">Lei et al., 2014;</ref><ref type="bibr" target="#b25">Srikumar and Manning, 2014;</ref><ref type="bibr" target="#b16">Lei et al., 2015)</ref>.</p><p>We can further generalize this with an addi- tional observation that is crucial for the rest of this paper. We argue that the conjunction opera- tor produces symmetric tensor products rather than general tensor products. To see why, consider the bigram example. Though we defined the bigram feature as the conjunction of w -1 and w, their or- dering is irrelevant from classification perspective -the eventual goal is to associate weights with this combination of features. This observation allows us to formally define the conjunction operator as:</p><formula xml:id="formula_1">(f 1 &amp;f 2 ) (x) = vec (f 1 (x) f 2 (x)) (2)</formula><p>Here, vec (·) stands for vectorize, which simply converts the resulting tensor into a vector and denotes the symmetric tensor product, introduced by Ryan (1980, Proposition 1.1). A symmetric tensor product is defined to be the average of the tensor products of all possible permutations of the operands, and thus, unlike a simple tensor product, is invariant to permutation of is operands. Infor- mally, if we think of a tensor as a mapping from an ordered sequence of keys to real numbers, then, symmetric tensor product can be thought of as a mapping from a set of keys to numbers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">An Algebra for Feature Extraction</head><p>In this section, we will see that the set of feature extractors F form a commutative semiring with respect to addition and conjunction. First, let us revisit the definition of a commutative semiring. Definition 1. A commutative semiring is an alge- braic structure consisting of a set K and two bi-nary operations ⊕ and ⊗ (addition and multipli- cation respectively) such that: S1. (K, ⊕) is a commutative monoid: ⊕ is asso- ciative and commutative, and the set K con- tains a unique additive identity 0 such that ∀x ∈ K, we have 0 ⊕ x = x ⊕ 0 = x. S2. (K, ⊗) is a commutative monoid: ⊗ is asso- ciative and commutative, and the set K con- tains a unique multiplicative identity 1 such that ∀x ∈ K, we have 1 ⊗ x = x ⊗ 1 = x. S3. Multiplication distributes over addition on both sides. That is, for any x, y, z ∈ K, we have</p><formula xml:id="formula_2">x ⊗ (y ⊕ z) = (x ⊗ y) ⊕ (x ⊗ z) and (x ⊕ y) ⊗ z = (x ⊗ z) ⊕ (y ⊗ z). S4.</formula><p>The additive identity is an annihilating ele- ment with respect to multiplication. That is, for any x ∈ K, we have x ⊗ 0 = 0 = 0 ⊗ x. We refer the reader to Golan (2013) for a broad- ranging survey of semiring theory. We can now state and prove the main result of this paper. Theorem 1. Let X be any set and let F denote the set of feature extractors defined on the set. Then, (F, +, &amp;) is a commutative semiring.</p><p>Proof. We will show that the properties of a commutative semiring hold for (F, +, &amp;) using the definitions of the operators from §2.2. Let f 1 , f 2 and f ∈ F be feature extractors. S1. For any example x ∈ X, we have</p><formula xml:id="formula_3">(f 1 + f 2 ) (x) = f 1 (x) + f 2 (y).</formula><p>The right hand side denotes vector addition, which is associative and commutative. The 0 feature extractor is the additive identify because it produces the zero vector for any input. Thus, (F, +) is a commutative monoid. S2. To show that the conjunction operator is as- sociative over feature extractors, it suffices to observe that the tensor product (and hence the symmetric tensor product) is associative. Furthermore, the symmetric tensor product is commutative by definition, because it is in- variant to permutation of its operands. Finally, the bias feature extractor, 1, that maps all inputs to the bias vector i 0 , is the multiplicative identity. To see this, consider the conjunction f&amp;1, applied to an input x:</p><formula xml:id="formula_4">(f&amp;1) (x) = vec (f (x) 1 (x)) = vec (f (x) i 0 )</formula><p>The product term within the vec (·) in the final expression is a symmetric tensor, de- fined by basis vectors that are sets of the form</p><formula xml:id="formula_5">{i 0 , i 0 }, {i 1 , i 0 }, · · · . Each basis {i j , i 0 } is associated</formula><p>with a feature value f (x) j . Thus, the vectorized form of this tensor will contain the same elements as f (x), perhaps mapped to different bases. The mapping from f (x) to the final vector is independent of the input x because the bias feature extractor is inde- pendent of x. Without loss of generality, we can fix this mapping to be the identity map- ping, thereby rendering the final vectorized form equal to f (x). That is, f&amp;1 = f. Thus, (F, &amp;) is a commutative monoid. S3. Since tensor products distribute over addi- tion, we get the distributive property. S4. By definition, conjoining with the 0 feature extractor annihilates all feature functions be- cause 0 maps all inputs to the zero vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">From Algebra to an Algorithm</head><p>The fact that feature extractors form a commuta- tive semiring has a computational consequence. The generalized distributive law (GDL) algo- rithm <ref type="bibr" target="#b0">(Aji and McEliece, 2000</ref>) exploits the prop- erties of a commutative semiring to potentially re- duce the computational effort for marginalizing sums of products. The GDL algorithm manifests itself as the Viterbi, Baum-Welch, Floyd-Warshall and belief propagation algorithms, and the Fast Fourier and Hadamard transforms. Each corre- sponds to a different commutative semiring and a specific associated marginalization problem. Here, we briefly describe the general marginal- ization problem from <ref type="bibr" target="#b0">Aji and McEliece (2000)</ref> to introduce notation and also highlight the analogies to inference in factor graphs. Let x 1 , x 2 , · · · , x n denote a collection of variables that can take val- ues from finite sets A 1 , A 2 , · · · , A n respectively. Let boldface x denote the entire set of variables. These variables are akin to inference variables in factor graphs that may be assigned values or marginalized away.</p><p>Let (K, ⊕, ⊗) denote a commutative semiring. Suppose α i is a function that maps a subset of the variables {x i 1 , x i 2 , · · · } to the set K. The sub- set of variables that constitute the domain of α i is called the local domain of the corresponding local function. Local domains and local functions are analogous to factors and factor potentials in a fac- tor graph. With a collection of local domains, each associated with a function α i , the "marginalize the product" problem is that of computing:</p><formula xml:id="formula_6">x i α i (x i 1 , x i 2 , · · · )<label>(3)</label></formula><p>Here, the sum and product use the semiring op- erators. The summation is over all possible valid assignments of the variables x over the cross prod- uct of the sets A 1 , A 2 , · · · , A n . This problem gen- eralizes the familiar max-product or sum-product settings. Indeed, the GDL algorithm is a gener- alization of the message passing (Pearl, 2014) for efficiently computing marginals.</p><p>To make feature extraction efficient using the GDL algorithm, in the next section, we will define a marginalization problem in terms of the semir- ing operators by specifying the variables involved, the local domains and local functions. Instead of describing the algorithm in the general setting, we will instantiate it on the semiring at hand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Marginalizing Feature Extractors</head><p>First, let us see why we can expect any benefit from the GDL algorithm by revisiting our running example (unigrams, bigrams and trigrams), writ- ten below using the semiring operations:</p><formula xml:id="formula_7">f = w + (w -1 &amp;w) + (w -2 &amp;w -1 &amp;w) (4)</formula><p>When applied to a token, f performs two additions and three conjunctions. However, by applying the distributive property, we can refactor it as follows to reduce the number of operations:</p><formula xml:id="formula_8">f = (1 + (1 + w -2 ) &amp;w -1 ) &amp;w (5)</formula><p>The refactored version f -equivalent to the orig- inal one -only performs two additions and two conjunctions, offering a computational saving of one operation. This refactoring is done at the level of feature templates (i.e., feature extractors); the actual savings are realized when the feature vec- tors are computed by applying this feature func- tion to an input. Thus, the simplification, though seemingly modest at the template level, can lead to a substantial speed improvements when the fea- tures vectors are actually manifested from data.</p><p>The GDL algorithm instantiated with the fea- ture extractor semiring, automates such factoriza- tion at a symbolic level. In the rest of this sec- tion, first ( §5.1), we will write our problem as a marginalization problem, as in Equation <ref type="formula" target="#formula_6">(3)</ref>. Then ( §5.2), we will construct a junction tree to apply the message passing algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Canonicalizing Feature Extractors</head><p>To frame feature simplification as marginalization, we need to first write any feature extractor as a canonical sum of products that is amenable for factorization (i.e., as in <ref type="formula" target="#formula_6">(3)</ref>). To do so, in this sec- tion, we will define: (a) the variables involved, (b) the local domains (i.e., subsets of variables contributing to each product term), and,</p><note type="other">(c) a lo- cal function for each local domain (i.e., the α i 's). Variables. First, we write a feature extrac- tor as a sum of products. Our running exam- ple (4) is already one. If we had an expres- sion like f 1 &amp; (f 2 + f 3 ), we can expand it into f 1 &amp;f 2 + f 1 &amp;f 3 . From the sum of products, we identify the base feature extractors (i.e., ones not composed of other feature extractors) and define a variable x i for each. In our example, we have w, w -1 and w -2 . Next, recall from §4 that each variable x i can take values from a finite set A i . If a base feature extractor f i corresponds to the variable x i , then, we define x i 's domain to be the set A i = {1, f i }. That is, each variable can either be the bias feature extractor or the feature extractor associated with it. Our example gives three variables x 1 , x 2 , x 3 with domains A</note><formula xml:id="formula_9">1 = {1, w}, A 2 = {1, w -1 }, A 3 = {1, w -2 } respectively.</formula><p>Local domains. Local domains are subsets of the variables defined above. They are the domains of functions that constitute products in the canonical form of a feature extractor. We define the follow- ing local domains, each illustrated with the corre- sponding instantiation in our running example:</p><p>1. A singleton set for each variable:</p><formula xml:id="formula_10">{x 1 }, {x 2 },</formula><note type="other">and {x 3 }. 2. One local domain consisting of all the vari- ables: The set {x 1 , x 2 , x 3 }. 3. One local domain consisting of no variables:</note><p>The empty set {}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">One local domain for each subset of base</head><p>feature extractors that participate in at least two conjunctions in the sum-of-products (i. e., the ones that can be factored away): Only {x 1 , x 2 } in our example, because only w and w -1 participate in two conjunctions in (4). Local functions. Each local domain is associated with a function that maps variable assignments to feature extractors. These functions (called local kernels by <ref type="bibr" target="#b0">Aji and McEliece (2000)</ref>) are like po- tential functions in a factor graph. We define two kinds of local functions, driven by the goal of de-signing a marginalization problem that pushes to- wards simpler feature functions.</p><p>1. We associate the identity function with all singleton local domains, and the constant function that returns the bias 1 with the empty domain {}. 2. With all other local domains, we asso- ciate an indicator function, denoted by z. For a local domain, z is an indicator for those assignments of the variables involved, whose conjunctions are present in any prod- uct term in sum-of-products. In our run- ning example, the function z(x 1 , x 2 ) is the indicator for (x 1 , x 2 ) belonging to the set {(w, 1) , (w, w -1 )}, represented by the table:</p><formula xml:id="formula_11">x1 x2 z(x1, x2) 1 1 0 1 w -1 0 w 1 1 w w -1 1</formula><p>The indicator returns the semiring's multi- plicative and additive identities. The value of z above for inputs (w, 1) is 1 because the first term in (4) that defines the feature ex- tractor contains w, but not w -1 . On the other hand, the input (1, 1) is mapped to 0 be- cause every product term contains either w or w -1 . For the local domain {x 1 , x 2 , x 3 }, the local function is the indicator for the set {(w, 1, 1), (w, w -1 , 1), (w, w -1 , w -2 )}, corre- sponding to each product term. In summary, for the running example we have:</p><formula xml:id="formula_12">Local domain Local function {x1} x1 {x2} x2 {x3} x3 {x1, x2, x3} z(x1, x2, x3) {} 1 {x1, x2} z(x1, x2)</formula><p>The procedure described here aims to convert any feature function into a canonical form that can be factorized using the GDL algorithm. Indeed, using local domains and functions specified above, any feature extractor can we written as a canonical sum of products as in (3). For example, using the table above, our running example is identical to</p><formula xml:id="formula_13">x 1 ,x 2 ,x 3 z(x 1 , x 2 , x 3 )&amp;z(x 1 , x 2 )&amp;x 1 &amp;x 2 &amp;x 3 (6)</formula><p>Here, the summation is over the cross product of the A i 's. The choice of the z functions ensures that only those conjunctions that were in the original feature extractor remain.</p><p>This section shows one approach for canonical- ization; the local domains and functions are a de- sign choice that may be optimized in future work. We should also point out that, while this process is notationally tedious, its actual computational cost is negligible, especially given that it is to be per- formed only once at the template level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Simplifying feature extractors</head><p>As mentioned in §4, a commutative semiring can allow us to employ the GDL algorithm to effi- ciently compute a sum of products. Starting from a canonical sum-of-products expression such as the one in (6), this process is similar to variable elim- ination for Bayesian networks. The junction tree algorithm is a general scheme to avoid redundant computation in such networks <ref type="bibr" target="#b5">(Cowell, 2006</ref>). To formalize this, we will first build a junction tree and then define the messages sent from the leaves to the root. The final message at the root will give us the simplified feature function. Constructing a Junction Tree. First, we will con- struct a junction tree using the local domains from § 5.1. In any junction tree, the edges should satisfy the running intersection property: i.e., if a vari- able x i is in two nodes in the tree, then it should be in every node in the path connecting them. To build a junction tree, we will first create a graph whose nodes are the local domains. The edges of this graph connect pairs of nodes if the variables in one are a subset of the other. For simplicity, we will assume that our nodes are arranged in a lat- tice as shown in <ref type="figure">Figure 1</ref>, with edges connecting nodes in subsequent levels. For example, there is no edge connecting nodes B and C. Every spanning tree of this lattice is a junction tree. Which one should we consider? Let us ex- amine the properties that we need. First, the root of the tree should correspond to the empty local domain {} because messages arriving at this node will accumulate all products. Second, as we will see, feature extractors farther from the root will appear in inner terms in the factorized form. That is, frequent or more expensive feature extractors should be incentivized to appear higher in the tree.</p><p>To capture these preferences, we frame the task of constructing the junction tree as a maximum spanning tree problem over the graph, with edge weights incorporating the preferences. One nat- ural weighting function is the computational ex- pense of the base feature extractors associated with that edge. For example, the weight associated with the edge connecting nodes E and D in the fig- <ref type="figure">Figure 1</ref>: The junction tree for our running example. The process of constructing the junction tree is described in the text. Here, we show both the tree and the graph from which it is constructed; dashed lines show edges are not in the tree. Filled circles denote the names of the nodes. The local do- main {x1} is connected to the empty local domain because the feature w corresponding to it is most frequent.</p><formula xml:id="formula_14">{} {x 1 } {x 2 } {x 3 } {x 1 , x 2 } {x 1 , x 2 , x 3 } A B C D E F</formula><p>ure can be the average cost of the w and w -1 feature extractors. If computational costs are unavailable, we can use the number times a feature extractor appears in the expression to be simplified. Under this criterion, in our example, edges connecting E to its neighbors will be weighted highest.</p><p>Once we have a spanning tree, we make the edges directed so that the empty set is the root. <ref type="figure">Figure 1</ref> shows the junction tree obtained for our running example. Message Passing for Feature Simplification. Given the junction tree, we can use a standard message passing scheme for factorization. The goal is to collect information at each node in the tree from its children all the way to the root.</p><p>Suppose v i , v j denote two nodes in the tree. Since nodes are associated with sets of variables, their intersections v i ∩ v j and differences v i \ v j are defined. For example, in the example, A ∩ B = {x 3 } and B \ D = {x 3 }. We will denote children of a node v i in the junction tree by C(v i ).</p><p>The message from any node v i to its parent v j is a function that maps the variables v i ∩ v j to a feature extractor by marginalizing out all variables that are in v i but not in v j . Formally, we define the message µ ij from a node v i to a node v j as:</p><formula xml:id="formula_15">µij (vi ∩ vj) = v j \v i αi (vi) v k ∈C(v i ) µ ki (v k ∩ vi) . (7)</formula><p>Here, α i is the local function at node v i . To com- plete the formal definition of the algorithm, we note that by performing post-order traversal of the junction tree, we will accumulate all messages at the root of the tree, that corresponds to the empty set of variables. The incoming message at this node represents the factorized feature extractor. Algorithm 1 briefly summarizes the entire simpli- fication process. The proof of correctness of the algorithm follows from the fact that the range of all the local functions is a commutative semiring, namely the feature extractor semiring. We refer the reader to <ref type="bibr" target="#b0">(Aji and McEliece, 2000</ref>, Appendix A) for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 The Generalized Distributive Law Algorithm</head><p>for simplifying a feature extractor f. See the text for details.</p><p>1: Convert f into a canonical sum of products representa- tion ( § 5.1). 2: Construct a junction tree whose nodes are local domains. 3: for edge (vj, vi) in the post-order traversal of the tree do 4:</p><p>Receive a message µij at vj using (7). 5: end for 6: return the incoming message at the root Example run of message propagation. As an il- lustration, let us apply it to our running example.</p><p>1. The first message is from A to B. Since A has no children and its local function is the iden- tity function, we have µ AB (x) = x. Simi- larly, we have µ CD (x) = x. 2. The message from B to D has to marginal- ize out the variable x 3 . That is, we have</p><formula xml:id="formula_16">µ BD (x 1 , x 2 ) = x 3 z(x 1 , x 2 , x 3 )µ AB (x 3 ).</formula><p>The summation is over the domain of x 3 , namely {1, w -2 }. By substituting for z and µ AB , and simplifying, we get the message:</p><formula xml:id="formula_17">x1 x2 µBD(x1, x2) 1 1 0 1 w -1 0 w 1 1 w w -1 1 + w -2</formula><p>3. The message from D to E marginalizes out the variable x 2 to give us µ DE (x 1 ) =</p><formula xml:id="formula_18">x 2 z(x 1 , x 2 )µ CD (x 2 )µ BD (x 1 , x 2 ).</formula><p>Here, the summation is over the domain of x 2 , namely {1, w -1 }. We can simplify the message as:</p><formula xml:id="formula_19">x1 µDE(x1) 1 0 w 1 + (1 + w -2 ) &amp;w -1</formula><p>4. Finally, the message from E to the root F marginalizes out the variable x 1 by summing over its domain {1, w} to give us the message (1 + (1 + w -2 ) &amp;w -1 ) &amp;w. The message received at the root is the factorized feature extractor. Note that the final form is iden- tical to (5) at the beginning of §5.</p><p>Discussion. An optimal refactoring algorithm would produce a feature extractor that is both cor- rect and fastest. The algorithm above has the for- mer guarantee. While it does reduce the number of operations performed, the closeness of the refac-tored feature function to the fastest one depends on the heuristic used to weight edges for iden- tifying the junction tree. Changing the heuristic can change the junction tree, thus changing the fi- nal factorized function. We found via experiments that using the number of times a feature extractor occurs in the sum-of-products to weight edges is promising. A formal study of optimality of factor- ization is an avenue of future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We show the practical usefulness of feature func- tion refactoring using text chunking and relation extraction. In both cases, the question we seek to evaluate empirically is: Does the feature function refactoring algorithm improve feature extraction time? We should point out that our goal is not to measure accuracy of prediction, but the efficiency of feature extraction. Indeed, we are guaranteed that refactoring will not change accuracy; factor- ized feature extractors produce the same feature vectors as the original ones.</p><p>In all experiments, we compare a feature extrac- tor and its refactored variant. For the factorization, we incentivized the junction tree to factor out base feature extractors that occurred most frequently in the feature extractor. For both tasks, we use ex- isting feature representations that we briefly de- scribe. We refer the reader to the original work that developed the feature representations for fur- ther details. For both the original and the factor- ized feature extractors, we report (a) the number of additions and conjunctions at the template level, and, (b) the time for feature extraction on the en- tire dataset. For the time measurements, we report average times for the original and factorized fea- ture extractors over five paired runs to average out variations in system load. <ref type="bibr">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Text Chunking</head><p>We use data from the CoNLL 2000 shared task <ref type="bibr" target="#b28">(Tjong Kim Sang and Buchholz, 2000</ref>) of text chunking and the feature set described by <ref type="bibr" target="#b19">Martins et al. (2011)</ref>, consisting of the following templates extracted at each word: (1) Up to 3-grams of POS tags within a window of size ten centered at the word, (2) up to 3-grams of words, within a win- dow of size six centered at the word, and (3) up to 2-grams of word shapes, within a window of size <ref type="bibr">1</ref> We performed all our experiments on a server with 128GB RAM and 24 CPU cores, each clocking at 2600 MHz.  four centered at the word. In all, there are 96 fea- ture templates.</p><p>We factorized the feature representation using Algorithm 1. <ref type="table">Table 1</ref> reports the number of opera- tions (addition and conjunction) in the templates in the original and factorized versions of the feature extractor. The table also reports feature extraction time taken from the entire training set of 8,936 sentences, corresponding to 211,727 tokens. First, we see that the factorization reduces the number of feature conjunction operations. Thus, to produce exactly the same feature vector, the factorized fea- ture extractor does less work. The time results show that this computational gain is not merely a theoretical one; it also manifests itself practically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Relation Extraction</head><p>Our second experiment is based on the task of re- lation extraction using the English section of the ACE 2005 corpus ( <ref type="bibr" target="#b30">Walker et al., 2006</ref>). The goal is to identify semantic relations between two en- tity mentions in text. We use the feature represen- tation developed by <ref type="bibr" target="#b32">Zhou et al. (2005)</ref> as part of an investigation of how various lexical, syntactic and semantic sources of information affect the re- lation extraction task. To this end, the feature set consists of word level information about mentions, their entity types, their relationships with chunks, path features from parse trees, and semantic fea- tures based on WordNet and various word lists. Given the complexity of the features, we do not describe them here and refer the reader to the orig- inal work for details. Note that compared to the chunking features, these features are more diverse in their computational costs.</p><p>We report the results of our experiments in Ta-ble 2. As before, we see that the number of conjunction operations decreases after factoriza- tion. Curiously, however, despite the complexity of the feature set, the actual number of operations is smaller than text chunking. Due to this, we see a more modest, yet significant decrease in the time for feature extraction after factorization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work and Discussion</head><p>Simplifying Expressions. The problem of sim- plifying expressions with an eye on computa- tional efficiency is the focus of logic synthesis (cf. <ref type="bibr" target="#b13">Hachtel and Somenzi, 2006</ref>), albeit largely geared towards analyzing and verifying digital circuits. Logic synthesis is NP-hard in general. In our case, the hardness is hidden in the fact that our approach does not guarantee that we will find the smallest (or most efficient) factorization. The junction tree construction determines the factorization quality. Semirings in NLP. Semirings abound in NLP, though primarily as devices to design efficient inference algorithms for various graphical mod- els (e.g. <ref type="bibr" target="#b29">Wainwright and Jordan, 2008;</ref><ref type="bibr" target="#b27">Sutton et al., 2012</ref>). Goodman (1999) synthesized var- ious parsing algorithms in terms of semiring op- erations. Since then, we have seen several ex- plorations of the interplay between weighted dy- namic programs and semirings for inference in tasks such as parsing and machine translation (e. g. <ref type="bibr" target="#b7">Eisner et al., 2005;</ref><ref type="bibr" target="#b17">Li and Eisner, 2009;</ref><ref type="bibr" target="#b18">Lopez, 2009;</ref><ref type="bibr" target="#b8">Gimpel and Smith, 2009</ref>  <ref type="bibr" target="#b6">and Roth, 2002;</ref><ref type="bibr" target="#b3">Broda et al., 2013;</ref><ref type="bibr" target="#b24">Sammons et al., 2016</ref>), but they do not for- malize feature extraction from an algebraic per- spective. We expect that the algorithm proposed in this paper can be integrated into such feature con- struction languages, and also into libraries geared towards designing feature rich models (e.g. <ref type="bibr" target="#b20">McCallum et al., 2009;</ref><ref type="bibr" target="#b4">Chang et al., 2015)</ref>. Representation vs. Speed. As the recent suc- cesses ( <ref type="bibr" target="#b10">Goodfellow et al., 2016)</ref> of distributed rep- resentations show, the representational capacity of a feature space is of primary importance. Indeed, several recent lines of work that use distributed representations have independently identified the connection between conjunctions (of features or factors in a factor graph) and tensor products ( <ref type="bibr" target="#b15">Lei et al., 2014;</ref><ref type="bibr" target="#b25">Srikumar and Manning, 2014;</ref><ref type="bibr" target="#b16">Lei et al., 2015;</ref><ref type="bibr" target="#b22">Primadhanty et al., 2015)</ref>. They typically impose sparsity or low-rank requirements to induce better representations for learning. In this paper, we use the connection between tensor products and con- junctions to prove algebraic properties of feature extractors, leading to speed improvements via fac- torization.</p><p>In this context, we note that in both our experi- ments, the number of conjunctions are reduced by factorization. We argue that this is an important saving because conjunctions can be a more expen- sive operation. This is especially true when deal- ing with dense feature representations, as is in- creasingly common with word vectors and neural networks, because conjunctions of dense feature vectors are tensor products, which can be slow.</p><p>Finally, while training classifiers can be time consuming, when trained classifiers are deployed, feature extraction will dominate computation time over the classifier's lifetime. However, the pre- diction step includes both feature extraction and computing inner products between features and weights. Many features may be associated with zero weights because of sparsity-inducing learn- ing (e.g. <ref type="bibr" target="#b2">Andrew and Gao, 2007;</ref><ref type="bibr" target="#b19">Martins et al., 2011;</ref><ref type="bibr" target="#b26">Strubell et al., 2015)</ref>. Since these two as- pects are orthogonal to each other, the factoriza- tion algorithm presented in this paper can be used to speed up extraction of those features that have non-zero weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this paper, we studied the process of feature ex- traction using an algebraic lens. We showed that the set of feature extractors form a commutative semiring over addition and conjunction. We ex- ploited this characterization to develop a factor- ization algorithm that simplifies feature extractors to be more computationally efficient. We demon- strated the practical value of the refactoring algo- rithm by speeding up feature extraction for text chunking and relation extraction tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Comparison of the original and factorized feature 

extractors for the relation extraction task. We measured time 
using 3191 training mention pairs. The time improvement is 
statistically significant using the paired t-test at p &lt; 0.01. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The author thanks the anonymous reviewers for their insightful comments and feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The generalized distributive law</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert J</forename><surname>Aji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mceliece</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generalized algorithms for constructing statistical language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyril</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scalable training of L 1-regularized log-linear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galen</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fextor: A feature extraction framework for natural language processing: A case study in word sense disambiguation, relation recognition and anaphora resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bartosz</forename><surname>Broda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K˛</forename><surname>Paweł</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michał</forename><surname>Edzia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Marci´nczukmarci´nczuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radosław</forename><surname>Radziszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Ramocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wardy´nskiwardy´nski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Linguistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="41" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyam</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Srikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.07179</idno>
		<title level="m">IllinoisSL: A JAVA library for Structured Prediction</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Probabilistic networks and expert systems: Exact computational methods for Bayesian networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cowell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning with feature description logics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Cumby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Inductive logic programming</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Compiling Comp Ling: Practical weighted dynamic programming and the Dyna language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Goldlust</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLTEMNLP</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cube summing, approximate inference with non-local features, and dynamic programming without semirings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Semirings and their Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jonathan S Golan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<title level="m">Deep Learning</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semiring parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="573" to="605" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improved relation extraction with feature-rich compositional embedding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Gormley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Logic synthesis and verification algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Hachtel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Somenzi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Factor graphs and the sum-product algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><forename type="middle">J</forename><surname>Frank R Kschischang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H-A</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loeliger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on information theory</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="498" to="519" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Low-rank tensors for scoring dependency structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">High-order lowrank tensors for semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">First-and secondorder expectation semirings with applications to minimum-risk training on translation forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Translation as weighted deduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Structured sparsity in structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Q</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aguiar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mário</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Factorie: Probabilistic programming via imperatively defined factor graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Probabilistic reasoning in intelligent systems: networks of plausible inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Low-rank regularization for sparse conjunctive feature spaces: An application to named entity classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Audi</forename><surname>Primadhanty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariadna</forename><surname>Quattoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Applications of topological tensor products to infinite dimensional holomorphy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raymond A Ryan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980" />
			<pubPlace>Trinity College</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">EDISON: Feature Extraction for NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sammons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Christodoulopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parisa</forename><surname>Kordjamshidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Srikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning distributed representations for structured output prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Srikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning Dynamic Feature Selection for Fast Sequential Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Silverstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<title level="m">An introduction to conditional random fields. Foundations and Trends R in Machine Learning</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">267</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik F Tjong Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Buchholz</surname></persName>
		</author>
		<title level="m">Introduction to the CoNLL-2000 shared task: Chunking. In CoNLL</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Graphical models, exponential families, and variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Wainwright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends R in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="305" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Medero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuaki</forename><surname>Maeda</surname></persName>
		</author>
		<title level="m">ACE 2005 multilingual training corpus. Linguistic Data Consortium</title>
		<meeting><address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">57</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Combining Word Embeddings and Feature Embeddings for Fine-grained Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Gormley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Exploring various knowledge in relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
