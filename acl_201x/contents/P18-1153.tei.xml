<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:12+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Neural Approach to Pun Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="laboratory">The MOE Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution" key="instit1">Peking University</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="laboratory">The MOE Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution" key="instit1">Peking University</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="laboratory">The MOE Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution" key="instit1">Peking University</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Neural Approach to Pun Generation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1650" to="1660"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Automatic pun generation is an interesting and challenging text generation task. Previous efforts rely on templates or laboriously manually annotated pun datasets, which heavily constrains the quality and diversity of generated puns. Since sequence-to-sequence models provide an effective technique for text generation , it is promising to investigate these models on the pun generation task. In this paper, we propose neural network models for homographic pun generation, and they can generate puns without requiring any pun data for training. We first train a conditional neural language model from a general text corpus, and then generate puns from the language model with an elaborately designed decoding algorithm. Automatic and human evaluations show that our models are able to generate homo-graphic puns of good readability and quality .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Punning is an ingenious way to make conversation enjoyable and plays important role in entertain- ment, advertising and literature. A pun is a means of expression, the essence of which is in the given context the word or phrase can be understood in two meanings simultaneously <ref type="bibr" target="#b17">(Mikhalkova and Karyakin, 2017)</ref>. Puns can be classified according to various standards, and the most essential dis- tinction for our research is between homographic and homophonic puns. A homographic pun ex- ploits distinct meanings of the same written word while a homophonic pun exploits distinct mean- ings of the same spoken word. Puns can be homo- graphic, homophonic, both, or neither <ref type="bibr" target="#b19">(Miller and Gurevych, 2015)</ref>.</p><p>Puns have the potential to combine novelty and familiarity appropriately, which can induce pleas- ing effect to advertisement <ref type="bibr" target="#b27">(Valitutti et al., 2008)</ref>. Using puns also contributes to elegancy in liter- ary writing, as laborious manual counts revealed that puns are one of the most commonly used rhetoric of Shakespeare, with the frequency in cer- tain of his plays ranging from 17 to 85 instances per thousand lines <ref type="bibr" target="#b19">(Miller and Gurevych, 2015)</ref>. It is not an overstatement to say that pun genera- tion has significance in human society. However, as a special branch of humor, generating puns is not easy for humans, let alone automatically gen- erating puns with artificial intelligence techniques. While text generation is a topic of interest in the natural language processing community, pun gen- eration has received little attention.</p><p>Recent sequence-to-sequence (seq2seq) frame- work is proved effective on text generation tasks including machine translation <ref type="bibr" target="#b25">(Sutskever et al., 2014</ref>), image captioning ( <ref type="bibr" target="#b29">Vinyals et al., 2015)</ref>, and text summarization ( <ref type="bibr" target="#b26">Tan et al., 2017)</ref>. The end-to-end framework has the potential to train a language model which can generate fluent and creative sentences from a large corpus. Great progress has achieved on the tasks with sufficient training data like machine translation, achieving state-of-the-art performance. Unfortunately, due to the limited puns which are deemed insuffi- cient for training a language model, there has not been any research concentrated on generating puns based on the seq2seq framework as far as we know.</p><p>The inherent property of humor makes the pun generation task more challenging. Despite decades devoted to theories and algorithms for hu- mor, computerized humor still lacks of creativ- ity, sophistication of language, world knowledge, empathy and cognitive mechanisms compared to humans, which are extremely difficult to model <ref type="bibr" target="#b9">(Hossain et al., 2017)</ref>.</p><p>In this paper, we study the challenging task of generating puns with seq2seq models without us- ing a pun corpus for training. We propose a brand- new method to generate homographic puns us- ing normal text corpus which can result in good quality of language model and avoid considerable expense of human annotators on the limited pun resources. Our proposed method can generate puns according to the given two senses of a tar- get word. We achieve this by first proposing an improved language model that is able to generate a sentence containing a given word with a specific sense. Based on the improved language model, we are able to generate a pun sentence that is suit- able for two specified senses of a homographic word, using a novel joint beam search algorithm we propose. Moreover, based on the observed characteristics of human generated puns, we fur- ther enhance the model to generate puns highlight- ing intended word senses. The proposed method demonstrates the ability to generate homographic puns containing the assigned two senses of a target word.</p><p>Our approach only requires a general text cor- pus, and we use the Wikipedia corpus in our ex- periment. We introduce both manual ways and automatic metrics to evaluate the generated puns. Experimental results demonstrate that our meth- ods are powerful and inspiring in generating ho- mographic puns.</p><p>The contributions of our work are as follows:</p><p>• To our knowledge, our work is the first at- tempt to adopt neural language models on pun generation. And we do not use any tem- plates or pun data sets in training the model.</p><p>• We propose a brand-new algorithm to gen- erate sentences containing assigned distinct senses of a target word.</p><p>• We further ameliorate our model with asso- ciative words and multinomial sampling to produce better pun sentences.</p><p>• Our approach yields substantial results on generating homographic puns with high ac- curacy of assigned senses and low perplexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Pun Generation</head><p>In recent decades, exploratory research into com- putational humor has developed to some extent, but seldom is research specifically concerned with puns. <ref type="bibr" target="#b19">Miller and Gurevych (2015)</ref> found that most previous studies on puns tend to focus on phono- logical or syntactic pattern rather than semantic pattern. In this subsection we briefly review some prior work on pun generation. <ref type="bibr" target="#b13">Lessard and Levison (1992)</ref> devised a pro- gram to create Tom Swifty, a type of pun which is present in a quoted utterance followed by a punning adverb. <ref type="bibr" target="#b3">Binsted and Ritchie (1994)</ref> came up with an early prototype of pun-generator Joke Analysis and Production Engine (JAPE). The model generates question-answer punning with two types of structures: schemata for determin- ing relationships between key words in a joke, and templates for producing the surface form of the joke. Later its successor JAPE-2 <ref type="bibr" target="#b1">(Binsted, 1996;</ref><ref type="bibr" target="#b2">Binsted et al., 1997</ref>) and STANDUP ( <ref type="bibr" target="#b24">Ritchie et al., 2007</ref>) introduced constructing descriptions. The Homonym Common Phrase Pun generator <ref type="bibr" target="#b28">(Venour, 1999</ref>) could create two-utterance texts: a one-sentence set-up and a punch-line. <ref type="bibr" target="#b28">Venour (1999)</ref> used schemata to specify the required lexi- cal items and their intern relations, and used tem- plates to indicate where to fit the lexical items in a skeleton text <ref type="bibr" target="#b23">(Ritchie, 2004)</ref>. <ref type="bibr" target="#b16">McKay (2002)</ref> proposed WISCRAIC program which can pro- duce puns in three forms: question-answer form, single sentence and a two-sentence sequence. The Template-Based Pun Extractor and Genera- tor <ref type="bibr" target="#b8">(Hong and Ong, 2009</ref>) utilized phonetic and semantic linguistic resources to extract word rela- tionships in puns automatically. The system stores the extracted knowledge in template form and re- sults in computer-generated puns.</p><p>Most previous research on pun generation is based on templates which is convenient but lacks linguistic subtlety and can be inflexible. None of the systems aimed to be creative as the skeletons of the sentences are fixed and the generation process based on lexical information rarely needs world knowledge or reasoning <ref type="bibr" target="#b23">(Ritchie, 2004)</ref>. Recently more and more work focuses on pun detection and interpretation <ref type="bibr" target="#b20">(Miller et al., 2017;</ref><ref type="bibr" target="#b19">Miller and Gurevych, 2015;</ref><ref type="bibr" target="#b6">Doogan et al., 2017)</ref>, rather than pun generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Natural Language Generation</head><p>Natural language generation is an important area of NLP and it is an essential foundation for the tasks like machine translation, dialogue response generation, summarization and of course pun gen- eration.</p><p>In the past, text generation is usually based on the techniques like templates or rules, proba- bilistic models like n-gram or log-linear models. Those models are fairly interpretable and well- behaved but require infeasible amounts of hand- engineering to scale with the increasing training data <ref type="bibr" target="#b30">(Xie, 2017)</ref>. In most cases larger corpus re- veals better what matters, so it is natural to tackle large scale modeling <ref type="bibr">(Józefowicz et al., 2016)</ref>.</p><p>Recently, neural network language models ( <ref type="bibr" target="#b0">Bengio et al., 2003)</ref> have shown the good ability to model language and fight the curse of dimen- sionality. <ref type="bibr" target="#b4">Cho et al. (2014)</ref> propose the encoder- decoder structure which proves very efficient to generate text. The encoder produces a fixed-length vector representation of the input sequence and the decoder uses the representation to generate an- other sequence of symbols. Such model has a sim- ple structure and maps the source to the target di- rectly, which outperforms the prior models in text generation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Models</head><p>The goal of our pun generation model is to gen- erate a sentence containing a given target word as homographic pun. Give two senses of the target word (a polyseme) as input, our model generates a sentence where both senses of the word are ap- propriate in the sentence. We adopt the encoder- decoder framework to train a conditional language model which can generate sentences containing each given sense of the target word. Then we pro- pose a joint beam search algorithm to generate an appropriate sentence to convey both senses of the target word. We call this Joint Model whose ba- sic structure is illustrated in <ref type="figure">Figure 1</ref>. We further propose an improved model to highlight the dif- ferent senses of the target word in one sentence, by reminding people the specific senses of the tar- get word, which may not easily come to mind. We achieve this by using Pointwise Mutual Informa- tion (PMI) to find the associative words of each sense of the target word and increase their proba- bility of appearance while decoding. To improve the diversity of the generated sentence, we use multinomial sampling to decode words in the de- coding process. The improved model is named the Highlight Model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Joint Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Conditional Language Model</head><p>For a given word as input, we would like to gen- erate a natural sentence containing the target word with the specified sense. We improve the neural language model to achieve this goal, and name it conditional language model. The conditional language model for pun gener- ation is similar to the seq2seq model with an in- put of only one word. We use Long Short-Term Memory (LSTM) as encoder to map the input se- quence (target word) to a vector of a fixed dimen- sionality, and then another LSTM network as de- coder to decode the target sequence from the vec- tor ( <ref type="bibr" target="#b25">Sutskever et al., 2014)</ref>.</p><p>Our goal is to generate a sentence contain- ing the target word. However, vanilla seq2seq model cannot guarantee the target word to ap- pear in the generated sequence all the time. To solve this problem, we adopt the asynchronous forward/backward generation model proposed by <ref type="bibr" target="#b21">Mou et al. (2015)</ref>, which employs a mechanism to guarantee some word to appear in the output in seq2seq models. The model first generates the backward sequence starting from the target word w t at position t of the sentence (i.e., the words be- fore w t ), and ending up with "&lt;/s&gt;" at the po- sition 0 of the sentence. The probability of the backward sequence is denoted as p(w 1 t ). Then we reverse the output of the backward sequence as the input to the forward model. In this pro- cess, the goal of the encoder is to map the gener- ated half sentence to a vector representation and the decoder will generate the latter part accord- ingly. The probability of the forward sequence is denoted as p(w n t ). Then the input and output of the forward model are concatenated to form the generated sentence. In the asynchronous for- ward/backward model, the probability of the out- put sentence can be decomposed as:</p><formula xml:id="formula_0">p( w 1 t = w n t ) = p(w t ) t i=0 p (bw) (w t−i |·) m−t+1 i=0 p (fw) (w t+i |·),</formula><p>(1) where p() denotes the probability of a particu- lar backward/forward sequence ( <ref type="bibr" target="#b21">Mou et al., 2015)</ref>. <ref type="figure">Figure 1</ref>: Framework of the proposed Joint Model. (Top) Two senses of the target word input 1 and input 2 (e.g. "countv01" and "countv08") are firstly provided to the backward model, to generate the backward sequence starting from the target senses and ending up with "&lt;/s&gt;". (Bottom) Then the backward sequence are reversed and inputted to the forward model, to generate the forward sequence. The inputs and outputs of the forward model are concatenated to form the final output sentence. Joint beam search algorithm is used to generate each word that has the potential to make the generated sentence suitable for both input senses. ity of w t given previous sequence · in the back- ward or forward model respectively. The above model can only guarantee the target word to ap- pear in the generated sentence. Since we hope to generate a sentence containing the specified word sense, we treat different senses of the same word as independent new pseudo-words. We label the senses of words with Word Sense Disambigua- tion (WSD) tools, and then we train the language model using the corpus with labeled senses so that for each word sense we can generate a sen- tence accordingly. We use the Python Implemen- tations of WSD Technologies 1 for WSD. This tool can return the most possible sense for the target word based on WordNet <ref type="bibr" target="#b18">(Miller, 1995)</ref>. We at- tach the sense label to the word and form a new pseudo-word accordingly. Taking "count" for ex- ample, "countv01" means "determine the number 1 https://github.com/alvations/pywsd or amount of ", while "countv08" means "have faith or confidence in".</p><formula xml:id="formula_1">p (bw) (w t |·) or p (fw) (w t |·) denotes the probabil-input input</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Decoding with Joint Beam Search Algorithm</head><p>Beam search is a frequently-used algorithm in the decoding stage of seq2seq models to generate the output sequence. It can be viewed as an adaptation of branch-and-bound search that uses an inadmis- sible pruning rule. In the beam search algorithm, only the most promising nodes at each level of the search graph are selected and the rest nodes are permanently removed. This strategy makes beam search able to find a solution within practical time or memory limits and work well in practical tasks ( <ref type="bibr" target="#b31">Zhou and Hansen, 2005;</ref><ref type="bibr" target="#b7">Freitag and Al-Onaizan, 2017</ref>). We also use beam search in our pun genera- tion model. According to the definition of homo- graphic puns, at least two senses of the target word should be interpreted in one sentence. We hope to generate a same sentence for distinct senses of the same word, and in this way the target word in the sentence can be interpreted as various senses. Pro- vided with two senses of a target word as inputs to the encoder in the backward generation pro- cess, e.g. "countv01" as input 1 and "countv08" as input 2 , we decode two output sentences in par- allel, and the two sentences should be the same except for the input pseudo-words. Assume h (s) t,i denotes the hidden state of the i-th beam at time step t, when given the s-th pseudo-word as input (s =1 or 2). In the traditional beam search algo- rithm, softmax layer is applied on the hidden state to get the probability distribution on the vocabu- lary, and the log likelihood of the probability is used to get a word score distribution d (s)</p><formula xml:id="formula_2">t,i : d (s) t,i = log(sof tmax layer(h (s) t,i )).<label>(2)</label></formula><p>The accumulated score distribution on the i-th beam is: p (s)</p><formula xml:id="formula_3">t,i = u (s) t−1,i + d (s) t,i ,<label>(3)</label></formula><p>|V | denotes the vocabulary size. u respectively. Since input 1 and input 2 are different, the candidates for two inputs will hardly be the same. However, our goal is to choose candidate words which have the potential to result in candidate sentences suitable for both senses. Our joint beam search algorithm selects b candidates while decoding for the two inputs ac- cording to the joint score distribution on all beams. The joint score distribution on the i-th beam is:</p><formula xml:id="formula_4">o t,i =p (1) t,i + p (2) t,i .<label>(4)</label></formula><p>The summation of the log scores can be viewed as the product of original probabilities, which rep- resents the joint probability if the two probabil- ity distributions are viewed independent. Given the b candidates selected according to the joint score distribution, our joint beam search algorithm</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Joint Beam Search Algorithm</head><p>b denotes the beam width. l denotes the number of unfinished beams. BeamId records which beams the candidates come from. W ordId records the indices of candidates in the vo- cabulary where 1 is the index of "&lt;s&gt;". BEAM t <ref type="bibr">[i]</ref> denotes the i-th beam history till time step t. |V | denotes the vocabu- lary size. Copy(m, n) aims to make an n-dimensional vector by replicating m for n times. The initial states of the decoder (h</p><formula xml:id="formula_5">(1) −1,i ,h<label>(2)</label></formula><p>−1,i ) are equal to the final states of the encoder ac- cordingly. m n denotes appending n to m.</p><formula xml:id="formula_6">BEAM −1[i]= [], i=0, 1, ..., b − 1 u (1) −1,i = u (2) −1,i = Copy(0, |V |),i = 0, 1, ..., b − 1 BeamId = [0, 1, ..., b − 1] W ordId = [1, .., 1] ∈ R b Outputs = []; t = 0; l = b while l &gt; 0 do o=[] for i= 0 to b − 1 do xt,i is the word embedding corresponding to W ordId[i] h (1) t,i = LSTM(xt,i, h<label>(1)</label></formula><formula xml:id="formula_7">t−1,i ) h (2) t,i = LSTM(xt,i, h (2) t−1,i ) p (1) t,i = u<label>(1)</label></formula><p>t−1,i + log(sof tmax layer(h</p><formula xml:id="formula_8">t,i )) p (2) t,i = u<label>(1)</label></formula><p>t−1,i + log(sof tmax layer(h </p><formula xml:id="formula_10">1 do BEAMt[i] = BEAMt−1[BeamId[i]] W ordId[i] u<label>(1)</label></formula><formula xml:id="formula_11">t,i = u (2) t,i = Copy(o t,BeamId[i] [W ordId[i]], |V |) if W ordId[i] represents "&lt;/s&gt;" l = l − 1 Outputs = Outputs BEAMt[i]</formula><p>end if end for t = t + 1 return top b items in Outputs is similar to the vanilla beam search algorithm, which generates the candidate sequences step by step. If any beam selects "&lt;/s&gt;" as the candi- date, we regard this branch has finished decod- ing. The decoding process will be finished after all the beams have selected "&lt;/s&gt;". The joint beam search algorithm is described in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Highlight Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Word Association</head><p>The joint model we described above is able to generate sentences suitable for both given senses of the target word. But we found this model is prone to generate monotonous sentences, making it difficult to discover that the target word in the sentence can be understood in two ways. For ex- ample, in the sentence "He couldn't count on his friends", people can easily realize that the com- mon meaning "have faith or confidence in" of the word "count", but may ignore other senses of the word. If we add some words and modify the sen- tence as "The inept mathematician couldn't count on his friends", people can also come up with the meaning "determine the number or amount of " due to the word "mathematician". Comparing the examples above, the two senses are proper in both sentences, but people may interpret "count" in the two sentences differently. Based on such ob- servations, we improve the pun generation model by adding some keywords to the sentence which could remind people some special sense of the target word. We call those keywords associative words, and the improved model is named as High- light Model.</p><p>To extract associative words of each sense of the target word, we first build word association norms in our corpus by using pointwise mutual informa- tion (PMI). As mutual information compares the probability of observing w 1 and w 2 together (the joint probability) with the probabilities of observ- ing w 1 and w 2 independently (chance) <ref type="bibr" target="#b5">(Church and Hanks, 1990)</ref>, positive PMI scores indicate that the words occur together more than would be expected under an independence assumption, and negative scores indicate that one word tends to ap- pear solely when the other does not <ref type="bibr" target="#b11">(Islam and Inkpen, 2006</ref>). In this case we take top k asso- ciative words for each sense with relatively high positive PMI scores, which are calculated as fol- lows:</p><formula xml:id="formula_12">P MI(w 1 , w 2 ) = log 2 p(w 1 , w 2 ) p(w 1 ) · p(w 2 ) .<label>(5)</label></formula><p>During decoding we increase the probability of the associative words to be chosen according to their PMI scores. For each sense of the target word, we normalize the PMI scores of the asso- ciative words as follows:</p><formula xml:id="formula_13">Asso(w (s) t , c p ) = σ( P MI(w (s) t , c p ) max c j P MI(w (s) t , c j ) ),<label>(6)</label></formula><p>where w (s) t represents the s-th sense of the tar- get word w t , and c p is the p-th associative word for w (s) t . To smooth the PMI scores we use sig- moid function σ which is differentiable and widely used in the neural network models. The final PMI score for each associative word is denoted as Asso(w (s) t , c p ). As we choose candidates accord- ing to a score distribution on the whole vocabulary, we need a PMI score distribution (S(w (s) t )) rather than single scores, and the value at position q is supposed to be:</p><formula xml:id="formula_14">S w (s) t [q] = Asso w (s) t ,v q , v q ∈ AssoTK(w (s) t ); 0, e l s e ,<label>(7)</label></formula><p>where v q denotes the q-th word in the vocabulary, and AssoT K(w (s) t ) denotes the top k associative words of w (s) t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Multinomial Sampling</head><p>In our highlight model, we add S(w <ref type="bibr">(1)</ref> t ) and S(w <ref type="bibr">(2)</ref> t ) to o t,i , as:</p><formula xml:id="formula_15">o t,i =o t,i +α 1 ·S(w (1) t )+α 2 ·S(w (2) t ), (8)</formula><p>where we use α 1 and α 2 as coefficient weights to balance the PMI scores of the two assigned senses and the joint score. In the Highlight Model, we first select 2b candidates according to the scores of words from Eq. 8. Then we use multinomial sam- pling to select the final b candidates. Sampling is useful in cases where we may want to get a variety of outputs for a particular input. One example of a situation where sampling is meaningful would be in a seq2seq model for a dialog system <ref type="bibr" target="#b22">(Neubig, 2017)</ref>. In our pun generation model we hope to produce relatively more creative sentences, so we use multinomial sampling to increase the uncer- tainty when generating the sentence. The multi- nomial distribution can be seen as a multivariate generalization of the binomial distribution and it is prone to choose the words with relatively high probabilities. If an associative word of one sense has been selected, we decay the scores for all as- sociative words of this sense. In this way we can prevent the sentence obviously being prone to re- flect one sense of the target word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data Preprocessing</head><p>Most text generation tasks using seq2seq model require large amount of training data. However, for many tasks, like pun generation, it is difficult to get adequate data to train a seq2seq model. In this study, our pun generation model does not rely on training data of puns. We only require a text corpus to train the conditional language model, which is very cheap to get. In this paper, we use the English Wikipedia corpus to train the language model. The corpus texts are firstly lowercased and tokenized, and all numeric characters are replaced with "#". We split the texts into sentences and discard the sentences whose length is less than 5 words or more than 50 words. We then select pol- ysemes appearing in the homographic pun data set <ref type="bibr" target="#b20">(Miller et al., 2017</ref>) and pun websites. Those pol- ysemes in the corpus are replaced by the labeled sense. We restrict that each sentence can be la- beled with at most two polysemes in order to train a reliable language model. If there are more pol- ysemes in one sentence, we keep the last two be- cause in our observation we found pun words tend to occur near the end of a sentence. After label- ing, we keep the 105,000 most frequently occur- ring words and other words are replaced with the "&lt;unk&gt;" token. We discard the sentences with two or more "&lt;unk&gt;" tokens. There are totally 3,974 distinct labeled senses corresponding to a total of 772 distinct polysemes. We assume those reserved senses are more likely to generate puns of good quality.</p><p>While training the language model we use 2,595,435 sentences as the training set, and 741,551 sentences as the development set to de- cide when to stop training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training Details</head><p>The number of LSTM layers we use in the seq2seq model is 2 and each layer has 128 units. To avoid overfitting, we set the dropout rate to 0.2. We use Stochastic Gradient Descent (SGD) with a de- creasing learning rate schedule as optimizer. The initial learning rate is 1.0 and is halved every 1k steps after training for 8k steps, which is the same as <ref type="bibr" target="#b15">Luong et al. (2017)</ref>. We set beam size b = 5 while decoding. For each sense we select at most 30 associative words (k=30). To increase the prob- ability of choosing the associative words, we set α 1 = 6.0 and α 2 = 6.0. If an associative word of some sense of a target word has been chosen, its corresponding α will be set to zero for all the associative words of this sense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines</head><p>Since there is no existing neural model applied on this special task, we implement two baseline mod- els for comparison. We select 100 target words and two senses for each word to test the quality of those models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Normal Language Model: It is trained with</head><p>an encoder-decoder model and uses beam search while decoding. In the training process, inputs are unlabeled target words and outputs are sentences containing the target words.</p><p>Pun Language Model: We use the data set of homographic puns from <ref type="bibr" target="#b20">Miller et al. (2017)</ref>. The model is trained on the data set in asynchronous forward/backward way. As the pun data set is limited, the pun language model has no creativity, which means if we input a word appearing in the training data, then the output will usually be an ex- isting sentence from the training data. Therefore, we remove the sentences which contain words in the 100 target words from the pun data set, and then train the model for test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Automatic Evaluation</head><p>We select 100 target words and two senses for each word for test. We use the language mod- eling toolkit SRILM 2 to train a trigram model with another 7,746,703 sentences extracted from Wikipedia, which are different from the data set used before. The perplexity scores (PPL) of our models and baseline models are estimated based on the trained language model, as shown in Ta- ble 1. Normal Language Model has no constraint of generating sentences suitable for both senses. This means at each time step the beam search algo- rithm can select the candidates with highest prob- abilities. And thus it is natural that it obtains the lowest perplexity. Taking the constraint of senses into consideration, the perplexity scores of Joint Model and Highlight Model are still comparable to that of Normal Language Model. However, Pun Language Model could not be trained well con- sidering the limit of the pun training data, so it gets the highest perplexity score. This result re- veals that it is not feasible to build a homographic pun generation system based on the pun data set since pun data is far from enough. In the table, We further compare the diversity of the generated sen- tences of four models following <ref type="bibr" target="#b14">Li et al. (2016)</ref>. Distinct-1 (d.-1) and distinct-2 (d.-2) are the ra- tios of the distinct unigrams and bigrams in gen- erated sentences, i.e., the number of distinct uni- grams or bigrams divided by the total number of unigrams or bigrams. The results show our mod- els are more creative than Normal Language and  </p><formula xml:id="formula_16">Model PPL d.-1(%) d.-2(%)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Human Evaluation</head><p>Because of the subtle and delicate structure of puns, automatic evaluation is not enough. So we sample one sentence for each word from four mod- els mentioned above and then get 100 sentences of each model generated from the target words, to- gether with 100 puns containing the same target words from homographic pun data set in <ref type="bibr" target="#b20">Miller et al. (2017)</ref>. We ask judges on Amazon Mechan- ical Turk to evaluate all the sentences and the rat- ing score ranges from 1 to 5. Five native English speakers are asked to give a score on each sen- tence in three aspects with the following informa- tion: Readability indicates whether the sentence is easy to understand semantically; Accuracy in- dicates whether the given senses are suitable in a sentence; Fluency indicates whether the sentence is fluent and consistent with the rules of grammar. The results in <ref type="figure" target="#fig_2">Figure 2</ref> show that pun data is not enough to train an ideal language model, while Normal Language Model has enough corpus to train a good language model. But Normal Lan- guage Model is unable to make the given two senses appear in one sentence and in a few cases even can not assure the appearance of the target <ref type="bibr">words</ref>  To test the potential of the sentences generated by our models to be homographic puns, we fur- ther design a Soft Turing Test. We select 30 sen- tences generated by Joint Model and 30 sentences generated by Highlight Model independently, to- gether with 30 gold puns from the homographic pun data set. We mix them up, and give the def- inition of homographic pun and ask 10 people on Amazon Mechanical Turk to judge each sentence. People can judge each sentence as one of three cat- egories: definitely by human, might by human and definitely by machine. The three categories cor- respond to the scores of 2, 1 and 0, respectively. If the average score of one sentence is equal or greater than 1, we regard it as judged to be gener- ated by human. The number of sentences judged as by human for each model and the average score for each model are shown in <ref type="table" target="#tab_2">Table 2</ref>.</p><p>Due to the flexible language structure of High- light Model, the generated homographic puns out- perform those generated by Joint Model in the Soft Turing Test, however still far from gold-standard puns. Our models are adept at generating homo- graphic puns containing assigned senses but weak in making homographic puns humorous.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Examples</head><p>We show some examples generated by differ- ent models in <ref type="table" target="#tab_3">Table 3</ref>. For the two senses of "pitch", Highlight Model generates a sentence which uses "high" to remind readers the sense re- lated to sound and uses "player" to highlight the sense related to throwing a baseball. Joint Model returns a sentence that can be understood in both way roughly only if we give the two senses in ad- vance, otherwise readers may only think of the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Sample pitch: 1) the property of sound that arise with variation in the frequency of vibration;</p><p>2) the act of throwing a baseball by a pitcher to a batter. Highlight in one that denotes player may have had a high pitch in the world Joint the object of the game is based on the pitch of the player Normal Language this is a list of high pitch plot Pun Language our bikinis are exciting they are simply the tops on the mouth Gold Puns if you sing while playing baseball you won't get a good pitch square: 1) a plane rectangle with four equal sides and four right angles, a four-sided regular polygon;</p><p>2) someone who doesn't understand what is going on. Highlight little is known when he goes back to the square of the football club Joint there is a square of the family Normal Language the population density was # people per square mile Pun Language when the pirate captain's ship ran aground he couldn't fathom why Gold Puns my advanced geometry class is full of squares problem: 1) a source of difficulty;</p><p>2) a question raised for consideration or solution. Highlight you do not know how to find a way to solve the problem which in the state Joint he is said to be able to solve the problem as he was a professor Normal Language in # he was appointed a member of the new york stock exchange Pun Language those who iron clothes have a lot of pressing veteran Gold Puns math teachers have lots of problems sense related to baseball. For Normal Language Model, it is difficult to be interpreted in two senses we assigned. Pun Language Model has no ability to return a sentence containing the assigned word at all. Observing the gold pun, the context de- scribes a more vivid scene which we need to pay attention to. For "square", sentences generated by Highlight Model and Joint Model can be inter- preted in two senses and Highlight Model results in a sentence with dexterity. Normal Language Model give a sentence where "square" means nei- ther of the two given senses. Pun Language Model cannot return a sentence we need with no sur- prise. For "problem", both Highlight Model and Joint Model can generate sentences containing as- signed two senses while Normal Language Model and Pun Language Model can not return sentences with the target word. Compare to our generated sentences, we find gold puns are more concise and accurate, which takes us into consideration on the delicate structure of puns and the conclusion is still in exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this paper, we proposed two models for pun generation without using training data of puns. Joint Model makes use of conditional language model and the joint beam search algorithm, which can assure the assigned senses of target words suit- able in one sentence. Highlight Model takes asso- ciative words into consideration, which makes the distinct senses more obvious in one sentence. The produced puns are evaluated using automatic eval- uation and human evaluation, and they outperform the sentences generated by our baseline models.</p><p>For future work, we hope to improve the results by using the pun data and design a more proper way to select candidates from associative words.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>t−1,i is a |V |- dimensional vector whose values are all equal to the accumulated score of the generated sequence till time step t − 1. Assume the beam width is b, p (s) t is the concatenation of p (s) t,i on all beams and its dimension size is |V | * b. The beam search algo- rithm selects b candidate words at each time step according to p (s) t (s =1 or 2). When decoding for input 1 and input 2 in parallel, at each time step there will be b candidates for each input according to p (1) t and p (2) t</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>end for W ordId = the indices of words with the top b scores in o BeamId = the indices of source beams w.r.t. W ordId for i= 0 to b −</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Results of human evaluation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>. Joint Model and Highlight Model can gen- erate fluent sentences for the assigned two senses. Although Highlight Model could remind people</figDesc><table>Model 
# sentences 
avg. score 
Highlight 
15 
0.98 
Joint 
12 
0.87 
Gold Puns 
28 
1.38 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 : Results of Soft Turing Test.</head><label>2</label><figDesc></figDesc><table>specific senses of the target words in most cases, in 
few cases sampled words make the whole sentence 
unsatisfactory and get a relatively lower score of 
accuracy. As to the Readability, the Joint Model 
performs better than other three models. Both 
Joint model and Highlight model outperform Nor-
mal Language Model and Pun Language Model. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Examples of outputs by different models.</figDesc><table></table></figure>

			<note place="foot" n="2"> http://www.speech.sri.com/projects/srilm/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work was supported by National Natural Sci-ence Foundation of China <ref type="bibr">(61772036,</ref><ref type="bibr">61331011)</ref> and Key Laboratory of Science, Technology and Standard in Press Industry (Key Laboratory of In-telligent Press Media Technology). We thank the anonymous reviewers for their helpful comments. Xiaojun Wan is the corresponding author.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>Pascal Vincent, and Christian Janvin</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Machine humour: An implemented model of puns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Binsted</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Children&apos;s evaluation of computer-generated punning riddles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Binsted</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Pain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme D</forename><surname>Ritchie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pragmatics &amp; Cognition</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="305" to="354" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An implemented model of punning riddles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Binsted</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Ritchie</surname></persName>
		</author>
		<ptr target="http://www.aaai.org/Library/AAAI/1994/aaai94-096.php" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th National Conference on Artificial Intelligence</title>
		<meeting>the 12th National Conference on Artificial Intelligence<address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994-07-31" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="633" to="638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<ptr target="http://arxiv.org/abs/1406.1078" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Word association norms, mutual information, and lexicography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><forename type="middle">Ward</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Hanks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="29" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Idiom savant at semeval2017 task 7: Detection and interpretation of english puns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Samuel Doogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanyang</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Veale</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/S17-2011</idno>
		<ptr target="https://doi.org/10.18653/v1/S17-2011" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
		<meeting>the 11th International Workshop on Semantic Evaluation<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-03" />
			<biblScope unit="page" from="103" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Beam search strategies for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Al-Onaizan</surname></persName>
		</author>
		<ptr target="https://aclanthology.info/papers/W17-3207/w17-3207" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Neural Machine Translation</title>
		<meeting>the First Workshop on Neural Machine Translation<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-04" />
			<biblScope unit="page" from="56" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatically extracting word relationships as templates for pun generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethel</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Computational Approaches to Linguistic Creativity</title>
		<meeting>the Workshop on Computational Approaches to Linguistic Creativity<address><addrLine>Stroudsburg, PA, USA, CALC &apos;09</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="24" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Filling the blanks (hint: plural noun) for mad libs humor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nabil</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Krumm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Horvitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><forename type="middle">A</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017</title>
		<meeting>the 2017</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
				<ptr target="https://aclanthology.info/papers/D17-1067/d17-1067" />
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<meeting><address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page" from="638" to="647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Second order co-occurrence PMI for determining the semantic similarity of words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aminul</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth International Conference on Language Resources and Evaluation, LREC</title>
		<meeting>the Fifth International Conference on Language Resources and Evaluation, LREC<address><addrLine>Genoa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-05-22" />
			<biblScope unit="page" from="1033" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Exploring the limits of language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Józefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02410</idno>
		<ptr target="http://arxiv.org/abs/1602.02410" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Computational modelling of linguistic humour: Tom swifties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Lessard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Levison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ALLC/ACH Joint Annual Conference</title>
		<meeting><address><addrLine>Oxford</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="175" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A diversity-promoting objective function for neural conversation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>San Diego California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-12" />
			<biblScope unit="page" from="110" to="119" />
		</imprint>
	</monogr>
	<note>NAACL HLT 2016</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Neural machine translation (seq2seq) tutorial</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<ptr target="https://github.com/tensorflow/nmt" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Generation of idiom-based witticisms to aid second language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Mckay</surname></persName>
		</author>
		<editor>Stock et al.</editor>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="77" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Punfields at semeval-2017 task 7: Employing roget&apos;s thesaurus in automatic pun recognition and interpretation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Mikhalkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Karyakin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05479</idno>
		<ptr target="http://arxiv.org/abs/1707.05479" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic disambiguation of english puns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tristan</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P/P15/P15-1070.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015-07-26" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="719" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SemEval-2017 Task 7: Detection and interpretation of English puns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tristan</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><forename type="middle">F</forename><surname>Hempelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
		<meeting>the 11th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="59" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Backbone language modeling for constrained natural language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.06612</idno>
		<ptr target="http://arxiv.org/abs/1512.06612" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Neural machine translation and sequence-to-sequence models: A tutorial</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01619</idno>
		<ptr target="http://arxiv.org/abs/1703.01619" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The linguistic analysis of jokes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Ritchie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>Routledge</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A practical application of computational humour</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruli</forename><surname>Manurung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Pain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annalu</forename><surname>Waller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rolf</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave O&amp;apos;</forename><surname>Mara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Joint Conference on Computational Creativity</title>
		<meeting>the 4th International Joint Conference on Computational Creativity</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="91" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-813" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Abstractive document summarization with a graphbased attentional neural model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/P17-1108</idno>
		<ptr target="https://doi.org/10.18653/v1/P17-1108" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers. Association for Computational Linguistics</publisher>
			<date type="published" when="2017-07-30" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1171" to="1181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Textual affect sensing for computational advertising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Valitutti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Strapparava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliviero</forename><surname>Stock</surname></persName>
		</author>
		<idno>SS-08-03</idno>
	</analytic>
	<monogr>
		<title level="m">Creative Intelligent Systems, Papers from the 2008 AAAI Spring Symposium</title>
		<meeting><address><addrLine>Stanford, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-03-26" />
			<biblScope unit="page" from="117" to="122" />
		</imprint>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The computational generation of a class of puns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Venour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Master&apos;s thesis</title>
		<meeting><address><addrLine>Kingston, Ontario</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
		<respStmt>
			<orgName>Queen&apos;s University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<idno type="doi">10.1109/CVPR.2015.7298935</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2015.7298935" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015-06-07" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziang</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09534</idno>
		<title level="m">Neural text generation: A practical guide</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Beamstack search: Integrating backtracking with beam search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">A</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth International Conference on Automated Planning and Scheduling (ICAPS 2005)</title>
		<meeting>the Fifteenth International Conference on Automated Planning and Scheduling (ICAPS 2005)<address><addrLine>Monterey, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06-05" />
			<biblScope unit="page" from="90" to="98" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
