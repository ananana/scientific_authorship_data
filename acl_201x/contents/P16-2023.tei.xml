<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:22+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving cross-domain n-gram language modelling with skipgrams</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>August 7-12, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis</forename><surname>Onrust</surname></persName>
							<email>l.onrust@let.ru.nl</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CLS</orgName>
								<orgName type="institution" key="instit2">Radboud University Nijmegen ESAT-PSI</orgName>
								<address>
									<settlement>KU Leuven</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Van</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">ESAT-PSI</orgName>
								<orgName type="institution">Radboud University</orgName>
								<address>
									<settlement>Nijmegen, Leuven</settlement>
									<region>KU</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving cross-domain n-gram language modelling with skipgrams</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="137" to="142"/>
							<date type="published">August 7-12, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper we improve over the hierarchical Pitman-Yor processes language model in a cross-domain setting by adding skip-grams as features. We find that adding skipgram features reduces the perplexity. This reduction is substantial when models are trained on a generic corpus and tested on domain-specific corpora. We also find that within-domain testing and cross-domain testing require different backoff strategies. We observe a 30-40% reduction in perplexity in a cross-domain language modelling task, and up to 6% reduction in a within-domain experiment, for both English and Flemish-Dutch.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Since the seminal paper on hierarchical Bayesian language models based on Pitman-Yor processes <ref type="bibr">(Teh, 2006</ref>), Bayesian language modelling has re- gained an interest. Although Bayesian language models are not new <ref type="bibr" target="#b5">(MacKay and Peto, 1995)</ref>, previously proposed models were reported to be inferior compared to other smoothing methods. Teh's work was the first to report on improve- ments over interpolated Kneser-Ney smoothing <ref type="bibr">(Teh, 2006</ref>).</p><p>To overcome the traditional problems of over- estimating the probabilities of rare occurrences and underestimating the probabilities of unseen events, a range of smoothing algorithms have been proposed in the literature <ref type="bibr" target="#b2">(Goodman, 2001</ref>). Most methods take a heuristic-frequentist ap- proach combining n-gram probabilities for vari- ous values of n, using back-off schemes or inter- polation. <ref type="bibr">Teh (2006)</ref> showed that MacKay and <ref type="bibr" target="#b5">Peto's (1995)</ref> research on parametric Bayesian language models with a Dirichlet prior could be extended to give better results, but also that one of the best smoothing methods, interpolated Kneser-Ney ( <ref type="bibr" target="#b4">Kneser and Ney, 1995)</ref>, can be derived as an ap- proximation of the Hierarchical Pitman-Yor pro- cess language model (HPYLM).</p><p>The success of the Bayesian approach to lan- guage modelling is due to the use of statistical dis- tributions such as the Dirichlet distribution, and distributions over distributions, such as the Dirich- let process and its two-parameter generalisation, the Pitman-Yor process. Both are widely stud- ied in the statistics and probability theory com- munities. Interestingly, language modelling has acquired the status of a "fruit fly" problem in these communities, to benchmark the performance of statistical models. In this paper we approach lan- guage modelling from a computational linguistics point of view, and consider the statistical methods to be the tool with the future goal of improving language models for extrinsic tasks such as speech recognition.</p><p>We derive our model from <ref type="bibr">Teh (2006)</ref>, and pro- pose an extension with skipgrams. A frequentist approach to language modelling with skipgrams is described by <ref type="bibr" target="#b7">Pickhardt et al. (2014)</ref>, who intro- duce an approach using skip-n-grams which are interpolated using modified Kneser-Ney smooth- ing. In this paper we show that a Bayesian skip-n- gram approach outperforms a frequentist skip-n- gram model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Traditionally, the most widely used pattern in lan- guage modelling is the n-gram, which represents a pattern of n contiguous words, of which we call the first (n − 1) words the history or context, and the nth word the focus word. The motivation for using n-grams can be traced back to the distribu- tional hypothesis of Harris <ref type="bibr" target="#b3">(Harris, 1954;</ref><ref type="bibr" target="#b8">Sahlgren, 2008</ref>). Although n-grams are small patterns without any explicit linguistic annotation, they are surprisingly effective in many tasks, such as lan- guage modelling in machine translation, automatic speech recognition, and information retrieval.</p><p>One of the main limitations of n-grams is their contiguity, because this limits the express- ive power to relations between neighboring words. Many patterns in language span a range that is longer than the typical length of n; we call these relations long-distance relations. Other patterns may be within the range of n, but are still non- contiguous; they skip over positions. Both types of relations may be modelled with (syntactic) dependencies, and modelling these explicitly re- quires a method to derive a parser, e.g. a depend- ency parser, from linguistically annotated data.</p><p>To be able to model long-distance and other non-contiguous relations between words without resorting to explicitly computing syntactic de- pendencies, we use skipgrams. Skipgrams are a generalisation of n-grams. They consist of n tokens, but now each token may represent a skip of at least one word, where a skip can match any word. Let {m} be a skip of length m, then the {1} house can match "the big house", or "the yellow house", etc. We do not allow skips to be at the beginning or end of the skipgram, so for n &gt; 2 skipgrams are a generalisation of n-grams <ref type="bibr" target="#b2">(Goodman, 2001;</ref><ref type="bibr" target="#b9">Shazeer et al., 2015;</ref><ref type="bibr" target="#b7">Pickhardt et al., 2014</ref>).</p><p>Pitman-Yor Processes (PYP) belong to the fam- ily of non-parametric Bayesian models. Let W be a fixed and finite vocabulary of V words. For each word w ∈ W let G(w) be the probability of w, and G = [G(w)] w∈W be the vector of word probabil- ities. Since word frequencies generally follow a power-law distribution, we use a Pitman-Yor pro- cess, which is a distribution over partitions with power-law distributions. In the context of a lan- guage model this means that for a space P (u), with c(u·) elements (tokens), we want to parti- tion P (u) in V subsets such that the partition is a good approximation of the underlying data, in which c(uw) is the size of subset w of P (u). We assume that the training data is an sample of the underlying data, and for this reason we seek to find an approximation, rather than using the partitions precisely as found in the training data.</p><p>Since we also assume that a power-law distribu- tion on the words in the underlying data, we place a PYP prior on G:</p><formula xml:id="formula_0">G ∼ PY(d, θ, G 0 ), with discount parameter 0 ≤ d &lt; 1, a strength parameter θ &gt; −d and a mean vector G 0 = [G 0 (w)] w∈W . G 0 (w)</formula><p>is the a-priori probability of word w, which we set uniformly: G 0 (w) = 1/V for all w ∈ W . In general, there is no known ana- lytic form for the density of PY(d, θ, G 0 ) when the vocabulary is finite. However, we are inter- ested in the distribution over word sequences in- duced by the PYP, which has a tractable form, and is sufficient for the purpose of language modelling.</p><p>Let G and G 0 be distributions over W , and x 1 , x 2 , . . . be a sequence of words drawn i.i.d. from G. The PYP is then described in terms of a generative procedure that takes x 1 , x 2 , . . . to pro- duce a separate sequence of i.i.d. draws y 1 , y 2 , . . . from the mean distribution G 0 as follows. The first word x 1 is assigned the value of the first draw y 1 from G 0 . Let t be the current number of draws from G 0 , c k the number of words assigned the value of draw y k and c · = t k=1 c k the number of draws from G 0 . For each subsequent word x c·+1 , we either assign it the value of a previous draw y k , with probability c k −d θ+c· , or assign it the value of a new draw from G 0 with probability θ+dt θ+c· . For an n-gram language model we use a hier- archical extension of the PYP. The hierarchical extension describes the probabilities over the cur- rent word given various contexts consisting of up to n − 1 words. Given a context u, let G u (w) be the probability of the current word taking on value w. A PYP is used as the prior for</p><formula xml:id="formula_1">G u = [G u (w)] w∈W : G u ∼ PY(d |u| , θ |u| , G π(u) ),</formula><p>where π(u) is the suffix of u consisting of all but the first word, and |u| being the length of u. The priors are recursively placed with parameters θ |π(u)| , d |π(u)| and mean vector G π(π(u)) , until we get to G ∅ :</p><formula xml:id="formula_2">G ∅ ∼ PY(d 0 , θ 0 , G 0 ),</formula><p>with G 0 being the uniformly distributed global mean vector for the empty context ∅.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Backoff Strategies</head><p>In this paper we investigate three backoff strategies: ngram, limited, and full. ngram is the traditional n-gram backoff method as described by <ref type="bibr">Teh (2006)</ref>; limited and full are extensions that also incorporate skipgram probabilities. The full backoff strategy is similar to ngram in that it al- ways backs off recursively to the word probabilit- ies, while limited halts as soon as a probability is known for a pattern. The backoff strategies can be formalised as follows. For all strategies, we have that p(w|u) = G 0 (w) if u = ∅. For ngram, the other case is defined as:</p><formula xml:id="formula_3">p(w|u) = c uw· − d |u| t uw· θ |u| + c u·· + θ |u| + d |u| t u·· θ |u| + c u·· p(w|π(u))</formula><p>with c uw· being the number of uw tokens, and c u·· the number of patterns starting with context u. Similarly, t uwk is 1 if draw the kth from G u was w, 0 otherwise. t uw· then denotes if there is a pattern uw, and t u·· is the number of types follow- ing context u. Now let σ n be the operator that adds a skip to a pattern u on the nth position if there is not already a skip. Then σ(u) = [σ n (u)] |u| n=2 is the set of patterns with one skip more than the num- ber of skips currently in u. The number of gen- erated patterns is ς = |σ(u)|. We also introduce the indicator function S, which for the full backoff strategy always returns its argument: S uw (y) = y. The full backoff strategy is defined as follows, with u x = σ x (u), and discount frequency δ u = 1:</p><formula xml:id="formula_4">p(w|u) = ς m=1 1 ς + 1 c umw· − δ um d |um| t umw· δ um θ |um| + c um·· + S umw θ |um| + d |um| t um·· δ um θ |um| + c um·· p(w|π(u m )) + 1 ς + 1 c uw· − δ u d |u| t uw· δ u θ |u| + c u·· + S uw θ |u| + d |u| t u·· δ u θ |u| + c u·· p(w|π(u))</formula><p>The limited backoff strategy is an extension of the full backoff strategy that stops the recursion if a test pattern uw has already occurred in the train- ing data. This means that the count is not zero, and hence at training time a probability has been assigned to that pattern. S is the indicator function which tells if a pattern has been seen during train- ing: S uw (·) = 0 if count(uw) &gt; 0, 1 otherwise; and δ u = V − w∈W S uw (·). Setting S uw (·) = 0 stops the recursion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Data</head><p>In this section we give an overview of the data sets we use for the English and Flemish-Dutch experi- ments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">English Data</head><p>For the experiments on English we use four cor- pora: two large generic mixed-domain corpora and two smaller domain-specific corpora. We train on the largest of the two mixed-domain corpora, and test on all four corpora.</p><p>The first generic corpus is the Google 1 billion words shuffled web corpus of 769 million tokens ( <ref type="bibr" target="#b1">Chelba et al., 2013</ref>). For training we use sets 1 through 100, out of the 101 available training sets; for testing we use all available 50 test sets (8M tokens). The second generic corpus, used as test data, is a Wikipedia snapshot (368M tokens) of November 2013 as used and provided by <ref type="bibr" target="#b7">Pickhardt et al. (2014)</ref>. The first domain-specific corpus is from JRC-Acquis v3.0 ( <ref type="bibr" target="#b10">Steinberger et al., 2006</ref>), which contains legislative text of the European Union (8M tokens). The second domain-specific corpus consists of documents from the European Medicines Agency, EMEA <ref type="bibr">(Tiedemann, 2009)</ref>. We shuffled all sentences, and selected 20% of them as the test set (3M tokens).</p><p>Since the HPYLM uses a substantial amount of memory, even with histogram-based sampling, we cannot model the complete 1bw data set without thresholding the patterns in the model. We used a high occurrence threshold of 100 on the uni- grams, yielding 99,553 types that occur above this threshold. We use all n-grams and skipgrams that occurred at least twice, consisting of the included unigrams as focus words, with UNKs occupying the positions of words not in the vocabulary. Note that because these settings are different from mod- els competing on this benchmark, the results in this paper cannot be compared to those results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Flemish-Dutch Data</head><p>For the experiments on Flemish-Dutch data, we use the Mediargus corpus as training material. It contains 5 years of newspaper texts from 12 Flem- ish newspapers and magazines, totaling 1.3 billion words.</p><p>For testing we use the Flemish part of the Spoken Dutch Corpus (CGN) <ref type="bibr" target="#b6">(Oostdijk, 2000</ref>) (3.2M words), divided over 15 components, ran- ging from spontaneous speech to books read aloud. CGN also contains two components which are news articles and news, which from a domain perspective are similar to the training data of Me- diargus. We report on each component separately.</p><p>Similarly to the 1bw models, we used a thres- hold on the word types, such that we have a sim- ilar size of vocabulary (100k types), which we pro- duced with a threshold of 250. We used the same occurrence threshold of 2 on the n-and skipgrams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head><p>We train 4-gram language model on the two train- ing corpora, the Google 1 billion word benchmark and the Mediargus corpus. We do not perform any preprocessing on the data except tokenisation. The models are trained with a HPYLM. We do not use sentence beginning and end markers. The res- ults for the ngram backoff strategy are obtained by training without skipgrams; for limited and full we added skipgram features during training.</p><p>At the core of our experimental framework we use cpyp, <ref type="bibr">1</ref> which is an existing library for non- parametric Bayesian modelling with PY priors with histogram-based sampling <ref type="bibr" target="#b0">(Blunsom et al., 2009)</ref>. This library has an example application to showcase its performance with n-gram based lan- guage modelling. Limitations of the library, such as not natively supporting skipgrams, and the lack of other functionality such as thresholding and dis- carding of certain patterns, led us to extend the lib- rary with Colibri Core, 2 a pattern modelling lib- rary. Colibri Core resolves the limitations, and to- gether the libraries are a complete language model that handles skipgrams: cococpyp. <ref type="bibr">3</ref> Each model is run for 50 iterations (without an explicit burn-in phase), with hyperparameters θ = 1.0 and γ = 0.8. The hyperparameters are resampled every 30 iterations with slice sampling <ref type="bibr">(Walker, 2007)</ref>. We test each model on different test sets, and we collect their intrinsic perform- ance by means of perplexity. Words in the test set <ref type="table">Table 2</ref>: Results of the full and limited backoff systems, trained on Mediargus, tested on CGN. Components range from spontaneous (a) to non- spontaneous (o), with components j (news reports) and k (news) being in-domain for the training corpus, and the other components being out-of- domain. ↓% is the relative reduction in perplexity for the column to its left. that were unseen in the training data are ignored in computing the perplexity on test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>The results are reported in terms of perplexity, in <ref type="table">Table 1</ref> for English, and in <ref type="table">Table 2</ref> for Flemish- Dutch. We computed baseline perplexity scores with SRILM <ref type="bibr" target="#b11">(Stolcke, 2002</ref>) for 1bw. We used an interpolated modified Kneser-Ney language model, with Good-Turing discounting to mimic our thresholding options. Although the models are not comparable, this is arguably the closest ap- proximation in SRILM of our HPYLM. For 1bw the baseline is 147; for jrc, emea, and wp, 1391, 1430, and 1403 respectively. In some cases the baseline is better compared to the ngram back- off strategy. With adding skipgrams we always outperform the baseline, especially on the out-of- domain test sets.</p><p>We find that with large data sets adding skip- grams lowers the perplexity, for both languages, in both within-and cross-domain experiments. For English, we observe absolute perplexity reduc- tions up to 680 (a relative reduction of 39%) in a cross-domain setting, and absolute perplexity reductions of 10 (relative reduction of 6%) in a within-domain setting. For Flemish-Dutch we ob- serve similar results with absolute reductions up to 560 (relative reduction of 36%) and 31 (relative reduction 11%), respectively.</p><p>If we consider the three backoff strategies in- dividually, we can see the following effects on both English and Flemish-Dutch data. In a within- domain experiment limited backoff is the best strategy. In a cross-domain setting, the full back- off strategy yields the lowest perplexity and largest perplexity reductions. In the first case, stopping the backoff when there is a pattern probability for the word and its context yields a more certain probability than when the probability is diffused by more uncertain backoff probabilities.</p><p>Upon inspection of the model sizes, we observe that the skipgram model contains almost five times as many parameters as the n-gram model. This difference is explained by the addition of skip- grams of length 3 and 4, and the bigrams and unigrams derived from these skipgrams. Each 4- gram can be deconstructed into three skipgrams of length 4, and one of these skipgrams yields a skipgram of length 3. Tests with ngram backoff on skipgram models show that the performance is worse compared to ngram backoff in pure n-gram models because of the extra bigrams and unigrams (ngram ignores the skipgrams). Yet, the exper- imental results also indicate that with sufficient data, skipgram models outperform n-gram mod- els. Because the difference in parameters is only noticeable in terms of memory, and it hardly im- pacts the run-time, this makes the skipgram model the favourable model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>In this paper we showed that by adding skipgrams, a straightforward but powerful generalisation of n- gram word patterns, we can reduce the perplex- ity of a Bayesian language model, especially in a cross-domain language modelling task. By chan- ging the backoff strategy we can also improve on a within-domain task. We found this effect in two languages. <ref type="bibr">YW Teh. 2006</ref>. A hierarchical Bayesian language model based on Pitman-Yor processes. In Proceed- ings of the 21st International Conference on Com- putational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 985-992. Association for Computational Lin- guistics. </p></div>
			<note place="foot" n="1"> https://github.com/redpony/cpyp 2 http://proycon.github.io/</note>
		</body>
		<back>
			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A note on the implementation of hierarchical Dirichlet processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACLIJCNLP 2009 Conference Short Papers</title>
		<meeting>the ACLIJCNLP 2009 Conference Short Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="337" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">One billion word benchmark for measuring progress in statistical language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brants</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robinson</surname></persName>
		</author>
		<idno>Google Tech Re- port 41880</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A bit of progress in language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="403" to="434" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Distributional structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">S</forename><surname>Harris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improved backing-off for m-gram language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kneser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="1995-05" />
			<biblScope unit="volume">I</biblScope>
			<biblScope unit="page" from="181" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A hierarchical Dirichlet language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djc</forename><surname>Mackay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lcb</forename><surname>Peto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural language engineering</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="289" to="308" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The spoken Dutch corpus. Overview and first evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Oostdijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A generalized language model as the combination of skipped n-grams and modified Kneser-Ney smoothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pickhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gottron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">G</forename><surname>Körner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Speicher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Staab</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.3377</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The distributional hypothesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sahlgren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Italian Journal of Linguistics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="54" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sparse non-negative matrix language modeling for skipgrams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pelemans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chelba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1428" to="1432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The JRCAcquis: A multilingual aligned parallel corpus with 20+ languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Steinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pouliquen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Widiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ignat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Erjavec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tufis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Varga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Language Resources and Evaluation</title>
		<meeting>the 5th International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SRILM-an extensible language modeling toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings International Conference on Spoken Language Processing</title>
		<meeting>International Conference on Spoken Language Processing</meeting>
		<imprint>
			<date type="published" when="2002-11" />
			<biblScope unit="page" from="257" to="286" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
