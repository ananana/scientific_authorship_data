<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:52+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Weakly Supervised Cross-Lingual Named Entity Recognition via Effective Annotation and Representation Projection</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ni</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
						</author>
						<title level="a" type="main">Weakly Supervised Cross-Lingual Named Entity Recognition via Effective Annotation and Representation Projection</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1470" to="1480"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1135</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The state-of-the-art named entity recognition (NER) systems are supervised machine learning models that require large amounts of manually annotated data to achieve high accuracy. However, annotating NER data by human is expensive and time-consuming, and can be quite difficult for a new language. In this paper , we present two weakly supervised approaches for cross-lingual NER with no human annotation in a target language. The first approach is to create automatically labeled NER data for a target language via annotation projection on comparable corpora, where we develop a heuris-tic scheme that effectively selects good-quality projection-labeled data from noisy data. The second approach is to project distributed representations of words (word embeddings) from a target language to a source language, so that the source-language NER system can be applied to the target language without retraining. We also design two co-decoding schemes that effectively combine the outputs of the two projection-based approaches. We evaluate the performance of the proposed approaches on both in-house and open NER data for several target languages. The results show that the combined systems outperform three other weakly supervised approaches on the CoNLL data.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Named entity recognition (NER) is a fundamen- tal information extraction task that automatically detects named entities in text and classifies them into pre-defined entity types such as PERSON, ORGANIZATION, GPE (GeoPolitical Entities), EVENT, LOCATION, TIME, DATE, etc. NER provides essential inputs for many information ex- traction applications, including relation extraction, entity linking, question answering and text min- ing. Building fast and accurate NER systems is a crucial step towards enabling large-scale auto- mated information extraction and knowledge dis- covery on the huge volumes of electronic docu- ments existing today.</p><p>The state-of-the-art NER systems are super- vised machine learning models ( <ref type="bibr" target="#b16">Nadeau and Sekine, 2007)</ref>, including maximum entropy Markov models (MEMMs) ( <ref type="bibr" target="#b13">McCallum et al., 2000</ref>), conditional random fields (CRFs) ( <ref type="bibr" target="#b10">Lafferty et al., 2001</ref>) and neural networks <ref type="bibr" target="#b1">(Collobert et al., 2011;</ref><ref type="bibr" target="#b11">Lample et al., 2016)</ref>. To achieve high ac- curacy, a NER system needs to be trained with a large amount of manually annotated data, and is often supplied with language-specific resources (e.g., gazetteers, word clusters, etc.). Annotating NER data by human is rather expensive and time- consuming, and can be quite difficult for a new language. This creates a big challenge in building NER systems of multiple languages for supporting multilingual information extraction applications.</p><p>The difficulty of acquiring supervised annota- tion raises the following question: given a well- trained NER system in a source language (e.g., English), how can one go about extending it to a new language with decent performance and no human annotation in the target language? There are mainly two types of approaches for building weakly supervised cross-lingual NER systems.</p><p>The first type of approaches create weakly la- beled NER training data in a target language. One way to create weakly labeled data is through an- notation projection on aligned parallel corpora or translations between a source language and a tar- get language, e.g., <ref type="bibr" target="#b31">(Yarowsky et al., 2001;</ref><ref type="bibr" target="#b32">Zitouni and Florian, 2008;</ref><ref type="bibr" target="#b3">Ehrmann et al., 2011</ref>). An- other way is to utilize the text and structure of Wikipedia to generate weakly labeled multilingual training annotations, e.g., <ref type="bibr" target="#b23">(Richman and Schone, 2008;</ref><ref type="bibr" target="#b20">Nothman et al., 2013;</ref><ref type="bibr" target="#b0">Al-Rfou et al., 2015)</ref>.</p><p>The second type of approaches are based on di- rect model transfer, e.g., <ref type="bibr" target="#b24">(Täckström et al., 2012;</ref>. The basic idea is to train a single NER system in the source language with language independent features, so the system can be applied to other languages using those universal features.</p><p>In this paper, we make the following contri- butions to weakly supervised cross-lingual NER with no human annotation in the target languages. First, for the annotation projection approach, we develop a heuristic, language-independent data se- lection scheme that seeks to select good-quality projection-labeled NER data from comparable corpora. Experimental results show that the data selection scheme can significantly improve the ac- curacy of the target-language NER system when the alignment quality is low and the projection- labeled data are noisy.</p><p>Second, we propose a new approach for direct NER model transfer based on representation pro- jection. It projects word representations in vector space (word embeddings) from a target language to a source language, to create a universal repre- sentation of the words in different languages. Un- der this approach, the NER system trained for the source language can be directly applied to the tar- get language without the need for re-training.</p><p>Finally, we design two co-decoding schemes that combine the outputs (views) of the two projection-based systems to produce an output that is more accurate than the outputs of individual sys- tems. We evaluate the performance of the pro- posed approaches on both in-house and open NER data sets for a number of target languages. The re- sults show that the combined systems outperform the state-of-the-art cross-lingual NER approaches proposed in <ref type="bibr">Täckström et al. (2012)</ref>, <ref type="bibr" target="#b20">Nothman et al. (2013)</ref> and  on the CoNLL NER test data <ref type="bibr" target="#b25">(Tjong Kim Sang, 2002;</ref><ref type="bibr" target="#b26">Tjong Kim Sang and De Meulder, 2003)</ref>.</p><p>We organize the paper as follows. In Section 2 we introduce three NER models that are used in the paper. In Section 3 we present an anno- tation projection approach with effective data se- lection. In Section 4 we propose a representation projection approach for direct NER model trans- fer. In Section 5 we describe two co-decoding schemes that effectively combine the outputs of two projection-based approaches. In Section 6 we evaluate the performance of the proposed ap- proaches. We describe related work in Section 7 and conclude the paper in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">NER Models</head><p>The NER task can be formulated as a sequence labeling problem: given a sequence of words x 1 , ..., x n , we want to infer the NER tag l i for each word x i , 1 ≤ i ≤ n. In this section we introduce three NER models that are used in the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">CRFs and MEMMs</head><p>Conditional random fields (CRFs) are a class of discriminative probabilistic graphical models that provide powerful tools for labeling sequential data ( <ref type="bibr" target="#b10">Lafferty et al., 2001</ref>). CRFs learn a conditional probability model p λ (l|x) from a set of labeled training data, where x = (x 1 , ..., x n ) is a random sequence of input words, l = (l 1 , ..., l n ) is the se- quence of label variables (NER tags) for x, and l has certain Markov properties conditioned on x. Specifically, a general-order CRF with order o as- sumes that label variable l i is dependent on a fixed number o of previous label variables l i−1 , ..., l i−o , with the following conditional distribution:</p><formula xml:id="formula_0">p λ (l|x) = e n i=1 K k=1 λ k f k (l i ,l i−1 ,...,l i−o ,x) Z λ (x)<label>(1)</label></formula><p>where f k 's are feature functions, λ k 's are weights of the feature functions (parameters to learn), and Z λ (x) is a normalization constant. When o = 1, we have a first-order CRF which is also known as a linear-chain CRF. Given a set of labeled training data D = (x (j) , l (j) ) j=1,...,N , we seek to find an optimal set of parameters λ * that maximize the conditional log-likelihood of the data:</p><formula xml:id="formula_1">λ * = arg max λ N j=1 log p λ (l (j) |x (j) )<label>(2)</label></formula><p>Once we obtain λ * , we can use the trained model p λ * (l|x) to decode the most likely label sequence l * for any new input sequence of words x (via the Viterbi algorithm for example):</p><formula xml:id="formula_2">l * = arg max l p λ * (l|x)<label>(3)</label></formula><p>A related conditional probability model, called maximum entropy Markov model (MEMM) <ref type="bibr" target="#b13">(McCallum et al., 2000</ref>), assumes that l is a Markov chain conditioned on x:</p><formula xml:id="formula_3">p λ (l|x) = n i=1 p λ (l i |l i−1 , ..., l i−o , x) = n i=1 e K k=1 λ k f k (l i ,l i−1 ,...,l i−o ,x) Z λ (l i−1 , ..., l i−o , x)<label>(4)</label></formula><p>The main difference between CRFs and MEMMs is that CRFs normalize the conditional distribution over the whole sequence as in <ref type="formula" target="#formula_0">(1)</ref>, while MEMMs normalize the conditional distribu- tion per token as in <ref type="formula" target="#formula_3">(4)</ref>. As a result, CRFs can bet- ter handle the label bias problem ( <ref type="bibr" target="#b10">Lafferty et al., 2001</ref>). This benefit, however, comes at a price. The training time of order-o CRFs grows exponen- tially (O(M o+1 )) with the number of output labels M , which is typically slow even for moderate-size training data if M is large. In contrast, the training time of order-o MEMMs is linear (O(M )) with respect to M independent of o, so it can handle larger training data with higher order of depen- dency. We have implemented both a linear-chain CRF model and a general-order MEMM model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Neural Networks</head><p>With the increasing popularity of distributed (vec- tor) representations of words, neural network models have recently been applied to tackle many NLP tasks including NER <ref type="bibr" target="#b1">(Collobert et al., 2011;</ref><ref type="bibr" target="#b11">Lample et al., 2016)</ref>.</p><p>We have implemented a feedforward neural net- work model which maximizes the log-likelihood of the training data similar to that of <ref type="bibr" target="#b1">(Collobert et al., 2011</ref>). We adopt a locally normalized model (the conditional distribution is normalized per to- ken as in MEMMs) and introduce context depen- dency by conditioning on the previously assigned tags. We use a target word and its surrounding context as features. We do not use other common features such as gazetteers or character-level rep- resentations as such features might not be readily available or might not transfer to other languages.</p><p>We have deployed two neural network architec- tures. The first one (called NN1) uses the word embedding of a word as the input. The sec- ond one (called NN2) adds a smoothing proto- type layer that computes the cosine similarity be- tween a word embedding and a fixed set of proto- type vectors (learned during training) and returns a weighted average of these prototype vectors as the input. In our experiments we find that with the smoothing layer, NN2 tends to have a more bal- anced precision and recall than NN1. Both net- works have one hidden layer, with sigmoid and softmax activation functions on the hidden and output layers respectively. The two neural network models are depicted in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Annotation Projection Approach</head><p>The existing annotation projection approaches re- quire parallel corpora or translations between a source language and a target language with alignment information. In this paper, we de- velop a heuristic, language-independent data se- lection scheme that seeks to select good-quality projection-labeled data from noisy comparable corpora. We use English as the source language.</p><p>Suppose we have comparable 1 sentence pairs (X, Y) between English and a target lan- guage, where X includes N English sentences x (1) , ..., x (N ) , Y includes N target-language sentences y (1) , ..., y (N ) , and y (j) is aligned to x (j) via an alignment model, 1 ≤ j ≤ N . We use a sentence pair (x, y) as an example to illustrate how the annotation projection procedure works, where x = (x 1 , x 2 , ..., x s ) is an English sentence, and y = (y 1 , y 2 , ..., y t ) is a target-language sentence that is aligned to x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Annotation Projection Procedure</head><p>1. Apply the English NER system on the En- glish sentence x to generate the NER tags</p><formula xml:id="formula_4">l = (l 1 , l 2 , ..., l s ) for x.</formula><p>2. Project the NER tags to the target-language sentence y using the alignment informa- tion. Specifically, if a sequence of English words (x i , ..., x i+p ) is aligned to a sequence of target-language words (y j , ..., y j+q ), and (x i , ..., x i+p ) is recognized (by the English NER system) as an entity with NER tag l,</p><formula xml:id="formula_5">then (y j , ..., y j+q ) is labeled with l 2 . Let l = (l 1 , l 2 , .</formula><p>.., l t ) be the projected NER tags for the target-language sentence y.</p><p>We can apply the annotation projection proce- dure on all the sentence pairs (X, Y), to generate projected NER tags L for the target-language sen- tences Y. (Y, L ) are automatically labeled NER data with no human annotation in the target lan- guage. One can use those projection-labeled data to train an NER system in the target language. The quality of such weakly labeled NER data, and con- sequently the accuracy of the target-language NER system, depend on both 1) the accuracy of the En- glish NER system, and 2) the alignment accuracy of the sentence pairs.</p><p>Since we don't require actual translations, but only comparable data, the downside is that if some of the data are not actually parallel and if we use all for weakly supervised learning, the accuracy of the target-language NER system might be ad- versely affected. We are therefore motivated to design effective data selection schemes that can select good-quality projection-labeled data from noisy data, to improve the accuracy of the anno- tation projection approach for cross-lingual NER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Selection Scheme</head><p>We first design a metric to measure the annotation quality of a projection-labeled sentence in the tar- get language. We construct a frequency table T which includes all the entities in the projection- labeled target-language sentences. For each entity e, T also includes the projected NER tags for e and the relative frequency (empirical probability) ˆ P (l|e) that entity e is labeled with tag l. <ref type="table">Table 1</ref> shows a snapshot of the frequency table where the target language is Portuguese.</p><p>We usê P (l|e) to measure the reliability of la- beling entity e with tag l in the target language. The intuition is that if an entity e is labeled by a tag l with higher frequency than other tags in the projection-labeled data, it is more likely that the annotation is correct. For example, if the joint ac- curacy of the source NER system and alignment system is greater than 0.5, then the correct tag of a random entity will have a higher relative fre- quency than other tags in a large enough sample.</p><p>Based on the frequency scores, we calculate the quality score of a projection-labeled target- 2 If the IOB (Inside, Outside, Beginning) tagging format is used, then (yj, yj+1, ..., yj+q) is labeled with (B-l, I-l,...,I-l). language sentence y by averaging the frequency scores of the projected entities in the sentence:</p><formula xml:id="formula_6">q(y) = Σ e∈yˆPe∈yˆ e∈yˆP (l (e)|e) n(y)<label>(5)</label></formula><p>where l (e) is the projected NER tag for e, and n(y) is the total number of entities in sentence y.</p><p>We use q(y) to measure the annotation quality of sentence y, and n(y) to measure the amount of annotation information contained in sentence y. We design a heuristic data selection scheme which selects projection-labeled sentences in the target language that satisfy the following condition:</p><formula xml:id="formula_7">q(y) ≥ q; n(y) ≥ n (6)</formula><p>where q is a quality score threshold and n is an entity number threshold. We can tune the two pa- rameters to make tradeoffs among the annotation quality of the selected sentences, the annotation information contained in the selected sentences, and the total number of sentence selected. One way to select the threshold parameters q and n is via a development set -either a small set of human-annotated data or a sample of the projection-labeled data. We select the threshold parameters via coordinate search using the devel- opment set: we first fix n = 3 and search the bestˆqbestˆ bestˆq in [0, 0.9] with a step size of 0.1; we then fix q = ˆ q and select the bestˆnbestˆ bestˆn in <ref type="bibr">[1,</ref><ref type="bibr">5]</ref> with a step size of 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Accuracy Improvements</head><p>We evaluate the effectiveness of the data selection scheme via experiments on 4 target languages: Japanese, Korean, German and Portuguese. We use comparable corpora between English and each target language (ranging from 2M to 6M tokens) with alignment information. For each target lan- guage, we also have a set of manually anno- tated NER data (ranging from 30K to 45K tokens)</p><formula xml:id="formula_8">Language (q, n) Training Size F1 Score Japanese (0, 0) 4.9M 41.2 (0.7, 4) 1.3M 53.4 Korean (0, 0) 4.5M 25.0 (0.4, 2) 1.5M 38.7 German (0, 0) 5.2M 67.2 (0.4, 4)</formula><p>2.6M 67.5 Portuguese (0, 0) 2.1M 61.5 (0.1, 4)</p><p>1.5M 62.7 <ref type="table">Table 2</ref>: Performance comparison of weakly su- pervised NER systems trained without data se- lection ((q, n) = (0, 0)) and with data selection ((ˆ q, ˆ n) determined by coordinate search).</p><p>which are served as the test data for evaluating the target-language NER system. The source (English) NER system is a linear- chain CRF model which achieves an accuracy of 88.9 F 1 score on an independent NER test set. The alignment systems between English and the target languages are maximum entropy models <ref type="bibr" target="#b8">(Ittycheriah and Roukos, 2005)</ref>, with an accu- racy of 69.4/62.0/76.1/88.0 F 1 score on indepen- dent Japanese/Korean/German/Portuguese align- ment test sets.</p><p>For each target language, we randomly select 5% of the projection-labeled data as the develop- ment set and the remaining 95% as the training set. We compare an NER system trained with all the projection-labeled training data with no data selection (i.e., (q, n) = (0, 0)) and an NER sys- tem trained with projection-labeled data selected by the data selection scheme where the develop- ment set is used to select the threshold parame- ters q and n via coordinate search. Both NER sys- tems are 2nd-order MEMM models 3 which use the same template of features.</p><p>The results are shown in <ref type="table">Table 2</ref>. For differ- ent target languages, we use the same source (En- glish) NER system for annotation projection, so the differences in the accuracy improvements are mainly due to the alignment quality of the com- parable corpora between English and different tar- get languages. When the alignment quality is low (e.g., as for Japanese and Korean) and hence the projection-labeled NER data are quite noisy, the proposed data selection scheme is very effective in selecting good-quality projection-labeled data and the improvement is big: +12.2 F 1 score for Japanese and +13.7 F 1 score for Korean. Us- ing a stratified shuffling test <ref type="bibr" target="#b19">(Noreen, 1989)</ref>, for a significance level of 0.05, data-selection is sta- tistically significantly better than no-selection for Japanese, Korean and Portuguese.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Representation Projection Approach</head><p>In this paper, we propose a new approach for di- rect NER model transfer based on representation projection. Under this approach, we train a single English NER system that uses only word embed- dings as input representations. We create mapping functions which can map words in any language into English and we simply use the English NER system to decode. In particular, by mapping all languages into English, we are using one univer- sal NER system and we do not need to re-train the system when a new language is added.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Monolingual Word Embeddings</head><p>We first build vector representations of words (word embeddings) for a language using mono- lingual data. We use a variant of the Con- tinuous Bag-of-Words (CBOW) word2vec model ( <ref type="bibr" target="#b14">Mikolov et al., 2013a</ref>), which concatenates the context words surrounding a target word instead of adding them (similarly to ( <ref type="bibr" target="#b12">Ling et al., 2015)</ref>). Additionally, we employ weights w = 1 dist <ref type="bibr">(x,xc)</ref> that decay with the distance of a context word x c to a target word x. Tests on word similarity bench- marks show this variant leads to small improve- ments over the standard CBOW model. We train 300-dimensional word embeddings for English. <ref type="bibr">Following (Mikolov et al., 2013b</ref>), we use larger dimensional embeddings for the target languages, namely 800. We train word2vec for 1 epoch for English/Spanish and 5 epochs for the rest of the languages for which we have less data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Cross-Lingual Representation Projection</head><p>We learn cross-lingual word embedding map- pings, similarly to ( <ref type="bibr" target="#b15">Mikolov et al., 2013b</ref>). For a target language f , we first extract a small train- ing dictionary from a phrase table that includes word-to-word alignments between English and the target language f . The dictionary contains En- glish and target-language word pairs with weights: (x i , y i , w i ) i=1,...,n , where x i is an English word, y i is a target-language word, and the weight w i = ˆ P (x i |y i ) is the relative frequency of x i given y i as extracted from the phrase table.</p><p>Suppose we have monolingual word embed- dings for English and the target language f . Let u i ∈ R d 1 be the vector representation for English word x i , v i ∈ R d 2 be the vector representation for target-language word y i . We find a linear mapping M f →e by solving the following weighted least squares problem where the dictionary is used as the training data:</p><formula xml:id="formula_9">M f →e = arg min M n i=1 w i ||u i − Mv i || 2<label>(7)</label></formula><p>In <ref type="formula" target="#formula_9">(7)</ref> we generalize the formulation in ( <ref type="bibr" target="#b15">Mikolov et al., 2013b</ref>) by adding frequency weights to the word pairs, so that more frequent pairs are of higher importance. Using M f →e , for any new word in f with vector representation v, we can project it into the English vector space as the vector M f →e v.</p><p>The training dictionary plays a key role in find- ing an effective cross-lingual embedding mapping. To control the size of the dictionary, we only include word pairs with a minimum frequency threshold. We set the threshold to obtain approx- imately 5K to 6K unique word pairs for a target language, as our experiments show that larger-size dictionaries might harm the performance of repre- sentation projection for direct NER model transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Direct NER Model Transfer</head><p>The source (English) NER system is a neural net- work model (with architecture NN1 or NN2) that uses only word embedding features (embeddings of a word and its surrounding context) in the En- glish vector space. Model transfer is achieved sim- ply by projecting the target language word embed- dings into the English vector space and decoding these using the English NER system.</p><p>More specifically, given the word embeddings of a sequence of words in a target language f , (v 1 , ..., v t ), we project them into the English vec- tor space by applying the linear mapping M f →e : (M f →e v 1 , ..., M f →e v t ). The English NER sys- tem is then applied on the projected input to pro- duce NER tags. Words not in the target-language vocabulary are projected into their English embed- dings if they are found in the English vocabulary, or into an NER-trained UNK vector otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Co-Decoding</head><p>Given two weakly supervised NER systems which are trained with different data using different mod- els (MEMM model for annotation projection and neural network model for representation projec- tion), we would like to design a co-decoding scheme that can combine the outputs (views) of the two systems to produce an output that is more accurate than the outputs of individual systems.</p><p>Since both systems are statistical models and can produce confidence scores (probabilities), a natural co-decoding scheme is to compare the con- fidence scores of the NER tags generated by the two systems and select the tags with higher con- fidences scores. However, confidence scores of two weakly supervised systems may not be di- rectly comparable, especially when comparing O tags with non-O tags (i.e., entity tags). We con- sider an exclude-O confidence-based co-decoding scheme which we find to be more effective empir- ically. It is similar to the pure confidence-based scheme, with the only difference that it always prefers a non-O tag of one system to an O tag of the other system, regardless of their confidence scores.</p><p>In our experiments we find that the annotation projection system tends to have a high precision and low recall, i.e., it detects fewer entities, but for the detected entities the accuracy is high. The representation projection system tends to have a more balanced precision and recall. Based on this observation, we develop the following rank-based co-decoding scheme that gives higher priority to the high-precision annotation projection system:</p><p>1. The combined output includes all the entities detected by the annotation projection system.</p><p>2. It then adds all the entities detected by the representation projection system that do not conflict 4 with entities detected by the annota- tion projection system (to improve recall).</p><p>Note that an entity X detected by the rep- resentation projection system does not conflict with the annotation projection system if the an- notation projection system produces O tags for the entire span of X. For example, suppose the output tag sequence of annotation projection is (B-PER,O,O,O,O), of representation projection is (B-ORG,I-ORG,O,B-LOC,I-LOC), then the com- bined output under the rank-based scheme will be (B-PER,O,O,B-LOC,I-LOC). Japanese P R F1 Annotation-Projection (AP) 69.9 43.2 53.4 Representation-Projection (NN1) 71.5 36.6 48.4 Representation-Projection (NN2) 59.9 42.4 49.7 Co-Decoding (Conf): AP+NN1 65.7 49.5 56.5 Co-Decoding (Rank): AP+NN1</p><p>68.3 51.6 58.8 Co-Decoding (Conf): AP+NN2 59.5 53.3 56.2 Co-Decoding (Rank): AP+NN2 61.6 54.5 57.8 Supervised <ref type="formula" target="#formula_1">(272K)</ref> 84.5 80.9 82.7 Korean P R F1 Annotation-Projection (AP) 69.5 26.8 38.7 Representation-Projection (NN1) 66.1 23.2 34.4 Representation-Projection (NN2) 68.5 43.4 53.1 Co-Decoding (Conf): AP+NN1</p><p>68.2 41.0 51.2 Co-Decoding (Rank): AP+NN1 71.3 42.8 53.5 Co-Decoding (Conf): AP+NN2</p><p>68.9 53.4 60.2 Co-Decoding (Rank): AP+NN2</p><p>70.0 53.3 60.5 Supervised (97K)</p><p>88.2 74.0 80.4 German P R F1 Annotation-Projection (AP) 76.5 60.5 67.5 Representation-Projection (NN1) 69.0 48.8 57.2 Representation-Projection (NN2) 63.7 66.1 64.9 Co-Decoding (Conf): AP+NN1 68.5 61.7 64.9 Co-Decoding (Rank): AP+NN1 72.7 65.0 68.6 Co-Decoding (Conf): AP+NN2 64.7 71.3 67.9 Co-Decoding (Rank): AP+NN2 67.1 72.6 69.7 Supervised (125K) 77.8 68.1 72.6</p><formula xml:id="formula_10">Portuguese P R F1 Annotation-Projection (AP)</formula><p>84.0 50.1 62.7 Representation-Projection (NN1) 70.5 47.6 56.8 Representation-Projection (NN2) 66.0 63.4 64.7 Co-Decoding (Conf): AP+NN1 72.0 55.8 62.9 Co-Decoding (Rank): AP+NN1 77.5 59.7 67.4 Co-Decoding (Conf): AP+NN2</p><p>68.1 67.1 67.6 Co-Decoding (Rank): AP+NN2 70.9 68.3 69.6 Supervised (173K) 79.8 71.9 75.6 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>In this section, we evaluate the performance of the proposed approaches for cross-lingual NER, in- cluding the 2 projection-based approaches and the 2 co-decoding schemes for combining them:</p><p>(1) The annotation projection (AP) approach with heuristic data selection; (2) The representation projection approach (with two neural network architectures NN1 and NN2); (3) The exclude-O confidence-based co-decoding scheme; (4) The rank-based co-decoding scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">NER Data Sets</head><p>We have used various NER data sets for evalu- ation. The first group includes in-house human- annotated newswire NER data for four languages: Japanese, Korean, German and Portuguese, anno- tated with over 50 entity types. The main motiva- tion of deploying such a fine-grained entity type set is to build cognitive question answering appli- cations on top of the NER systems. The entity type set has been engineered to cover many of the fre- quent entity types that are targeted by naturally- phrased questions. The sizes of the test data sets are ranging from 30K to 45K tokens. The second group includes open human- annotated newswire NER data for Spanish, Dutch and German from the CoNLL NER data sets <ref type="bibr" target="#b25">(Tjong Kim Sang, 2002;</ref><ref type="bibr" target="#b26">Tjong Kim Sang and De Meulder, 2003)</ref>. The CoNLL data have 4 en- tity types: PER (persons), ORG (organizations), LOC (locations) and MISC (miscellaneous enti- ties). The sizes of the development/test data sets are ranging from 35K to 70K tokens. The devel- opment data are used for tuning the parameters of learning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Evaluation for In-House NER Data</head><p>In <ref type="table" target="#tab_1">Table 3</ref>, we show the results of different ap- proaches for the in-house NER data. For annota- tion projection, the source (English) NER system is a linear-chain CRF model trained with 328K to- kens of human-annotated English newswire data. The target-language NER systems are 2nd-order MEMM models trained with 1.3M, 1.5M, 2.6M and 1.5M tokens of projection-labeled data for Japanese, Korean, German and Portuguese, re- spectively. The projection-labeled data are se- lected using the heuristic data selection scheme (see <ref type="table">Table 2</ref>). For representation projection, the source (English) NER systems are neural network models with architectures NN1 and NN2 (see <ref type="figure" target="#fig_0">Fig- ure 1</ref>), both trained with 328K tokens of human- annotated English newswire data.</p><p>The results show that the annotation projection (AP) approach has a relatively high precision and low recall. For representation projection, neural network model NN2 (with a smoothing layer) is better than NN1, and NN2 tends to have a more balanced precision and recall. The rank-based co- decoding scheme is more effective for combining the two projection-based approaches. In particu- lar, the rank-based scheme that combines AP and NN2 achieves the highest F 1 score among all the weakly supervised approaches for Korean, Ger- man and Portuguese (second highest F 1 score for Japanese), and it improves over the best of the two</p><formula xml:id="formula_11">Spanish P R F1 Annotation-Projection (AP)</formula><p>65.5 59.1 62.1 Representation-Projection (NN1) 63.9 52.2 57.4 Representation-Projection (NN2) 55.3 51.8 53.5 Co-Decoding (Conf): AP+NN1 64.3 66.8 65.5 Co-Decoding (Rank): AP+NN1 63.7 65.3 64.5 Co-Decoding (Conf): AP+NN2 58.0 63.9 60.8 Co-Decoding (Rank): AP+NN2 60.8 64.5 62.6 Supervised (264K) 81.3 79.8 80.6 Dutch P R F1 Annotation-Projection (AP) 73.3 63.0 67.8 Representation-Projection (NN1) 82.6 47.4 60.3 Representation-Projection (NN2) 66.3 43.5 52.5 Co-Decoding (Conf): AP+NN1 72.3 66.5 69.3 Co-Decoding (Rank): AP+NN1 72.8 65.3 68.8 Co-Decoding (Conf): AP+NN2 65.3 64.7 65.0 Co-Decoding (Rank): AP+NN2 69.7 66.0 67.8 Supervised (199K) 82.9 81.7 82.3</p><formula xml:id="formula_12">German P R F1 Annotation-Projection (AP)</formula><p>71.8 54.7 62.1 Representation-Projection (NN1) 79.4 41.4 54.4 Representation-Projection (NN2) 64.6 42.7 51.4 Co-Decoding (Conf): AP+NN1 70.1 59.5 64.4 Co-Decoding (Rank): AP+NN1 71.0 59.4 64.7 Co-Decoding (Conf): AP+NN2 64.2 59.9 62.0 Co-Decoding (Rank): AP+NN2 66.8 60.6 <ref type="bibr">63.6 Supervised (206K)</ref> 81.2 64.3 71.8 <ref type="table">Table 4</ref>: CoNLL NER development data.</p><p>projection-based systems by 2.2 to 7.4 F 1 score. We also provide the performance of supervised learning where the NER system is trained with human-annotated data in the target language (with size shown in the bracket). While the performance of the weakly supervised systems is not as good as supervised learning, it is important to build weakly supervised systems with decent performance when supervised annotation is unavailable. Even if su- pervised annotation is feasible, the weakly super- vised systems can be used to pre-annotate the data, and we observed that pre-annotation can improve the annotation speed by 40%-60%, which greatly reduces the annotation cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Evaluation for CoNLL NER Data</head><p>For the CoNLL data, the source (English) NER system for annotation projection is a linear- chain CRF model trained with the CoNLL En- glish training data (203K tokens), and the target- language NER systems are 2nd-order MEMM models trained with 1.3M, 7.0M and 1.2M to- kens of projection-labeled data for Spanish, Dutch and German, respectively. The projection-labeled data are selected using the heuristic data selection scheme, where the threshold parameters q and n are determined via coordinate search based on the CoNLL development sets. Compared with no data selection, the data selection scheme improves the annotation projection approach by 2.7/2.0/2.7 F 1 score on the Spanish/Dutch/German development data. In addition to standard NER features such as n-gram word features, word type features, pre- fix and suffix features, the target-language NER systems also use the multilingual Wikipedia en- tity type mappings developed in <ref type="bibr" target="#b18">(Ni and Florian, 2016)</ref> to generate dictionary features and as de- coding constraints, which improve the annotation projection approach by 3.0/5.4/7.9 F 1 score on the Spanish/Dutch/German development data.</p><p>For representation projection, the source (En- glish) NER systems are neural network models (NN1 and NN2) trained with the CoNLL En- glish training data. Compared with the stan- dard CBOW word2vec model, the concatenated variant improves the representation projection ap- proach (NN1) by 8.9/11.4/6.8 F 1 score on the Spanish/Dutch/German development data, as well as by 2.0 F 1 score on English. In addition, the frequency-weighted cross-lingual word em- bedding projection (7) improves the representation projection approach (NN1) by 2.2/6.3/3.7 F 1 score on the Spanish/Dutch/German development data, compared with using uniform weights on the same data. We do observe, however, that using uni- form weights when keeping only the most frequent translation of a word instead of all word pairs above a threshold in the training dictionary, leads to performance similar to that of the frequency- weighted projection.</p><p>In <ref type="table">Table 4</ref> we show the results for the CoNLL development data. For representation projection, NN1 is better than NN2. Both the annotation pro- jection approach and NN1 tend to have a high pre- cision. In this case, the exclude-O confidence- based co-decoding scheme that combines AP and NN1 achieves the highest F 1 score for Spanish and Dutch (second highest F 1 score for German), and improves over the best of the two projection-based systems by 1.5 to 3.4 F 1 score.</p><p>In <ref type="table" target="#tab_3">Table 5</ref> we compare our top systems (confi- dence or rank-based co-decoding of AP and NN1, determined by the development data) with the best results of the cross-lingual NER approaches pro- posed in <ref type="bibr">Täckström et al. (2012)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>The traditional annotation projection approaches ( <ref type="bibr" target="#b31">Yarowsky et al., 2001;</ref><ref type="bibr" target="#b32">Zitouni and Florian, 2008;</ref><ref type="bibr" target="#b3">Ehrmann et al., 2011</ref>) project NER tags across language pairs using parallel corpora or transla- tions. <ref type="bibr" target="#b30">Wang and Manning (2014)</ref> proposed a vari- ant of annotation projection which projects expec- tations of tags and uses them as constraints to train a model based on generalized expectation crite- ria. Annotation projection has also been applied to several other cross-lingual NLP tasks, includ- ing word sense disambiguation ( <ref type="bibr" target="#b2">Diab and Resnik, 2002)</ref>, part-of-speech (POS) tagging ( <ref type="bibr" target="#b31">Yarowsky et al., 2001</ref>) and dependency parsing <ref type="bibr" target="#b22">(Rasooli and Collins, 2015)</ref>. Wikipedia has been exploited to generate weakly labeled multilingual NER training data. The basic idea is to first categorize Wikipedia pages into entity types, either based on manually constructed rules that utilize the category informa- tion of Wikipedia ( <ref type="bibr" target="#b23">Richman and Schone, 2008)</ref> or Freebase attributes (Al-Rfou et al., 2015), or via a classifier trained with manually labeled Wikipedia pages <ref type="bibr" target="#b20">(Nothman et al., 2013)</ref>. Heuristic rules are then developed in these works to automatically la- bel the Wikipedia text with NER tags. <ref type="bibr" target="#b18">Ni and Florian (2016)</ref> built high-accuracy, high-coverage multilingual Wikipedia entity type mappings us- ing weakly labeled data and applied those map- pings as decoding constrains or dictionary features to improve multilingual NER systems.</p><p>For direct NER model transfer, <ref type="bibr">Täckström et al. (2012)</ref> built cross-lingual word clusters using monolingual data in source/target languages and aligned parallel data between source and target languages. The cross-lingual word clusters were then used to generate universal features.  applied the cross-lingual wikifier developed in  and mul- tilingual Wikipedia dump to generate language- independent labels (FreeBase types and Wikipedia categories) for n-grams in a document, and those labels were used as universal features.</p><p>Different ways of obtaining cross-lingual em- beddings have been proposed in the literature. One approach builds monolingual representations sep- arately and then brings them to the same space typically using a seed dictionary ( <ref type="bibr" target="#b15">Mikolov et al., 2013b;</ref><ref type="bibr" target="#b4">Faruqui and Dyer, 2014)</ref>. Another line of work builds inter-lingual representations simulta- neously, often by generating mixed language cor- pora using the supervision at hand (aligned sen- tences, documents, etc.) <ref type="bibr" target="#b29">(Vuli´cVuli´c and Moens, 2015;</ref>. We opt for the first solution in this paper because of its flexibility: we can map all languages to English rather than requiring separate embeddings for each language pair. Additionally we are able to easily add a new language without any constraints on the type of data needed. Note that although we do not specifically create inter- lingual representations, by training mappings to the common language, English, we are able to map words in different languages to a common space. Similar approaches for cross-lingual model trans- fer have been applied to other NLP tasks such as document classification ( <ref type="bibr" target="#b9">Klementiev et al., 2012)</ref>, dependency parsing ( <ref type="bibr" target="#b7">Guo et al., 2015)</ref> and POS tagging ( <ref type="bibr" target="#b6">Gouws and Søgaard, 2015</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this paper, we developed two weakly super- vised approaches for cross-lingual NER based on effective annotation and representation projection. We also designed two co-decoding schemes that combine the two projection-based systems in an intelligent way. Experimental results show that the combined systems outperform three state-of- the-art cross-lingual NER approaches, providing a strong baseline for building cross-lingual NER systems with no human annotation in target lan- guages.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture of the two neural network models: left-NN1, right-NN2.</figDesc><graphic url="image-1.png" coords="3,307.28,65.75,102.06,52.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>In-house NER data: Precision, Recall and 
F 1 score on exact phrasal matches. The highest F 1 
score among all the weakly supervised approaches 
is shown in bold. Same for Tables 4 and 5. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>CoNLL NER test data. 

supervised learning. 

</table></figure>

			<note place="foot" n="1"> Ideally, the sentences would be translations of each other, but we only require possibly parallel sentences.</note>

			<note place="foot" n="3"> In our experiments, CRFs cannot handle training data with a few million words, since our NER system has over 50 entity types, and the training time of CRFs grows at least quadratically in the number of entity types.</note>

			<note place="foot" n="4"> Two entities detected by two different systems conflict with each other if either 1) the two entities have different spans but overlap with each other; or 2) the two entities have the same span but with different NER tags.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Polyglot-ner: Massive multilingual named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
		<idno type="doi">10.1137/1.9781611974010.66</idno>
		<ptr target="https://doi.org/10.1137/1.9781611974010.66" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 SIAM International Conference on Data Mining</title>
		<meeting>the 2015 SIAM International Conference on Data Mining<address><addrLine>SIAM, Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An unsupervised method for word sense tagging using parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
		<idno type="doi">10.3115/1073083.1073126</idno>
		<ptr target="https://doi.org/10.3115/1073083.1073126" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, ACL&apos;02</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, ACL&apos;02</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="255" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Building a multilingual named entity-annotated corpus using annotation projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maud</forename><surname>Ehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Turchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Steinberger</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/R11-1017" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of Recent Advances in Natural Language Processing</title>
		<meeting>Recent Advances in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="118" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving vector space word representations using multilingual correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/E14-1049" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics. Association for Computational Linguistics<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="462" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bilbowa: Fast bilingual distributed representations without word alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning. JMLR Workshop and Conference Proceedings</title>
		<meeting>the 32nd International Conference on Machine Learning. JMLR Workshop and Conference Proceedings</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="748" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Simple task-specific bilingual word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N15-1157" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1386" to="1390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cross-lingual dependency parsing based on distributed representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P15-1119" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. Association for Computational Linguistics<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1234" to="1244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A maximum entropy word aligner for arabic-english machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><surname>Ittycheriah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/H05-1012" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Human Language Technology and Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Inducing crosslingual distributed representations of words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Klementiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binod</forename><surname>Bhattarai</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/C12-1089" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2012. The COLING 2012 Organizing Committee</title>
		<meeting>COLING 2012. The COLING 2012 Organizing Committee<address><addrLine>Mumbai, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1459" to="1474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning</title>
		<meeting>the Eighteenth International Conference on Machine Learning<address><addrLine>San Francisco, CA, USA, ICML&apos;01</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/N16-1030</idno>
		<ptr target="https://doi.org/10.18653/v1/N16-1030" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Two/too simple adaptations of word2vec for syntax problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N15-1142" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1299" to="1304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Maximum entropy markov models for information extraction and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dayne</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth International Conference on Machine Learning</title>
		<meeting>the Seventeenth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>CoRR abs/1301.3781</idno>
		<ptr target="http://arxiv.org/abs/1301.3781" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Exploiting similarities among languages for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<idno>CoRR abs/1309.4168</idno>
		<ptr target="http://arxiv.org/abs/1309.4168" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A survey of named entity recognition and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Nadeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Sekine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
		<idno type="doi">10.1075/li.30.1.03nad</idno>
		<ptr target="https://doi.org/10.1075/li.30.1.03nad" />
	</analytic>
	<monogr>
		<title level="j">Linguisticae Investigationes</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="26" />
			<publisher>John Benjamins Publishing Company</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving multilingual named entity recognition with wikipedia entity type mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/D16-1135</idno>
		<ptr target="https://doi.org/10.18653/v1/D16-1135" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1275" to="1284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Computer-Intensive Methods for Testing Hypotheses: An Introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">W</forename><surname>Noreen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>John Wiley &amp; Sons, Inc</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning multilingual named entity recognition from wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Nothman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicky</forename><surname>Ringland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">194</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<idno type="doi">10.1016/j.artint.2012.03.006</idno>
		<ptr target="https://doi.org/10.1016/j.artint.2012.03.006" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Density-driven cross-lingual transfer of dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Sadegh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Rasooli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collins</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/D15-1039</idno>
		<ptr target="https://doi.org/10.18653/v1/D15-1039" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="328" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mining wiki resources for multilingual named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Schone</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P08-1001" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT. Association for Computational Linguistics</title>
		<meeting>ACL-08: HLT. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cross-lingual word clusters for direct transfer of linguistic structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/N12-1052" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="477" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2002 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sang</surname></persName>
		</author>
		<idno type="doi">10.3115/1118853.1118877</idno>
		<ptr target="https://doi.org/10.3115/1118853.1118877" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Conference on Natural Language Learning</title>
		<meeting>the Sixth Conference on Natural Language Learning<address><addrLine>Stroudsburg, PA, USA, CONLL&apos;02</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien De</forename><surname>Meulder</surname></persName>
		</author>
		<idno type="doi">10.3115/1119176.1119195</idno>
		<ptr target="https://doi.org/10.3115/1119176.1119195" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</title>
		<meeting>the Seventh Conference on Natural Language Learning at HLT-NAACL 2003<address><addrLine>Stroudsburg, PA, USA, CONLL&apos;03</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cross-lingual named entity recognition via wikification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Tse</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Mayhew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/K16-1022</idno>
		<ptr target="https://doi.org/10.18653/v1/K16-1022" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="219" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cross-lingual wikification using multilingual embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Tse</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/N16-1072</idno>
		<ptr target="https://doi.org/10.18653/v1/N16-1072" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="589" to="598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bilingual word embeddings from non-parallel documentaligned data applied to bilingual lexicon induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P15-2118" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. Association for Computational Linguistics<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="719" to="725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cross-lingual projected expectation regularization for weakly supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D. Christopher</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/Q14-1005" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association of Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="55" to="66" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Inducing multilingual text analysis tools via robust projection across aligned corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Ngai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Wicentowski</surname></persName>
		</author>
		<idno type="doi">10.3115/1072133.1072187</idno>
		<ptr target="https://doi.org/10.3115/1072133.1072187" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First International Conference on Human Language Technology Research. Association for Computational Linguistics</title>
		<meeting>the First International Conference on Human Language Technology Research. Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA, HLT&apos;01</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mention detection crossing the language barrier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imed</forename><surname>Zitouni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D08-1063" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2008 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="600" to="609" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
