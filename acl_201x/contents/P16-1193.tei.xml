<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Vector Space for Distributional Semantics for Entailment *</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>August 7-12, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
							<email>james.henderson@xrce.xerox.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Xerox Research Centre Europe</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><forename type="middle">Nicoleta</forename><surname>Popa</surname></persName>
							<email>diana.popa@xrce.xerox.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Xerox Research Centre Europe</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Vector Space for Distributional Semantics for Entailment *</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2052" to="2062"/>
							<date type="published">August 7-12, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Distributional semantics creates vector-space representations that capture many forms of semantic similarity, but their relation to semantic entailment has been less clear. We propose a vector-space model which provides a formal foundation for a distributional semantics of entailment. Using a mean-field approximation, we develop approximate inference procedures and entailment operators over vectors of probabilities of features being known (ver-sus unknown). We use this framework to reinterpret an existing distributional-semantic model (Word2Vec) as approximating an entailment-based model of the distributions of words in contexts, thereby predicting lexical entailment relations. In both unsupervised and semi-supervised experiments on hyponymy detection, we get substantial improvements over previous results.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Modelling entailment is a fundamental issue in computational semantics. It is also important for many applications, for example to produce ab- stract summaries or to answer questions from text, where we need to ensure that the input text entails the output text. There has been a lot of interest in modelling entailment in a vector-space, but most of this work takes an empirical, often ad-hoc, ap- proach to this problem, and achieving good results has been difficult ( <ref type="bibr" target="#b12">Levy et al., 2015)</ref>. In this work, we propose a new framework for modelling entail- ment in a vector-space, and illustrate its effective- * This work was partially supported by French ANR grant CIFRE N 1324/2014.  <ref type="table">Table 1</ref>: Pattern of logical entailment between nothing known (unk), two different features f and g known, and the complement of f (¬f ) known.</p><p>ness with a distributional-semantic model of hy- ponymy detection. Unlike previous vector-space models of entail- ment, the proposed framework explicitly models what information is unknown. This is a crucial property, because entailment reflects what infor- mation is and is not known; a representation y en- tails a representation x if and only if everything that is known given x is also known given y. Thus, we model entailment in a vector space where each dimension represents something we might know. As illustrated in <ref type="table">Table 1</ref>, knowing that a feature f is true always entails knowing that same feature, but never entails knowing that a different feature g is true. Also, knowing that a feature is true always entails not knowing anything (unk), since strictly less information is still entailment, but the reverse is never true. Table 1 also illustrates that knowing that a feature f is false (¬f ) patterns exactly the same way as knowing that an unrelated feature g is true. This illustrates that the relevant dichotomy for entailment is known versus unknown, and not true versus false.</p><p>Previous vector-space models have been very successful at modelling semantic similarity, in par- ticular using distributional semantic models (e.g. ( <ref type="bibr" target="#b2">Deerwester et al., 1990;</ref><ref type="bibr" target="#b20">Schütze, 1993;</ref><ref type="bibr" target="#b14">Mikolov et al., 2013a)</ref>). Distributional semantics uses the distributions of words in contexts to induce vector- space embeddings of words, which have been shown to be useful for a wide variety of tasks. Two words are predicted to be similar if the dot product between their vectors is high. But the dot product is an anti-symmetric operator, which makes it more natural to interpret these vectors as representing whether features are true or false, whereas the dichotomy known versus unknown is asymmetric. We surmise that this is why distribu- tional semantic models have had difficulty mod- elling lexical entailment ( <ref type="bibr" target="#b12">Levy et al., 2015</ref>).</p><p>To develop a vector-space model of whether features are known or unknown, we start with dis- crete binary vectors, where 1 means known and 0 means unknown. Entailment between these dis- crete binary vectors can be calculated by indepen- dently checking each dimension. But as soon as we try to do calculations with distributions over these vectors, we need to deal with the case where the features are not independent. For example, if feature f has a 50% chance of being true and a 50% chance of being false, we can't assume that there is a 25% chance that both f and ¬f are known. This simple case of mutual exclusion is just one example of a wide range of constraints between features which we need to handle in se- mantic models. These constraints mean that the different dimensions of our vector space are not independent, and therefore exact models are not factorised. Because the models are not factorised, exact calculations of entailment and exact infer- ence of vectors are intractable.</p><p>Mean-field approximations are a popular ap- proach to efficient inference for intractable mod- els. In a mean-field approximation, distributions over binary vectors are represented using a sin- gle probability for each dimension. These vectors of real values are the basis of our proposed vector space for entailment.</p><p>In this work, we propose a vector-space model which provides a formal foundation for a distri- butional semantics of entailment. This framework is derived from a mean-field approximation to en- tailment between binary vectors, and includes op- erators for measuring entailment between vectors, and procedures for inferring vectors in an entail- ment graph. We validate this framework by us- ing it to reinterpret existing <ref type="bibr">Word2Vec (Mikolov et al., 2013a</ref>) word embedding vectors as approxi- mating an entailment-based model of the distribu- tion of words in contexts. This reinterpretation al- lows us to use existing word embeddings as an un- supervised model of lexical entailment, success- fully predicting hyponymy relations using the pro- posed entailment operators in both unsupervised and semi-supervised experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Modelling Entailment in a Vector Space</head><p>To develop a model of entailment in a vector space, we start with the logical definition of en- tailment in terms of vectors of discrete known fea- tures: y entails x if and only if all the known fea- tures in x are also included in y. We formalise this relation with binary vectors x, y where 1 means known and 0 means unknown, so this discrete en- tailment relation (y⇒x) can be defined with the binary formula:</p><formula xml:id="formula_0">P ((y⇒x) | x, y) = k (1 − (1−y k )x k )</formula><p>Given prior probability distributions P (x), P (y) over these vectors, the exact joint and marginal probabilities for an entailment relation are:</p><formula xml:id="formula_1">P (x, y, (y⇒x)) = P (x) P (y) k (1−(1−y k )x k ) P ((y⇒x)) = E P (x) E P (y) k (1−(1−y k )x k ) (1)</formula><p>We cannot assume that the priors P (x) and P (y) are factorised, because there are many im- portant correlations between features and there- fore we cannot assume that the features are in- dependent. As discussed in Section 1, even just representing both a feature f and its negation ¬f requires two different dimensions k and k in the vector space, because 0 represents unknown and not false. Given valid feature vectors, calculating entailment can consider these two dimensions sep- arately, but to reason with distributions over vec- tors we need the prior P (x) to enforce the con- straint that x k and x k are mutually exclusive. In general, such correlations and anti-correlations ex- ist between many semantic features, which makes inference and calculating the probability of entail- ment intractable.</p><p>To allow for efficient inference in such a model, we propose a mean-field approximation. This in effect assumes that the posterior distribution over vectors is factorised, but in practice this is a much weaker assumption than assuming the prior is fac- torised. The posterior distribution has less un- certainty and therefore is influenced less by non- factorised prior constraints. By assuming a fac- torised posterior, we can then represent distribu- tions over feature vectors with simple vectors of probabilities of individual features (or as below, with their log-odds). These real-valued vectors are the basis of the proposed vector-space model of entailment.</p><p>In the next two subsections, we derive a mean- field approximation for inference of real-valued vectors in entailment graphs. This derivation leads to three proposed vector-space operators for approximating the log-probability of entailment, summarised in <ref type="table">Table 2</ref>. These operators will be used in the evaluation in Section 5. This inference framework will also be used in Section 3 to model how existing word embeddings can be mapped to vectors to which the entailment operators can be applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">A Mean-Field Approximation</head><p>A mean-field approximation approximates the posterior P using a factorised distribution Q. First of all, this gives us a concise description of the posterior P (x| . . .) as a vector of continuous val- ues Q(x=1), where</p><formula xml:id="formula_2">Q(x=1) k = Q(x k =1) ≈ E P (x|...) x k = P (x k =1| . . .) (i.</formula><p>e. the marginal probabilities of each bit). Secondly, as is shown below, this gives us efficient methods for doing ap- proximate inference of vectors in a model.</p><p>First we consider the simple case where we want to approximate the posterior distribu- tion P (x, y|y⇒x). In a mean-field approxi- mation, we want to find a factorised distribu- tion Q(x, y) which minimises the KL-divergence D KL (Q(x, y)||P (x, y|y⇒x)) with the true distri- bution P (x, y|y⇒x).</p><formula xml:id="formula_3">L = D KL (Q(x, y)||P (x, y|(y⇒x))) ∝ x Q(x) log Q(x, y) P (x, y, (y⇒x)) = k E Q(x k ) log Q(x k ) + k E Q(y k ) log Q(y k ) − E Q(x) log P (x) − E Q(y) log P (y) − k E Q(x k ) E Q(y k ) log(1−(1−y k )x k )</formula><p>In the final equation, the first two terms are the negative entropy of Q, −H(Q), which acts as a maximum entropy regulariser, the final term en- forces the entailment constraint, and the middle two terms represent the prior for x and y. One ap- proach (generalised further in the next subsection) to the prior terms −E Q(x) log P (x) is to bound them by assuming P (x) is a function in the ex- ponential family, giving us:</p><formula xml:id="formula_4">E Q(x) log P (x) ≥ E Q(x) log exp( k θ x k x k ) Z θ = k E Q(x k ) θ x k x k − log Z θ</formula><p>where the log Z θ is not relevant in any of our in- ference problems and thus will be dropped below.</p><p>As typically in mean-field approximations, in- ference of Q(x) and Q(y) can't be done efficiently with this exact objective L, because of the non- linear interdependence between x k and y k in the last term. Thus, we introduce two approximations to L, one for use in inferring Q(x) given Q(y) (forward inference), and one for the reverse in- ference problem (backward inference). In both cases, the approximation is done with an appli- cation of Jensen's inequality to the log function, which gives us an upper bound on L, as is stan- dard practice in mean-field approximations. For forward inference:</p><formula xml:id="formula_5">L ≤ − H(Q) − Q(x k =1)θ x k − E Q(y k ) θ y k y k (2) − Q(x k =1) log Q(y k =1) )</formula><p>which we can optimise for Q(x k =1):</p><formula xml:id="formula_6">Q(x k =1) = σ( θ x k + log Q(y k =1) )<label>(3)</label></formula><p>where σ() is the sigmoid function. The sig- moid function arises from the entropy regulariser, making this a specific form of maximum entropy model. And for backward inference:</p><formula xml:id="formula_7">L ≤ − H(Q) − E Q(x k ) θ x k x k − Q(y k =1)θ y k (4) − (1−Q(y k =1)) log(1−Q(x k =1)) )</formula><p>which we can optimise for Q(y k =1):</p><formula xml:id="formula_8">Q(y k =1) = σ( θ y k − log(1−Q(x k =1)) ) (5)</formula><p>Note that in equations <ref type="formula">(2)</ref> and <ref type="formula">(4)</ref> the final terms,</p><formula xml:id="formula_9">Q(x k =1) log Q(y k =1) and (1−Q(y k =1)) log(1−Q(x k =1))</formula><p>respectively, are approximations to the log-probability of the entailment. We define two vector-space operators, &lt; and &gt; , to be these same approximations. <ref type="table">Table 2</ref>: The proposed entailment operators, ap- proximating log P (y⇒x).</p><formula xml:id="formula_10">log Q(y⇒x) ≈ k E Q(x k ) log(E Q(y k ) (1 − (1−y k )x k )) = Q(x=1) · log Q(y=1) ≡ X &lt; Y log Q(y⇒x) ≈ k E Q(y k ) log(E Q(x k ) (1 − (1−y k )x k )) = (1−Q(y=1)) · log(1−Q(x=1)) ≡ Y &gt; X X &lt; Y ≡ σ(X) · log σ(Y ) Y &gt; X ≡ σ(−Y ) · log σ(−X) Y ˜ ⇒X ≡ k log(1 − σ(−Y k )σ(X k ))</formula><p>We parametrise these operators with the vectors X, Y of log-odds of Q(x), Q(y), namely X = log Q(x=1) Q(x=0) = σ -1 (Q(x=1)). The resulting opera- tor definitions are summarised in <ref type="table">Table 2</ref>.</p><p>Also note that the probability of entailment given in equation <ref type="formula">(1)</ref> becomes factorised when we replace P with Q. We define a third vector-space operator, ˜ ⇒, to be this factorised approximation, also shown in <ref type="table">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Inference in Entailment Graphs</head><p>In general, doing inference for one entailment is not enough; we want to do inference in a graph of entailments between variables. In this section we generalise the above mean-field approximation to entailment graphs.</p><p>To represent information about variables that comes from outside the entailment graph, we as- sume we are given a prior P (x) over all variables x i in the graph. As above, we do not assume that this prior is factorised. Instead we assume that the prior P (x) is itself a graphical model which can be approximated with a mean-field approximation.</p><p>Given a set of variables x i each represent- ing vectors of binary variables x ik , a set of en- tailment relations r = {(i, j)|(x i ⇒x j )}, and a set of negated entailment relations ¯ r = {(i, j)|(x i / ⇒x j )}, we can write the joint posterior probability as:</p><formula xml:id="formula_11">P (x, r, ¯ r) = 1 Z P (x) i ( j:r(i,j) k P (x ik ⇒x jk |x ik , x jk )) ( j:¯ r(i,j) (1 − k P (x ik ⇒x jk |x ik , x jk )))</formula><p>We want to find a factorised distribution Q that minimises L = D KL (Q(x)||P (x|r, ¯ r)). As above, we bound this loss for each element X ik = σ -1 (Q(x ik =1)) of each vector we want to infer, using analogous Jensen's inequalities for the terms involving nodes i and j such that r(i, j) or r(j, i). For completeness, we also propose similar inequalities for nodes i and j such that ¯ r(i, j) or ¯ r(j, i), and bound them using the constants</p><formula xml:id="formula_12">C ijk ≥ k =k (1−σ(−X ik )σ(X jk )).</formula><p>To represent the prior P (x), we use the terms</p><formula xml:id="formula_13">θ ik (X ¯ ik ) ≤ log E Q(x ¯ ik ) P (x ¯ ik , x ik =1) 1 − E Q(x ¯ ik ) P (x ¯ ik , x ik =1) where x ¯</formula><p>ik is the set of all x i k such that either i =i or k =k. These terms can be thought of as the log- odds terms that would be contributed to the loss function by including the prior's graphical model in the mean-field approximation. Now we can infer the optimal X ik as:</p><formula xml:id="formula_14">X ik = θ ik (X ¯ ik ) + j:r(i,j) − log σ(−X jk )<label>(6)</label></formula><formula xml:id="formula_15">+ j:r(j,i) log σ(X jk ) + j:¯ r(j,i) log 1−C ijk σ(X jk ) 1−C ijk + j:¯ r(i,j) − log 1−C ijk σ(−X jk ) 1−C ijk</formula><p>In summary, the proposed mean-field approx- imation does inference in entailment graphs by iteratively re-estimating each X i as the sum of: the prior log-odds, − log σ(−X j ) for each en- tailed variable j, and log σ(X j ) for each entailing variable j. 1 This inference optimises X i &lt; X j for each entailing j plus X i &gt; X j for each entailed j, plus a maximum entropy regulariser on X i . Neg- ative entailment relations, if they exist, can also be incorporated with some additional approxima- tions. Complex priors can also be incorporated through their log-odds, simulating the inclusion of the prior within the mean-field approximation.</p><p>Given its dependence on mean-field approxima- tions, it is an empirical question to what extent we should view this model as computing real entail- ment probabilities and to what extent we should view it as a well-motivated non-linear mapping for which we simply optimise the input-output be- haviour (as for neural networks <ref type="bibr" target="#b5">(Henderson and Titov, 2010)</ref>). In Sections 3 and 5 we argue for the former (stronger) view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Interpreting Word2Vec Vectors</head><p>To evaluate how well the proposed framework pro- vides a formal foundation for the distributional se- mantics of entailment, we use it to re-interpret an existing model of distributional semantics in terms of semantic entailment. There has been a lot of work on how to use the distribution of contexts in which a word occurs to induce a vector represen- tation of the semantics of words. In this paper, we leverage this previous work on distributional semantics by re-interpreting a previous distribu- tional semantic model and using this understand- ing to map its vector-space word embeddings to vectors in the proposed framework. We then use the proposed operators to predict entailment be- tween words using these vectors. In Section 5 be- low, we evaluate these predictions on the task of hyponymy detection. In this section we motivate three different ways to interpret the Word2Vec ( <ref type="bibr" target="#b14">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b15">Mikolov et al., 2013b)</ref> dis- tributional semantic model as an approximation to an entailment-based model of the semantic rela- tionship between a word and its context.</p><p>Distributional semantics learns the semantics of words by looking at the distribution of contexts in which they occur. To model this relationship, we assume that the semantic features of a word are (statistically speaking) redundant with those of its context words, and consistent with those of its context words. We model these properties using a hidden vector which is the consistent unification of the features of the middle word and the context. In other words, there must exist a hidden vector which entails both of these vectors, and is consis- tent with prior constraints on vectors. We split this into two steps, inference of the hidden vector Y from the middle vector X m , context vectors X c and prior, and computing the log-probability (7) that this hidden vector entails the middle and con- text vectors:</p><formula xml:id="formula_16">max Y (log P (y, y⇒x m , y⇒x c ))<label>(7)</label></formula><p>We interpret Word2Vec's Skip-Gram model as learning its context and middle word vectors so that the log-probability of this entailment is high for the observed context words and low for other (sampled) context words. The word embeddings produced by Word2Vec are only related to the vec- tors X m assigned to the middle words; context vectors are computed but not output. We model the context vectors X c as combining (as in equa- tion (5)) information about a context word itself with information which can be inferred from this word given the prior, X c = θ c − log σ(−X c ). The numbers in the vectors output by Word2Vec are real numbers between negative infinity and in- finity, so the simplest interpretation of them is as the log-odds of a feature being known. In this case we can treat these vectors directly as the X m in the model. The inferred hidden vector Y can then be calculated using the model of backward inference from the previous section.</p><formula xml:id="formula_17">Y = θ c − log σ(−X c ) − log σ(−X m ) = X c − log σ(−X m )</formula><p>Since the unification Y of context and middle word features is computed using backward infer- ence, we use the backward-inference operator &gt; to calculate how successful that unification was. This gives us the final score:</p><formula xml:id="formula_18">log P (y, y⇒x m , y⇒x c ) ≈ Y &gt; X m + Y &gt; X c + −σ(−Y )·θ c = Y &gt; X m + −σ(−Y )·X c</formula><p>This is a natural interpretation, but it ignores the equivalence in Word2Vec between pairs of posi- tive values and pairs of negative values, due to its use of the dot product. As a more accurate in- terpretation, we interpret each Word2Vec dimen- sion as specifying whether its feature is known to be true or known to be false. Translating this Word2Vec vector into a vector in our entailment vector space, we get one copy Y + of the vector representing known-to-be-true features and a sec- ond negated duplicate Y − of the vector represent- ing known-to-be-false features, which we concate- nate to get our representation Y .</p><formula xml:id="formula_19">Y + = X c − log σ(−X m ) Y − = − X c − log σ(X m ) log P (y, y⇒x m , y⇒x c ) ≈ Y + &gt; X m + −σ(−Y + )·X c + Y − &gt; (−X m ) + −σ(−Y − )·(−X c )</formula><p>As a third alternative, we modify this latter in- terpretation with some probability mass reserved for unknown in the vicinity of zero. By subtract- ing 1 from both the original and negated copies of each dimension, we get a probability of unknown of 1−σ(X m −1) − σ(−X m −1). This gives us:</p><formula xml:id="formula_20">Y + = X c − log σ(−(X m −1)) Y − = − X c − log σ(−(−X m −1)) log P (y, y⇒x m , y⇒x c ) ≈ Y + &gt; (X m −1) + −σ(−Y + )·X c + Y − &gt; (−X m −1)) + −σ(−Y − )·(−X c )</formula><p>Figure 1: The learning gradients for Word2Vec, the log-odds &gt; , and the unk dup &gt; interpretation of its vectors.</p><p>To understand better the relative accuracy of these three interpretations, we compared the train- ing gradient which Word2Vec uses to train its middle-word vectors to the training gradient for each of these interpretations. We plotted these gra- dients for the range of values typically found in Word2Vec vectors for both the middle vector and the context vector. <ref type="figure">Figure 1</ref> shows three of these plots. As expected, the second interpretation is more accurate than the first because its plot is anti- symmetric around the diagonal, like the Word2Vec gradient. In the third alternative, the constant 1 was chosen to optimise this match, producing a close match to the Word2Vec training gradient, as shown in <ref type="figure">Figure 1</ref> (Word2Vec versus Unk dup).</p><p>Thus, Word2Vec can be seen as a good ap- proximation to the third model, and a progres- sively worse approximation to the second and first models. Therefore, if the entailment-based distri- butional semantic model we propose is accurate, then we would expect the best accuracy in hy- ponymy detection using the third interpretation of Word2Vec vectors, and progressively worse accu- racy for the other two interpretations. As we will see in Section 5, this prediction holds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>There has been a significant amount of work on us- ing distributional-semantic vectors for hyponymy detection, using supervised, semi-supervised or unsupervised methods (e.g. ( <ref type="bibr" target="#b28">Yu et al., 2015;</ref><ref type="bibr" target="#b16">Necsulescu et al., 2015;</ref><ref type="bibr" target="#b24">Vylomova et al., 2015;</ref><ref type="bibr" target="#b27">Weeds et al., 2014;</ref><ref type="bibr" target="#b3">Fu et al., 2015;</ref><ref type="bibr" target="#b17">Rei and Briscoe, 2014)</ref>). Because our main concern is modelling entailment within a vector space, we do not do a thorough comparison to models which use mea- sures computed outside the vector space (e.g. symmetric measures <ref type="figure">(LIN (Lin, 1998)</ref>), asym- metric measures (WeedsPrec ( <ref type="bibr" target="#b25">Weeds and Weir, 2003;</ref><ref type="bibr" target="#b26">Weeds et al., 2004</ref>), balAPinc ( <ref type="bibr" target="#b6">Kotlerman et al., 2010)</ref>, invCL ( <ref type="bibr" target="#b8">Lenci and Benotto, 2012)</ref>) and entropy-based measures (SLQS ( <ref type="bibr" target="#b19">Santus et al., 2014)</ref>)), nor to models which encode hyponymy in the parameters of a vector-space operator or clas- sifier ( <ref type="bibr" target="#b3">Fu et al., 2015;</ref><ref type="bibr" target="#b18">Roller et al., 2014;</ref><ref type="bibr" target="#b1">Baroni et al., 2012)</ref>). We also limit our evaluation of lex- ical entailment to hyponymy, not including other related lexical relations (cf. ( <ref type="bibr" target="#b27">Weeds et al., 2014;</ref><ref type="bibr" target="#b24">Vylomova et al., 2015;</ref><ref type="bibr" target="#b21">Turney and Mohammad, 2014;</ref>), leaving more complex cases to future work on compositional semantics. We are also not concerned with models or evalua- tions which require supervised learning about in- dividual words, instead limiting ourselves to semi- supervised learning where the words in the train- ing and test sets are disjoint.</p><p>For these reasons, in our evaluations we repli- cate the experimental setup of <ref type="bibr" target="#b27">Weeds et al. (2014)</ref>, for both unsupervised and semi-supervised mod- els. Within this setup, we compare to the results of the models evaluated by <ref type="bibr" target="#b27">Weeds et al. (2014)</ref> and to previously proposed vector-space operators. This includes one vector space operator for hyponymy which doesn't have trained parameters, proposed by <ref type="bibr" target="#b17">Rei and Briscoe (2014)</ref>, called weighted cosine. The dimensions of the dot product (normalised to make it a cosine measure) are weighted to put more weight on the larger values in the entailed (hypernym) vector.</p><p>We base this evaluation on the Word2Vec ( <ref type="bibr" target="#b14">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b15">Mikolov et al., 2013b</ref>) dis- tributional semantic model and its publicly avail- able word embeddings. We choose it because it is popular, simple, fast, and its embeddings have been derived from a very large corpus. <ref type="bibr" target="#b9">Levy and Goldberg (2014)</ref> showed that it is closely related to the previous PMI-based distributional semantic models (e.g. ( <ref type="bibr" target="#b22">Turney and Pantel, 2010)</ref>).</p><p>The most similar previous work, in terms of mo- tivation and aims, is that of <ref type="bibr" target="#b23">Vilnis and McCallum (2015)</ref>. They also model entailment directly using a vector space, without training a classifier. But instead of representing words as a point in a vec- tor space (as in this work), they represent words as a Gaussian distribution over points in a vector space. This allows them to represent the extent to which a feature is known versus unknown as the amount of variance in the distribution for that fea- ture's dimension. While nicely motivated theoret- ically, the model appears to be more computation- ally expensive than the one proposed here, particu- larly for inferring vectors. They do make unsuper- vised predictions of hyponymy relations with their learned vector distributions, using KL-divergence between the distributions for the two words. They evaluate their models on the hyponymy data from ( <ref type="bibr" target="#b1">Baroni et al., 2012)</ref>. As discussed further in sec- tion 5.2, our best models achieve non-significantly better average precision than their best models.</p><p>The semi-supervised model of <ref type="bibr" target="#b7">Kruszewski et al. (2015)</ref> also models entailment in a vector space, but they use a discrete vector space. They train a mapping from distributional semantic vectors to Boolean vectors such that feature inclusion re- spects a training set of entailment relations. They then use feature inclusion to predict hyponymy, and other lexical entailment relations. This ap- proach is similar to the one used in our semi- supervised experiments, except that their discrete entailment prediction operator is very different from our proposed entailment operators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>To evaluate whether the proposed framework is an effective model of entailment in vector spaces, we apply the interpretations from Section 3 to pub- licly available word embeddings and use them to predict the hyponymy relations in a benchmark dataset. This framework predicts that the more ac- curate interpretations of Word2Vec result in more accurate unsupervised models of hyponymy. We evaluate on detecting hyponymy relations between words because hyponymy is the canonical type of lexical entailment; most of the semantic features of a hypernym (e.g. "animal") must be included in the semantic features of the hyponym (e.g. "dog"). We evaluate in both a fully unsupervised setup and a semi-supervised setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Hyponymy with Word2Vec Vectors</head><p>For our evaluation on hyponymy detection, we replicate the experimental setup of <ref type="bibr" target="#b27">Weeds et al. (2014)</ref>, using their selection of word pairs 2 from the BLESS dataset ( <ref type="bibr" target="#b0">Baroni and Lenci, 2011</ref>). 3</p><p>These noun-noun word pairs include positive hy- ponymy pairs, plus negative pairs consisting of some other hyponymy pairs reversed, some pairs in other semantic relations, and some random pairs. Their selection is balanced between positive and negative examples, so that accuracy can be used as the performance measure. For their semi- supervised experiments, ten-fold cross validation is used, where for each test set, items are removed from the associated training set if they contain any word from the test set. Thus, the vocabulary of the training and testing sets are always disjoint, thereby requiring that the models learn about the vector space and not about the words themselves. We had to perform our own 10-fold split, but apply the same procedure to filter the training set.</p><p>We could not replicate the word embeddings used in <ref type="bibr" target="#b27">Weeds et al. (2014)</ref>, so instead we use pub- licly available word embeddings. <ref type="bibr">4</ref> These vectors were trained with the Word2Vec software applied to about 100 billion words of the Google-News dataset, and have 300 dimensions.</p><p>The hyponymy detection results are given in Ta- ble 3, including both unsupervised (upper box) and semi-supervised (lower box) experiments. We report two measures of performance, hyponymy detection accuracy (50% Acc) and direction clas- sification accuracy (Dir Acc). Since all the opera- tors only determine a score, we need to choose a threshold to get detection accuracies. Given that the proportion of positive examples in the dataset has been artificially set at 50%, we threshold each model's score at the point where the proportion of positive examples output is 50%, which we call "50% Acc". Thus the threshold is set after seeing the testing inputs but not their target labels.</p><p>Direction classification accuracy (Dir Acc) in- dicates how well the method distinguishes the rel- ative abstractness of two nouns. Given a pair of nouns which are in a hyponymy relation, it classi- fies which word is the hypernym and which is the hyponym. This measure only considers positive examples and chooses one of two directions, so it is inherently a balanced binary classification task. Classification is performed by simply comparing the scores in both directions. If both directions produce the same score, the expected random ac- curacy (50%) is used.</p><p>As representative of previous work, we report operator supervision 50% Acc Dir Acc Weeds et.al. None 58% - log-odds &lt; None 54.0% 55.9% weighted cos None 55.5% 57.9% dot None 56.3% 50% dif None 56.9% 59.6% log-odds˜⇒odds˜ odds˜⇒ None 57.0% 59.4% log-odds &gt;  the best results from <ref type="bibr" target="#b27">Weeds et al. (2014)</ref>, who try a number of unsupervised and semi-supervised models, and use the same testing methodology and hyponymy data. However, note that their word embeddings are different. For the semi- supervised models, <ref type="bibr" target="#b27">Weeds et al. (2014)</ref> trains clas- sifiers, which are potentially more powerful than our linear vector mappings. We also compare the proposed operators to the dot product (dot), 5 vec- tor differences (dif ), and the weighted cosine of Rei and Briscoe (2014) (weighted cos), all com- puted with the same word embeddings as for the proposed operators. In Section 3 we argued for three progressively more accurate interpretations of Word2Vec vec- tors in the proposed framework, the log-odds inter- pretation (log-odds &gt; ), the negated duplicate inter- pretation (dup &gt; ), and the negated duplicate inter- pretation with unknown around zero (unk dup &gt; ). We also evaluate using the factorised calculation of entailment (log-odds˜⇒odds˜ odds˜⇒, unk dup˜⇒dup˜ dup˜⇒), and the backward-inference entailment operator (log-odds &lt; ), neither of which match the proposed interpre-tations. For the semi-supervised case, we train a linear vector-space mapping into a new vector space, in which we apply the operators (mapped operators). All these results are discussed in the next two subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Unsupervised Hyponymy Detection</head><p>The first set of experiments evaluate the vector- space operators in unsupervised models of hy- ponymy detection. The proposed models are com- pared to the dot product, because this is the stan- dard vector-space operator and has been shown to capture semantic similarity very well. However, because the dot product is a symmetric operator, it always performs at chance for direction clas- sification. Another vector-space operator which has received much attention recently is vector dif- ferences. This is used (with vector sum) to per- form semantic transforms, such as "king -male + female = queen", and has previously been used for modelling hyponymy <ref type="bibr" target="#b24">(Vylomova et al., 2015;</ref><ref type="bibr" target="#b27">Weeds et al., 2014</ref>). For our purposes, we sum the pairwise differences to get a score which we use for hyponymy detection.</p><p>For the unsupervised results in the upper box of table 3, the best unsupervised model of <ref type="bibr" target="#b27">Weeds et al. (2014)</ref>, and the operators dot, dif and weighted cos all perform similarly on accuracy, as does the log-odds factorised entailment calculation (log- odds˜⇒odds˜ odds˜⇒). The forward-inference entailment op- erator (log-odds &lt; ) performs above chance but not well, as expected given the backward-inference- based interpretation of Word2Vec vectors. By def- inition, dot is at chance for direction classification, but the other models all perform better, indicat- ing that all these operators are able to measure relative abstractness. As predicted, the &gt; opera- tor performs significantly better than all these re- sults on accuracy, as well as on direction classifi- cation, even assuming the log-odds interpretation of Word2Vec vectors.</p><p>When we move to the more accurate interpreta- tion of Word2Vec vectors as specifying both orig- inal and negated features (dup &gt; ), we improve (non-significantly) on the log-odds interpretation. Finally, the third and most accurate interpretation, where values around zero can be unknown (unk dup &gt; ), achieves the best results in unsupervised hyponymy detection, as well as for direction clas- sification. Changing to the factorised entailment operator (unk dup˜⇒dup˜ dup˜⇒) is worse but also signifi-cantly better than the other accuracies.</p><p>To allow a direct comparison to the model of <ref type="bibr" target="#b23">Vilnis and McCallum (2015)</ref>, we also evalu- ated the unsupervised models on the hyponymy data from ( <ref type="bibr" target="#b1">Baroni et al., 2012</ref>). Our best model achieved 81% average precision on this dataset, non-significantly better than the 80% achieved by the best model of <ref type="bibr" target="#b23">Vilnis and McCallum (2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Semi-supervised Hyponymy Detection</head><p>Since the unsupervised learning of word embed- dings may reflect many context-word correlations which have nothing to do with hyponymy, we also consider a semi-supervised setting. Adding some supervision helps distinguish features that capture semantic properties from other features which are not relevant to hyponymy detection. But even with supervision, we still want the resulting model to be captured in a vector space, and not in a parametrised scoring function. Thus, we train mappings from the Word2Vec word vectors to new word vectors, and then apply the entailment opera- tors in this new vector space to predict hyponymy. Because the words in the testing set are always dis- joint from the words in the training set, this experi- ment measures how well the original unsupervised vector space captures features that generalise en- tailment across words, and not how well the map- ping can learn about individual words.</p><p>Our objective is to learn a mapping to a new vector space in which an operator can be applied to predict hyponymy. We train linear mappings for the &gt; operator (mapped &gt; ) and for vector dif- ferences (mapped dif ), since these were the best performing proposed operator and baseline opera- tor, respectively, in the unsupervised experiments. We do not use the duplicated interpretations be- cause these transforms are subsumed by the ability to learn a linear mapping. <ref type="bibr">6</ref> Previous work on using vector differences for semi-supervised hyponymy detection has used a linear SVM ( <ref type="bibr" target="#b24">Vylomova et al., 2015;</ref><ref type="bibr" target="#b27">Weeds et al., 2014</ref>), which is mathemati- cally equivalent to our vector-differences model, except that we use cross entropy loss and they use a large-margin loss and SVM training.</p><p>The <ref type="table" target="#tab_1">semi-supervised results in the bottom box  of table 3</ref> show a similar pattern to the unsuper- vised results. <ref type="bibr">7</ref> The &gt; operator achieves the best generalisation from training word vectors to test- ing word vectors. The mapped &gt; model has the best accuracy, followed by the factorised entail- ment operator mapped˜⇒mapped˜ mapped˜⇒ and <ref type="bibr" target="#b27">Weeds et al. (2014)</ref>. Direction accuracies of all the proposed operators (mapped &gt; , mapped˜⇒mapped˜ mapped˜⇒, mapped &lt; ) reach into the 90's. The dif operator performs particularly poorly in this mapped setting, perhaps because both the mapping and the operator are linear. These semi- supervised results again support our distributional- semantic interpretations of Word2Vec vectors and their associated entailment operator &gt; .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we propose a vector-space model which provides a formal foundation for a distri- butional semantics of entailment. We developed a mean-field approximation to probabilistic entail- ment between vectors which represent known ver- sus unknown features. And we used this frame- work to derive vector operators for entailment and vector inference equations for entailment graphs. This framework allows us to reinterpret Word2Vec as approximating an entailment-based distribu- tional semantic model of words in context, and show that more accurate interpretations result in more accurate unsupervised models of lexical en- tailment, achieving better accuracies than previ- ous models. Semi-supervised evaluations confirm these results. A crucial distinction between the semi- supervised models here and much previous work is that they learn a mapping into a vector space which represents entailment, rather than learning a parametrised entailment classifier. Within this new vector space, the entailment operators and inference equations apply, thereby generalising naturally from these lexical representations to the compositional semantics of multi-word expres- sions and sentences. Further work is needed to explore the full power of these abilities to extract information about entailment from both unla- belled text and labelled entailment data, encode it all in a single vector space, and efficiently perform complex inferences about vectors and entailments. This future work on compositional distributional semantics should further demonstrate the full power of the proposed framework for modelling entailment in a vector space.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Accuracies on the BLESS data from 
Weeds et al. (2014), for hyponymy detection (50% 
Acc) and hyponymy direction classification (Dir 
Acc), in the unsupervised (upper box) and semi-
supervised (lower box) experiments. For unsuper-
vised accuracies, * marks a significant difference 
with the previous row. 

</table></figure>

			<note place="foot" n="1"> It is interesting to note that − log σ(−Xj) is a nonnegative transform of Xj, similar to the ReLU nonlinearity which is popular in deep neural networks (Glorot et al., 2011). log σ(Xj) is the analogous non-positive transform.</note>

			<note place="foot" n="2"> https://github.com/SussexCompSem/ learninghypernyms 3 Of the 1667 word pairs in this data, 24 were removed because we do not have an embedding for one of the words.</note>

			<note place="foot" n="4"> https://code.google.com/archive/p/ word2vec/</note>

			<note place="foot" n="5"> We also tested the cosine measure, but results were very slightly worse than dot.</note>

			<note place="foot" n="6"> Empirical results confirm that this is in practice the case, so we do not include these results in the table. 7 It is not clear how to measure significance for crossvalidation results, so we do not attempt to do so.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">How we blessed distributional semantic evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics, GEMS &apos;11</title>
		<meeting>the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics, GEMS &apos;11</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Entailment above the word level in distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc-Quynh</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Chieh</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL)</title>
		<meeting>the 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL)<address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="23" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning semantic hierarchies: A continuous vector space approach. Audio, Speech, and Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiji</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="461" to="471" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">In International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Incremental sigmoid belief networks for grammar learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3541" to="3570" />
			<date type="published" when="2010-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Directional distributional similarity for lexical inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Kotlerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Szpektor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maayan Zhitomirsky-Geffet</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="359" to="389" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deriving boolean structures from distributional vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germn</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Paperno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="375" to="388" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Identifying hypernyms in distributional semantic spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulia</forename><surname>Benotto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Joint Conference on Lexical and Computational Semantics, SemEval &apos;12</title>
		<meeting>the First Joint Conference on Lexical and Computational Semantics, SemEval &apos;12</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="75" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Neural word embedding as implicit matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Focused entailment graphs for open ie propositions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Ido Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goldberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="87" to="97" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Do supervised distributional methods really learn lexical inference relations?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Remus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-05" />
			<biblScope unit="page" from="970" to="976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatic retrieval and clustering of similar words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Computational Linguistics</title>
		<meeting>the 17th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="768" to="774" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Reading between the lines: Overcoming data sparsity for accurate classification of lexical relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Necsulescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Núria</forename><surname>Bel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics</title>
		<meeting>the Fourth Joint Conference on Lexical and Computational Semantics<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="182" to="192" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Looking for hyponyms in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Rei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="68" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Inclusive yet selective: Supervised distributional hypernymy detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Boleda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1025" to="1036" />
		</imprint>
		<respStmt>
			<orgName>Dublin City University and Association for Computational Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Chasing hypernyms in vector spaces with entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Santus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Schulte Im Walde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Gothenburg</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04-26" />
			<biblScope unit="page" from="38" to="42" />
		</imprint>
	</monogr>
	<note>Sweden</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Word space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 5</title>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page" from="895" to="902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Experiments with three approaches to recognizing lexical entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saif</forename><forename type="middle">M</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mohammad</surname></persName>
		</author>
		<idno>abs/1401.8269</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">From frequency to meaning: Vector space models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Int. Res</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Word representations via Gaussian embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Take and took, gaggle and goose, book and read: Evaluating the utility of vector differences for lexical relation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Vylomova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Rimell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A general framework for distributional similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Weeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;03</title>
		<meeting>the 2003 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;03</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Characterising measures of lexical distributional similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Weeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Mccarthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Computational Linguistics, COLING &apos;04</title>
		<meeting>the 20th International Conference on Computational Linguistics, COLING &apos;04</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1015" to="1021" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to distinguish hypernyms and co-hyponyms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Weeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daoud</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Reffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2249" to="2259" />
		</imprint>
		<respStmt>
			<orgName>Dublin City University and Association for Computational Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning term embeddings for hypernymy identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haixun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuemin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015. AAAI Press / International Joint Conferences on Artificial Intelligence</title>
		<meeting>the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015. AAAI Press / International Joint Conferences on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
