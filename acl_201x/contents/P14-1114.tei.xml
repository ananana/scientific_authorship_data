<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:13+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Probabilistic Soft Logic for Semantic Textual Similarity</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Islam</forename><surname>Beltagy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Linguistics</orgName>
								<orgName type="institution">The University of Texas at Austin Austin</orgName>
								<address>
									<postCode>78712 §</postCode>
									<settlement>Texas</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Linguistics</orgName>
								<orgName type="institution">The University of Texas at Austin Austin</orgName>
								<address>
									<postCode>78712 §</postCode>
									<settlement>Texas</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Linguistics</orgName>
								<orgName type="institution">The University of Texas at Austin Austin</orgName>
								<address>
									<postCode>78712 §</postCode>
									<settlement>Texas</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Probabilistic Soft Logic for Semantic Textual Similarity</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1210" to="1219"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Probabilistic Soft Logic (PSL) is a recently developed framework for proba-bilistic logic. We use PSL to combine logical and distributional representations of natural-language meaning, where distri-butional information is represented in the form of weighted inference rules. We apply this framework to the task of Semantic Textual Similarity (STS) (i.e. judging the semantic similarity of natural-language sentences), and show that PSL gives improved results compared to a previous approach based on Markov Logic Networks (MLNs) and a purely distribu-tional approach.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>When will people say that two sentences are sim- ilar? This question is at the heart of the Semantic Textual Similarity task (STS)( <ref type="bibr" target="#b0">Agirre et al., 2012)</ref>. Certainly, if two sentences contain many of the same words, or many similar words, that is a good indication of sentence similarity. But that can be misleading. A better characterization would be to say that if two sentences use the same or similar words in the same or similar relations, then those two sentences will be judged similar. <ref type="bibr">1</ref> Interest- ingly, this characterization echoes the principle of compositionality, which states that the meaning of a phrase is uniquely determined by the meaning of its parts and the rules that connect those parts. <ref type="bibr" target="#b4">Beltagy et al. (2013)</ref> proposed a hybrid ap- proach to sentence similarity: They use a very deep representation of sentence meaning, ex- pressed in first-order logic, to capture sentence structure, but combine it with distributional sim- ilarity ratings at the word and phrase level. Sen- tence similarity is then modelled as mutual entail- ment in a probabilistic logic. This approach is in- teresting in that it uses a very deep and precise representation of meaning, which can then be re- laxed in a controlled fashion using distributional similarity. But the approach faces large hurdles in practice, stemming from efficiency issues with the Markov Logic Networks (MLN) ( <ref type="bibr" target="#b28">Richardson and Domingos, 2006</ref>) that they use for performing probabilistic logical inference.</p><p>In this paper, we use the same combined logic- based and distributional framework as <ref type="bibr" target="#b4">Beltagy et al., (2013)</ref> but replace Markov Logic Networks with Probabilistic Soft Logic (PSL) ( <ref type="bibr" target="#b19">Kimmig et al., 2012;</ref><ref type="bibr" target="#b1">Bach et al., 2013)</ref>. PSL is a proba- bilistic logic framework designed to have efficient inference. Inference in MLNs is theoretically in- tractable in the general case, and existing approxi- mate inference algorithms are computationally ex- pensive and sometimes inaccurate. Consequently, the MLN approach of <ref type="bibr" target="#b4">Beltagy et al. (2013)</ref> was unable to scale to long sentences and was only tested on the relatively short sentences in the Mi- crosoft video description corpus used for STS ( <ref type="bibr" target="#b0">Agirre et al., 2012</ref>). On the other hand, inference in PSL reduces to a linear programming problem, which is theoretically and practically much more efficient. Empirical results on a range of prob- lems have confirmed that inference in PSL is much more efficient than in MLNs, and frequently more accurate ( <ref type="bibr" target="#b19">Kimmig et al., 2012;</ref><ref type="bibr" target="#b1">Bach et al., 2013)</ref>.</p><p>We show how to use PSL for STS, and describe changes to the PSL framework that make it more effective for STS. For evaluation, we test on three STS datasets, and compare our PSL system with the MLN approach of <ref type="bibr" target="#b4">Beltagy et al., (2013)</ref> and with distributional-only baselines. Experimental results demonstrate that, overall, PSL models hu- man similarity judgements more accurately than these alternative approaches, and is significantly faster than MLNs.</p><p>The rest of the paper is organized as follows: section 2 presents relevant background material, section 3 explains how we adapted PSL for the STS task, section 4 presents the evaluation, and sections 5 and 6 discuss future work and conclu- sions, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Logical Semantics</head><p>Logic-based representations of meaning have a long tradition <ref type="bibr" target="#b26">(Montague, 1970;</ref><ref type="bibr" target="#b18">Kamp and Reyle, 1993)</ref>. They handle many complex semantic phe- nomena such as relational propositions, logical operators, and quantifiers; however, their binary nature prevents them from capturing the "graded" aspects of meaning in language. Also, it is difficult to construct formal ontologies of properties and re- lations that have broad coverage, and semantically parsing sentences into logical expressions utilizing such an ontology is very difficult. Consequently, current semantic parsers are mostly restricted to quite limited domains, such as querying a specific database ( <ref type="bibr" target="#b20">Kwiatkowski et al., 2013;</ref><ref type="bibr" target="#b7">Berant et al., 2013)</ref>. In contrast, our system is not limited to any formal ontology and can use a wide-coverage tool for semantic analysis, as discussed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Distributional Semantics</head><p>Distributional models <ref type="bibr" target="#b29">(Turney and Pantel, 2010)</ref>, on the other hand, use statistics on contextual data from large corpora to predict semantic sim- ilarity of words and phrases <ref type="bibr" target="#b21">(Landauer and Dumais, 1997;</ref><ref type="bibr" target="#b25">Mitchell and Lapata, 2010)</ref>. They are relatively easier to build than logical representa- tions, automatically acquire knowledge from "big data," and capture the "graded" nature of linguis- tic meaning, but do not adequately capture logical structure <ref type="bibr" target="#b17">(Grefenstette, 2013)</ref>.</p><p>Distributional models are motivated by the ob- servation that semantically similar words occur in similar contexts, so words can be represented as vectors in high dimensional spaces generated from the contexts in which they occur <ref type="bibr" target="#b21">(Landauer and Dumais, 1997;</ref><ref type="bibr" target="#b23">Lund and Burgess, 1996)</ref>. Such models have also been extended to compute vec- tor representations for larger phrases, e.g. by adding the vectors for the individual words <ref type="bibr" target="#b21">(Landauer and Dumais, 1997)</ref> or by a component-wise product of word vectors <ref type="bibr" target="#b24">(Mitchell and Lapata, 2008;</ref><ref type="bibr" target="#b25">Mitchell and Lapata, 2010)</ref>, or more com- plex methods that compute phrase vectors from word vectors and tensors ( <ref type="bibr" target="#b3">Baroni and Zamparelli, 2010;</ref><ref type="bibr" target="#b16">Grefenstette and Sadrzadeh, 2011</ref>). We use vector addition <ref type="bibr" target="#b21">(Landauer and Dumais, 1997)</ref>, and component-wise product <ref type="bibr" target="#b24">(Mitchell and Lapata, 2008)</ref> as baselines for STS. Vector addition was previously found to be the best performing sim- ple distributional method for STS ( <ref type="bibr" target="#b4">Beltagy et al., 2013</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Markov Logic Networks</head><p>Markov Logic Networks (MLN) ( <ref type="bibr" target="#b28">Richardson and Domingos, 2006</ref>) are a framework for probabilis- tic logic that employ weighted formulas in first- order logic to compactly encode complex undi- rected probabilistic graphical models (i.e., Markov networks). Weighting the rules is a way of soft- ening them compared to hard logical constraints and thereby allowing situations in which not all clauses are satisfied. MLNs define a probability distribution over possible worlds, where a world's probability increases exponentially with the to- tal weight of the logical clauses that it satisfies. A variety of inference methods for MLNs have been developed, however, developing a scalable, general-purpose, accurate inference method for complex MLNs is an open problem. <ref type="bibr" target="#b4">Beltagy et al. (2013)</ref> use MLNs to represent the meaning of natural language sentences and judge textual en- tailment and semantic similarity, but they were un- able to scale the approach beyond short sentences due to the complexity of MLN inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Probabilistic Soft Logic</head><p>Probabilistic Soft Logic (PSL) is a recently pro- posed alternative framework for probabilistic logic ( <ref type="bibr" target="#b19">Kimmig et al., 2012;</ref><ref type="bibr" target="#b1">Bach et al., 2013)</ref>. It uses logical representations to compactly define large graphical models with continuous variables, and includes methods for performing efficient proba- bilistic inference for the resulting models. A key distinguishing feature of PSL is that ground atoms have soft, continuous truth values in the interval <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> rather than binary truth values as used in MLNs and most other probabilistic logics. Given a set of weighted logical formulas, PSL builds a graphical model defining a probability distribution over the continuous space of values of the random variables in the model. A PSL model is defined using a set of weighted if-then rules in first-order logic, as in the following example:</p><formula xml:id="formula_0">∀x, y, z. f riend(x, y) ∧ votesF or(y, z) → votesF or(x, z) | 0.3 (1) ∀x, y, z. spouse(x, y) ∧ votesF or(y, z) → votesF or(x, z) | 0.8 (2)</formula><p>In our notation, we use lower case letters like x, y, z to represent variables and upper case let- ters for constants. The first rule states that a per- son is likely to vote for the same person as his/her friend. The second rule encodes the same regular- ity for a person's spouse. The weights encode the knowledge that a spouse's influence is greater than a friend's in this regard.</p><p>In addition, PSL includes similarity functions. Similarity functions take two strings or two sets as input and return a truth value in the interval <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> denoting the similarity of the inputs. For example, in our application, we generate inference rules that incorporate the similarity of two predicates. This can be represented in PSL as:</p><formula xml:id="formula_1">∀x. similarity("predicate1", "predicate2") ∧ predicate1(x) → predicate2(x)</formula><p>As mentioned above, each ground atom, a, has a soft truth value in the interval <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>, which is denoted by I(a). To compute soft truth values for logical formulas, Lukasiewicz's re- laxation of conjunctions(∧), disjunctions(∨) and negations(¬) are used:</p><formula xml:id="formula_2">I(l 1 ∧ l 1 ) = max{0, I(l 1 ) + I(l 2 ) − 1} I(l 1 ∨ l 1 ) = min{I(l 1 ) + I(l 2 ), 1} I(¬l 1 ) = 1 − I(l 1 )</formula><p>Then, a given rule r ≡ r body → r head , is said to be satisfied (i.e. I(r) = 1) iff I(r body ) ≤ I(r head ). Otherwise, PSL defines a distance to satisfaction d(r) which captures how far a rule r is from being satisfied: d(r) = max{0, I(r body ) − I(r head )}. For example, assume we have the set of evidence: I(spouse(B, A)) = 1, I(votesF or(A, P )) = 0.9, I(votesF or(B, P )) = 0.3, and that r is the resulting ground instance of rule (2). Then I(spouse(B, A) ∧ votesF or(A, P )) = max{0, 1 + 0.9 − 1} = 0.9, and d(r) = max{0, 0.9 − 0.3} = 0.6.</p><p>Using distance to satisfaction, PSL defines a probability distribution over all possible interpre- tations I of all ground atoms. The pdf is defined as follows:</p><formula xml:id="formula_3">p(I) = 1 Z exp [− r∈R λ r (d(r)) p ];<label>(3)</label></formula><formula xml:id="formula_4">Z = I exp [− r∈R λ r (d(r)) p ]</formula><p>where Z is the normalization constant, λ r is the weight of rule r, R is the set of all rules, and p ∈ {1, 2} provides two different loss functions. For our application, we always use p = 1 PSL is primarily designed to support MPE in- ference (Most Probable Explanation). MPE infer- ence is the task of finding the overall interpretation with the maximum probability given a set of evi- dence. Intuitively, the interpretation with the high- est probability is the interpretation with the lowest distance to satisfaction. In other words, it is the interpretation that tries to satisfy all rules as much as possible. Formally, from equation 3, the most probable interpretation, is the one that minimizes r∈R λ r (d(r)) p . In case of p = 1, and given that all d(r) are linear equations, then minimizing the sum requires solving a linear program, which, compared to inference in other probabilistic logics such as MLNs, can be done relatively efficiently using well-established techniques. In case p = 2, MPE inference can be shown to be a second-order cone program (SOCP) ( <ref type="bibr" target="#b19">Kimmig et al., 2012</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Semantic Textual Similarity</head><p>Semantic Textual Similarity (STS) is the task of judging the similarity of a pair of sentences on a scale from 0 to 5, and was recently introduced as a SemEval task ( <ref type="bibr" target="#b0">Agirre et al., 2012</ref>). Gold standard scores are averaged over multiple hu- man annotations and systems are evaluated using the Pearson correlation between a system's out- put and gold standard scores. The best perform- ing system in 2012's competition was by <ref type="bibr">Bär et al. (2012)</ref>, a complex ensemble system that inte- grates many techniques including string similarity, n-gram overlap, WordNet similarity, vector space similarity and MT evaluation metrics. Two of the datasets we use for evaluation are from the 2012 competition. We did not utilize the new datasets added in the 2013 competition since they did not contain naturally-occurring, full sentences, which is the focus of our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Combining logical and distributional methods using probabilistic logic</head><p>There are a few recent attempts to combine log- ical and distributional representations in order to obtain the advantages of both. <ref type="bibr" target="#b22">Lewis and Steedman (2013)</ref> use distributional information to deter- mine word senses, but still produce a strictly log- ical semantic representation that does not address the "graded" nature of linguistic meaning that is important to measuring semantic similarity. <ref type="bibr" target="#b15">Garrette et al. (2011)</ref> introduced a framework for combining logic and distributional models us- ing probabilistic logic. Distributional similarity between pairs of words is converted into weighted inference rules that are added to the logical repre- sentation, and Markov Logic Networks are used to perform probabilistic logical inference. <ref type="bibr" target="#b4">Beltagy et al. (2013)</ref> extended this framework by generating distributional inference rules from phrase similarity and tailoring the system to the STS task. STS is treated as computing the prob- ability of two textual entailments T |= H and H |= T , where T and H are the two sentences whose similarity is being judged. These two en- tailment probabilities are averaged to produce a measure of similarity. The MLN constructed to determine the probability of a given entailment includes the logical forms for both T and H as well as soft inference rules that are constructed from distributional information. Given a similar- ity score for all pairs of sentences in the dataset, a regressor is trained on the training set to map the system's output to the gold standard scores. The trained regressor is applied to the scores in the test set before calculating Pearson correlation. The regression algorithm used is Additive Regres- sion <ref type="bibr" target="#b13">(Friedman, 2002)</ref>.</p><p>To determine an entailment probability, first, the two sentences are mapped to logical repre- sentations using Boxer <ref type="bibr" target="#b8">(Bos, 2008)</ref>, a tool for wide-coverage semantic analysis that maps a CCG (Combinatory Categorial Grammar) parse into a lexically-based logical form. Boxer uses C&amp;C for CCG parsing <ref type="bibr" target="#b11">(Clark and Curran, 2004</ref>).</p><p>Distributional semantic knowledge is then en- coded as weighted inference rules in the MLN. A rule's weight (w) is a function of the cosine similarity (sim) between its antecedent and con- sequent. Rules are generated on the fly for each T and H. Let t and h be the lists of all words and phrases in T and H respectively. For all pairs (a, b), where a ∈ t, b ∈ h, it generates an inference rule:</p><formula xml:id="formula_5">a → b | w, where w = f (sim( − → a , − → b ))</formula><p>. Both a and b can be words or phrases. Phrases are defined in terms of Boxer's output. A phrase is more than one unary atom sharing the same variable like "a little kid" which in logic is little(K) ∧ kid(K). A phrase also can be two unary atoms connected by a relation like "a man is driving" which in logic is man <ref type="bibr">(2013)</ref> found that the logical con- junction in H is very restrictive for the STS task, so they relaxed the conjunction by using an aver- age evidence combiner ( <ref type="bibr" target="#b27">Natarajan et al., 2010)</ref>. The average combiner results in computationally complex inference and only works for short sen- tences. In case inference breaks or times-out, they back off to a simpler combiner that leads to much faster inference but loses most of the structure of the sentence and is therefore less accurate.</p><note type="other">(M ) ∧ agent(D, M ) ∧ drive(D). The similarity func- tion sim takes two vectors as input. Phrasal vec- tors are constructed using Vector Addition (Lan- dauer and Dumais, 1997). The set of generated inference rules can be regarded as the knowledge base KB. Beltagy et al.</note><p>Given T , KB and H from the previous steps, MLN inference is then used to compute p(H|T, KB), which is then used as a measure of the degree to which T entails H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PSL for STS</head><p>For several reasons, we believe PSL is a more ap- propriate probabilistic logic for STS than MLNs. First, it is explicitly designed to support efficient inference, therefore it scales better to longer sen- tences with more complex logical forms. Sec- ond, it was also specifically designed for com- puting similarity between complex structured ob- jects rather than determining probabilistic logical entailment. In fact, the initial version of PSL ( <ref type="bibr" target="#b9">Broecheler et al., 2010</ref>) was called Probabilis- tic Similarity Logic, based on its use of similar- ity functions. This initial version was shown to be very effective for measuring the similarity of noisy database records and performing record linkage (i.e. identifying database entries referring to the same entity, such as bibliographic citations refer- ring to the same paper). Therefore, we have devel- oped an approach that follows that of <ref type="bibr" target="#b4">Beltagy et al. (2013)</ref>, but replaces Markov Logic with PSL.</p><p>This section explains how we formulate the STS task as a PSL program. PSL does not work very well "out of the box" for STS, mainly because Lukasiewicz's equation for the conjunction is very restrictive. Therefore, we use a different interpre- tation for conjunction that uses averaging, which requires corresponding changes to the optimiza- tion problem and the grounding technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Representation</head><p>Given the logical forms for a pair of sentences, a text T and a hypothesis H, and given a set of weighted rules derived from the distributional se- mantics (as explained in section 2.6) composing the knowledge base KB, we build a PSL model that supports determining the truth value of H in the most probable interpretation (i.e. MPE) given T and KB.</p><p>Consider the pair of sentences is "A man is driv- ing", and "A guy is walking". Parsing into logical form gives:</p><formula xml:id="formula_6">T : ∃x, y. man(x) ∧ agent(y, x) ∧ drive(y) H : ∃x, y. guy(x) ∧ agent(y, x) ∧ walk(y)</formula><p>The PSL program is constructed as follows:</p><p>T : The text is represented in the evidence set. For the example, after Skolemizing the existential quantifiers, this contains the ground atoms: {man(A), agent(B, A), drive(B)}</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KB:</head><p>The knowledge base is a set of lexical and phrasal rules generated from distributional semantics, along with a similarity score for each rule (section 2.6). For the exam- ple, we generate the rules: ∀x. man(x) ∧ vs sim("man", "guy") → guy(x) , ∀x.drive(x)∧vs sim("drive", "walk")</p><formula xml:id="formula_7">→ walk(x)</formula><p>where vs sim is a similarity function that calculates the distributional similarity score between the two lexical predicates. All rules are assigned the same weight because all rules are equally important.</p><p>H: The hypothesis is represented as H → result(), and then PSL is queried for the truth value of the atom result(). For our example, the rule is: ∀x, y. guy(x) ∧ agent(y, x) ∧ walk(y) → result().</p><p>Priors: A low prior is given to all predicates. This encourages the truth values of ground atoms to be zero, unless there is evidence to the con- trary.</p><p>For each STS pair of sentences S 1 , S 2 , we run PSL twice, once where T = S 1 , H = S 2 and another where T = S 2 , H = S 1 , and output the two scores. To produce a final similarity score, we train a regressor to learn the mapping between the two PSL scores and the overall similarity score. As in <ref type="bibr" target="#b4">Beltagy et al., (2013)</ref> we use Additive Re- gression <ref type="bibr" target="#b13">(Friedman, 2002</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Changing Conjunction</head><p>As mentioned above, Lukasiewicz's formula for conjunction is very restrictive and does not work well for STS. For example, for T: "A man is driv- ing" and H: "A man is driving a car", if we use the standard PSL formula for conjunction, the output value is zero because there is no evidence for a car and max(0, X + 0 − 1) = 0 for any truth value 0 ≤ X ≤ 1. However, humans find these sen- tences to be quite similar.</p><p>Therefore, we introduce a new averaging inter- pretation of conjunction that we use for the hy- pothesis H. The truth value for a conjunction is defined as</p><formula xml:id="formula_8">I(p 1 ∧ .... ∧ p n ) = 1 n n i=1 I(p i ).</formula><p>This averaging function is linear, and the result is a valid truth value in the interval <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>, therefore this change is easily incorporated into PSL with- out changing the complexity of inference which remains a linear-programming problem.</p><p>It would perhaps be even better to use a weighted average, where weights for different components are learned from a supervised train- ing set. This is an important direction for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Grounding Process</head><p>Grounding is the process of instantiating the vari- ables in the quantified rules with concrete con- stants in order to construct the nodes and links in the final graphical model. In principle, ground- ing requires instantiating each rule in all possible ways, substituting every possible constant for each variable in the rule. However, this is a combinato- rial process that can easily result in an explosion in the size of the final network. Therefore, PSL em- ploys a "lazy" approach to grounding that avoids the construction of irrelevant groundings. If there is no evidence for one of the antecedents in a par- ticular grounding of a rule, then the normal PSL formula for conjunction guarantees that the rule is</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Heuristic Grounding</head><p>Input: r body = a 1 ∧ .... ∧ a n : antecedent of a rule with average interpretation of conjunction Input: V : set of variables used in r body Input: Ant(v i ): subset of antecedents a j con- taining variable v i Input: Const(v i ): list of possible constants of variable v i Input: Gnd(a i ): set of ground atoms of a i . Input <ref type="figure">: GndConst(a, g, v)</ref>: takes an atom a, grounding g for a, and variable v, and returns the constant that substitutes v in g Input: gnd limit: limit on the number of groundings 1: for all v i ∈ V do 2:</p><formula xml:id="formula_9">for all C ∈ Const(v i ) do 3: score(C) = a∈Ant(v i ) (max I(g)) for g ∈ Gnd(a) ∧ GndConst(a, g, v i ) = C 4:</formula><p>end for <ref type="bibr">5:</ref> sort Const(v i ) on scores, descending 6: end for 7: return For all v i ∈ V , take the Cartesian- product of the sorted Const(v i ) and return the top gnd limit results trivially satisfied (I(r) = 1) since the truth value of the antecedent is zero. Therefore, its distance to satisfaction is also zero, and it can be omitted from the ground network without impacting the result of MPE inference. However, this technique does not work once we switch to using averaging to interpret conjunc- tions. For example, given the rule ∀x. p(x) ∧ q(x) → t() and only one piece of evidence p(C) there are no relevant groundings because there is no evidence for q(C), and therefore, for normal PSL, I(p(C) ∧ q(C)) = 0 which does not affect I(t()). However, when using averaging with the same evidence, we need to generate the grounding p(C)∧q(C) because I(p(C)∧q(C)) = 0.5 which does affect I(t()).</p><p>One way to solve this problem is to eliminate lazy grounding and generate all possible ground- ings. However, this produces an intractably large network. Therefore, we developed a heuristic ap- proximate grounding technique that generates a subset of the most impactful groundings.</p><p>Pseudocode for this heuristic approach is shown in algorithm 1. Its goal is to find constants that participate in ground propositions with high truth value and preferentially use them to construct a limited number of groundings of each rule.</p><p>The algorithm takes the antecedents of a rule employing averaging conjunction as input. It also takes the grounding limit which is a threshold on the number of groundings to be returned. The al- gorithm uses several subroutines, they are:</p><p>• Ant(v i ): given a variable v i , it returns the set of rule antecedent atoms containing v i . E.g, for the rule: a(x) ∧ b(y) ∧ c(x), Ant(x) re- turns the set of atoms {a(x), c(x)}.</p><p>• Const(v i ): given a variable v i , it returns the list of possible constants that can be used to instantiate the variable v i .</p><p>• Gnd(a i ): given an atom a i , it returns the set of all possible ground atoms generated for a i .</p><p>• GndConst(a, g, v): given an atom a and grounding g for a, and a variable v, it finds the constant that substitutes for v in g. E.g, assume there is an atom a = a i (v 1 , v 2 ), and the ground atom g = a i (A, B) is one of its groundings. GndConst(a, g, v 2 ) would re- turn the constant B since it is the substitution for the variable v 2 in g.</p><p>Lines 1-6 loop over all variables in the rule. For each variable, lines 2-5 construct a list of constants for that variable and sort it based on a heuristic score. In line 3, each constant is assigned a score that indicates the importance of this constant in terms of its impact on the truth value of the overall grounding. A constant's score is the sum, over all antedents that contain the variable in question, of the maximum truth value of any grounding of that antecedent that contains that constant.</p><p>Pushing constants with high scores to the top of each variable's list will tend to make the over- all truth value of the top groundings high. Line 7 computes a subset of the Cartesian product of the sorted lists of constants, selecting constants in ranked order and limiting the number of results to the grounding limit.</p><p>One point that needs to be clarified about this approach is how it relies on the truth values of ground atoms when the goal of inference is to ac- tually find these values. PSL's inference is ac- tually an iterative process where in each itera- tion a grounding phase is followed by an opti- mization phase (solving the linear program). This loop repeats until convergence, i.e. until the truth values stop changing. The truth values used in each grounding phase come from the previous op- timization phase. The first grounding phase as- sumes only the propositions in the evidence pro- vided have non-zero truth values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>This section evaluates the performance of PSL on the STS task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluate our system on three STS datasets.</p><p>• msr-vid: Microsoft Video Paraphrase Cor- pus from STS 2012. The dataset consists of 1,500 pairs of short video descriptions collected using crowdsourcing (Chen and Dolan, 2011) and subsequently annotated for the STS task ( <ref type="bibr" target="#b0">Agirre et al., 2012</ref>). Half of the dataset is for training, and the second half is for testing.</p><p>• msr-par: Microsoft Paraphrase Corpus from STS 2012 task. The dataset is 5,801 pairs of sentences collected from news sources ( <ref type="bibr" target="#b12">Dolan et al., 2004</ref>). Then, for STS 2012, 1,500 pairs were selected and anno- tated with similarity scores. Half of the dataset is for training, and the second half is for testing.</p><p>• SICK: Sentences Involving Compositional Knowledge is a dataset collected for SemEval 2014. Only the training set is available at this point, which consists of 5,000 pairs of sen- tences. Pairs are annotated for RTE and STS, but we only use the STS data. Training and testing was done using 10-fold cross valida- tion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Systems Compared</head><p>We compare our PSL system with several others.</p><p>In all cases, we use the distributional word vec- tors employed by <ref type="bibr" target="#b4">Beltagy et al. (2013)</ref> based on context windows from Gigaword.</p><p>• vec-add: Vector Addition ( <ref type="bibr" target="#b21">Landauer and Dumais, 1997</ref>). We compute a vector rep- resentation for each sentence by adding the distributional vectors of all of its words and measure similarity using cosine. This is a simple yet powerful baseline that uses only distributional information.</p><p>• vec-mul: Component-wise Vector Multipli- cation ( <ref type="bibr" target="#b24">Mitchell and Lapata, 2008)</ref>. The same as vec-add except uses component- wise multiplication to combine word vectors.</p><p>• MLN: The system of <ref type="bibr" target="#b4">Beltagy et al. (2013)</ref>, which uses Markov logic instead of PSL for probabilistic inference. MLN inference is very slow in some cases, so we use a 10 minute timeout. When MLN times out, it backs off to a simpler sentence representation as explained in section 2.6.</p><p>• PSL: Our proposed PSL system for combin- ing logical and distributional information.</p><p>• PSL-no-DIR: Our PSL system without dis- tributional inference rules(empty knowledge base). This system uses PSL to compute sim- ilarity of logical forms but does not use dis- tributional information on lexical or phrasal similarity. It tests the impact of the proba- bilistic logic only</p><p>• PSL+vec-add: PSL ensembled with vec- add. Ensembling the MLN approach with a purely distributional approach was found to improve results ( <ref type="bibr" target="#b4">Beltagy et al., 2013</ref>), so we also tried this with PSL. The methods are en- sembled by using both entailment scores of both systems as input features to the regres- sion step that learns to map entailment scores to STS similarity ratings. This way, the train- ing data is used to learn how to weight the contribution of the different components.</p><p>• PSL+MLN: PSL ensembled with MLN in the same manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiments</head><p>Systems are evaluated on two metrics, Pearson correlation and average CPU time per pair of sen- tences.</p><p>• Pearson correlation: The Pearson correlation between the system's similarity scores and the human gold-standards.</p><p>• CPU time: This metric only applies to MLN and PSL. The CPU time taken by the infer- ence step is recorded and averaged over all pairs in each of the test datasets. In many cases, MLN inference is very slow, so we timeout after 10 minutes and report the num- ber of timed-out pairs on each dataset.  We also evaluated the effect of changing the grounding limit on both Pearson correlation and CPU time for the msr-par dataset. Most of the sentences in msr-par are long, which results is large number of groundings, and limiting the num- ber of groundings has a visible effect on the over- all performance. In the other two datasets, the sentences are fairly short, and the full number of groundings is not large; therefore, changing the grounding limit does not significantly affect the re- sults. <ref type="table">Table 1</ref> shows the results for Pearson correlation. PSL out-performs the purely distributional base- lines (vec-add and vec-mul) because PSL is able to combine the information available to vec-add and vec-mul in a better way that takes sentence structure into account. PSL also outperforms the unaided probabilistic-logic baseline that does not use distributional information (PSL-no-DIR). PSL-no-DIR works fairly well because there is significant overlap in the exact words and struc- ture of the paired sentences in the test data, and PSL combines the evidence from these similari- ties effectively. In addition, PSL always does sig- nificantly better than MLN, because of the large <ref type="figure">Figure 1</ref>: Effect of PSL's grounding limit on the correlation score for the msr-par dataset number of timeouts, and because the conjunction- averaging in PSL is combining evidence bet- ter than MLN's average-combiner, whose perfor- mance is sensitive to various parameters. These results further support the claim that using prob- abilistic logic to integrate logical and distribu- tional information is a promising approach to natural-language semantics. More specifically, they strongly indicate that PSL is a more effective probabilistic logic for judging semantic similarity than MLNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results and Discussion</head><p>Like for MLNs ( <ref type="bibr" target="#b4">Beltagy et al., 2013)</ref>, en- sembling PSL with vector addition improved the scores a bit, except for msr-par where vec-add's performance is particularly low. However, this en- semble still does not beat the state of the art <ref type="bibr">(Bär et al., 2012</ref>) which is a large ensemble of many dif- ferent systems. It would be informative to add our system to their ensemble to see if it could improve it even further. <ref type="table" target="#tab_1">Table 2</ref> shows the CPU time for PSL and MLN. The results clearly demonstrate that PSL is an or- der of magnitude faster than MLN. <ref type="figure">Figures 1 and 2</ref> show the effect of changing the grounding limit on Pearson correlation and CPU time. As expected, as the grounding limit is in- creased, accuracy improves but CPU time also increases. However, note that the difference in scores between the smallest and largest grounding limit tested is not large, suggesting that the heuris- tic approach to limiting grounding is quite effec- tive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Future Work</head><p>As mentioned in Section 3.2, it would be good to use a weighted average to compute the truth <ref type="figure">Figure 2</ref>: Effect of PSL's grounding limit on CPU time for the msr-par dataset values for conjunctions, weighting some predi- cates more than others rather than treating them all equally. Appropriate weights for different com- ponents could be learned from training data. For example, such an approach could learn that the type of an object determined by a noun should be weighted more than a property specified by an ad- jective. As a result, "black dog" would be appro- priately judged more similar to "white dog" than to "black cat."</p><p>One of the advantages of using a probabilis- tic logic is that additional sources of knowledge can easily be incorporated by adding additional soft inference rules. To complement the soft in- ference rules capturing distributional lexical and phrasal similarities, PSL rules could be added that encode explicit paraphrase rules, such as those mined from monolingual text <ref type="bibr" target="#b6">(Berant et al., 2011)</ref> or multi-lingual parallel text ( <ref type="bibr" target="#b14">Ganitkevitch et al., 2013)</ref>. This paper has focused on STS; however, as shown by <ref type="bibr" target="#b4">Beltagy et al. (2013)</ref>, probabilistic logic is also an effective approach to recognizing tex- tual entailment (RTE). By using the appropriate functions to combine truth values for various log- ical connectives, PSL could also be adapted for RTE. Although we have shown that PSL outper- forms MLNs on STS, we hypothesize that MLNs may still be a better approach for RTE. However, it would be good to experimentally confirm this in- tuition. In any case, the high computational com- plexity of MLN inference could mean that PSL is still a more practical choice for RTE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper has presented an approach that uses Probabilistic Soft Logic (PSL) to determine Se- mantic Textual Similarity (STS). The approach uses PSL to effectively combine logical seman- tic representations of sentences with soft infer- ence rules for lexical and phrasal similarities com- puted from distributional information. The ap- proach builds upon a previous method that uses Markov Logic (MLNs) for STS, but replaces the probabilistic logic with PSL in order to improve the efficiency and accuracy of probabilistic infer- ence. The PSL approach was experimentally eval- uated on three STS datasets and was shown to out- perform purely distributional baselines as well as the MLN approach. The PSL approach was also shown to be much more scalable and efficient than using MLNs</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Average CPU time per STS pair, and 
number of timed-out pairs in MLN with a 10 
minute time limit. PSL's grounding limit is set to 
10,000 groundings. 

</table></figure>

			<note place="foot" n="1"> Mitchell and Lapata (2008) give an amusing example of two sentences that consist of all the same words, but are very different in their meaning: (a) It was not the sales manager who hit the bottle that day, but the office worker with the serious drinking problem. (b) That day the office manager, who was drinking, hit the problem sales worker with a bottle, but it was not serious.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported by the DARPA DEFT program under AFRL grant FA8750-13-2-0026. Any opinions, findings, and conclusions or recom-mendations expressed in this material are those of the author and do not necessarily reflect the view of DARPA, DoD or the US government. Some ex-periments were run on the Mastodon Cluster sup-ported by NSF Grant EIA-0303609.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semeval-2012 task 6: A pilot on semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Semantic Evaluation</title>
		<meeting>Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>SemEval-12</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hinge-loss Markov random fields: Convex inference for structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>London</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Uncertainty in Artificial Intelligence (UAI-13)</title>
		<meeting>Uncertainty in Artificial Intelligence (UAI-13)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">UKP: Computing semantic textual similarity by combining multiple content similarity measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bär</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Zesch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Semantic Evaluation</title>
		<meeting>Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>SemEval-12</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuong</forename><surname>Islam Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Boleda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Erk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep semantics with probabilistic logical form</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markov</forename><surname>Montague Meets</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Joint Conference on Lexical and Computational Semantics (*SEM-13)</title>
		<meeting>the Second Joint Conference on Lexical and Computational Semantics (*SEM-13)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Global learning of typed entailment rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association for Computational Linguistics (ACL-11)</title>
		<meeting>Association for Computational Linguistics (ACL-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semantic parsing on Freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Wide-coverage semantic analysis with Boxer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Semantics in Text Processing</title>
		<meeting>Semantics in Text Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>STEP-08</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Probabilistic Similarity Logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Broecheler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lilyana</forename><surname>Mihalkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Uncertainty in Artificial Intelligence (UAI-20)</title>
		<meeting>Uncertainty in Artificial Intelligence (UAI-20)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Collecting highly parallel data for paraphrase evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association for Computational Linguistics (ACL-11)</title>
		<meeting>Association for Computational Linguistics (ACL-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Parsing the WSJ using CCG and log-linear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association for Computational Linguistics (ACL-04)</title>
		<meeting>Association for Computational Linguistics (ACL-04)</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Linguistics (COLING-04)</title>
		<meeting>the International Conference on Computational Linguistics (COLING-04)</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Stochastic gradient boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jerome H Friedman</surname></persName>
		</author>
		<idno>CSDA-02</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Statistics &amp; Data Analysis</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">PPDB: The paraphrase database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT-13)</title>
		<meeting>North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT-13)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Integrating logical representations with probabilistic information using Markov logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computational Semantics</title>
		<meeting>International Conference on Computational Semantics</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>IWCS-11</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Experimental support for a categorical compositional distributional model of meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Towards a formal distributional semantics: Simulating logical calculi with tensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Second Joint Conference on Lexical and Computational Semantics (*SEM 2013)</title>
		<meeting>Second Joint Conference on Lexical and Computational Semantics (*SEM 2013)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">From Discourse to Logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Kamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Reyle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>Kluwer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A short introduction to Probabilistic Soft Logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelika</forename><surname>Kimmig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><surname>Broecheler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS Workshop on Probabilistic Programming: Foundations and Applications (NIPS Workshop-12)</title>
		<meeting>NIPS Workshop on Probabilistic Programming: Foundations and Applications (NIPS Workshop-12)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Scaling semantic parsers with on-the-fly ontology matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A solution to Plato&apos;s problem: The Latent Semantic Analysis theory of the acquisition, induction, and representation of knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Combined distributional and logical semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Producing high-dimensional semantic spaces from lexical cooccurrence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Curt</forename><surname>Burgess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Behavior Research Methods, Instruments, and Computers</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Vector-based models of semantic composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association for Computational Linguistics (ACL08)</title>
		<meeting>Association for Computational Linguistics (ACL08)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Composition in distributional models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cognitive Science</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Universal grammar. Theoria</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Montague</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1970" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="373" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Exploiting causal independence in Markov logic networks: Combining undirected and directed models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Sriraam Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasad</forename><surname>Lowd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Tadepalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jude</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shavlik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference in Machine Learning (ECML-10)</title>
		<meeting>European Conference in Machine Learning (ECML-10)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
		<title level="m">Markov logic networks. Machine Learning</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="107" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">From frequency to meaning: Vector space models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
