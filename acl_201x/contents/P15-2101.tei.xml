<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reducing infrequent-token perplexity via variational corpora</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusheng</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northwestern University Evanston</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research Yorktown Heights</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Maryland College Park</orgName>
								<address>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">#</forename><surname>Pranjal Daga</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northwestern University Evanston</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research Yorktown Heights</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Maryland College Park</orgName>
								<address>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Agrawal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northwestern University Evanston</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research Yorktown Heights</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Maryland College Park</orgName>
								<address>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Choudhary</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northwestern University Evanston</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research Yorktown Heights</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Maryland College Park</orgName>
								<address>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Reducing infrequent-token perplexity via variational corpora</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="609" to="615"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recurrent neural network (RNN) is recognized as a powerful language model (LM). We investigate deeper into its performance portfolio, which performs well on frequent grammatical patterns but much less so on less frequent terms. Such portfolio is expected and desirable in applications like autocomplete, but is less useful in social content analysis where many creative, unexpected usages occur (e.g., URL insertion). We adapt a generic RNN model and show that, with variational training corpora and epoch unfolding, the model improves its performance for the task of URL insertion suggestions.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Just 135 most frequent words account for 50% text of the entire Brown corpus <ref type="bibr" target="#b5">(Francis and Kucera, 1979)</ref>. But over 44% (22,010 out of 49,815) of Brown's vocabulary are hapax legomena <ref type="bibr">1</ref> . The in- tricate relationship between vocabulary words and their utterance frequency results in some impor- tant advancements in natural language process- ing (NLP). For example, tf-idf results from rules applied to word frequencies in global and local context <ref type="bibr" target="#b14">(Manning and Schütze, 1999)</ref>. A com- mon preprocessing step for tf-idf is filtering rare words, which is usually justified for two reasons. First, low frequency cutoff promises computa- tional speedup due to Zipf's law <ref type="bibr">(1935)</ref>. Second, many believe that most NLP and machine learning algorithms demand repetitive patterns and reoc- currences, which are by definition missing in low frequency words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Should infrequent words be filtered?</head><p>Infrequent words have high probability of becom- ing frequent as we consider them in a larger con- 1 Words appear only once in corpus.</p><p>text (e.g., Ishmael, the protagonist name in Moby- Dick, appears merely once in the novel's dialogues but is a highly referenced word in the discus- sions/critiques around the novel). In many modern NLP applications, context grows constantly: fresh news articles come out on CNN and New York Times everyday; conversations on Twitter are up- dated in real time. In processing online social me- dia text, it would seem premature to filter words simply due to infrequency, the kind of infrequency that can be eliminated by taking a larger corpus available from the same source.</p><p>To further undermine the conventional justifica- tion, computational speedup is attenuated in RNN- based LMs (compared to n-gram LMs), thanks to modern GPU architecture. We train a large RNN- LSTM (long short-term memory unit) <ref type="bibr" target="#b9">(Hochreiter and Schmidhuber, 1997</ref>) model as our LM on two versions of Jane Austen's complete works. Deal- ing with 33% less vocabulary in the filtered ver- sion, the model only gains marginally on running time or memory usage. In <ref type="table">Table 1</ref>.1, "Filtered cor- pus" filters out all the hapax legomena in "Full cor- pus". Since RNN LMs suffer only small penalty in keeping the full corpus, can we take advantage of this situation to improve the LM?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Full corpus Filtered corpus</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Improving performance portfolio of LM</head><p>One improvement is LM's performance portfo- lio. A LM's performance is usually quantified as perplexity, which is exponentialized negative log- likelihood in predictions.</p><p>For our notation, let V X denote the vocabu- lary of words that appear in a text corpus X = {x 1 , x 2 , . . .}. Given a sequence x 1 , x 2 , . . . , x m−1 , where each x ∈ V X , the LM predicts the next in sequence, x m ∈ V X , as a probability distribu- tion over the entire vocabulary V (its prediction denoted as p). If v m ∈ V X is the true token at position m, the model's perplexity at index m is quantified as exp(− ln(p[v m ])). The training goal is to minimize average perplexity across X.</p><p>However, a deeper look into perplexity beyond corpus-wide average reveals interesting findings. Using the same model setting as for <ref type="table">Table 1</ref>.1, <ref type="figure">Figure 1</ref> illustrates the relationship between word- level perplexity and its frequency in corpus. In general, the less frequent a word appears, the more unpredictable it becomes. In <ref type="table">Table 1</ref>.2, the trained model achieves an average perplexity of 78 on filtered corpus. But also shown in <ref type="table">Table  1</ref>.2, many common words register with perplexity over 1,000, which means they are practically un- predictable. More details are summarized in <ref type="table">Table  1</ref>.2. The LM achieves exceptionally low perplex- ity on words such as &lt;apostr.&gt;s ('s, the posses- sive case), &lt;comma&gt; (, the comma). And these tokens' high frequencies in corpus have promised the model's average performance. Meanwhile, the LM has bafflingly high perplexity on common- place words such as read and considering. Figure 1: (best viewed in color) We look at word level perplexity with respect to the word frequency in corpus. The less frequent a word appears, the more unpredictable it becomes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>We describe a novel approach of constructing and utilizing pre-training corpus that eventually reduce LMs's high perplexity on rare tokens. The stan- dard way to utilize a pre-training corpus W is to  <ref type="table">Table 2</ref>: A close look at RNN-LSTM's perplexity at word level. "Perplexity 1" is model perplexity based on filtered corpus (c.f., <ref type="table">Table 1</ref>.1) and "Per- plexity 2" is based on full corpus. first train the model on W then fine-tune it on tar- get corpus X. Thanks to availability of text, W can be orders of magnitude larger than X, which makes pre-training on W challenging. A more efficient way to utilize W is to construct variational corpora based on X and W . In the fol- lowing subsections, we first describe how replace- ment tokens are selected from a probability mass function (pmf), which is built from W ; then ex- plain how the variational corpora variates with re- placement tokens through epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Learn from pre-training corpus</head><p>One way to alleviate the impact from infrequent vocabulary is to expose the model to a larger and overarching pre-training corpus ( <ref type="bibr" target="#b4">Erhan et al., 2010)</ref>, if available. Let W be a larger corpus than X and assume that V X ⊆ V W . For exam- ple, if X is Herman Melville's Moby-Dick, W can be Melville's complete works. Further, we use V X,1 to denote the subset of V X that are ha- pax legonema in corpus X; similarly, V X,n (for n = 2, 3, . . .) denotes the subset of V X that occur n times in X. Many hapax legomena in V X,1 are likely to become more frequent tokens in V W .</p><p>Suppose that x ∈ V X,1 . Denoted by ReplacePMF(W, V W , x) in Algorithm 1, we rep- resent x as a probability mass function (pmf) over</p><formula xml:id="formula_0">{x 1 , x 2 , . . .},</formula><p>where each x i is selected from V W ∩ V X,n for n &gt; 1 using one of the two methods be- low. For illustration purpose, suppose the hapax legomenon, x, in question is matrimonial:</p><p>1) e.g., matrimony. Words that have very high literal similarity with x. We measure literal sim- ilarity using Jaro-Winkler measure, which is an empirical, weighted measure based on string edit distance. We set the measure threshold very high (&gt; 0.93), which minimizes false positives as well as captures many hapax legonema due to adv./adj., pl./singular (e.g, -y/-ily and -y/-ies).</p><p>2) e.g., marital Words that are direct syno/hypo- nyms to x in the WordNet <ref type="bibr" target="#b16">(Miller, 1995)</ref>.</p><p>getContextAround(x ) function in Algorithm 1 simply extracts symmetric context words from both left and right sides of x . Although the in- vestigated LM only uses left context in predicting word x , context right of x is still useful informa- tion in general. Given a context word c right of x , the LM can learn x 's predictability over c, which is beneficial to the corpus-wide perplexity reduc- tion.</p><p>In practice, we select no more than 5 substitu- tion words from each method above. The prob- ability mass on each x i is proportional to its fre- quency in W and then normalized by softmax:</p><formula xml:id="formula_1">pmf(x i ) = freq(x i )/ 5 k=1 freq(x k )</formula><p>. This sub- stitution can help LMs learn better because we re- place the un-trainable V X,1 tokens with tokens that can be trained from the larger corpus W . In con- cept, it is like explaining a new word to school kids by defining it using vocabulary words in their ex- isting knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Unfold training epochs</head><p>Epoch in machine learning terminology usually means a complete pass of the training dataset. many iterative algorithms take dozens of epochs on the same training data as they update the model's weights with smaller and smaller adjust- ments through the epochs.</p><p>We refer to the the training process proposed in <ref type="figure" target="#fig_1">Figure 2</ref> (b) as "variational corpora". Com- pared to the traditional structure in <ref type="figure" target="#fig_1">Figure 2 (a)</ref>, the main advantage of using variational corpora is the ability to freely adjust the corpus at each ver- sion. Effectively, we unfold the training into sep- arate epochs. This allows us to gradually incorpo- rate the replacement tokens without severely dis- torting the target corpus X, which is the learning goal. In addition, variational corpora can further regularize the training of LM in batch mode <ref type="bibr" target="#b18">(Srivastava et al., 2014</ref>).</p><p>Algorithm 1 constructs variational corpora X(s) at epoch s. Assuming X(s + 1) being avail- able, Algorithm 1 appends snippets, which are sampled from W , into X(s) for the sth epoch. For the last epoch s = S, X(S) = X. As the epoch   7.6 (0.09) 7.6 (0.09) 7.0 (0.09) 7.0 (0.10) all tokens 82.1 (2.0) 77.9 (1.9) 68.6 (2.1) 68.9 (2.1) GPU memory 959MB 783MB 1.8GB 971MB running time 1,446 sec 1,181 sec 9,061 sec 6,960 sec  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Perplexity reduction</head><p>We validate our method in <ref type="table" target="#tab_3">Table 3</ref> by showing per- plexity reduction on infrequent words. We split Jane Austen's novels (0.7 million words) as tar- get corpus X and test corpus, and her contem- poraries' novels 4 as pre-training corpus W (2.7 million words). In <ref type="table" target="#tab_3">Table 3</ref>, nofilter is the unfil- tered corpus; 3filter replaces all tokens in V X,3 by &lt;unk&gt;; ptw performs naive pre-training on W then on X; vc performs training with the proposed variational corpora. Our LM implements the RNN training as described in ( <ref type="bibr">Zaremba et al., 2014</ref>). Ta- ble 3 also illustrates the GPU memory usage and running time of the compared methods and shows that vc is more efficient than simply ptw. vc has the best performance on low-frequency words by some margin. ptw is the best on frequent words because of its access to a large pre-training <ref type="bibr">3</ref> Favim.com is a website for sharing crafts, creativity ideas. Esty.com is a e-commerce website for trading hand- made crafts. Nelly.com is Scandinavia's largest online fash- ion store. Macy's a US-based department store. Harrod's is a luxury department store in London. <ref type="bibr">4</ref> Dickens and the Bronte sisters corpus. But somewhat to our surprise, ptw per- forms badly on low-frequency words, which we reckon is due to the rare words introduced in W : while pre-training on W helps reduce perplexity of words in V X,1 but also introduces additional ha- pax legomena in V W,1 \ V X,1 . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Locating URLs in Pinterest captions</head><p>Beyond evaluations in <ref type="table" target="#tab_3">Table 3</ref>. We apply our method to locate URLs in over 400,000 Pinterest captions. Unlike Facebook, Twitter, Pinterest is not a "social hub" but rather an interest-discovery site ( <ref type="bibr" target="#b12">Linder et al., 2014;</ref><ref type="bibr" target="#b22">Zhong et al., 2014)</ref>. To maximally preserve user experience, postings on Pinterest embed URLs in a natural, nonintrusive manner and a very small portion of the posts con- tain URLs.</p><p>In <ref type="figure" target="#fig_3">Figure 3</ref>, we ask the LM to suggest a po- sition for the URL in the context and verify the suggest with test data in each category. For ex- ample, the model is presented with a sequence of tokens: find, more, top, dresses, at, afford- able, prices, &lt;punctuation&gt;, visit, and is asked to predict if the next token is an URL link. In the given example, plausible tokens after visit can be either &lt;http://macys.com&gt; or nearest, Macy, &lt;apostr.&gt;s, store. The proposed vc mechanism outperforms others in 5 of the 6 categories. In <ref type="figure" target="#fig_3">Figure 3</ref>, accuracy is measured as the percentage of correctly suggested positions. Any prediction next to or close to the correct position is counted as incorrect.</p><p>In <ref type="table" target="#tab_4">Table 4</ref>, we list some of the false nega- tive and false positive errors made by the LM. Many URLs on Pinterest are e-commerce URLs and the vendors often also have physical stores. So in predicting such e-commerce URLs, some mis- takes are "excusable" because the LM is confused whether the upcoming token should be an URL (web store) or the brand name (physical store) (e.g, http://macys.com vs. Macy's).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related work</head><p>Recurrent neural network (RNN) is a type of neu- ral sequence model that have high capacity across various sequence tasks such as language model- ing ( <ref type="bibr" target="#b0">Bengio et al., 2000</ref>), machine translation ( <ref type="bibr" target="#b13">Liu et al., 2014</ref>), speech recognition ( <ref type="bibr" target="#b8">Graves et al., 2013)</ref>. Like other neural network models (e.g., feed-forward), RNNs can be trained using back- propogation algorithm <ref type="bibr" target="#b19">(Sutskever et al., 2011)</ref>. Recently, the authors in ( <ref type="bibr">Zaremba et al., 2014</ref>) successfully apply dropout, an effective regular- ization method for feed-forward neural networks, to RNNs and achieve strong empirical improve- ments.</p><p>Reducing perplexity on text corpus is proba- bly the most demonstrated benchmark for mod- ern language models (n-gram based and neural models alike) ( <ref type="bibr" target="#b2">Chelba et al., 2013;</ref><ref type="bibr" target="#b3">Church et al., 2007;</ref><ref type="bibr" target="#b7">Goodman and Gao, 2000;</ref><ref type="bibr" target="#b6">Gao and Zhang, 2002</ref>). Based on Zipf's law <ref type="bibr" target="#b23">(Zipf, 1935)</ref>, a fil- tered corpus greatly reduces the vocabulary size and computation complexity. Recently, a rigor- ous study <ref type="bibr" target="#b11">(Kobayashi, 2014</ref>) looks at how per- plexity can be manipulated by simply supplying the model with the same corpus reduced to vary- ing degrees. <ref type="bibr" target="#b11">Kobayashi (2014)</ref> describes his study from a macro point of view (i.e., the overall corpus level perplexity). In this work, we present, at word level, the correlation between perplexity and word frequency.</p><p>Token rarity is a long-standing issue with n- gram language models <ref type="bibr" target="#b14">(Manning and Schütze, 1999</ref>). Katz smoothing <ref type="bibr" target="#b10">(Katz, 1987)</ref> and Kneser- Ney based smoothing methods <ref type="bibr" target="#b20">(Teh, 2006</ref>) are well known techniques for addressing sparsity in n-gram models. However, they are not directly used to resolve unigram sparsity.</p><p>Using word morphology information is another way of dealing with rare tokens <ref type="bibr" target="#b1">(Botha and Blunsom, 2014</ref>). By decomposing words into mor- phemes, the authors in ( <ref type="bibr" target="#b1">Botha and Blunsom, 2014)</ref> are able to learn representations on the morpheme level and therefore scale the language modeling to unseen words as long as they are made of previ- ously seen morphemes. Shown in their work, this technique works with character-based language in addition to English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Acknowledgements</head><p>This work is supported in part by the following grants: NSF awards CCF-1029166, IIS-1343639, and CCF-1409601; DOE award DESC0007456; AFOSR award FA9550-12-1-0458; NIST award 70NANB14H012.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions &amp; future work</head><p>This paper investigates the performance portfolio of popular neural language models. We propose a variational training scheme that has the advan- tage of a large pre-training corpus but without us- ing as much computing resources. On low fre- quency words, our proposed scheme also outper- forms naive pre-training.</p><p>In the future, we want to incorporate WordNet knowledge to further reduce perplexity on infre- quent words.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Word frequency in corpus Word level average perplexity word perplexity word frequency (log scale)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Unfold the training process in units of epochs. (a) Typical flow where model parses the same corpus at each epoch. (b) The proposed training architecture with variational corpora to incorporate the substitution algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Freq</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Accuracy of suggested URL positions across different categories of Pinterest captions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>.</head><label></label><figDesc></figDesc><table>nofilter 
3filter 
ptw 
vc 
10 
28,542 (668.1) 23,649 (641.2) 27,986 (1,067.2) 20,994 (950.9) 
100 
1,180.3 (21.7) 1,158.2 (19.2) 735.8 (29.8) 
755.8 (31.5) 
1K 
163.2 (12.9) 
163.9 (12.2) 
138.5 (14.1) 
137.7 (15.7) 
5K 
47.5 (3.3) 
47.2 (3.1) 
40.2 (3.2) 
40.2 (3.3) 
10K 
16.4 (0.31) 
16.7 (0.29) 
14.4 (0.42) 
14.1 (0.41) 
40K 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Experiments compare average perplexity produced by the proposed variational corpora approach 
and other methods on a same test corpus. Bold fonts indicate best. "Freq." indicates the average corpus-
frequency (e.g., Freq.=1K means that words in this group, on average, appear 1,000 times in corpus). 
Perplexity numbers are averaged over 5 runs with standard deviation reported in parentheses. GPU 
memory usage and running time are also reported for each method. 

Err. type Context before 
True token 
LM prediction 
False neg. &lt;unk&gt;, via, &lt;unk&gt;, banana, muffin, chocolate, 
URL to a cooking blog recipe 
False neg. sewing, ideas, &lt;unk&gt;, inspiring, picture, on, 
URL to favim.com 
esty 
False neg. nike, sports, fashion, &lt;unk&gt;, women, &lt;unk&gt;, 
URL to nelly.com 
macy 
False pos. new, york, yankees, endless, summer, tee, &lt;unk&gt;, 
shop 
&lt;url&gt; 
False pos. take, a, rest, from, your, #harrodssale, 
shopping 
&lt;url&gt; 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc>False positives and false negatives predicted by the model in the Pinterest application. The context words preceding to token in questions are provided for easier analysis 3 .</figDesc><table></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural 613 probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Departement</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Centre</forename><surname>&amp;apos;informatique Et Recherche Operationnelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De Recherche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Compositional morphology for word representations and language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">A</forename><surname>Botha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing</title>
		<meeting>the 31th International Conference on Machine Learning, ICML 2014, Beijing</meeting>
		<imprint>
			<date type="published" when="2014-06-26" />
			<biblScope unit="page" from="1899" to="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">One billion word benchmark for measuring progress in statistical language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<pubPlace>Google</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
	<note>Thorsten Brants, Phillipp Koehn, and Tony Robinson</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Compressing trigram language models with golomb coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06-28" />
			<biblScope unit="page" from="199" to="207" />
		</imprint>
	</monogr>
	<note>EMNLP-CoNLL</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Why does unsupervised pre-training help deep learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Dumitru Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="625" to="660" />
			<date type="published" when="2010-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Brown corpus manual</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nelson</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Kucera</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979" />
			<pubPlace>Providence, Rhode Island, US</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Linguistics, Brown University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improving language model size reduction using better pruning criteria</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL &apos;02</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics, ACL &apos;02<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="176" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Language model size reduction by pruning and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth International Conference on Spoken Language Processing</title>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-10-16" />
			<biblScope unit="page" from="110" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Abdel-Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-05-26" />
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Estimation of probabilities from sparse data for the language model component of a speech recognizer. Acoustics, Speech and Signal Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="400" to="401" />
			<date type="published" when="1987-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Perplexity on reduced corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayato</forename><surname>Kobayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="797" to="806" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Everyday ideation: all of my ideas are on pinterest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rhema</forename><surname>Linder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clair</forename><surname>Snodgrass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andruid</forename><surname>Kerne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI Conference on Human Factors in Computing Systems, CHI&apos;14</title>
		<meeting><address><addrLine>Toronto, ON, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04-26" />
			<biblScope unit="page" from="2411" to="2420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A recursive recurrent neural network for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1491" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Foundations of Statistical Natural Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Wordnet: A lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
			<publisher>November</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning, ICML 2013</title>
		<meeting>the 30th International Conference on Machine Learning, ICML 2013<address><addrLine>GA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generating text with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11), ICML &apos;11</title>
		<editor>Lise Getoor and Tobias Scheffer</editor>
		<meeting>the 28th International Conference on Machine Learning (ICML-11), ICML &apos;11<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="1017" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A bayesian interpretation of interpolated kneserney</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<title level="m">Ilya Sutskever, and Oriol Vinyals. 2014. Recurrent neural network regularization</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Social bootstrapping: how pinterest and last.fm social communities benefit by borrowing links from facebook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changtao</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Salehi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunil</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cobzarenco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishanth</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meeyoung</forename><surname>Cha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">23rd International World Wide Web Conference, WWW &apos;14</title>
		<meeting><address><addrLine>Seoul, Republic of Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04-07" />
			<biblScope unit="page" from="305" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The Psycho-biology of Language: An Introduction to Dynamic Philology. The MIT paperback series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Zipf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1935" />
			<pubPlace>Houghton Mifflin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
