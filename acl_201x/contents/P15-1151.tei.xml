<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:08+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Convolutional Architecture for Word Sequence Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Noah&apos;s Ark Lab, Huawei Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Noah&apos;s Ark Lab, Huawei Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">ADAPT Centre</orgName>
								<orgName type="department" key="dep2">School of Computing</orgName>
								<orgName type="institution">Dublin City University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Convolutional Architecture for Word Sequence Prediction</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1567" to="1576"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a convolutional neural network , named genCNN, for word sequence prediction. Different from previous work on neural network-based language modeling and generation (e.g., RNN or LSTM), we choose not to greedily summarize the history of words as a fixed length vector. Instead , we use a convolutional neural network to predict the next word with the history of words of variable length. Also different from the existing feed-forward networks for language mod-eling, our model can effectively fuse the local correlation and global correlation in the word sequence, with a convolution-gating strategy specifically designed for the task. We argue that our model can give adequate representation of the history, and therefore can naturally exploit both the short and long range dependencies. Our model is fast, easy to train, and readily parallelized. Our extensive experiments on text generation and n-best re-ranking in machine translation show that genCNN outperforms the state-of-the-arts with big margins.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Both language modeling ( <ref type="bibr" target="#b21">Wu and Khudanpur, 2003;</ref><ref type="bibr" target="#b14">Mikolov et al., 2010;</ref><ref type="bibr">Bengio et al., 2003</ref>) and text generation <ref type="bibr">(Axelrod et al., 2011</ref>) boil down to modeling the conditional proba- bility of a word given the proceeding words. Previously, it is mostly done through purely memory-based approaches, such as n-grams, which cannot deal with long sequences and has to use some heuristics (called smoothing) for rare ones. Another family of methods are based on distributed representations of words, which is usually tied with a neural-network (NN) ar- chitecture for estimating the conditional prob- abilities of words.</p><p>Two categories of neural networks have been used for language modeling: 1) recurrent neu- ral networks (RNN), and 2) feedfoward net- work (FFN):</p><p>• The RNN-based models, including its variants like LSTM, enjoy more popu- larity, mainly due to their flexible struc- tures for processing word sequences of ar- bitrary lengths, and their recent empiri- cal success <ref type="bibr" target="#b6">Graves, 2013)</ref>. We however argue that RNNs, with their power built on the recursive use of a relatively simple computation units, are forced to make greedy summarization of the history and consequently not effi- cient on modeling word sequences, which clearly have a bottom-up structures.</p><p>• The FFN-based models, on the other hand, avoid this difficulty by feeding di- rectly on the history. However, the FFNs are built on fully-connected networks, rendering them inefficient on capturing local structures of languages. Moreover their "rigid" architectures make it futile to handle the great variety of patterns in long range correlations of words.</p><p>We propose a novel convolutional architec- ture, named genCNN, as a model that can ef- ficiently combine local and long range struc- tures of language for the purpose of modeling conditional probabilities. genCNN can be di- rectly used in generating a word sequence (i.e., text generation) or evaluating the likelihood of word sequences (i.e., language modeling). We also show the empirical superiority of genCNN on both tasks over traditional n-grams and its RNN or FFN counterparts.</p><p>Notations: We will use V to denote the vo- cabulary, e t (∈ {1, · · · , |V|}) to denote the t th word in a sequence e 1:t</p><formula xml:id="formula_0">def = [e 1 , · · · , e t ]</formula><p>, and e (n) t if the sequence is further indexed by n.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview</head><p>As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, genCNN is overall re- cursive, consisting of CNN-based processing units of two types:</p><p>• αCNN as the "front-end", dealing with the history that is closest to the prediction;</p><p>• βCNNs (which can repeat), in charge of more "ancient" history.</p><p>Together, genCNN takes history e 1:t of arbi- trary length to predict the next word e t+1 with probability p(e t+1 |e 1:t ; ¯ Θ),</p><p>based on a representation φ(e 1:t ; ¯ Θ) produced by the CNN, and a |V|-class soft-max:</p><formula xml:id="formula_2">p(e t+1 |e 1:t ; ¯ Θ) ∝ e µ e t+1 φ(e 1:t )+be t+1 .<label>(2)</label></formula><p>genCNN is devised (tailored) fully for mod- eling the sequential structure in natural lan- guage, notably different from conventional CNN ( <ref type="bibr" target="#b12">Lawrence et al., 1997;</ref><ref type="bibr" target="#b8">Hu et al., 2014</ref>) in 1) its specifically designed weights-sharing strategy (in αCNN), 2) its gating design, and 3) certainly its recursive architectures. Also distinct from RNN, genCNN gains most of its processing power from the heavy-duty pro- cessing units (i.e.,αCNN and βCNNs), which follow a bottom-up information flow and yet can adequately capture the temporal structure in word sequence with its convolutional-gating architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">genCNN: Architecture</head><p>We start with discussing the convolutional ar- chitecture of αCNN as a stand-alone sentence model, and then proceed to the recursive struc- ture. After that we give a comparative analysis on the mechanism of genCNN.</p><p>αCNN, just like a normal CNN, has fixed architecture with predefined maximum words (denoted as L α ). History shorter than L α will filled with zero paddings, and history longer than that will be folded to feed to βCNN after it, as will be elaborated in Section 3.3. Similar to most other CNNs, αCNN alternates between convolution layers and pooling layers, and fi- nally a fully connected layer to reach the repre- sentation before soft-max, as illustrated by <ref type="figure" target="#fig_2">Fig- ure 2</ref>. Unlike the toyish example in <ref type="figure" target="#fig_2">Figure 2</ref>, in practice we use a larger and deeper αCNN with L α = 30 or 40, and two or three convolution layers (see Section 4.1). Different from con- ventional CNN, genCNN has 1) weight shar- ing strategy for convolution, and 2)"external" gating networks to replace the normal pooling mechanism, both of which are specifically de- signed for word sequence prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">αCNN: Convolution</head><p>Different from conventional CNN, the weights of convolution units in αCNN is only partially shared. More specifically, in the convolution units there are two types feature-maps: TIME- FLOW and the TIME-ARROW, illustrated re- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>/ / /</head><p>"dinner" "breakfast" "us" "the" … A 3-layer αCNN</p><p>Time-Flow Time-Arrow Gating Here the shadowed nodes stand for the TIME- ARROW feature-maps and the unfilled nodes for the TIME-FLOW.</p><p>spectively with the unfilled nodes and filled nodes in <ref type="figure" target="#fig_2">Figure 2</ref>. The parameters for TIME- FLOW are shared among different convolution units, while for TIME-ARROW the parame- ters are location-dependent. Intuitively, TIME- FLOW acts more like a conventional CNN (e.g., that in ( <ref type="bibr" target="#b8">Hu et al., 2014)</ref>), aiming to understand the overall temporal structure in the word se- quences; TIME-ARROW, on the other hand, works more like a traditional NN-based lan- guage model ( <ref type="bibr" target="#b20">Vaswani et al., 2013;</ref><ref type="bibr">Bengio et al., 2003)</ref>: with its location-dependent param- eters, it focuses on capturing the direction of time and prediction task. For sentence input x = {x 1 , · · · , x T }, the feature-map of type-f on Layer-is if f ∈ TIME-FLOW:</p><formula xml:id="formula_3">z (,f ) i (x) = σ(w (,f ) TFˆzTFˆ TFˆz (−1) i + b (,f ) TF ), (3) if f ∈ TIME-ARROW: z (,f ) i (x) = σ(w (,f,i) TAˆz TAˆ TAˆz (−1) i + b (,f,i) TA ), (4) where • z (,f ) i (x)</formula><p>gives the output of feature-map of type-f for location i in Layer-;</p><p>• σ(·) is the activation function, e.g., Sig- moid or Relu ( <ref type="bibr" target="#b3">Dahl et al., 2013)</ref> • w (,f ) TF denotes the location-independent parameters for f ∈TIME-FLOW on Layer- , while w (,f,i) TA stands for that for f ∈ TIME-ARROW and location i on Layer-;</p><formula xml:id="formula_4">• ˆ z (−1) i</formula><p>denotes the segment of Layer-−1 for the convolution at location i , whilê</p><formula xml:id="formula_5">whilê z (0) i def = [x i , x i+1 , · · · , x i+k 1 −1 ]</formula><p>concatenates the vectors for k 1 words from sentence input x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Gating Network</head><p>Previous CNNs, including those for NLP tasks ( <ref type="bibr" target="#b8">Hu et al., 2014;</ref><ref type="bibr" target="#b9">Kalchbrenner et al., 2014</ref>), take a straightforward convolution- pooling strategy, in which the "fusion" deci- sions (e.g., selecting the largest one in max- pooling) are based on the values of feature- maps. This is essentially a soft template match- ing, which works for tasks like classification, but undesired for maintaining the composition functionality of convolution. In this paper, we propose to use separate gating networks to re- lease the scoring duty from the convolution, and let it focus on composition. Similar idea has been proposed by <ref type="bibr" target="#b17">(Socher et al., 2011</ref>) for recursive neural networks on parsing, but never been combined with a convolutional structure.  Suppose we have convolution feature-maps on Layer-and gating (with window size = 2) on Layer-+ 1. For the j th gating win- dow (2j−1, 2j), we mergê z</p><formula xml:id="formula_6">(−1) 2j−1 andˆzandˆ andˆz (−1) 2j</formula><p>as the input (denoted as ¯ z () j ) for gating network, as illustrated in <ref type="figure" target="#fig_4">Figure 3</ref>. We use a separate gate for each feature-map, but follow a differ- ent parametrization strategy for TIME-FLOW and TIME-ARROW. With window size = 2, the gating is binary, we use a logistic regressor to determine the weights of two candidates. For f ∈ TIME-ARROW, with location-dependent w (,f,j) gate , the normalized weight for left side is</p><formula xml:id="formula_7">g (+1,f ) j = 1/(1 + e −w (,f,j) gate ¯ z () j ),</formula><p>while for For f ∈ TIME-FLOW, the parameters for the corresponding gating network, denoted as w (,f ) gate , are shared. The gated feature map is then a weighted sum to feature-maps from the two windows:</p><formula xml:id="formula_8">z (+1,f ) j = g (+1,f ) j z (,f ) 2j−1 + (1 − g (+1,f ) j )z (,f ) 2j</formula><p>. <ref type="formula">(5)</ref> We find that this gating strategy works signifi- cantly better than pooling directly over feature- maps, and slightly better than a hard gate ver- sion of Equation 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Recursive Architecture</head><p>As suggested early on in Section 2 and <ref type="figure" target="#fig_0">Fig- ure 1</ref>, we use extra CNNs with conventional weight-sharing, named βCNN, to summarize the history out of scope of αCNN. More specif- ically, the output of βCNN (with the same di- mension of word-embedding) is put before the first word as the input to the αCNN, as il- lustrated in <ref type="figure" target="#fig_5">Figure 4</ref>. Different from αCNN, βCNN is designed just to summarize the his- tory, with weight shared across its convolution units. In a sense, βCNN has only TIME-FLOW feature-maps. All βCNN are identical and re- cursively aligned, enabling genCNN to handle sentences with arbitrary length. We put a spe- cial switch after each βCNN to turn it off (re- placing a pading vector shown as "/" in <ref type="figure" target="#fig_5">Fig- ure 4 )</ref> when there is no history assigned to it. As the result, when the history is shorter than L α , the recursive structure reduces to αCNN.</p><p>In practice, 90+% sentences can be mod- eled by αCNN with L α = 40 and 99+% sen- tences can be contained with one extra βCNN. Our experiment shows that this recursive strat- egy yields better estimate of conditional den- sity than neglecting the out-of-scope history (Section 6.1.2). In practice, we found that a larger (greater L α ) and deeper αCNN works better than small αCNN and more recursion, which is consistent with our intuition that the convolutional architecture is better suited for modeling the sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Analysis</head><p>3.4.1 TIME-FLOW vs. TIME-ARROW Both conceptually and systemically, genCNN gives two interweaved treatments of word his- tory. With the globally-shared parameters in the convolution units, TIME-FLOW summa- rizes what has been said. The hierarchi- cal convolution+gating architecture in TIME- FLOW enables it to model the composition in language, yielding representation of segments at different intermediate layers. TIME-FLOW is aware of the sequential direction, inherited from the space-awareness of CNN, but it is not sensitive enough about the prediction task, due to the uniform weights in the convolution. On the other hand, TIME-ARROW, living in location-dependent parameters of convolu- tion units, acts like an arrow pin-pointing the prediction task. TIME-ARROW has predictive power all by itself, but it concentrates on cap- turing the direction of time and consequently short on modelling the long-range dependency. TIME-FLOW and TIME-ARROW have to work together for optimal performance in pre- dicting what is going to be said. This intuition has been empirically verified, as our experi- ments have demonstrated that TIME-FLOW or TIME-ARROW alone perform inferiorly. One can imagine, through the layer-by-layer convo- lution and gating, the TIME-ARROW gradually picks the most relevant part from the represen- tation of TIME-FLOW for the prediction task, even if that part is long distance ahead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">genCNN vs. RNN-LM</head><p>Different from RNNs, which recursively ap- plies a relatively simple processing units, genCNN gains its ability on sequence mod- eling mostly from its flexible and power- ful bottom-up and convolution architecture. genCNN takes the "uncompressed" history, therefore avoids</p><p>• the difficulty in finding the representation for history, e.g., those end in the middle of a chunk (e.g.,"the cat sat on the"),</p><p>• the damping effort in RNN when the history-summarizing hidden state is up- dated at each time stamp, which renders the long-range memory rather difficult, both of which can only be partially ameliorated with complicated design of gates (Hochreiter and Schmidhuber, 1997) and or more heavy processing units (essentially a fully connected DNN) ( ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">genCNN: Training</head><p>The parameters of a genCNN ¯ Θ consists of the parameters for CNN Θ nn , word-embedding Θ embed , and the parameters for soft-max Θ sof tmax . All the parameters are jointly learned by maximizing the likelihood of ob- served sentences. Formally the log-likelihood of sentence</p><formula xml:id="formula_9">S n ( def = [e (n) 1 , e (n) 2 , · · · , e (n) Tn ]) is log p(S n ; ¯ Θ) = Tn t=1 log p(e (n) t |e (n) 1:t−1 ; ¯ Θ),</formula><p>which can be trivially split into T n training in- stances during the optimization, in contrast to the training of RNN that requires unfolding through time due to the temporal-dependency of the hidden states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>Architectures: In all of our experiments (Section 5 and 6) we set the maximum words for αCNN to be 30 and that for βCNN to be 20. αCNN have two convolution layers (both con- taining TIME-FLOW and TIME-ARROW con- volution) and two gating layers, followed by a fully connected layer (400 dimension) and then a soft-max layer. The numbers of feature- maps for TIME-FLOW are respectively 150 (1st convolution layer) and 100 (2nd convolu- tion layer), while TIME-ARROW has the same feature-maps. βCNN is relatively simple, with two convolution layer containing only TIME- FLOW with 150 feature-maps, two gating lay- ers and a fully connected layer. We use ReLU as the activation function for convolution lay- ers and switch to Sigmoid for fully connected layers. We use word embedding with dimen- sion 100.</p><p>Soft-max: Calculating a full soft-max is ex- pensive since it has to enumerate all the words in vocabulary (in our case 40K words) in the denominator. Here we take a simple hierarchi- cal approximation of it, following ( <ref type="bibr">Bahdanau et al., 2014</ref>). Basically we group the words into 200 clusters (indexed by c m ), and factor- ize (in an approximate sense) the conditional probability of a word p(e t |e 1:t−1 ; ¯ Θ) into the probability of its cluster and the probability of e t given its cluster p(c m |e 1:t−1 ; ¯ Θ) p(e t |c m ; Θ sof tmax ).</p><p>We found that this simple heuristic can speed- up the optimization by 5 times with only slight loss of accuracy.</p><p>Optimization: We use stochastic gradient descent with mini-batch (size 500) for opti- mization, aided further by AdaGrad ( <ref type="bibr" target="#b5">Duchi et al., 2011</ref>). For initialization, we use Word2Vec ( <ref type="bibr" target="#b15">Mikolov et al., 2013</ref>) for the start- ing state of the word-embeddings (trained on the same dataset as the main task), and set all the other parameters by randomly sampling from uniform distribution in [−0.1, 0.1]. The optimization is done mainly on a Tesla K40 GPU, which takes about 2 days for the train- ing on a dataset containing 1M sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments: Sentence Generation</head><p>In this experiment, we randomly generate sen- tences by recurrently sampling e t+1 ∼ p(e t+1 |e 1:t ; ¯ Θ),</p><p>and put the newly generated word into history, until EOS (end-of-sentence) is generated. We consider generating two types of sentences: 1) the plain sentences, and 2) sentences with de- pendency parsing, which will be covered re- spectively in Section 5.1 and 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Natural Sentences</head><p>We train genCNN on Wiki data with 112M words for one week, with some representative examples randomly generated given in <ref type="table">Table 1</ref> (upper and middle blocks). We try two settings, by letting genCNN generate a sentence 1)from the very beginning (middle block), or 2) start- ing with a few words given by human (upper block). It is fairly clear that most of the time genCNN can generate sentences that are syn- tactically grammatical and semantically mean- ingful. More specifically, most of the sentences can be aligned to a parse tree with reasonable structure. It is also worth noting that quotation marks <ref type="bibr">('' and '')</ref> are always generated in pairs and in the correct order, even across a relatively long distance, as exemplified by the first gener- ated sentence in the upper block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Sentences with Dependency Tags</head><p>For training, we first parse( <ref type="bibr" target="#b10">Klein and Manning, 2002</ref>) the English sentences and feed se- quences with dependency tags as follows</p><formula xml:id="formula_10">( I like ( red apple ) )</formula><p>to genCNN in training, where 1) each paired parentheses contain a subtree, and 2) the sym- bol "" indicates that the word next to it is the dependency head in the corresponding sub- tree. Some representative examples gener- ated by genCNN are given in <ref type="table">Table 1 (bottom  block)</ref>. As it suggests, genCNN is fairly ac- curate on respecting the rules of parentheses, and probably more remarkably, it can get the dependency tree head right most of the time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments: Language Modeling</head><p>We evaluate our model as a language model in terms of both perplexity <ref type="bibr" target="#b2">(Brown et al., 1992)</ref> and its efficacy in re-ranking the n-best can- didates from state-of-the-art models in statisti- cal machine translation, with comparison to the following competitor language models.</p><p>Competitor Models we compare genCNN to the following competitor models</p><p>• 5-gram: We use SRI Language Modeling Toolkit ( <ref type="bibr">Stolcke and others, 2002</ref>) to train a 5-gram language model with modified Kneser-Ney smoothing;</p><p>• FFN-LM: The neural language model based on feedfoward network ( <ref type="bibr" target="#b20">Vaswani et al., 2013</ref>). We vary the input window-size from 5 to 20, while the performance stops increasing after window size 20;</p><p>• RNN: we use the implementation 1 of RNN-based language model with hidden size 600;</p><p>• LSTM: we adopt the code in Ground- hog 2 , but vary the hyper-parameters, including the depth and word-embedding dimension, for best performance. LSTM <ref type="bibr">(Hochreiter and Schmidhuber, 1997</ref>) is widely considered to be the state-of-the-art for sequence modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Perplexity</head><p>We test the performance of genCNN on PENN TREEBANK and FBIS, two public datasets with different sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">On PENN TREEBANK</head><p>Although a relatively small dataset 3 , PENN TREEBANK is widely used as a language mod- elling benchmark <ref type="bibr" target="#b6">(Graves, 2013;</ref><ref type="bibr" target="#b14">Mikolov et al., 2010)</ref>. It has 930, 000 words in train- ing set, 74, 000 words in validation set, and 82, 000 words in test set. We use exactly the same settings as in ( <ref type="bibr" target="#b14">Mikolov et al., 2010)</ref>, with a 10, 000-words vocabulary (all out-of- vocabulary words are replaced with unknown) <ref type="bibr">'</ref>' we are in the building of china 's social development and the businessmen audience , '' he said .</p><p>clinton was born in DDDD , and was educated at the university of edinburgh.</p><p>bush 's first album , '' the man '' , was released on DD november DDDD . it is one of the first section of the act in which one is covered in real place that recorded in norway .</p><p>this objective is brought to us the welfare of our country russian president putin delivered a speech to the sponsored by the 15th asia pacific economic cooperation ( apec ) meeting in an historical arena on oct .</p><p>light and snow came in kuwait and became operational , but was rarely placed in houston .</p><p>johnson became a drama company in the DDDDs , a television broadcasting company owned by the broadcasting program .</p><p>( ( the two sides ) should ( assume ( a strong target ) ) ) . ) ( it is time ( in ( every country ) signed ( the speech ) ) . ) ( ( initial investigations ) showed ( that ( spot could ( be ( further improved significantly ) ) . ) ( ( a book ( to northern ( the 21 st century ) ) ) . ) <ref type="table">Table 1</ref>: Examples of sentences generated by genCNN. In the upper block (row 1-4) the underline words are given by the human; In the middle block (row 5-8), all the sentences are generated without any hint. The bottom block (row 9-12) shows the sentences with dependency tag generated by genCNN trained with parsed examples. and end-of-sentence token (EOS) at the end of each sentence. In addition to the conventional testing strategy where the models are kept un- changed during testing, <ref type="bibr" target="#b14">Mikolov et al. (2010)</ref> proposes to also update the parameters in an online fashion when seeing test sentences. This new way of testing, named "dynamic evalua- tion", is also adopted by <ref type="bibr" target="#b6">Graves (2013)</ref>.</p><p>From <ref type="table">Table 2</ref> genCNN manages to give per- plexity superior in both metrics, with about 25 point reduction over the widely used 5-gram, and over 10 point reduction from LSTM, the state-of-the-art and the second-best performer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">On FBIS</head><p>The FBIS corpus (LDC2003E14) is relatively large, with 22.5K sentences and 8.6M English words. The validation set is NIST MT06 and test set is NIST MT08. For training the neural network, we limit the vocabulary to the most frequent 40,000 words, covering ∼ 99.4% of the corpus. Similar to the first experiment, all out-of-vocabulary words are replaced with unknown and the EOS token is counted in the sequence loss.</p><p>From  <ref type="table">Table 2</ref>: PENN TREEBANK results, where the 3rd column are the perplexity in dynamic eval- uation, while the numbers for RNN and LSTM are taken as reported in the paper cited above. The numbers in boldface indicate that the re- sult is significantly better than all competitors in the same setting.</p><p>clearly wins again in the comparison to com- petitors, with over 25 point margin over LSTM (in its optimal setting), the second best per- former. Interestingly genCNN outperforms its variants also quite significantly (bottom block): 1) with only TIME-ARROW (same number of feature-maps), the performance deteriorates considerably for losing the ability of capturing long range correlation reliably; 2) with only TIME-TIME the performance gets even worse,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Perplexity</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5-gram, KN5</head><p>278.6 FFN-LM <ref type="table">(5-gram)</ref> 248.3 FFN-LM <ref type="bibr">(20-gram)</ref> 228  for partially losing the sensitivity to the predic- tion task. It is quite remarkable that, although αCNN (with L α = 30) can achieve good re- sults, the recursive structure in full genCNN can further decrease the perplexity by over 3 points, indicating that genCNN can benefit from modeling the dependency over range as long as 30 words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Re-ranking for Machine Translation</head><p>In this experiment, we re-rank the 1000-best English translation candidates for Chinese sen- tences generated by statistical machine transla- tion (SMT) system, and compare it with other language models in the same setting.</p><p>SMT setup The baseline hierarchical phrase- based SMT system ( Chines→ English) was built using Moses, a widely accepted state- of-the-art, with default settings. The bilin- gual training data is from NIST MT2012 con- strained track, with reduced size of 1.1M sen- tence pairs using selection strategy in <ref type="bibr">(Axelrod et al., 2011</ref>). The baseline use conven- tional 5-gram language model (LM), estimated with modified Kneser-Ney smoothing <ref type="bibr" target="#b2">(Chen and Goodman, 1996</ref>) on the English side of the 329M-word Xinhua portion of English Giga- word(LDC2011T07). We also try FFN-LM, as a much stronger language model in decoding. The weights of all the features are tuned via MERT ( <ref type="bibr" target="#b16">Och and Ney, 2002)</ref> on NIST MT05, and tested on NIST MT06 and MT08. Case-  Re-ranking with genCNN significantly im- proves the quality of the final translation. In- deed, it can increase the BLEU score by over 1.33 point over Moses baseline on average. This boosting force barely slacks up on trans- lation with a enhanced language model in de- coding: genCNN re-ranker still achieves 1.29 point improvement on top of Moses with FFN- LM, which is 1.76 point over the Moses (de- fault setting). To see the significance of this improvement, the state-of-the-art Neural Net- work Joint <ref type="bibr">Model (Devlin et al., 2014</ref>) usually brings less than one point increase on this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>In addition to the long thread of work on neu- ral network based language model ( <ref type="bibr" target="#b1">Auli et al., 2013;</ref><ref type="bibr" target="#b14">Mikolov et al., 2010;</ref><ref type="bibr" target="#b6">Graves, 2013;</ref><ref type="bibr">Bengio et al., 2003;</ref><ref type="bibr" target="#b20">Vaswani et al., 2013)</ref>, our work is also related to the effort on modeling long range dependency in word sequence predic- tion( <ref type="bibr" target="#b21">Wu and Khudanpur, 2003)</ref>. Different from those work on hand-crafting features for incor- porating long range dependency, our model can elegantly assimilate relevant information in an unified way, in both long and short range, with the bottom-up information flow and convolu- tional architecture.</p><p>CNN has been widely used in computer vision and speech ( <ref type="bibr" target="#b12">Lawrence et al., 1997;</ref><ref type="bibr" target="#b11">Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b13">LeCun and Bengio, 1995;</ref><ref type="bibr" target="#b0">Abdel-Hamid et al., 2012)</ref>, and lately in sentence representation <ref type="bibr" target="#b9">(Kalchbrenner and Blunsom, 2013</ref>), matching( <ref type="bibr" target="#b8">Hu et al., 2014</ref>) and classification <ref type="bibr" target="#b9">(Kalchbrenner et al., 2014</ref>). To our best knowledge, it is the first time this is used in word sequence prediction. Model-wise the previous work that is closest to genCNN is the convolution model for predicting moves in the Go game <ref type="bibr" target="#b14">(Maddison et al., 2014)</ref>, which, when applied recurrently, essentially gener- ates a sequence. Different from the conven- tional CNN taken in ( <ref type="bibr" target="#b14">Maddison et al., 2014</ref>), genCNN has architectures designed for mod- eling the composition in natural language and the temporal structure of word sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We propose a convolutional architecture for natural language generation and modeling. Our extensive experiments on sentence generation, perplexity, and n-best re-ranking for machine translation show that our model can signifi- cantly improve upon state-of-the-arts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>/ / I was starving after this long meeting, so I rushed to wal-mart to buyFigure 1 :</head><label>1</label><figDesc>Figure 1: The overall diagram of a genCNN. Here "/" stands for a zero padding. In this example, each CNN component covers 6 words, while in practice the coverage is 30-40 words.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of a 3-layer αCNN. Here the shadowed nodes stand for the TIMEARROW feature-maps and the unfilled nodes for the TIME-FLOW.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration for gating network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: genCNN with recursive structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 3 (upper block), genCNN</head><label>3</label><figDesc></figDesc><table>Model 
Perplexity Dynamic 
5-gram, KN5 
141.2 
-
FFNN-LM 
140.2 
-
RNN 
124.7 
123.2 
LSTM 
126 
117 
genCNN 
116.4 
106.3 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>FBIS results. The upper block 
(row 1-6) compares genCNN and the competi-
tor models, and the bottom block (row 7-9) 
compares different variants of genCNN. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>The results for re-ranking the 1000-
best of Moses. Note that the two bottom rows 
are on a baseline with enhanced LM. 

insensitive NIST BLEU 4 is used in evaluation. 
</table></figure>

			<note place="foot" n="1"> http://rnnlm.org/ 2 https://github.com/lisa-groundhog/GroundHog 3 http://www.fit.vutbr.cz/∼imikolov/rnnlm/simpleexamples.tgz</note>

			<note place="foot" n="4"> ftp://jaguar.ncsl.nist.gov/mt/resources/mtevalv11b.pl</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Applying convolutional neural networks concepts to hybrid nn-hmm model for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdel-Hamid</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="4277" to="4280" />
		</imprint>
	</monogr>
	<note>2012 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Joint language and translation modeling with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>Bengio, Rjean Ducharme, Pascal Vincent, and Christian Jauvin</editor>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>A neural probabilistic language model</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An empirical study of smoothing techniques for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th annual meeting on Association for Computational Linguistics</title>
		<meeting>the 34th annual meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1992-03" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="310" to="318" />
		</imprint>
	</monogr>
	<note>An estimate of an upper bound for the entropy of english</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improving deep neural networks for lvcsr using rectified linear units and dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast and robust neural network joint models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Devlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1370" to="1380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Duchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno>abs/1308.0850</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long shortterm memory</title>
	</analytic>
	<monogr>
		<title level="m">Sepp Hochreiter and Jürgen Schmidhuber</title>
		<imprint>
			<date type="published" when="1997-11" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional neural network architectures for matching natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>ward Grefenstette, and Phil Blunsom</editor>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA, October</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1700" to="1709" />
		</imprint>
	</monogr>
	<note>Recurrent continuous translation models</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast exact inference with a factored model for natural language parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manning2002] Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Krizhevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Face recognition: A convolutional neuralnetwork approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="113" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
	<note>Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio1995] Yann Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">3361</biblScope>
			<biblScope unit="page">310</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Move evaluation in go using deep convolutional neural networks. CoRR, abs/1412.6564</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maddison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
	<note>Mikolov et al.2010</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mikolov</surname></persName>
		</author>
		<idno>abs/1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Discriminative training and maximum entropy models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Ney2002] Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="295" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Parsing Natural Scenes and Natural Language with Recursive Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Machine Learning (ICML)</title>
		<meeting>the 26th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Srilm-an extensible language modeling toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international conference on spoken language processing</title>
		<meeting>the international conference on spoken language processing</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="901" to="904" />
		</imprint>
	</monogr>
	<note>Stolcke and others2002</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Decoding with large-scale neural language models improves translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1387" to="1392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Maximum entropy language modeling with non-local dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
		<respStmt>
			<orgName>Johns Hopkins University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
