<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automatic Labeling of Topic Models Using Text Summaries</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
							<email>{wanxiaojun, wangtm}@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="laboratory">The MOE Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianming</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="laboratory">The MOE Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Automatic Labeling of Topic Models Using Text Summaries</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="2297" to="2305"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Labeling topics learned by topic models is a challenging problem. Previous studies have used words, phrases and images to label topics. In this paper, we propose to use text summaries for topic labeling. Several sentences are extracted from the most related documents to form the summary for each topic. In order to obtain summaries with both high relevance, coverage and discrimination for all the topics, we propose an algorithm based on sub-modular optimization. Both automatic and manual analysis have been conducted on two real document collections, and we find 1) the summaries extracted by our proposed algorithm are superior over the summaries extracted by existing popular summarization methods; 2) the use of summaries as labels has obvious advantages over the use of words and phrases.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Statistical topic modelling plays very important roles in many research areas, such as text mining, natural language processing and information re- trieval. Popular topic modeling techniques in- clude Latent Dirichlet Allocation (LDA) ( <ref type="bibr" target="#b2">Blei et al., 2003)</ref> and Probabilistic Latent Semantic Anal- ysis (pLSA) <ref type="bibr" target="#b6">(Hofmann, 1999</ref>). These techniques can automatically discover the abstract "topics" that occur in a collection of documents. They model the documents as a mixture of topics, and each topic is modeled as a probability distribution over words.</p><p>Although the discovered topics' word distribu- tions are sometimes intuitively meaningful, a ma- jor challenge shared by all such topic models is to accurately interpret the meaning of each topic ( <ref type="bibr" target="#b13">Mei et al., 2007)</ref>. The interpretation of each topic is very important when people want to browse, understand and leverage the topic. However, it is usually very hard for a user to understand the dis- covered topics based only on the multinomial dis- tribution of words. For example, here are the top terms for a discovered topic: {fire miles area north southern people coast homes south damage northern river state friday central water rain high california weather}. It is not easy for a user to fully understand this topic if the user is not very familiar with the document collection. The situa- tion may become worse when the user faces with a number of discovered topics and the sets of top terms of the topics are often overlapping with each other on many practical document collections.</p><p>In order to address the above challenge, a few previous studies have proposed to use phrases, concepts and even images for labeling the discov- ered topics ( <ref type="bibr" target="#b13">Mei et al., 2007;</ref><ref type="bibr" target="#b9">Lau et al., 2011;</ref><ref type="bibr" target="#b7">Hulpus et al., 2013;</ref><ref type="bibr" target="#b0">Aletras and Stevenson, 2013)</ref>. For example, we may automatically extract the phrase "southern california" to represent the ex- ample topic mentioned earlier. These topic labels can help the user to understand the topics to some extent. However, the use of phrases or concepts as topic labels are not very satisfactory in practice, because the phrases or concepts are still very short, and the information expressed in these short labels is not adequate for user's understanding. The case will become worse when some ambiguous phrase is used or multiple discrete phrases with poor co- herence are used for a topic. To address the draw- backs of the above short labels, we need to pro- vide more contextual information and consider using long text descriptions to represent the topics. The long text descriptions can be used inde- pendently or used as beneficial complement to the short labels. For example, below is part of the summary label produced by our proposed method and it provides much more contextual information for understanding the topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Showers and thunderstorms developed in parched areas of the southeast , from western north carolina into south central alabama , north central and northeast texas and the central and southern gulf coast . … The quake was felt over a large area , extending from santa rosa , about 60 miles north of san francisco , to the santa cruz area 70 miles to the south …. Fourteen homes were destroyed in baldwin park 20 miles northeast of downtown los angeles and five were damaged along with five commercial buildings when 75 mph gusts snapped power lines , igniting a fire at allan paper co. , fire officials said . …</head><p>The contributions of this paper are summarized as follows:</p><p>1) We are the first to invesitage using text summaries for topic labeling;</p><p>2) We propose a summarization algorithm based on submodular optimization to extract summaries with both high relevance, coverage and discrimination for all topics.</p><p>3) Automatic and manual analysis reveals the usefulness and advantages of the summaries pro- duced by our algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Topic Labeling</head><p>After topics are discovered by topic modeling techniques, these topics are conventionally repre- sented by their top N words or terms ( <ref type="bibr" target="#b2">Blei et al., 2003;</ref><ref type="bibr" target="#b18">Griffiths and Steyvers, 2004</ref>). The words or terms in a topic are ranked based on the condi- tional probability p(í µí±¤ í µí± |í µí±¡ í µí± ) in that topic. It is sometimes not easy for users to understand each topic based on the terms. Sometimes topics are presented with manual labeling for exploring re- search publications ( <ref type="bibr" target="#b22">Wang and McCallum, 2006;</ref><ref type="bibr" target="#b12">Mei et al., 2006</ref>), and the labeling process is time consuming.</p><p>In order to make the topic representations more interpretable and make the topics easier to under- stand, there are a few studies proposing to auto- matically find phrases, concepts or even images for topic labeling. <ref type="bibr" target="#b13">Mei et al. (2007)</ref> proposed to use phrases (chunks or ngrams) for topic labeling and cast the labeling problem as an optimization problem involving minimizing <ref type="bibr">Kullback-Leibler (KL)</ref> divergence between word distributions and maximizing mutual information between a label and a topic model. <ref type="bibr" target="#b9">Lau et al. (2011)</ref> also used phrases as topic labels and they proposed to use supervised learning techniques for ranking candi- date labels. In their work, candidate labels include the top-5 topic terms and a few noun chunks ex- tracted from related Wikipedia articles. <ref type="bibr" target="#b14">Mao et al. (2012)</ref> proposed two effective algorithms that au- tomatically assign concise labels to each topic in a hierarchy by exploiting sibling and parent-child relations among topics. <ref type="bibr" target="#b8">Kou et al. (2015)</ref> pro- posed to map topics and candidate labels (phrases) to word vectors and letter trigram vectors in order to find which candidate label is more semantically related to that topic. <ref type="bibr" target="#b7">Hulpus et al. (2013)</ref> took a new approach based on graph centrality measures to topic labelling by making use of structured data exposed by DBpedia. Different from the above works, <ref type="bibr" target="#b0">Aletras and Stevenson (2013)</ref> proposed to use images for representing topics, where candi- date images for each topic are retrieved from the web and the most suitable image is selected by us- ing a graph-based algorithm. In a very recent study ( <ref type="bibr" target="#b1">Aletras et al., 2015)</ref>, 3 different topic rep- resentations (lists of terms, textual phrase labels and images labels) are compared in a document retrieval task, and results show that textual phrase labels are easier for users to interpret than term lists and image labels.</p><p>The phrase-based labels in the above works are still very short and are sometimes not adequate for interpreting the topics. Unfortunately, none of previous works has investigated using textual summaries for representing topics yet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Document Summarization</head><p>The task of document summarization aims to pro- duce a summary with a length limit for a given document or document set. The task has been ex- tensively investigated in the natural language pro- cessing and information retrieval fields, and most previous works focus on directly extracting sen- tences from a news document or collection to form the summary. The summary can be used for helping users quickly browse and understand a document or document collection.</p><p>Typical multi-document summarization meth- ods include the centroid-based method ), integer linear programming (ILP) <ref type="bibr" target="#b5">(Gillick et al., 2008)</ref>, sentence-based LDA <ref type="bibr" target="#b3">(Chang and Chien, 2009)</ref>, submodular function maximization ( <ref type="bibr" target="#b10">Lin and Bilmes, 2010;</ref><ref type="bibr" target="#b11">Lin and Bilmes, 2011)</ref>, graph based methods ( <ref type="bibr" target="#b4">Erkan and Radev, 2004;</ref><ref type="bibr" target="#b20">Wan et al., 2007;</ref><ref type="bibr" target="#b21">Wan and Yang, 2008)</ref>, and su- pervised learning based methods ( <ref type="bibr" target="#b15">Ouyang et al., 2007;</ref>. Though different sum- marization methods have been proposed in recent years, the submodular function maximization method is still one of the state-of-the-art summa- rization methods. Moreover, the method is easy to follow and its framework is very flexible. One can design specific submodular functions for address- ing special summarization tasks, without altering the overall greedy selection framework.</p><p>Though various summarization methods have been proposed, none of existing works has inves- tigated or tried to adapt document summarization techniques for the task of automatic labeling of topic models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Formulation</head><p>Given a set of latent topics extracted from a text collection and each topic is represented by a mul- tinomial distribution over words, our goal is to produce understandable text summaries as labels for interpreting all the topics. We now give two useful definitions for later use.</p><p>Topic: Each topic í µí¼ is a probability distribu- tion of words {í µí± í µí¼ (í µí±¤)} í µí±¤∈í µí± , where V is the vocab- ulary set, and we have ∑ í µí± í µí¼ (í µí±¤) = 1 í µí±¤∈í µí± . Topic Summary: In this study, a summary for each topic í µí¼ is a set of sentences extracted from the document collection and it can be used as a label to represent the latent meaning of í µí¼. Typi- cally, the length of the summary is limited to 250 words, as defined in recent DUC and TAC confer- ences.</p><p>Like the criteria for the topic labels in ( <ref type="bibr" target="#b13">Mei et al., 2007)</ref>, the topic summary for each topic needs to meet the following two criteria:</p><p>High Relevance: The summary needs to be se- mantically relevant to the topic, i.e., the summary needs to be closely relevant to all representative documents of the topic. The higher the relevance is, the better the summary is. This criterion is in- tuitive because we do not expect to obtain a sum- mary unrelated to the topic.</p><p>High Coverage: The summary needs to cover as much semantic information of the topic as pos- sible. The summary usually consists of several sentences, and we do not expect all the sentences to focus on the same piece of semantic infor- mation. A summary with high coverage will cer- tainly not contain redundant information. This cri- terion is very similar to the diversity requirement of multi-document summarization.</p><p>Since we usually produce a set of summaries for all the topics discovered in a document collec- tion. In order to facilitate users to understand all the topics, the summaries need to meet the follow- ing additional criterion:</p><p>High Discrimination: The summaries for dif- ferent topics need to have inter-topic discrimina- tion. If the summaries for two or more topics are very similar with each other, users can hardly un- derstand each topic appropriately. The higher the inter-topic discrimination is, the better the sum- maries are.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Our Method</head><p>Our proposed method is based on submodular op- timization, and it can extract summaries with both high relevance, coverage and discrimination for all topics. We choose the framework of submodu- lar optimization because the framework is very flexible and different objectives can be easily in- corporated into the framework. The overall frame- work of our method consists of two phases: can- didate sentence selection, and topic summary ex- traction. The two phrases are described in the next two subsections, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Candidate Sentence Selection</head><p>There are usually many thousands of sentences in a document collection for topic modelling, and all the sentences are more or less correlated with each topic. If we use all the sentences for summary ex- traction, the summarization efficiency will be very low. Moreover, many sentences are not suit- able for summarization because of their low rele- vance with the topic. Therefore, we filter out the large number of unrelated sentences and treat the remaining sentences as candidates for summary extraction.</p><p>For each topic í µí¼, we compute the Kullback- Leibler (KL) divergence between the word distri- butions of the topic and each sentence s in the whole document collection as follows:</p><p>í µí°¾í µí°¿(í µí¼, í µí± ) = ∑ í µí± í µí¼ (í µí±¤) * í µí±í µí±í µí± í µí± í µí¼ (í µí±¤) í µí±¡í µí±(í µí±¤, í µí± ) í µí±í µí±í µí±(í µí± ) ⁄ í µí±¤∈í µí±í µí±∪í µí±í µí± where í µí± í µí¼ (í µí±¤) is the probability of word w in topic í µí¼. TW denotes the set of top 500 words in topic í µí¼ according to the probability distribution. SW de- notes the set of words in sentence s after removing stop words. í µí±¡í µí±(í µí±¤, í µí± ) denotes the frequency of word w in sentence s, and í µí±í µí±í µí±(í µí± ) denotes the length of sentence s after removing stop words. For a word w which does not appear in SW, we set í µí±¡í µí±(í µí±¤, í µí± ) í µí±í µí±í µí±(í µí± ) ⁄ to a very small value (0.00001 in this study).</p><p>Then we rank the sentences by an increasing or- der of the divergence scores and keep the top 500 sentences which are most related to the topic. These 500 sentences are treated as candidate sen- tences for the subsequent summarization step for each topic. Note that different topics have differ- ent candidate sentence sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Topic Summary Extraction</head><p>Our method for topic summary extraction is based on submodular optimization. For each topic í µí¼ as- sociated with the candidate sentence set V, our method aims to find an optimal summary í µí°¸̃µí°¸̃ from all possible summaries by maximizing a score function under budget constraint: í µí°¸̃µí°¸̃ = í µí±í µí±í µí±í µí±í µí±í µí±¥ í µí°¸⊆í µí± {í µí±(í µí°¸)} s.t. í µí±í µí±í µí±(í µí°¸) ≤ í µí°¿ where í µí±í µí±í µí±(í µí°¸) denotes the length of summary E.</p><p>Here E is also used to denote the set of sentences in the summary. L is a predefined length limit, i.e. 250 words in this study. í µí±(í µí°¸) is the score function to evaluate the over- all quality of summary E. Usually, í µí±(í µí°¸) is re- quired to be a submodular function, so that we can use a simple greedy algorithm to find the near-op- timal summary with theoretical guarantee. For- mally, for any í µí°´⊆µí°´⊆ í µí°µ ⊆ í µí±\í µí±£, we have í µí±(í µí°´+µí°´+ í µí±£) − í µí±(í µí°´) ≥ í µí±(í µí°µ + í µí±£) − í µí±(í µí°µ) which means that the incremental "value" of v de- creases as the context in which v is considered grows from A to B.</p><p>In this study, the score function í µí±(í µí°¸) is decom- posed into three parts and each part evaluates one aspect of the summary: í µí±(í µí°¸) = í µí± í µí°¸í µí°¿(í µí°¸) + í µí° ¶í µí±í µí±(í µí°¸) + í µí°·í µí°¼í µí±(í µí°¸) where í µí± í µí°¸í µí°¿(í µí°¸) , í µí° ¶í µí±í µí±(í µí°¸) and í µí°·í µí°¼í µí±(í µí°¸) evaluate the relevance, coverage and discrimination of summary E respectively. We will describe them in details respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Relevance Function</head><p>Instead of intuitively measuring relevance be- tween the summary and the topic via the KL di- vergence between the word distributions of them, we consider to measure the relevance of summary E for topic í µí¼ by the relevance of the sentences in the summary to all the candidate sentences for the topic as follows: where V represents the candidate sentence set for topic í µí¼, and E is used to represent the sentence set of the summary. í µí± í µí±í µí±(í µí± ′ , í µí± ) is the standard co- sine similarity between sentences í µí± ′ ⁡and s. í µí»¼ ∈ [0,1] is a threshold co-efficient.</p><p>The above function is a monotone submodular function because í µí±(í µí±¥) = í µí±í µí±í µí±⁡ (í µí±¥, í µí±) where í µí± ≥ 0 is a concave non-decreasing function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>∑ í µí± í µí±í µí±(í µí± ′ , í µí± ) í µí± ∈í µí°¸measures</head><p>µí°¸measures how similar E is to sen- tence í µí± ′ and then ∑ í µí± í µí±í µí±(í µí± ′ , í µí± ) í µí± ∈í µí± is the largest value that ∑ í µí± í µí±í µí±(í µí± ′ , í µí± ) í µí± ∈í µí°çan µí°çan achieve. Therefore, í µí± ′ is saturated by E when ∑ í µí± í µí±í µí±(í µí± ′ , í µí± ) ≥ í µí± ∈í µí°¸í µí°¸í µí»¼ ∑ í µí± í µí±í µí±(í µí± ′ , í µí± ) í µí± ∈í µí± . When í µí± ′ is already saturated by E in this way, any new sentence very similar to í µí± ′ cannot further improve the overall relevance of E, and this sentence is less possible to be added to the summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Coverage Function</head><p>We want the summary to cover as many topic words as possible and contain as many different sentences as possible. The coverage function is thus defined as follows: The above function is a monotone submodular function and it encourages the summary E to con- tain many different words, rather than a small set of words. Because í µí±(í µí±¥) = √í µí±¥ where í µí±¥ ≥ 0 is a concave non-decreasing function, we have í µí±(í µí±¥ + í µí±¦) ≤ í µí±(í µí±¥) + í µí±(í µí±¦). The value of the function will be larger when we use x and y to represent two frequency values of two different words respec- tively than that when we use (í µí±¥ + í µí±¦) to represent the frequency value of a single word. Therefore, the use of this function encourages the coverage of more different words in the summary. In other words, the diversity of the summary is enhanced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Discrimination Function</head><p>The function for measuring the discrimination be- tween the summary E of topic í µí¼ and all other top- ics {í µí¼ ′ } is defined as follows:</p><p>í µí°·í µí°¼í µí±(í µí°¸) = −í µí»¾ ∑ ∑ ∑ í µí± í µí¼ ′ (í µí±¤) * í µí±¡í µí±(í µí±¤, í µí± ) ⁡ í µí±¤∈í µí±í µí± í µí± ∈í µí°¸í µí°¸í µí¼ ′ where í µí»¾ ≥ 0 is a combination co-efficient.</p><p>The above function is still a monotone submod- ular function. The negative sign indicates that the summary E of topic í µí¼ needs to be as irrelevant with any other topic as possible, and thus making different topic summaries have much differences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Greedy Selection</head><p>Since í µí± í µí°¸í µí°¿(í µí°¸), í µí° ¶í µí±í µí±(í µí°¸) and í µí°·í µí°¼í µí±(í µí°¸) are all sub- modular functions, í µí±(í µí°¸) is also a submodular function. In order to find a good approximation to the optimal summary, we use a greedy algorithm similar to ( <ref type="bibr" target="#b10">Lin and Bilmes, 2010)</ref> to select sen- tence one by one and produce the final summary, as shown in Algorithm 1. µí°¸In the algorithm, í µí±í µí±í µí±(í µí± ) denotes the length of sentence s and í µí¼ &gt; 0 is the scaling factor. At each iteration, the sentence with the largest ratio of ob- jective function gain to scaled cost is found in step 4, and if adding the sentence can increase the ob- jective function value while not violating the length constraint, it is then selected into the sum- mary and otherwise bypassed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation Setup</head><p>We used two document collections as evaluation datasets, as in ( <ref type="bibr" target="#b13">Mei et al. 2007</ref>): AP news and SIGMOD proceedings. The AP news dataset con- tains a set of 2250 AP news articles, which are provided by TREC. There is a total of 43803 sen- tences in the AP news dataset and the vocabulary size is 37547 (after removing stop words). The SIGMOD proceeding dataset contains a set of 2128 abstracts of SIGMOD proceedings between the year 1976 and 2015, downloaded from the ACM digital library. There is a total of 15211sen- tences in the SIGMOD proceeding dataset and the vocabulary size is 13688.</p><p>For topic modeling, we adopted the most popu- lar LDA to discover topics in the two datasets, re- spectively. Particularly, we used the LDA module implemented in the MALLET toolkit <ref type="bibr">1</ref> . Without loss of generality, we extracted 25 topics from the AP news dataset and 25 topics from the SIGMOD proceeding dataset.</p><p>The parameter values of our proposed summa- rization method is either directly borrowed from previous works or empirically set as follows: í µí»¼ = 0.05, í µí»½ = 250, í µí»¾ = 300 and í µí¼ = 0.15.</p><p>1 http://mallet.cs.umass.edu/ We have two goals in the evaluation: compari- son of different summarization methods for topic labeling, and comparison of different kinds of la- bels (summaries, words, and phrases).</p><p>In particular, we compare our proposed summa- rization method (denoted as Our Method) with the following typical summarization methods and all of them extract summaries from the same can- didate sentence set for each topic:</p><p>MEAD: It uses a heuristic way to obtain each sentence's score by summing the scores based on different features ( ): centroid- based weight, position and similarity with first sentence.</p><p>LexRank: It constructs a graph based on the sentences and their similarity relationships and then applies the PageRank algorithm for sentence ranking ( <ref type="bibr" target="#b4">Erkan and Radev, 2004)</ref>.</p><p>TopicLexRank: It is an improved version of LexRank by considering the probability distribu- tion of top 500 words in a topic as a prior vector, and then applies the topic-sensitive PageRank al- gorithm for sentence ranking, similar to <ref type="bibr" target="#b19">(Wan 2008)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Submodular(REL): It is based on submodular function maximization but only the relevance function is considered.</head><p>Submodular(REL+COV): It is based on sub- modular function maximization and combines two functions: the relevance function and the cov- erage function.</p><p>We also compare the following three different kinds of labels:</p><p>Word label: It shows ten topic words as labels for each topic, which is the most intuitive inter- pretation of the topic.</p><p>Phrase label: It uses three phrases as labels for each topic, and the phrase labels are extracted by using the method proposed in ( <ref type="bibr" target="#b13">Mei et al., 2007)</ref>, which is very closely related to our work and con- sidered a strong baseline in this study.</p><p>Summary Label: It uses a topic summary with a length of 250 words to label each topic and the summary is produced by our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Automatic Comparison of Summarization</head><p>Methods In this section, we compare different summariza- tion methods with the following automatic measures:</p><p>KL divergence between word distributions of summary and topic: For each summarization method, we compute the KL divergence between the word distributions of each topic and the sum- mary for the topic, then average the KL diver- gence across all topics. <ref type="table">Table 1</ref> shows the results. We can see that our method and Submodu- lar(REL+COV) have the lowest KL divergence with the topic, which means our method can pro- duce summaries relevant to the topic representa- tion.</p><p>Topic word coverage: For each summarization method, we compute the ratio of the words cov- ered by the summary out of top 20 words for each topic, and then average the ratio across all topics. We use top 20 words instead of 500 words be- cause we want to focus on the most important words. The results are shown in <ref type="table" target="#tab_1">Table 2</ref>. We can see that our method has almost the best coverage ratio and the produced summary can cover most important words in a topic.   Similarity between topic summaries: For each summarization method, we compute the co- sine similarity between the summaries of any two topics, and then obtain the average similarity and the maximum similarity. Seen from <ref type="table" target="#tab_2">Table 3</ref>, the topic summaries produced by our method has the lowest average and maximum similarity with each other, and thus the summaries for different topics have much difference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Manual Comparison of Summarization</head><p>Methods In this section, we compare our summarization method with three typical summarization methods (MEAD, TopicLexRank and Submodular(REL)) manually. We employed three human judges to read and rank the four summaries produced for each topic by the four methods in three aspects: relevance between the summary and the topic with the corresponding sentence set, the content coverage (or diversity) in the summary and the discrimination between different summaries. The human judges were encouraged to read a few closely related documents for better understand- ing each topic. Note that the judges did not know which summary was generated by our method and which summaries were generated by the baseline methods. The rank k for each summary ranges from 1 to 4 (1 means the best, and 4 means the worst; we allow equal ranks), and the score is thus (4-k). We average the scores across all summaries and all judges and the results on the two datasets are shown in <ref type="table" target="#tab_4">Tables 4 and 5</ref>, respectively. In the table, the higher the score is, the better the corre- sponding summaries are. We can see that our pro- posed method outperforms all the three baselines over almost all metrics.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Manual Comparison of Different Kinds of Labels</head><p>In this section, we manually compare the three kinds of labels: words, phrases and summary, as mentioned in Section 5.1. Similarly, the three hu- man judges were asked to read and rank the three kinds of labels in the same three aspects: rele- vance between the label and the topic with the cor- responding sentence set, the content coverage (or diversity) in the label and the discrimination be- tween different labels. The rank k for each kind of labels ranges from 1 to 3 (1 means the best, and 3 means the worst; we allow equal ranks), and the score is thus <ref type="figure">(3-k)</ref>. We average the scores across all labels and all judges and the results on the two datasets are shown in <ref type="table" target="#tab_6">Tables 6 and 7</ref>, respectively. It is clear that the summary labels produced by our proposed method have obvious advantages over the conventional word labels and phrase labels. The summary labels have better evaluation results on relevance, coverage and discrimination.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">Example Analysis</head><p>In this section, we demonstrate some running ex- amples on the SIGMOD proceeding dataset. Two topics and the three kinds of labels are shown be- low. For brevity, we only show the first 100 words of the summaries to users unless they want to see more. We can see that the word labels are very confusing, and the phrase labels for the two topics are totally overlapping with each other and have no discrimination. Therefore, it is hard to under- stand the two topics by looking at the word or phrase labels. Fortunately, by carefully reading the topic summaries, we can understand what the two topics are really about. In this example, the first topic is about data analysis and data integra- tion, while the second topic is about data privacy. Though the summary labels are much longer than the word labels or phrase labels, users can obtain more reliable information after reading the sum- mary labels and the summaries can help users to better understand each topic and also know the difference between different topics.</p><p>In practice, the different kinds of labels can be used together to allow users to browse topic mod- els in a level-wise matter, as described in next sec- tion. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic 2 on SIGMOD proceeding dataset:</head><p>word label: user information attribute model privacy quality record result individual provide phrase label: data set ; data analysis ; data integration summary label: An essential element for privacy metric is the measure of how much adversaries can know about an individual ' sensitive attribute ( sa ) if they know the individual ' quasi-identifier ( qi) ….We present an automated solution that elicit user preference on attribute and value , employing different disambiguation technique ranging from simple keyword matching , to more sophisticated probabilistic model ….Privgene need significantly less perturbation than previous method , and it achieve higher overall result quality , even for model fitting task where ga is not the first choice without privacy consideration ….</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.5">Discussion of Practical Use</head><p>Although the summary labels produced by our method have higher relevance, coverage and dis- crimination than the word labels and the phrase labels, the summary labels have one obvious shortcoming of consuming more reading time of users, because the summaries are much longer than the words and phrases. The feedback from the human judges also reveals the above problem and all the three human judges said they need to take more than five times longer to read the sum- maries. Therefore, we want to find a better way to make use of the summary label in practice.</p><p>In order to consider both the shorter reading time of the phrase labels and the better quality of the summary labels, we can use both of the two kinds of labels in the following hierarchical way:</p><p>For each topic, we first present only the phrase label to users, and if they can easily know about the topic after they read the phrase label, the sum- mary label will not be shown to them. Whereas, if users cannot know well about the topic based on the phrase label, or they need more information about the topic, they may choose to read the sum- mary label for better understanding the topic. Only the first 100 words of the summary label are shown to users, and the rest words will be shown upon request. In this way, the summary label is used as an important complement to the phrase la- bel, and the burden of reading the longer summary label can be greatly alleviated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>In this study, we addressed the problem of topic labeling by using text summaries. We propose a summarization algorithm based on submodular optimization to extract representative summaries for all the topics. Evaluation results demonstrate that the summaries produced by our proposed al- gorithm have high relevance, coverage and dis- crimination, and the use of summaries as labels has obvious advantages over the use of words and phrases.</p><p>In future work, we will explore to make use of all the three kinds of labels together to improve the users' experience when they want to browse, understand and leverage the topics.</p><p>In this study, we do not consider the coherence of the topic summaries because it is really very challenging to get a coherent summary by extract- ing different sentences from a large set of different documents. In future work, we will try to make the summary label more coherent by considering the discourse structure of the summary and leveraging sentence ordering techniques.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>í µí± í µí°¸í µí°¿(í µí°¸) = ∑ min⁡ {∑ í µí± í µí±í µí±(í µí± ′ , í µí± ), í µí»¼ ∑ í µí± í µí±í µí±(í µí± ′ , í µí± ) í µí± ∈í µí± í µí± ∈í µí°¸} µí°¸} í µí± ′ ∈í µí±</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>í</head><label></label><figDesc>µí° ¶í µí±í µí±(í µí°¸) = í µí»½ * ∑ {í µí± í µí¼ (í µí±¤) * √ ∑ í µí±¡í µí±(í µí±¤, í µí± ) í µí± ∈í µí°¸} µí°¸} í µí±¤∈í µí±í µí± where í µí»½ ≥ 0 is a combination co-efficient.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1</head><label>1</label><figDesc>Greedy algorithm for summary extraction 1: í µí°¸←µí°¸← ∅ 2: í µí± ← í µí± 3: while í µí± ≠ ∅ do 4: í µí± ̂ ← í µí±í µí±í µí±í µí±í µí±í µí±¥ í µí± ∈í µí± í µí±(í µí°¸∪{í µí± })−í µí±(í µí°¸) í µí±í µí±í µí±(í µí± ) í µí¼ 5: í µí°¸←µí°¸← í µí°¸∪µí°¸∪ {í µí± ̂ } if ∑ í µí±í µí±í µí±(í µí± ) + í µí±í µí±í µí±(í µí± ̂ ) ≤ í µí°¿ í µí± ∈í µí°¸and µí°¸and í µí±(í µí°¸∪µí°¸∪ {í µí± }) − í µí±(í µí°¸) ≥ 0 6: í µí± ← í µí± ∖ {í µí± ̂ } 7: end while 8: return í µí°¸In</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table>Comparison of the ratio of the covered 
words out of top 20 topic words 

AP 
SIGMOD 

average 
max 
average 
max 
MEAD 

0.026961 0.546618 0.078826 0.580055 

LexRank 

0.019466 0.252074 0.05635 0.357491 

TopicLexRank 0.022548 0.283742 0.062034 0.536886 
Submodu-
lar(REL) 

0.028035 
0.47012 
0.07522 
0.52629 

Submodular 
(REL+COV) 

0.023206 0.362795 0.048872 0.524863 

Our Method 

0.010304 0.093017 0.024551 0.116905 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table>Comparison of the average and max similar-
ity between different topic summaries 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table>Manual comparison of different summariza-
tion methods on AP news dataset 

rele-
vance 

cover-
age 

discrimina-
tion 
MEAD 
1.6 
1.4 
1.83 
TopicLexRank 
1.77 
2.1 
2.1 
Submodu-
lar(REL) 
2.07 
2.1 
2.03 

Our Method 
2.43 
2.17 
2.1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table>Manual comparison of different summariza-
tion methods on SIGMOD proceeding dataset 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 6 .</head><label>6</label><figDesc></figDesc><table>Manual comparison of different kinds of la-
bels on AP news dataset 

rele-
vance 

cover-
age 

discrimina-
tion 
Word label 
0.87 
0.877 
1.27 
Phrase label 
1.4 
1.53 
1.43 
Summary la-
bel 
1.8 
1.97 
1.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 7 .</head><label>7</label><figDesc></figDesc><table>Manual comparison of different kinds of la-
bels on AP news dataset 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The work was supported by National Natural Sci-ence Foundation of China <ref type="formula">(61331011)</ref> </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Representing topics using images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Aletras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Stevenson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>HLT-NAACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Evaluating topic representations for exploring document collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Aletras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jey</forename><forename type="middle">Han</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Stevenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Association for Information Science and Technology</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent Dirichlet Allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Latent Dirichlet learning for document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying-Lang</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jen-Tzung</forename><surname>Chien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proccedings of IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>cedings of IEEE International Conference on Acoustics, Speech and Signal essing</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">LexPageRank: Prestige in multi-document text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Güneş</forename><surname>Erkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The ICSI summarization system at TAC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Benoit Favre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hakkani-Tur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Text Understanding Conference</title>
		<meeting>the Text Understanding Conference</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Probabilistic latent semantic indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 22nd annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised graph-based topic labelling using dbpedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioana</forename><surname>Hulpus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Conor</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><surname>Karnstedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Greene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixth ACM international conference on Web search and data mining</title>
		<meeting>the sixth ACM international conference on Web search and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Automatic labelling of topic models using word vectors and letter trigram vectors. Information Retrieval Technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqiu</forename><surname>Kou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="253" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automatic labelling of topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Jey Han Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-document summarization via budgeted maximization of submodular functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A class of submodular functions for document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A probabilistic approach to spatiotemporal theme pattern mining on weblogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th international conference on World Wide Web</title>
		<meeting>the 15th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="533" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatic labeling of multinomial topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 13th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automatic labeling hierarchical topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Ling</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao-Yan</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatseng</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongfei</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM international conference on Information and knowledge management</title>
		<meeting>the 21st ACM international conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2383" to="2386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Developing learning strategies for topic-based summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">You</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixteenth ACM conference on Conference on information and knowledge management</title>
		<meeting>the sixteenth ACM conference on Conference on information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Centroid-based summarization of multiple documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dragomir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyan</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Małgorzata</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Styś</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="919" to="938" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Document summarization using Conditional Random Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dou</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Tao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCAI</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2862" to="2867" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Finding scientific topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="5228" to="5235" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>suppl</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Using only cross-document relationships for both generic and topic-focused multi-document summarizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="25" to="49" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Manifold-ranking based topic-focused multidocument summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCAI</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2903" to="2908" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-document summarization using cluster-based link analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwu</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 31st annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Topics over time: a non-Markov continuous-time model of topical trends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuerui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
