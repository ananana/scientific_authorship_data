<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:20+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Agreement-based Learning of Parallel Lexicons and Phrases from Non-Parallel Corpora</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyang</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Jiangsu Collaborative Innovation Center for Language Competence</orgName>
								<address>
									<settlement>Jiangsu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Maosong</roleName><forename type="first">Sun</forename><surname>†#</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Jiangsu Collaborative Innovation Center for Language Competence</orgName>
								<address>
									<settlement>Jiangsu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Yu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Samsung R&amp;D Institute of China</orgName>
								<address>
									<postCode>100028</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Intelligent Technology and Systems Tsinghua National Laboratory for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Agreement-based Learning of Parallel Lexicons and Phrases from Non-Parallel Corpora</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1024" to="1033"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We introduce an agreement-based approach to learning parallel lexicons and phrases from non-parallel corpora. The basic idea is to encourage two asym-metric latent-variable translation models (i.e., source-to-target and target-to-source) to agree on identifying latent phrase and word alignments. The agreement is defined at both word and phrase levels. We develop a Viterbi EM algorithm for jointly training the two unidirectional models efficiently. Experiments on the Chinese-English dataset show that agreement-based learning significantly improves both alignment and translation performance.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Parallel corpora, which are large collections of parallel texts, serve as an important resource for inducing translation correspondences, either at the level of words ( <ref type="bibr" target="#b0">Brown et al., 1993;</ref><ref type="bibr" target="#b21">Smadja and McKeown, 1994;</ref><ref type="bibr" target="#b26">Wu and Xia, 1994)</ref> or phrases <ref type="bibr" target="#b10">(Kupiec, 1993;</ref><ref type="bibr" target="#b14">Melamed, 1997;</ref><ref type="bibr" target="#b13">Marcu and Wong, 2002;</ref><ref type="bibr" target="#b8">Koehn et al., 2003)</ref>. However, the availabil- ity of large-scale, wide-coverage corpora still re- mains a challenge even in the era of big data: par- allel corpora are usually only existent for resource- rich languages and restricted to limited domains such as government documents and news articles.</p><p>Therefore, intensive attention has been drawn to exploiting non-parallel corpora for acquiring translation correspondences. Most previous ef- forts have concentrated on learning parallel lexi- cons from non-parallel corpora, including parallel sentence and lexicon extraction via bootstrapping ( <ref type="bibr" target="#b5">Fung and Cheung, 2004</ref>), inducing parallel lexi- cons via canonical correlation analysis (Haghighi * Corresponding author: Yang Liu.</p><p>et <ref type="bibr">al., 2008)</ref>, training IBM models on monolin- gual corpora as decipherment ( <ref type="bibr" target="#b20">Ravi and Knight, 2011;</ref><ref type="bibr" target="#b17">Nuhn et al., 2012;</ref><ref type="bibr" target="#b4">Dou et al., 2014</ref>), and deriving parallel lexicons from bilingual word em- beddings <ref type="bibr" target="#b24">(Vuli´cVuli´c and Moens, 2013;</ref><ref type="bibr" target="#b15">Mikolov et al., 2013;</ref><ref type="bibr" target="#b25">Vuli´cVuli´c and Moens, 2015)</ref>.</p><p>Recently, a number of authors have turned to a more challenging task: learning parallel phrases from non-parallel corpora <ref type="bibr" target="#b27">(Zhang and Zong, 2013;</ref><ref type="bibr" target="#b3">Dong et al., 2015)</ref>. <ref type="bibr" target="#b27">Zhang and Zong (2013)</ref> present a method for retrieving parallel phrases from non-parallel corpora using a seed parallel lexicon. <ref type="bibr" target="#b3">Dong et al. (2015)</ref> continue this line of research to further introduce an iterative ap- proach to joint learning of parallel lexicons and phrases. They introduce a corpus-level latent- variable translation model in a non-parallel sce- nario and develop a training algorithm that alter- nates between (1) using a parallel lexicon to ex- tract parallel phrases from non-parallel corpora and (2) using the extracted parallel phrases to en- large the parallel lexicon. They show that starting from a small seed lexicon, their approach is capa- ble of learning both new words and phrases grad- ually over time.</p><p>However, due to the structural divergence be- tween natural languages as well as the presence of noisy data, only using asymmetric translation models might be insufficient to accurately identify parallel lexicons and phrases from non-parallel corpora. <ref type="bibr" target="#b3">Dong et al. (2015)</ref> report that the ac- curacy on Chinese-English dataset is only around 40% after running for 70 iterations. In addition, their approach seems prone to be affected by noisy data in non-parallel corpora as the accuracy drops significantly with the increase of noise.</p><p>Since asymmetric word alignment and phrase alignment models are usually complementary, it is natural to combine them to make more accu- rate predictions. In this work, we propose to in-troduce agreement-based learning ( <ref type="bibr" target="#b11">Liang et al., 2006;</ref> into extracting parallel lexicons and phrases from non-parallel corpora. Based on the latent-variable model proposed by <ref type="bibr" target="#b3">Dong et al. (2015)</ref>, we propose two kinds of loss functions to take into account the agreement between both phrase alignment and word align- ment in two directions. As the inference is in- tractable, we resort to a Viterbi EM algorithm to train the two models efficiently. Experiments on the Chinese-English dataset show that agreement- based learning is more robust to noisy data and leads to substantial improvements in phrase align- ment and machine translation evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Given a monolingual corpus of source language phrases E = {e (s) } S s=1 and a monolingual cor- pus of target language phrases F = {f (t) } T t=1 , we assume there exists a parallel corpus D = {{e (s) , f (t) |e (s) ↔ f (t) }, where e (s) ↔ f (t) de- notes that e (s) and f (t) are translations of each other.</p><p>As a long sentence in E is usually unlikely to have an translation in F and vise versa, most pre- vious efforts build on the assumption that phrases are more likely to have translational equivalents on the other side ( <ref type="bibr" target="#b16">Munteanu and Marcu, 2006;</ref><ref type="bibr" target="#b1">Cettolo et al., 2010;</ref><ref type="bibr" target="#b27">Zhang and Zong, 2013;</ref><ref type="bibr" target="#b3">Dong et al., 2015)</ref>. Such a set of phrases can be con- structed by collecting either constituents of parsed sentences or strings with hyperlinks on webpages (e.g., Wikipedia). Therefore, we assume the two monolingual corpora are readily available and fo- cus on how to extract D from E and F .</p><p>To address this problem, <ref type="bibr" target="#b3">Dong et al. (2015)</ref> introduce a corpus-level latent-variable translation model in a non-parallel scenario:</p><formula xml:id="formula_0">P (F |E; θ) = m P (F, m|E; θ) phrase alignment ,<label>(1)</label></formula><p>where m is phrase alignment and θ is a set of model parameters. Each target phrase f (t) is restricted to connect to exactly one source phrase: m = (m 1 , . . . , m t , . . . m T ), where m t ∈ {0, 1, . . . , S}. For example, m t = s denotes that f (t) is aligned to e (s) . Note that e (0) represents an empty source phrase. They follow IBM Model 1 ( <ref type="bibr" target="#b0">Brown et al., 1993)</ref> to further decompose the model as</p><formula xml:id="formula_1">P (F, m|E; θ) = p(T |S) (S + 1) T T t=1 P (f (t) |e (mt) ; θ),<label>(2)</label></formula><p>where P (f (t) |e (mt) ; θ) is a phrase translation model that can be further defined as</p><formula xml:id="formula_2">P (f (t) |e (mt) ; θ) = δ(m t , 0) + (1 − δ(m t , 0)) a P (f (t) , a|e (mt) ; θ) word alignment .<label>(3)</label></formula><p>Dong et al. <ref type="formula" target="#formula_0">(2015)</ref> distinguish between empty and non-empty phrase translations. If a target phrase f (t) is aligned to the empty source phrase e (0) (i.e., m t = 0), they set the phrase transla- tion probability to a fixed number . Otherwise, conventional word alignment models such as IBM Model 1 can be used for non-empty phrase trans- lation:</p><formula xml:id="formula_3">P (f (t) , a|e (mt) ; θ) = p(J (t) |I (mt) ) (I (mt) + 1) J (t) J (t) j=1 p(f (t) j |e (mt) a j ),<label>(4)</label></formula><p>where p(J|I) is a length model and p(f |e) is a translation model. We use J (t) to denote the length of f (t) . Therefore, the latent-variable model involves two kinds of latent structures: (1) phrase align- ment m between source and target phrases, (2) word alignment a between source and target words within phrases.</p><p>Given the two monolingual corpora E and F , the training objective is to maximize the likelihood of the training data:</p><formula xml:id="formula_4">θ * = argmax θ L(θ) ,<label>(5)</label></formula><p>where</p><formula xml:id="formula_5">L(θ) = log P (F |E; θ) − I λ I J p(J|I) − 1 − e γ e f p(f |e) − 1 − f e σ(f, e, d) log σ(f, e, d) p(f |e)</formula><p>. <ref type="formula">(</ref>   <ref type="formula" target="#formula_0">(14)</ref>) aims to encourage the agreement at the phrase level.</p><p>Note that d is a small seed parallel lexicon for ini- tializing training 1 and σ(f, e, d) checks whether an entry f, e exists in d.</p><p>Given the monolingual corpora and the opti- mized model parameters, the Viterbi phrase align- ment is calculated as</p><formula xml:id="formula_6">m * = argmax m P (F, m|E; θ * )<label>(7)</label></formula><p>= argmax</p><formula xml:id="formula_7">m T t=1 P (f (t) |e (mt) ; θ * )</formula><p>. <ref type="formula">(8)</ref> Finally, parallel lexicons can be derived from the translation probability table of IBM model 1 θ * and parallel phrases can be collected from the Viterbi phrase alignment m * . This process iterates and enlarges parallel lexicons and phrases gradu- ally over time.</p><p>As it is very challenging to extract parallel phrases from non-parallel corpora, unidirectional models might only capture partial aspects of trans- lation modeling on non-parallel corpora. Indeed, <ref type="bibr" target="#b3">Dong et al. (2015)</ref> find that the accuracy of phrase alignment is only around 50% on the Chinese- English dataset. More importantly, their approach seems to be vulnerable to noise as the accuracy drops significantly with the increase of noise. As source-to-target and target-to-source transla- tion models are usually complementary <ref type="bibr" target="#b18">(Och and Ney, 2003;</ref><ref type="bibr" target="#b8">Koehn et al., 2003;</ref><ref type="bibr" target="#b11">Liang et al., 2006</ref>), it is appealing to combine them to improve align- ment accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Agreement-based Learning</head><p>The basic idea of our work is to encourage the source-to-target and target-to-source translation models to agree on both phrase and word align- ments.</p><p>For example, <ref type="figure" target="#fig_0">Figure 1</ref> shows two exam- ple Chinese-to-English and English-to-Chinese phrase alignments on the same non-parallel data. As each model only captures partial aspects of translation modeling, our intuition is that the links on which two models agree (highlighted in red) are more likely to be correct.</p><p>More formally, let P (F |E; − → θ ) be a source- to-target translation model and P (E|F ; ← − θ ) be a target-to-source model, where</p><formula xml:id="formula_8">− → θ and ← − θ are corresponding model parameters. We use − → m = ( − → m 1 , . . . , − → m t , . . . , − → m T )</formula><p>to denote source- to-target phrase alignment. Likewise, the target- to-source phrase alignment is denoted by</p><formula xml:id="formula_9">← − m = ( ← − m 1 , . . . , ← − m s , . . . , ← − m S ).</formula><p>To ease the comparison between − → m and ← − m, we represent them as sets of non-empty links equiva- lently:</p><formula xml:id="formula_10">− → m = − → m t , t| − → m t = 0<label>(9)</label></formula><formula xml:id="formula_11">← − m = s, ← − m s | ← − m s = 0 .<label>(10)</label></formula><p>For example, suppose the source-to-target and target-to-source phrase alignments are</p><formula xml:id="formula_12">− → m = 1: procedure VITERBIEM(E, F , d) 2:</formula><p>Initialize Θ <ref type="formula">(0)</ref> 3:</p><formula xml:id="formula_13">for all k = 1, . . . , K do 4: ˆ m (k) ← SEARCH(E, F, Θ (k−1) ) 5: Θ (k) ← UPDATE(E, F, d, ˆ m (k) ) 6:</formula><p>end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>returnˆmreturnˆ returnˆm (K) , Θ (K) 8: end procedure <ref type="figure">Figure 2</ref>: A Viterbi EM algorithm for agreement- based learning of parallel lexicons and phrases from non-parallel corpora. F and E are non- parallel corpora, d is a seed parallel lexicon, Θ <ref type="bibr">(k)</ref> is the set of model parameters at the k-th iteration, ˆ m (k) is the Viterbi phrase alignment on which two models agree at the k-th iteration.  <ref type="formula" target="#formula_1">(2006)</ref>, we introduce a new training objective that favors the agreement between two unidirectional models:</p><formula xml:id="formula_14">J ( − → θ , ← − θ ) = log P (F |E; − → θ ) + log P (E|F ; ← − θ ) − log − → m, ← − m P ( − → m|E, F ; − → θ )P ( ← − m|F, E; ← − θ ) ×∆(E, F, − → m, ← − m, − → θ , ← − θ ),<label>(11)</label></formula><p>where the posterior probabilities in two directions are defined as</p><formula xml:id="formula_15">P ( − → m|E, F ; − → θ ) = T t=1 P (f (t) |e ( − → mt) ; − → θ ) S s=0 P (f (t) |e (s) ; − → θ )<label>(12)</label></formula><formula xml:id="formula_16">P ( ← − m|F, E; ← − θ ) = S s=1 P (e (s) |f ( ← − ms) ; ← − θ ) T t=0 P (e (s) |f (t) ; ← − θ ) .<label>(13)</label></formula><p>The loss function</p><formula xml:id="formula_17">∆(E, F, − → m, ← − m, − → θ , ← − θ )</formula><p>mea- sures the disagreement between the two models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Outer Agreement</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Definition</head><p>A straightforward loss function is to force the two models to generate identical phrase alignments:</p><formula xml:id="formula_18">∆ outer (E, F, − → m, ← − m, − → θ , ← − θ ) = 1 − δ( − → m, ← − m).<label>(14)</label></formula><p>We refer to Eq. <ref type="formula" target="#formula_0">(14)</ref> as outer agreement since it only considers phrase alignment and ignores the word alignment within aligned phrases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Training Objective</head><p>Since the outer agreement forces two models to generate identical phrase alignments, the training objective can be written as</p><formula xml:id="formula_19">J outer ( − → θ , ← − θ ) = log P (F |E; − → θ ) + log P (E|F ; ← − θ ) + log m P (m|E, F ; − → θ )P (m|F, E; ← − θ ),<label>(15)</label></formula><p>where m is a phrase alignment on which two mod- els agree. The partial derivatives of the training objective with respect to source-to-target model parameters − → θ are given by</p><formula xml:id="formula_20">∂J outer ( − → θ , ← − θ ) ∂ − → θ = ∂P (F |E; − → θ )/∂ − → θ P (F |E; − → θ ) + E m|F,E; ← − θ ∂P (F |E; − → θ )/∂ − → θ m P (m|E, F ; − → θ )P (m|F, E; ← − θ ) .<label>(16)</label></formula><p>The partial derivatives with respect to ← − θ are de- fined likewise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Training Algorithm</head><p>As the expectation in Eq. (16) is usually in- tractable to calculate due to the exponential search space of phrase alignment, we follow <ref type="bibr" target="#b3">Dong et al. (2015)</ref> to use a Viterbi EM algorithm instead.</p><p>As shown in <ref type="figure">Figure 2</ref>, the algorithm takes a set of source phrases E, a set of target phrases F , and a seed parallel lexicon d as input (line 1). After initializing model parameters</p><formula xml:id="formula_21">Θ = { − → θ , ← − θ } (line 2)</formula><p>, the algorithm calls the procedure ALIGN(F, E, Θ) to compute the Viterbi phrase alignment between E and F on which two mod- els agree. Then, the algorithm updates the two models by normalizing counts collected from the Viterbi phrase alignment. The process iterates for K iterations and returns the final Viterbi phrase alignment and model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Computing Viterbi Phrase Alignments</head><p>The procedure ALIGN(F, E, Θ) computes the Viterbi phrase alignmentˆmalignmentˆ alignmentˆm between E and F on which two models agree as follows:</p><formula xml:id="formula_22">ˆ m = argmax m P (m|E, F ; − → θ ) × P (m|F, E; ← − θ ) .<label>(17)</label></formula><p>Unfortunately, due to the exponential search space of phrase alignment, computingˆmcomputingˆ computingˆm is also intractable. As a result, we approximate it as the intersection of two unidirectional Viterbi phrase alignments:</p><formula xml:id="formula_23">ˆ m ≈ − → m * ∩ ← − m * ,<label>(18)</label></formula><p>where the unidirectional Viterbi phrase alignments are calculated as</p><formula xml:id="formula_24">− → m * = argmax − → m T t=1 P (f (t) |e ( − → mt) ; − → θ )<label>(19)</label></formula><formula xml:id="formula_25">← − m * = argmax ← − m S s=1 P (e (s) |f ( ← − ms) ; ← − θ )</formula><p>.</p><p>The source-to-target Viterbi phrase alignment is calculated as</p><formula xml:id="formula_27">− → m * = argmax − → m P ( − → m|E, F ; − → θ )<label>(21)</label></formula><formula xml:id="formula_28">= argmax − → m T t=1 P (f (t) |e ( − → mt) ; − → θ ) .<label>(22)</label></formula><p>Dong et al. <ref type="formula" target="#formula_0">(2015)</ref> indicate that computing the Viterbi alignment for individual target phrases is independent and only need to focus on finding the most probable source phrase for each target phrase:</p><formula xml:id="formula_29">− → m * t = argmax s∈{0,1,...,S} P (f (t) |e (s) ; − → θ ) .<label>(23)</label></formula><p>This can be cast as a translation retrieval prob- lem ( <ref type="bibr" target="#b27">Zhang and Zong, 2013;</ref><ref type="bibr" target="#b2">Dong et al., 2014)</ref>. Please refer to ( <ref type="bibr" target="#b3">Dong et al., 2015</ref>) for more details. The target-to-source Viterbi phrase alignment can be calculated similarly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.5">Updating Model Parameters</head><p>Following <ref type="bibr" target="#b11">Liang et al. (2006)</ref>, we collect counts of model parameters only from the agreement term. <ref type="bibr">2</ref> Given the agreed Viterbi phrase alignmentˆmalignmentˆ alignmentˆm, the count of the source-to-target length model p(J|I) is given by c(J|I; E, F ) = s,t∈ˆmt∈ˆ t∈ˆm δ(J (t) , J)δ(I (s) , I). <ref type="formula" target="#formula_1">(24)</ref> The new length probabilities can be obtained by</p><formula xml:id="formula_30">p(J|I) = c(J|I; E, F ) J c(J |I; E, F ) .<label>(25)</label></formula><p>2 We experimented with collecting counts from both the unidirectional and agreement terms but obtained much worse results than counting only from the agreement term. </p><formula xml:id="formula_31">p(f |e) I (s) i=0 p(f |e (s) i ) × J (t) j=1 δ(f, f (t) j ) I (s) i=0 δ(e, e (s) i ) +σ(f, e, d).<label>(26)</label></formula><p>The new translation probabilities can be ob- tained by</p><formula xml:id="formula_32">p(f |e) = c(f |e; E, F ) f c(f |e; E, F ) .<label>(27)</label></formula><p>Counts of target-to-source length and transla- tion models can be calculated in a similar way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Inner Agreement</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Definition</head><p>As the outer agreement only considers the phrase alignment, the inner agreement takes both phrase alignment and word alignment into consideration:</p><formula xml:id="formula_33">∆ inner (E, F, − → m, ← − m, − → θ , ← − θ ) = −δ( − → m, ← − m) × s,t∈ − → m − → a , ← − a P ( − → a |e (s) , f (t) ; − → θ ) × P ( ← − a |f (t) , e (s) ; ← − θ ) × δ( − → a , ← − a ).<label>(28)</label></formula><p>For example, <ref type="figure" target="#fig_2">Figure 3</ref> shows two examples of Chinese-to-English and English-to-Chinese word alignments. The shared links are highlighted in red. Our intuition is that a source phrase and a target phrase are more likely to be translations of each other if the two translation models also agree on word alignment within aligned phrases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Training Objective and Algorithm</head><p>The training objective for inner agreement is given by</p><formula xml:id="formula_34">J inner ( − → θ , ← − θ ) = log P (F |E; − → θ ) + log P (E|F ; ← − θ ) + log m P (m|E, F ; − → θ )P (m|F, E; ← − θ ) × s,t∈m a P (a|e (s) , f (t) ; − → θ ) × P (a|f (t) , e (s) ; ← − θ ). (29)</formula><p>We still use the Viterbi EM algorithm as shown in <ref type="figure">Figure 2</ref> for training the two models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Computing Viterbi Phrase Alignments</head><p>The agreed Viterbi phrase alignment is defined asˆm asˆ asˆm = argmax</p><formula xml:id="formula_35">m P (m|E, F ; − → θ )P (m|F, E; ← − θ ) × s,t∈m a P (a|e (s) , f (t) ; − → θ ) ×P (a|f (t) , e (s) ; ← − θ ) .<label>(30)</label></formula><p>As computingˆmcomputingˆ computingˆm is intractable, we still approxi- mate it using the intersection of two unidirectional Viterbi phrase alignments (see Eq. <ref type="formula" target="#formula_0">(18)</ref>). The source-to-target Viterbi phrase alignment is calcu- lated as</p><formula xml:id="formula_36">− → m * = argmax − → m P ( − → m|E, F ; − → θ ) × s,t∈ − → m J (t) j=1 I (s) i=1 P (i, j|e (s) , f (t) ; − → θ ) × P (i, j|f (t) , e (s) ; ← − θ ) ,<label>(31)</label></formula><p>where P (i, j|e (s) , f (t) ; − → θ ) is source-to-target link posterior probability of the link i, j be- ing present (or absent) in the word align- ment according to the source-to-target model,</p><formula xml:id="formula_37">P (i, j|f (t) , e (s) ; ← − θ )</formula><p>is target-to-source link pos- terior probability. We follow <ref type="bibr" target="#b11">Liang et al. (2006)</ref> to use the product of link posteriors to encourage the agreement at the level of word alignment.</p><p>We use a coarse-to-fine approach ( <ref type="bibr" target="#b3">Dong et al., 2015)</ref> to compute the Viterbi alignment: first re- trieving a coarse set of candidate source phrases using translation probabilities and then selecting the candidate with the highest score according to <ref type="bibr">Eq. (31)</ref>. The target-to-source Viterbi phrase alignment can be calculated similarly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4">Updating Model Parameters</head><p>Given the agreed Viterbi phrase alignmentˆmalignmentˆ alignmentˆm, the count of the source-to-target length model p(J|I) is still given by Eq. <ref type="bibr">(24)</ref>. The count of the transla- tion model p(f |e) is calculated as c(f |e; E, F ) = s,t∈ˆmt∈ˆ t∈ˆm</p><formula xml:id="formula_38">I (s) i=1 J (t) j=1 P (i, j|e (s) , f (t) ; − → θ ) × P (i, j|f (t) , e (s) ; ← − θ ) × δ(f, f (t) )δ(e, e (s) ) +σ(f, e, d).<label>(32)</label></formula><p>Counts of target-to-source length and transla- tion models can be calculated in a similar way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we evaluate our approach in two tasks: phrase alignment (Section 4.1) and machine translation (Section 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Alignment Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Evaluation Metrics</head><p>Given two monolingual corpora E and F , we sup- pose there exists a ground truth parallel corpus G and denote an extracted parallel corpus as D. The quality of an extracted parallel corpus can be mea- sured by F1 = 2|D ∩ G|/(|D| + |G|).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Data Preparation</head><p>Although it is appealing to apply our approach to dealing with real-world non-parallel corpora, it is time-consuming and labor-intensive to manually construct a ground truth parallel corpus. There- fore, we follow <ref type="bibr" target="#b3">Dong et al. (2015)</ref> to build syn- thetic E, F , and G to facilitate the evaluation.</p><p>We first extract a set of parallel phrases from a sentence-level parallel corpus using the state- of-the-art phrase-based translation system Moses ( <ref type="bibr" target="#b9">Koehn et al., 2007)</ref> and discard low-probability parallel phrases. Then, E and F can be con- structed by corrupting the parallel phrase set by  <ref type="table">Table 1</ref>: Effect of seed lexicon size in terms of F1 on the development set.</p><p>adding irrelevant source and target phrases ran- domly. Note that the parallel phrase set can serve as the ground truth parallel corpus G. We refer to the non-parallel phrases in E and F as noise.</p><p>From LDC Chinese-English parallel corpora, we constructed a development set and a test set. The development set contains 20K paral- lel phrases, 20K noisy Chinese phrases, and 20K noisy English phrases. The test test contains 20K parallel phrases, 180K noisy Chinese phrases, and 180K noisy English phrases. The seed parallel lex- icon contains 1K entries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Comparison of Agreement Ratios</head><p>We introduce agreement ratio to measure to what extent two unidirectional models agree on phrase alignment: <ref type="figure">Figure 4</ref> shows the agreement ratios of inde- pendent training ("no agreement"), joint training with the outer agreement ("outer"), and joint train- ing with the inner agreement ("inner"). We find that independently trained unidirectional models  <ref type="table">Table 2</ref>: Effect of noise in terms of F1 on the de- velopment set.</p><formula xml:id="formula_39">ratio = 2| − → m * ∩ ← − m * | | − → m * | + | ← − m * | . (33)</formula><formula xml:id="formula_40">noise C → E E → C Outer Inner C E 0</formula><p>hardly agree on phrase alignment, suggesting that each model can only capture partial aspects of translation modeling on non-parallel corpora. In contrast, imposing the agreement term signifi- cantly increases the agreement ratios: after 10 it- erations, about 40% of phrase alignment links are shared by two models. <ref type="table">Table 1</ref> shows the F1 scores of the Chinese-to- English model ("C → E"), the English-to-Chinese model ("E → C"), joint learning based on the outer agreement ("outer"), and jointing learning based on the inner agreement ("inner") over various sizes of seed lexicons on the development set. We find that agreement-based learning obtains substantial improvements over independent learn- ing across all sizes. More importantly, even with a seed lexicon containing only 50 entries, agreement-based learning is able to achieve F1 scores above 60%. The inner agreement performs better than the outer agreement by taking the con- sensus at the word level into account. <ref type="table">Table 2</ref> demonstrates the effect of noise on the de- velopment set. In row 1, "0+0" denotes there is no noise, which can be seen as an upper bound. Adding noise, either on the Chinese side or on the English side, deteriorates the F1 scores for all methods. Adding noise on the English side makes predicting phrase alignment in the C → E direc- tion more challenging due to the enlarged search space. The situation is similar in the reverse di- rection. It is clear that agreement-based learning is more robust to noise: while independent train- ing suffers from a reduction of 40% in terms of F1 for the "20K + 20K" setting, agreement-based learning still achieves F1 scores over 70%.  <ref type="table">Table 3</ref>: Example learned parallel lexicons and phrases. New words that are not included in the seed lexicon are highlighted in italic. <ref type="figure" target="#fig_4">Figure 5</ref> gives the final results on the test set. We find that agreement-based training achieves significant improvements over independent train- ing. By considering the consensus on both phrase and word alignments, the inner agreement signif- icantly outperforms the outer agreement. Notice that <ref type="bibr" target="#b3">Dong et al. (2015)</ref> only add noise on one side while we add noisy phrases on both sides, which makes phrase alignment more challenging. <ref type="table">Table 3</ref> shows example learned parallel words and phrases. The lexicon is built from the transla- tion table by retaining high-probability word pairs. Therefore, our approach is capable of learning both new words and new phrases unseen in the seed lexicon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Effect of Seed Lexicon Size</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.5">Effect of Noise</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.6">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Translation Evaluation</head><p>Following <ref type="bibr" target="#b27">Zhang and Zong (2013)</ref> and <ref type="bibr" target="#b3">Dong et al. (2015)</ref>, we evaluate our approach on domain adaptation for machine translation.</p><p>The data set consists of two in-domain non- parallel corpora and an out-domain parallel cor- pus. The in-domain non-parallel corpora con- sists of 2.65M Chinese phrases and 3.67M English phrases extracted from LDC news articles. We use a small out-domain parallel corpus extracted from financial news of FTChina which contains 10K phrase pairs. The task is to extract a parallel corpus from in-domain non-parallel corpora start- ing from a small out-domain parallel corpus.</p><p>We use the state-of-the-art translation system Moses ( <ref type="bibr" target="#b9">Koehn et al., 2007)</ref> and evaluate the per- formance on Chinese-English NIST datasets. The development set is NIST 2006 and the test set is NIST 2005. The evaluation metric is case- insensitive BLEU4 ( <ref type="bibr" target="#b19">Papineni et al., 2002</ref>). We use the SRILM toolkit <ref type="bibr" target="#b22">(Stolcke, 2002</ref>) to train a 4-gram English language model on a monolingual corpus with 399M English words. <ref type="table" target="#tab_3">Table 4</ref> shows the results. At iteration 0, only the out-domain corpus is used and the BLEU score is 5.61. All methods iteratively extract parallel phrases from non-parallel corpora and enlarge the extracted parallel corpus. We find that agreement- based learning achieves much higher BLEU scores while obtains a smaller parallel corpus as com- pared with independent learning. One possible reason is that the agreement-based learning rules out most unlikely phrase pairs by encouraging consensus between two models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented agreement-based training for learning parallel lexicons and phrases from non- parallel corpora. By modeling the agreement on both phrase alignment and word alignment, our approach achieves significant improvements in both alignment and translation evaluations.</p><p>In the future, we plan to apply our approach to real-world non-parallel corpora to further ver- ify its effectiveness. It is also interesting to ex- tend the phrase translation model to more sophis- ticated models such as IBM models 2-5 ( <ref type="bibr" target="#b0">Brown et al., 1993)</ref> and HMM ( <ref type="bibr" target="#b23">Vogel and Ney, 1996)</ref>. <ref type="table" target="#tab_3">0  10k  5.61  1  145k  162k  59k  73k  8.65  8.90 13.53 13.74  2  195k  215k  69k  101k  8.82  9.47 15.26 15.61  3  209k  231k  88k  132k  8.</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Iteration</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corpus Size BLEU E→ C C→ E Outer Inner E→ C C→ E Outer Inner</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Agreement between (a) Chinese-to-English and (b) English-to-Chinese phrase alignments. The arrows indicate translation directions. The links on which two models agree are highlighted in bold red. The outer agreement loss function (see Eq. (14)) aims to encourage the agreement at the phrase level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Following Liang et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Agreement between (a) Chinese-toEnglish and (b) English-to-Chinese word alignments. The links on which two models agree are highlighted in red. The inner agreement loss function (see Eq. (28)) aims to encourage the agreement at both the phrase and word levels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 4: Comparison of agreement ratios on the development set. seed C → E E → C Outer Inner 50 4.1 4.8 60.8 66.2 100 5.1 5.5 65.6 69.8 500 7.5 8.4 70.4 72.5 1,000 22.4 23.1 73.6 74.3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Comparison of F1 scores on the test set. Chinese jingji English economy Chinese jialebi English caribbean Chinese zhengzhi huanjing English political environment Chinese jiaoyisuo shichang jiage zhishu English exchange market price index Chinese qianding bianjing maoyi xieding English signed border trade agreements</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Results on domain adaptation for machine translation. 

ported by the National Natural Science Founda-
tion of China (No. 61522204), the 863 Program 
(2015AA011808), and Samsung R&amp;D Institute of 
China. Huanbo Luan is supported by the Na-
tional Natural Science Foundation of China (No. 
61303075). Maosong Sun is supported by the Ma-
jor Project of the National Social Science Founda-
tion of China (13&amp;ZD190). </table></figure>

			<note place="foot" n="1"> Due to the difficulty of learning translation correspondences from non-parallel corpora, many authors have assumed that a small seed lexicon is readily available (Gaussier et al., 2004; Zhang and Zong, 2013; Vuli´cVuli´c and Moens, 2013; Mikolov et al., 2013; Dong et al., 2015).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We sincerely thank the reviewers for their valuable suggestions. We also thank Meng Zhang, Yankai Lin, Shiqi Shen, Meiping Dong and Congyu Fu for their insightful discussions. Yang Liu is sup-</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The mathematics of statistical machine translation: Parameter estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">A Della</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguisitics</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mining parallel fragments from comparable texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IWSLT</title>
		<meeting>IWSLT</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Query lattice for translation retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meiping</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Izuha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Iterative learning of parallel lexicons and phrases from non-parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meiping</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Izuha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dakun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Beyond parallel data: Joint word alignment and decipherment improves machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mining verynon-parallel corpora: Parallel sentence and lexicon extraction via bootstrapping and em</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A geometric view on bilingual lexicon extraction from comparable corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Renders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matveeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Goutte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dejean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning bilingual lexicons from monolingual corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions</title>
		<meeting>the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume the Demo and Poster Sessions</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An algorithm for finding noun phrase correspondences in bilingual corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Kupiec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Alignment by agreement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Alignment-based learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A phrase-based joint probability model for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automatic discovery of noncompositional compounds in parallel data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Melamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Exploiting similarities among languages for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.4168</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Extracting parallel sub-sentential fragments from nonparallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Dragos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Munteanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deciphering foreign language by combining language models and context vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malte</forename><surname>Nuhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arne</forename><surname>Mauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A systematic comparison of various statistical alignment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguisitics</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bleu: a methof for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deciphering foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Translating collocations for use in bilingual lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Smadja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ARPA Human Language Technology Workshop</title>
		<meeting>the ARPA Human Language Technology Workshop</meeting>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Srilm-an extensible language modeling toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICSLP</title>
		<meeting>ICSLP</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hhm-based word alignment in statistical translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A study on bootstrapping bilingual vector spaces from nonparallel data (and nothing else)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bilingual word embeddings from non-parallel documentaligned data applied to bilingual lexicon induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning an english-chinese lexicon from a parallel corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyin</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ARPA Human Language Technology Workshop</title>
		<meeting>the ARPA Human Language Technology Workshop</meeting>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning a phrase-based translation model from monolingual data with application to domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
