<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:23+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Ground Truth for Grammatical Error Correction Metrics</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Language and Speech Processing</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Language and Speech Processing</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Human Language Technology Center of Excellence</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Yahoo Labs</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Ground Truth for Grammatical Error Correction Metrics</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="588" to="593"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>How do we know which grammatical error correction (GEC) system is best? A number of metrics have been proposed over the years, each motivated by weaknesses of previous metrics; however, the metrics themselves have not been compared to an empirical gold standard grounded in human judgments. We conducted the first human evaluation of GEC system outputs, and show that the rankings produced by metrics such as MaxMatch and I-measure do not correlate well with this ground truth. As a step towards better metrics, we also propose GLEU, a simple variant of BLEU, modified to account for both the source and the reference, and show that it hews much more closely to human judgments .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic metrics are a critical component for all tasks in natural language processing. For many tasks, such as parsing and part-of-speech tagging, there is a single correct answer, and thus a sin- gle metric to compute it. For other tasks, such as machine translation or summarization, there is no effective limit to the size of the set of correct answers. For such tasks, metrics proliferate and compete with each other for the role of the domi- nant metric. In such cases, an important question to answer is by what means such metrics should be compared. That is, what is the metric metric?</p><p>The answer is that it should be rooted in the end-use case for the task under consideration. This could be some other metric further downstream of the task, or something simpler like direct human evaluation. This latter approach is the one often taken in machine translation; for example, the or- ganizers of the Workshop on Statistical Machine Translation have long argued that human evalua- tion is the ultimate ground truth, and have there- fore conducted an extensive human evaluation to produce a system ranking, which is then used to compare metrics ( <ref type="bibr" target="#b0">Bojar et al., 2014</ref>).</p><p>Unfortunately, for the subjective task of gram- matical error correction (GEC), no such ground truth has ever been established. Instead, the rank- ings produced by new metrics are justified by their correlation with explicitly-corrected errors in one or more references, and by appeals to intuition for the resulting rankings. However, arguably even more so than for machine translation, the use case for grammatical error correction is human con- sumption, and therefore, the ground truth ranking should be rooted in human judgments.</p><p>We establish a ground truth for GEC by con- ducting a human evaluation and producing a hu- man ranking of the systems entered into the CoNLL-2014 Shared Task on GEC. We find that existing GEC metrics correlate very poorly with the ranking produced by this human evaluation. As a step in the direction of better metrics, we de- velop the Generalized Language Evaluation Un- derstanding metric (GLEU) inspired by BLEU, which correlates much better with the human rank- ing than current GEC metrics. <ref type="bibr">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Grammatical error correction metrics</head><p>GEC is often viewed as a matter of correcting iso- lated grammatical errors, but is much more com- plicated, nuanced, and subjective than that. As dis- cussed in <ref type="bibr" target="#b2">Chodorow et al. (2012)</ref>, there is often no single correction for an error (e.g., whether to correct a subject-verb agreement error by chang- ing the number of the subject or the verb), and er- rors cover a range of factors including style, reg- ister, venue, audience, and usage questions, about which there can be much disagreement. In addi- tion, errors are not always errors, as can be seen from the existence of different style manuals at newspapers, and questions about the legitimacy of prescriptivist grammar conventions.</p><p>Several automatic metrics have been used for evaluating GEC systems. F-score, the harmonic mean of precision and recall, is one of the most commonly used metrics. It was used as an official evaluation metric for several shared tasks <ref type="bibr" target="#b5">(Dale et al., 2012;</ref><ref type="bibr" target="#b4">Dale and Kilgarriff, 2011)</ref>, where par- ticipants were asked to detect and correct closed- class errors (i.e., determiners and prepositions).</p><p>One of the issues with F-score is that it fails to capture phrase-level edits. Thus <ref type="bibr" target="#b3">Dahlmeier and Ng (2012)</ref> proposed the MaxMatch (M 2 ) scorer, which calculates the F-score over an edit lattice that captures phrase-level edits. For GEC, M 2 is the standard, having been used to rank error correction systems in the 2013 and 2014 CoNLL shared tasks, where the error types to be corrected were not limited to closed-class errors. ( <ref type="bibr" target="#b9">Ng et al., 2013;</ref><ref type="bibr" target="#b10">Ng et al., 2014</ref>). M 2 was assessed by com- paring its output against that of the official Help- ing Our Own (HOO) scorer ( <ref type="bibr" target="#b4">Dale and Kilgarriff, 2011)</ref>, itself based on the GNU wdiff utility. <ref type="bibr">2</ref> In other words, it was evaluated under the assump- tion that evaluating GEC can be reduced to check- ing whether a set of predefined errors have been changed into a set of associated corrections.</p><p>M 2 is not without its own issues. First, phrase- level edits can be gamed because the lattice treats a long phrase deletion as one edit. 3 Second, the F-score does not capture the difference between "no change" and "wrong edits" made by systems. <ref type="bibr" target="#b2">Chodorow et al. (2012)</ref> also list other complica- tions arising from using F-score or M 2 , depending on the application of GEC.</p><p>Considering these problems, <ref type="bibr" target="#b7">Felice and Briscoe (2015)</ref> proposed a new metric, I-measure, which is based on accuracy computed by edit distance between the source, reference, and system output. Their results are striking: there is a negative corre- lation between the M 2 and I-measure scores (Pear- son's r = −0.694).</p><p>A difficulty with all these metrics is that they require detailed annotations of the location and er-   ror type of each correction in response to an ex- plicit error annotation scheme. Due to the inherent subjectivity and poor definition of the task, men- tioned above, it is difficult for annotators to reli- ably produce these annotations <ref type="bibr" target="#b1">(Bryant and Ng, 2015)</ref>. However, this requirement can be relin- quished by treating GEC as a text-to-text rewriting task and borrowing metrics from machine trans- lation, as Park and Levy (2011) did with BLEU ( <ref type="bibr" target="#b11">Papineni et al., 2002</ref>) and METEOR <ref type="bibr" target="#b8">(Lavie and Agarwal, 2007)</ref>. As we will show in more detail in Section 5, taking the twelve publicly released system out- puts from the CoNLL-2014 Shared Task, <ref type="bibr">4</ref> we ac- tually find a negative correlation between the M 2 and BLEU scores (r = −0.772) and positive correlation between I-measure and BLEU scores (r = 0.949) <ref type="figure" target="#fig_2">(Figure 1</ref>). With the earlier-reported negative correlation between I-measure and M 2 , we have a troubling picture: which of these met- rics is best? Which one actually captures and re- wards the behaviors we would like our systems to report? Despite these many proposed metrics, no prior work has attempted to answer these ques- tions by comparing them to human judgments. We propose to answer these questions by producing a definitive human ranking, against which the rank- ings of different metrics can be compared.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M2 Score</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The human ranking</head><p>The Workshop on Statistical Machine Translation (WMT) faces the same question each year as part <ref type="figure">Figure 2</ref>: The Appraise evaluation system. of its metrics shared task. Arguing that humans are the ultimate judge of quality, they gather hu- man judgments and use them to produce a ranking of the systems for each task. Machine translation metrics are then evaluated based on how closely they match this ranking, using Pearson's r (prior to 2014) or Spearman's ρ (2014).</p><p>We borrow their approach to conduct a human evaluation. We used Appraise <ref type="bibr" target="#b6">(Federmann, 2012)</ref>  <ref type="bibr">5</ref> to collect pairwise judgments among 14 systems: the output of 12 systems entered in the CoNLL-14 Shared Task, plus the source and a reference sen- tence. Appraise presents the judge with the source and reference sentence <ref type="bibr">6</ref> and asks her to rank four randomly selected systems from best to worst, ties allowed ( <ref type="figure">Figure 2</ref>). The four-way ranking is trans- formed into a set of pairwise judgments.</p><p>We collected data from three native English speakers, resulting in 28,146 pairwise system judgements. Each system's quality was estimated and the total ranking was produced on this dataset using the TrueSkill model ( <ref type="bibr" target="#b13">Sakaguchi et al., 2014</ref>), as done in WMT 2014. The annotators had strong correlations in terms of the total system ranking and estimated quality, with the reference being ranked at the top <ref type="table">(Table 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Generalized BLEU</head><p>Current metrics for GEC rely on references with explicitly labeled error annotations, the type and form of which vary from task to task and can Judges r ρ 1 and 2 0.80 0.69 1 and 3 0.73 0.80 2 and 3 0.81 0.71 <ref type="table">Table 1</ref>: Pearson's r and Spearman's ρ correla- tions among judges (excluding the reference).</p><p>be difficult to convert. Recognizing the inher- ent ambiguity in the error-correction task, a better metric might be independent of such an annota- tion scheme and only require corrected references. This is the view of GEC as a generic text-rewriting task, and it is natural to apply standard metrics from machine translation. However, applied off- the-shelf, these metrics yield unintuitive results. For example, BLEU ranks the source sentence as second place in the CoNLL-2014 shared task. <ref type="bibr">7</ref> The problem is partially due to the subtle but important difference between machine translation and monolingual text-rewriting tasks. In MT, an untranslated word or phrase is almost always an error, but in grammatical error correction, this is not the case. Some, but not all, regions of the source sentence should be changed. This obser- vation motivates a small change to BLEU that computes n-gram precisions over the reference but assigns more weight to n-grams that have been correctly changed from the source. This revised metric, Generalized Language Evaluation Under- standing (GLEU), rewards corrections while also correctly crediting unchanged source text.</p><p>Recall that BLEU(C, R) ( <ref type="bibr" target="#b11">Papineni et al., 2002</ref>) is computed as the geometric mean of the modified precision scores of the test sentences C relative to the references R, multiplied by a brevity penalty to control for recall. The precisions are computed over bags of n-grams derived from the candidate translation and the references. Each n-gram in the candidate sentence is "clipped" to the maximum count of that n-gram in any of the references, en- suring that no precision is greater than 1.</p><p>Similar to I-measure, which calculates a weighted accuracy of edits, we calculate a weighted precision of n-grams. In our adaptation, we modify the precision calculation to assign ex- tra weight to n-grams present in the candidate that overlap with the reference but not the source (the set of n-grams R \ S). The precision is also penal-</p><formula xml:id="formula_0">p n = n-gram∈C Count R\S (n-gram) − λ Count S\R (n-gram) + CountR(n-gram) n-gram ∈C CountS(n-gram ) + n-gram∈R\S Count R\S (n-gram)<label>(1)</label></formula><p>ized by a weighted count of n-grams in the can- didate that are in the source but not the reference (false negatives, S \ R). For a correction candidate C with a corresponding source S and reference R, the modified n-gram precision for GLEU(C,R,S) is shown in Equation 1. The weight λ determines by how much incorrectly changed n-grams are pe- nalized. Equations 2-3 describe how the counts are collected given a bag of n-grams B.</p><formula xml:id="formula_1">CountB(n-gram) = n-gram ∈B d(n-gram, n-gram ) (2) d(n-gram, n-gram ) = 1 if n-gram = n-gram 0 otherwise (3) BP = 1 if c &gt; r e (1−c/r) if c ≤ r (4) GLEU (C, R, S) = BP · exp N n=1</formula><p>wn log p n</p><p>In our experiments, we used N = 4 and w n = 1 N , which are standard parameters for MT, the same brevity penalty as BLEU <ref type="formula">(Equation 4)</ref>, and report results on λ = {0.1, 0} (GLEU 0.1 and GLEU 0 , respectively). For this task, not penal- izing false negatives correlates best with human judgments, but the weight can be tuned for dif- ferent tasks and datasets. GLEU can be easily ex- tended to additionally punish false positives (in- correctly editing grammatical text) as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>The respective system rankings of each metric are presented in <ref type="table" target="#tab_0">Table 2</ref>. The human ranking is con- siderably different from those of most of the met- rics, a fact that is also captured in correlation co- efficients <ref type="table">(Table 3)</ref>. <ref type="bibr">8</ref> From the human evaluation, we learn that the source falls near the middle of the rankings, even though the BLEU, I-measure and M 2 rank it among the best or worst systems. M 2 , the metric that has been used for the CoNLL shared tasks, only correlates moderately with human rankings, suggesting that it is not an ideal metric for judging the results of a competi- tion. Even though I-measure perceptively aims to predict whether an output is better or worse than the input, it actually has a slight negative correla- tion with human rankings. GLEU 0 is the only met- ric that strongly correlates with the human ranks, and performs closest to the range of human-to- human correlation (0.73 ≤ r ≤ 0.81) GLEU 0 correctly ranks four out of five of the top human- ranked systems at the top of its list, while the other metrics rank at most three of these systems in the top five.</p><p>All metrics deviate from the human rankings, which may in part be because automatic metrics equally weight all error types, when some errors may be more tolerable to human judges than oth- ers. For example, inserting a missing token is re- warded the same by automatic metrics, whether it is a comma or a verb, while a human would much more strongly prefer the insertion of the latter. An example of system outputs with their automatic scores and human rankings is included in <ref type="table">Table 4</ref>.</p><p>This example illustrates some challenges faced when using automatic metrics to evaluate GEC. The automatic metrics weight all corrections equally and are limited to the gold-standard refer- ences provided. Both automatic metrics, M 2 and GLEU, prefer the AMU output in this example, even though it corrects one error and introduces another. The human judges rank the UMC out- put as the best for correcting the main verb even though it ignored the spelling error. The UMC and NTHU sentences both receive M 2 = 0 because they make none of the gold-standard edits, even though UMC correctly inserts be into the sentence. M 2 does not recognize this since it is in a differ- ent location from where the annotators placed it.</p><p>Human BLEU I-measure M 2 GLEU0 GLEU0. <ref type="table">1  CAMB  UFC  UFC  CUUI  CUUI  CUUI  AMU  source  source  CAMB  AMU  AMU  RAC  IITB  IITB  AMU  UFC  CAMB  CUUI  SJTU  SJTU  POST  CAMB  UFC  source  UMC  CUUI  UMC  source  IITB  POST  CUUI  PKU  NTHU  IITB  SJTU  UFC  PKU  AMU  PKU  SJTU  PKU  SJTU  AMU  UMC  RAC  PKU  UMC  IITB  IPN  IPN  SJTU  UMC  NTHU  PKU  NTHU  POST  UFC  NTHU  POST  UMC  CAMB  RAC  IPN  POST  RAC  NTHU  RAC  CAMB  IITB  RAC  IPN  IPN  POST  NTHU  source  IPN  source</ref>  We may in actual fact communicating with a hoax Facebook acccount of a cyber friend , which we assume to be real but in reality , it is a fake account . - Reference 1 We may in actual fact be communicating with a hoax Facebook acccount of a cyber friend , which we assume to be real but in reality , it is a fake account . - Reference 2 We may in actual fact be communicating with a fake Facebook account of an online friend , which we assume to be real but , in reality , it is a fake account . -</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UMC</head><p>We may be in actual fact communicating with a hoax Facebook acccount of a cyber friend , we assume to be real but in reality , it is a fake account . GLEU = 0.62 M 2 = 0.00 Human rank= 1 AMU We may in actual fact communicating with a hoax Facebook account of a cyber friend , which we assume to be real but in reality , it is a fake accounts . GLEU = 0.64</p><formula xml:id="formula_3">M 2 = 0.39 Human rank= 2 NTHU</formula><p>We may of actual fact communicating with a hoax Facebook acccount of a cyber friend , which we assumed to be real but in reality , it is a fake account . GLEU = 0.60 M 2 = 0.00 Human rank= 4</p><p>Table 4: Examples of system output (changes are in bold) and the sentence-level scores assigned by different metrics.</p><p>However, GLEU awards UMC partial credit for adding the correct unigram, and further assigns all sentences a real score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Summary</head><p>As with other metrics used in natural language processing tasks, grammatical error correction metrics must be evaluated against ground truth. The inherent subjectivity in what constitutes a grammatical correction, together with the fact that the use case for grammatically-corrected output is human readers, argue for grounding metric evalu- ations in a human evaluation, which we produced following procedures established by the Workshop on Statistical Machine Translation. This human ranking shows us that the metric commonly used for GEC is not appropriate, since it does not cor- relate strongly; newly proposed alternatives fare little better.</p><p>Attending to how humans perceive the quality of the sentences, we developed GLEU by making a simple variation to an existing metric. GLEU more closely models human judgments than previ- ous metrics because it rewards correct edits while penalizing ungrammatical edits, while capturing fluency and grammatical constraints by virtue of using n-grams. While this simple modification to BLEU accounts for crucial differences in a mono- lingual setting, fares well, and could take the place of existing metrics, especially for rapid system de- velopment as in machine translation, there is still room for further work as there is a gap in how well it correlates with human judgments.</p><p>Most importantly, the results and data from this paper establish a method for objectively evaluating future metric proposals, which is crucial to yearly incremental improvements to the GEC task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Correlation among M 2 , I-measure, and BLEU scores: M 2 score shows negative correlations to other metrics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Table 3 :</head><label>3</label><figDesc>Correlation of metrics with the human ranking (excluding the reference), as calculated with Pearson's r and Spearman's ρ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 : System outputs scored by different metrics, ranked best to worst.</head><label>2</label><figDesc></figDesc><table>System 
Sentence 
Scores 
Original 
sentence 
</table></figure>

			<note place="foot" n="1"> Our code and rankings of the CoNLL-2014 Shared Task system outputs can be downloaded from github.com/ cnap/gec-ranking/.</note>

			<note place="foot" n="2"> http://www.gnu.org/s/wdiff/ 3 For example, when we put a single character &apos;X&apos; as system output for each sentence, we obtain P = 0.27, R = 0.29, M 2 = 0.28, which would be ranked 6/13 systems in the 2014 CoNLL shared task.</note>

			<note place="foot" n="4"> www.comp.nus.edu.sg/ ˜ nlp/conll14st. html</note>

			<note place="foot" n="5"> github.com/cfedermann/Appraise 6 CoNLL-14 has two references. For each sentence, we randomly chose one to present as the answer and one to be among the systems to be ranked.</note>

			<note place="foot" n="7"> Of course, it could be the case that the source sentence is actually the second best, but our human evaluation ( §5) confirms that this is not the case.</note>

			<note place="foot" n="8"> Pearson&apos;s measure assumes the scores are normally distributed, which may not be true here.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Christopher Bryant, Hwee Tou Ng, Mariano Felice, and Ted Briscoe for shar-ing their research with us pre-publication. We also thank the reviewers and Wei Xu for their valuable feedback, and Benjamin Van Durme for his sup-port.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Findings of the 2014 Workshop on Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Pecina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Saint-Amand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleš</forename><surname>Tamchyna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation<address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="12" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">How far are we from fully automatic high quality grammatical error correction?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Problems in evaluating grammatical error detection systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Israel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The COLING 2012 Organizing Committee</title>
		<meeting><address><addrLine>Mumbai, India, December</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="611" to="628" />
		</imprint>
	</monogr>
	<note>Proceedings of COLING 2012</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Better evaluation for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="568" to="572" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Helping our own: The HOO 2011 pilot shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kilgarriff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Generation Challenges Session at the 13th European Workshop on Natural Language Generation</title>
		<meeting>the Generation Challenges Session at the 13th European Workshop on Natural Language Generation<address><addrLine>Nancy, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-09" />
			<biblScope unit="page" from="242" to="249" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">HOO 2012: A report on the preposition and determiner error correction shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Anisimoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Narroway</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Workshop on Building Educational Applications Using NLP</title>
		<meeting>the Seventh Workshop on Building Educational Applications Using NLP<address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="54" to="62" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Appraise: An opensource toolkit for manual evaluation of machine translation output</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Prague Bulletin of Mathematical Linguistics</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="25" to="35" />
			<date type="published" when="2012-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards a standard evaluation method for grammatical error detection and correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariano</forename><surname>Felice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter</title>
		<meeting>the 2015 Conference of the North American Chapter<address><addrLine>Denver, CO, June</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">METEOR: An automatic metric for MT evaluation with high levels of correlation with human judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhaya</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Statistical Machine Translation</title>
		<meeting>the Second Workshop on Statistical Machine Translation<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="228" to="231" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The CoNLL2013 shared task on grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hwee Tou Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Siew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Hadiwinoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning: Shared Task<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The CoNLL-2014 shared task on grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hwee Tou Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Siew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">Hendy</forename><surname>Hadiwinoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Susanto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bryant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning: Shared Task<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">BLEU: A method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-07" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automated whole sentence grammar correction using a noisy channel model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="934" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient elicitation of annotations for human evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation<address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
