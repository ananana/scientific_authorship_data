<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:31+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Topic Extraction from Microblog Posts Using Conversation Structures</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shatin, Hong Kong</settlement>
									<region>N.T</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">MoE Key Laboratory of High Confidence Software Technologies</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shatin, Hong Kong</settlement>
									<region>N.T</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">MoE Key Laboratory of High Confidence Software Technologies</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Gao</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Qatar Computing Research Institute</orgName>
								<orgName type="institution" key="instit2">Hamad Bin Khalifa University</orgName>
								<address>
									<settlement>Doha</settlement>
									<country key="QA">Qatar</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>He</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">School of Engineering and Applied Science</orgName>
								<orgName type="institution">Aston University</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shatin, Hong Kong</settlement>
									<region>N.T</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">MoE Key Laboratory of High Confidence Software Technologies</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Topic Extraction from Microblog Posts Using Conversation Structures</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="2114" to="2123"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Conventional topic models are ineffective for topic extraction from microblog messages since the lack of structure and context among the posts renders poor message-level word co-occurrence patterns. In this work, we organize microblog posts as conversation trees based on re-posting and replying relations, which enrich context information to alleviate data sparseness. Our model generates words according to topic dependencies derived from the conversation structures. In specific , we differentiate messages as leader messages, which initiate key aspects of previously focused topics or shift the focus to different topics, and follower messages that do not introduce any new information but simply echo topics from the messages that they repost or reply. Our model captures the different extents that leader and follower messages may contain the key topical words, thus further enhances the quality of the induced topics. The results of thorough experiments demonstrate the effectiveness of our proposed model.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The increasing popularity of microblog platforms results in a huge volume of user-generated short posts. Automatically modeling topics out of such massive microblog posts can uncover the hid- den semantic structures of the underlying collec- tion and can be useful to downstream applications such as microblog summarization ( <ref type="bibr" target="#b7">Harabagiu and Hickl, 2011</ref>), user profiling ( <ref type="bibr" target="#b24">Weng et al., 2010)</ref>, event tracking ( <ref type="bibr" target="#b13">Lin et al., 2010</ref>) and so on.</p><p>Popular topic models, like Probabilistic La- tent Semantic Analysis (pLSA) <ref type="bibr" target="#b8">(Hofmann, 1999</ref>) * * Part of this work was conducted when the first author was visiting Aston University. and Latent Dirichlet Allocation (LDA) ( <ref type="bibr" target="#b1">Blei et al., 2003b)</ref>, model the semantic relationships be- tween words based on their co-occurrences in doc- uments. They have demonstrated their success in conventional documents such as news reports and scientific articles, but perform poorly when directly applied to short and colloquial microblog content due to severe sparsity in microblog mes- sages ( <ref type="bibr" target="#b22">Wang and McCallum, 2006;</ref><ref type="bibr" target="#b9">Hong and Davison, 2010)</ref>.</p><p>A common way to deal with short text sparsity is to aggregate short messages into long pseudo- documents. Most of the studies heuristically aggregate messages based on authorship <ref type="bibr" target="#b26">(Zhao et al., 2011;</ref><ref type="bibr" target="#b9">Hong and Davison, 2010)</ref>, shared words ( <ref type="bibr" target="#b24">Weng et al., 2010)</ref>, or hashtags <ref type="bibr" target="#b18">(Ramage et al., 2010;</ref><ref type="bibr" target="#b14">Mehrotra et al., 2013)</ref>. Some works directly take into account the word re- lations to alleviate document-level word sparse- ness ( <ref type="bibr" target="#b25">Yan et al., 2013;</ref><ref type="bibr" target="#b21">Sridhar, 2015)</ref>. More recently, a self-aggregation-based topic model called SATM <ref type="bibr" target="#b17">(Quan et al., 2015</ref>) was proposed to aggregate texts jointly with topic inference.</p><p>However, we argue that the existing aggrega- tion strategies are suboptimal for modeling top- ics in short texts. Microblogs allow users to share and comment on messages with friends through reposting or replying, similar to our everyday con- versations. Intuitively, the conversation structures can not only enrich context, but also provide use- ful clues for identifying relevant topics. This is nonetheless ignored in previous approaches. Moreover, the occurrence of non-topic words such as emotional, sentimental, functional and even meaningless words are very common in microblog posts, which may distract the models from recog- nizing topic-related key words and thus fail to pro- duce coherent and meaningful topics.</p><p>We propose a novel topic model by utilizing the structures of conversations in microblogs. We link microblog posts using reposting and replying rela-tions to build conversation trees. Particularly, the root of a conversation tree refers to the original post and its edges represent the reposting/replying relations.</p><p>[O] Just an hour ago, a series of coordinated terrorist attacks occurred in Paris !!!</p><p>[R2] Gunmen and suicide bombers hit a concert hall. More than 100 are killed already.</p><p>[R1] OMG! I can't believe it's real. Paris?! I've just been there last month.</p><p>[ [R6] poor guys, terrible <ref type="figure">Figure 1</ref>: An example of conversation tree.</p><p>[O]: the original post; <ref type="bibr">[Ri]</ref>: the i-th repost/reply; Ar- row lines: reposting/replying relations; Dark black posts: leaders to be detected; Underlined italic words: key words representing topics <ref type="figure">Figure 1</ref> illustrates an example of a conversa- tion tree, in which messages can initiate a new topic such as <ref type="bibr">[O]</ref> and <ref type="bibr">[R7]</ref> or raise a new aspect (subtopic) of the previously discussed topics such as <ref type="bibr">[R2]</ref> and <ref type="bibr">[R10]</ref>. These messages are named as leaders, which contain salient content in topic de- scription, e.g., the italic and underlined words in <ref type="figure">Figure 1</ref>. The remaining messages, named as fol- lowers, do not raise new issues but simply respond to their reposted or replied messages following what has been raised by the leaders and often con- tain non-topic words, e.g., OMG, OK, agree, etc.</p><p>Conversation tree structures from microblogs have been previously shown helpful to microblog summarization ( <ref type="bibr" target="#b12">Li et al., 2015</ref>), but have never been explored for topic modeling. We follows <ref type="bibr" target="#b12">Li et al. (2015)</ref> to detect leaders and followers across paths of conversation trees using Conditional Ran- dom Fields (CRF) trained on annotated data. The detected leader/follower information is then in- corporated as prior knowledge into our proposed topic model.</p><p>Our experimental results show that our model, which captures parent-child topic correlations in conversation trees and generates topics by consid- ering messages being leaders or followers sepa- rately, is able to induce high-quality topics and outperforms a number of competitive baselines. In summary, our contributions are three-fold:</p><p>• We propose a novel topic model, which ex- plicitly exploits the topic dependencies contained in conversation structures to enhance topic assign- ments.</p><p>• Our model differentiates the generative pro- cess of topical and non-topic words, according to the message where a word is drawn from being a leader or a follower. This helps the model dis- tinguish the topic-specific information from back- ground noise.</p><p>• Our model outperforms state-of-the-art topic models when evaluated on a large real-world mi- croblog dataset containing over 60K conversation trees, which is publicly available 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Topic models aim to discover the latent seman- tic information, i.e., topics, from texts and have been extensively studied. One of the most popu- lar and well-known topic models is LDA ( <ref type="bibr" target="#b1">Blei et al., 2003b</ref>). It utilizes Dirichlet priors to generate document-topic and topic-word distributions, and has been shown effective in extracting topics from conventional documents.</p><p>Nevertheless, prior research has demonstrated that standard topic models, essentially focusing on document-level word co-occurrences, are not suitable for short and informal microblog mes- sages due to severe data sparsity exhibited in short texts ( <ref type="bibr" target="#b22">Wang and McCallum, 2006;</ref><ref type="bibr" target="#b9">Hong and Davison, 2010)</ref>. Therefore, how to enrich and ex- ploit context information becomes a main concern. <ref type="bibr" target="#b24">Weng et al. (2010)</ref>, <ref type="bibr" target="#b9">Hong et al. (2010)</ref> and <ref type="bibr" target="#b26">Zhao et al. (2011)</ref> first heuristically aggregated mes- sages posted by the same user or sharing the same words before applying classic topic models to ex- tract topics. However, such a simple strategy poses some problems. For example, it is common that a user has various interests and posts messages cov- ering a wide range of topics. <ref type="bibr" target="#b18">Ramage et al. (2010)</ref> and <ref type="bibr" target="#b14">Mehrotra et al. (2013)</ref> used hashtags as labels to train supervised topic models. But these mod- els depend on large-scale hashtag-labeled data for model training, and their performance is inevitably compromised when facing unseen topics irrelevant to any hashtag in training data due to the rapid change and wide variety of topics in social media.</p><p>SATM <ref type="bibr" target="#b17">(Quan et al., 2015</ref>) combined short texts aggregation and topic induction into a unified model. But in their work, no prior knowledge was given to ensure the quality of text aggrega- tion, which therefore can affect the performance of topic inference. In this work, we organize mi- croblog messages as conversation trees based on reposting/reply relations, which is a more advan- tageous message aggregation strategy.</p><p>Another line of research tackled the word sparseness by modeling word relations instead of word occurrences in documents. For example, Gaussian Mixture Topic Model (GMTM) <ref type="bibr" target="#b21">(Sridhar, 2015)</ref> utilized word embeddings to model the distributional similarities of words and then in- ferred clusters of words represented by word dis- tributions using Gaussian Mixture Model (GMM) that capture the notion of latent topics. However, GMTM heavily relies on meaningful word embed- dings that require a large volume of high-quality external resources for training.</p><p>Biterm Topic Model (BTM) ( <ref type="bibr" target="#b25">Yan et al., 2013</ref>) directly explores unordered word-pair co- occurrence patterns in each individual message. Our model learns topics from aggregated mes- sages based on conversation trees, which naturally provide richer context since word co-occurrence patterns can be captured from multiple relevant messages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LeadLDA Topic Model</head><p>In this section, we describe how to extract top- ics from a microblog collection utilizing conversa- tion tree structures, where the trees are organized based on reposting and replying relations among the messages 2 .</p><p>To identify key topic-related content from collo- quial texts, we differentiate the messages as lead- ers and followers. Following <ref type="bibr" target="#b12">Li et al. (2015)</ref>, we extract all root-to-leaf paths on conversation trees and utilize the state-of-the-art sequence learning model CRF ( <ref type="bibr" target="#b10">Lafferty et al., 2001</ref>) to detect the leaders <ref type="bibr">3</ref> . As a result, the posterior probability of each node being a leader or follower is obtained by averaging the different marginal probabilities of the same node over all the tree paths that contain the node. Then, the obtained probability distribu- tion is considered as the observed prior variable input into our model. <ref type="bibr">2</ref> Reposting/replying relations are straightforward to ob- tain by using microblog APIs from Twitter and Sina Weibo. <ref type="bibr">3</ref> The CRF model for leader detection was trained on a public corpus with all the messages annotated on the tree paths. Details are described in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Topics and Conversation Trees</head><p>Previous works <ref type="bibr" target="#b26">(Zhao et al., 2011;</ref><ref type="bibr" target="#b25">Yan et al., 2013;</ref><ref type="bibr" target="#b17">Quan et al., 2015)</ref> have proven that assum- ing each short post contains a single topic is useful to alleviate the data sparsity problem. Thus, given a corpus of microblog posts organized as conver- sation trees and the estimated leader probabilities of tree nodes, we assume that each message only contains a single topic and a tree covers a mixture of multiple topics. Since leader messages subsume the content of their followers, the topic of a leader can be generated from the topic distribution of the entire tree. Consequently, the topic mixture of a conversation tree is determined by the topic as- signments to the leader messages on it. The topics of followers, however, exhibit strong and explicit dependencies on the topics of their ancestors. So, their topics need to be generated in consideration of local constraints. Here, we mainly address how to model the topic dependencies of followers.</p><p>Enlighten by the general Structural Topic Model (strTM) ( <ref type="bibr" target="#b23">Wang et al., 2011</ref>), which incorporates document structures into topic model by explic- itly modeling topic dependencies between adja- cent sentences, we exploit the topical transitions between parents and children in the trees for guid- ing topic assignments.</p><p>Intuitively, the emergence of a leader results in potential topic shift. It tends to weaken the topic similarities between the emerging leaders and their predecessors. For example, <ref type="bibr">[R7]</ref> in <ref type="figure">Figure 1</ref> trans- fers the topic to a new focus, thus weakens the tie with its parent. We can simplify our case by as- suming that followers are topically responsive just up to (hence not further than) their nearest ances- tor leaders. Thus, we can dismantle each conver- sation tree into forest by removing the links be- tween leaders and their parents hence producing a set of subgraphs like <ref type="figure">Figure 1</ref>. Then, we model the internal topic dependencies within each subgraph by inferring the parent-child topic transition probabilities sat- isfying the first-order Markov properties in a simi- lar way as estimating the transition distribution of adjacent sentences in strTM ( <ref type="bibr" target="#b23">Wang et al., 2011</ref>). At topic assignment stage, the topic of a follower will be assigned by referring to its parent's topic and the transition distribution that captures topic similarities of followers to their parents (see Sec- tion 3.2).</p><formula xml:id="formula_0">[R2]-[R6] and [R7]-[R9] in</formula><p>In addition, every word in the corpus is either a topical or non-topic (i.e., background) word, which highly depends on whether it occurs in a leader or a follower message. <ref type="figure">Figure 2</ref> illustrates the graphical model of our generative process, which is named as LeadLDA.</p><formula xml:id="formula_1">T Mt í µí»½ K í µí¼ $ í µí¼ % í µí¼ ' í µí± § ',* Nt,m í µí± § ',+(*) í µí±¦ ',* í µí± ',* í µí»¾ K í µí¼ $ í µí±¤ ',*,3 í µí±¥ ',*,3 í µí»¿ 2 í µí¼ 7 α</formula><p>Figure 2: Graphical Model of LeadLDA</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Topic Modeling</head><p>Formally, we assume that the microblog posts are organized as T conversation trees. Each tree t contains M t message nodes and each message m contains N t,m words in the vocabulary. The vo- cabulary size is V and there are K topics em- bedded in the corpus represented by word distri- bution φ k ∼ Dir(β) (k = 1, 2, ..., K). Also, a background word distribution φ B ∼ Dir(β) is in- cluded to capture the general information, which is not topic specific. φ k and φ B are multinomial dis- tributions over the vocabulary. A tree t is modeled as a mixture of topics θ t ∼ Dir(α) and any mes- sage m on t is assumed to contain a single topic z t,m ∈ {1, 2, ..., K}.</p><p>(1) Topic assignments: The topic assignments of LeadLDA is inspired by  that combines syntactic and semantic dependen- cies between words. LeadLDA integrates the out- comes of leader detection with a binomial switcher y t,m ∈ {0, 1} indicating whether m is a leader (y t,m = 1) or a follower (y t,m = 0), given each message m on the tree t. y t,m is parameterized by its leader probability l t,m , which is the posterior probability output from the leader detection model and serves as an observed prior variable.</p><p>According to the notion of leaders, they initiate key aspects of previously discussed topics or sig- nal a new topic shifting the focus of its descendant followers. So, the topics of leaders on tree t are directly sampled from the topic mixture θ t .</p><p>To model the internal topic correlations within the subgraph of conversation tree consisting of a leader and all its followers, we capture parent- child topic transitions π k ∼ Dir(γ), which is a distribution over K topics, and use π k,j to denote the probability of a follower assigned topic j when the topic of its parent is k. Specifically, if message m is sampled as a follower and the topic assign- ment to its parent message is z t,p(m) , where p(m) indexes the parent of m, then z t,m (i.e., the topic of m) is generated from topic transition distribution π z t,p(m) . In particular, since the root of a conver- sation tree has no parent and can only be a leader, we make the leader probability l t,root = 1 to force its topic only to be generated from the topic distri- bution of tree t.</p><p>(2) Topical and non-topic words: We sep- arately model the distributions of leader and follower messages emitting topical or non-topic words with τ 0 and τ 1 , respectively, both of which are drawn from a symmetric Beta prior parame- tererized by δ. Specifically, for each word n in message m on tree t, we add a binomial back- ground switcher x t,m,n controlled by whether m is a leader or a follower, i.e., x t,m,n ∼ Bi(τ yt,m ), which indicates n is a topical word if x t,m,n = 0 or a background word if x t,m,n = 1, and x t,m,n con- trols n to be generated from the topic-word dis- tribution φ zt,m , where z t,m is the topic of m, or from background word distribution φ B modeling non-topic information.</p><p>(3) Generation process: To sum up, condi- tioned on the hyper-parameters Θ = (α, β, γ, δ), the generation process of a conversation tree t can be described as follows: # of words occurring in messages whose leader switchers are s, i.e., r∈{0,1} C LB s,(r) . N B (r) # of words occurring in message (t, m) and with background switchers assigned as r.</p><formula xml:id="formula_2">• Draw θ t ∼ Dir(α) • For message m = 1 to M t on tree t -Draw y t,m ∼ Bi(l t,m ) -If y t,m == 1 * Draw z t,m ∼ M ult(θ t ) -If y t,m == 0 * Draw z t,m ∼ M ult(π z t,p(m) ) -For word n = 1 to N t,m in m * Draw x t,m,n ∼ Bi(τ yt,m ) * If x t,m,n == 0 · Draw w t,m,n ∼ M ult(φ zt,m ) * If x t,m,n == 1 · Draw w t,m,n ∼ M ult(φ B ) C LB s,(r)</formula><note type="other"># of words with background switchers assigned as r and oc- curring in messages with leader switchers s.</note><formula xml:id="formula_3">N B (·) # of words in message (t, m), i.e., N B (·) = r∈{0,1} N B (r) . C T W k,(v)</formula><p># of words indexing v in vocabulary, sampled as topic (non- background) words, and occurring in messages assigned topic k.</p><formula xml:id="formula_4">C T W k,(·)</formula><p># of words assigned as topic (non-background) word and occurring in messages assigned topics k, i.e.,</p><formula xml:id="formula_5">C T W k,(·) = V v=1 C T W k,(v) . N W (v)</formula><p># of words indexing v in vocabulary that occur in message (t, m) and are assigned as topic (non-background) word.</p><formula xml:id="formula_6">N W (·)</formula><p># of words assigned as topic (non-background) words and oc- curring in message (t, m), i.e.,</p><formula xml:id="formula_7">N W (·) = V v=1 N W (v) . C T R i,(j)</formula><p># of messages sampled as followers and assigned topic j, whose parents are assigned topic i.</p><formula xml:id="formula_8">C T R i,(·)</formula><p># of messages sampled as followers whose parents are as-</p><formula xml:id="formula_9">signed topic i, i.e., C T R i,(·) = K j=1 C T R i,(j) . I(·)</formula><p>An indicator function, whose value is 1 when its argument inside () is true, and 0 otherwise.</p><formula xml:id="formula_10">N CT (j)</formula><p># of messages that are children of message (t, m), sampled as followers and assigned topic j.</p><formula xml:id="formula_11">N CT (·)</formula><p># of message (t, m)'s children sampled as followers, i.e.,</p><formula xml:id="formula_12">N CT (·) = K j=1 N CT (j) C T T t,(k)</formula><p># of messages on conversation tree t sampled as leaders and assigned topic k.</p><formula xml:id="formula_13">C T T t,(·)</formula><p># of messages on conversation tree t sampled as leaders, i.e.,</p><formula xml:id="formula_14">C T T t,(·) = K k=1 C T T t,(k) C BW (v)</formula><p># of words indexing v in vocabulary and assigned as back- ground (non-topic) words</p><formula xml:id="formula_15">C BW (·)</formula><p># of words assigned as background (non-topic) words, i.e.,  <ref type="formula" target="#formula_17">(1)</ref> and <ref type="formula" target="#formula_19">(2)</ref>. (t, m): message m on con- versation tree t.</p><formula xml:id="formula_16">C BW (·) = V v=1 C BW (v)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Inference for Parameters</head><p>We use collapsed Gibbs Sampling <ref type="bibr" target="#b6">(Griffiths, 2002</ref>) to carry out posterior inference for param- eter learning. The hidden multinomial variables, i.e., message-level variables (y and z) and word- level variables (x) are sampled in turn, conditioned on a complete assignment of all other hidden vari- ables. Due to the space limitation, we leave out the details of derivation but give the core formulas in the sampling steps.</p><p>We first define the notations of all variables needed by the formulation of Gibbs sampling, which are described in <ref type="table" target="#tab_1">Table 1</ref>. In particular, the various C variables refer to counts excluding the message m on conversation tree t.</p><p>For each message m on a tree t, we sample the leader switcher y t,m and topic assignment z t,m according to the following conditional probability distribution:</p><formula xml:id="formula_17">p(yt,m = s, zt,m = k|y ¬(t,m) , z ¬(t,m) , w, x, l, Θ) ∝ Γ(C LB s,(·) + 2δ) Γ(C LB s,(·) + N B (·) + 2δ) r∈{0,1} Γ(C LB s,(r) + N B (r) + δ) Γ(C LB s,(r) + δ) · Γ(C T W k,(·) + V β) Γ(C T W k,(·) + N W (·) + V β) V v=1 Γ(C T W k,(v) + N W (v) + β) Γ(C T W k,(v) + β) ·g(s, k, t, m)<label>(1)</label></formula><p>where g(s, k, t, m) takes different forms depend- ing on the value of s:</p><formula xml:id="formula_18">g(0, k, t, m) = Γ(C T R z t,p(m) ,(·) + Kγ) Γ(C T R z t,p(m) ,(·) + I(z t,p(m) = k) + Kγ) · Γ(C T R k,(·) + Kγ) Γ(C T R k,(·) + I(z t,p(m) = k) + N CT (·) + Kγ) · K j=1 Γ(C T R k,(j) + N CT (j) + I(z t,p(m) = j = k) + γ) Γ(C T R k,(j) + γ) · Γ(C T R z t,p(m) ,(k) + I(z t,p(m) = k) + γ) Γ(C T R z t,p(m) ,(k) + γ) · (1 − lt,m) and g(1, k, t, m) = C T T t,(k) + α C T T t,(·) + Kα · l t,m</formula><p>For each word n in m on t, the sampling for- mula of its background switcher is given as the following:</p><formula xml:id="formula_19">p(xt,m,n = r|x ¬(t,m,n) , y, z, w, l, Θ) ∝ C LB y t,m ,(r) + δ C LB y t,m ,(·) + 2δ · h(r, t, m, n)<label>(2)</label></formula><p>where h(r, t, m, n) =</p><formula xml:id="formula_20">     C T W z t,m ,(w t,m,n ) +β C T W z t,m ,(·) +V β if r = 0 C BW (w t,m,n ) +β C BW (·)</formula><p>+V β if r = 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Data Collection and Experiment Setup</head><p>To evaluate our LeadLDA model, we conducted experiments on real-world microblog dataset col- lected from Sina Weibo that has the same 140- character limitation and shares the similar mar- ket penetration as Twitter <ref type="bibr" target="#b19">(Rapoza, 2011</ref>). For the hyper-parameters of LeadLDA, we fixed α = 50/K, β = 0.1, following the common practice in previous works ( <ref type="bibr" target="#b17">Quan et al., 2015)</ref>. Since there is no analogue of γ and δ in prior works, where γ controls topic dependencies of follower messages to their an- cestors and δ controls the different tendencies of <ref type="table" target="#tab_1">Month # of trees # of messages Vocab size  May  10,812  38,926  6,011  June  29,547  98,001  9,539  July  26,103  102,670  10,121   Table 2</ref>: Statistics of our three evaluation datasets leaders and followers covering topical and non- topic words. We tuned γ and δ by grid search on a large development set containing around 120K posts and obtained γ = 50/K, δ = 0.5. Because the content of posts are often incom- plete and informal, it is difficult to manually an- notate topics in a large scale. Therefore, we fol- low <ref type="bibr" target="#b25">Yan et al. (2013)</ref> to utilize hashtags led by '#', which are manual topic labels provided by users, as ground-truth categories of microblog messages. We collected the real-time trending hashtags on Sina Weibo and utilized the hashtag-search API <ref type="bibr">4</ref> to crawl the posts matching the given hashtag queries. In the end, we built a corpus containing 596,318 posts during May 1 -July 31, 2014.</p><p>To examine the performance of models on var- ious topic distributions, we split the corpus into 3 datasets, each containing messages of one month. Similar to <ref type="bibr" target="#b25">Yan et al. (2013)</ref>, for each dataset, we manually selected 50 frequent hashtags as topics, e.g. #mh17, #worldcup, etc. The experiments were conducted on the subsets of posts with the selected hashtags. <ref type="table">Table 2</ref> shows the statistics of the three subsets used in our experiments.</p><p>We preprocessed the datasets before topic ex- traction in the following steps: 1) Use FudanNLP toolkit ( <ref type="bibr" target="#b16">Qiu et al., 2013</ref>) for word segmentation, stop words removal and POS tagging for Chinese Weibo messages; 2) Generate a vocabulary for each dataset and remove words occurring less than 5 times; 3) Remove all hashtags in texts before in- put them to models, since the models are expected to extract topics without knowing the hashtags, which are ground-truth topics; 4) For LeadLDA, we use the CRF-based leader detection model ( <ref type="bibr" target="#b12">Li et al., 2015)</ref> to classify messages as leaders and followers. The leader detection model was im- plemented by using CRF++ <ref type="bibr">5</ref> , which was trained on the public dataset composed of 1,300 conversa- tion paths and achieved state-of-the-art 73.7% F1- score of classification accuracy ( <ref type="bibr" target="#b12">Li et al., 2015</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>We evaluated topic models with two sets of K, i.e., the number of topics. One is K = 50, to match the count of hashtags following <ref type="bibr" target="#b25">Yan et al. (2013)</ref>, and the other is K = 100, much larger than the "real" number of topics. We compared LeadLDA with the following 5 state-of-the-art basedlines.</p><p>TreeLDA: Analogous to <ref type="bibr" target="#b26">Zhao et al. (2011)</ref>, where they aggregated messages posted by the same author, TreeLDA aggregates messages from one conversation tree as a pseudo-document. Ad- ditionally, it includes a background word distribu- tion to capture non-topic words controlled by a general Beta prior without differentiating leaders and followers. TreeLDA can be considered as a degeneration of LeadLDA, where topics assigned to all messages are generated from the topic distri- butions of the conversation trees they are on.</p><p>StructLDA: It is another variant of LeadLDA, where topics assigned to all messages are gener- ated based on topic transitions from their parents. The strTM ( <ref type="bibr" target="#b23">Wang et al., 2011</ref>) utilized a similar model to capture the topic dependencies of adja- cent sentences in a document. Following strTM, we add a dummy topic T start emitting no word to the "pseudo parents" of root messages. Also, we add the same background word distribution to cap- ture non-topic words as TreeLDA does.</p><p>BTM: Biterm Topic Model (BTM) <ref type="bibr">6 (Yan et al., 2013</ref>) directly models topics of all word pairs (biterms) in each post, which outperformed LDA, Mixture of Unigrams model, and the model pro- posed by <ref type="bibr" target="#b26">Zhao et al. (2011)</ref> that aggregated posts by authorship to enrich context. SATM: A general unified model proposed by <ref type="bibr" target="#b17">Quan et al. (2015)</ref> that aggregates documents and infers topics simultaneously. We implemented SATM and examined its effectiveness specifically on microblog data.</p><p>GMTM: To tackle word sparseness, <ref type="bibr" target="#b21">Sridhar et al. (2015)</ref> utilized Gaussian Mixture Model (GMM) to cluster word embeddings generated by a log-linear word2vec model <ref type="bibr">7</ref> .</p><p>The hyper-parameters of BTM, SATM and GMTM were set according to the best hyper- parameters reported in their original papers. For TreeLDA and StructLDA, the parameter settings were kept the same as LeadLDA since they are its variants. And the background switchers were pa- rameterized by symmetric Beta prior on 0.5, fol- lowing <ref type="bibr" target="#b3">Chemudugunta et al. (2006)</ref>. We ran Gibbs samplings (in LeadLDA, TreeLDA, StructLDA, BTM and SATM) and EM algorithm (in GMTM) with 1,000 iterations to ensure convergence.</p><p>Topic model evaluation is inherently difficult. In previous works, perplexity is a popular metric to evaluate the predictive abilities of topic mod- els given held-out dataset with unseen words ( <ref type="bibr" target="#b1">Blei et al., 2003b</ref>). However, <ref type="bibr" target="#b2">Chang et al. (2009)</ref> have demonstrated that models with high perplex- ity do not necessarily generate semantically co- herent topics in human perception. Therefore, we conducted objective and subjective analysis on the coherence of produced topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Objective Analysis</head><p>The quality of topics is commonly measured by coherence scores <ref type="bibr" target="#b15">(Mimno et al., 2011</ref>), assuming that words representing a coherent topic are likely to co-occur within the same document. However, due to the severe sparsity of short text posts, we modify the calculation of commonly-used topic coherence measure based on word co-occurrences in messages tagged with the same hashtag, named as hashtag-document, assuming that those mes- sages discuss related topics 8 .</p><p>Specifically, we calculate the coherence score of a topic given the top N words ranked by likelihood as below:</p><formula xml:id="formula_21">C = 1 K · K k=1 N i=2 i−1 j=1 log D(w k i , w k j ) + 1 D(w k j ) ,<label>(3)</label></formula><p>where w k i represents the i-th word in topic k ranked by p(w|k), D(w k i , w k j ) refers to the count of hashtag-documents where word w k i and w k j co- occur, and D(w k i ) denotes the number of hashtag- documents that contain word w k i . <ref type="table" target="#tab_3">Table 3</ref> shows the absolute values of C scores for topics produced on three evaluation datasets (May, June and July), and the top 10, 15, 20 words of topics were selected for evaluation. Lower scores indicate better coherence in the induced topic.</p><p>We have the following observations:</p><p>• GMTM gave the worst coherence scores, which may be ascribed to its heavy reliance on rel- evant large-scale high-quality external data, with- <ref type="bibr">8</ref> We sampled posts and their corresponding hashtags in our evaluation set and found only 1% mismatch.  out which the trained word embedding model failed to capture meaningful semantic features for words, and hence could not yield coherent topics.</p><p>• TreeLDA and StructLDA produced competi- tive results compared to the state-of-the-art base- line models, which indicates the effectiveness of using conversation structures to enrich context and thus generate topics of reasonably good quality.</p><p>• The coherence of topics generated by LeadLDA outperformed all the baselines on the three datasets, most of time by large margins and was only outperformed by BTM on the May dataset when K = 50 and N = 10. The gen- erally higher performance of LeadLDA is due to three reasons: 1) It effectively identifies top- ics using the conversation tree structures, which provide richer context information; 2) It jointly models the topics of leaders and the topic depen- dencies of other messages on a tree. TreeLDA and StructLDA, each only considering one of the factors, performed worse than LeadLDA; 3) LeadLDA separately models the probabilities of leaders and followers containing topical or non- topic words while the baselines only model the general background information regardless of the different types of messages. This implies that leaders and followers do have different capaci- ties in covering key topical words or background noise, which is useful to identify key words for topic representation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TreeLDA</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>StructLDA</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BTM SATM LeadLDA</head><formula xml:id="formula_22">香港 微博 马航 家属 证 实 入境处 客机 消息 曹 格 投给 二胎 选项 教父 滋养 飞机 外国 心情 坠 毁 男子 同胞 乌克兰 航空 亲爱 国民 绕开 飞行 航班 领空 所 有 避开 宣布 空域 东部 俄罗斯 终于 忘记 公司 绝望 看看 珍贵 香港 入境处 家属 证实 男子 护照 外国 消息 坠 毁 马航 报道 联系 电台 客机 飞机 同胞 确认 事 件 霍家 直接 马航 祈祷 安息 生命 逝 者 世界 艾滋病 恐怖 广 州 飞机 无辜 默哀 远离 事件 击落 公交车 中国 人 国际 愿逝者 真的 乌克兰 马航 客机 击落 飞机 坠毁 导弹 俄罗斯 消息 乘客 中国 马来西 亚 香港 遇难 事件 武装 航班 恐怖 目前 证实</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Subjective Analysis</head><p>To evaluate the coherence of induced topics from human perspective, we invited two annotators to subjectively rate the quality of every topic (by dis- playing the top 20 words) generated by different models on a 1-5 Likert scale. A higher rating in- dicates better quality of topics. The Fless's Kappa of annotators' ratings measured for various models on different datasets given K = 50 and 100 range from 0.62 to 0.70, indicating substantial agree- ments ( <ref type="bibr" target="#b11">Landis and Koch, 1977)</ref>. <ref type="table">Table 4</ref> shows the overall subjective ratings. We noticed that humans preferred topics pro- duced given K = 100 to K = 50, but coher- ence scores gave generally better grades to mod- els for K = 50, which matched the number of topics in ground truth. This is because models more or less mixed more common words when K is larger. Coherence score calculation (Equa- tion (3)) penalizes common words that occur in many documents, whereas humans could some- how "guess" the meaning of topics based on the rest of words thus gave relatively good ratings. Nevertheless, annotators gave remarkably higher ratings to LeadLDA than baselines on all datasets regardless of K being 50 or 100, which con- firmed that LeadLDA effectively yielded good- quality topics.</p><p>For a detailed analysis, <ref type="figure" target="#fig_1">Figure 3</ref> lists the top 20 words about "MH17 crash" induced by different models 9 when K = 50. We have the following <ref type="bibr">9</ref> As shown in <ref type="table" target="#tab_3">Table 3</ref>   <ref type="table">Table 4</ref>: Subjective ratings of topics. The mean- ings of K50, K100, TREE, STR and LEAD are the same as in <ref type="table" target="#tab_3">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>observations:</head><p>• BTM, based on word-pair co-occurrences, mistakenly grouped "Fok's family" (a tycoon fam- ily in Hong Kong), which co-occurred frequently with "Hong Kong" in other topics, into the topic of "MH17 crash". "Hong Kong" is relevant here as a Hong Kong passenger died in the MH17 crash.</p><p>• The topical words generated by SATM were mixed with words relevant to the bus explosion in Guangzhou, since it aggregated messages accord- ing to topic affinities based on the topics learned in the previous step. Thus the posts about bus explo- sion and MH17 crash, both pertaining to disasters, were aggregated together mistakenly, which gen- erated spurious topic results.</p><p>• Both TreeLDA and StructLDA generated topics containing non-topic words like "mi- croblog" and "dear". This means that without distinguishing leaders and followers, it is diffi- cult to filter out non-topic words. The topic qual- ity of StructLDA nevertheless seems better than GMTM is not shown due to space limitation.</p><p>TreeLDA, which implies the usefulness of exploit- ing topic dependencies of posts in conversation structures.</p><p>• LeadLDA not only produced more semanti- cally coherent words describing the topic, but also revealed some important details, e.g., MH17 was shot down by a missile.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Works</head><p>This paper has proposed a novel topic model by considering the conversation tree structures of mi- croblog posts. By rigorously comparing our pro- posed model with a number of competitive base- lines on real-world microblog datasets, we have demonstrated the effectiveness of using conversa- tion structures to help model topics embedded in short and colloquial microblog messages.</p><p>This work has proven that detecting leaders and followers, which are coarse-grained discourse de- rived from conversation structures, is useful to model microblogging topics. In the next step, we plan to exploit fine-grained discourse structures, e.g., dialogue acts ( <ref type="bibr" target="#b20">Ritter et al., 2010)</ref>, and propose a unified model that jointly inferring discourse roles and topics of posts in context of conversa- tion tree structures. Another extension is to ex- tract topic hierarchies by integrating the conversa- tion structures into hierarchical topic models like HLDA ( <ref type="bibr" target="#b0">Blei et al., 2003a</ref>) to extract fine-grained topics from microblog posts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The extracted topics describing MH17 crash. Each column represents the similar topic generated by the corresponding model with the top 20 words. The 2nd row: original Chinese words; The 3rd row: English translations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc>The notations of symbols in the sampling formulas</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Absolute values of coherence scores. 
Lower is better. K50: 50 topics; K100: 100 topics; 
N: # of top words ranked by topic-word probabil-
ities; TREE: TreeLDA; STR: StructLDA; LEAD: 
LeadLDA. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>and 4, the topic coherence scores of GMTM were the worst. Hence, the topic generated by</figDesc><table>Model 
May 
June 
July 
K50 K100 K50 K100 K50 K100 
TREE 
3.12 
3.41 
3.42 
3.44 
3.03 
3.48 
STR 
3.05 
3.45 
3.38 
3.48 
3.08 
3.53 
BTM 
3.04 
3.26 
3.40 
3.37 
3.15 
3.57 
SATM 
3.08 
3.43 
3.30 
3.55 
3.09 
3.54 
GMTM 2.02 
2.37 
1.99 
2.27 
1.97 
1.90 
LEAD 
3.40 
3.57 
3.52 
3.63 
3.55 
3.72 

</table></figure>

			<note place="foot" n="1"> http://www1.se.cuhk.edu.hk/ ˜ lijing/ data/microblog-topic-extraction-data.zip</note>

			<note place="foot" n="4"> http://open.weibo.com/wiki/2/search/ topics 5 https://taku910.github.io/crfpp/</note>

			<note place="foot" n="6"> https://github.com/xiaohuiyan/BTM 7 https://code.google.com/archive/p/ word2vec/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work is supported by General Research Fund of Hong Kong (417112), the Innova-tion and Technology Fund of Hong Kong SAR (ITP/004/16LP), Shenzhen Peacock Plan Re-search Grant (KQCX20140521144507925) and Innovate UK (101779). We would like to thank Shichao Dong for his efforts on data process-ing and anonymous reviewers for the useful com-ments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hierarchical topic models and the nested chinese restaurant process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Annual Conference on Neural Information Processing Systems, NIPS</title>
		<meeting>the 17th Annual Conference on Neural Information Processing Systems, NIPS</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reading tea leaves: How humans interpret topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><forename type="middle">L</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Gerrish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Annual Conference on Neural Information Processing Systems, NIPS</title>
		<meeting>the 23rd Annual Conference on Neural Information Processing Systems, NIPS</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="288" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Modeling general and specific aspects of documents with a probabilistic topic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Chemudugunta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padhraic</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Annual Conference on Neural Information Processing Systems, NIPS</title>
		<meeting>the 20th Annual Conference on Neural Information Processing Systems, NIPS</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="241" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Finding scientific topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5228" to="5235" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>suppl</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Integrating topics and syntax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Annual Conference on Neural Information Processing Systems, NIPS</title>
		<meeting>the 18th Annual Conference on Neural Information Processing Systems, NIPS</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="537" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Gibbs sampling in the generative model of latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Griffiths</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Relevance modeling for microblog summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hickl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Web and Social Media</title>
		<meeting>the 5th International Conference on Web and Social Media</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>ICWSM</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Probabilistic latent semantic indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Annual International, ACM SIGIR</title>
		<meeting>the 22nd Annual International, ACM SIGIR</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="50" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Empirical study of topic modeling in twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangjie</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brian D Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the first workshop on social media analytics</title>
		<meeting>the first workshop on social media analytics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="80" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Machine Learning, ICML</title>
		<meeting>the 18th International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The measurement of observer agreement for categorical data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Landis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary G</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">biometrics</title>
		<imprint>
			<biblScope unit="page" from="159" to="174" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Using content-level structures for summarizing microblog repost trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2168" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">PET: a statistical model for popular events tracking in social communities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Cindy Xide Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Knowledge Discovery and Data Mining, ACM SIGKDD</title>
		<meeting>the 16th International Conference on Knowledge Discovery and Data Mining, ACM SIGKDD</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="929" to="938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improving LDA topic models for microblogs via tweet pooling and automatic labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Mehrotra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Sanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lexing</forename><surname>Buntine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International conference on research and development in Information Retrieval, ACM SIGIR</title>
		<meeting>the 36th International conference on research and development in Information Retrieval, ACM SIGIR</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="889" to="892" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Optimizing semantic coherence in topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edmund</forename><forename type="middle">M</forename><surname>Talley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Leenders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="262" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fudannlp: A toolkit for chinese natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">51st Annual Meeting of the Association for Computational Linguistics, ACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="49" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Short and sparse text topic modeling via self-aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Kit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinno Jialin</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Joint Conference on Artificial Intelligence, IJCAI</title>
		<meeting>the 24th International Joint Conference on Artificial Intelligence, IJCAI</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2270" to="2276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Characterizing microblogs with topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">J</forename><surname>Liebling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Conference on Web and Social Media, ICWSM</title>
		<meeting>the 4th International Conference on Web and Social Media, ICWSM</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">China&apos;s weibos vs us&apos;s twitter: And the winner is? Forbes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Rapoza</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011-05-17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised modeling of twitter conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference of the North American Chapter of the Association of Computational Linguistics, NAACL</title>
		<meeting>the 2010 Conference of the North American Chapter of the Association of Computational Linguistics, NAACL</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="172" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised entity linking with abstract meaning representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Kumar Rangarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sridhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1130" to="1139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Topics over time: a non-markov continuous-time model of topical trends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuerui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Knowledge Discovery and Data Mining, ACM SIGKDD</title>
		<meeting>the 12th International Conference on Knowledge Discovery and Data Mining, ACM SIGKDD</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="424" to="433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Structural topic model for latent topical structure analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, ACL</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics, ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1526" to="1535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Twitterrank: finding topic-sensitive influential twitterers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ee-Peng</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Web Search and Web Data Mining, WSDM</title>
		<meeting>the 3rd International Conference on Web Search and Web Data Mining, WSDM</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="261" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A biterm topic model for short texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International World Wide Web Conference</title>
		<meeting>the 22nd International World Wide Web Conference</meeting>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1445" to="1456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Comparing twitter and traditional media using topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Wayne Xin Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ee-Peng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongfei</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval-33rd</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">European Conference on IR Research, ECIR</title>
		<imprint>
			<biblScope unit="page" from="338" to="349" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
