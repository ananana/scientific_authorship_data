<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:57+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Semantic Representations of Users and Products for Document Level Sentiment Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Semantic Representations of Users and Products for Document Level Sentiment Classification</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1014" to="1023"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Neural network methods have achieved promising results for sentiment classification of text. However, these models only use semantics of texts, while ignoring users who express the sentiment and products which are evaluated, both of which have great influences on interpreting the sentiment of text. In this paper, we address this issue by incorporating user-and product-level information into a neural network approach for document level sentiment classification. Users and products are modeled using vector space models , the representations of which capture important global clues such as individual preferences of users or overall qualities of products. Such global evidence in turn facilitates embedding learning procedure at document level, yielding better text representations. By combining evidence at user-, product-and document-level in a unified neural framework, the proposed model achieves state-of-the-art performances on IMDB and Yelp dataset-s 1 .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Document-level sentiment classification is a fun- damental problem in the field of sentiment analy- sis and opinion mining <ref type="bibr" target="#b30">(Pang and Lee, 2008;</ref><ref type="bibr" target="#b23">Liu, 2012)</ref>. The task is to infer the sentiment polari- ty or intensity (e.g. 1-5 or 1-10 stars on review sites) of a document. Dominating studies follow <ref type="bibr" target="#b31">Pang et al. (2002;</ref><ref type="bibr" target="#b29">2005)</ref> and regard this problem as a multi-class classification task. They usually use machine learning algorithms, and build sen- timent classifier from documents with accompa- nying sentiment labels. Since the performance of a machine learner is heavily dependent on the choice of data representations <ref type="bibr" target="#b4">(Domingos, 2012)</ref>, many works focus on designing effective features ( <ref type="bibr" target="#b31">Pang et al., 2002;</ref><ref type="bibr" target="#b35">Qu et al., 2010;</ref><ref type="bibr" target="#b17">Kiritchenko et al., 2014</ref>) or learning discriminative features from data with neural networks <ref type="bibr" target="#b37">(Socher et al., 2013;</ref><ref type="bibr" target="#b15">Kalchbrenner et al., 2014;</ref><ref type="bibr" target="#b19">Le and Mikolov, 2014</ref>).</p><p>Despite the apparent success of neural network methods, they typically only use text information while ignoring the important influences of users and products. Let us take reviews with respect to 1-5 rating scales as an example. A critical user might write a review "it works great" and mark 4 stars, while a lenient user might give 5 stars even if he posts an (almost) identical review. In this case, user preference affects the sentiment rating of a re- view. Product quality also has an impact on review sentiment rating. Reviews towards high-quality products (e.g. Macbook) tend to receive higher ratings than those towards low-quality products. Therefore, it is feasible to leverage individual pref- erences of users and overall qualities of products to build a smarter sentiment classifier and achieve better performance <ref type="bibr">2</ref> .</p><p>In this paper, we propose a new model dubbed User Product Neural Network (UPNN) to capture user-and product-level information for sentiment classification of documents (e.g. reviews). UPNN takes as input a variable-sized document as well as the user who writes the review and the product which is evaluated. It outputs sentiment polarity label of a document. Users and products are en- coded in continuous vector spaces, the representa- tions of which capture important global clues such as user preferences and product qualities. These representations are further integrated with contin- uous text representation in a unified neural frame- work for sentiment classification.</p><p>We apply UPNN to three datasets derived from IMDB and Yelp Dataset Challenge. We compare to several neural network models including recur- sive neural networks <ref type="bibr" target="#b37">(Socher et al., 2013)</ref>, para- graph vector ( <ref type="bibr" target="#b19">Le and Mikolov, 2014)</ref>, sentiment- specific word embedding ( <ref type="bibr" target="#b41">Tang et al., 2014b)</ref>, and a state-of-the-art recommendation algorithm JMARS ( <ref type="bibr" target="#b3">Diao et al., 2014</ref>). Experimental results show that: (1) UPNN outperforms baseline meth- ods for sentiment classification of documents; (2) incorporating representations of users and prod- ucts significantly improves classification accuracy. The main contributions of this work are as follows:</p><p>• We present a new neural network method (UPNN) by leveraging users and products for document-level sentiment classification.</p><p>• We validate the influences of users and prod- ucts in terms of sentiment and text on massive IMDB and Yelp reviews.</p><p>• We report empirical results on three datasets, and show that UPNN outperforms state-of-the-art methods for sentiment classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Consistency Assumption Verification</head><p>We detail the effects of users and products in terms of sentiment (e.g. 1-5 rating stars) and text, and verify them on review datasets.</p><p>We argue that the influences of users and prod- ucts include the following four aspects.</p><p>• user-sentiment consistency. A user has spe- cific preference on providing sentiment ratings. Some users favor giving higher ratings like 5 stars and some users tend to give lower ratings. In oth- er words, sentiment ratings from the same user are more consistent than those from different users.</p><p>• product-sentiment consistency. Similar with user-sentiment consistency, a product also has its "preference" to receive different average ratings on account of its overall quality. Sentiment ratings towards the same product are more consis- tent than those towards different products.</p><p>• user-text consistency. A user likes to use per- sonalized sentiment words when expressing opin- ion polarity or intensity. For example, a strict user might use "good" to express an excellent attitude, but a lenient user may use "good" to evaluate an ordinary product.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Consistency Assumption Testing</head><p>Input: data X, number of users/products m, number of iterations n Output:</p><formula xml:id="formula_0">meaSame k , meaDif f k , 1 ≤ k ≤ n for k = 1 to n do meaSame k = 0, meaSame k = 0 for i = 1 to m do Sample x i , x + i , x − i from X meaSame k += measure(x i , x + i ) meaDif f k += measure(x i , x − i ) end for meaSame k /= m, meaDif f k /= m end for</formula><p>• product-text consistency. Similar with user- text consistency, a product also has a collection of product-specific words suited to evaluate it. For example, people prefer using "sleek" and "stable" to evaluate a smartphone, while like to use "wire- less" and "mechanical" to evaluate a keyboard.</p><p>We test four consistency assumptions men- tioned above with the same testing criterion, which is formalized in Algorithm 1. For each consistency assumption, we test it for n = 50 iter- ations on each of IMDB, Yelp Dataset Challenge 2013 and 2014 datasets. Taking user-sentiment consistency as an example, in each iteration, we randomly select two reviews x i , x + i written by the same user u i , and a review x − i written by another randomly selected user. Afterwards, we calculate the measurements of (x i , x + i ) and (x i , x − i ), and aggregate these statistics for m users. In user-sentiment assumption test, we use absolute rating difference ||rating a − rating b || as the measurement between two reviews a and b. We illustrate the results in <ref type="figure" target="#fig_2">Figure 1 (a)</ref>  <ref type="bibr">3</ref> , where 2013same/2014same/amzsame (red plots) means that two reviews are written by a same user, and 2013dif f /2014dif f /amzdif f (black plots) means that two reviews are written by different users. We can find that: the absolute rating differences between two reviews written by a same user are lower than those written by dif- ferent users (t-test with p-value &lt; 0.01). In other words, sentiment ratings from the same user are more consistent than those from different users. This validates the user-sentiment consistency.</p><p>For testing product-sentiment consistency, we 1.25</p><p>1.3</p><p>1.35</p><p>1.4</p><p>1.45</p><p>1.5</p><p>1.55</p><p>1.6</p><p>(a) user-sentiment consistency 1.25</p><p>1.3</p><p>1.35</p><p>1.4</p><p>1.45</p><p>1. use absolute rating difference as the measuremen- t. The reviews x i , x + i are towards a same product p i , and x − i is towards another randomly selected product. From <ref type="figure" target="#fig_2">Figure 1</ref> (b), we can see that sen- timent ratings towards the same product are more consistent than those towards different products. In order to verify the assumptions of user-text and product-text consistencies, we use cosine similar- ity between bag-of-words of two reviews as the measurement. Results are given in <ref type="figure" target="#fig_2">Figure 1</ref> (c) and (d). We can see that the textual similarity between two reviews written by a same user (or towards a same product) are higher than those written by d- ifferent users (or towards different products).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">User Product Neural Network (UPNN) for Sentiment Classification</head><p>We present the details of User Product Neural Net- work (UPNN) for sentiment classification. An il- lustration of UPNN is given in <ref type="figure">Figure 2</ref>. It takes as input a review, the user who posts the review, and the product which is evaluated. UPNN cap- tures four kinds of consistencies which are veri- fied in Section 2. It outputs the sentiment category (e.g. 1-5 stars) of a review by considering not only the semantics of review text, but also the informa- tion of user and product. In following subsection- s, we first describe the use of neural network for modeling semantics of variable-sized documents. We then present the methods for incorporating us- er and product information, followed by the use of UPNN in a supervised learning framework for sentiment classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Modeling Semantics of Document</head><p>We model the semantics of documents based on the principle of compositionality (Frege, 1892), which states that the meaning of a longer expres- sion (e.g. a sentence or a document) comes from the meanings of its words and the rules used to combine them. Since a document consists of a list of sentences and each sentence is made up of a list of words, we model the semantic representation of a document in two stages. We first produce con- tinuous vector of each sentence from word repre- sentations. Afterwards, we feed sentence vectors as inputs to compose document representation. For modeling the semantics of words, we rep- resent each word as a low dimensional, continu-</p><formula xml:id="formula_1">Softmax gold rating = 2 w 1 h 1 U k P j h 2 h n Lookup Linear …… Convolution Pooling u k p j v d Tanh w 1 × × w 2 U k P j w 2 × × w n U k P j w n × ×</formula><p>Figure 2: An illustration of the neural network approach for sentiment classification. w i means the i-th word of a review text. u k and p j are continuous vector representations of user k and product j for capturing user-sentiment and product-sentiment consistencies. U k and P j are continuous matrix representations of user k and product j for capturing user-text and product-text consistencies.</p><p>ous and real-valued vector, also known as word embedding ( <ref type="bibr" target="#b1">Bengio et al., 2003</ref>). All the word vectors are stacked in a word embedding matrix L w ∈ R d×|V | , where d is the dimension of word vector and |V | is the size of word vocabulary. These word vectors can be randomly initialized from a uniform distribution, regarded as a param- eter and jointly trained with other parameters of neural networks. Alternatively, they can be pre- trained from text corpus with embedding learning algorithms ( <ref type="bibr" target="#b27">Mikolov et al., 2013;</ref><ref type="bibr" target="#b33">Pennington et al., 2014;</ref><ref type="bibr" target="#b41">Tang et al., 2014b</ref>), and applied as ini- tial values of word embedding matrix. We adopt the latter strategy which better exploits the seman- tic and grammatical associations of words.</p><p>To model semantic representations of sentences, convolutional neural network (CNN) and recur- sive neural network <ref type="bibr" target="#b37">(Socher et al., 2013)</ref> are t- wo state-of-the-art methods. We use CNN <ref type="bibr" target="#b16">(Kim, 2014;</ref><ref type="bibr" target="#b15">Kalchbrenner et al., 2014</ref>) in this work as it does not rely on external parse tree. Specifical- ly, we use multiple convolutional filters with dif- ferent widths to produce sentence representation. The reason is that they are capable of capturing lo- cal semantics of n-grams of various granularities, which are proven powerful for sentiment classifi- cation. The convolutional filter with a width of 3 essentially captures the semantics of trigrams in a sentence. Accordingly, multiple convolutional fil- ters with widths of 1, 2 and 3 encode the semantics of unigrams, bigrams and trigrams in a sentence.</p><p>An illustration of CNN with three convolu- tional filters is given in <ref type="figure" target="#fig_3">Figure 3</ref>.</p><p>Let us denote a sentence consisting of n words as {w 1 , w 2 , ...w i , ...w n }. Each word w i is mapped to its embedding representation e i ∈ R d . A convo- lutional filter is a list of linear layers with shared parameters. Let l cf be the width of a convolution- al filter, and let W cf , b cf be the shared parameters of linear layers in the filter. The input of a linear layer is the concatenation of word embeddings in a fixed-length window size l cf , which is denoted as</p><formula xml:id="formula_2">I cf = [e i ; e i+1 ; ...; e i+l cf −1 ] ∈ R d·l cf .</formula><p>The output of a linear layer is calculated as</p><formula xml:id="formula_3">O cf = W cf · I cf + b cf (1)</formula><p>where W cf ∈ R len×d·l cf , b cf ∈ R len , len is the output length of linear layer. In order to capture the global semantics of a sentence, we feed the output of a convolutional filter to an average pool- ing layer, resulting in an output vector with fixed- length. We further add hyperbolic tangent func- tions (tanh) to incorporate element-wise nonlin- earity, and fold (average) their outputs to generate sentence representation. We feed sentence vectors as the input of an av- erage pooling layer to obtain the document rep- resentation. Alternative document modeling ap- proaches include CNN or recurrent neural net- work. However, we prefer average pooling for its computational efficiency and good performance in our experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Modeling Semantics of Users and Products</head><p>We integrate semantic representations of users and products in UPNN to capture user-sentiment, product-sentiment, user-text and product-text con- sistencies.</p><p>For modeling user-sentiment and product- sentiment consistencies, we embed each user as a continuous vector u k ∈ R du and embed each product as a continuous vector p j ∈ R dp , where d u and d p are dimensions of user vector and product vector, respectively. The basic idea behind this is to map users with similar rating preferences (e.g. prefer assigning 4 stars) into close vectors in user embedding space. Similarly, the products which receive similar averaged ratings are mapped into neighboring vectors in product embedding space.</p><p>In order to model user-text consistency, we rep- resent each user as a continuous matrix U k ∈ R d U ×d , which acts as an operator to modify the semantic meaning of a word. This is on the basis of vector based semantic composition <ref type="bibr" target="#b28">(Mitchell and Lapata, 2010)</ref>. They regard compositional modifier as a matrix X 1 to modify another com- ponent x 2 , and use matrix-vector multiplication y = X 1 × x 2 as the composition function. Multi- plicative semantic composition is suitable for our need of user modifying word meaning, and it has been successfully utilized to model adjective- noun composition <ref type="bibr" target="#b2">(Clark et al., 2008;</ref><ref type="bibr" target="#b0">Baroni and Zamparelli, 2010)</ref> and adverb-adjective composi- tion ( <ref type="bibr" target="#b36">Socher et al., 2012)</ref>. Similarly, we model product-text consistency by encoding each prod- uct as a matrix P j ∈ R d P ×d , where d is the di- mension of word vector, d P is the output length of product-word multiplicative composition. After conducting user-word multiplication and product- word multiplication operations, we concatenate their outputs and feed them to CNN (detailed in Section 3.1) for producing user and product en- hanced document representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Sentiment Classification</head><p>We apply UPNN to document level sentiment clas- sification under a supervised learning framework (Pang and <ref type="bibr" target="#b29">Lee, 2005</ref>). Instead of using hand- crafted features, we use continuous representation of documents, users and products as discrimina- tive features. The sentiment classifier is built from documents with gold standard sentiment labels.</p><p>As is shown in <ref type="figure">Figure 2</ref>, the feature represen- tation for building rating predictor is the concate- nation of three parts: continuous user representa- tion u k , continuous product representation p j and continuous document representation v d , where v d encodes user-text consistency, product-text consis- tency and document level semantic composition. We use sof tmax to build the classifier because its outputs can be interpreted as conditional probabil- ities. Sof tmax is calculated as given in Equation 2, where C is the category number (e.g. 5 or 10).</p><formula xml:id="formula_4">sof tmax i = exp(x i ) C i =1 exp(x i )<label>(2)</label></formula><p>We regard cross-entropy error between gold sentiment distribution and predicted sen- timent distribution as the loss function of sof tmax.</p><p>We take the derivative of loss function through back-propagation with re- spect to the whole set of parameters θ = [W 1,2,3 cf ; b 1,2,3 cf ; u k ; p j ; U k ; P j ; W sof tmax , b sof tmax ], and update parameters with stochastic gradient descent. We set the widths of three convolutional filters as 1, 2 and 3. We learn 200-dimensional sentiment-specific word embeddings (Tang et al., 2014b) on each dataset separately, randomly initialize other parameters from a uniform distri- bution U (−0.01, 0.01), and set learning rate as 0.03.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>We conduct experiments to evaluate UPNN by ap- plying it to sentiment classification of documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setting</head><p>Existing benchmark datasets for sentiment clas- sification such as Stanford Sentiment Treebank ( <ref type="bibr" target="#b37">Socher et al., 2013</ref>) typically only have text infor- mation, but do not contain users who express the sentiment or products which are evaluated. There- fore, we build the datasets by ourselves. In order to obtain large scale corpora without manual anno- tation, we derive three datasets from IMDB (Diao  <ref type="table" target="#tab_0">Table 1</ref>. We split each corpus into training, development and testing sets with a 80/10/10 split, and conduct tokenization and sentence splitting with Stanford CoreNLP ( ). We use standard accuracy ( <ref type="bibr" target="#b25">Manning and Schütze, 1999;</ref><ref type="bibr" target="#b14">Jurafsky and Martin, 2000</ref>) to measure the overall senti- ment classification performance, and use M AE and RM SE to measure the divergences between predicted sentiment ratings (pr) and ground truth ratings (gd).</p><note type="other">Dataset #users #products #reviews #docs/user #docs/product #sents/doc #words/</note><formula xml:id="formula_5">M AE = i |gd i − pr i | N (3) RM SE = i (gd i − pr i ) 2 N (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline Methods</head><p>We compare UPNN with the following baseline methods for document-level sentiment classifica- tion.</p><p>(1) Majority is a heuristic baseline method, which assigns the majority sentiment category in training set to each review in the test dataset.</p><p>(2) In Trigram, we use unigrams, bigrams and trigrams as features and train classifier with sup- ported vector machine (SVM) <ref type="bibr" target="#b6">(Fan et al., 2008)</ref>.</p><p>(3) In TextFeature, we implement hand-crafted text features including word/character ngrams, sentiment lexicon features, negation features, etc al. ( <ref type="bibr" target="#b17">Kiritchenko et al., 2014</ref>).</p><p>(4) We extract user-leniency features ( <ref type="bibr" target="#b9">Gao et al., 2013</ref>) and corresponding product features (denot- ed as UPF) from training data, and concatenate them with the features in baseline (2) and (3).</p><p>(5) We learn word embeddings from training and development sets with word2vec ( <ref type="bibr" target="#b27">Mikolov et al., 2013)</ref>, average word embeddings to get docu- ment representation, and train a SVM classifier.</p><p>(6) We learn sentiment-specific word embed- dings (SSWE) from training and development set- s, and use max/min/average pooling ( <ref type="bibr" target="#b41">Tang et al., 2014b</ref>) to generate document representation.</p><p>(7) We represent sentence with <ref type="bibr">RNTN (Socher et al., 2013</ref>) and compose document representa- tion with recurrent neural network. We average hidden vectors of recurrent neural network as the features for sentiment classification.</p><p>(8) We re-implement PVDM in Paragraph Vec- tor ( <ref type="bibr" target="#b19">Le and Mikolov, 2014</ref>) because its codes are not officially provided. The window size is tuned on development set.</p><p>(9) We compare with a state-of-the-art recom- mendation algorithm JMARS ( <ref type="bibr" target="#b3">Diao et al., 2014</ref>), which leverages user and aspects of a review with collaborative filtering and topic modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model Comparisons</head><p>Experimental results are given in <ref type="table">Table 2</ref>. The re- sults are separated into two groups: the methods above only use texts of review, and the methods below also use user and product information.</p><p>From the first group, we can see that majori- ty performs very poor because it does not cap- ture any text or user information. SVM classi- fiers with trigrams and hand-crafted text features are powerful for document level sentiment classi- fication and hard to beat. We compare the word embedding learnt from each corpus with off-the- shell general word embeddings <ref type="bibr">5</ref> . Results show that tailored word embedding from each corpus performs slightly better than general word embed- dings (about 0.01 improvement in terms of accu- racy). SSWE performs better than context-based word embedding by incorporating sentiment in- formation of texts. Setting a large window size (e.g. 15) is crucial for effectively training SS- WE from documents with accompanying senti-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IMDB</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Yelp 2014</head><p>Yelp 2013 Acc MAE RMSE Acc MAE RMSE Acc MAE RMSE Majority 0.196 1.838 2.495 0.392 0.779 1.097 0.411 0.744 1.060 Trigram 0.399 1.147 1.783 0.577 0.487 0.804 0.569 0.513 0.814 TextFeature 0.402 1.134 1.793 0.572 0.490 0.800 0.556 0.520 0.845 AvgWordvec + SVM 0.304 1.361 1.985 0.530 0.562 0.893 0.526 0.568 0.898 SSWE + SVM 0.312 1.347 1.973 0.557 0.523 0.851 0.549 0.529 0.849 Paragraph Vector 0.341 1.211 1.814 0.564 0.496 0.802 0.554 0.515 0.832 RNTN + Recurrent 0.400 1.133 1.764 0.582 0.478 0.821 0.574 0.489 0.804 UPNN (no UP) 0.405 1.030 1.629 0.585 0.483 0.808 0.577 0.485 0.812 Trigram + UPF 0.404 1.132 1.764 0.576 0.471 0.789 0.570 0.491 0.803 TextFeature + UPF 0.402 1.129 1.774 0.579 0.476 0.791 0.561 0.509 0.822 JMARS N/A 1.285 1.773 N/A 0.710 0.999 N/A 0.699 0. <ref type="figure">985  UPNN (full)</ref> 0.435 0.979 1.602 0.608 0.447 0.764 0.596 0.464 0.784 <ref type="table">Table 2</ref>: Sentiment classification on IMDB, Yelp 2014 and Yelp 2013 datasets. Evaluation metrics are accuracy (Acc, higher is better), MAE (lower is better) and RMSE (lower is better). Our full model is UPNN (full). Our model without using user and product information is abbreviated as UPNN (no UP). The best method in each group is in bold.</p><p>ment labels. RNTN+Reccurent is a strong per- former by effectively modeling document repre- sentation with semantic composition. Our text based model (UPNN no UP) performs slightly bet- ter than RNTN+Reccurent, trigram and text fea- tures.</p><p>From the second group, we can see that con- catenating user product feature (UPF) with exist- ing feature sets does not show significant improve- ments. This is because the dimension of existing feature sets is typically huge (e.g. 1M trigram fea- tures in Yelp 2014), so that concatenating a smal- l number of UPF features does not have a great influence on the whole model. We do not evalu- ate JMARS in terms of accuracy because JMARS outputs real-valued ratings. Our full model UPNN yields the best performance on all three dataset- s. Incorporating semantic representations of us- er and product significantly (t-test with p-value &lt; 0.01) boosts our text based model (UPNN no UP). This shows the effectiveness of UPNN over stan- dard trigrams and hand-crafted features when in- corporating user and product information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Model Analysis: Effect of User and Product Representations</head><p>We investigate the effects of vector based user and product representations (u k , p j ) as well as ma- trix based user and product representations (U k , P j ) for sentiment classification. We remove vec- tor based representations (u k , p j ) and matrix based representations (U k , P j ) from UPNN separately, and conduct experiments on three datasets. From <ref type="table">Table 3</ref>, we can find that vector based representa- tions (u k , p j ) are more effective than matrix based representations (U k , P j ). This is because u k and p j encode user-sentiment and product-sentiment consistencies, which are more directly associat- ed with sentiment labels than user-text (U k ) and product-text (P j ) consistencies. Another reason might be that the parameters of vector represen- tations are less than the matrix representations, so that the vector representations are better estimat- ed. We also see the contribution from each of user and product by removing (U k , u k ) and (P j , p j ) separately. Results are given in <ref type="table">Table 3</ref>. It is in- teresting to find that user representations are obvi- ously more effective than product representations for review rating prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Discussion: Out-Of-Vocabulary Users and Products</head><p>Out-of-vocabulary (OOV) situation occurs if a us- er or a product in testing/decoding process is n- ever seen in training data. We give two natu- ral solutions (avg UP and unk UP) to deal with OOV users and products. One solution (avg UP) is to regard the averaged representations of user- s/products in training data as the representation of OOV user/product. Another way (unk UP) is to learn a shared "unknown" user/product represen- tation for low-frequency users in training data, and apply it to OOV user/product.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IMDB</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Yelp 2014 Yelp 2013 Acc MAE RMSE Acc MAE RMSE Acc MAE RMSE UPNN (full)</head><p>0.435 0.979 1.602 0.608 0.447 0.764 0.596 0.464 0.784 UPNN − u k − p j 0.409 1.021 1.622 0.585 0.483 0.808 0.572 0.491 0.823 UPNN − U k − P j 0.426 0.993 1.607 0.597 0.465 0.789 0.585 0.482 0.802 UPNN − U k − u k 0.324 1.209 1.743 0.577 0.475 0.778 0.566 0.505 0.828 UPNN − P j − p j 0.397 1.075 1.712 0.595 0.462 0.776 0.590 0.476 0.802 <ref type="table">Table 3</ref>: Influence of user and product representations. For user k and product j, u k and p j are their continuous vector representations, U k and P j are their continuous matrix representations (see <ref type="figure">Figure 2)</ref>. In order to evaluate the two strategies for OOV problem, we randomly select 10 percent users and products from each development set, and mask their user and product information. We run avg UP, unk UP together with UPNN (no UP) which only uses text information, and UPNN (full) which learns tailored representation for each user and product. We evaluate classification accuracy on the extracted OOV test set. Experimental results are given in <ref type="figure">Figure 5</ref>. We can find that these two strategies perform slightly better than UPNN (no UP), but still worse than the full model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Sentiment Classification</head><p>Sentiment classification is a fundamental prob- lem in sentiment analysis, which targets at infer- ring the sentiment label of a document. <ref type="bibr" target="#b31">Pang and Lee (2002;</ref><ref type="bibr" target="#b29">2005)</ref> cast this problem a classifica- tion task, and use machine learning method in a supervised learning framework. <ref type="bibr" target="#b11">Goldberg and Zhu (2006)</ref> use unlabelled reviews in a graph- based semi-supervised learning method. Many s- tudies design effective features, such as text top- ic ( <ref type="bibr" target="#b8">Ganu et al., 2009)</ref>, bag-of-opinion ( <ref type="bibr" target="#b35">Qu et al., 2010)</ref> and sentiment lexicon features ( <ref type="bibr" target="#b17">Kiritchenko et al., 2014</ref>). User information is also used for sentiment classification. <ref type="bibr" target="#b9">Gao et al. (2013)</ref> de- sign user-specific features to capture user lenien- cy. <ref type="bibr" target="#b5">Li et al. (2014)</ref> incorporate textual topic and user-word factors with supervised topic modeling. <ref type="bibr" target="#b39">Tan et al. (2011)</ref> and <ref type="bibr" target="#b12">Hu et al. (2013)</ref> utilize user- text and user-user relations for Twitter sentimen- t analysis. Unlike most previous studies that use hand-crafted features, we learn discriminative fea- tures from data. We differ from <ref type="bibr" target="#b5">Li et al. (2014)</ref> in that we encode four kinds of consistencies and use neural network approach. User representation is also leveraged for recommendation <ref type="bibr" target="#b43">(Weston et al., 2013)</ref>, web search ( <ref type="bibr" target="#b38">Song et al., 2014</ref>) and social media analytics ( <ref type="bibr" target="#b34">Perozzi et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Neural Network for Sentiment Classification</head><p>Neural networks have achieved promising results for sentiment classification. Existing neural net- work methods can be divided into two groups: word embedding and semantic composition. For learning word embeddings, ( <ref type="bibr" target="#b27">Mikolov et al., 2013;</ref><ref type="bibr" target="#b33">Pennington et al., 2014</ref>) use local and global con- texts, <ref type="bibr" target="#b24">(Maas et al., 2011;</ref><ref type="bibr" target="#b18">Labutov and Lipson, 2013;</ref><ref type="bibr" target="#b41">Tang et al., 2014b;</ref><ref type="bibr" target="#b40">Tang et al., 2014a</ref>  <ref type="bibr" target="#b22">(Li, 2014)</ref>, and also used for opinion relation detection ( ). <ref type="bibr" target="#b21">Li et al. (2015)</ref> compare the effectiveness of re- cursive neural network and recurrent neural net- work on five NLP tasks including sentiment clas- sification. ( <ref type="bibr" target="#b15">Kalchbrenner et al., 2014;</ref><ref type="bibr" target="#b16">Kim, 2014;</ref><ref type="bibr" target="#b13">Johnson and Zhang, 2014</ref>) use convolutional neu- ral networks. <ref type="bibr" target="#b19">Le and Mikolov (2014)</ref> introduce Paragraph Vector. Unlike existing neural network approaches that only use the semantics of texts, we take consideration of user and product rep- resentations and leverage their connections with text semantics for sentiment classification. This work is an extension of our previous work <ref type="bibr" target="#b42">(Tang et al., 2015)</ref>, which only takes consideration of user- word association.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we introduce User Product Neu- ral Network (UPNN) for document level senti- ment classification under a supervised learning framework. We validate user-sentiment, product- sentiment, user-text and product-text consistencies on massive reviews, and effectively integrate them in UPNN. We apply the model to three datasets derived from IMDB and Yelp Dataset Challenge. Empirical results show that: (1) UPNN outper- forms state-of-the-art methods for document level sentiment classification; (2) incorporating contin- uous user and product representations significantly boosts sentiment classification accuracy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Assumption testing of user-sentiment, product-sentiment, user-text and product-text consistencies. We test them on the datasets from IMDB and Yelp Dataset Challenge in 2013 and 2014.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Convolutional neural network with multiple convolutional filters for sentence modeling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Accuracy of OOV user and product on OOV test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : Statistical information of IMDB, Yelp 2014 and Yelp 2013 datasets used for sentiment classifi- cation. The rating scale of IMDB dataset is 1-10. The rating scale of Yelp 2014 and</head><label>1</label><figDesc></figDesc><table>doc 
</table></figure>

			<note place="foot">users who express the sentiment and products which are evaluated, both of which have great influences on interpreting the sentiment of text. In this paper, we address this issue by incorporating user-and product-level information into a neural network approach for document level sentiment classification. Users and products are modeled using vector space models, the representations of which capture important global clues such as individual preferences of users or overall qualities of products. Such global evidence in turn facilitates embedding learning procedure at document level, yielding better text representations. By combining evidence at user-, product-and documentlevel in a unified neural framework, the proposed model achieves state-of-the-art performances on IMDB and Yelp datasets 1 .</note>

			<note place="foot" n="1"> The codes and datasets are available at http://ir. hit.edu.cn/ ˜ dytang/</note>

			<note place="foot" n="2"> One can manually design a small number of user and product features (Gao et al., 2013). However, we argue that they are not effective enough to capture sophisticated semantics of users and products.</note>

			<note place="foot" n="3"> Since the rating scale of IMDB (1-10) is different from Yelp (1-5), we divide the rating difference of IMDB reviews by two for better visualizing and analyzing the results.</note>

			<note place="foot" n="4"> http://www.yelp.com/dataset_challenge</note>

			<note place="foot" n="5"> We compare with Glove embeddings learnt from Wikipedia and Twitter http://nlp.stanford.edu/ projects/glove/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors give great thanks to Furu Wei, Lei Cui, Nan Yang, Jiwei Li, Yaming Sun, Mao Zheng and anonymous reviewers for their valuable com-ments. We would like to thank Qiming Diao for providing the IMDB dataset as well as the codes of JMARS. This work was supported by the Nation-al High Technology Development 863 Program of China (No. 2015AA015407), National Natural Science Foundation of China <ref type="bibr">(No. 61133012 and No. 61273321)</ref>. Duyu Tang is supported by Baidu Fellowship and IBM Ph.D. Fellowship.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1183" to="1193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A compositional distributional model of meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Coecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Quantum Interaction Symposium</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="133" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Jointly modeling aspects, ratings and sentiments for movie recommendation (jmars)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiming</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="193" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A few useful things to know about machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="78" to="87" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptive multi-compositionality for recursive neural models with applications to sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1537" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Liblinear: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Rong-En Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Gottlob Frege. 1892. On sense and reference</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="563" to="584" />
			<pubPlace>Ludlow</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Beyond the stars: Improving rating predictions using review text content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gayatree</forename><surname>Ganu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noemie</forename><surname>Elhadad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amélie</forename><surname>Marian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WebDB</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Modeling user leniency and product popularity for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoki</forename><surname>Yoshinaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nobuhiro</forename><surname>Kaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaru</forename><surname>Kitsuregawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICJNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1107" to="1111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Domain adaptation for large-scale sentiment classification: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Seeing stars when there aren&apos;t many stars: graph-based semisupervised learning for sentiment categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GraphBased Method for NLP</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="45" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploiting social relations for sentiment analysis in microblogging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="537" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Effective use of word order for text categorization with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rie</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<idno>arXiv preprint: 1412.1058</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Speech &amp; language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>James H Martin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Pearson Education India</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="655" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sentiment analysis of short informal texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mohammad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="page" from="723" to="762" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Re-embedding words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Labutov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Suit: A supervised user-item based topic model for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1636" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eudard</forename><surname>Hovy</surname></persName>
		</author>
		<idno>arXiv preprint:1503.00185</idno>
		<title level="m">When are tree structures necessary for deep learning of representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Feature weight tuning for recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<idno>1412.3714</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">Arxiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sentiment analysis and opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Human Language Technologies</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="167" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Andrew L Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Foundations of statistical natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The stanford corenlp natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Composition in distributional models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1388" to="1429" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Opinion mining and sentiment analysis. Foundations and trends in information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Thumbs up?: sentiment classification using machine learning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivakumar</forename><surname>Vaithyanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Global belief recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2888" to="2896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The bag-of-opinions method for review rating prediction from sparse text patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Ifrim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="913" to="921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Semantic Compositionality Through Recursive Matrix-Vector Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Adapting deep ranknet for personalized search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="83" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">User-level sentiment analysis incorporating social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1397" to="1405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Building large-scale twitter-specific sentiment lexicon: A representation learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="172" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning sentimentspecific word embedding for twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1555" to="1565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">User modeling with neural network for review rating prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuekui</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Nonlinear latent factorization by embedding multiple user interests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hector</forename><surname>Yee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RecSys</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="65" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Joint opinion relation detection using one-class deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="677" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Representation learning for aspect category detection in online reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
