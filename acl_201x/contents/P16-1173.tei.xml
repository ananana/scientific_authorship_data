<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Phrase Structure Annotation and Parsing for Learner English</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryo</forename><surname>Nagata</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
							<email>keisuke@cs.jhu.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Konan University</orgName>
								<address>
									<addrLine>8-9-1 Okamoto</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Higashinada Kobe</orgName>
								<address>
									<postCode>658-8501</postCode>
									<settlement>Hyogo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<addrLine>3400 North Charles Street Baltimore</addrLine>
									<postCode>21218</postCode>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Phrase Structure Annotation and Parsing for Learner English</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1837" to="1847"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>There has been almost no work on phrase structure annotation and parsing specially designed for learner English despite the fact that they are useful for representing the structural characteristics of learner En-glish. To address this problem, in this paper , we first propose a phrase structure annotation scheme for learner English and annotate two different learner corpora using it. Second, we show their usefulness, reporting on (a) inter-annotator agreement rate, (b) characteristic CFG rules in the corpora, and (c) parsing performance on them. In addition, we explore methods to improve phrase structure parsing for learner English (achieving an F-measure of 0.878). Finally, we release the full annotation guidelines, the annotated data, and the improved parser model for learner English to the public.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learner corpora have been essential for NLP tasks related to learner language such as grammatical error correction. They are normally annotated with linguistic properties. In the beginning, at- tention was mainly focused on grammatical error annotation ( <ref type="bibr" target="#b15">Izumi et al., 2004;</ref><ref type="bibr" target="#b7">Díaz-Negrillo et al., 2009;</ref><ref type="bibr" target="#b5">Dale and Kilgarriff, 2011;</ref><ref type="bibr" target="#b18">Ng et al., 2013)</ref>. Recently, it has been expanded to gram- matical annotation -first, Part-Of-Speech (POS) tagging <ref type="bibr" target="#b7">(Díaz-Negrillo et al., 2009;</ref><ref type="bibr" target="#b17">Nagata et al., 2011</ref>) and then syntactic annotation ( <ref type="bibr" target="#b16">Kepser et al., 2004;</ref><ref type="bibr" target="#b8">Dickinson and Ragheb, 2009;</ref><ref type="bibr" target="#b21">Ragheb and Dickinson, 2012;</ref><ref type="bibr" target="#b22">Ragheb and Dickinson, 2013)</ref>; syntactic annotation for learner corpora is now in- tensively studied. Among a variety of studies, a series of work by <ref type="bibr" target="#b8">Ragheb and Dickinson (Dickinson and Ragheb, 2009;</ref><ref type="bibr" target="#b21">Ragheb and Dickinson, 2012;</ref><ref type="bibr" target="#b22">Ragheb and Dickinson, 2013</ref>) is important in that they proposed a dependency annotation scheme, theoretically and empirically evaluated it, and revealed its theoretical problems, which gives a good starting point to those who wish to develop a new annotation scheme for learner corpora. Re- searchers including <ref type="bibr" target="#b10">Foster (2004)</ref> and <ref type="bibr" target="#b19">Ott and Ziai (2010)</ref> have even started using dependency- annotated learner corpora to develop dependency parsers for learner language.</p><p>Although research on syntactic analysis for learner corpora has been making great progress as noted above, it is not yet complete. There are at least three limitations in the previous work: (i) as far as we are aware, there has been almost no work on phrase structure annotation specially designed for learner corpora; (ii) there are no publicly avail- able learner corpora annotated with syntax; (iii) phrase structure parsing performance on learner English has not yet been reported.</p><p>The first limitation is that there exists no phrase structure annotation scheme specially designed for learner English. As related work, <ref type="bibr" target="#b11">Foster (2007a;</ref><ref type="bibr" target="#b12">2007b)</ref> and <ref type="bibr" target="#b9">Foster and Andersen (2009)</ref> propose a method for creating a pseudo-learner corpus by ar- tificially generating errors in a native corpus with phrase structures. However, the resulting corpus does not capture various error patterns in learner English.</p><p>Concerning the second limitation, a corpus greatly increases in value when it is available to the public as has been seen in other domains. Nev- ertheless, whether dependency or phrase structure, there seems to be no publicly available learner cor- pora annotated with syntax.</p><p>The above two limitations cause the third one that phrase structure parsing performance on leaner <ref type="bibr">English</ref> has not yet been reported. For this reason, Cahill (2015) demonstrates how ac-curately an existing parser performs on a pseudo- learner corpus (section 23 of WSJ with errors arti- ficially generated by <ref type="bibr" target="#b9">Foster and Andersen (2009)</ref>'s method). <ref type="bibr" target="#b1">Cahill et al. (2014)</ref> show the perfor- mance of a phrase structure parser augmented by self-training on students' essays, many of which are presumably written by native speakers of En- glish. <ref type="bibr" target="#b24">Tetreault et al. (2010)</ref> partially show phrase structure parsing performance concerning prepo- sition usage in learner English, concluding that it is effective in extracting features for preposition error correction. We need to reveal full parsing performance to be able to confirm that this is true for other syntactic categories and whether or not we should use phrase structure parsing to facilitate related tasks such as grammatical error correction and automated essay scoring.</p><p>Here, we emphasize that phrase structure anno- tation has at least two advantages over dependency annotation <ref type="bibr">1</ref> . First of all, it can directly encode in- formation about word order. This is particularly important because learner corpora often contain errors in word order. For example, phrase struc- ture parsing will reveal in which phrases errors in word order tend to occur as we will partly do in Sect. 3. Second of all, phrase structure rather ab- stractly represents syntactic information in terms of phrase-to-phrase relations. This means that the characteristics of learner English are represented by means of phrase-to-phrase relations (e.g., con- text free grammar (CFG) rules) or even as trees. Take as an example, one of the characteristic trees we found in the corpora we have created: S NP VP ϕ ADJP As we will discuss in Sect. 3, this tree suggests the mother tongue interference that the copula is not necessary in adjective predicates in certain lan- guages. It would be linguistically interesting to re- veal what CFG rules we need to add to, or subtract from, the native CFG rule set to be able to generate learner English. This is our primary motivation for this work although our other motivations include developing a parser for learner English.</p><p>In view of this background, we address the above problems in this paper. Our contributions are three-fold. First, we present a phrase struc- ture annotation scheme for dealing with learner English consistently and reliably. For this, we pro- pose five principles which can be applied to creat- ing a novel annotation scheme for learner corpora. Second, we evaluate the usefulness of the anno- tation scheme by annotating learner corpora us- ing it. To be precise, we report on inter-annotator agreement rate and characteristic CFG rules in the corpora, and take the first step to revealing phrase structure parsing performance on learner English. In addition, we explore methods to improve phrase structure parsing for learner English. Finally, we release the full annotation guidelines, the anno- tated corpora, and the improved parser model to the public.</p><p>The rest of this paper is structured as follows. Sect. 2 describes the annotation scheme. Sect. 3 explores the annotated learner corpora. Sect. 4 evaluates parsing performance using it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Phrase Structure Annotation Scheme</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">General Principles</head><p>The annotation scheme is designed to consistently retrieve the structure in the target text that is clos- est to the writer's intention. The following are the five principles we created to achieve it: (P1) Consistency-first principle (P2) Minimal rule set principle (P3) Locally superficially-oriented principle (P4) Minimum edit distance principle (P5) Intuition principle (P1) states that the most important thing in our annotation scheme is consistency. It is a trade-off between quality and quantity of information; de- tailed rules that are too complicated make anno- tation unmanageable yet they may bring out valu- able information in learner corpora. Corpus anno- tation will be useless if it is inconsistent and un- reliable no matter how precisely the rules can de- scribe linguistic phenomena. Therefore, this prin- ciple favors consistency over completeness. Once we annotate a corpus consistently, we consider adding further detailed information to it.</p><p>(P2) also has to do with consistency. The smaller the number of rules is, the easier it be- comes to practice the rules. Considering this, if we have several candidates for describing a new lin- guistic phenomenon particular to learner English, we will choose the one that minimizes the number of modifications to the existing rule set. Note that this applies to the entire rule set; an addition of a rule may change the existing rule set.</p><p>(P3) is used to determine the tag of a given token or phrase. As several researchers <ref type="bibr">(DíazNegrillo et al., 2009;</ref><ref type="bibr" target="#b8">Dickinson and Ragheb, 2009;</ref><ref type="bibr" target="#b17">Nagata et al., 2011;</ref><ref type="bibr" target="#b21">Ragheb and Dickinson, 2012</ref>) point out, there are two ways of per- forming annotation, according to either superficial (morphological) or contextual (distributional) evi- dence. For example, in the sentence *My univer- sity life is enjoy., the word enjoy can be interpreted as a verb according to its morphological form or as an adjective (enjoyable) or a noun (enjoyment) according to its context. As the principle itself construes, our annotation scheme favors superfi- cial evidence over distributional. This is because the interpretation of superficial evidence has much less ambiguity and (P3) can determine the tag of a given token by itself as seen in the above example. Distributional information is also partly encoded in our annotation scheme as we discuss in Sub- sect. 2.2.</p><p>(P4) regulates how to reconstruct a correct form of a given sentence containing errors, which helps to determine its phrase structure. The problem is that often one can think of several candidates as possible corrections, which can become a source of inconsistency. (P4) gives a clear solution to this problem. It selects the one that minimizes the edit distance from the original sentence. Note that the edit distances for deletion, addition, and replace- ment are one, one, and two (deletion and addition), respectively in our definition.</p><p>For the cases to which these four principles do not apply, the fifth and final principle (P5) al- lows annotators to use their intuition. It should be noted, however, that the five principles apply in the above order to avoid unnecessary inconsistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Annotation Rules</head><p>Our annotation scheme is based on the POS- tagging and shallow-parsing annotation guidelines for learner <ref type="bibr">English (Nagata et al., 2011</ref>), which in turn are based on the Penn Treebank II-style bracketing guidelines ( <ref type="bibr" target="#b0">Bies et al., 1995</ref>) (which will be referred to as PTB-II, hereafter). This natu- rally leads us to adopt the PTB-II tag set in ours; an exception is that we exclude the function tags and null elements from our present annotation scheme for annotation efficiency 2 . Accordingly, we revise the above guidelines to be able to describe phrase structures characteristic of learner English.</p><p>The difficulties in syntactic annotation of learner English mainly lie in the fact that gram- matical errors appear in learner English. Gram- matical errors are often classified into three types as in <ref type="bibr" target="#b15">Izumi et al. (2004)</ref>: omission, insertion, and replacement type errors. In addition, we include other common error types (word order errors and fragments) in the error types to be able to describe learners' characteristics more precisely. The fol- lowing discuss how to deal with these five error types based on the five principles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Omission Type Errors</head><p>This type of error is an error where a necessary word is missing. For example, some kind of deter- miner is missing in the sentence *I am student.</p><p>The existing annotation rules in PTB-II can han- dle most omission type errors. For instance, the PTB-II rule set would parse the above example as "(S (NP I) (VP am (NP student).))." Note that syn- tactic tags for irrelevant parts are omitted in this example (and hereafter).</p><p>A missing head word may be more problematic. Take as an example the sentence *I busy. where a verb is missing. The omission prevents the rule S → NP VP from applying to it. If we created a new rule for every head-omission with no limitation, it would undesirably increase the number of rules, which violates (P2).</p><p>To handle head-omissions, we propose a func- tion tag -ERR. It denotes that a head is missing in the phrase in question. The function tag makes it possible to apply the PTB-II rule set to sentences containing head-omissions as in:</p><formula xml:id="formula_0">S NP I VP-ERR ϕ ADJP busy</formula><p>We need to reconstruct a correct form of a given sentence to determine whether or not a head word is missing. We use Principle (P4) for solving the problem as discussed in Sect. 2.1. For instance, the sentence *I want to happy. can be corrected as either I want to be happy. (edit distance is one; an addition of a word) or I want happiness. (three; two deletions and an addition). Following (P4), we select the first correction that minimizes the edit distance, resulting in: Insertion type errors are more problematic than omission type errors. It is not trivial how to an- notate an erroneous extra word. On the one hand, one can argue that the extra word about is a prepo- sition from its morphological form. On the other hand, one can also argue that it is not, because the verb discuss takes no preposition. As with this ex- ample, insertion type errors involve an ambiguity between superficial and distributional categories.</p><p>Principles (P2) and (P3) together solve the am- biguity. According to (P3), one should always stick to the superficial evidence. For example, the extra word about should be tagged as a prepo- sition. After this, PTB-II applies to the rest of the sentence, which satisfies (P2). As a result, one would obtain the parse "(S (NP She) (VP dis- cussed (PP (IN about) (NP it)))).)."</p><p>Insertion type errors pose a more vital problem in some cases. Take as an example the sentence *It makes me to happy. where the word to is erro- neous. As before, one can rather straightforwardly tag it as a preposition, giving the POS sequence:</p><formula xml:id="formula_1">*It/PRP makes/VBZ me/PRP to/TO happy/JJ ./.</formula><p>However, none of the PTB-II rules applies to the POS sequence TO JJ to make a phrase. This means that we have to create a new rule for such cases. There are at most three possibilities of grouping the words in question to make a phrase:</p><p>to happy me to me to happy</p><p>Intuitively, the first one seems to be the most ac- ceptable. To be precise, the second one assumes a postposition, contrary to the English preposi- tion system. The third one assumes a whole new rule generating a phrase from a personal pro- noun, a preposition, and an adjective into a phrase. Thus, they cause significant modifications to PTB- II, which violates (P2). In contrast, a preposi- tion normally constitutes a prepositional phrase with another phrase (although not normally with an adjective phrase). Moreover, the first grouping would produce for the rest of the words the per- fect phrase structure corresponding to the correct sentence without the preposition to:</p><formula xml:id="formula_2">S NP me ?</formula><p>TO to ADJP happy which satisfies (P2) unlike the second and third ones. Accordingly, we select the first one.</p><p>All we have to do now is to name the phrase to happy. There is an ambiguity between PP and ADJP, both of which can introduce the parent S. The fact that a preposition constitutes a preposi- tional phrase with another phrase leads us to select PP for the phrase. Furthermore, the tag of a phrase is normally determined by the POS of one of the immediate constituents, if any, that is entitled to be a head (i.e., the headedness). Considering this, we select PP in this case, which would give the parse to the entire sentence as follows:"(S (NP It) (VP makes (S (NP me) (PP (TO to) (ADJP happy)))).)"</p><p>In summary, for insertion errors to which PTB- II do not apply, we determine their phrase struc- tures as follows: (i) intuitively group words into a phrase, minimizing the number of new rules added (it is often helpful to examine whether an existing rule is partially applicable to the words in ques- tion); (ii) name the resulting phrase by the POS of one of the immediate children that is entitled to be a head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Replacement Type Errors</head><p>A replacement type error is an error where a word should be replaced with another word. For exam- ple, in the sentence: *I often study English con- versation., the verb study should be replaced with a more appropriate verb such as practice.</p><p>To handle replacement type errors systemati- cally, we introduce a concept called POS class, which is a grouping of POS categories defined as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Members</head><p>Noun NN, NNS, NNP, NNPS Verb VB, VBP, VBZ, VBD Adjective JJ, JJR, JJS Adverb RB, RBR, RBS Participle VBN, VBG <ref type="table">Table 1</ref>: POS class.</p><p>in <ref type="table">Table 1</ref>; POS tags that are not shown in <ref type="table">Table 1</ref> form a POS class by itself. If the replacement in question is within the same POS class, it is anno- tated following Principles (P2) and (P3). Namely, the erroneous word is tagged according to its su- perficial form and the rest of the sentence is anno- tated by the original rule set, which avoids creating new rules <ref type="bibr">3</ref> . If the replacement in question is from one POS class to another, we will need to take spe- cial care because of the ambiguity between super- ficial and distributional POS categories. For ex- ample, consider the sentence *I went to the see. where the word see is used as a noun, which is not allowed in the standard English, and the inten- tion of the learner is likely to be sea (from the sur- rounding context). Thus, the word see is ambigu- ous between a verb and a noun in the sentence. To avoid the ambiguity, we adopt a two layer- annotation scheme <ref type="bibr" target="#b7">(Díaz-Negrillo et al., 2009;</ref><ref type="bibr" target="#b17">Nagata et al., 2011;</ref><ref type="bibr" target="#b21">Ragheb and Dickinson, 2012</ref>) to include both POSs. In our annotation scheme, we use a special tag (CE) for the replacement error and encode the two POSs as its attribute values as in CE:VB:NN. Then we can use the distributional POS tag to annotate the rest of the sentence. For example, the above example sentence would give a tree:  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4">Errors in Word Order</head><p>Errors in word order often appear in learner En- glish. A typical example would be the reverse of the subject-object order: *This place like my friends. (correctly, My friends like this place.). Principles (P2) and (P3) again play an impor- tant role in handling errors in word order. We first determine the POS tag of each word according to its morphological form. This is rather straightfor- ward because errors in word order do not affect the morphological form. Then we determine the whole structure based on the resulting POS tags, following Principle (P2); if rules in PTB-II apply to the sentence in question, we parse it according to them just as in the above example sentence: "(S (NP This place) (VP like (NP my friends)).)" Even if any of the existing rules do not apply to a part of the sequence of the given POS tags, we stick to Principle (P3) as much as possible. In other words, we determine partial phrase structures according to the given POS sequence to which the existing rule set applies. Then we use the XP-ORD tag to put them together into a phrase. As an example, consider the sentence *I ate lunch was delicious. (correctly, The lunch I ate was delicious.). Ac- cording to the superficial forms and local contexts, the phrase I ate lunch would form an S: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.5 Fragments</head><p>In learner corpora, sentences are sometimes in- complete. They are called fragments (e.g., missing main clause: Because I like it.). Fortunately, there exists already a tag for frag- ments in PTB-II: FRAG. Accordingly, we use it in our annotation scheme as well. For ex- ample, the above example would give the parse "(FRAG (SBAR Because (S (NP I (VP like (NP it))))).)" An exception is incomplete sentences which are defined as S in the bracketing guide- lines for biomedical texts ( <ref type="bibr" target="#b25">Warner et al., 2012</ref>). We tag such incomplete sentences as S following the convention. For example, an adjective phrase can form an S (e.g., (S (ADVP Beautiful)!)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.6">Unknown Words and Phrases</head><p>There are cases where one cannot tell the tag of a given word. We use the UK tag for such words (e.g., Everyone is don/UK).</p><p>Even if its tag is unknown, it is somehow clear in some cases that the unknown word is the head word of the phrase just as in the above example.</p><p>In that case, we use the UP tag so that it satis- fies the rule about the headedness of a phrase we have introduced in Subsect. 2.2.2. Based on this, the above example would give the parse "(S (NP everyone) (VP is (UP (UK don ))).)"</p><p>For a phrase whose head word is unknown due to some error(s) in it, we use the XP tag instead of the UP tag. As a special case of XP, we use the XP-ORD tag to denote the information that we cannot determine the head of the phrase because of an error in word order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Corpus Annotation</head><p>We selected the Konan-JIEM (KJ) learner cor- pus <ref type="bibr" target="#b17">(Nagata et al., 2011</ref>) (beginning to interme- diate levels) as our target data. It is manually an- notated with POSs, chunks, and grammatical er- rors, which helps annotators to select correct tags. We also included in the target data a part of the essays in ICNALE (Ishikawa, 2011) consisting of a variety of learners (beginning to advanced lev- els <ref type="bibr">4</ref> ) in Asia (China, Indonesia, Japan, Korea, Tai- wan, Thailand, Hong Kong, Singapore, Pakistan, Philippines). <ref type="table" target="#tab_0">Table 2</ref> shows the statistics on the two learner corpora.</p><p>Two professional annotators 5 participated in the annotation process. One of them first annotated the KJ data and double-checked the results. Be- tween the first and second checks, we discussed the results with the annotator. We revised the an- notation scheme based on the discussion which resulted in the present version. Then the second annotator annotated a part of the KJ data to eval- uate the consistency between the two annotators. We took out 11 texts (955 tokens) as a develop- ment set. The second annotator annotated it us- ing the revised annotation scheme where she con- sulted the first annotator if necessary. After this, we provided her with the differences between the results of the two annotators. Finally, the first an- notator annotated the data in ICNALE while the second independently another part of the KJ data and a part of the ICNALE data (59 texts, 12,052 tokens in total), which were treated as a test set. <ref type="table">Table 3</ref> shows inter-annotator agreement mea- sured in recall, precision, F -measure, com- plete match rate, and chance-corrected mea- sure <ref type="bibr" target="#b23">(Skjaerholt, 2014)</ref>. We used the EVALB tool <ref type="bibr">6</ref> with the <ref type="bibr" target="#b4">Collins (1997)</ref>'s evaluation parame- ter where we regarded the annotation results of the first annotator as the gold standard set. We also used the syn-agreement tool <ref type="bibr">7</ref> to calculate chance- corrected measure. It turns out that the agreement is very high. Even in the test set, they achieve an F -measure of 0.928 and a chance-corrected measure of 0.982. This shows that our annota- tion scheme enabled the annotators to consistently recognize the phrase structures in the learner cor- pora in which grammatical errors frequently ap- pear. The comparison between the results of the two annotators shows the major sources of the dis- agreements. One of them is annotation concerning adverbial phrases. In PTB-II, an adverbial phrase between the subject NP and the main verb is al- lowed to be a constituent of the VP (e.g., (S (NP I) (VP (ADVP often) go))) and also of the S (e.g., (S (NP I) (ADVP often) (VP go))). Another ma- jor source is the tag FRAG (fragments); the anno- tators disagreed on distinguishing between FRAG and S in some cases.</p><p>The high agreement shows that the annotation scheme provides an effective way of consistently annotating learner corpora with phrase structures. However, one might argue that the annotation does not represent the characteristics of learner English well because it favors consistency (and rather sim- ple annotation rules) over completeness.</p><p>To see if the annotation results represent the  <ref type="table">Table 3</ref>: Inter-annotator agreement measured in Recall (R), Precision (P ), F -measure (F ), Complete Match Rate (CMR), and Chance-Corrected Measure (CCM).</p><note type="other">Corpus # essays # sentences # tokens # errors/token # errors/sentence KJ 233 3,260 30,517 0.15 1.4 ICNALE 134 1,930 33,913 0.08 1.4</note><p>characteristics of learner English, we extracted characteristic CFG rules from them. The basic idea is that we compare the CFG rules obtained from them with those from a native corpus (the Penn Treebank-II) <ref type="bibr">8</ref> ; we select as characteristic CFG rules those that often appear in the learner corpora and not in the native corpus. To formal- ize the extraction procedures, we denote a CFG rule and its conditional probability as A → B and p(B|A), respectively. Then we define the score for</p><formula xml:id="formula_3">A → B by s(A → B) = log p L (B|A) p N (B|A)</formula><p>where we distinguish between learner and native corpora by the subscripts L and N , respectively. We estimate p(B|A) by expected likelihood estimation. Note that we remove the function tags to reduce the dif- ferences in the syntactic tags in both corpora when we calculate the score. <ref type="table">Table 4</ref> shows the top 10 characteristic CFG rules sorted in descending and ascending or- der according to their scores, which correspond to overused and underused rules in the learner corpora, respectively. Note that <ref type="table">Table 4</ref> ex- cludes rules consisting of only terminal and/or pre- terminal symbols to focus on the structural char- acteristics. Also, it excludes rules containing a Quantifier Phrase (QP; e.g., (NP (QP 100 million) dollars)), which frequently appear and is one of the characteristics in the native corpus.</p><p>In the overused column, CFG rules often con- tain the ϕ element. At first sight, this does not seem so surprising because ϕ never appears in the native corpus. However, the rules actually show in which syntactic environment missing heads tend to occur. For example, the CFG rule PP → ϕ S shows that prepositions tend to be missing in the prepositional phrase governing an S as in *I am good doing this, which we had not realized before this investigation. More interestingly, the CFG rule VP → ϕ ADJP reveals that an adjective phrase can form a verb phrase without a verb in learner English. Looking into the annotated data shows that the copula is missing in predicative ad- jectives as in the tree:</p><formula xml:id="formula_4">S NP I VP ϕ ADJP busy</formula><p>This suggests the transfer of the linguistic system that the copula is not necessary or may be omitted in predicate adjectives in certain languages such as Japanese and Chinese. Similarly, the rule VP → ϕ NP shows in which environment a verb taking the object tends to be missing. Out of the 28 instances, 18 (64%) are in a subordinate clause, which im- plies that learners tend to omit a verb when more than one verb appear in a sentence.</p><p>The second rule S → XP VP . implies that the subject NP cannot be recognized because of a combination of grammatical errors (c.f., S → NP VP .). The corpus data show that 21% of XP in S → XP VP . are actually XP-ORD concerning an error in a relative clause just as shown in the tree in Subsect. 2.2.4. Some of the learners appar- ently have problems in appropriately using relative clauses in the subject position. It seems that the structure of the relative clause containing another verb before the main verb confuses them.</p><p>Most of the underused CFG rules are those that introduce rather complex structures. For exam-Overuse Score Underuse Score PP → ϕ NP 9.0 NP → NP , NP , -4.6 S → XP VP .</p><p>7.2 S → NP NP -2.7 PP → IN IN S 6.7 S → NP VP . " -2.6 S → XP .</p><p>6.6 ADVP → NP RBR -2.5 VP → ϕ ADJP 6.5 S → S , NP VP .</p><formula xml:id="formula_5">-2.4 VP → ϕ NP 6.3 NP → NP , SBAR -2.4 SBAR → IN NN TO S 6.1 SBAR → WHPP S -2.3 PP → ϕ S 6.1 VP → VBD SBAR -2.2 S → ADVP NP ADVP VP .</formula><p>5.8 S → NP PRN VP . -2.2 PP → IN TO NP 5.7 S → PP , NP VP . " -2.1 <ref type="table">Table 4</ref>: Characteristic CFG rules.</p><p>ple, the eighth rule VP → VBD SBAR implies a structure such as He thought that · · · . The under- used CFG rules are a piece of the evidence that this population of learners of English cannot use such complex structures as fluently as native speakers do. Considering this, it will be useful feedback to provide them with the rules (transformed into interpretable forms). As in this example, phrase structure annotation should be useful not only for second language acquisition research but also for language learning assistance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Parsing Performance Evaluation</head><p>We tested the following two state-of-the-art parsers on the annotated data: Stanford Statistical Natural Language Parser (ver.2.0.3) ( <ref type="bibr" target="#b6">de Marneffe et al., 2006</ref>) and Charniak-Johnson parser <ref type="bibr" target="#b3">(Charniak and Johnson, 2005</ref>). We gave the tokenized sentences to them as their inputs. We used again the EVALB tool with the Collins (1997)'s evalua- tion parameter. <ref type="table" target="#tab_2">Table 5</ref> shows the results. To our surprise, both parsers perform very well on the learner corpora despite the fact that it contains a number of gram- matical errors and also syntactic tags that are not defined in PTB-II. Their performance is compara- ble to, or even better than, that on the Penn Tree- bank (reported in <ref type="bibr" target="#b20">Petrov (2010)</ref>).</p><p>To achieve further improvement, we augmented the Charniak-Johnson parser with the learner data. We first retrained its parser model using the 2- 21 sections of Penn Treebank Wall Street Journal (hereafter, WSJ) as training data and its 24 sec- tion as development data, following the settings shown in <ref type="bibr" target="#b3">Charniak and Johnson (2005</ref>  each of which approximately consisted of 61 es- says, used one sixth as test data, another one sixth as development data instead of the 24 section, and retrained the parser model using the development data and the training data consisting of the remain- ing four-sixths part of the learner data and the 2-21 sections of WSJ. We also conducted experiments where we copied the four sixths of the learner data n times (1 ≤ n ≤ 50) and added them to the train- ing data to increase its weight in retraining. <ref type="figure">Figure 1</ref> shows the results. The simple addition of the learner data (n = 1) already outperforms the parser trained only on the 2-21 sections of WSJ (n = 0) in both recall and precision, achieving an F -measure of 0.866 and a complete match rate of 0.515. The augmented parser model particu- larly works well on recognizing erroneous frag- ments in the learner data; F -measure improved to 0.796 (n = 1) from 0.683 (n = 0) in the sentences containing fragments (i.e., FRAG) (46 out of the 111 sentences that were originally er- roneously parsed made even a complete match). It was also robust against spelling errors. The perfor- mance further improves as the weight n increases (up to F = 0.878 when n = 24), which shows the effectiveness of using learner corpus data as train- ing data. ICNALE (classified by country code 9 ). In most of the sub-corpora, the parser achieves an F -measure of 0.800 or better. By contrast, it performs much worse on the Korean sub-corpus. The major rea- son for this is that it contains a number of word order errors (i.e., XP-ORD); to be precise, 27 in- stances compared to zero to two instances in the other sub-corpora. Similarly, FRAG is a source of parsing errors in the Thai sub-corpus. We need further investigation to determine whether the dif- ferences in parsing performance are due to the writers' mother tongue or other factors (e.g., pro- ficiency).</p><p>We can summarize the findings as follows: (1) the state-of-the-art phrase structure parsers for na- tive English are effective even in parsing learner English; (2) they are successfully augmented by learner corpus data; (3) the evaluation results sup- port the previous report <ref type="bibr" target="#b24">(Tetreault et al., 2010</ref>) that they are effective in extracting parse features for grammatical error correction (and probably for re- lated NLP tasks such as automated essay scoring); (4) however, performance may vary depending on the writer's mother tongue and/or other factors, which we need further investigation to confirm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>This paper explored phrase structure annotation and parsing specially designed for learner English. Sect. 3 showed the usefulness of our phrase struc- ture annotation scheme and the learner corpora annotated using it. The annotation results exhib- ited high consistency. They also shed light on (at least, part of) the characteristics of the learners of <ref type="bibr">9</ref> Ideally, it would be better to use sub-corpora classified by their mother tongues. Unfortunately, however, only coun- try codes are provided in ICNALE. English. Sect. 4 further reported on the perfor- mance of the two state-of-the-art parsers on the annotated corpus, suggesting that they are accu- rate for providing NLP applications with phrase structures in learner English. All these findings support the effectiveness of our phrase structure annotation scheme for learner English. It would be much more difficult to conduct similar analy- ses and investigations without the phrase structure annotation scheme and a learner corpus annotated based on it. The annotation guidelines, the anno- tated data, and the parsing model for learner En- glish created in this work are now available to the public <ref type="bibr">10</ref> .</p><p>In our future work, we will evaluate parsing performance on other learner corpora such as ICLE ( <ref type="bibr" target="#b13">Granger et al., 2009</ref>) consisting of a wide variety of learner Englishes. We will also extend phrase structure annotation, especially working on function tags.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>An insertion type error is an error where an extra word is used incorrectly. For example, the word about is an extra word in the sentence *She dis- cussed about it.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>see</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>However, the relations of the S to the rest of the constituents are not clear. Here, we use the XP- ORD tag to combine the S with the rest together:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 showsFigure 1 :</head><label>21</label><figDesc>Figure 2 shows the parsing performance of the Charniak-Johnson parser in each sub-corpus of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>HKG</head><label></label><figDesc>Figure 2: Parsing performance in each sub-corpus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 2 : Statistics on annotated learner corpora.</head><label>2</label><figDesc></figDesc><table>Set 
R 
P 
F 
CMR CCM 
Development 0.981 0.981 0.981 0.913 0.995 
Test 
0.919 0.927 0.928 0.549 0.982 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>). We then added the learner corpora to the training data using six-fold cross validation. We split it into six parts,</figDesc><table>Parser 
R 
P 
F 
CMR 
Stanford 
0.812 0.832 0.822 0.398 
Charniak-Johnson 0.845 0.865 0.855 0.465 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 5 :</head><label>5</label><figDesc>Parsing performance on learner English.</figDesc><table></table></figure>

			<note place="foot" n="1"> We are not arguing that phrase structure annotation is better than dependency annotation; they both have their own advantages, and thus both should be explored.</note>

			<note place="foot" n="2"> We will most likely include them in a future version.</note>

			<note place="foot" n="3"> This means that spelling and morphological errors are not directly coded in our annotation scheme as in He/PRP has/VBZ a/DT books/NNS.</note>

			<note place="foot" n="4"> The details about the proficiency levels are available in http://language.sakura.ne.jp/icnale/ about.html 5 The annotators, whose mother tongue is Japanese, have a good command of English. They have engaged in corpus annotation including phrase structure annotation for around 20 years.</note>

			<note place="foot" n="6"> http://nlp.cs.nyu.edu/evalb/ 7 https://github.com/arnsholt/ syn-agreement</note>

			<note place="foot" n="8"> To confirm that the extracted characteristics are not influenced by the differences in the domains of the two corpora, we also compared the learner data with the native speaker sub-corpus in ICNALE that is in the same domain. It turned out that the extracted CFG rules, were very similar to those shown in Table 4.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Shin'ichiro Ishikawa, who created the ICNALE corpus, for providing us with the data and Arne Skaerholt for the assistance to run the syn-agreement tool. We would also like to thank the three anonymous reviewers for their valuable feedback. This work was partly sup-ported by Grant-in-Aid for Young Scientists (B) Grant Number JP26750091. <ref type="bibr">10</ref> We released the Konan-JIEM corpus with phrase struc-tures on March 2015, which is available at http://www. gsk.or.jp/en/catalog/gsk2015-a/. We anno-tated the existing ICNALE, which was created by Dr. Ishikawa and his colleagues, with phrase structures. We re-leased the data on Jun 2016, which is available at http: //language.sakura.ne.jp/icnale/</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Bracketing guidelines for Treebank II-style Penn treebank project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Bies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Macintyre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Self-training for parsing learner text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoife</forename><surname>Cahill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binod</forename><surname>Gyawali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">V</forename><surname>Bruno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 1st Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages</title>
		<meeting>of 1st Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="66" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Parsing learner text: to Shoehorn or not to Shoehorn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoife</forename><surname>Cahill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 9th Linguistic Annotation Workshop</title>
		<meeting>of 9th Linguistic Annotation Workshop</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="144" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Coarseto-fine N-best parsing and MaxEnt discriminative reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 43rd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>of 43rd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Three generative, lexicalised models for statistical parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 35th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>of 35th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="16" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Helping our own: The HOO 2011 pilot shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kilgarriff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 13th European Workshop on Natural Language Generation</title>
		<meeting>of 13th European Workshop on Natural Language Generation</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="242" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generating typed dependency parses from phrase structure parses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 5th International Conference on Language Resources and Evaluation</title>
		<meeting>of 5th International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="449" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards interlanguage POS annotation for effective learner corpora in SLA and FLT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><surname>Díaz-Negrillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Detmar</forename><surname>Meurers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salvador</forename><surname>Valera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Wunsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language Forum</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="139" to="154" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dependency annotation for learner corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marwa</forename><surname>Ragheb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 8th Workshop on Treebanks and Linguistic Theories</title>
		<meeting>of 8th Workshop on Treebanks and Linguistic Theories</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="59" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">GenERRate: Generating errors for use in grammatical error detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Øistein</forename><forename type="middle">E</forename><surname>Andersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 4th Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>of 4th Workshop on Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="82" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Parsing ungrammatical input: An evaluation procedure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Foster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 4th International Conference on Language Resources and Evaluation</title>
		<meeting>of 4th International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="2039" to="2042" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Treebanks gone bad: generating a treebank of ungrammatical English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Foster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Analytics for Noisy Unstructured Data</title>
		<imprint>
			<date type="published" when="2007-01" />
			<biblScope unit="page" from="39" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Treebanks gone bad: Parser evaluation and retraining using a treebank of ungrammatical sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Foster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Document Analysis and Recognition</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="129" to="145" />
			<date type="published" when="2007-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">International Corpus of Learner English v2. Presses universitaires de Louvain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylviane</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Estelle</forename><surname>Dagneaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanny</forename><surname>Meunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magali</forename><surname>Paquot</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<pubPlace>Louvain</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A new horizon in learner corpus studies: The aim of the ICNALE project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinichiro</forename><surname>Ishikawa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3" to="11" />
			<pubPlace>Glasgow</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Strathclyde Publishing</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The NICT JLE Corpus: Exploiting the language learners&apos; speech database for research and education</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emi</forename><surname>Izumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toyomi</forename><surname>Saiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thepchai</forename><surname>Supnithi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyotaka</forename><surname>Uchimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hitoshi</forename><surname>Isahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of The Computer, the Internet and Management</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="119" to="125" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Annotating and querying a treebank of suboptimal structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Kepser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilona</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Sternefeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 3rd Workshop on Treebanks and Linguistic Theories</title>
		<meeting>of 3rd Workshop on Treebanks and Linguistic Theories</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="63" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Creating a manually error-tagged and shallow-parsed learner corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryo</forename><surname>Nagata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Whittaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vera</forename><surname>Sheinman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>of 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1210" to="1219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The CoNLL2013 shared task on grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hwee Tou Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Siew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Hadiwinoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 17th Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>17th Conference on Computational Natural Language Learning: Shared Task</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Evaluating dependency parsing performance on German learner language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niels</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramon</forename><surname>Ziai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 9th International Workshop on Treebanks and Linguistic Theories</title>
		<meeting>of 9th International Workshop on Treebanks and Linguistic Theories</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="175" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Products of random latent variable grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Defining syntax for learner language annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marwa</forename><surname>Ragheb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Dickinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 24th International Conference on Computational Linguistics</title>
		<meeting>of 24th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="965" to="974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Interannotator agreement for dependency annotation of learner language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marwa</forename><surname>Ragheb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Dickinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 8th Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>of 8th Workshop on Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="169" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A chance-corrected measure of inter-annotator agreement for syntax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arne</forename><surname>Skjaerholt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>of 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="934" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Using parse features for preposition selection and error detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 48nd Annual Meeting of the Association for Computational Linguistics Short Papers</title>
		<meeting>of 48nd Annual Meeting of the Association for Computational Linguistics Short Papers</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="353" to="358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Bracketing biomedical text: An addendum to Penn Treebank II guidelines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Warner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arrick</forename><surname>Lanfranchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>O&amp;apos;gorman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Regan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
