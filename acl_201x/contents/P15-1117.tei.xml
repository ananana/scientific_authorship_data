<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:08+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Neural Probabilistic Structured-Prediction Model for Transition-Based Dependency Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">†State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">‡Singapore University of Technology and Design</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Neural Probabilistic Structured-Prediction Model for Transition-Based Dependency Parsing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1213" to="1222"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Neural probabilistic parsers are attractive for their capability of automatic feature combination and small data sizes. A transition-based greedy neural parser has given better accuracies over its linear counterpart. We propose a neural probabilistic structured-prediction model for transition-based dependency parsing, which integrates search and learning. Beam search is used for decoding, and contrastive learning is performed for maximizing the sentence-level log-likelihood. In standard Penn Treebank experiments, the structured neural parser achieves a 1.8% accuracy improvement upon a competitive greedy neural parser baseline, giving performance comparable to the best linear parser.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Transition-based methods have given competitive accuracies and efficiencies for dependency pars- ing ( <ref type="bibr" target="#b30">Yamada and Matsumoto, 2003;</ref><ref type="bibr" target="#b22">Nivre and Scholz, 2004;</ref><ref type="bibr" target="#b31">Zhang and Clark, 2008;</ref><ref type="bibr" target="#b13">Huang and Sagae, 2010;</ref><ref type="bibr" target="#b33">Zhang and Nivre, 2011;</ref><ref type="bibr" target="#b10">Goldberg and Nivre, 2013)</ref>. These parsers construct depen- dency trees by using a sequence of transition ac- tions, such as SHIFT and REDUCE, over input sen- tences. High accuracies are achieved by using a linear model and millions of binary indicator fea- tures. Recently, <ref type="bibr" target="#b1">Chen and Manning (2014)</ref> pro- pose an alternative dependency parser using a neu- ral network, which represents atomic features as dense vectors, and obtains feature combination au- tomatically other than devising high-order features manually.</p><p>The greedy neural parser of <ref type="bibr" target="#b1">Chen and Manning (2014)</ref> gives higher accuracies compared to * Work done while the first author was visiting SUTD. the greedy linear MaltParser ( <ref type="bibr" target="#b22">Nivre and Scholz, 2004</ref>), but lags behind state-of-the-art linear sys- tems with sparse features ( <ref type="bibr" target="#b33">Zhang and Nivre, 2011)</ref>, which adopt global learning and beam search decoding ( <ref type="bibr" target="#b34">Zhang and Nivre, 2012</ref>). The key difference is that <ref type="bibr" target="#b1">Chen and Manning (2014)</ref> is a local classifier that greedily optimizes each ac- tion. In contrast, Zhang and Nivre (2011) leverage a structured-prediction model to optimize whole sequences of actions, which correspond to tree structures.</p><p>In this paper, we propose a novel framework for structured neural probabilistic dependency pars- ing, which maximizes the likelihood of action se- quences instead of individual actions. Follow- ing <ref type="bibr" target="#b32">Zhang and Clark (2011)</ref>, beam search is ap- plied to decoding, and global structured learn- ing is integrated with beam search using early- update ( <ref type="bibr" target="#b4">Collins and Roark, 2004</ref>). Designing such a framework is challenging for two main reasons:</p><p>First, applying global structured learning to transition-based neural parsing is non-trivial. A direct adaptation of the framework of <ref type="bibr" target="#b32">Zhang and Clark (2011)</ref> under the neural probabilistic model setting does not yield good results. The main rea- son is that the parameter space of a neural network is much denser compared to that of a linear model such as the structured perceptron <ref type="bibr" target="#b5">(Collins, 2002</ref>). Due to the dense parameter space, for neural mod- els, the scores of actions in a sequence are rela- tively more dependent than that in the linear mod- els. As a result, the log probability of an action se- quence can not be modeled just as the sum of log probabilities of each action in the sequence, which is the case of structured linear model. We address the challenge by using a softmax function to di- rectly model the distribution of action sequences.</p><p>Second, for the structured model above, maximum-likelihood training is computationally intractable, requiring summing over all possible action sequences, which is difficult for transition-based parsing. To address this challenge, we take a contrastive learning approach <ref type="bibr" target="#b12">(Hinton, 2002;</ref><ref type="bibr" target="#b15">LeCun and Huang, 2005;</ref><ref type="bibr" target="#b17">Liang and Jordan, 2008;</ref><ref type="bibr" target="#b28">Vickrey et al., 2010;</ref><ref type="bibr" target="#b18">Liu and Sun, 2014)</ref>. Using the sum of log probabilities over the action se- quences in the beam to approximate that over all possible action sequences.</p><p>In standard PennTreebank ( <ref type="bibr" target="#b20">Marcus et al., 1993</ref>) evaluations, our parser achieves a significant accu- racy improvement (+1.8%) over the greedy neu- ral parser of <ref type="bibr" target="#b1">Chen and Manning (2014)</ref>, and gives the best reported accuracy by shift-reduce parsers. The incremental neural probabilistic framework with global contrastive learning and beam search could be used in other structured prediction tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Arc-standard Parsing</head><p>Transition-based dependency parsers scan an in- put sentence from left to right, and perform a se- quence of transition actions to predict its parse tree <ref type="bibr" target="#b24">(Nivre, 2008)</ref>. In this paper, we employ the arc-standard system ( <ref type="bibr" target="#b23">Nivre et al., 2007)</ref>, which maintains partially-constructed outputs us- ing a stack, and orders the incoming words in the input sentence in a queue. Parsing starts with an empty stack and a queue consisting of the whole input sentence. At each step, a transition action is taken to consume the input and construct the output. The process repeats until the input queue is empty and stack contains only one dependency tree.</p><p>Formally, a parsing state is denoted as ⟨j, S, L⟩, where S is a stack of subtrees [. . . s 2 , s 1 , s 0 ], j is the head of the queue (i.e. [ q 0 = w j , q 1 = w j+1 · · · ]), and L is a set of dependency arcs. At each step, the parser chooses one of the following actions:</p><p>• SHIFT: move the front word w j from the queue onto the stacks.</p><p>• LEFT-ARC(l): add an arc with label l between the top two trees on the stack (s 1 ← s 0 ), and remove s 1 from the stack.</p><p>• RIGHT-ARC(l): add an arc with label l be- tween the top two trees on the stack (s 1 → s 0 ), and remove s 0 from the stack.</p><p>The arc-standard parser can be summarized as the deductive system in <ref type="figure" target="#fig_0">Figure 1</ref>, where k denotes input : w 0 . . . w n−1 axiom : 0 : ⟨0, ϕ, ϕ, 0⟩ the current parsing step. For a sentence with size n, parsing stops after performing exactly 2n − 1 actions.</p><formula xml:id="formula_0">goal : 2n − 1 : ⟨n, s 0 , L⟩ SHIFT k : ⟨j, S, L⟩ k + 1 : ⟨j + 1, S|w j , L⟩ LEFT-ARC(l) k : ⟨j, S|s 1 |s 0 , L⟩ k + 1 : ⟨j, S|s 0 , L ∪ {s 1 l ← − s 0 }⟩ RIGHT-ARC(l) k : ⟨j, S|s 1 |s 0 , L⟩ k + 1 : ⟨j, S|s 1 , L ∪ {s 1 l − → s 0 }⟩</formula><p>MaltParser uses an SVM classifier for deter- ministic arc-standard parsing. At each step, Malt- Parser generates a set of successor states according to the current state, and deterministically selects the highest-scored one as the next state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Global Learning and Beam Search</head><p>The drawback of deterministic parsing is error propagation. An incorrect action will have a nega- tive influence to its subsequent actions, leading to an incorrect output parse tree.</p><p>To address this issue, global learning and beam search ( <ref type="bibr" target="#b32">Zhang and Clark, 2011;</ref><ref type="bibr" target="#b0">Bohnet and Nivre, 2012;</ref><ref type="bibr" target="#b3">Choi and McCallum, 2013)</ref> are used. Given an input x, the goal of decoding is to find the highest-scored action sequence globally.</p><formula xml:id="formula_1">y = arg max y ′ ∈GEN(x) score(y ′ )<label>(1)</label></formula><p>Where GEN(x) denotes all possible action se- quences on x, which correspond to all possible parse trees. The score of an action sequence y is:</p><formula xml:id="formula_2">score(y) = ∑ a∈y θ · Φ(a)<label>(2)</label></formula><p>Here a is an action in the action sequence y, Φ is a feature function for a, and θ is the parameter vector of the linear model. The score of an action sequence is the linear sum of the scores of each action. During training, action sequence scores are globally learned.</p><p>The parser of <ref type="bibr" target="#b33">Zhang and Nivre (2011)</ref> is devel- oped using this framework. The structured percep- tron <ref type="bibr" target="#b5">(Collins, 2002</ref>) with early update ( <ref type="bibr" target="#b4">Collins and Roark, 2004</ref>) is applied for training. By utilizing rich manual features, it gives state-of-the-art accu- racies in standard Penn Treebank evaluation. We take this method as one baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Greedy Neural Network Model</head><p>Chen and Manning (2014) build a greedy neural arc-standard parser. The model can be regarded as an alternative implementation of MaltParser, using a feedforward neural network to replace the SVM classifier for deterministic parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Model</head><p>The greedy neural model extracts n atomic fea- tures from a parsing state, which consists of words, POS-tags and dependency labels from the stack ans queue. Embeddings are used to rep- resent word, POS and dependency label atomic features. Each embedding is represented as a d- dimensional vector e i ∈ R. Therefore, the full embedding matrix is E ∈ R d×V , where V is the number of distinct features. A projection layer is used to concatenate the n input embeddings into a vector x = [e 1 ; e 2 . . . e n ], where x ∈ R d·n . The purpose of this layer is to fine-tune the embedding features. Then x is mapped to a d h -dimensional hidden layer by a mapping matrix W 1 ∈ R d h ×d·n and a cube activation function:</p><formula xml:id="formula_3">h = (W 1 x + b 1 ) 3 (3)</formula><p>Finally, h is mapped into a softmax output layer for modeling the probabilistic distribution of can- didate shift-reduce actions:</p><formula xml:id="formula_4">p =sof tmax(o)<label>(4)</label></formula><p>where</p><formula xml:id="formula_5">o = W 2 h (5)</formula><p>W 2 ∈ R do×d h and d o is the number of shift-reduce actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Features</head><p>One advantage of <ref type="bibr" target="#b1">Chen and Manning (2014)</ref> is that the neural network parser achieves feature combination automatically. Their atomic features are defined by following <ref type="bibr" target="#b33">Zhang and Nivre (2011)</ref>. As shown in <ref type="table" target="#tab_0">Table 1</ref>, the features are categorized into three types: F w , F t , F l , which represents word features, POS-tag features and dependency label features, respectively. For example, s 0 w and q 0 w represent the first word on the stack and queue, respectively; lc 1 (s 0 )w and rc 1 (s 0 )w represent the leftmost and rightmost child of s 0 , respectively. Similarly, lc 1 (s 0 )t and lc 1 (s 0 )l represent the POS-tag and dependency label of the leftmost child of s 0 , re- spectively. <ref type="bibr" target="#b1">Chen and Manning (2014)</ref> find that the cube activation function in Equation <ref type="formula">(3)</ref> is highly ef- fective in capturing feature interaction, which is a novel contribution of their work. The cube func- tion achieves linear combination between atomic word, POS and label features via the product of three element combinations. Empirically, it works better compared to a sigmoid activation function.</p><formula xml:id="formula_6">Templates F w s0w, s2w, q0w, q1w, q2w, lc1(s0)w, lc2(s0)w s1w, rc2(s0)w, lc1(s1)w, lc2(s1)w, rc2(s1)w rc1(s0)w, rc1(s1)w, lc1(lc1(s0))w, lc1(lc1(s1))w rc1(rc1(s1))w, rc1(rc1(s0))w F t s0t, q0t, q1t, q2t, rc1(s0)t, lc1(s0)t, lc2(s0)t s1t, s2t, lc1(s1)t, lc2(s1)t, rc1(s1)t, rc2(s0)t rc2(s1)t, lc1(lc1(s0))t, lc1(lc1(s1))t rc1(rc1(s0))t, rc1(rc1(s1))t F l rc1(s0)l, lc1(s0)l, lc2(s0)l, lc1(s1)l, lc2(s1)l rc1(s1)l, rc2(s0)l, rc2(s1)l, lc1(lc1(s0))l lc1(lc1(s1))l, rc1(rc1(s0))l, rc1(rc1(s1))l</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Training</head><p>Given a set of training examples, the training ob- jective of the greedy neural parser is to minimize the cross-entropy loss, plus a l 2 -regularization term:</p><formula xml:id="formula_7">L(θ) = − ∑ i∈A log p i + λ 2 ∥ θ ∥ 2 (6)</formula><p>θ is the set of all parameters (i.e. W 1 , W 2 , b, E), and A is the set of all gold actions in the train- ing data. <ref type="bibr">AdaGrad (Duchi et al., 2011</ref>) with mini- batch is adopted for optimization. We take the greedy neural parser of <ref type="bibr" target="#b1">Chen and Manning (2014)</ref> as a second baseline.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Neural Probabilistic Ranking</head><p>Given the baseline system in Section 2.2, the most intuitive structured neural dependency parser is to replace the linear scoring model with a neu- ral probabilistic model. Following Equation 1, the score of an action sequence y, which corresponds to its log probability, is sum of log probability scores of each action in the sequence.</p><formula xml:id="formula_8">s(y) = ∑ a∈y log p a<label>(7)</label></formula><p>where p a is defined by the baseline neural model of Section 2.3 (Equation 4). The training objec- tive is to maximize the score margin between the gold action sequences (y g ) and these of incorrectly predicated action sequences (y p ):</p><formula xml:id="formula_9">L(θ) = max(0, δ−s(y g )+s(y p ))+ λ 2 ∥ θ ∥ 2 (8)</formula><p>With this ranking model, beam search and early-update are used. Given a training instance, the negative example is the incorrectly predicted output with largest score ( <ref type="bibr" target="#b33">Zhang and Nivre, 2011</ref>).</p><p>However, we find that the ranking model works poorly. One explanation is that the actions in a sequence is probabilistically dependent on each other, and therefore using the total log probabil- ities of each action to compute the log probabil- ity of an action sequence (Equation 7) is inaccu- rate. Linear models do not suffer from this prob- lem, because the parameter space of linear models is much more sparse than that of neural models. For neural networks, the dense parameter space is shared by all the actions in a sequence. Increasing the likelihood of a gold action may also change the likelihood of incorrect actions through the shared parameters. As a result, increasing the scores of a gold action sequence and simultaneously reducing the scores of an incorrect action sequence does not work well for neural models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sentence-Level Log-Likelihood</head><p>To overcome the above limitation, we try to di- rectly model the probabilistic distribution of whole action sequences. Given a sentence x and neural networks parameter θ, the probability of the action sequence y i is given by the softmax function:</p><formula xml:id="formula_10">p(y i | x, θ) = e f (x, θ) i ∑ y j ∈GEN(x) e f (x, θ) j<label>(9)</label></formula><p>where</p><formula xml:id="formula_11">f (x, θ) i = ∑ a k ∈y i o(x, y i , k, a k )<label>(10)</label></formula><p>Here GEN(s) is the set of all possible valid ac- tion sequences for a sentence x; o(x, y i , k, a k ) denotes the neural network score for the action a k given x and y i . We use the same sub net- work as <ref type="bibr" target="#b1">Chen and Manning (2014)</ref> to calculate o(x, y i , k, a k ) (Equation 5). The same features in <ref type="table" target="#tab_0">Table 1</ref> are used.</p><p>Given the training data as (X, Y ), our train- ing objective is to minimize the negative log- likelihood:</p><formula xml:id="formula_12">L(θ) = − ∑ (x i , y i )∈(X,Y ) log p(y i | x i , θ)<label>(11)</label></formula><formula xml:id="formula_13">= − ∑ (x i , y i )∈(X,Y ) log e f (x i ,θ) i Z(x i , θ)<label>(12)</label></formula><p>= ∑</p><formula xml:id="formula_14">(x i , y i )∈(X,Y ) log Z(x i , θ) − f (x i , θ) i<label>(13)</label></formula><p>where</p><formula xml:id="formula_15">Z(x, θ) = ∑ y j ∈GEN(x) e f (x, θ) j<label>(14)</label></formula><p>Here, Z(x, θ) is called the partition function. Following Chen and Manning(2014), we apply l 2 - regularization for training.</p><p>For optimization, we need to compute gradients for L(θ), which includes gradients of exponential numbers of negative examples in partition func- tion Z(x, θ). However, beam search is used for transition-based parsing, and no efficient optimal dynamic program is available to estimate Z(x, θ) accurately. We adopt a novel contrastive learning approach to approximately compute Z(x, θ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Contrastive Learning</head><p>As an alternative to maximize the likelihood on some observed data, contrastive learning <ref type="bibr" target="#b12">(Hinton, 2002;</ref><ref type="bibr" target="#b15">LeCun and Huang, 2005;</ref><ref type="bibr" target="#b17">Liang and Jordan, 2008;</ref><ref type="bibr" target="#b28">Vickrey et al., 2010;</ref><ref type="bibr" target="#b18">Liu and Sun, 2014</ref>) is an approach that assigns higher probabilities to ob- served data and lower probabilities to noisy data.</p><p>We adopt the contrastive learning approach, as- signing higher probabilities to the gold action se- quence compared to incorrect action sequences in the beam. Intuitively, this method only penalizes incorrect action sequences with high probabilities. Our new training objective is approximated as:</p><formula xml:id="formula_16">L ′ (θ) = − ∑ (x i , y i )∈(X,Y ) log p ′ (y i | x i , θ)<label>(15)</label></formula><formula xml:id="formula_17">= − ∑ (x i , y i )∈(X,Y ) log e f (x i ,θ) i Z ′ (x i , θ)<label>(16)</label></formula><formula xml:id="formula_18">= ∑ (x i , y i )∈(X,Y ) log Z ′ (x i , θ) − f (x i , θ) i<label>(17)</label></formula><p>where</p><formula xml:id="formula_19">Z ′ (x, θ) = ∑ y j ∈BEAM(x) e f (x, θ) j<label>(18)</label></formula><p>p ′ (y i | x, θ) is the relative probability of the ac- tion sequence y i , computed over only the action sequences in the beam. Z ′ (x, θ) is the contrastive approximation of Z(x, θ). BEAM(x) returns the predicated action sequences in the beam and the gold action sequence.</p><p>We assume that the probability mass concen- trates on a relatively small number of action se- quences, which allows the use of a limited num- ber of probable sequences to approximate the full set of action sequences. The concentration may be enlarged dramatically with an exponential activa- tion function of the neural network (i.e. a &gt; b ⇒ e a ≫ e b ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">The Neural Probabilistic Structured-Prediction Framework</head><p>We follow <ref type="bibr" target="#b32">Zhang and Clark (2011)</ref> to integrate search and learning. Our search and learning</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Training Algorithm for Struc- tured Neural Parsing</head><p>Input: training examples (X, Y) Output: θ θ ← pretrained embedding for i ← 1 to N do x, y = RANDOMSAMPLE(X, Y) δ = 0 foreach x j , y j ∈ x, y do beam = ϕ goldState = null terminate = false beamGold = true while beamGold and not terminate do</p><formula xml:id="formula_20">beam = DECODE(beam, x j , y j ) goldState = GOLDMOVE(goldState, x j , y j ) if not ISGOLD(beam) then beamGold = false if ITEMSCOMPLETE(beam) then terminate = true; δ = δ + UPDATE(goldState, beam) θ = θ + delta</formula><p>framework for dependency parsing is shown as Algorithm 1. In every training iteration i, we randomly sample the training instances, and per- form online learning with early update ( <ref type="bibr" target="#b4">Collins and Roark, 2004</ref>). In particular, given a training example, we use beam-search to decode the sen- tence. At any step, if the gold action sequence falls out of the beam, we take all the incorrect action sequences in the beam as negative examples, and the current gold sequence as a positive example for parameter update, using the training algorithm of Section 3.3. AdaGrad algorithm <ref type="bibr" target="#b8">(Duchi et al., 2011</ref>) with mini-batch is adopted for optimization.</p><p>In this way, the distribution of ot only full ac- tion sequences (i.e. complete parse trees), but also partial action sequences (i.e. partial outputs) are modeled, which makes training more challenging. The advantage of early update is that training is used to guide search, minimizing search errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Set-up</head><p>Our experiments are performed using the English Penn Treebank (PTB; <ref type="bibr" target="#b20">Marcus et al., (1993)</ref>). We follow the standard splits of PTB3, using sections 2-21 for training, section 22 for development test- ing and section 23 for final testing. For compar- ison with previous work, we use Penn2Malt 1 to convert constituent trees to dependency trees. We use the POS-tagger of <ref type="bibr" target="#b5">Collins (2002)</ref> to assign POS automatically. 10-fold jackknifing is per- formed for tagging the training data.</p><p>We follow <ref type="bibr" target="#b1">Chen and Manning (2014)</ref>, and use the set of pre-trained word embeddings 2 from Collobert et al. <ref type="formula" target="#formula_1">(2011)</ref> with a dictionary size of 13,000. The word embeddings were trained on the entire English Wikipedia, which contains about 631 million words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">WSJ Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Development experiments</head><p>We set the following hyper-parameters according to the baseline greedy neural parser <ref type="bibr" target="#b1">(Chen and Manning, 2014)</ref>: embedding size d = 50, hidden layer size d h = 200, regularization parameter λ = 10 −8 , initial learning rate of Adagrad α = 0.01. For the structured neural parser, beam size and mini-batch size are important to the parsing per- formance. We tune them on the development set.</p><p>Beam size. Beam search enlarges the search space. More importantly, the larger the beam is, the more accurate our training algorithm is. the Contrastive learning approximates the exact prob- abilities over exponential many action sequences by computing the relative probabilities over action sequences in the beam (Equation 18). Therefore, the larger the beam is, the more accurate the rela- tive probability is.</p><p>The first column of <ref type="table" target="#tab_4">Table 3</ref> shows the accura- cies of the structured neural parser on the devel- opment set with different beam sizes, which im- proves as the beam size increases. We set the final beam size as 100 according to the accuracies on development set.   <ref type="table">Table 4</ref>: Comparison between sentence-level log- likelihood and ranking model. <ref type="bibr" target="#b1">Chen and Manning (2014)</ref> with beam search de- coding. The score of a whole action sequence is computed by the sum of log action probabili- ties (Equation 7). As shown in the second col- umn of <ref type="table" target="#tab_4">Table 3</ref>, beam search can improve pars- ing slightly. When the beam size increases beyond 16, however, accuracy improvements stop. In con- trast, by integrating beam search and global learn- ing, our parsing performance benefits from large beam sizes much more significantly. With a beam size as 16, the structured neural parser gives an accuracy close to that of baseline greedy parser 3 . When the beam size is 100, the structured neural parser outperforms baseline by 1.6%. <ref type="bibr" target="#b34">Zhang and Nivre (2012)</ref> find that global learn- ing and beam search should be used jointly for improving parsing using a linear transition-based model. In particular, increasing the beam size, the accuracy of ZPar ( <ref type="bibr" target="#b33">Zhang and Nivre, 2011</ref>) in- creases significantly, but that of MaltParser does not. For structured neural parsing, our finding is similar: integrating search and learning is much more effective than using beam search only in de- coding.</p><p>Our results in <ref type="table" target="#tab_4">Table 3</ref> are obtained by using the same beam sizes for both training and testing. Zhang and Nivre (2012) also find that for their lin- ear model, the best results are achieved by using the same beam sizes during training and testing. We find that this observation does not apply to our neural parser. In our case, a large training beam al- ways leads to better results. This is likely because a large beam improves contrastive learning. As a result, our training beam size is set to 100 for the final test.</p><p>Batch size. Parsing performance using neural networks is highly sensitive to the batch size of training. In greedy neural parsing <ref type="bibr" target="#b1">(Chen and Manning, 2014</ref>), the accuracy on the development data improves from 85% to 91% by setting the batch size to 10 and 100000, respectively. In structured neural parsing, we fix the beam size as 100 and draw the accuracies on the development set by the training iteration.</p><p>As shown in <ref type="figure" target="#fig_2">Figure 2</ref>, in 5000 training itera- tions, the parsing accuracies improve as the itera- tion grows, yet different batch sizes result in dif- ferent convergence accuracies. With a batch size of 5000, the parsing accuracy is about 25% higher than with a batch size of 1 (i.e. SGD). For the re- maining experiments, we set batch size to 5000, which achieves the best accuracies on develop- ment testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Sentence-level maximum likelihood vs. ranking model</head><p>We compare parsing accuracies of the sentence- level log-likelihood + beam contrastive learning (Section 3.2), and the structured neural parser with probabilistic ranking (Section 3.1). As shown in <ref type="table">Table 4</ref>, performance of global learning with ranking model is weaker than the baseline greedy System UAS LAS Speed baseline greedy parser 91.47 90.43 0.001 <ref type="bibr" target="#b13">Huang and Sagae (2010)</ref> 92.10 0.04 <ref type="bibr" target="#b33">Zhang and Nivre (2011)</ref> 92.90 91.80 0.03 <ref type="bibr" target="#b3">Choi and McCallum (2013)</ref> 92.96 91.93 0.009 <ref type="bibr" target="#b19">Ma et al. (2014)</ref> 93.06 Bohnet and Nivre <ref type="formula" target="#formula_1">(2012)</ref>   parser. In contrast, structured neural parsing with sentence-level log-likelihood and contrastive learning gives a 1.8% accuracy improvement upon the baseline greedy parser. As mentioned in Section 3.1, a likely reason for the poor performance of the structured neu- ral ranking model may be that, the likelihoods of action sequences are highly influenced by each other, due to the dense parameter space of neural networks. To maximize likelihood of gold action sequence, we need to decrease the likelihoods of more than one incorrect action sequences. <ref type="table" target="#tab_6">Table 5</ref> shows the results of our final parser and a line of transition-based parsers on the test set. Our structured neural parser achieves an accu- racy of 93.28%, 0.38% higher than <ref type="bibr" target="#b33">Zhang and Nivre (2011)</ref>, which employees millions of high- order binary indicator features in parsing. The model size of ZPar ( <ref type="bibr" target="#b33">Zhang and Nivre, 2011</ref>) is over 250 MB on disk. In contrast, the model size of our structured neural parser is only 25 MB. To our knowledge, the result is the best reported re- sult achieved by shift-reduce parsers on this data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Final Results</head><p>Bohnet and Nivre (2012) obtain an accuracy of 93.67%, which is higher than our parser. How- ever, their parser is a joint model of parsing and POS-tagging, and they use external data in pars- ing. We also list the result of , <ref type="bibr" target="#b14">Koo et al. (2008)</ref> and <ref type="bibr" target="#b27">Suzuki et al. (2009)</ref> in <ref type="table" target="#tab_6">Table 5</ref>, which make use of large-scale unanno- tated text to improve parsing accuracies. The input embeddings of our parser are also trained over large raw text, and in this perspective our model is correlated with the semi-supervised mod- els. However, because we fine-tune the word em- beddings in supervised training, the embeddings of in-vocabulary words become systematically dif- ferent from these of out-of-vocabulary words af- ter training, and the effect of pre-trained out-of- vocabulary embeddings become uncertain. In this sense, our model can also be regarded as an al- most fully supervised model. The same applies to the models of <ref type="bibr" target="#b1">Chen and Manning (2014)</ref>.</p><p>We also compare the speed of the structured neural parser on an Intel Core i7 3.40GHz CPU with 16GB RAM. The structured neural parser runs about as fast as <ref type="bibr" target="#b33">Zhang and Nivre (Zhang and Nivre, 2011)</ref> and <ref type="bibr" target="#b13">Huang and Sagae (Huang and Sagae, 2010)</ref>. The results show that our parser combines the benefits of structured models and neural probabilistic models, offering high accura- cies, fast speed and slim model size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Parsing with neural networks. A line of work has been proposed to explore the effect of neu- ral network models for constituent parsing <ref type="bibr" target="#b11">(Henderson, 2004;</ref><ref type="bibr" target="#b21">Mayberry III and Miikkulainen, 2005;</ref><ref type="bibr" target="#b7">Collobert, 2011;</ref><ref type="bibr" target="#b25">Socher et al., 2013;</ref><ref type="bibr" target="#b16">Legrand and Collobert, 2014</ref>). Performances of most of these methods are still well below the state-of-the-art, except for <ref type="bibr" target="#b25">Socher et al.(2013)</ref>, who propose a neural reranker based on a PCFG parser. For transition-based dependency parsing, Stenetorp (2013) applies a compositional vector method <ref type="bibr" target="#b25">(Socher et al., 2013), and</ref><ref type="bibr" target="#b1">Manning (2014)</ref> propose a feed-forward neural parser. The performances of these neural parsers lag be- hind the state-of-the-art.</p><p>More recently, <ref type="bibr" target="#b9">Dyer et al. (2015)</ref> propose a greedy transition-based dependency parser, using three stack LSTMs to represent the input, the stack of partial syntactic trees and the history of parse actions, respectively. By modeling more history, the parser gives significant better accuracies com- pared to the greedy neural parser of <ref type="bibr" target="#b1">Chen and Manning (2014)</ref>. They propose to use sentence-level log-likelihood to enhance a neural probabilistic model, which inspires our model. Sequence labeling is used for graph-based de- coding. Using the Viterbi algorithm, they can compute the exponential partition function in linear time without approximation. However, with a dynamic programming decoder, their sequence labeling model can only extract local features. In contrast, our integrated approximated search and learning framework allows rich global features. <ref type="bibr" target="#b29">Weiss et al. (2015)</ref> also propose a structured neural transition-based parser by adopting beam search and early updates. Their model is close in spirit to ours in performing structured predic- tion using a neural network. The main difference is that their structured neural parser uses a greedy parsing process for pre-training, and fine-tunes an additional perceptron layer consisting of the pre-trained hidden and output layers using struc- tured perceptron updates. Their structured neural parser achieves an accuracy of 93.36% on Stan- ford conversion of the PTB, which is significant higher than the baseline parser of <ref type="bibr" target="#b1">Chen and Manning (2014)</ref>. Their results are not directly compa- rable with ours due to different dependency con- versions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We built a structured neural dependency parsing model. Compared to the greedy neural parser of <ref type="bibr" target="#b1">Chen and Manning (2014)</ref>, our parser integrates beam search and global contrastive learning. In standard PTB evaluation, our parser achieved a 1.8% accuracy improvement over the parser of <ref type="bibr" target="#b1">Chen and Manning (2014)</ref>, which shows the effect of combining search and learning. To our knowl- edge, the structured neural parser is the first neural parser that outperforms the best linear shift-reduce dependency parsers. The structured neural proba- bilistic framework can be used in other incremen- tal structured prediction tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The deductive system for arc-standard dependency parsing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>local</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Parsing performance with different training batch sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Structured neural models.</head><label></label><figDesc>Collobert et al. (2011) presents a unified neural network architecture for various natural language pro- cessing (NLP) tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Feature templates. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Correlation between different parsers. 

or a structured-prediction alternative of Chen and 
Manning (2014). It combines the advantages of 
both Zhang and Nivre (2011) and Chen and Man-
ning (2014) over the greedy linear MaltParser. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>The effect of integrating search and learning. We also conduct experiments on the parser of</figDesc><table>Description 
UAS 
Baseline 
91.63 
structured greedy 
beam = 1 
74.90 
91.63 
beam = 4 
84.64 
91.92 
beam = 16 
91.53 
91.90 
beam = 64 
93.12 
91.84 
beam = 100 
93.23 
91.81 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Accuracies of structured neural parsing 
and local neural classification parsing with differ-
ent beam sizes. 

Description 
UAS 
greedy neural parser 
91.47 
ranking model 
89.08 
beam contrastive learning 93.28 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Results on WSJ. Speed: sentences per 
second.  †: semi-supervised learning.  ‡: joint 
POS-tagging and dependency parsing models. 

</table></figure>

			<note place="foot" n="3"> Structured Neural Network Model We propose a neural structured-prediction model that scores whole sequences of transition actions, rather than individual actions. As shown in Table 2, the model can be seen as a neural probabilistic alternative of Zhang and Nivre (2011),</note>

			<note place="foot" n="1"> http://stp.lingfil.uu.se/ nivre/research/Penn2Malt.html 2 http://ronan.collobert.com/senna/</note>

			<note place="foot" n="3"> Our baseline accuracy is a little lower than accuracy reported in baseline paper (Chen and Manning, 2014), because we use Penn2Malt to convert the Penn Treebank, and they use LTH Conversion.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgments</head><p>We would like to thank the anonymous review-ers for their insightful comments. This work was partially founded by the Natural Science Foundation of China (61170181, 61300158), the Jiangsu Provincial Research Foundation for Ba-sic Research (BK20130580), Singapore Ministra-try of Education Tier 2 Grant T2MOE201301 and SRGISTD2012038 from Singapore University of Technology and Design.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A transitionbased system for joint part-of-speech tagging and labeled non-projective dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1455" to="1465" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Feature embedding for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international conference on Computational linguistics</title>
		<meeting>the international conference on Computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing with selectional branching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jinho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1052" to="1062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Incremental parsing with the perceptron algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 111. Association for Computational Linguistics</title>
		<meeting>the 42nd Annual Meeting on Association for Computational Linguistics, page 111. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-02 conference on Empirical methods in natural language processing</title>
		<meeting>the ACL-02 conference on Empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep learning for efficient discriminative parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Transitionbased dependency parsing with stack long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL conference</title>
		<meeting>the ACL conference</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Training deterministic parsers with non-deterministic oracles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="403" to="414" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Discriminative training of a neural network statistical parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 95. Association for Computational Linguistics</title>
		<meeting>the 42nd Annual Meeting on Association for Computational Linguistics, page 95. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Training products of experts by minimizing contrastive divergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1771" to="1800" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dynamic programming for linear-time incremental parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1077" to="1086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Simple semi-supervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Loss functions for discriminative training of energybased models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>AIStats</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recurrent greedy parsing with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Legrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD)</title>
		<meeting>the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An asymptotic analysis of generative, discriminative, and pseudolikelihood estimators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="584" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Contrastive unsupervised word alignment with non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.2082</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Punctuation processing for projective dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Mitchell P Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Broad-coverage parsing with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marshall R Mayberry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risto</forename><surname>Iii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miikkulainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Processing Letters</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="121" to="132" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deterministic dependency parsing of English text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Scholz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Computational Linguistics (COLING)</title>
		<meeting>the 20th International Conference on Computational Linguistics (COLING)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="64" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Maltparser: A language-independent system for data-driven dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atanas</forename><surname>Chanev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gülsen</forename><surname>Eryigit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetoslav</forename><surname>Marinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erwin</forename><surname>Marsi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="95" to="135" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Algorithms for deterministic incremental dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="513" to="553" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Parsing with compositional vector grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL conference</title>
		<meeting>the ACL conference</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing using recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An empirical study of semisupervised structured conditional models for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Isozaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="551" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Non-local contrastive objectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vickrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning (ICML-10)</title>
		<meeting>the 27th International Conference on Machine Learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1103" to="1110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Structured training for neural network transition-based parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL conference</title>
		<meeting>the ACL conference</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Statistical dependency analysis with support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyasu</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IWPT</title>
		<meeting>IWPT</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A tale of two parsers: investigating and combining graphbased and transition-based dependency parsing using beam-search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="562" to="571" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Syntactic processing using the generalized perceptron and beam search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="105" to="151" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing with rich non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="188" to="193" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Analyzing the effect of global learning and beam-search on transition-based dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING (Posters)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1391" to="1400" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
