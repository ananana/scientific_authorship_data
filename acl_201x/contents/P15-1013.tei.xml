<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Low-Rank Regularization for Sparse Conjunctive Feature Spaces: An Application to Named Entity Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Audi</forename><surname>Primadhanty</surname></persName>
							<email>primadhanty@cs.upc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Xerox Research Centre Europe</orgName>
								<orgName type="institution">Universitat Politècnica de Catalunya</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
							<email>xavier.carreras@xrce.xerox.com</email>
							<affiliation key="aff0">
								<orgName type="department">Xerox Research Centre Europe</orgName>
								<orgName type="institution">Universitat Politècnica de Catalunya</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariadna</forename><surname>Quattoni</surname></persName>
							<email>ariadna.quattoni@xrce.xerox.com</email>
							<affiliation key="aff0">
								<orgName type="department">Xerox Research Centre Europe</orgName>
								<orgName type="institution">Universitat Politècnica de Catalunya</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Low-Rank Regularization for Sparse Conjunctive Feature Spaces: An Application to Named Entity Classification</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="126" to="135"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Entity classification, like many other important problems in NLP, involves learning classifiers over sparse high-dimensional feature spaces that result from the conjunction of elementary features of the entity mention and its context. In this paper we develop a low-rank reg-ularization framework for training max-entropy models in such sparse conjunctive feature spaces. Our approach handles con-junctive feature spaces using matrices and induces an implicit low-dimensional representation via low-rank constraints. We show that when learning entity classifiers under minimal supervision, using a seed set, our approach is more effective in controlling model capacity than standard techniques for linear classifiers.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many important problems in NLP involve learn- ing classifiers over sparse high-dimensional fea- ture spaces that result from the conjunction of el- ementary features. For example, to classify an en- tity in a document, it is standard to exploit features of the left and right context in which the entity oc- curs as well as spelling features of the entity men- tion itself. These sets of features can be grouped into vectors which we call elementary feature vec- tors. In our example, there will be one elementary feature vector for the left context, one for the right context and one for the features of the mention. Observe that, when the elementary vectors consist of binary indicator features, the outer product of any pair of vectors represents all conjunctions of the corresponding elementary features.</p><p>Ideally, we would like to train a classifier that can leverage all conjunctions of elementary fea- tures, since among them there might be some that are discriminative for the classification task at hand. However, allowing for such expressive high dimensional feature space comes at a cost: data sparsity becomes a key challenge and controlling the capacity of the model is crucial to avoid over- fitting the training data.</p><p>The problem of data sparsity is even more se- vere when the goal is to train classifiers with min- imal supervision, i.e. small training sets. For ex- ample, in the entity classification setting we might be interested in training a classifier using only a small set of examples of each entity class. This is a typical scenario in an industrial setting, where developers are interested in classifying entities ac- cording to their own classification schema and can only provide a handful of examples of each class.</p><p>A standard approach to control the capacity of a linear classifier is to use 1 or 2 regularization on the parameter vector. However, this type of regu- larization does not seem to be effective when deal- ing with sparse conjunctive feature spaces. The main limitation is that 1 and 2 regularization can not let the model give weight to conjunctions that have not been observed at training. Without such ability it is unlikely that the model will generalize to novel examples, where most of the conjunctions will be unseen in the training set.</p><p>Of course, one could impose a strong prior on the weight vector so that it assigns weight to un- seen conjunctions, but how can we build such a prior? What kind of reasonable constraints can we put on unseen conjunctions?</p><p>Another common approach to handle high di- mensional conjunctive feature spaces is to manu- ally design the feature function so that it includes only a subset of "relevant" conjunctions. But de- signing such a feature function can be time con- suming and one might need to design a new fea- ture function for each classification task. Ide- ally, we would have a learning algorithm that does not require such feature engineering and that it can automatically leverage rich conjunctive fea- ture spaces.</p><p>In this paper we present a solution to this prob- lem by developing a regularization framework specifically designed for sparse conjunctive fea- ture spaces. Our approach results in a more effec- tive way of controlling model capacity and it does not require feature engineering.</p><p>Our strategy is based on:</p><p>• Employing tensors to define the scoring func- tion of a max-entropy model as a multilinear form that computes weighted inner products between elementary vectors.</p><p>• Forcing the model to induce low-dimensional embeddings of elementary vectors via low- rank regularization on the tensor parameters.</p><p>The proposed regularization framework is based on a simple conceptual trick. The standard ap- proach to handle conjunctive feature spaces in NLP is to regard the parameters of the linear model as long vectors computing an inner prod- uct with a high dimensional feature representation that lists explicitly all possible conjunctions. In- stead, the parameters of our the model will be ten- sors and the compatibility score between an input pattern and a class will be defined as the sum of multilinear functions over elementary vectors.</p><p>We then show that the rank 1 of the tensor has a very natural interpretation. It can be seen as the intrinsic dimensionality of a latent embedding of the elementary feature vectors. Thus by impos- ing a low-rank penalty on the tensor parameters we are encouraging the model to induce a low- dimensional projection of the elementary feature vectors . Using the rank itself as a regularization constraint in the learning algorithm would result in a non-convex optimization. Instead, we follow a standard approach which is to use the nuclear norm as a convex relaxation of the rank.</p><p>In summary the main contributions of this paper are:</p><p>• We develop a new regularization frame- work for training max-entropy models in high-dimensional sparse conjunctive feature spaces. Since the proposed regularization im- plicitly induces a low dimensional embed- ding of feature vectors, our algorithm can also be seen as a way of implicitly learning a latent variable model.</p><p>• We present a simple convex learning al- gorithm for training the parameters of the model.</p><p>• We conduct experiments on learning entity classifiers with minimal supervision. Our re- sults show that the proposed regularization framework is better for sparse conjunctive feature spaces than standard 2 and 1 reg- ularization. These results make us conclude that encouraging the max-entropy model to operate on a low-dimensional space is an ef- fective way of controlling the capacity of the model an ensure good generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Entity Classification with Log-linear Models</head><p>The formulation we develop in this paper applies to any prediction task whose inputs are some form of tuple. We focus on classification of entity men- tions, or entities in the context of a sentence. For- mally, our input objects are tuples x = l, e, r consisting of an entity e, a left context l and a right context r. The goal is to classify x into one entity class in the set Y.</p><p>We will use log-linear models of the form:</p><formula xml:id="formula_0">Pr(y | x; θ) = exp{s θ (x, y)} y exp{s θ (x, y )}<label>(1)</label></formula><p>where s θ : X × Y → R is a scoring function of entity tuples with a candidate class, and θ are the parameters of this function, to be specified below.</p><p>In the literature it is common to employ a feature-based linear model. That is, one defines a feature function φ : X → {0, 1} n that represents entity tuples in an n-dimensional binary feature space 2 , and the model has a weight vector for each class, θ = {w y } y∈Y . Then s θ (x, y) = φ(x) · w y .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Low-rank Entity Classification Models</head><p>In this section we propose a specific family of models for classifying entity tuples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">A Low-rank Model of Left-Right Contexts</head><p>We start from the observation that when repre- senting tuple objects such as x = l, e, r with features, we often depart from a feature represen- tation of each element of the tuple. Hence, let φ l and φ r be two feature functions representing left and right contexts, with binary dimensions d 1 and d 2 respectively. For now, we will define a model that ignores the entity mention e and makes predictions using context features. It is natural to define conjunctions of left and right features. Hence, in its most general form, one can define a matrix W y ∈ R d 1 ×d 2 for each class, such that θ = {W y } y∈Y and the score is:</p><formula xml:id="formula_1">s θ (l, e, r, y) = φ l (l) W y φ r (r) .<label>(2)</label></formula><p>Note that this corresponds to a feature-based linear model operating in the product space of φ l and φ r , that is, the score has one term for each pair of features:</p><formula xml:id="formula_2">i,j φ l (l)[i] φ r (r)[j] W y [i, j]</formula><p>. Note also that it is trivial to include elementary features of φ l and φ r , in addition to conjunctions, by having a constant dimension in each of the two represen- tations set to 1.</p><p>In all, the model in Eq. (2) is very expressive, with the caveat that it can easily overfit the data, specially when we work only with a handful of la- beled examples. The standard way to control the capacity of a linear model is via 1 or 2 regular- ization.</p><p>Regarding our parameters as matrices allows us to control the capacity of the model via regulariz- ers that favor parameter matrices with low rank. To see the effect of these regularizers, consider that W y has rank k, and let W y = U y Σ y V y be the singular value decomposition, where U y ∈ R d 1 ×k and V y ∈ R d 2 ×k are orthonormal projec- tions and Σ y ∈ R k×k is a diagonal matrix of sin- gular values. We can rewrite the score function as</p><formula xml:id="formula_3">s θ (l, e, r, y) = (φ l (l) U y ) Σ y (V y φ r (r)) .<label>(3)</label></formula><p>In words, the rank k is the intrinsic dimensionality of the inner product behind the score function. A low-rank regularizer will favor parameter matrices that have low intrinsic dimensionality. Below we describe a convex optimization for low-rank mod- els using nuclear norm regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Adding Entity Features</head><p>The model above classifies entities based only on the context. Here we propose an extension to make use of features of the entity. Let T be a set of pos- sible entity feature tags, i.e. tags that describe an entity, such as ISCAPITALIZED, CONTAINSDIG- ITS, SINGLETOKEN, . . . Let φ e be a feature func- tion representing entities. For this case, to simplify our expression, we will use a set notation and de- note by φ e (e) ⊆ T the set of feature tags that de- scribe e. Our model will be defined with one pa- rameter matrix per feature tag and class label, i.e. θ = {W t,y } t∈T ,y∈Y . The model form is:</p><formula xml:id="formula_4">s θ (l, e, r, y) = t∈φe(e) φ l (l) W t,y .φ r (r).</formula><p>(4)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning with Low-rank Constraints</head><p>In this section we describe a convex procedure to learn models of the above form that have low rank. We will define an objective that combines a loss and a regularization term.</p><p>Our first observation is that our parameters are a tensor with up to four axes, namely left and right context representations, entity features, and entity classes. While a matrix has a clear definition of rank, it is not the case for general tensors, and there exist various definitions in the literature. The technique that we use is based on matricization of the tensor, that is, turning the tensor into a matrix that has the same parameters as the tensor but or- ganized in two axes. This is done by partitioning the tensor axes into two sets, one for matrix rows and another for columns. Once the tensor has been turned into a matrix, we can use the standard def- inition of matrix rank. A main advantage of this approach is that we can make use of standard rou- tines like singular value decomposition (SVD) to decompose the matricized tensor. This is the main reason behind our choice.</p><p>In general, different ways of partitioning the tensor axes will lead to different notions of intrin- sic dimensions. In our case we choose the left con- text axes as the row dimension, and the rest of axes as the column dimension. <ref type="bibr">3</ref> In this section, we will denote as W the matricized version of the param- eters θ of our models.</p><p>The second observation is that minimizing the rank of a matrix is a non-convex problem. We make use of a convex relaxation based on the nu- clear norm <ref type="bibr" target="#b15">(Srebro and Shraibman, 2005</ref>). The nuclear norm 4 of a matrix W, denoted W , is the sum of its singular values: W = i Σ i,i where W = UΣV is the singular value decom- position of W. This norm has been used in several applications in machine learning as a convex sur- rogate for imposing low rank, e.g. ( <ref type="bibr" target="#b16">Srebro et al., 2004</ref>).</p><p>Thus, the nuclear norm is used as a regularizer. With this, we define our objective as follows:</p><formula xml:id="formula_5">argmin W L(W) + τ R(W) ,<label>(5)</label></formula><p>where L(W) is a convex loss function, R(W) is a regularizer, and τ is a constant that trades off error and capacity. In experiments we will compare nu- clear norm regularization with 1 and 2 regulariz- ers. In all cases we use the negative log-likelihood as loss function, denoting the training data as D:</p><formula xml:id="formula_6">L(W) = (l,e,r,y)∈D</formula><p>− log Pr(y | l, e, r; W) .</p><p>(6) To solve the objective in Eq. (5) we use a simple optimization scheme known as forward-backward splitting (FOBOS) <ref type="bibr" target="#b4">(Duchi and Singer, 2009)</ref>. In a series of iterations, this algorithm performs a gradient update followed by a proximal projec- tion of the parameters. Such projection depends on the regularizer used: for 1 it thresholds the pa- rameters; for 2 it scales them; and for nuclear- norm regularization it thresholds the singular val- ues. This means that, for nuclear norm regulariza- tion, each iteration requires to decompose W us- ing SVD. See (Madhyastha et al., <ref type="bibr">2014</ref>) for details about this optimization for a related application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>The main aspect of our approach is the use of a spectral penalty (i.e., the rank) to control the capacity of multilinear functions parameterized by matrices or tensors.  used nuclear-norm regularization to learn latent- variable max-margin sequence taggers. <ref type="bibr" target="#b8">Madhyastha et al. (2014)</ref> defined bilexical distribu- <ref type="bibr" target="#b6">Lei et al. (2014)</ref> also use low-rank tensor learn- ing in the context of dependency parsing, where like in our case dependencies are represented by conjunctive feature spaces. While the motivation is similar, their technical solution is different. We use the technique of matricization of a tensor com- bined with a nuclear-norm relaxation to obtain a convex learning procedure. In their case they ex- plicitly look for a low-dimensional factorization of the tensor using a greedy alternating optimization.</p><p>Also recently,  have framed entity classification as a low-rank matrix comple- tion problem. The idea is based on the fact that if two entities (in rows) have similar descriptions (in columns) they should have similar classes. The low-rank structure of the matrix defines intrin- sic representations of entities and feature descrip- tions. The same idea was applied to relation ex- traction ( , using a matrix of entity pairs times descriptions that corresponds to a matricization of an entity-entity-description ten- sor. Very recently <ref type="bibr" target="#b14">Singh et al. (2015)</ref> explored al- ternative ways of applying low-rank constraints to tensor-based relation extraction.</p><p>Another aspect of this paper is training entity classification models using minimal supervision, which has been addressed by multiple works in the literature. A classical successful approach for this problem is to use co-training <ref type="bibr" target="#b1">(Blum and Mitchell, 1998)</ref>: learn two classifiers that use dif- ferent views of the data by using each other's pre- dictions. In the same line, Collins and Singer (1999) trained entity classifiers by bootstraping from an initial set of seeds, using a boosting ver- sion of co-training. Seed sets have also been ex- ploited by graphical model approaches. <ref type="bibr" target="#b5">Haghighi and Klein (2006</ref>) define a graphical model that is soft-constrained such that the prediction for an un- labeled example agrees with the labels of seeds that are distributionally similar. <ref type="bibr" target="#b7">Li et al. (2010)</ref> present a Bayesian approach to expand an initial seed set, with the goal of creating a gazetteer.</p><p>Another approach to entity recognition that, like in our case, learns projections of contextual fea- tures is the method by <ref type="bibr" target="#b0">Ando and Zhang (2005)</ref>.  <ref type="table" target="#tab_2">Mentions  10-30 Seed  10-30 40-120 640-1920  All   PER  clinton, dole, arafat, yeltsin, wasim akram, lebed, dutroux, waqar you- nis, mushtaq ahmed, croft  334  747  3,</ref>  <ref type="table">Table 1</ref>: For each entity class, the seed of entities for the 10-30 set, together with the number of mentions in the training data that involve entities in the seed for various sizes of the seeds.</p><p>They define a set of auxiliary tasks, which can be supervised using unlabeled data, and find a projec- tion of the data that works well as input represen- tation for the auxiliary tasks. This representation is then used for the target task.</p><p>More recently Neelakantan and Collins (2014) presented another approach to gazetteer expansion using an initial seed. A novel aspect is the use of Canonical Correlation Analysis (CCA) to com- pute embeddings of entity contexts, that are used by the named entity classifier. Like in our case, their method learns a compressed representation of contexts that helps prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section we evaluate our regulariza- tion framework for training models in high- dimensional sparse conjunctive feature spaces. We run experiments on learning entity classifiers with minimal supervision. We focus on classification of unseen entities to highlight the ability of the reg- ularizer to generalize over conjunctions that are not observed at training. We simulate minimal supervision using the CoNLL-2003 Shared Task data <ref type="bibr" target="#b17">(Tjong Kim Sang and De Meulder, 2003)</ref>, and compare the performance to 1 and 2 regularizers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Minimal Supervision Task</head><p>We use a minimal supervision setting where we provide the algorithm a seed of entities for each class, that is, a list of entities that is representative for that class. The assumption is that any men- tion of an entity in the seed is a positive example for the corresponding class. Given unlabeled data and a seed of entities for each class, the goal is to learn a model that correctly classifies mentions of entities that are not in the seed. In addition to standard entity classes, we also consider a special non-entity class, which is part of the classification but is excluded from evaluation.</p><p>Note that named entity classification for unseen entities is a challenging problem. Even in the stan- dard fully-supervised scenario, when we measure the performance of state-of-the-art methods on un- seen entities, the F1 values are in the range of 60%. This represents a significant drop with respect to the standard metrics for named entity recognition, which consider all entity mentions of the test set irrespective of whether they appear in the training data or not, and where F1 values at 90% levels are obtained (e.g. ( <ref type="bibr" target="#b12">Ratinov and Roth, 2009)</ref>). This suggests that part of the success of state-of-the-art models is in storing known entities together with their type (in the form of gazetteers or directly in lexicalized parameters of the model).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Setting</head><p>We use the CoNLL-2003 English data, which is annotated with four types: person (PER), location (LOC), organization (ORG), and miscellaneous (MISC). In addition, the data is tagged with parts- of-speech (PoS), and we compute word clusters running the Brown clustering algorithm <ref type="bibr" target="#b2">(Brown et al., 1992</ref>) on the words in the training set.</p><p>We consider annotated entity phrases as candi- date entities, and all single nouns that are not part of an entity as candidate non-entities (O). Both candidate entities and non-entities will be referred to as candidates in the remaining of this section. We lowercase all candidates and remove the am-   biguous ones (i.e., those with more than one label in different mentions). <ref type="bibr">5</ref> To simulate a minimal supervision, we create supervision seeds by picking the n most frequent training candidates for entity types, and the m most frequent candidate non-entities. We create seeds of various sizes n-m, namely <ref type="bibr">10-30, 40-120, 640-1920</ref>, as well as all of the candidates. For each seed, the training set consists of all training mentions that involve entities in the seed. <ref type="table">Table 1</ref> shows the smaller seed, as well as the number of mentions for each seed size.</p><p>For evaluation we use the development and test sections of the data, but we remove the instances of candidates in the training data (i.e., that are in the all seed). We do not remove instances that are ambiguous in the tests. <ref type="bibr">6</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Context Representations</head><p>We refer to context as the sequence of tokens be- fore (left context) and after (right context) a can- didate mention in a sentence. Different classifiers can be built using different representations of the contexts. For example we can change the window size of the context sequence (i.e., for a window size of 1 we only use the last token before the men- tion and the first token after the mention). We can treat the left and right contexts independently of each other, we can treat them as a unique combi- nation, or we can use both. We can also choose to use the word form of a token, its PoS tag, a word cluster, or a combination of these. <ref type="table" target="#tab_2">Table 2</ref> compares different context represen- tations and their performance in classifying un- seen candidates using maximum-entropy classi- fiers trained with Mallet <ref type="bibr" target="#b9">(McCallum, 2002</ref>) with 2 regularization, using the 10-30 seed. We use the lexical representation (the word itself) and a word cluster representation of the context tokens and use a window size of one to three. We use two types of features: bag-of-words features (1- grams of tokens in the specified window) and n- gram features (with n smaller or equal to the win- dow size). The performance of using word clusters is comparable, and sometimes better, to using lexi- cal representations. Moreover, using a longer win- dow, in this case, does not necessarily result in bet- ter performance. <ref type="bibr">7</ref> In the rest of the experiments <ref type="bibr">10</ref> Figure 1: Average F1 of classification of unseen entity candidates on development data, with respect to the size of the seed. NN refers to models with nuclear norm regularization, L1 and L2 refer to 1 and 2 regularization. Each plot corresponds to a different conjunctive feature space with respect to window size (1 or 2), context representation (cluster with/out PoS), using entity features or not, and combining or not full conjunctions with lower-order conjunctions and elementary features.</p><p>• cap=1, cap=0: whether the first letter of the entity candidate is uppercase, or not • all-low=1, all-low=0: whether all letters of the candidate are lowercase letters, or not • all-cap1=1, all-cap1=0: whether all letters of the candidate are uppercase letters, or not • all-cap2=1, all-cap2=0: whether all letters of the candidate are uppercase letters and periods, or not • num-tokens=1, num-tokens=2, num-tok&gt;2: whether the candidate consists of one token, two or more • dummy: a tag that holds for any entity candidate, used to capture context features alone  we will use the elementary features that are more predictive and compact: clusters and PoS tags in windows of size at most 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Comparing Regularizers</head><p>We compare the performance of models trained using the nuclear norm regularizer with models trained using 1 and 2 regularizers. To train each model, we validate the regularization parameter and the number of iterations on development data, trying a wide range of values. The best performing configuration is then used for the comparison. <ref type="figure">Figure 1</ref> shows results on the development set for different feature sets. We started representing context using cluster labels, as it is the most com- pact representation obtaining good results in pre- liminary experiments. We tried several conjunc- tions: a conjunction of the left and right context, as well as conjunctions of left and right contexts and features of the candidate entity. We also tried all different conjunction combinations of the con- texts and the candidate entity features, as well as adding PoS tags to represent contexts. To repre- sent an entity candidate we use standard traits of the spelling of the mention, such as capitalization, ation. Using our richest feature set, the model obtains 76.76 of accuracy in the development, for the task of classifing enti- ties with correct boundaries. If we add features capturing the full entity and its tokens, then the accuracy is 87.63, which is similar to state-of-the-art performance (the best results in literature typically exploit additional gazetteers). Since our evaluation focuses on unknown entities, our features do not include information about the word tokens of entites.  <ref type="figure">Figure 2</ref>: Avg. F1 on development for increasing dimensions, using the low-rank model in <ref type="figure">Figure 1e</ref> trained with all seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">2 3 4 5 6 7 8 9 10203040506070</head><p>the existence of symbols, as well as the number of tokens in the candidate. See <ref type="table" target="#tab_5">Table 3</ref> for the defini- tion of the features describing entity candidates. We observe that for most conjunction settings our regularizer performs better than the 1 and 2 regularizers. Using the best model from each regularizer, we evaluated on the test set. <ref type="table" target="#tab_6">Table  4</ref> shows the test results. For all seed sets, the nuclear norm regularizer obtains the best aver- age F1 performance. This shows that encourag- ing the max-entropy model to operate on a low- dimensional space is effective. Moreover, <ref type="figure">Figure  2</ref> shows model performance as a function of the number of dimensions of the intrinsic projection. The model obtains a good performance even if only a few intrinsic dimensions are used.   <ref type="figure">Figure 1f</ref> trained with the 10-30 seed, with respect to observations of the associated features in training and development. Non-white conjunctions correspond to non-zero weights: black is for conjunctions seen in both the training and development sets; blue is for those seen in training but not in the development; red indicates that the conjunctions were observed only in the development; yellow is for those not observed in training nor development.</p><p>rank model in <ref type="figure">Figure 1f</ref> trained with the 10-30 seed, with respect to observed features in training and development data. Many of the conjunctions of the development set were never observed in the training set. Our regularizer framework is able to propagate weights from the conjunctive features seen in training to unseen conjunctive features that are close to each other in the projected space (these are the yellow and red cells in the matrix). In con- trast, 1 and 2 regularization techniques can not put weight on unseen conjunctions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have developed a low-rank regularization framework for training max-entropy models in sparse conjunctive feature spaces. Our formula- tion is based on using tensors to parameterize clas- sifiers. We control the capacity of the model using the nuclear-norm of a matricization of the tensor. Overall, our formulation results in a convex proce- dure for training model parameters. We have experimented with these techniques in the context of learning entity classifiers. Com- pared to 1 and 2 penalties, the low-rank model obtains better performance, without the need to manually specify feature conjunctions. In our analysis, we have illustrated how the low-rank ap- proach can assign non-zero weights to conjunc- tions that were unobserved at training, but are sim- ilar to observed conjunctions with respect to the low-dimensional projection of their elements. We have used matricization of a tensor to define its rank, using a fixed transformation of the tensor into a matrix. Future work should explore how to combine efficiently different transformations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 shows</head><label>3</label><figDesc>the parameter matrix of the low-O PER LOC ORG MISC Cluster PoS (a) Full parameter matrix of the low-rank model. The ticks in x-axis indicate the space for different entity types, while the ticks in y-axis indicate the space for different prefix context representations.The subblock for PER entity type and PoS representation of the prefixes. The ticks in x-axis indicate the space of the entity features used, while the tick in y-axis indicates an example of a frequently observed prefix for this entity type.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Parameter matrix of the low-rank model in Figure 1f trained with the 10-30 seed, with respect to observations of the associated features in training and development. Non-white conjunctions correspond to non-zero weights: black is for conjunctions seen in both the training and development sets; blue is for those seen in training but not in the development; red indicates that the conjunctions were observed only in the development; yellow is for those not observed in training nor development.</figDesc><graphic url="image-2.png" coords="9,105.32,206.22,413.16,104.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Average-F1 of classification of unseen entity candidates on development data, using the 10-30 
training seed and 2 regularization, for different conjunctive spaces (elementary only, full conjunctions, 
all). Bag-of-words elementary features contain all clusters/PoS in separate windows to the left and to 
the right of the candidate. N-grams elementary features contain all n-grams of clusters/PoS in separate 
left and right windows (e.g. for size 3 it includes unigrams, bigrams and trigrams on each side). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>As evaluation metric we use the average F1 score computed over all entity types, excluding the non-entity type.</figDesc><table>training 
dev. 
test 
PER 
6,516 (3,489) 1,040 (762) 1,342 (925) 
LOC 
6,159 ( 987) 
176 (128) 
246 (160) 
ORG 
5,271 (2,149) 
400 (273) 
638 (358) 
MISC 
3,205 ( 760) 
177 (142) 
213 (152) 
O 
36,673 (5,821) 
951 (671) 
995 (675) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>The 12 entity tags used to represent entity candidates. The tags all-cap1 and all-cap2 are from 
(Neelakantan and Collins, 2014). </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Results on the test for models trained with different sizes of the seed, using the parameters 
and features that obtain the best evaluation results the development set. NN refers to nuclear norm 
regularization, L1 and L2 refer to 1 and 2 regularization. Only test entities unseen at training are 
considered. Avg. F1 is over PER, LOC, ORG and MISC, excluding O. 

</table></figure>

			<note place="foot" n="1"> There are many ways of defining the rank of a tensor. In this paper we matricize tensors into matrices and use the rank of the resulting matrix. Matricization is also referred to as unfolding.</note>

			<note place="foot" n="2"> In general, all models in this paper accept real-valued feature functions. But we focus on binary indicator features because in practice these are the standard type of features in NLP classifiers, and the ones we use here. In fact, in this paper we develop feature spaces based on products of elementary feature functions, in which case the resulting representations correspond to conjunctions of the elementary features.</note>

			<note place="foot" n="3"> In preliminary experiments we tried variations, such as having right prefixes in the columns, and left prefixes, entity tags and classes in the rows. We only observer minor, nonsignificant variations in the results.</note>

			<note place="foot" n="4"> Also known as the trace norm. tions parameterized by matrices which result lexical embeddings tailored for a particular linguistic relation. Like in our case, the low-dimensional latent projections in these papers are learned implicitly by imposing low-rank constraints on the predictions of the model.</note>

			<note place="foot" n="5"> In the CoNLL-2003 English training set, only 235 candidates are ambiguous out of 13,441 candidates, i.e. less than 2%. This suggests that in this data the difficulty behind the task is in recognizing and classifying unseen entities, and not in disambiguating known entities in a certain context. 6 After removing the ambiguous candidates from the training data, and removing candidates seen in the training from the development and test sets, this is the number of mentions (and number of unique candidates in parenthesis) in the data used in our experiments:</note>

			<note place="foot" n="7"> Our learner and feature configuration, using 2 regularization, obtains state-of-the-art results on the standard evalu</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Gabriele Musillo and the anonymous re-viewers for their helpful comments and sugges-tions. This work has been partially funded by the Spanish Government through the SKATER project (TIN2012-38584-C06-01) and an FPI predoctoral grant for Audi Primadhanty.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A framework for learning predictive structures from multiple tasks and unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kubota</forename><surname>Rie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Ando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1817" to="1853" />
			<date type="published" when="2005-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avrim</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh Annual Conference on Computational Learning Theory, COLT&apos; 98</title>
		<meeting>the Eleventh Annual Conference on Computational Learning Theory, COLT&apos; 98<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="92" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Class-based n-gram models of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenifer</forename><forename type="middle">C</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="467" to="479" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised models for named entity classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Efficient online and batch learning using forward backward splitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="2899" to="2934" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Prototype-driven learning for sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, HLT-NAACL &apos;06</title>
		<meeting>the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, HLT-NAACL &apos;06<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="320" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Low-rank tensors for scoring dependency structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1381" to="1391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Distributional similarity vs. pu learning for entity set expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">See-Kiong</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2010 Conference Short Papers, ACLShort &apos;10</title>
		<meeting>the ACL 2010 Conference Short Papers, ACLShort &apos;10<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="359" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning Task-specific Bilexical Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Pranava Swaroop Madhyastha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariadna</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quattoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-08" />
			<biblScope unit="page" from="161" to="171" />
		</imprint>
		<respStmt>
			<orgName>Dublin City University and Association for Computational Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Mallet: A machine learning for language toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">K</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning dictionaries for named entity recognition using minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-04" />
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spectral regularization for max-margin sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariadna</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borja</forename><surname>Balle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning (ICML14)</title>
		<editor>Tony Jebara and Eric P. Xing</editor>
		<meeting>the 31st International Conference on Machine Learning (ICML14)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1710" to="1718" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Design challenges and misconceptions in named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL-2009)</title>
		<meeting>the Thirteenth Conference on Computational Natural Language Learning (CoNLL-2009)<address><addrLine>Boulder, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="147" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Relation extraction with matrix factorization and universal schemas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">M</forename><surname>Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="74" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Towards Combined Matrix and Tensor Factorization for Universal Schema Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL Workshop on Vector Space Modeling for NLP (VSM)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rank, tracenorm and max-norm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adi</forename><surname>Shraibman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning Theory</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="545" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Maximum-margin matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1329" to="1336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien De</forename><surname>Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</title>
		<meeting>the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Universal schema for entity type prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Workshop on Automated Knowledge Base Construction, AKBC &apos;13</title>
		<meeting>the 2013 Workshop on Automated Knowledge Base Construction, AKBC &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="79" to="84" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
