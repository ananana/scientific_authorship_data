<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:16+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Corpus of Natural Language for Visual Reasoning</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alane</forename><surname>Suhr</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Yeh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
						</author>
						<title level="a" type="main">A Corpus of Natural Language for Visual Reasoning</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="217" to="223"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-2034</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a new visual reasoning language dataset, containing 92,244 pairs of examples of natural statements grounded in synthetic images with 3,962 unique sentences. We describe a method of crowd-sourcing linguistically-diverse data, and present an analysis of our data. The data demonstrates a broad set of linguistic phenomena , requiring visual and set-theoretic reasoning. We experiment with various models, and show the data presents a strong challenge for future research.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Understanding complex compositional language in context is a challenge shared by many tasks. Visual question answering and robot instruction systems require reasoning about sets of objects, quantities, comparisons, and spatial relations; for example, when instructing home assistance or assembly-line robots to manipulate objects in clut- tered environments. This reasoning requires ro- bust language understanding, and is only partially addressed by existing datasets. VQA ( <ref type="bibr" target="#b2">Antol et al., 2015)</ref>, while lexically and visually diverse, in- cludes relatively short sentences with limited cov- erage of such phenomena. CLEVR <ref type="bibr" target="#b13">(Johnson et al., 2016</ref>) and SHAPES ( <ref type="bibr" target="#b1">Andreas et al., 2016b</ref>), in contrast, display complex compositional structure, but include only synthetic language.</p><p>In this paper, we introduce the Cornell Natural Language Visual Reasoning (NLVR) corpus and task. We define the binary prediction task of judg- ing if a statement is true for an image or not, and introduce a corpus of annotated pairs of natural language statements and synthetic images.</p><p>Collecting this kind of language presents two challenges. First, we must design environments to  support such descriptions. We use simple visual environments displaying objects with complex vi- sual relations between them. <ref type="figure" target="#fig_1">Figure 1</ref> shows two generated images. The second challenge is elic- iting complex descriptions displaying a range of syntactic and semantic phenomena. We use a two- stage crowdsourcing process. In the first stage, we present sets of images and ask workers to write de- scriptive statements that distinguish them. Using synthetic images with abstract shapes allows us to control the potential distinctions between them; for example, by discouraging simple statements about object existence. In the second stage, we ask workers to label the truth value for the sen- tences and images generated in the first stage.</p><p>Our data includes 92,244 sentence-image pairs with 3,962 unique sentences. We include both images and the structured representation used to generate them to support research using both raw visual information and structured data. <ref type="figure" target="#fig_1">Figure 1</ref> shows two examples. To assess the difficulty of NLVR, we experiment with multiple baselines. The best model using images achieves an accu- racy of 66.12, demonstrating remaining challenges in the data. We also analyze the language in our data for presence of certain linguistic phe- nomena, and compare this analysis with related datasets. The data and leaderboard are available at http://lic.nlp.cornell.edu/nlvr.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work and Datasets</head><p>Several datasets have been created to study vi- sual reasoning and language. VQA ( <ref type="bibr" target="#b2">Antol et al., 2015;</ref><ref type="bibr" target="#b29">Zitnick and Parikh, 2013</ref>) includes crowd- sourced questions and answers for photographs and abstract scenes, and has been studied exten- sively (e.g., <ref type="bibr" target="#b15">Lu et al., 2016;</ref><ref type="bibr" target="#b25">Xu and Saenko, 2016;</ref><ref type="bibr" target="#b28">Zhou et al., 2015;</ref><ref type="bibr" target="#b6">Chen et al., 2015a;</ref><ref type="bibr">Andreas et al., 2016b,a;</ref><ref type="bibr" target="#b22">Ray et al., 2016)</ref>. In contrast to VQA, we use synthetic images and emphasize rep- resenting a broad range of language phenomena. Our motivation is similar to that of SHAPES <ref type="bibr" target="#b1">(Andreas et al., 2016b</ref>) and CLEVR <ref type="bibr" target="#b13">(Johnson et al., 2016)</ref>. Both datasets also use synthetic images and emphasize representing diverse spatial language. However, unlike our approach, they include only automatically generated language.</p><p>Visual reasoning has also been addressed in in- structional language corpora (e.g., <ref type="bibr" target="#b16">MacMahon et al., 2006;</ref><ref type="bibr" target="#b5">Chen and Mooney, 2011;</ref><ref type="bibr" target="#b3">Bisk et al., 2016)</ref>, where executable instructions are grounded in manipulable environments. The language we observe is similar to the type of language studied for understanding and generation of referential ex- pressions ( <ref type="bibr" target="#b18">Mitchell et al., 2010;</ref><ref type="bibr" target="#b17">Matuszek et al., 2012;</ref><ref type="bibr" target="#b11">FitzGerald et al., 2013)</ref>.</p><p>Our task is related to caption generation, which has been studied extensively (e.g., <ref type="bibr" target="#b20">Pedersoli et al., 2016;</ref><ref type="bibr" target="#b4">Carrara et al., 2016;</ref><ref type="bibr" target="#b7">Chen et al., 2016</ref>) with MSCOCO ( <ref type="bibr" target="#b8">Chen et al., 2015b</ref>) and Flickr30K ( <ref type="bibr" target="#b26">Young et al., 2014;</ref><ref type="bibr" target="#b21">Plummer et al., 2015)</ref>. In contrast to caption generation, our task does not require approximate metrics like BLEU.</p><p>Several existing datasets focus on natural lan- guage querying of structured representations, in- cluding GeoQuery <ref type="bibr" target="#b27">(Zelle, 1995)</ref> and WikiTa- bles ( <ref type="bibr" target="#b19">Pasupat and Liang, 2015)</ref>. Our work is com- plementary to these resources. While our corpus was collected using images, we also provide struc- tured representations. When used with these rep- resentations, our corpus is similar to WikiTables, where questions are paired with small web tables. Instead of web tables, we use object sets and focus on visual language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task</head><p>Statements in our data are grounded in synthetic images rendered from structured representations. Given an example, the task is to determine whether a statement is true or false for the image or struc- tured representation. While we describe the im- age, the structured representation is equivalent. We provide examples of the structured represen- tation in the supplementary material. Images are divided into three boxes. <ref type="figure" target="#fig_1">Figure 1</ref> shows two images. Each box contains 1-8 objects. Each object has four properties: position (x/y coordi- nates), color (black, blue, yellow), shape (triangle, square, circle), and size (small, medium, large). Objects within a box cannot overlap and must be contained entirely in the box. We distinguish be- tween images containing scattered objects and im- ages containing only squares arranged in towers up to four blocks tall. The top image in <ref type="figure" target="#fig_1">Figure 1</ref> is a tower example; the bottom is a scatter example.</p><p>This design encourages compositional language with complex visual reasoning. We divide the im- age into boxes to encourage set theoretic reasoning within and between boxes. We also use a relatively limited number of values for each property. While a large number of properties provides a more di- verse image, it is likely to result in descriptions that refer to property differences. We find that the limited number of properties elicits descriptions with rich compositional structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Data Collection</head><p>We generate images following the structure de- scribed in Section 3, and collect grounded natu- ral language descriptions. Data is collected in two phases: sentence writing and validation. During sentence writing, workers are asked to write con- trasting descriptions about a set of images. To val- idate sentences, the description is paired with each of the images. We execute the collection process four times to collect training, development, and two test sets (Test-P and Test-U). We retain one test set as unreleased (Test-U). Generating Images We generate images by ren- dering a randomly sampled structured representa- tion. The number of objects in each box and their properties are sampled uniformly. We generate an equal number of scatter and tower images. To gen- erate the sets of images presented to annotators, we generate two images independently, a third im- age by using the set of objects in the first im-</p><formula xml:id="formula_0">(A) (B) (C) (D)</formula><p>Write one sentence. This sentence must meet all of the following requirements:</p><p>• It describes A.</p><p>• It describes B.</p><p>• It does not describe C.</p><p>• It does not describe D.</p><p>• It does not mention the images explicitly (e.g. "In im- age A, ...").</p><p>• It does not mention the order of the light grey squares (e.g. "In the rightmost square...") There is no one correct sentence for this image. There may be multiple sentences which satisfy the above re- quirements. If you can think of more than one sentence, submit only one. age and randomly re-shuffling them between the boxes, and a fourth image by re-shuffling the ob- jects in the second image. For images with towers, we constrain the re-shuffling to form towers.</p><p>Phase 1 -Sentence Writing Each writing task presents an annotator with four images. <ref type="figure" target="#fig_2">Figure 2</ref> shows the sentence writing prompt, including the set of constraints, which is shown for all writing tasks. The constraints force the worker to contrast two pairs by referring to similarities and differ- ences between the images, but not to refer to the position of the image in the prompt, or of each box in each image. These constraints are placed to elicit more set-theoretic language, and to allow us to divide the result of each task into four exam- ples, pairing the annotator's sentence with each of the four images it was presented with.</p><p>Phase 2 -Validation In the second phase, we pair each sentence with the four images used to generate it. We re-label all sentence-image pairs as true or false, correcting for any violations of the constraints in the first phase. We do not use the original position of the image as any part of the final label to neutralize any ordering effect. In practice, 8.2% of examples had a different la- bel than inferred from their original position in <ref type="table" target="#tab_2">Unique sentences Examples  Train  3,163  74,460  Dev  267  5,940  Test-P  266  5,934  Test-U  266  5,910  Total  3,962  92,244   Table 1</ref>: Data statistics.</p><p>the first phase. During validation, boxes are ran- domly permuted to ensure the last constraint was followed. We allow workers to annotate a sentence as nonsensical with regard to the image, and in- struct annotators to ignore grammar errors. Post-processing We prune pairs when their ma- jority class is nonsensical. When collecting mul- tiple annotations for a pair, we prune pairs if the gap between the classes is less than two votes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Data Statistics and Analysis</head><p>We use the crowdsourcing platform Upwork, 1 and select ten annotators using a small set of exam- ple questions. We collect 3,974 task instances and 28,723 total validation judgments at a total cost of $5,526. From these 3,974 task instances we extract 15,896 sentence-image pairs. We prune 522 pairs in post-processing. For the training set we collect a single validation annotation for each sentence-image pair; for the rest of the data we collect five annotations each. Finally, we gener- ate six sentence-image pairs from each sample by permuting the boxes. The validation step ensures this permutation does not change the label. <ref type="table">Table 1</ref> shows the number of sentences and pairs, includ- ing permutations, for each split. We merge the development and test splits to cal- culate agreement statistics. We calculate Krippen- dorf's α and Fleiss' κ (Cocos et al., 2015) on both the full and pruned datasets. To calculate Fleiss' κ, we randomly permute the five annotations to be assigned to five "raters" and compute average kappa from 100 iterations. Before pruning, we ob- serve α = 0.768 and κ = 0.709, indicating sub- stantial agreement <ref type="bibr" target="#b14">(Landis and Koch, 1977)</ref>. Prun- ing improves agreement to α = 0.831 (indicating almost-perfect agreement) and κ = 0.808.</p><p>We analyze 200 development sentences to iden- tify the distribution of semantic phenomena and syntactic ambiguity <ref type="table">(Table 2</ref>). For comparison, we apply this analysis to 200 abstract-image and 200 real-image sentences from VQA ( <ref type="bibr" target="#b2">Antol et al., 2015</ref> There is a blue triangle touching the wall with its side. Spatial Relations 31 42.5 66 61.6 there is one tower with a yellow block above a yellow block Comparative 1.5 1 3 73.6</p><p>There is a box with multiple items and only one item has a different color. Presupposition <ref type="bibr">2</ref> 79 80 19.5 54.0 There is a box with seven items and the three black items are the same in shape. Negation 0 1 9.5 51.0 there is exactly one black triangle not touching the edge Syntax Coordination 0 0 4.5 53.4</p><p>There is a box with at least one square and at least three triangles. PP Attachment 7 3 23 70.9</p><p>There is a black block on a black block as the base of a tower with three blocks. <ref type="table">Table 2</ref>: Qualitative and empirical analysis of our data and VQA ( <ref type="bibr" target="#b2">Antol et al., 2015)</ref>. We analyze 200 sentences for each dataset. The data is categorized to semantic and syntactic categories. We use the terms hard and soft cardinality to differentiate between language using exact numerical values and ranges. For each dataset, we show the percentage of the samples analyzed that demonstrate the phenomena. We analyze abstract (abs) and real images from VQA separately. For our data, we also include the accuracy using the NMN system (Section 6) for the subset of images we tagged with this category. length in our data is 11.22 tokens and the vocabu- lary size is 262. In <ref type="figure" target="#fig_3">Figure 3</ref>, we compare sentence length distribution to VQA, MSCOCO <ref type="bibr" target="#b8">(Chen et al., 2015b)</ref>, and CLEVR ( <ref type="bibr" target="#b13">Johnson et al., 2016)</ref>. Our sentences are generally longer than VQA and more similar in length to MSCOCO. However, our task is more similar to VQA, where context is used to understand language, rather than to generate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Methods</head><p>We evaluate multiple methods on the rendered images and structured representations. Hyper- parameters and initialization details are described in the supplementary material. <ref type="bibr">2</ref> We say a statement or question uses presupposition when it assumes the truth value of some proposition in order for its entire truth value to be defined. In this example, an image which does not have three black items will have no defined truth value for this statement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Majority Class and Single Modality</head><p>We use image-and text-only models to measure how well biases in our data can be used to solve the task. If the model is able to do well on the text-or image-only baselines, this implies our data does not require the two modalities. <ref type="bibr" target="#b2">Antol et al. (2015)</ref> performed a similar analysis of VQA with the questions only to gauge how and if background knowledge of the domain could aid performance.</p><p>Majority Assign the most common label (true) to all examples.</p><p>Text Only Encode the sentence with a recurrent neural network (RNN; Elman, 1990) with long short-term memory units (LSTM; Hochreiter and Schmidhuber, 1997) and a binary softmax com- puted from the final output.</p><p>Image Only Encode the image with a convo- lutional neural network (CNN) with three layers. The CNN output is used by a three-layer percep- tron with a softmax on the final layer. <ref type="bibr">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Structured Representation</head><p>We use the structured representations described in Sections 3 and 4.  MaxEnt Train a MaxEnt classifier. We use the text and structured representation to compute property-and count-based features. Property- based features trigger when some property (e.g., an object is touching a wall) is true in the structure. We create features by crossing triggered proper- ties with each n-grams from the sentence, up to n = 6. Count-based features trigger when a count we observe in the image (e.g., the number of black triangles) is present in the sentence. We generate features combining the type of item counted (e.g., black triangles) with the n-grams surrounding the count in the sentence, up to n = 6. We provide details in the supplementary material. MLP Train a single-layer perceptron with a soft- max layer. The input to the perceptron is the mean of the feature embeddings. We use the same fea- ture set as the MaxEnt model. Image Features+RNN Compute features from the structure representation only, and encode the text with an LSTM RNN. The two representations are concatenated, and used as input to a two-layer perceptron and a softmax layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Image Representation</head><p>CNN+RNN Concatenate the CNN and RNN representations (Section 6.1) and apply a multi- layer perceptron with a softmax. NMN The neural module networks approach of <ref type="bibr" target="#b1">Andreas et al. (2016b)</ref>. We experiment with the default maximum leaves of two, and with allowing for more expressive representations with a max- imum leaves of five. We observe higher devel- opment accuracy with the trees using maximum leaves of five (63.06% vs. 62.4% with the default of two), which we use in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head><p>We run each experiment ten times and report mean accuracy as well as standard deviation for ran- domly initialized models. <ref type="table" target="#tab_2">Table 3</ref> shows our re- sults. NMN is the best performing model using images. <ref type="table">Table 2</ref> shows the NMN accuracy for each category in our qualitative analysis sample. While the number of sentences in some categories is relatively small, we observe a higher number of failures in sentences that include negations and coordinations. For models using the structured representation, the MaxEnt model provides the best performance. When ablating count-based fea- tures from the MaxEnt model, development accu- racy decreases from 68.04 to 57.7. This indicates counting is an important aspect of the problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Discussion</head><p>We introduce the Cornell Natural Language Vi- sual Reasoning dataset and task. The data in- cludes complex compositional language grounded in images and structured representations. The task requires addressing challenges in visual and set-theoretic reasoning. We experiment with multiple systems and, in general, observe rel- atively low performance. Together with our qualitative analysis, this exemplifies the com- plexity of the data. We release our annotated training and development sets, and create two test sets. The public test set will be released along with its annotation. Computing results on the unreleased test data will require submitting trained models. Procedures for submitting mod- els and the task leader board are available at http://lic.nlp.cornell.edu/nlvr.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>There are two towers with the same height but their base is not the same in color. There is a box with 2 triangles of same color nearly touching each other.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example sentences and images from our corpus. Each image includes three boxes with different object types. The truth value of the top sentence is true, while the bottom is false.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Sentence writing prompt. The bottom sentence in Figure 1 was generated from this prompt.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Distribution of sentence lengths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>). The difference in the distribution illustrates the complexity of our data. The mean sentence</figDesc><table>1 http://upwork.com 

219 

VQA VQA 

Our 
NMN 
Example 
(abs) (real) 
Data Correct 
Semantics 
Cardinality (hard) 
12 
11.5 
66 
63.8 
There are exactly four objects not touching any edge 
Cardinality (soft) 
0 
1 
16 
63.4 
There is a box with at least one square and at least three 
triangles. 
Existential 
4.5 
11.5 
88 
64.2 
There is a tower with yellow base. 
Universal 
1 
1 
7.5 
67.8 
There is a black item in every box. 
Coordination 
3 
5 
17 
58.5 
There are 2 blue circles and 1 blue triangle 
Coreference 
8.5 
6.5 
3 
55.3 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Mean accuracy and standard deviation results. We report accuracy for the train, development, and both 
test sets. Three systems use the structured representation. Two systems (and Image Only) use the raw image. 

</table></figure>

			<note place="foot" n="3"> We also experimented using the ImageNet-trained Inception v4 model (Szegedy et al., 2017), but found it did not improve performance, possibly due to the difference between our images and ImageNet.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported by a Microsoft Re-search Women's Fellowship, a Google Faculty Award, and an Amazon Web Services Cloud Cred-its for Research Grant. We thank the Cornell and University of Washington NLP groups for their support and helpful comments. We thank the anonymous reviewers for their feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to compose neural networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/N16-1181</idno>
		<ptr target="https://doi.org/10.18653/v1/N16-1181" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">VQA: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Natural language communication with robots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Yuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/N16-1089</idno>
		<ptr target="https://doi.org/10.18653/v1/N16-1089" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Picture it in your mind: Generating high level visual representations from textual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Carrara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiziano</forename><surname>Fagni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Falchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Moreo</surname></persName>
		</author>
		<idno>CoRR abs/1606.07287</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to interpret natural language navigation instructions from observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Conference on Artificial Intelligence</title>
		<meeting>the National Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">ABC-CNN: An attention based convolutional neural network for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyuan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakant</forename><surname>Nevatia</surname></persName>
		</author>
		<idno>CoRR abs/1511.05960</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Bootstrap, review, decode: Using out-ofdomain textual data to improve image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurélien</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<idno>CoRR abs/1611.05321</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Microsoft COCO captions: Data collection and evaluation server</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<idno>CoRR abs/1504.00325</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Effectively crowdsourcing radiology report annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Cocos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Masino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/W15-2614</idno>
		<ptr target="https://doi.org/10.18653/v1/W15-2614" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Workshop on Health Text Mining and Information Analysis</title>
		<meeting>the Sixth International Workshop on Health Text Mining and Information Analysis</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning distributions over logical forms for referring expression generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D13-1197" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<idno>CoRR abs/1612.06890</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The measurement of observer agreement for categorical data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Landis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="159" to="74" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hierarchical question-image coattention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Walk the talk: Connecting language, knowledge, action in route instructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Macmahon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Stankiewics</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Kuipers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Conference on Artificial Intelligence</title>
		<meeting>the National Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A joint model of language and perception for grounded attribute learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Matuszek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Natural reference to objects in a visual domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Kees Van Deemter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reiter</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/W10-4210" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Natural Language Generation Conference</title>
		<meeting>the 6th International Natural Language Generation Conference</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Compositional semantic parsing on semi-structured tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="doi">10.3115/v1/P15-1142</idno>
		<ptr target="https://doi.org/10.3115/v1/P15-1142" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Areas of attention for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<idno>CoRR abs/1612.01033</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">C</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Question relevance in VQA: Identifying non-visual and false</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arijit</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Christie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">222</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<ptr target="http://aclweb.org/anthology/D16-1090" />
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ask, attend and answer: Exploring question-guided spatial attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/Q14-1006" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association of Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Using inductive logic programming to automate the construction of natural language parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">M</forename><surname>Zelle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<pubPlace>Austin</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Texas</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Simple baseline for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno>CoRR abs/1512.02167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bringing semantics into focus using visual abstraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
