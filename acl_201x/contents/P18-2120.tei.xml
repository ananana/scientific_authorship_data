<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:20+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Multi-sentiment-resource Enhanced Attention Network for Sentiment Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyang</forename><surname>Lei</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Peking University Shenzhen Institute</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujiu</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Peking University Shenzhen Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Liu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Graduate School at Shenzhen</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Multi-sentiment-resource Enhanced Attention Network for Sentiment Classification</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="758" to="763"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Deep learning approaches for sentiment classification do not fully exploit sentiment linguistic knowledge. In this paper, we propose a Multi-sentiment-resource Enhanced Attention Network (MEAN) to alleviate the problem by integrating three kinds of sentiment linguistic knowledge (e.g., sentiment lexicon, negation words, intensity words) into the deep neural network via attention mechanisms. By using various types of sentiment resources, MEAN utilizes sentiment-relevant information from different representation sub-spaces, which makes it more effective to capture the overall semantics of the sentiment , negation and intensity words for sentiment prediction. The experimental results demonstrate that MEAN has robust superiority over strong competitors.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sentiment classification is an important task of natural language processing (NLP), aiming to classify the sentiment polarity of a given text as positive, negative, or more fine-grained classes. It has obtained considerable attention due to it- s broad applications in natural language process- ing ( <ref type="bibr" target="#b6">Hao et al., 2012;</ref>). Most existing studies set up sentiment classifiers using supervised machine learning approaches, such as support vector machine (SVM) ( <ref type="bibr" target="#b13">Pang et al., 2002</ref>), convolutional neural network (CNN) <ref type="bibr" target="#b9">(Kim, 2014;</ref><ref type="bibr" target="#b0">Bonggun et al., 2017)</ref>, long short-term memo- ry (LSTM) <ref type="bibr" target="#b7">(Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr" target="#b15">Qian et al., 2017)</ref>, Tree-LSTM ( <ref type="bibr" target="#b17">Tai et al., 2015)</ref>, and attention-based methods ( <ref type="bibr" target="#b23">Zhou et al., 2016;</ref><ref type="bibr" target="#b21">Yang et al., 2016;</ref><ref type="bibr" target="#b5">Lin et al., 2017;</ref><ref type="bibr" target="#b3">Du et al., 2017)</ref>.</p><p>Despite the remarkable progress made by the previous work, we argue that sentiment analysis still remains a challenge. Sentiment resources in- cluding sentiment lexicon, negation words, inten- sity words play a crucial role in traditional senti- ment classification approaches ( <ref type="bibr" target="#b12">Maks and Vossen, 2012;</ref><ref type="bibr" target="#b4">Duyu et al., 2014</ref>). Despite its usefulness, to date, the sentiment linguistic knowledge has been underutilized in most recent deep neural network models (e.g., <ref type="bibr">CNNs and LSTMs)</ref>.</p><p>In this work, we propose a Multi-sentiment- resource Enhanced Attention Network (MEAN) for sentence-level sentiment classification to inte- grate many kinds of sentiment linguistic knowl- edge into deep neural networks via multi-path attention mechanism. Specifically, we first de- sign a coupled word embedding module to model the word representation from character-level and word-level semantics. This can help to capture the morphological information such as prefixes and suffixes of words. Then, we propose a multi- sentiment-resource attention module to learn more comprehensive and meaningful sentiment-specific sentence representation by using the three types of sentiment resource words as attention sources attending to the context words respectively. In this way, we can attend to different sentiment- relevant information from different representation subspaces implied by different types of sentimen- t sources and capture the overall semantics of the sentiment, negation and intensity words for senti- ment prediction.</p><p>The main contributions of this paper are sum- marized as follows. First, we design a coupled word embedding obtained from character-level embedding and word-level embedding to capture both the character-level morphological informa- tion and word-level semantics. Second, we pro- pose a multi-sentiment-resource attention module to learn more comprehensive sentiment-specific sentence representation from multiply subspaces implied by three kinds of sentiment resources in- cluding sentiment lexicon, intensity words, nega- tion words. Finally, the experimental results show that MEAN consistently outperforms competitive methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>Our proposed MEAN model consists of three key components: coupled word embedding module, multi-sentiment-resource attention module, sen- tence classifier module. In the rest of this section, we will elaborate these three parts in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Coupled Word Embedding</head><p>To exploit the sentiment-related morphological in- formation implied by some prefixes and suffix- es of words (such as "Non-", "In-", "Im-"), we design a coupled word embedding learned from character-level embedding and word-level embed- ding. We first design a character-level convolution neural network (Char-CNN) to obtain character- level embedding ( <ref type="bibr" target="#b22">Zhang et al., 2015)</ref>. Differen- t from ( <ref type="bibr" target="#b22">Zhang et al., 2015)</ref>, the designed Char- CNN is a fully convolutional network without max-pooling layer to capture better semantic in- formation in character chunk. Specifically, we first input one-hot-encoding character sequences to a 1 × 1 convolution layer to enhance the seman- tic nonlinear representation ability of our mod- el ( <ref type="bibr" target="#b11">Long et al., 2015)</ref>, and the output is then fed into a multi-gram (i.e. different window sizes) convolution layer to capture different local charac- ter chunk information. For word-level embedding, we use pre-trained word vectors, GloVe <ref type="bibr" target="#b14">(Pennington et al., 2014</ref>), to map each word to a low- dimensional vector space. Finally, each word is represented as a concatenation of the character- level embedding and word-level embedding. This is performed on the context words and the three types of sentiment resource words 1 , resulting in four final coupled word embedding matrices: the</p><formula xml:id="formula_0">W c = [w c 1 , ..., w c t ] ∈ R d×t for context words, the W s = [w s 1 , ..., w s m ] ∈ R d×m for sentiment words, the W i = [w i 1 , ..., w i k ] ∈ R d×k for intensity word- s, the W n = [w n 1 , .</formula><p>.., w n p ] ∈ R d×p for negation words. Here, t, m, k, p are the length of the corre- sponding items respectively, and d is the embed- ding dimension. Each W is normalized to better calculate the following word correlation. <ref type="bibr">1</ref> To be precise, sentiment resource words include senti- ment words, negation words and intensity words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multi-sentiment-resource Attention Module</head><p>After obtaining the coupled word embedding, we propose a multi-sentiment-resource attention mechanism to help select the crucial sentiment- resource-relevant context words to build the sentiment-specific sentence representation. Con- cretely, we use the three kinds of sentiment re- source words as attention sources to attend to the context words respectively, which is beneficial to capture different sentiment-relevant context word- s corresponding to different types of sentimen- t sources. For example, using sentiment words as attention source attending to the context words helps form the sentiment-word-enhanced sentence representation. Then, we combine the three kind- s of sentiment-resource-enhanced sentence repre- sentations to learn the final sentiment-specific sen- tence representation. We design three types of at- tention mechanisms: sentiment attention, intensi- ty attention, negation attention to model the three kinds of sentiment resources, respectively. In the following, we will elaborate the three types of at- tention mechanisms in details. First, inspired by (Xiong et al.), we expect to establish the word-level relationship between the context words and different kinds of sentiment re- source words. To be specific, we define the dot products among the context words and the three kinds of sentiment resource words as correlation matrices. Mathematically, the detailed formula- tion is described as follows.</p><formula xml:id="formula_1">M s = (W c ) T · W s ∈ R t×m (1) M i = (W c ) T · W i ∈ R t×k (2) M n = (W c ) T · W n ∈ R t×p (3)</formula><p>where M s , M i , M n are the correlation matrices to measure the relationship among the context words and the three kinds of sentiment resource word- s, representing the relevance between the context words and the sentiment resource word. After obtaining the correlation matrices, we can compute the sentiment-resource-relevant con- text word representations X c s , X c i , X c n by the dot products among the context words and differ- ent types of corresponding correlation matrices. Meanwhile, we can also obtain the context-word- relevant sentiment word representation matrix X s by the dot product between the correlation ma- trix M s and the sentiment words W s , the context-word-relevant intensity word representation ma- trix X i by the dot product between the intensi- ty words W i and the correlation matrix M i , the context-word-relevant negation word representa- tion matrix X n by the dot product between the negation words W n and the correlation matrix M n . The detailed formulas are presented as fol- lows:</p><formula xml:id="formula_2">X c s = W c M s , X s = W s (M s ) T<label>(4)</label></formula><formula xml:id="formula_3">X c i = W c M i , X i = W i (M i ) T<label>(5)</label></formula><formula xml:id="formula_4">X c n = W c M n , X n = W n (M n ) T<label>(6)</label></formula><p>The final enhanced context word representation matrix is computed as:</p><formula xml:id="formula_5">X c = X c s + X c i + X c n .<label>(7)</label></formula><p>Next, we employ four independent GRU net- works ( <ref type="bibr" target="#b2">Chung et al., 2015</ref>) to encode hidden s- tates of the context words and the three types of sentiment resource words, respectively. Formally, given the word embedding X c , X s , X i , X n , the hidden state matrices H c , H s , H i , H n can be ob- tained as follows:</p><formula xml:id="formula_6">H c = GRU (X c ) (8) H s = GRU (X s )<label>(9)</label></formula><formula xml:id="formula_7">H i = GRU (X i ) (10) H n = GRU (X n )<label>(11)</label></formula><p>After obtaining the hidden state matrices, the sentiment-word-enhanced sentence representation o 1 can be computed as:</p><formula xml:id="formula_8">o 1 = t i=1 α i h c i , q s = m i=1 h s i /m<label>(12)</label></formula><formula xml:id="formula_9">β([h c i ; q s ]) = u T s tanh(W s [h c i ; q s ])<label>(13)</label></formula><formula xml:id="formula_10">α i = exp(β([h c i ; q s ])) t i=1 exp(β([h c i ; q s ]))<label>(14)</label></formula><p>where q s denotes the mean-pooling operation to- wards H s , β is the attention function that calcu- lates the importance of the i-th word h c i in the context and α i indicates the importance of the i- th word in the context, u s and W s are learnable parameters.</p><p>Similarly, with the hidden states H i and H n for the intensity words and the negation words as attention sources, we can obtain the intensity- word-enhanced sentence representation o 2 and the negation-word-enhanced sentence representation o 3 . The final comprehensive sentiment-specific sentence representatioñ o is the composition of the above three sentiment-resource-specific sentence representations o 1 , o 2 , o 3 :</p><formula xml:id="formula_11">˜ o = [o 1 , o 2 , o 3 ]<label>(15)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Sentence Classifier</head><p>After obtaining the final sentence representatioñ o, we feed it to a softmax layer to predict the senti- ment label distribution of a sentence:</p><formula xml:id="formula_12">ˆ y = exp( ˜ W o T ˜ o + ˜ b o ) C i=1 exp( ˜ W o T ˜ o + ˜ b o )<label>(16)</label></formula><p>wherê y is the predicted sentiment distribution of the sentence, C is the number of sentiment labels, ˜ W o and˜band˜ and˜b o are parameters to be learned.</p><p>For model training, our goal is to minimize the cross entropy between the ground truth and pre- dicted results for all sentences. Meanwhile, in or- der to avoid overfitting, we use dropout strategy to randomly omit parts of the parameters on each training case. Inspired by ( <ref type="bibr" target="#b5">Lin et al., 2017)</ref>, we al- so design a penalization term to ensure the diversi- ty of semantics from different sentiment-resource- specific sentence representations, which reduces information redundancy from different sentimen- t resources attention. Specifically, the final loss function is presented as follows:</p><formula xml:id="formula_13">L(ˆ y, y) = − N i=1 C j=1 y j i log(ˆ y j i ) + λ( θ∈Θ θ 2 )<label>(17)</label></formula><p>+ µ||˜O˜Oµ||˜ µ||˜Oµ||˜O˜ µ||˜O˜O T − ψI|| 2</p><formula xml:id="formula_14">F ˜ O =[o 1 ; o 2 ; o 3 ]<label>(18)</label></formula><p>where y j i is the target sentiment distribution of the sentence, ˆ y j i is the prediction probabilities, θ de- notes each parameter to be regularized, Θ is pa- rameter set, λ is the coefficient for L 2 regulariza- tion, µ is a hyper-parameter to balance the three terms, ψ is the weight parameter, I denotes the the identity matrix and ||.|| F denotes the Frobe- nius norm of a matrix. Here, the first two terms of the loss function are cross-entropy function of the predicted and true distributions and L 2 regulariza- tion respectively, and the final term is a penaliza- tion term to encourage the diversity of sentiment sources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets and Sentiment Resources</head><p>Movie Review (MR) <ref type="bibr">2</ref> and Stanford Sentimen- t Treebank (SST) 3 are used to evaluate our model. MR dataset has 5,331 positive samples and 5,331 negative samples. We adopt the same data split as in ( <ref type="bibr" target="#b15">Qian et al., 2017)</ref>. SST consists of 8,545 train- ing samples, 1,101 validation samples, 2210 test samples. Each sample is marked as very negative, negative, neutral, positive, or very positive. Senti- ment lexicon combines the sentiment words from both ( <ref type="bibr" target="#b15">Qian et al., 2017)</ref> and ( <ref type="bibr" target="#b8">Hu and Liu, 2004</ref>), resulting in 10,899 sentiment words in total. We collect negation and intensity words manually as the number of these words is limited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Baselines</head><p>In order to comprehensively evaluate the perfor- mance of our model, we list several baselines for sentence-level sentiment classification.</p><p>RNTN: Recursive Tensor Neural Network ( <ref type="bibr" target="#b16">Socher et al., 2013</ref>) is used to model correlations between different dimensions of child nodes vec- tors.</p><p>LSTM/Bi-LSTM: Cho et al. <ref type="formula" target="#formula_2">(2014)</ref> employs Long Short-Term Memory and the bidirectional variant to capture sequential information.</p><p>Tree-LSTM: Memory cells was introduced by Tree-Structured Long Short-Term Memory ( <ref type="bibr" target="#b17">Tai et al., 2015</ref>) and gates into tree-structured neural network, which is beneficial to capture semantic relatedness by parsing syntax trees.</p><p>CNN: Convolutional Neural Networks <ref type="bibr" target="#b9">(Kim, 2014</ref>) is applied to generate task-specific sentence representation.</p><p>NCSL ID-LSTM: <ref type="bibr" target="#b19">(Tianyang et al., 2018</ref>) uses rein- forcement learning to learn structured sentence representation for sentiment classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Implementation Details</head><p>In our experiments, the dimensions of character- level embedding and word embedding (GloVe) are both set to 300. Kernel sizes of multi-gram convo- lution for Char-CNN are set to 2, 3, respectively. All the weight matrices are initialized as random orthogonal matrices, and we set all the bias vec- tors as zero vectors. We optimize the proposed model with RMSprop algorithm, using mini-batch training. The size of mini-batch is 60. The dropout rate is 0.5, and the coefficient λ of L 2 normaliza- tion is set to 10 −5 . µ is set to 10 −4 . ψ is set to 0.9. When there are not sentiment resource words in the sentences, all the context words are treat- ed as sentiment resource words to implement the multi-path self-attention strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Experiment Results</head><p>In our experiments, to be consistent with the re- cent baseline methods, we adopt classification ac- curacy as evaluation metric. We summarize the experimental results in <ref type="table">Table 1</ref>. Our model has robust superiority over competitors and sets state- of-the-art on MR and SST datasets. First, our model brings a substantial improvement over the methods that do not leverage sentiment linguis- tic knowledge (e.g., RNTN, LSTM, BiLSTM, C- NN and ID-LSTM) on both datasets. This veri- fies the effectiveness of leveraging sentiment lin- guistic resource with the deep learning algorithms. Second, our model also consistently outperforms LR-Bi-LSTM which integrates linguistic roles of sentiment, negation and intensity words into neu- ral networks via the linguistic regularization. For example, our model achieves 2.4% improvements over the MR dataset and 0.8% improvements over the SST dataset compared to LR-Bi-LSTM. This is because that MEAN designs attention mecha- nisms to leverage sentiment resources efficiently, which utilizes the interactive information between context words and sentiment resource words.</p><p>In order to analyze the effectiveness of each component of MEAN, we also report the abla- tion test in terms of discarding character-level em- bedding (denoted as MEAN w/o CharCNN) and sentiment words/negation words/intensity words (denoted as MEAN w/o sentiment words/negation words/intensity words). All the tested factors con-tribute greatly to the improvement of the MEAN. In particular, the accuracy decreases sharply when discarding the sentiment words. This is within our expectation since sentiment words are vital when classifying the polarity of the sentences.  <ref type="table">Table 1</ref>: Evaluation results. The best result for each dataset is in bold. The result marked with # are retrieved from <ref type="bibr" target="#b15">(Qian et al., 2017)</ref>, and the re- sults marked with * denote the results are obtained by our implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we propose a novel Multi-sentiment- resource Enhanced Attention Network (MEAN) to enhance the performance of sentence-level senti- ment analysis, which integrates the sentiment lin- guistic knowledge into the deep neural network.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>:</head><label></label><figDesc>Teng et al. (2016) designs a Neural Context-Sensitive Lexicon (NSCL) to obtain pri- or sentiment scores of words in the sentence. LR-Bi-LSTM: Qian et al. (2017) imposes lin- guistic roles into neural networks by applying lin- guistic regularization on intermediate outputs with KL divergence. Self-attention: Lin et al. (2017) proposes a self- attention mechanism to learn structured sentence embedding.</figDesc></figure>

			<note place="foot" n="2"> http://www.cs.cornell.edu/people/ pabo/movie-review-data/ 3 https://nlp.stanford.edu/sentiment/we train the model on both phrases and sentences but only test on sentences</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported in part by the Research Fund for the development of s-trategic emerging industries by ShenZhen c-ity (No.JCYJ20160301151844537 and No. J-CYJ20160331104524983).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Lexicon integrated cnn models with attention for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin</forename><surname>Bonggun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Timothy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Choi Jinho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</title>
		<meeting>the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Gated feedback recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stance classification with target-specific neural attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruifeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Coooolll: A deep learning system for twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tang</forename><surname>Duyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Furu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Ming</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation<address><addrLine>SemEval@COLING</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning representations from heterogeneous network for sentiment classification of product reviews. Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruifeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="page" from="34" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Combining social cognitive theories with linguistic features for multi-genre sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muresan</forename><surname>Smaranda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Dequan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Pacific Asia Conference on Language, Information and Computation</title>
		<meeting>the 26th Pacific Asia Conference on Language, Information and Computation</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGKDD</title>
		<meeting>SIGKDD</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cicero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR 2017</title>
		<meeting>ICLR 2017</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><forename type="middle">Darrell</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR 2015</title>
		<meeting>CVPR 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A lexicon model for deep sentiment analysis and opinion mining applications. Decision Support Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isa</forename><surname>Maks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piek</forename><surname>Vossen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="680" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Thumbs up?: sentiment classification using machine learning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivakumar</forename><surname>Vaithyanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Linguistically regularized LSTM for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiao</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2017</title>
		<meeting>ACL 2017</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1679" to="1689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2013</title>
		<meeting>EMNLP 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Context-sensitive lexicon features for neural sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyang</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duy-Tin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning structured representation for text classification via reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Tianyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huang</forename><surname>Minlie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Dynamic coattention networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Xiaodong He, Alex Smola, and Eduard Hovy</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attention-based lstm network for cross-lingual sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2016</title>
		<meeting>EMNLP 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
