<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:08+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ScoutBot: A Dialogue System for Collaborative Navigation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><forename type="middle">M</forename><surname>Lukin</surname></persName>
							<email>stephanie.m.lukin.civ@mail.mil, felix.gervits@tufts.edu,</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Army Research Laboratory</orgName>
								<orgName type="institution">U.S</orgName>
								<address>
									<postCode>20783</postCode>
									<settlement>Adelphi</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Gervits</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tufts University</orgName>
								<address>
									<postCode>02155</postCode>
									<settlement>Medford</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cory</forename><forename type="middle">J</forename><surname>Hayes</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Army Research Laboratory</orgName>
								<orgName type="institution">U.S</orgName>
								<address>
									<postCode>20783</postCode>
									<settlement>Adelphi</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Leuski</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">USC Institute for Creative Technologies</orgName>
								<address>
									<postCode>90094</postCode>
									<settlement>Playa Vista</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pooja</forename><surname>Moolchandani</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Army Research Laboratory</orgName>
								<orgName type="institution">U.S</orgName>
								<address>
									<postCode>20783</postCode>
									<settlement>Adelphi</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">G</forename><surname>Rogers Iii</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Army Research Laboratory</orgName>
								<orgName type="institution">U.S</orgName>
								<address>
									<postCode>20783</postCode>
									<settlement>Adelphi</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Sanchez Amaro</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Army Research Laboratory</orgName>
								<orgName type="institution">U.S</orgName>
								<address>
									<postCode>20783</postCode>
									<settlement>Adelphi</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Marge</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Army Research Laboratory</orgName>
								<orgName type="institution">U.S</orgName>
								<address>
									<postCode>20783</postCode>
									<settlement>Adelphi</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clare</forename><forename type="middle">R</forename><surname>Voss</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Army Research Laboratory</orgName>
								<orgName type="institution">U.S</orgName>
								<address>
									<postCode>20783</postCode>
									<settlement>Adelphi</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Traum</surname></persName>
							<email>traum@ict.usc.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">USC Institute for Creative Technologies</orgName>
								<address>
									<postCode>90094</postCode>
									<settlement>Playa Vista</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ScoutBot: A Dialogue System for Collaborative Navigation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics-System Demonstrations</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics-System Demonstrations <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="93" to="98"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>93</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>ScoutBot is a dialogue interface to physical and simulated robots that supports collaborative exploration of environments. The demonstration will allow users to issue unconstrained spoken language commands to ScoutBot. ScoutBot will prompt for clarification if the user&apos;s instruction needs additional input. It is trained on human-robot dialogue collected from Wizard-of-Oz experiments, where robot responses were initiated by a human wizard in previous interactions. The demonstration will show a simulated ground robot (Clearpath Jackal) in a simulated environment supported by ROS (Robot Operating System).</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We are engaged in a long-term project to create an intelligent, autonomous robot that can collab- orate with people in remote locations to explore the robot's surroundings. The robot's capabili- ties will enable it to effectively use language and other modalities in a natural manner for dialogue with a human teammate. This demo highlights the current stage of the project: a data-driven, auto- mated system, ScoutBot, that can control a simu- lated robot with verbally issued, natural language instructions within a simulated environment, and can communicate in a manner similar to the in- teractions observed in prior Wizard-of-Oz experi- ments. We used a Clearpath Jackal robot (shown in <ref type="figure" target="#fig_0">Figure 1a</ref>), a small, four-wheeled unmanned ground vehicle with an onboard CPU and iner- tial measurement unit, equipped with a camera and light detection and ranging (LIDAR) mapping that * Contributions were primarily performed during an in- ternship at the Institute for Creative <ref type="bibr">Technologies.</ref> readily allows for automation. The robot's task is to navigate through an urban environment (e.g., rooms in an apartment or an alleyway between apartments), and communicate discoveries to a hu- man collaborator (the Commander). The Com- mander verbally provides instructions and guid- ance for the robot to navigate the space.</p><p>The reasons for an intelligent robot collabora- tor, rather than one teleoperated by a human, are twofold. First, the human resources needed for completely controlling every aspect of robot mo- tion (including low-level path-planning and nav- igation) may not be available. Natural language allows for high-level tasking, specifying a desired plan or end-point, such that the robot can figure out the details of how to execute these natural lan- guage commands in the given context and report back to the human as appropriate, requiring less time and cognitive load on humans. Second, the interaction should be robust to low-bandwidth and unreliable communication situations (common in disaster relief and search-and-rescue scenarios), where it may be impossible or impractical to ex- ercise real-time control or see full video streams. Natural language interaction coupled with low- bandwidth, multimodal updates addresses both of these issues and provides less need for high- bandwidth, reliable communication and full atten- tion of a human controller.</p><p>We have planned the research for developing ScoutBot as consisting of five conceptual phases, each culminating in an experiment to validate the approach and collect evaluation data to inform the development of the subsequent phase. These phases are: The first two phases are described in <ref type="bibr" target="#b5">Marge et al. (2016)</ref> and <ref type="bibr" target="#b0">Bonial et al. (2017)</ref>, respec- tively. Both consist of Wizard-of-Oz settings in which participants believe that they are interact- ing with an autonomous robot, a common tool in Human-Robot Interaction for supporting not-yet fully realized algorithms <ref type="bibr" target="#b6">(Riek, 2012)</ref>. In our two- wizard design, one wizard (the Dialogue Man- ager or DM-Wizard) handles the communication aspects, while another (the Robot Navigator or RN-Wizard) handles robot navigation. The DM- Wizard acts as an interpreter between the Com- mander and robot, passing simple and full instruc- tions to the RN-Wizard for execution based on the Commander instruction (e.g., the Commander in- struction, Now go and make a right um 45 degrees is passed to the RN-Wizard for execution as, Turn right 45 degrees). In turn, the DM-Wizard informs the Commander of instructions the RN-Wizard successfully executes or of problems that arise during execution. Unclear instructions from the Commander are clarified through dialogue with the DM-Wizard (e.g., How far should I turn?). Additional discussion between Commander and DM-Wizard is allowed at any time. Note that be- cause of the aforementioned bandwidth and relia- bility issues, it is not feasible for the robot to start turning or moving and wait for the Commander to tell it when to stop -this may cause the robot to move too far, which could be dangerous in some circumstances and confusing in others. In the first phase, the DM-Wizard uses unconstrained texting to send messages to both the Commander and RN- Wizard. In the second phase, the DM-Wizard uses a click-button interface that facilitates faster messaging. The set of DM-Wizard messages in this phase were constrained based on the messages from the first phase.</p><p>This demo introduces ScoutBot automation of the robot to be used in the upcoming phases: a simulated robot and simulated environment to sup- port the third and fourth project phases, and initial automation of DM and RN roles, to be used in the fourth and fifth phases. Simulation and automa- tion were based on analyses from data collected in the first two phases. Together, the simulated envi- ronment and robot allow us to test the automated system in a safe environment, where people, the physical robot, and the real-world environment are not at risk due to communication or navigation er- rors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Capabilities</head><p>ScoutBot engages in collaborative exploration di- alogues with a human Commander, and controls a robot to execute instructions and navigate through and observe an environment. The real-world Jackal robot measures 20in x 17in x 10in, and weights about 37lbs (pictured in <ref type="figure" target="#fig_0">Figure 1a</ref> Interactions with ScoutBot are collaborative in that ScoutBot and the Commander exchange in- formation through dialogue. Commanders issue spoken navigation commands to the robot, request photos, or ask questions about its knowledge and perception. ScoutBot responds to indicate when commands are actionable, and gives status feed- back on its processing, action, or inability to act. When ScoutBot accepts a command, it is carried out in the world. <ref type="figure">Figure 2</ref> shows a dialogue between a Comman- der and ScoutBot. The right two columns show how the Commander's language was interpreted, and the action the robot takes. In this dialogue, the Commander begins by issuing a single instruction, Move forward (utterance #1), but the end-point is underspecified, so ScoutBot responds by request- ing additional information (#2). The Commander supplies the information in #3, and the utterance is understood as an instruction to move forward 3 feet. ScoutBot provides feedback to the Comman- der as the action is begins (#4) and upon comple- tion (#5). Another successful action is executed in #6-8. Finally, the Commander's request to know what the robot sees in #9 is interpreted as a re-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Move forward</head><p>You can tell me to move a certain distance or to move to an object.</p><p>1.</p><p>2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">move forward 3 feet</head><p>Go  <ref type="figure">Figure 2</ref>: Dialogue between a Commander and ScoutBot quest for a picture from the robot's camera, which is taken and sent to the Commander. ScoutBot accepts unconstrained spoken lan- guage, and uses a statistical text classifier trained on annotated data from the first two phases of the project for interpreting Commander instruc- tions. Dialogue management policies are used to generate messages to both the Commander and interpreted actions for automated robot naviga- tion. Our initial dialogue management policies are fairly simple, yet are able to handle a wide range of phenomena seen in our domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">System Overview</head><p>ScoutBot consists of several software systems, running on multiple machines and operating sys- tems, using two distributed agent platforms. The architecture utilized in simulation is shown in <ref type="figure" target="#fig_2">Fig- ure 3</ref>. A parallel architecture exists for a real- world robot and environment. Components pri- marily involving language processing use parts of the Virtual Human (VH) architecture <ref type="bibr" target="#b1">(Hartholt et al., 2013)</ref>, and communicate using the Virtual Human Messaging (VHMSG), a thin layer on top of the publicly available ActiveMQ message pro- tocol <ref type="bibr">1</ref> . Components primarily involving the real or simulated world, robot locomotion, and sens- ing are embedded in ROS 2 . To allow the VHMSG and ROS modules to interact with each other, we created ros2vhmsg, software that bridges the two messaging architectures. The components are de- scribed in the remainder of this section.</p><p>The system includes several distinct worksta- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">VHMSG Components</head><p>VHMSG includes several protocols that imple- ment parts of the Virtual Human architecture. We use the protocols for speech recognition, as well as component monitoring and logging. The NPCEd- itor and other components that use these proto- cols are available as part of the ICT Virtual Hu- man Toolkit <ref type="figure" target="#fig_0">(Hartholt et al., 2013</ref>). These proto- cols have also been used in systems, as reported by <ref type="bibr" target="#b2">Hill et al. (2003)</ref> and <ref type="bibr">Traum et al. (2012</ref><ref type="bibr" target="#b8">Traum et al. ( , 2015</ref>. In particular, we used the adaptation of Google's Automated Speech Recognition (ASR) API used in <ref type="bibr" target="#b8">Traum et al. (2015)</ref>. The NPCEditor ( <ref type="bibr" target="#b4">Leuski and Traum, 2011</ref>) was used for Natural Language Understanding (NLU) and dialogue management.</p><p>The new ros2vhmsg component for bridging the messages was used to send instructions from the NPCEditor to the automated RN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">NPCEditor</head><p>We implemented NLU using the statistical text classifier included in the NPCEditor. The classi- fier learns a mapping from inputs to outputs from training data using cross-language retrieval mod- els ( <ref type="bibr" target="#b4">Leuski and Traum, 2011</ref>). The dialogues collected from our first two experimental phases served as training data, and consisted of 1,500 pairs of Commander (user) utterances and the DM-Wizard's responses. While this approach lim- its responses to the set that were seen in the train- ing data, in practice it works well in our domain, achieving accuracy on a held out test set of 86%.</p><p>The system is particularly effective at translat- ing actionable commands to the RN for execu- tion (e.g., Take a picture). It is robust at handling commands that include pauses, fillers, and other disfluent features (e.g., Uh move um 10 feet). It can also handle simple metric-based motion com- mands (e.g., Turn right 45 degrees, Move forward 10 feet) as well as action sequences (e.g., Turn 180 degrees and take a picture every 45). The system can interpret some landmark-based instructions re- quiring knowledge of the environment (e.g., Go to the orange cone), but the automated RN compo- nent does not yet have the capability to generate the low-level, landmark-based instructions for the robot.</p><p>Although the NPCEditor supports simultane- ous participation in multiple conversations, exten- sions were needed for ScoutBot to support multi- floor interaction <ref type="bibr">(Traum et al., 2018)</ref>, in which two conversations are linked together. Rather than just responding to input in a single conversation, the DM-Wizard in our first project phases often translates input from one conversational floor to another (e.g., from the Commander to the RN- Wizard, or visa versa), or responds to input with messages to both the Commander and the RN. These responses need to be consistent (e.g. trans- lating a command to the RN should be accom- panied by positive feedback to the Commander, while a clarification to the commander should not include an RN action command). Using the di- alogue relation annotations described in <ref type="bibr">Traum et al. (2018)</ref>, we trained a hybrid classifier, includ- ing translations to RN if they existed, and negative feedback to the Commander where they did not. We also created a new dialogue manager policy that would accompany RN commands with appro- priate positive feedback to the commander, e.g., response of "Moving.." vs "Turning..." as seen in #4 and #7 in <ref type="figure">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">ros2vhmsg</head><p>ros2vhmsg is a macOS application to bridge the VHMSG and ROS components of ScoutBot. It can be run from a command line or using a graph- ical user interface that simplifies the configura- tion of the application. Both VHMSG and ROS are publisher-subscriber architectures with a cen- tral broker software. Each component connects to the broker. They publish messages by delivering them to the broker and receive messages by telling the broker which messages they want to receive. When the broker receives a relevant message, it is delivered to the subscribers. ros2vhmsg registers itself as a client for both VHMSG and ROS brokers, translating and reissu- ing the messages. For example, when ros2vhmsg receives a message from the ROS broker, it con- verts the message into a compatible VHMSG for- mat and publishes it with the VHMSG broker. Within ros2vhmsg, the message names to translate along with the corresponding ROS message types must be specified. Currently, the application sup- ports translation for messages carrying either text or robot navigation commands. The application is flexible and easily extendable with additional mes- sage conversion rules. ros2vhmsg annotates its messages with a special metadata symbol and uses that information to avoid processing messages that it already published. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">ROS Components</head><p>ROS provides backend support for robots to oper- ate in both real-world and simulated environments. Here, we describe our simulated testbed and auto- mated navigation components. Developing auto- mated components in simulation allows for a safe test space before software is transitioned to the real-world on a physical robot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Environment Simulator</head><p>Running operations under ROS makes the tran- sition between real-world and simulated testing straightforward. Gazebo 3 is a software package compatible with ROS for rendering high-fidelity virtual simulations, and supports communications in the same manner as one would in real-world en- vironments. Simulated environments were mod- eled in Gazebo after their real-world counterparts as shown in <ref type="figure" target="#fig_3">Figure 4</ref>. Replication took into con- sideration the general dimensions of the physical space, and the location and size of objects that populate the space. Matching the realism and fi- delity of the real-world environment in simulation comes with a rendering trade-off: objects requir- ing a higher polygon count due to their natural ge- ometries results in slower rendering in Gazebo and a lag when the robot moves in the simulation. As a partial solution, objects were constructed starting from their basic geometric representations, which could be optimized accordingly, e.g., the illusion of depth was added with detailed textures or shad- ing on flat-surfaced objects. Environments ren- dered in Gazebo undergo a complex workflow to support the aforementioned requirements. Objects and their vertices are placed in an environment, and properties about collisions and gravity are en- coded in the simulated environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Jackal Simulator</head><p>Clearpath provides a ROS package with a simu- lated model of the Jackal robot <ref type="figure" target="#fig_0">(Figure 1b</ref>) and customizable features to create different simu- lated configurations. We configured our simulated Jackal to have the default inertial measurement unit, a generic camera, and a Hokuyo LIDAR laser scanner.</p><p>As the simulated Jackal navigates through the Gazebo environment, data from the sensors is re- layed to the workstation views through rviz 4 , a vi- sualization tool included with ROS that reads and displays sensor data in real-time. Developers can select the data to display by adding a panel, se- lecting the data type, and then selecting the spe- cific ROS data stream. Both physical and simu- lated robot sensors are supported by the same rviz configuration, since rviz only processes the data sent from these sensors; this means that an rviz configuration created for data from a simulated robot will also work for a physical robot if the data stream types remain the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Automated Robot Navigator</head><p>Automated robot navigation is implemented with a python script and the ROSPY package 5 . The script runs on a ROS-enabled machine running the sim- ulation. The script subscribes to messages from NPCEditor, which are routed through ros2vhmsg. These messages contain text output from the NLU classifier that issue instructions to the robot based on the user's unconstrained speech.</p><p>A mapping of pre-defined instructions was cre- ated, with keys matching the strings passed from NPCEditor via ros2vhmsg (e.g., Turn left 90 de- grees). For movement, the map values are a ROS TWIST message that specifies the linear motion and angular rotation payload for navigation. These TWIST messages are the only way to manipulate the robot; measures in units of feet and degrees cannot be directly translated. The mapping is straightforward to define for metric instructions. Included is basic coverage of moving forward or backward between 1 and 10 feet, and turning right and left 45, 90, 180, or 360 degrees. Pictures from the robot's simulated camera can be requested by sending a ROS IMAGE message. The current map- ping does not yet support collision avoidance or landmark-based instructions that require knowl- edge of the surrounding environment (e.g., Go to the nearby chair).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Demo Summary</head><p>In the demo, visitors will see the simulated robot dynamically move in the simulated environment, guided by natural language interaction. Visitors will be able to speak instructions to the robot to move in the environment. Commands can be given in metric terms (e.g., #3 and #6 in <ref type="figure">Figure 2)</ref>, and images requested (e.g., #9). Undecipherable or incomplete instructions will result in clarifica- tion subdialogues rather than robot motion (e.g., #1). A variety of command formulations can be accommodated by the NLU classifier based on the training data from our experiments. Visual- izations of different components can be seen in: http://people.ict.usc.edu/ Ëœ traum/ Movies/scoutbot-acl2018demo.wmv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Summary and Future Work</head><p>ScoutBot serves as a research platform to support experimentation. ScoutBot components will be used in our upcoming third through fifth develop- ment phases. We are currently piloting phase 3 using ScoutBot's simulated environment, with hu- man wizards. Meanwhile, we are extending the automated dialogue and navigation capabilities.</p><p>This navigation task holds potential for collab- oration policies to be studied, such as the amount and type of feedback given, how to negotiate to successful outcomes when an initial request was underspecified or impossible to carry out, and the impact of miscommunication. More sophisticated NLU methodologies can be tested, including those that recognize specific slots and values or more de- tailed semantics of spatial language descriptions. The use of context, particularly references to rel- ative landmarks, can be tested by either using the assumed context as part of the input to the NLU classifier, transforming the input before clas- sifying, or deferring the resolution and requiring the action interpreter to handle situated context- dependent instructions ( <ref type="bibr" target="#b3">Kruijff et al., 2007)</ref>.</p><p>Some components of the ScoutBot architecture may be substituted for different needs, such as dif- ferent physical or simulated environments, robots, or tasks. Training new DM and RN components can make use of this general architecture, and the resulting components aligned back with ScoutBot.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 .</head><label>1</label><figDesc>Figure 1: Jackal Robot</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>). Both it and its simulated counterpart (as seen in Fig- ure 1b) can drive around the environment, but can- not manipulate objects or otherwise interact with the environment. While navigating, the robot uses LIDAR to construct a map of its surroundings as well as to indicate its position in the map. The Commander is shown this information, as well as static photographs of the robot's frontal view in the environment (per request) and dialogue responses in a text interface.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1Figure 3 :</head><label>3</label><figDesc>Figure 3: ScoutBot architecture interfacing with a simulated robot and environment: Solid arrows represent communications over a local network; dashed arrows indicate connections with external resources. Messaging for the spoken language interface is handled via VHMSG, while robot messages are facilitated by ROS. A messaging bridge, ros2vhmsg, connects the components.</figDesc><graphic url="image-5.png" coords="3,359.28,356.70,165.21,56.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Real-world and simulated instances of the same environment.</figDesc></figure>

			<note place="foot" n="3"> http://gazebosim.org/</note>

			<note place="foot" n="4"> http://wiki.ros.org/rviz 5 http://wiki.ros.org/rospy</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the U.S. Army.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Laying Down the Yellow Brick Road: Development of a Wizard-of-Oz Interface for Collecting Human-Robot Dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Bonial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Marge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashley</forename><surname>Foots</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Gervits</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cory</forename><forename type="middle">J</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cassidy</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Susan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leuski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pooja</forename><surname>Stephanie M Lukin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimberly</forename><forename type="middle">A</forename><surname>Moolchandani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clare</forename><forename type="middle">R</forename><surname>Traum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Voss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI Fall Symposium Series</title>
		<meeting>of AAAI Fall Symposium Series</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">All Together Now Introducing the Virtual Human Toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arno</forename><surname>Hartholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Traum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Marsella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giota</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Stratou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louisphilippe</forename><surname>Leuski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gratch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Intelligent Virtual Agents</title>
		<meeting>of Intelligent Virtual Agents</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="368" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Virtual Humans in the Mission Rehearsal Exercise System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randall</forename><surname>Hill</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Gratch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stacy</forename><surname>Marsella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Rickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Swartout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Traum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KI Embodied Conversational Agents</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="32" to="38" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Incremental, Multi-Level Processing for Comprehending Situated Dialogue in Human-Robot Interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geert-Jan M</forename><surname>Kruijff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Lison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Benjamin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><surname>Jacobsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Hawes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Language and Robots</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">NPCEditor: Creating virtual human dialogue using information retrieval techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Leuski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Traum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>AI Magazine</publisher>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="42" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Assessing Agreement in Human-Robot Dialogue Strategies: A Tale of Two Wizards</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Marge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Bonial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimberly</forename><forename type="middle">A</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Artstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Byrne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Susan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clare</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Traum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Intelligent Virtual Agents</title>
		<meeting>of Intelligent Virtual Agents</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="484" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Wizard of Oz Studies in HRI: A Systematic Review and New Reporting Guidelines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurel</forename><surname>Riek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Human-Robot Interaction</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Athanasios Katsamanis, Anton Leuski, Dan Noren, and William Swartout. 2012. Ada and Grace: Direct Interaction with Museum Visitors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Traum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priti</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Artstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><surname>Foutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jillian</forename><surname>Gerten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Intelligent Virtual Agents</title>
		<meeting>of Intelligent Virtual Agents</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Evaluating Spoken Dialogue Processing for Time-Offset Interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Traum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kallirroi</forename><surname>Georgila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Artstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Leuski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Special Interest Group on Discourse and Dialogue</title>
		<meeting>of Special Interest Group on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="199" to="208" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
