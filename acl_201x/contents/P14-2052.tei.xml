<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:06+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Single Document Summarization based on Nested Tree Structure</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Kikuchi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsutomu</forename><surname>Hirao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroya</forename><surname>Takamura</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Tokyo Institute of technology 4295</orgName>
								<address>
									<addrLine>Midori-ku</addrLine>
									<postCode>226-8503</postCode>
									<settlement>Nagatsuta, Yokohama</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">NTT Communication Science Laboratories</orgName>
								<orgName type="institution" key="instit2">NTT Corporation</orgName>
								<address>
									<addrLine>2-4, Hikaridai, Seika-cho, Soraku-gun</addrLine>
									<postCode>619-0237</postCode>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Single Document Summarization based on Nested Tree Structure</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="315" to="320"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Many methods of text summarization combining sentence selection and sentence compression have recently been proposed. Although the dependency between words has been used in most of these methods, the dependency between sentences , i.e., rhetorical structures, has not been exploited in such joint methods. We used both dependency between words and dependency between sentences by constructing a nested tree, in which nodes in the document tree representing dependency between sentences were replaced by a sentence tree representing dependency between words. We formulated a sum-marization task as a combinatorial optimization problem, in which the nested tree was trimmed without losing important content in the source document. The results from an empirical evaluation revealed that our method based on the trimming of the nested tree significantly improved the summarization of texts.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Extractive summarization is one well-known ap- proach to text summarization and extractive meth- ods represent a document (or a set of documents) as a set of some textual units (e.g., sentences, clauses, and words) and select their subset as a summary. Formulating extractive summarization as a combinational optimization problem greatly improves the quality of summarization <ref type="bibr" target="#b16">(McDonald, 2007;</ref><ref type="bibr" target="#b6">Filatova and Hatzivassiloglou, 2004;</ref>. There has re- cently been increasing attention focused on ap- proaches that jointly optimize sentence extraction and sentence compression ( <ref type="bibr" target="#b21">Tomita et al., 2009;</ref><ref type="bibr" target="#b19">Qian and Liu, 2013;</ref><ref type="bibr" target="#b17">Morita et al., 2013;</ref><ref type="bibr" target="#b8">Gillick and Favre, 2009;</ref><ref type="bibr" target="#b0">Almeida and Martins, 2013;</ref><ref type="bibr" target="#b1">Berg-Kirkpatrick et al., 2011</ref>). We can only ex- tract important content by trimming redundant parts from sentences.</p><p>However, as these methods did not include the discourse structures of documents, the generated summaries lacked coherence. It is important for generated summaries to have a discourse struc- ture that is similar to that of the source docu- ment. Rhetorical Structure Theory (RST) ( <ref type="bibr" target="#b14">Mann and Thompson, 1988</ref>) is one way of introduc- ing the discourse structure of a document to a summarization task <ref type="bibr" target="#b15">(Marcu, 1998;</ref><ref type="bibr" target="#b4">Daumé III and Marcu, 2002;</ref><ref type="bibr" target="#b10">Hirao et al., 2013</ref>). Hirao et al. recently transformed RST trees into dependency trees and used them for single document summa- rization ( <ref type="bibr" target="#b10">Hirao et al., 2013</ref>). They formulated the summarization problem as a tree knapsack prob- lem with constraints represented by the depen- dency trees.</p><p>We propose a method of summarizing a single document that utilizes dependency between sen- tences obtained from rhetorical structures and de- pendency between words obtained from a depen- dency parser. We have explained our method with an example in <ref type="figure">Figure 1</ref>. First, we represent a doc- ument as a nested tree, which is composed of two types of tree structures: a document tree and a sentence tree. The document tree is a tree that has sentences as nodes and head modifier relationships between sentences obtained by RST as edges. The sentence tree is a tree that has words as nodes and head modifier relationships between words obtained by the dependency parser as edges. We can build the nested tree by regarding each node of the document tree as a sentence tree. Finally, we formulate the problem of single document sum- marization as that of combinatorial optimization, which is based on the trimming of the nested tree. John was running on a track in the park.</p><p>He looks very tired. Mike said he is training for a race.</p><p>The race is held next month.</p><p>Source document John was running on a track in the park. He looks very tired. Mike said he is training for a race. The race is held next month.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>John was running on a track. he is training for a race. * The race is held next month.</p><p>Figure 1: Overview of our method. The source document is represented as a nested tree. Our method simultaneously selects a rooted document subtree and sentence subtree from each node.</p><p>Our method jointly utilizes relations between sen- tences and relations between words, and extracts a rooted document subtree from a document tree whose nodes are arbitrary subtrees of the sentence tree.</p><p>Elementary Discourse Units (EDUs) in RST are defined as the minimal building blocks of dis- course. EDUs roughly correspond to clauses. Most methods of summarization based on RST use EDUs as extraction textual units. We converted the rhetorical relations between EDUs to the re- lations between sentences to build the nested tree structure. We could thus take into account both relations between sentences and relations between words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Extracting a subtree from the dependency tree of words is one approach to sentence compression ( <ref type="bibr" target="#b21">Tomita et al., 2009;</ref><ref type="bibr" target="#b19">Qian and Liu, 2013;</ref><ref type="bibr" target="#b17">Morita et al., 2013;</ref><ref type="bibr" target="#b8">Gillick and Favre, 2009)</ref>. However, these studies have only extracted rooted subtrees from sentences. We allowed our model to extract a subtree that did not include the root word (See the sentence with an asterisk * in <ref type="figure">Figure 1</ref>). The method of <ref type="bibr" target="#b7">Filippova and Strube (2008)</ref> allows the model to extract non-rooted subtrees in sentence compression tasks that compress a single sentence with a given compression ratio. However, it is not trivial to apply their method to text summariza- tion because no compression ratio is given to sen- tences. None of these methods use the discourse structures of documents.</p><p>Daumé III and Marcu (2002) proposed a noisy- channel model that used RST. Although their method generated a well-organized summary, no optimality of information coverage was guaran- teed and their method could not accept large texts because of the high computational cost. In addi- -The scare over Alar, a growth regulator -that makes apples redder and crunchier -but may be carcinogenic, -made consumers shy away from the Delicious, -though they were less affected than the McIntosh. tion, their method required large sets of data to cal- culate the accurate probability. There have been some studies that have used discourse structures locally to optimize the order of selected sentences ( <ref type="bibr" target="#b18">Nishikawa et al., 2010;</ref><ref type="bibr" target="#b3">Christensen et al., 2013</ref>).</p><p>3 Generating summary from nested tree</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Building Nested Tree with RST</head><p>A document in RST is segmented into EDUs and adjacent EDUs are linked with rhetorical relations to build an RST-Discourse Tree (RST-DT) that has a hierarchical structure of the relations. There are 78 types of rhetorical relations between two spans, and each span has one of two aspects of a nu- cleus and a satellite. The nucleus is more salient to the discourse structure, while the other span, the satellite, represents supporting information. RST- DT is a tree whose terminal nodes correspond to EDUs and whose nonterminal nodes indicate the relations. Hirao et al. converted RST-DTs into dependency-based discourse trees (DEP-DTs) whose nodes corresponded to EDUs and whose edges corresponded to the head modifier relation- ships of EDUs. See Hirao et al. for details ( <ref type="bibr" target="#b10">Hirao et al., 2013)</ref>.</p><p>Our model requires sentence-level dependency. Fortunately we can simply convert DEP-DTs to obtain dependency trees between sentences. We specifically merge EDUs that belong to the same sentence. Each sentence has only one root EDU that is the parent of all the other EDUs in the sen- tence. Each root EDU in a sentence has the parent max.</p><formula xml:id="formula_0">n ∑ i m i ∑ j w ij z ij s.t. ∑ n i ∑ m i j z ij ≤ L;<label>(1)</label></formula><formula xml:id="formula_1">x parent(i) ≥ x i ; ∀i (2) z parent(i,j) − z ij + r ij ≥ 0; ∀i, j (3) x i ≥ z ij ; ∀i, j (4) ∑ m i j z ij ≥ min(θ, len(i))x i ; ∀i (5) ∑ m i j r ij = x i ; ∀i (6) ∑ j / ∈Rc(i) r ij = 0; ∀i (7) r ij ≤ z ij ; ∀i, j (8) r ij + z parent(i,j) ≤ 1; ∀i, j (9) r iroot(i) = z iroot(i) ; ∀i (10) ∑ j∈sub(i) z ij ≥ x i ; ∀i (11) ∑ j∈obj(i) z ij ≥ x i ; ∀i (12) Figure 3: ILP formulation (x i , z ij , r ij ∈ {0, 1})</formula><p>EDU in another sentence. Hence, we can deter- mine the parent-child relations between sentences.</p><p>As a result, we obtain a tree that represents the parent-child relations of sentences, and we can use it as a document tree. After the document tree is obtained, we use a dependency parser to obtain the syntactic dependency trees of sentences. Finally, we obtain a nested tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">ILP formulation</head><p>Our method generates a summary by trimming a nested tree. In particular, we extract a rooted docu- ment subtree from the document tree, and sentence subtrees from sentence trees in the document tree.</p><p>We formulate our problem of optimization in this section as that of integer linear programming. Our model is shown in <ref type="figure">Figure 3</ref>. Let us denote by w ij the term weight of word ij (word j in sentence i). x i is a variable that is one if sentence i is selected as part of a sum- mary, and z ij is a variable that is one if word ij is selected as part of a summary. According to the objective function, the score for the resulting sum- mary is the sum of the term weights w ij that are included in the summary. We denote by r ij the variable that is one if word ij is selected as a root of an extracting sentence subtree. Constraint (1) guarantees that the summary length will be less than or equal to limit L. Constraints <ref type="formula">(2)</ref> and <ref type="formula">(3)</ref> are tree constraints for a document tree and sen- tence trees. r ij in Constraint (3) allows the system to extract non-rooted sentence subtrees, as we pre- viously mentioned. Function parent(i) returns the parent of sentence i and function parent(i, j) re- turns the parent of word ij. Constraint (4) guaran- tees that words are only selected from a selected sentence. <ref type="bibr">Constraint (5)</ref> </p><note type="other">guarantees that each se- lected sentence subtree has at least θ words. Func- tion len(i) returns the number of words in sentence i. Constraints (6)-(10) allow the model to extract subtrees that have an arbitrary root node. Con- straint (6) guarantees that there is only one root per selected sentence. We can set the candidate for the root node of the subtree by using constraint (7). The R c (i) returns a set of the nodes that are the candidates of the root nodes in sentence i. It returned the parser's root node and the verb nodes in this study. Constraint (8) maintains consistency between z ij</note><p>and r ij . Constraint (9) prevents the system from selecting the parent node of the root node. Constraint (10) guarantees that the parser's root node will only be selected when the system extracts a rooted sentence subtree. The root(i) re- turns the word index of the parser's root. Con- straints (11) and (12) guarantee that the selected sentence subtree has at least one subject and one object if it has any. The sub(i) and obj(i) return the word indices whose dependency tag is "SUB" and "OBJ".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Additional constraint for grammaticality</head><p>We added two types of constraints to our model to extract a grammatical sentence subtree from a dependency tree: </p><formula xml:id="formula_2">z ik = z il ,<label>(13)</label></formula><formula xml:id="formula_3">z ik = |s(i, j)|x i .<label>(14)</label></formula><p>Equation <ref type="formula" target="#formula_0">(13)</ref> means that words z ik and z il have to be selected together, i.e., a word whose depen- dency tag is PMOD or VC and its parent word, a negation and its parent word, a word whose de- pendency tag is SUB or OBJ and its parent verb, a comparative (JJR) or superlative (JJS) adjective and its parent word, an article (a/the) and its par- ent word, and the word "to" and its parent word. Equation <ref type="bibr">(14)</ref> means that the sequence of words has to be selected together, i.e., a proper noun se- quence whose POS tag is PRP$, WP%, or POS and a possessive word and its parent word and the words between them. The s(i, j) returns the set of word indices that are selected together with word ij.  <ref type="bibr">, 1998)</ref>. We used ROUGE <ref type="bibr" target="#b13">(Lin, 2004</ref>) as an eval- uation criterion.</p><p>We compared our method (sentence subtree) with that of EDU selection ( <ref type="bibr" target="#b10">Hirao et al., 2013</ref>). We examined two other methods, i.e., rooted sen- tence subtree and sentence selection. These two are different from our method in the way that they select a sentence subtree. Rooted sentence subtree only selects rooted sentence subtrees 2 . Sentence selection does not trim sentence trees. It simply selects full sentences from a document tree <ref type="bibr">3</ref> . We built all document trees from the RST-DTs that were annotated in the corpus.</p><p>We set the term weight, w ij , for our model as:</p><formula xml:id="formula_4">w ij = log(1 + tf ij ) depth(i) 2 ,<label>(15)</label></formula><p>where tf ij is the term frequency of word ij in a document and depth(i) is the depth of sentence i within the sentence-level DEP-DT that we de- scribed in Section 3.1. For Constraint (5), we set θ to eight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Comparing ROUGE scores</head><p>We have summarized the Recall-Oriented Under- study for Gisting Evaluation (ROUGE) scores for each method in <ref type="table" target="#tab_2">Table 1</ref>. The score for sentence selection is low (0.254). However, introducing sentence compression to the system greatly im- proved the ROUGE score (0.354). The score is also higher than that with EDU selection, which is a state-of-the-art method. We applied a multi- ple test by using Holm's method and found that our method significantly outperformed EDU se- lection and sentence selection. The difference be- tween the sentence subtree and the rooted sentence subtree methods was fairly small. We therefore qualitatively analyzed some actual examples that will be discussed in Section 4.2.2. We also exam- ined the ROUGE scores of two LEAD 4 methods with different textual units: EDUs (LEAD EDU ) and sentences (LEAD SNT ). Although LEAD works well and often obtains high ROUGE scores for news articles, the scores for LEAD EDU and LEAD SNT were very low.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Qualitative Evaluation of Sentence Subtree Selection</head><p>This subsection compares the methods of subtree selection and rooted subtree selection. <ref type="figure">Figure 4</ref> has two example sentences for which both meth- ods selected a subtree as part of a summary. The {·} indicates the parser's root word. The <ref type="bibr">[·]</ref> indi- cates the word that the system selected as the root of the subtree. Subtree selection selected a root in both examples that differed from the parser's root.</p><p>As we can see, subtree selection only selected im- portant subtrees that did not include the parser's root, e.g., purpose-clauses and that-clauses. This capability is very effective because we have to contain important content in summaries within given length limits, especially when the compres- sion ratio is high (i.e., the method has to gener- ate much shorter summaries than the source docu- ments).</p><p>Original sentence : John Kriz, a Moody's vice president, {said} Boston Safe Deposit's performance has been hurt this year by a mismatch in the maturities of its assets and liabilities. Rooted subtree selection : John Kriz a Moody's vice president [{said}] Boston Safe Deposit's performance has been hurt this year Subtree selection : Boston Safe Deposit's performance has <ref type="bibr">[been]</ref> hurt this year Original sentence : Recent surveys by Leo J. Shapiro &amp; Associates, a market research firm in Chicago, {suggest} that Sears is having a tough time attracting shoppers because it hasn't yet done enough to improve service or its selection of merchandise. Rooted subtree selection : surveys <ref type="bibr">[{suggest}]</ref> that Sears is having a time Subtree selection : Sears [is] having a tough time attracting shoppers <ref type="figure">Figure 4</ref>: Example sentences and subtrees selected by each method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Fragmentation of Information</head><p>Many studies that have utilized RST have simply adopted EDUs as textual units <ref type="bibr" target="#b14">(Mann and Thompson, 1988;</ref><ref type="bibr" target="#b4">Daumé III and Marcu, 2002;</ref><ref type="bibr" target="#b10">Hirao et al., 2013;</ref><ref type="bibr" target="#b12">Knight and Marcu, 2000</ref>). While EDUs are textual units for RST, they are too fine grained as textual units for methods of extractive summa- rization. Therefore, the models have tended to se- lect small fragments from many sentences to max- imize objective functions and have led to frag- mented summaries being generated. <ref type="figure" target="#fig_0">Figure 2</ref> has an example of EDUs. A fragmented summary is generated when small fragments are selected from many sentences. Hence, the number of sen- tences in the source document included in the re- sulting summary can be an indicator to measure the fragmentation of information. We counted the number of sentences in the source document that each method used to generate a summary 5 . The average for our method was 4.73 and its me- dian was four sentences. In contrast, methods of EDU selection had an average of 5.77 and a median of five sentences. This meant that our method generated a summary with a significantly smaller number of sentences 6 . In other words, our method relaxed fragmentation without decreasing the ROUGE score. There are boxplots of the num- bers of selected sentences in <ref type="figure" target="#fig_1">Figure 5</ref>. <ref type="table" target="#tab_3">Table 2</ref> lists the number of words in each textual unit extracted by each method. It indicates that EDUs are shorter than the other textual units. Hence, the number of sentences tends to be large. <ref type="bibr">5</ref> Note that the number for the EDU method is not equal to selected textual units because a sentence in the source docu- ment may contain multiple EDUs. <ref type="bibr">6</ref> We used the Wilcoxon signed-rank test (p &lt; 0.05).</p><p>John was running on a track in the park.</p><p>He looks very tired. Mike said he is trainning for a race.</p><p>The race is held on next month.</p><p>Source document John was running on a track in the park. He looks very tired. Mike said he is training for a race. The race is held on next month. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed a method of summarizing a sin- gle document that included relations between sen- tences and relations between words. We built a nested tree and formulated the problem of summa- rization as that of integer linear programming. Our method significantly improved the ROUGE score with significantly fewer sentences than the method of EDU selection. The results suggest that our method relaxed the fragmentation of information. We also discussed the effectiveness of sentence subtree selection that did not restrict rooted sub- trees. Although ROUGE scores are widely used as evaluation metrics for text summarization sys- tems, they cannot take into consideration linguis- tic qualities such as human readability. Hence, we plan to conduct evaluations with people <ref type="bibr">7</ref> . We only used the rhetorical structures between sentences in this study. However, there were also rhetorical structures between EDUs inside individ- ual sentences. Hence, utilizing these for sentence compression has been left for future work. In addi- tion, we used rhetorical structures that were man- ually annotated. There have been related studies on building RST parsers <ref type="bibr" target="#b5">(duVerle and Prendinger, 2009;</ref><ref type="bibr" target="#b9">Hernault et al., 2010)</ref> and by using such parsers, we should be able to apply our model to other corpora or to multi-document settings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example of one sentence. Each line corresponds to one EDU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Number of sentences that each method selected.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 1 : ROUGE score of each model. Note that the top two rows are both our proposals.</head><label>1</label><figDesc></figDesc><table>ROUGE-1 
Sentence subtree 
0.354 
Rooted sentence subtree 
0.352 
Sentence selection 
0.254 
EDU selection (Hirao et al., 2013) 
0.321 
LEADEDU 
0.240 
LEADsnt 
0.157 

4 Experiment 

4.1 Experimental Settings 

We experimentally evaluated the test collection for 
single document summarization contained in the 
RST Discourse Treebank (RST-DTB) (Carlson et 
al., 2001) distributed by the Linguistic Data Con-
sortium (LDC) 1 . The RST-DTB Corpus includes 
385 Wall Street Journal articles with RST anno-
tations, and 30 of these documents also have one 
manually prepared reference summary. We set the 
length constraint, L, as the number of words in 
each reference summary. The average length of 
the reference summaries corresponded to approxi-
mately 10% of the length of the source document. 
This dataset was first used by Marcu et al. for 
evaluating a text summarization system (Marcu</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Average number of words that individual 
extracted textual units contained. 

Subtree 
Sentence 
EDU 
15.29 
18.96 
9.98 

</table></figure>

			<note place="foot" n="1"> http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp? catalogId=LDC2002T07 2 We achieved this by making Rc(i) only return the parser&apos;s root node in Figure 7. 3 We achieved this by setting θ to a very large number.</note>

			<note place="foot" n="4"> LEAD methods simply take the first K textual units from a source document until the summary length reaches L.</note>

			<note place="foot" n="7"> For example, the quality question metric from the Document Understanding Conference (DUC).</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast and robust compressive summarization with dual decomposition and multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><surname>Martins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="page" from="196" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Jointly learning to extract and compress</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<meeting><address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="481" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Building a discourse-tagged corpus in the framework of rhetorical structure theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynn</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ellen</forename><surname>Okurowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGDIAL</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards coherent multidocument summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janara</forename><surname>Christensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Mausam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL:HLT</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1163" to="1173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A noisychannel model for document compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<biblScope unit="page" from="449" to="456" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A novel discourse parser based on support vector machine classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duverle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Prendinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNLP</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="665" to="673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A formal model for information selection in multisentence text extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Filatova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Hatzivassiloglou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dependency tree based sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Filippova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INLG</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A scalable global model for summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Favre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ILP</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hilda: A discourse parser using support vector machine classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Hernault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Prendinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitsuru</forename><surname>Ishizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dialogue &amp; Discourse</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsutomu</forename><surname>Hirao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuhisa</forename><surname>Yoshida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nishino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norihito</forename><surname>Yasuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Single-document summarization as a tree knapsack problem</title>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<biblScope unit="page" from="1515" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Statisticsbased summarization-step one: Sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">National Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="703" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL workshop on Text Summarization Branches Out</title>
		<meeting>ACL workshop on Text Summarization Branches Out</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><forename type="middle">A</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thompson</surname></persName>
		</author>
		<title level="m">Rhetorical structure theory: Toward a functional theory of text organization. Text</title>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="page" from="243" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving summarization through rhetorical parsing tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 6th Workshop on Very Large Corpora</title>
		<meeting>of the 6th Workshop on Very Large Corpora</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="206" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A study of global inference algorithms in multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">T</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECIR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="557" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Subtree extractive summarization via submodular maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajime</forename><surname>Morita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryohei</forename><surname>Sasano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroya</forename><surname>Takamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1023" to="1032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Opinion summarization with integer linear programming formulation for sentence extraction and ordering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hitoshi</forename><surname>Nishikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Hasegawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshihiro</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Genichiro</forename><surname>Kikui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="910" to="918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fast joint compression and summarization via graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1492" to="1502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Text summarization model based on the budgeted median problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroya</forename><surname>Takamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1589" to="1592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A new approach of extractive summarization combining sentence selection and compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kohei</forename><surname>Tomita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroya</forename><surname>Takamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IPSJ SIG Notes</title>
		<imprint>
			<biblScope unit="page" from="13" to="20" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
