<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:10+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Modelling Events through Memory-based, Open-IE Patterns for Abstractive Summarization</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Pighin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Google Inc</orgName>
								<orgName type="institution" key="instit2">University of Pisa</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cornolti</surname></persName>
							<email>cornolti@di.unipi.it</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Enrique Alfonseca Google Inc</orgName>
								<orgName type="institution" key="instit2">Katja Filippova Google Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Modelling Events through Memory-based, Open-IE Patterns for Abstractive Summarization</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="892" to="901"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>ive text summarization of news requires a way of representing events, such as a collection of pattern clusters in which every cluster represents an event (e.g., marriage) and every pattern in the cluster is a way of expressing the event (e.g., X married Y, X and Y tied the knot). We compare three ways of extracting event patterns: heuristics-based, compression-based and memory-based. While the former has been used previously in multi-document abstraction, the latter two have never been used for this task. Compared with the first two techniques, the memory-based method allows for generating significantly more grammatical and informative sentences, at the cost of searching a vast space of hundreds of millions of parse trees of known grammatical utterances. To this end, we introduce a data structure and a search method that make it possible to efficiently extrapolate from every sentence the parse sub-trees that match against any of the stored utterances.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text summarization beyond extraction requires a semantic representation that abstracts away from words and phrases and from which a summary can be generated <ref type="bibr" target="#b11">(Mani, 2001;</ref><ref type="bibr" target="#b18">Spärck-Jones, 2007)</ref>. Following and extending recent work in semantic parsing, information extraction (IE), paraphrase generation and summarization <ref type="bibr" target="#b19">(Titov and Klementiev, 2011;</ref><ref type="bibr" target="#b0">Alfonseca et al., 2013;</ref><ref type="bibr" target="#b21">Zhang and Weld, 2013;</ref><ref type="bibr" target="#b13">Mehdad et al., 2013)</ref>, the represen- tation we consider in this paper is a large collec- * Work done during an internship at Google Zurich.</p><p>[John Smith] and <ref type="bibr">[Mary Brown]</ref> wed in <ref type="bibr">[Baltimore]</ref>...</p><p>[Smith] tied the knot with <ref type="bibr">[Brown]</ref> this Monday... tion of clusters of event patterns. An abstractive summarization system relying on such a represen- tation proceeds by (1) detecting the most relevant event cluster for a given sentence or sentence col- lection, and (2) using the most representative pat- tern from the cluster to generate a concise sum- mary sentence. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the summa- rization architecture we are assuming in this pa- per. Given input text(s) with resolved and typed entity mentions, event mentions and the most rele- vant event cluster are detected (first arrow). Then, a summary sentence is generated from the event and entity representations (second arrow).</p><p>However, the utility of such a representation for summarization depends on the quality of pattern clusters. In particular, event patterns must cor- respond to grammatically correct sentences. In- troducing an incomplete or incomprehensible pat- tern (e.g., PER said PER) may negatively affect both event detection and sentence generation. Re- lated work on paraphrase detection and relation extraction is mostly heuristics-based and has re- lied on hand-crafted rules to collect such patterns (see <ref type="bibr">Sec. 2)</ref>. A standard approach is to focus on binary relations between entities and extract the dependency path between the two entities as an event representation. An obvious limitation of this approach is there is no guarantee that the extracted pattern corresponds to a grammatically correct sentence, e.g., that an essential preposi- tional phrase is retained like in file for a divorce.</p><p>In this paper we explore two novel, data-driven methods for event pattern extraction. The first, compression-based method uses a robust sentence compressor with an aggressive compression rate to get to the core of the sentence (Sec. 3). The second, memory-based method relies on a vast collection of human-written headlines and sen- tences to find a substructure which is known to be grammatically correct <ref type="bibr">(Sec. 4)</ref>. While the lat- ter method comes closer to ensuring perfect gram- maticality, it introduces a problem of efficiently searching the vast space of known well-formed patterns. Since standard iterative approaches com- paring every pattern with every sentence are pro- hibitive here, we present a search strategy which scales well to huge collections (hundreds of mil- lions) of sentences.</p><p>In order to evaluate the three methods, we con- sider an abstractive summarization task where the goal is to get the gist of single sentences by recog- nizing the underlying event and generating a short summary sentence. To the best of our knowledge, this is the first time that this task has been pro- posed; it can be considered as abstractive sentence compression, in contrast to most existing sentence compression systems which are based on selecting words from the original sentence or rewriting with simpler paraphrase tables. An extensive evalua- tion with human raters demonstrates the utility of the new pattern extraction techniques. Our analy- sis highlights advantages and disadvantages of the three methods.</p><p>To better isolate the qualities of the three ex- traction methodologies, all three methods use the same training data and share components of the Algorithm 1 HEURISTICEXTRACTOR(T, E): heuristi- cally extract relational patterns for the dependency parse T and the set of entities E.</p><p>1: /* Global constants /* 2: global Vp, Vc, Np, Nc 3: Vc ← {subj, nsubj, nsubjpass, dobj, iobj, xcomp, 4: acomp, expl, neg, aux, attr, prt} 5: Vp ← {xcomp} 6: Nc ← {det, predet, num, ps, poss, nc, conj} 7: Np ← {ps, poss, subj, nsubj, nsubjpass, dobj, iobj} 8: /* Entry point /* 9: P ← ∅ 10: for all C ∈ COMBINATIONS(E) do 11: N ← MENTIONNODES(T, C) 12: N ← APPLYHEURISTICS(T, BUILDMST(T, N ))</p><formula xml:id="formula_0">13: P ← P ∪ {BUILDPATTERN(T, N )} 14: return P 15: /* Procedures /* 16: procedure APPLYHEURISTICS(T, N ) 17: N ← N 18: while |N | &gt; 0 do 19: N ← ∅ 20:</formula><p>for all n ∈ N do 21:</p><p>if n.ISVERB() then 22:</p><formula xml:id="formula_1">N ← N ∪ INCLUDECHILDREN(n, Vc) 23: N ← N ∪ INCLUDEPARENT(n, Vp) 24: else if n.ISNOUN() then 25: N ← N ∪ INCLUDECHILDREN(n, Nc) 26: N ← N ∪ INCLUDEPARENT(n, Np) 27: N ← N \ N 28: procedure INCLUDECHILDREN(n, L) 29: R ← ∅ 30: for all c ∈ n.CHILDREN() do 31: if c.PARENTEDGELABEL() ∈ L then 32: R ← R ∪ {c} 33: return R 34: procedure INCLUDEPARENT(n, L) 35: if n.PARENTEDGELABEL() ∈ L then 36:</formula><p>return {n} 37: else return ∅ very same summarization architecture, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>: an event model is constructed by clus- tering the patterns extracted according to the se- lected extraction method. Then, the same extrac- tion method is used to collect patterns from sen- tences in never-seen-before news articles. Finally, the patterns are used to query the event model and generate an abstractive summary. The three differ- ent pattern extractors are detailed in the next three sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Heuristics-based pattern extraction</head><p>In order to be able to work in an Open-IE man- ner, applicable to different domains, most existing pattern extraction systems are based on linguisti- cally motivated heuristics. <ref type="bibr" target="#b21">Zhang and Weld (2013)</ref> is based on REVERB <ref type="bibr" target="#b6">(Fader et al., 2011</ref>), which uses a regular expression on part-of-speech tags to produce the extractions. An alternative system, OLLIE ( <ref type="bibr" target="#b16">Schmitz et al., 2012</ref>), uses syntactic de- pendency templates to guide the pattern extraction process.</p><p>The heuristics used in this paper are inspired by <ref type="bibr" target="#b0">Alfonseca et al. (2013)</ref>, who built well formed re- lational patterns by extending minimum spanning trees (MST) which connect entity mentions in a dependency parse. Algorithm 1 details our re- implementation of their method and the specific set of rules that we rely on to enforce pattern gram- maticality. We use the standard Stanford-style set of dependency labels <ref type="bibr" target="#b5">(de Marneffe et al., 2006</ref>). The input to the algorithm are a parse tree T and a set of target entities E. We first generate com- binations of 1-3 elements of E (line 10), then for each combination C we identify all the nodes in T that mention any of the entities in C. We con- tinue by constructing the MST of these nodes, and finally apply our heuristics to the nodes in the MST. The procedure APPLYHEURISTICS (:16) re- cursively grows a nodeset N by including chil- dren and parents of noun and verb nodes in N based on dependency labels. For example, we in- clude all children of verbs in N whose label is listed in V c (:3), e.g., active or passive subjects, direct or indirect objects, particles and auxiliary verbs. Similarly, we include the parent of a noun in N if the dependency relation between the node and its parent is listed in N p .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Pattern extraction by sentence compression</head><p>Sentence compression is a summarization tech- nique that shortens input sentences preserving the most important content <ref type="bibr" target="#b9">(Grefenstette, 1998;</ref><ref type="bibr" target="#b12">McDonald, 2006;</ref><ref type="bibr">Clarke and Lapata, 2008, inter alia)</ref>. While first attempts at integrating a com- pression module into an extractive summarization system were not particularly successful <ref type="bibr" target="#b4">(Daumé III and Marcu, 2004</ref>, inter alia), recent work has been very promising <ref type="bibr" target="#b1">(Berg-Kirkpatrick et al., 2011;</ref><ref type="bibr" target="#b20">Wang et al., 2013)</ref>. It has shown that drop- ping constituents of secondary importance from selected sentences -e.g., temporal modifiers or relative clauses -results in readable and more in- formative summaries. Unlike this related work, our goal here is to compress sentences to obtain an event pattern -the minimal grammatical struc- ture expressing an event. To our knowledge, this application of sentence compressors is novel. As in Section 2, we only consider sentences mention- ing entities and require the compression (pattern) to retain at least one such mention. Sentence compression methods are abundant but very few can be configured to produce out- put satisfying certain constraints. For example, most compression algorithms do not accept com- pression rate as an argument. In our case, sen- tence compressors which formulate the compres- sion task as an optimization problem and solve it with integer linear programming (ILP) tools un- der a number of constraints are particularly attrac- tive ( <ref type="bibr" target="#b3">Clarke and Lapata, 2008;</ref><ref type="bibr" target="#b7">Filippova and Altun, 2013</ref>). They can be extended relatively easily with both the length constraint and the constraint on retaining certain words. The method of <ref type="bibr" target="#b3">Clarke and Lapata (2008)</ref> uses a trigram language model (LM) to score compressions. Since we are inter- ested in very short outputs, a LM trained on stan- dard, uncompressed text would not be suitable. In- stead, we chose to modify the method of <ref type="bibr" target="#b7">Filippova and Altun (2013)</ref> because it relies on dependency parse trees and does not use any LM scoring.</p><p>Like other syntax-based compressors, the sys- tem of <ref type="bibr" target="#b7">Filippova and Altun (2013)</ref> prunes depen- dency structures to obtain compression trees and hence sentences. The objective function to maxi- mize in an ILP problem (Eq. 1) is formulated over weighted edges in a transformed dependency tree and is subject to a number of constraints. Edge weight is defined as a linear function over a fea- ture set: w(e) = w · f (e).</p><formula xml:id="formula_2">F (X) = e∈E x e × w(e)</formula><p>(1)</p><p>In our reimplementation we followed the algo- rithm as described by <ref type="bibr" target="#b7">Filippova and Altun (2013)</ref>. The compression tree is obtained in two steps. First, the input tree is transformed with determin- istic rules, most of which aim at collapsing indis- pensable modifiers with their heads (determiners, auxiliary verbs, negation, multi-word expressions, etc.). Then a sub-tree maximizing the objective function is found under a number of constraints. Apart from the structural constrains from the original system which ensure that the output is a valid tree, the constraints we add state that:</p><p>1. tree size in edges must be in <ref type="bibr">[3,</ref><ref type="bibr">6]</ref>, 2. entity mentions must be retained, 3. subject of the clause must be retained, 4. the sub-tree must be covered by a single clause -exactly one finite verb must used.</p><p>Since we consider compressions with different lengths as candidates, from this set we select the one with the maximum averaged edge weight as the final compression. <ref type="figure">Figure 3</ref> illustrates the use of the compressor for obtaining event pat- terns. Dashed edges are dropped as a result of constrained compression so that the output is John Smith married Mary Brown and the event pattern is PER married PER. Note that the root of a sub- clause is allowed to be the top-level node in the extracted compression.</p><p>Compared with patterns obtaines with heuris- tics, compression patterns should retain preposi- tional verb arguments whose removal would ren- der the pattern ungrammatical. As an example consider [C. Zeta-Jones] and [M. Douglas] filed for divorce. The heuristics-based pattern is PER and PER filed which is incomplete. Unlike it, the compression-based method keeps the essential prepositional phrase for divorce in the pattern be- cause the average edge weight is greater for the tree with the prepositional phrase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Memory-based pattern extraction</head><p>Neither heuristics-based, nor compression-based methods provide a guarantee that the extracted pattern is grammatically correct. In this sec- tion we introduce an extraction technique which makes it considerably more likely because it only extracts patterns which have been observed as full sentences in a human-written text (Sec. 4.1). However, this memory-based method also poses a problem not encountered by the two previous methods: how to search over the vast space of ob- served headlines and sentences to extract a pattern from a given sentence? Our trie-based solution, which we present in the remainder of this sec- tion, makes it possible to compare a dependency graph against millions of observed grammatical utterances in a fraction of a second.</p><p>4.1 A tree-trie to store them all. . .</p><p>Our objective is to construct a compact representa- tion of hundreds of millions of observed sentences that can fit in the memory of a standard worksta- tion. This data structure should make it possible to efficiently identify the sub-trees of a sentence that match any complete utterance previously ob- served. To this end, we build a trie of depen- dency trees (which we call a tree-trie) by scan- ning all the dependency parses in the news training Algorithm 2 STORE(T, I): store the dependency tree T in the tree-trie I.</p><p>1:</p><formula xml:id="formula_3">/* Entry point /* 2: L ← T.LINEARIZE() 3: STORERECURSION(I.ROOT(), L, 0) 4: return M 5: /* Procedures /* 6: procedure STORERECURSION(n, L, o) 7: if o == L.LENGTH() then 8: n.ADDTREESTRUCTURE(L.STRUCTURE()) 9: return 10: if not n.HASCHILD(L.TOKEN(o)) then 11: n.ADDCHILD(L.TOKEN(o)) 12: n ← n.GETCHILD(L.TOKEN(o)) 13: STORERECURSION(n , L, o + 1)</formula><p>data, and index each tree in the tree-trie accord- ing to Algorithm 2. For better clarity, the process is also described graphically in <ref type="figure" target="#fig_2">Figure 4</ref>. First, each dependency tree (a) is linearized, resulting in a data structure that consists of two aligned se- quences (b). The first sequence (tokens) encodes word/parent-relation pairs, while the second se- quence (structure) encodes the offsets of parent nodes in the linearized tree. As an example, the first word "The" is a determiner ("det") for the sec- ond node (offset 1) in the sequence, which is "cat". In turn, "cat" is the subject ("nsubj") of the node in position 2, i.e., "sleeps". As described in Algo- rithm 2, we recursively store the token sequence in the trie, each word/relation pair being stored in a node. When the token sequence is completely consumed, we store in the current trie node the structure of the linearized tree. Combining struc- tural information with the sequential information encoded by each path in the trie makes it possi- ble to rebuild a complete dependency graph. <ref type="bibr">Figure 4(c)</ref> shows an example trie encoding 4 differ- ent sentences. We highlighted in bold the path cor- responding to the linearized form (b) of the exam- ple parse tree (a).</p><p>The figure shows that the tree contains two kinds of nodes: end-of-sentence (EOS) nodes (red) and non-terminal nodes (in blue). EOS nodes do not necessarily coincide with trie leaves, as it is possible to observe complete sentences embed- ded in longer ones. EOS nodes differ from non- terminal nodes in that they store one or more struc- tural sequences corresponding to different syntac- tic representations of observed sentences with the same tokens.</p><p>Space-complexity and generalization. Storing all the observed sentences in a single trie requires huge amounts of memory. To make it possible to root Our sources report John Smith married Mary Brown in Baltimore yesterday root root subj subj obj in tmod <ref type="figure">Figure 3</ref>: Transformed dependency tree with a sub-tree expressing an event pattern.  store a complete tree-trie in memory, we adopt the following strategy. We replace the surface form of entity nodes with the coarse entity type (e.g., PER, LOC, ORG) of the entity. Similarly, we replace proper nouns with the placeholder "[P]", thus sig- nificantly reducing lexical sparsity. Then, we en- code each distinct word/relation pair as a 32-bit unsigned integer. Assuming a maximum tree size of 255 nodes, we represent structure sequences as vectors of type unsigned char (8 bit per element). Finally, we store trie-node children as sorted vec- tors instead of hash maps to reduce memory foot- print. As a result, we are able to load a trie encod- ing 400M input dependency parses, 170M distinct nodes and 48M distinct sentence structures in un- der 10GB of RAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">. . . and in the vastness match them</head><p>At lookup time, we want to use the tree-trie to identify all sub-graphs of an input dependency tree T that match at least a complete observed sen- tence. To do so, we need to identify all paths in the trie that match any sub-sequence s of the lin- earized sequence of T nodes. Whenever we en- counter an EOS node e, we verify if any of the structures stored at e matches the sub-tree gener- ated by s. If so, then we have a positive match. As a sentence might embed many shorter utter- ances, each input T will generally yield multiple matches. For example, querying the tree-trie in n ← n.GETCHILD(L.TOKEN(i)) 11:</p><p>P ← P ∪ {i} 12:</p><p>for all S ∈ n .TREESTRUCTURES() do 13:</p><p>if L.ISCOMPATIBLE(S, P ) then 14:</p><p>M ← M ∪ {T.GETNODES(P )} 15:</p><p>LOOKUPRECURSIVE(L, i, o + 1, n , P , M ) Algorithm 3 describes the lookup process in more detail. The first step consists in the lineariza- tion of the input tree T . Then, we recursively tra- verse the trie calling LOOKUPRECURSIVE. The inputs of this procedure are: the input tree T , its linearization L and an offset o (starting at 0), the trie node currently being traversed n (starting with the root), the set of offsets in L that constitute a partial match P (initially empty) and the set of complete matches found M . We recursively tra- verse all the nodes in the trie that yield a partial match with any sub-sequence of the linearized to- kens of T . At each step, we scan all the tokens in L in the range [o, L.LENGTH()) looking for to- kens matching any of the children of n. If a match- ing node is found, a new partial match P is con- structed by extending P with the matching token 1.0E-05</p><p>1.0E-04</p><p>1.0E-03</p><p>1.0E-02</p><p>1.0E-01</p><p>1.0E+00</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1.0E+01</head><p>f(x) = 1.9E-07 x^3.3E+00</p><p>Tree size (number of nodes)</p><p>Time (seconds) <ref type="figure">Figure 5</ref>: Time complexity of lookup operations for inputs of different sizes.</p><p>offset i (line 11), and the recursion continues from the matching trie node n and offset i (line 15). Every time a partial match is found, we verify if the partial match is compatible with any of the tree structures stored in the matching node. If that is the case, we identify the corresponding set of matching nodes in T and add it to the result M (lines <ref type="bibr">[12]</ref><ref type="bibr">[13]</ref><ref type="bibr">[14]</ref>. A pattern is generated from each complete match returned by LOOKUP after apply- ing a simple heuristic: for each verb node v in the match, we enforce that negations and auxiliaries in T depending from x are also included in the pat- tern.</p><p>Time complexity of lookups. Let k be the max- imum fan-out of trie nodes, d be the depth of the trie and n be the size of an input tree (num- ber of nodes). If trie node children are hashed (which has a negative effect on space complex- ity), then worst case complexity of LOOKUP() is O(nk) d−1 . If they are stored as sorted lists, as in our memory-efficient implementation, theoretical complexity becomes O(nk log(k)) d−1 . It should be noted that worst case complexity can only be observed under extremely unlikely circumstances, i.e., that at every step of the recursion all the nodes in the tail of the linearized tree match a child of the current node. Also, in the actual trie used in our experiments the average branching factor k is very small. We observed that a trie storing 400M sentences (170M nodes) has an average branching factor of 1.02. While the root of the trie has unsur- prisingly many children (210K, all the observed first sentence words), already at depth 2 the aver- age fan-out is 13.7, and at level 3 it is 4.9.</p><p>For an empirical analysis of lookup complexity, <ref type="figure">Figure 5</ref> plots, in black, wall-clock lookup time as a function of tree size n for a random sample of 1,600 inputs. As shown by the polynomial re- gression curve (red), observed lookup complexity is approximately cubic with a very small constant factor. In general, we can see that for sentences of common length (20-50 words) a lookup operation can be completed in well under one second.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental settings</head><p>All the models for the experiments that we present have been trained using the same corpus of news crawled from the web between 2008 and 2013. The news have been processed with a to- kenizer, a sentence splitter ( <ref type="bibr" target="#b8">Gillick and Favre, 2009</ref>), a part-of-speech tagger and dependency parser <ref type="bibr" target="#b15">(Nivre, 2006</ref>), a co-reference resolution module <ref type="bibr" target="#b10">(Haghighi and Klein, 2009)</ref> and an entity linker based on <ref type="bibr">Wikipedia and Freebase (Milne and Witten, 2008)</ref>. We use Freebase types as fine- grained named entity types, so we are also able to label e.g. instances of sports teams as such instead of the coarser label ORG.</p><p>Next, the news have been grouped based on temporal closeness <ref type="bibr" target="#b21">(Zhang and Weld, 2013)</ref> and cosine similarity (using tf·idf weights). For each of the three pattern extraction methods we used the same summarization pipeline (as shown above in <ref type="figure" target="#fig_1">Figure 2</ref>):</p><p>1. Run pattern extraction on the news.</p><p>2. For every news collection Coll and entity set E, generate a set containing all the extracted patterns from news in Coll mentioning all the entities in E. These are patterns that are likely to be paraphrasing each other.</p><p>3. Run a clustering algorithm to group together patterns that typically co-occur in the sets generated in the previous step. There are many choices for clustering algorithms <ref type="bibr" target="#b0">(Alfonseca et al., 2013;</ref><ref type="bibr" target="#b21">Zhang and Weld, 2013)</ref>. Following <ref type="bibr" target="#b0">Alfonseca et al. (2013)</ref> we use in this work a Noisy-OR Bayesian Network be- cause it has already been applied for abstrac- tive summarization (albeit multi-document), it provides an easily interpretable probabilis- tic clustering, and training can be easily par- allelized to be able to handle large training sets. The hidden events in the Bayesian net- work represent pattern clusters. When train- ing is done, for each extraction pattern p j</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original sentence</head><p>Abstractive summary (method)</p><p>Two-time defending overall World Cup champion Marcel Hirscher won the challenging giant slalom on the Gran Risa course with two solid runs Sunday and attributed his victory to a fixed screw in his equipment setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Marcel Hirscher has won the giant slalom. (C)</head><p>Zodiac Aerospace posted a 7.9 percent rise in first-quarter revenue, below mar- ket expectations, but reaffirmed its full-year financial targets. Zodiac Aerospace has reported a rise in profits. (C)</p><p>Australian free-agent closer Grant Balfour has agreed to terms with the Balti- more Orioles on a two-year deal, the Baltimore Sun reported on Tuesday citing multiple industry sources.</p><p>Balfour will join the Baltimore Orioles.   and pattern cluster c i , the network provides p(p j |c i ) -the probability that c i will gener- ate p j -and p(c i |p j ) -the probability that, given a pattern p j , c i was the hidden event that generated it.</p><p>At generation time we proceed in the following way:</p><p>1. Given the title or first sentence of a news ar- ticle, run the same pattern extraction method that was used in training and, if possible, ob- tain a pattern p involving some entities.</p><p>2. Find the model clusters that contain this pat- tern, C p = {c i such that p(c i |p) &gt; 0}.</p><p>3. Return a ranked list of model patterns output = {(p j , score(p j ))}, scored as fol- lows:</p><formula xml:id="formula_5">score(p j ) = c i ∈Cp p(p j |c i )p(c i |p)</formula><p>where p was the input pattern.</p><p>4. Replace the entity placeholders in the top- scored patterns p j with the entities that were actually mentioned in the input news article.</p><p>In all cases the parameters of the network were predefined as 20,000 nodes in the hidden layer (model clusters) and 40 Expectation Maximization (EM) training iterations. Training was distributed across 20 machines with 10 GB of memory each.</p><p>For testing we used 37,584 news crawled dur- ing December 2013, which had not been used for training the models. <ref type="table">Table 3</ref> shows one pattern cluster example from each of the three trained models. The table shows only the surface form of the pattern for simplicity.</p><p>Pattern cluster (MEMORY-BASED) organization1 gets organization0 nod for drug organization1 gets organization0 nod for tablets organization0 approves organization1 drug organizations0 approves organization1 's drug organization1 gets organization0 nod for capsules Pattern cluster (HEURISTIC) organization0 to buy organization1 organization0 to acquire organization1 organization0 buys organization1 organization0 acquires organization1 organization0 to acquire organizations1 organization0 buys organizations1 organization0 acquires organizations1 organization0 agrees to buy organization1 organization0 snaps up organization1 organization0 to purchase organizations1 organization0 is to acquire organization1 organization0 has agreed to buy organization1 organization0 announces acquisition of organizations1 organization0 may bid for organization1 organization1 sold to organization0 organization1 acquired by organization0</p><p>Pattern cluster (COMPRESSION) the sports team1 have acquired person0 from the sports team2 the sports team1 acquired person0 from the sports team2 the sports team2 have traded person0 to the sports team1 sports team1 acquired the rights to person0 from sports team2 sports team2 acquired from sports team1 in exchange for person0 sports team2 have acquired from the sports team1 in exchange for person0 <ref type="table">Table 3</ref>: Examples of pattern clusters. In each cluster c i , patterns are sorted by p(p j |c i ). <ref type="table" target="#tab_2">Table 2</ref> shows the number of extracted patterns from the test set, and the number of abstractive event descriptions produced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>As expected, the number of extracted patterns using the memory-based model is smaller than with the two other models, which are based on generic rules and are less restricted in what they can generate. As mentioned, the memory-based model can only extract previously-seen structures. Compared to this model, with heuristics we can obtain patterns for more than twice more news ar- ticles. At the same time, looking at the number of summary sentences generated they are com- parable, meaning that a larger proportion of the memory-based patterns actually appeared in the pattern clusters and could be used to produce sum- maries. This is also consistent with the fact that us- ing heuristics the space of extracted patterns is ba- sically unbounded and many new patterns can be generated that were previously unseen -and these cannot generate abstractions. A positive outcome is that restricting the syntactic structure of the ex- tracted patterns to what has been observed in past news does not negatively affect end-to-end cover- age when generating the abstractive summaries. <ref type="table" target="#tab_1">Table 1</ref> shows some of the abstractive sum- maries generated with the different methods. For manually evaluating their quality, a random sam- ple of 100 original sentences was selected for each method. The top ranked summary for each origi- nal sentence was sent to human raters for evalua- tion, and received three different ratings. None of the raters had any involvement in the development of the work or the writing of the paper, and a con- straint was added that no rater could rate more than 50 abstractions. Raters were presented with the original sentence and the compressed abstraction, and were asked to rate it along two dimensions, in both cases using a 5-point Likert scale:</p><p>• Readability: whether the abstracted com- pression is grammatically correct.</p><p>• Informativeness: whether the abstracted compression conveys the most important in- formation from the original sentence.</p><p>Inter-judge agreement was measured using the Intra-Class Correlation (ICC) <ref type="bibr" target="#b17">(Shrout and Fleiss, 1979;</ref><ref type="bibr" target="#b2">Cicchetti, 1994</ref>  <ref type="table">Table 4</ref>: Results for the three methods when rating the top-ranked abstraction. and for informativeness it was 0.64 (95% confi- dence interval [0,60, 0.67]), representing fair and substantial reliability. <ref type="table">Table 4</ref> shows the results when rating the top ranked abstraction using either of the three dif- ferent models for pattern extraction. The abstrac- tions produced with the memory-based method are more readable than those produced with the other two methods (statistically significant with 95% confidence).</p><p>Regarding informativeness, the differences be- tween the methods are bigger, because the first two methods have a proportionally larger number of items with a high readability but a low informa- tiveness score. For each method, we have man- ually reviewed the 25 items where the difference between readability and informativeness was the largest, to understand in which cases grammatical, yet irrelevant compressions are produced. The re- sults are shown in <ref type="table">Table 5</ref>. Be+adjective includes examples where the pattern is of the form Entity is Adjective, which the compression-based systems extracts often represents an incomplete extraction. Wrong inference contains the cases where patterns that are related but not equivalent are clustered, e.g. Person arrived in Country and Person arrived in Country for talks. Info. missing represents cases where very relevant information has been dropped and the summary sentence is not complete. Pos- sibility contains cases where the original sentence described a possibility and the compression states it as a fact, or vice versa. Disambiguation are en- tity disambiguations errors, and Opposite contains cases of patterns clustered together that are op- posite along some dimension, e.g. Person quits TV Program and Person to return to TV Program.</p><p>The method with the largest drop between the readability and informativeness scores is COM- PRESSION. As can be seen, many of these mis- takes are due to relevant information being miss- ing in the summary sentence. This is also the largest source of errors for the HEURISTIC system. For the MEMORY-BASED system, the drop in read-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Be+adjective Wrong inference Info. missing Possibility Disambiguation <ref type="table" target="#tab_1">Opposite   HEURISTIC  0  7  14  3  1  0  COMPRESSION  3  10  10  0  0  2  MEMORY-BASED  0  17  4  2  0  2   Table 5</ref>: Sources of errors for the top 25 items with high readability and low informativeness.  <ref type="table">Table 6</ref>: Examples of compression (C), heuristic (H) and memory-based (M) patterns that led to abstrac- tions with high readability but a low informativeness score. Both incomplete summary sentences and wrong inferences can be observed.</p><p>ability score is much smaller, so there were less of these examples. And most of these examples be- long to the class of wrong inferences (patterns that are related but not equivalent, so we should not abstract one of them from the other, but they were clustered together in the model). Our conclusion is that the examples with missing information are not such a big problem with the MEMORY-BASED system, as using the trie is an additional safeguard that the generated titles are complete statements, but the method is not preventing the wrong infer- ence errors so this class of errors become the dom- inant class by a large margin. Some examples with high readability but low informativeness are shown in <ref type="table">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>Most Open-IE systems are based on linguistically- motivated heuristics for learning patterns that ex- press relations between entities or events. How- ever, it is common for these patterns to be incom- plete or ungrammatical, and therefore they are not suitable for abstractive summary generation of the relation or event mentioned in the text.</p><p>In this paper, we describe a memory-based ap- proach in which we use a corpus of past news to learn valid syntactic sentence structures. We discuss the theoretical time complexity of look- ing up extraction patterns in a large corpus of syntactic structures stored as a trie and demon- strate empirically that this method is effective in practice. Finally, the evaluation shows that sum- mary sentences produced by this method outper- form heuristics and compression-based ones both in terms of readability and informativeness. The problem of generating incomplete summary sen- tences, which was the main source of informative- ness errors for the alternative methods, becomes a minor problem with the memory-based approach. Yet, there are some cases in which also the mem- ory based approach extracts correct but misleading utterances, e.g., a pattern like PER passed away from the sentence PER passed the ball away. To solve this class of problems, a possible research direction would be the inclusion of more complex linguistic features in the tree-trie, such as verb sub- categorization frames.</p><p>As another direction for future work, more ef- fort is needed in making sure that no incorrect in- ferences are made with this model. These happen when a more specific pattern is clustered together with a less specific pattern, or when two non- equivalent patterns often co-occur in news as two events are somewhat correlated in real life, but it is generally incorrect to infer one from the other. Im- provements in the pattern-clustering model, out- side the scope of this paper, will be required.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of abstracting from input sentences to an event representation and generation from that representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A generic pipeline for event-driven abstractive headline generation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: A dependency tree (a), its linearized form (b) and the resulting path in a trie (c), in bold.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 (</head><label>4</label><figDesc>c) with the input tree shown in (a) would yield two results, as both The cat sleeps and The cat sleeps under the table are complete utterances stored in the trie. Algorithm 3 LOOKUP(T, I): Lookup for matches of sub- set of tree T in the trie index I.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Paul</head><label></label><figDesc>Rudd is 'Ant-Man': 5 reasons he needs an 'Agents of SHIELD' appear- ance. Paul Rudd to play Ant-Man. (H) Millwall defender Karleigh Osborne has joined Bristol City on a two-and-a-half year deal after a successful loan spell. Bristol City have signed Karleigh Os- borne. (M) Simon Hoggart, one of the Spectator's best-loved columnists, died yesterday after fighting pancreatic cancer for over three years. Simon Hoggart passed away yesterday. (M)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Abstraction examples from compression (C), heuristic (H) and memory-based (M) patterns. 

Method 
Extractions Abstractions 

HEURISTIC 
24,630 
956 
COMPRESSION 
15,687 
657 
MEMORY-BASED 
11,459 
967 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Patterns extracted in each method, before 
Noisy-OR inference. 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">HEADY: News headline abstraction through event pattern clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Alfonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Pighin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Garrido</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="page" from="1243" to="1253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Jointly learning to extract and compress</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Portland, OR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="19" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Guidelines, criteria, and rules of thumb for evaluating normed and standardized assessment instruments in psychology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Domenic V Cicchetti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Assessment</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">284</biblScope>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Global inference for sentence compression: An integer linear programming approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="399" to="429" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A treeposition kernel for document compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 Document Understanding Conference held at the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the 2004 Document Understanding Conference held at the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics<address><addrLine>Boston, Mass</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-05" />
			<biblScope unit="page" from="6" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generating typed dependency parses from phrase structure parses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Language Resources and Evaluation</title>
		<meeting>the 5th International Conference on Language Resources and Evaluation<address><addrLine>Genoa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-05" />
			<biblScope unit="page" from="449" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Identifying relations for open information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Edinburgh, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-07" />
			<biblScope unit="page" from="1535" to="1545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Overcoming the lack of parallel data in sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Filippova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasemin</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1481" to="1491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A scalable global model for summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Favre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ILP for NLP Workshop</title>
		<meeting>the ILP for NLP Workshop<address><addrLine>Boulder, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-06-04" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Producing intelligent telegraphic text reduction to provide an audio scanning service for the blind</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Grefenstette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Working Notes of the Workshop on Intelligent Text Summarization</title>
		<meeting><address><addrLine>Palo Alto, Cal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-03-23" />
			<biblScope unit="page" from="111" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Simple coreference resolution with rich syntactic and semantic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-06-07" />
			<biblScope unit="page" from="1152" to="1161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inderjeet</forename><surname>Mani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">John Benjamins</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Discriminative sentence compression with soft syntactic evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 11th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Trento, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-04" />
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Abstractive meeting summarization with entailment and fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><forename type="middle">W</forename><surname>Tompa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th European Workshop on Natural Language Generation</title>
		<meeting>the 14th European Workshop on Natural Language Generation<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08-09" />
			<biblScope unit="page" from="136" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An effective, low-cost measure of semantic relatedness obtained from Wikipedia links</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Milne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI 2008 Workshop on Wikipedia and Artificial Intelligence</title>
		<meeting>the AAAI 2008 Workshop on Wikipedia and Artificial Intelligence<address><addrLine>Chicago, IL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-07" />
			<biblScope unit="page" from="13" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Inductive Dependency Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Open language learning for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schmitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Bart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2012 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-07" />
			<biblScope unit="page" from="523" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph L</forename><surname>Shrout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fleiss</surname></persName>
		</author>
		<title level="m">Intraclass correlations: uses in assessing rater reliability. Psychological bulletin</title>
		<imprint>
			<date type="published" when="1979" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page">420</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Automatic summarising: A review and discussion of the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Spärck-Jones</surname></persName>
		</author>
		<idno>UCAM-CL-TR-679</idno>
		<imprint>
			<date type="published" when="2007" />
			<pubPlace>Cambridge, U.K</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Cambridge, Computer Laboratory</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A Bayesian model for unsupervised semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Klementiev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Portland, OR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06-24" />
			<biblScope unit="page" from="1445" to="1455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A sentence compression based framework to query-focused multidocument summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hema</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Castelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="page" from="1384" to="1394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Harvesting parallel news streams to generate paraphrases of event relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congle</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1776" to="1786" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
