<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Compositional Sequence Labeling Models for Error Detection in Learner Writing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Rei</surname></persName>
							<email>marek.rei@cl.cam.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">The ALTA Institute Computer Laboratory University of Cambridge United Kingdom</orgName>
								<orgName type="institution">The ALTA Institute Computer Laboratory University of Cambridge United Kingdom</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Yannakoudakis</surname></persName>
							<email>helen.yannakoudakis@cl.cam.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">The ALTA Institute Computer Laboratory University of Cambridge United Kingdom</orgName>
								<orgName type="institution">The ALTA Institute Computer Laboratory University of Cambridge United Kingdom</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Compositional Sequence Labeling Models for Error Detection in Learner Writing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1181" to="1191"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we present the first experiments using neural network models for the task of error detection in learner writing. We perform a systematic comparison of alternative compositional architectures and propose a framework for error detection based on bidirectional LSTMs. Experiments on the CoNLL-14 shared task dataset show the model is able to outper-form other participants on detecting errors in learner writing. Finally, the model is integrated with a publicly deployed self-assessment system, leading to performance comparable to human annotators.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automated systems for detecting errors in learner writing are valuable tools for second language learning and assessment. Most work in recent years has focussed on error correction, with er- ror detection performance measured as a byprod- uct of the correction output ( <ref type="bibr" target="#b11">Ng et al., 2013;</ref><ref type="bibr" target="#b27">Ng et al., 2014</ref>). However, this assumes that systems are able to propose a correction for every detected error, and accurate systems for correction might not be optimal for detection. While closed-class errors such as incorrect prepositions and determin- ers can be modeled with a supervised classification approach, content-content word errors are the 3rd most frequent error type and pose a serious chal- lenge to error correction frameworks ( <ref type="bibr">Leacock et al., 2014;</ref><ref type="bibr">Kochmar and Briscoe, 2014</ref>). Eval- uation of error correction is also highly subjec- tive and human annotators have rather low agree- ment on gold-standard corrections <ref type="bibr" target="#b3">(Bryant and Ng, 2015)</ref>. Therefore, we treat error detection in learner writing as an independent task and propose a system for labeling each token as being correct or incorrect in context.</p><p>Common approaches to similar sequence label- ing tasks involve learning weights or probabilities for context n-grams of varying sizes, or relying on previously extracted high-confidence context pat- terns. Both of these methods can suffer from data sparsity, as they treat words as independent units and miss out on potentially related patterns. In ad- dition, they need to specify a fixed context size and are therefore often limited to using a small window near the target.</p><p>Neural network models aim to address these weaknesses and have achieved success in various NLP tasks such as language modeling ( <ref type="bibr" target="#b2">Bengio et al., 2003</ref>) and speech recognition ( <ref type="bibr" target="#b10">Dahl et al., 2012)</ref>. Recent developments in machine transla- tion have also shown that text of varying length can be represented as a fixed-size vector using convolutional networks <ref type="bibr">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b5">Cho et al., 2014a</ref>) or recurrent neu- ral networks ( <ref type="bibr" target="#b6">Cho et al., 2014b;</ref><ref type="bibr" target="#b1">Bahdanau et al., 2015)</ref>.</p><p>In this paper, we present the first experiments using neural network models for the task of er- ror detection in learner writing. We perform a systematic comparison of alternative composi- tional structures for constructing informative con- text representations. Based on the findings, we propose a novel framework for performing er- ror detection in learner writing, which achieves state-of-the-art results on two datasets of error- annotated learner essays. The sequence labeling model creates a single variable-size network over the whole sentence, conditions each label on all the words, and predicts all labels together. The effects of different datasets on the overall perfor- mance are investigated by incrementally provid- ing additional training data to the model. Finally, we integrate the error detection framework with a publicly deployed self-assessment system, leading to performance comparable to human annotators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Related Work</head><p>The field of automatically detecting errors in learner text has a long and rich history. Most work has focussed on tackling specific types of errors, such as usage of incorrect prepositions <ref type="bibr" target="#b28">(Tetreault and Chodorow, 2008;</ref><ref type="bibr" target="#b8">Chodorow et al., 2007)</ref>, ar- ticles ( <ref type="bibr" target="#b19">Han et al., 2004;</ref><ref type="bibr" target="#b20">Han et al., 2006</ref>), verb forms ( <ref type="bibr">Lee and Seneff, 2008)</ref>, and adjective-noun pairs <ref type="bibr">(Kochmar and Briscoe, 2014)</ref>.</p><p>However, there has been limited work on more general error detection systems that could handle all types of errors in learner text. <ref type="bibr" target="#b7">Chodorow and Leacock (2000)</ref> proposed a method based on mu- tual information and the chi-square statistic to de- tect sequences of part-of-speech tags and func- tion words that are likely to be ungrammatical in English. <ref type="bibr" target="#b17">Gamon (2011)</ref> used Maximum En- tropy Markov Models with a range of features, such as POS tags, string features, and outputs from a constituency parser. The pilot Helping Our Own shared task ( <ref type="bibr" target="#b12">Dale and Kilgarriff, 2011</ref>) also evaluated grammatical error detection of a num- ber of different error types, though most systems were error-type specific and the best approach was heavily skewed towards article and preposition er- rors <ref type="bibr">(Rozovskaya et al., 2011</ref>). We extend this line of research, working towards general error de- tection systems, and investigate the use of neural compositional models on this task.</p><p>The related area of grammatical error correction has also gained considerable momentum in the past years, with four recent shared tasks highlight- ing several emerging directions <ref type="bibr" target="#b12">(Dale and Kilgarriff, 2011;</ref><ref type="bibr" target="#b13">Dale et al., 2012;</ref><ref type="bibr" target="#b11">Ng et al., 2013;</ref><ref type="bibr" target="#b27">Ng et al., 2014</ref>). The current state-of-the-art approaches can broadly be separated into two categories:</p><p>1. Phrase-based statistical machine translation techniques, essentially translating the incor- rect source text into the corrected version <ref type="bibr" target="#b16">(Felice et al., 2014;</ref><ref type="bibr" target="#b23">Junczys-Dowmunt and Grundkiewicz, 2014)</ref> 2. Averaged Perceptrons and Naive Bayes clas- sifiers making use of native-language error correction priors ( <ref type="bibr" target="#b26">Rozovskaya et al., 2014;</ref><ref type="bibr" target="#b25">Rozovskaya et al., 2013</ref>).</p><p>Error correction systems require very specialised models, as they need to generate an improved ver- sion of the input text, whereas a wider range of tagging and classification models can be deployed on error detection. In addition, automated writing feedback systems that indicate the presence and location of errors may be better from a pedagogic point of view, rather than providing a panacea and correcting all errors in learner text. In Section 7 we evaluate a neural sequence tagging model on the latest shared task test data, and compare it to the top participating systems on the task of error detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Sequence Labeling Architectures</head><p>We construct a neural network sequence labeling framework for the task of error detection in learner writing. The model receives only a series of tokens as input, and outputs the probability of each token in the sentence being correct or incorrect in a given context. The architectures start with the vector representations of individual words,</p><formula xml:id="formula_0">[x 1 , ..., x T ],</formula><p>where T is the length of the sentence. Different composition functions are then used to calculate a hidden vector representation of each token in con- text, [h 1 , ..., h T ]. These representations are passed through a softmax layer, producing a probability distribution over the possible labels for every to- ken in context:</p><formula xml:id="formula_1">p t = sof tmax(W o h t )<label>(1)</label></formula><p>where W o is the weight matrix between the hidden vector h t and the output layer. We investigate six alternative neural network ar- chitectures for the task of error detection: con- volutional, bidirectional recurrent, bidirectional LSTM, and multi-layer variants of each of them. In the convolutional neural network <ref type="figure" target="#fig_0">(CNN, Fig- ure 1a)</ref> for token labeling, the hidden vector h t is calculated based on a fixed-size context win- dow. The convolution acts as a feedforward net- work, using surrounding context words as input, and therefore it will learn to detect the presence of different types of n-grams. The assumption behind the convolutional architecture is that memorising erroneous token sequences from the training data is sufficient for performing error detection.</p><p>The convolution uses d w tokens on either side of the target token, and the vectors for these tokens are concatenated, preserving the ordering:</p><formula xml:id="formula_2">c t = x t−dw : ... : x t+dw (2)</formula><p>where x 1 : x 2 is used as notation for vector con- catenation of x 1 and x 2 . The combined vector is then passed through a non-linear layer to produce the hidden representation:</p><formula xml:id="formula_3">h t = tanh(W c c t )<label>(3)</label></formula><p>The deep convolutional network <ref type="figure" target="#fig_0">(Figure 1b</ref>) adds an extra convolutional layer to the architec- ture, using the first layer as input. It creates con- volutions of convolutions, thereby capturing more complex higher-order features from the dataset.</p><p>In a recurrent neural network (RNN), each hid- den representation is calculated based on the cur- rent token embedding and the hidden vector at the previous time step:</p><formula xml:id="formula_4">h t = f (W x t + V h t−1 ) (4)</formula><p>where f (z) is a nonlinear function, such as the sigmoid function. Instead of a fixed context win- dow, information is passed through the sentence using a recursive function and the network is able to learn which patterns to disregard or pass for- ward. This recurrent network structure is referred to as an Elman-type network, after Elman (1990). The bidirectional RNN ( <ref type="figure" target="#fig_0">Figure 1c</ref>) consists of two recurrent components, moving in opposite di- rections through the sentence. While the unidirec- tional version takes into account only context on the left of the target token, the bidirectional ver- sion recursively builds separate context represen- tations from either side of the target token. The left and right context are then concatenated and used as the hidden representation:</p><formula xml:id="formula_5">h → t = f (W r x t + V r h → t−1 )<label>(5)</label></formula><formula xml:id="formula_6">h ← t = f (W l x t + V l h ← t+1 ) (6) h t = h → t : h ← t (7)</formula><p>Recurrent networks have been shown to per- form well on the task of language modeling <ref type="bibr">(Mikolov et al., 2011;</ref><ref type="bibr" target="#b4">Chelba et al., 2013</ref>), where they learn an incremental composition function for predicting the next token in the sequence. However, while language models can estimate the probability of each token, they are unable to differentiate between infrequent and incorrect to- ken sequences. For error detection, the compo- sition function needs to learn to identify seman- tic anomalies or ungrammatical combinations, in- dependent of their frequency. The bidirectional model provides extra information, as it allows the network to use context on both sides of the target token.</p><p>Irsoy and Cardie (2014) created an extension of this architecture by connecting together mul- tiple layers of bidirectional Elman-type recurrent network modules. This deep bidirectional RNN ( <ref type="figure" target="#fig_0">Figure 1d</ref>) calculates a context-dependent rep- resentation for each token using a bidirectional RNN, and then uses this as input to another bidi- rectional RNN. The multi-layer structure allows the model to learn more complex higher-level fea- tures and effectively perform multiple recurrent passes through the sentence.</p><p>The long-short term memory (LSTM) (Hochre- iter and <ref type="bibr" target="#b21">Schmidhuber, 1997</ref>) is an advanced al- ternative to the Elman-type networks that has recently become increasingly popular. It uses two separate hidden vectors to pass information between different time steps, and includes gat- ing mechanisms for modulating its own output. LSTMs have been successfully applied to var- ious tasks, such as speech recognition ( <ref type="bibr" target="#b18">Graves et al., 2013)</ref>, machine translation ( <ref type="bibr">Luong et al., 2015)</ref>, and natural language generation <ref type="bibr" target="#b29">(Wen et al., 2015)</ref>.</p><p>Two sets of gating values (referred to as the in- put and forget gates) are first calculated based on the previous states of the network:</p><formula xml:id="formula_7">i t = σ(W i x t + U i h t−1 + V f c t−1 + b i )<label>(8)</label></formula><formula xml:id="formula_8">f t = σ(W f x t + U f h t−1 + V f c t−1 + b f ) (9)</formula><p>where x t is the current input, h t−1 is the previous hidden state, b i and b f are biases, c t−1 is the pre- vious internal state (referred to as the cell), and σ is the logistic function. The new internal state is calculated based on the current input and the pre- vious hidden state, and then interpolated with the previous internal state using f t and i t as weights:</p><formula xml:id="formula_9">c t = tanh(W c x t + U c h t−1 + b c )<label>(10)</label></formula><formula xml:id="formula_10">c t = f t c t−1 + i t c t<label>(11)</label></formula><p>where is element-wise multiplication. Finally, the hidden state is calculated by passing the inter- nal state through a tanh nonlinearity, and weight- ing it with o t . The values of o t are conditioned on the new internal state (c t ), as opposed to the previ- ous one (c t−1 ):</p><formula xml:id="formula_11">o t = σ(W o x t + U o h t−1 + V o c t + b o )<label>(12)</label></formula><formula xml:id="formula_12">h t = o t tanh(c t )<label>(13)</label></formula><p>Because of the linear combination in equation (11), the LSTM is less susceptible to vanishing gradients over time, thereby being able to make use of longer context when making predictions. In addition, the network learns to modulate itself, ef- fectively using the gates to predict which operation is required at each time step, thereby incorporating higher-level features.</p><p>In order to use this architecture for error de- tection, we create a bidirectional LSTM, mak- ing use of the advanced features of LSTM and in- corporating context on both sides of the target to- ken. In addition, we experiment with a deep bidi- rectional LSTM, which includes two consecu- tive layers of bidirectional LSTMs, modeling even more complex features and performing multiple passes through the sentence.</p><p>For comparison with non-neural models, we also report results using CRFs ( <ref type="bibr">Lafferty et al., 2001</ref>), which are a popular choice for sequence labeling tasks. We trained the CRF++ 1 imple- mentation on the same dataset, using as features unigrams, bigrams and trigrams in a 7-word win- dow surrouding the target word (3 words before and after). The predicted label is also conditioned on the previous label in the sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate the alternative network structures on the publicly released First Certificate in En- glish dataset (FCE-public, <ref type="bibr" target="#b30">Yannakoudakis et al. (2011)</ref>). The dataset contains short texts, writ- ten by learners of English as an additional lan- guage in response to exam prompts eliciting free- text answers and assessing mastery of the upper- intermediate proficiency level. The texts have been manually error-annotated using a taxonomy of 77 error types. We use the released test set for evaluation, containing 2,720 sentences, leaving 30,953 sentences for training. We further separate 2,222 sentences from the training set for develop- ment and hyper-parameter tuning.</p><p>The dataset contains manually annotated error spans of various types of errors, together with their suggested corrections. We convert this to a token- level error detection task by labeling each token inside the error span as being incorrect. In order to capture errors involving missing words, the error label is assigned to the token immediately after the incorrect gap -this is motivated by the intuition that while this token is correct when considered in isolation, it is incorrect in the current context, as another token should have preceeded it.</p><p>As the main evaluation measure for error de- tection we use F 0.5 , which was also the measure adopted in the CoNLL-14 shared task on error cor- rection ( <ref type="bibr" target="#b27">Ng et al., 2014</ref>). It combines both pre- cision and recall, while assigning twice as much weight to precision, since accurate feedback is often more important than coverage in error de- tection applications <ref type="bibr">(Nagata and Nakatani, 2010</ref> Briscoe, 2015), require the system to propose a correction and are therefore not directly applica- ble on the task of error detection.</p><p>During the experiments, the input text was low- ercased and all tokens that occurred less than twice in the training data were represented as a single unk token. Word embeddings were set to size 300 and initialised using the publicly released pre- trained Word2Vec vectors ( <ref type="bibr" target="#b4">Mikolov et al., 2013</ref>). The convolutional networks use window size 3 on either side of the target token and produce a 300-dimensional context-dependent vector. The recurrent networks use hidden layers of size 200 in either direction. We also added an extra hid- den layer of size 50 between each of the compo- sition functions and the output layer -this allows the network to learn a separate non-linear trans- formation and reduces the dimensionality of the compositional vectors. The parameters were opti- mised using gradient descent with initial learning rate 0.001, the ADAM algorithm ( <ref type="bibr">Kingma and Ba, 2015)</ref> for dynamically adapting the learning rate, and batch size of 64 sentences. F 0.5 on the devel- opment set was evaluated at each epoch, and the best model was used for final evaluations. <ref type="table">Table 1</ref> contains results for experiments compar- ing different composition architectures on the task of error detection. The CRF has the lowest F 0.5 score compared to any of the neural models. It memorises frequent error sequences with high pre- cision, but does not generalise sufficiently, result- ing in low recall. The ability to condition on the previous label also does not provide much help on this task -there are only two possible labels and the errors are relatively sparse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>The architecture using convolutional networks performs well and achieves the second-highest re- sult on the test set. It is designed to detect error patterns from a fixed window of 7 words, which is large enough to not require the use of more advanced composition functions. In contrast, the performance of the bidirectional recurrent network (Bi-RNN) is somewhat lower, especially on the test set. In Elman-type recurrent networks, the context signal from distant words decreases fairly rapidly due to the sigmoid activation function and diminishing gradients. This is likely why the Bi- RNN achieves the highest precision of all sys- tems -the predicted label is mostly influenced by the target token and its immediate neighbours, allowing the network to only detect short high- confidence error patterns. The convolutional net- work, which uses 7 context words with equal at- tention, is able to outperform the Bi-RNN despite the fixed-size context window.</p><p>The best overall result and highest F 0.5 is achieved by the bidirectional LSTM composition model <ref type="bibr">(Bi-LSTM)</ref>. This architecture makes use of the full sentence for building context vectors on both sides of the target token, but improves on Bi- RNN by utilising a more advanced composition function. Through the application of a linear up- date for the internal cell representation, the LSTM is able to capture dependencies over longer dis- tances. In addition, the gating functions allow it to adaptively decide which information to include in the hidden representations or output for error de- tection.</p><p>We found that using multiple layers of compo- sitional functions in a deeper network gave com- parable or slightly lower results for all the com- position architectures. This is in contrast to Ir-   <ref type="formula" target="#formula_1">(2014)</ref>, who experimented with Elman-type networks and found some improve- ments using multiple layers of Bi-RNNs. The dif- ferences can be explained by their task benefiting from alternative features: the evaluation was per- formed on opinion mining where most target se- quences are longer phrases that need to be identi- fied based on their semantics, whereas many errors in learner writing are short and can only be iden- tified by a contextual mismatch. In addition, our networks contain an extra hidden layer before the output, which allows the models to learn higher- level representations without adding complexity through an extra compositional layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Additional Training Data</head><p>There are essentially infinitely many ways of com- mitting errors in text and introducing additional training data should alleviate some of the prob- lems with data sparsity. We experimented with in- crementally adding different error-tagged corpora into the training set and measured the resulting performance. This allows us to provide some con- text to the results obtained by using each of the datasets, and gives us an estimate of how much annotated data is required for optimal performance on error detection. The datasets we consider are as follows:</p><p>• FCE-public -the publicly released subset of FCE (Yannakoudakis et al., 2011), as de- scribed in Section 4.</p><p>• NUCLE -the NUS Corpus of Learner En- glish ( <ref type="bibr" target="#b11">Dahlmeier et al., 2013)</ref>, used as the main training set for CoNLL shared tasks on error correction.</p><p>• IELTS -a subset of the IELTS examina- tion dataset extracted from the Cambridge Learner Corpus (CLC, Nicholls <ref type="formula" target="#formula_3">(2003)</ref>), con- taining 68,505 sentences from all proficiency levels, also used by <ref type="bibr" target="#b16">Felice et al. (2014)</ref>.</p><p>• FCE -a larger selection of FCE texts from the CLC, containing 323,192 sentences.</p><p>• CPE -essays from the proficient examination level in the CLC, containing 210,678 sen- tences.</p><p>• CAE -essays from the advanced examina- tion level in the CLC, containing 219,953 sentences. <ref type="table" target="#tab_2">Table 2</ref> contains results obtained by incremen- tally adding training data to the Bi-LSTM model. We found that incorporating the NUCLE dataset does not improve performance over using only the FCE-public dataset, which is likely due to the two corpora containing texts with different domains and writing styles. The texts in FCE are writ- ten by young intermediate students, in response to prompts eliciting letters, emails and reviews, whereas NUCLE contains mostly argumentative essays written by advanced adult learners. The differences in the datasets offset the benefits from additional training data, and the performance re- mains roughly the same. In contrast, substantial improvements are ob- tained when introducing the IELTS and FCE datasets, with each of them increasing the F 0.5 score by roughly 10%. The IELTS dataset con- tains essays from all proficiency levels, and FCE from mid-level English learners, which provides the model with a distribution of 'average' errors to learn from. Adding even more training data from  high-proficiency essays in CPE and CAE only pro- vides minor further improvements. <ref type="figure" target="#fig_1">Figure 2</ref> also shows F 0.5 on the FCE-public test set as a function of the total number of tokens in the training data. The optimal trade-off between performance and data size is obtained at around 8 million tokens, after introducing the FCE dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CoNLL-14 Shared Task</head><p>The CoNLL-14 shared task ( <ref type="bibr" target="#b27">Ng et al., 2014</ref>) focussed on automatically correcting errors in learner writing. The NUCLE dataset was pro- vided as the main training dataset, but participants were allowed to include other annotated corpora and external resources. For evaluation, 25 stu- dents were recruited to each write two new essays, which were then annotated by two experts.</p><p>We used the same methods from Section 4 for converting the shared task annotation to a token- level labeling task in order to evaluate the mod- els on error detection. In addition, the correction outputs of all the participating systems were made available online, therefore we are able to report their performance on this task. In order to con- vert their output to error detection labels, the cor- rected sentences were aligned with the original in- put using Levenshtein distance, and any changes proposed by the system resulted in the correspond- ing source words being labeled as errors.</p><p>The results on the two annotations of the shared task test data can be seen in <ref type="table" target="#tab_4">Table 3</ref>. We first eval- uated each of the human annotators with respect to the other, in order to estimate the upper bound on this task. The average F 0.5 of roughly 50% shows that the task is difficult and even human experts have a rather low agreement. It has been shown before that correcting grammatical errors is highly subjective <ref type="bibr" target="#b3">(Bryant and Ng, 2015)</ref>, but these results indicate that trained annotators can disagree even on the number and location of errors.</p><p>In the same table, we provide error detection re- sults for the top 3 participants in the shared task: CAMB <ref type="bibr" target="#b16">(Felice et al., 2014</ref>), CUUI ( <ref type="bibr" target="#b26">Rozovskaya et al., 2014)</ref>, and AMU <ref type="bibr" target="#b23">(Junczys-Dowmunt and Grundkiewicz, 2014</ref>). They each preserve their relative ranking also in the error detection evalu- ation. The CAMB system has a lower precision but the highest recall, also resulting in the highest F 0.5 . CUUI and AMU are close in performance, with AMU having slightly higher precision. After the official shared task, <ref type="bibr" target="#b27">Susanto et al. (2014)</ref> published a system which combines several alternative models and outperforms the shared task participants when evaluated on error correction. However, on error detection it receives lower re- sults, ranking 3rd and 4th when evaluated on F 0.5 (P1+P2+S1+S2 in <ref type="table" target="#tab_4">Table 3</ref>). The system has de- tected a small number of errors with high preci- sion, and does not reach the highest F 0.5 .</p><p>Finally, we present results for the Bi-LSTM se- quence labeling system for error detection. Using only FCE-public for training, the overall perfor- mance is rather low as the training set is very small and contains texts from a different domain. How- ever, these results show that the model behaves as expected -since it has not seen similar language during training, it labels a very large portion of to- kens as errors. This indicates that the network is trying to learn correct language constructions from the limited data and classifies unseen structures as errors, as opposed to simply memorising error se- quences from the training data.</p><p>When trained on all the datasets from Section 6, the model achieves the highest F 0.5 of all sys- tems on both of the CoNLL-14 shared task test annotations, with an absolute improvement of 3% over the previous best result. It is worth noting that the full Bi-LSTM has been trained on more data than the other CoNLL contestants. However, as the shared task systems were not restricted to the NUCLE training set, all the submissions also used differing amounts of training data from vari- ous sources. In addition, the CoNLL systems are mostly combinations of many alternative models: the CAMB system is a hybrid of machine transla- tion, a rule-based system, and a language model re-ranker; CUUI consists of different classifiers for each individual error type; and P1+P2+S1+S2 is a combination of four different error correction systems. In contrast, the Bi-LSTM is a single model for detecting all error types, and therefore represents a more scalable data-driven approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Essay Scoring</head><p>In this section, we perform an extrinsic evalua- tion of the efficacy of the error detection system and examine the extent to which it generalises at higher levels of granularity on the task of auto- mated essay scoring. More specifically, we repli- cate experiments using the text-level model de- scribed by <ref type="bibr" target="#b0">Andersen et al. (2013)</ref>, which is cur- rently deployed in a self-assessment and tutoring system (SAT), an online automated writing feed- back tool actively used by language learners. <ref type="bibr">2</ref> The SAT system predicts an overall score for a given text, which provides a holistic assessment of linguistic competence and language proficiency. The authors trained a supervised ranking percep- tron model on the FCE-public dataset, using fea- tures such as error-rate estimates from a language model and various lexical and grammatical prop- erties of text (e.g., word n-grams, part-of-speech n-grams and phrase-structure rules). We replicate this experiment and add the average probability of each token in the essay being correct, accord- ing to the error detection model, as an additional feature for the scoring framework. The system was then retrained on FCE-public and evaluated on correctly predicting the assigned essay score. <ref type="table" target="#tab_5">Table 4</ref> presents the experimental results.</p><p>The human performance on the test set is cal-r ρ</p><p>Human annotators 79.6 79.2 SAT 75.1 76.0 SAT + Bi-LSTM (FCE-public) 76.0 77.0 SAT + Bi-LSTM (full)</p><p>78.0 79.9 culated as the average inter-annotator correlation on the same data, and the existing SAT system has demonstrated levels of performance that are very close to that of human assessors. Nevertheless, the Bi-LSTM model trained only on FCE-public complements the existing features, and the com- bined model achieves an absolute improvement of around 1% percent, corresponding to 20-31% rela- tive error reduction with respect to the human per- formance. Even though the Bi-LSTM is trained on the same dataset and the SAT system already includes various linguistic features for capturing errors, our error detection model manages to fur- ther improve its performance. When the Bi-LSTM is trained on all the avail- able data from Section 6, the combination achieves further substantial improvements. The relative er- ror reduction on Pearson's correlation is 64%, and the system actually outperforms human annotators on Spearman's correlation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusions</head><p>In this paper, we presented the first experiments using neural network models for the task of er- ror detection in learner writing. Six alternative compositional network architectures for modeling context were evaluated. Based on the findings, we propose a novel error detection framework us- ing token-level embeddings, bidirectional LSTMs for context representation, and a multi-layer archi- tecture for learning more complex features. This structure allows the model to classify each token as being correct or incorrect, using the full sen- tence as context. The self-modulation architecture of LSTMs was also shown to be beneficial, as it al- lows the network to learn more advanced composi- tion rules and remember dependencies over longer distances.</p><p>Substantial performance improvements were achieved by training the best model on additional datasets. We found that the largest benefit was ob- tained from training on 8 million tokens of text from learners with varying levels of language pro- ficiency. In contrast, including even more data from higher-proficiency learners gave marginal further improvements. As part of future work, it would be beneficial to investigate the effect of au- tomatically generated training data for error detec- tion (e.g., <ref type="bibr">Rozovskaya and Roth (2010)</ref>).</p><p>We evaluated the performance of existing error correction systems from CoNLL-14 on the task of error detection. The experiments showed that suc- cess on error correction does not necessarily mean success on error detection, as the current best cor- rection system (P1+P2+S1+S2) is not the same as the best shared task detection system (CAMB). In addition, the neural sequence tagging model, spe- cialised for error detection, was able to outperform all other participating systems.</p><p>Finally, we performed an extrinsic evaluation by incorporating probabilities from the error detec- tion system as features in an essay scoring model. Even without any additional data, the combina- tion further improved performance which is al- ready close to the results from human annotators. In addition, when the error detection model was trained on a larger training set, the essay scorer was able to exceed human-level performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Alternative neural composition architectures for error detection. a) Convolutional network b) Deep convolutional network c) Recurrent bidirectional network d) Deep recurrent bidirectional network. The bottom layers are embeddings for individual tokens. The middle layers are context-dependent representations, built using different composition functions. The top layers are softmax output layers, predicting a label distribution for every input token.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: F 0.5 measure on the public FCE test set, as a function of the total number of tokens in the training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Results on the public FCE test set when 
incrementally providing more training data to the 
error detection model. 

soy and Cardie </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 : Error detection results on the two official annotations for the CoNLL-14 shared task test dataset.</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Pearson's correlation r and Spearman's 
correlation ρ on the public FCE test set on the task 
of automated essay scoring. 

</table></figure>

			<note place="foot" n="2"> http://www.cambridgeenglish.org/learning-english/freeresources/write-and-improve/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Prof Ted Briscoe and the reviewers for providing useful feedback.</p></div>
			</div>

			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Developing and testing a self-assessment and tutoring system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Øistein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fiona</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the Eighth Workshop on Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Neural Probabilistic Language Model Yoshua</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">How Far are We from Fully Automatic High Quality Grammatical Error Correction?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Thorsten Brants, Phillipp Koehn, and Tony Robinson. In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the Properties of Neural Machine Translation: EncoderDecoder Approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning Phrase Representations using RNN EncoderDecoder for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An unsupervised method for detecting grammatical errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Leacock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the first conference on North American chapter of the Association for Computational Linguistics</title>
		<meeting>the first conference on North American chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Detection of grammatical errors involving prepositions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">R</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Na-Rae</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th ACLSIGSEM Workshop on Prepositions</title>
		<meeting>the 4th ACLSIGSEM Workshop on Prepositions</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Problems in evaluating grammatical error detection systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Israel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">E</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of learner English: The NUS corpus of learner English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siew Mei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the Eighth Workshop on Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Helping Our Own: The HOO 2011 Pilot Shared Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kilgarriff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th European Workshop on Natural Language Generation</title>
		<meeting>the 13th European Workshop on Natural Language Generation</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">HOO 2012: A report on the Preposition and Determiner Error Correction Shared Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Anisimoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Narroway</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Seventh Workshop on Building Educational Applications Using NLP</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards a standard evaluation method for grammatical error detection and correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariano</forename><surname>Felice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2015 Annual Conference of the North American Chapter of the ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Grammatical error correction using hybrid systems and type filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariano</forename><surname>Felice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Øistein</forename><forename type="middle">E</forename><surname>Andersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computational Natural Language Learning: Shared Task</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Helen Yannakoudakis, and Ekaterina Kochmar. CoNLL2014</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">High-Order Sequence Modeling for Language Learner Error Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the Sixth Workshop on Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hybrid speech recognition with Deep Bidirectional LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdel</forename><forename type="middle">Rahman</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Automatic Speech Recognition and Understanding</title>
		<imprint>
			<publisher>ASRU</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Detecting Errors in English Article Usage with a Maximum Entropy Classifier Trained on a Large</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Na-Rae</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Leacock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Diverse Corpus. Proceedings of the 4th International Conference on Language Resources and Evaluation</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Detecting errors in English article usage by non-native speakers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Na-Rae</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Leacock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Long Short-term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Opinion Mining with Deep Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP2014</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Dowmunt</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
		<title level="m">The AMU System in the CoNLL-2014</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Shared Task: Grammatical Error Correction by Data-Intensive and Feature-Rich Statistical Machine Translation</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning: Shared Task</meeting>
		<imprint/>
	</monogr>
	<note>CoNLL-2014</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The University of Illinois System in the CoNLL-2013 Shared Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alla</forename><surname>Rozovskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sammons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning: Shared Task</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The IllinoisColumbia System in the CoNLL-2014 Shared Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alla</forename><surname>Rozovskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sammons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning: Shared Task</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>CoNLL-2014</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">System Combination for Grammatical Error Correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond Hendy</forename><surname>Susanto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Phandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">2014</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The Ups and Downs of Preposition Error Detection in ESL Writing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">R</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Computational Linguistics</title>
		<meeting>the 22nd International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A New Dataset and Method for Automatically Grading ESOL Texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Medlock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
