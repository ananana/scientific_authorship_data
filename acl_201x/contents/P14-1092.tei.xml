<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Discourse Complements Lexical Semantics for Non-factoid Answer Reranking</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Jansen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Allen Institute for Artificial Intelligence Seattle</orgName>
								<orgName type="institution">University of Arizona Tucson</orgName>
								<address>
									<region>AZ, WA</region>
									<country>USA, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Allen Institute for Artificial Intelligence Seattle</orgName>
								<orgName type="institution">University of Arizona Tucson</orgName>
								<address>
									<region>AZ, WA</region>
									<country>USA, USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
							<email>peterc@allenai.org</email>
							<affiliation key="aff0">
								<orgName type="department">Allen Institute for Artificial Intelligence Seattle</orgName>
								<orgName type="institution">University of Arizona Tucson</orgName>
								<address>
									<region>AZ, WA</region>
									<country>USA, USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Discourse Complements Lexical Semantics for Non-factoid Answer Reranking</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="977" to="986"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a robust answer reranking model for non-factoid questions that integrates lexical semantics with discourse information , driven by two representations of discourse: a shallow representation centered around discourse markers, and a deep one based on Rhetorical Structure Theory. We evaluate the proposed model on two corpora from different genres and domains: one from Yahoo! Answers and one from the biology domain, and two types of non-factoid questions: manner and reason. We experimentally demonstrate that the discourse structure of non-factoid answers provides information that is complementary to lexical semantic similarity between question and answer, improving performance up to 24% (relative) over a state-of-the-art model that exploits lexical semantic similarity alone. We further demonstrate excellent domain transfer of discourse information, suggesting these discourse features have general utility to non-factoid question answering.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Driven by several international evaluations and workshops such as the Text REtrieval Conference (TREC) <ref type="bibr">1</ref> and the Cross Language Evaluation Fo- rum (CLEF), <ref type="bibr">2</ref> the task of question answering (QA) has received considerable attention. However, most of this effort has focused on factoid questions rather than more complex non-factoid (NF) ques- tions, such as manner, reason, or causation ques- tions. Moreover, the vast majority of QA mod- els explore only local linguistic structures, such as syntactic dependencies or semantic role frames, which are generally restricted to individual sen- tences. This is problematic for NF QA, where questions are answered not by atomic facts, but by larger cross-sentence conceptual structures that convey the desired answers. Thus, to answer NF questions, one needs a model of what these answer structures look like.</p><p>Driven by this observation, our main hypothe- sis is that the discourse structure of NF answers provides complementary information to state-of- the-art QA models that measure the similarity (ei- ther lexical and/or semantic) between question and answer. We propose a novel answer reranking (AR) model that combines lexical semantics (LS) with discourse information, driven by two rep- resentations of discourse: a shallow representa- tion centered around discourse markers and sur- face text information, and a deep one based on the Rhetorical Structure Theory (RST) discourse framework ( <ref type="bibr" target="#b6">Mann and Thompson, 1988)</ref>. To the best of our knowledge, this work is the first to systematically explore within-and cross-sentence structured discourse features for NF AR. The con- tributions of this work are:</p><p>1. We demonstrate that modeling discourse is greatly beneficial for NF AR for two types of NF questions, manner ("how") and rea- son ("why"), across two large datasets from different genres and domains -one from the community question-answering (CQA) site of Yahoo! Answers 3 , and one from a biology textbook. Our results show statistically sig- nificant improvements of up to 24% on top of state-of-the-art LS models ( <ref type="bibr" target="#b21">Yih et al., 2013)</ref>.</p><p>2. We demonstrate that both shallow and deep discourse representations are useful, and, in general, their combination performs best.</p><p>3. We show that discourse-based QA models us- ing inter-sentence features considerably out-perform single-sentence models when an- swers span multiple sentences. 4. We demonstrate good domain transfer per- formance between these corpora, suggesting that answer discourse structures are largely independent of domain, and thus broadly ap- plicable to NF QA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The body of work on factoid QA is too broad to be discussed here (see, e.g., the TREC workshops for an overview). However, in the context of LS, <ref type="bibr" target="#b21">Yih et al. (2013)</ref> recently addressed the problem of an- swer sentence selection and demonstrated that LS models, including recurrent neural network lan- guage models (RNNLM), have a higher contribu- tion to overall performance than exploiting syntac- tic analysis. We extend this work by showing that discourse models coupled with LS achieve the best performance for NF AR.</p><p>The related work on NF QA is considerably more scarce, but several trends are clear. First, most NF QA approaches tend to use multiple sim- ilarity models (information retrieval or alignment) as features in discriminative rerankers ( <ref type="bibr" target="#b15">Riezler et al., 2007;</ref><ref type="bibr" target="#b4">Higashinaka and Isozaki, 2008;</ref><ref type="bibr" target="#b18">Verberne et al., 2010;</ref><ref type="bibr" target="#b16">Surdeanu et al., 2011</ref>). Sec- ond, and more relevant to this work, all these ap- proaches focus either on bag-of-word representa- tions or linguistic structures that are restricted to single sentences (e.g., syntactic dependencies, se- mantic roles, or standalone discourse cue phrases).</p><p>Answering how questions using a single dis- course marker, by, was previously explored by <ref type="bibr" target="#b13">Prager et al. (2000)</ref>, who searched for by followed by a present participle (e.g. by *ing) to elevate an- swer candidates in a ranking framework. Verberne et al. (2011) extracted 47 cue phrases such as be- cause from a small collection of web documents, and used the cosine similarity between an answer candidate and a bag of words containing these cue phrases as a single feature in their reranking model for non-factoid why QA. Extending this, <ref type="bibr" target="#b11">Oh et al. (2013)</ref> built a classifier to identify causal re- lations using a small set of cue phrases (e.g., be- cause and is caused by). This classifier was then used to extract instances of causal relations in an- swer candidates, which were turned into features in a reranking model for Japanense why QA.</p><p>In terms of discourse parsing, <ref type="bibr" target="#b17">Verberne et al. (2007)</ref> conducted an initial evaluation of the util- ity of RST structures to why QA by evaluating performance on a small sample of seven WSJ ar- ticles drawn from the RST Treebank ( <ref type="bibr" target="#b0">Carlson et al., 2003</ref>). They later concluded that while dis- course parsing appears to be useful for QA, auto- mated discourse parsing tools are required before this approach can be tested at scale <ref type="bibr" target="#b18">(Verberne et al., 2010)</ref>. Inspired by this previous work and re- cent work in discourse parsing <ref type="bibr" target="#b2">(Feng and Hirst, 2012)</ref>, our work is the first to systematically ex- plore structured discourse features driven by sev- eral discourse representations, combine discourse with lexical semantic models, and evaluate these representations on thousands of questions using both in-domain and cross-domain experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>The proposed answer reranking component is em- bedded in the QA framework illustrated in <ref type="figure" target="#fig_0">Figure  1</ref>. This framework functions in two distinct sce- narios, which use the same AR model, but differ in the way candidate answers are retrieved:</p><p>CQA: In this scenario, the task is defined as reranking all the user-posted answers for a particu- lar question to boost the community-selected best answer to the top position. This is a commonly used setup in the CQA community ( <ref type="bibr" target="#b20">Wang et al., 2009</ref>). <ref type="bibr">4</ref> Thus, for a given question, all its answers are fetched from the answer collection, and an ini- tial ranking is constructed based on the cosine sim- ilarity between theirs and the question's lemma vector representations, with lemmas weighted us- ing tf.idf (Ch. 6, ( <ref type="bibr" target="#b7">Manning et al., 2008)</ref>).</p><p>Traditional QA: In this scenario answers are dynamically constructed from larger docu- ments <ref type="bibr" target="#b12">(Pasca, 2001</ref>). We use this setup to answer questions from a biology textbook, where each section is indexed as a standalone document, and each paragraph in a given document is considered as a candidate answer. We implemented the docu- ment indexing and retrieval stage using Lucene <ref type="bibr">5</ref> . The candidate answers are scored using a linear interpolation of two cosine similarity scores: one between the entire parent document and question (to model global context), and a second between the answer candidate and question (for local context). <ref type="bibr">6</ref> Because the number of answer candidates is typically large (e.g., equal to the number of paragraphs in the textbook), we return the N top candidates with the highest scores.</p><p>These answer candidates are then passed to the answer reranking component, the focus of this work. AR analyzes the candidates using more expensive techniques to extract discourse and LS features (detailed in §4), and these features are then used in concert with a learning framework to rerank the candidates and elevate correct answers to higher positions. For the learning framework, we used SVM rank , a variant of Support Vector Machines for structured output adapted to rank- ing problems. <ref type="bibr">7</ref> In addition to these features, each reranker also includes a single feature containing the score of each candidate, as computed by the above candidate retrieval (CR) component. <ref type="bibr">8</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Models and Features</head><p>We propose two separate discourse representation schemes -one shallow, centered around discourse markers, and one deep, based on RST.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Discourse Marker Model</head><p>The discourse marker model (DMM) extracts cross-sentence discourse structures centered around a discourse marker. This extraction pro- cess is illustrated in the top part of <ref type="figure" target="#fig_1">Figure 2</ref>. These structures are represented using three components: (1) A discourse marker from Daniel Marcu's list <ref type="bibr">5</ref> http://lucene.apache.org <ref type="bibr">6</ref> We empirically observed that this combination of scores performs better than using solely the cosine similarity be- tween the answer and question.</p><p>7 http://www.cs.cornell.edu/people/tj/ svm_light/svm_rank.html 8 Including these scores as features in the reranker model is a common strategy that ensures that the reranker takes ad- vantage of the analysis already performed by the CR model. (see <ref type="bibr">Appendix B in Marcu (1997)</ref>), that serves as a divisive boundary between sentences. Examples of these markers include and, in, that, for, if, as, not, by, and but; (2) two marker arguments, i.e., text segments before and after the marker, labeled to indicate if they are related to the question text or not; and (3) a sentence range around the marker, which defines the length of these segments (e.g., ±2 sentences). For example, a marker feature may take the form of: QSEG BY OTHER SR2, which means that the the marker by has been detected in an answer candidate. Further, the text preceeding by matches text from the question (and is therefore labeled QSEG), while the text after by differs considerably from the question text, and is labeled OTHER. In this particular example, the scope of this similarity matching occurs over a span of ±2 sentences around the marker.</p><p>Note that our marker arguments are akin to EDUs in RST, but, in this shallow representa- tion, they are simply constructed around discourse markers and bound by an arbitrary sentence range.</p><p>Argument Labels: We label marker arguments based on their similarity to question content. If text before or after a marker out to a given sen- tence range matches the entire text of the ques- tion (with a cosine similarity score larger than a threshold), that argument takes on the label QSEG, or OTHER otherwise. In this way the features are only partially lexicalized with the discourse mark- ers. Argument labels indicate only if lemmas from the question were found in a discourse structure present in an answer candidate, and do not speak to the specific lemmas that were found. We show in §5 that these lightly lexicalized features perform well in domain and transfer between domains. We explore other argument labeling strategies in §5.7.</p><p>Feature Values: Our reranking framework uses real-valued features. The values of the discourse features are the mean of the similarity scores (e.g., cosine similarity using tf.idf weighting) of the two marker arguments and the corresponding question. For example, the value of the QSEG BY QSEG SR1 feature in <ref type="figure" target="#fig_1">Figure 2</ref> is the average of the cosine sim- ilarities of the question text with the answer texts before/after by out to a distance of one sentence before/after the marker.</p><p>It is important to note that these discourse features are more expressive than features based on discourse markers alone <ref type="bibr" target="#b4">(Higashinaka and Isozaki, 2008;</ref><ref type="bibr" target="#b18">Verberne et al., 2010</ref>). First, the argument sequences used here capture cross- sentence discourse structures. Second, these fea- tures model the intensity of the match between the text surrounding the discourse structure and the question text using both the assigned argument la- bels and the feature values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Discourse Parser Model</head><p>The discourse parser model (DPM) is based on the RST discourse framework ( <ref type="bibr" target="#b6">Mann and Thompson, 1988)</ref>. In RST, the text is segmented into a se- quence of non-overlapping fragments called ele- mentary discourse units (EDUs), and binary dis- course relations recursively connect neighboring units. Most relations are hypotactic, where one of the units in the relation (the nucleus) is consid- ered more important than the other (the satellite). A few relations are paratactic, where both partici- pants have equal importance. In the bottom part of <ref type="figure" target="#fig_1">Figure 2</ref>, we show hypotactic relations as directed arrows, from the nucleus to the satellite. In this work, we construct the RST discourse trees using the parser of <ref type="bibr" target="#b2">Feng and Hirst (2012)</ref>.</p><p>Relying on a proper discourse framework facil- itates the modeling of the numerous implicit re- lations that are not driven by discourse markers (see Ch. 21 in <ref type="bibr" target="#b5">Jurafsky and Martin (2009)</ref>). How- ever, this also introduces noise because discourse analysis is a complex task and discourse parsers are not perfect. To mitigate this, we used a sim- ple feature generation strategy, which creates one feature for each individual discourse relation by concatenating the relation type with the labels of the discourse units participating in it. To this end, for every relation, we extract the entire text dom- inated by each of its arguments, and we gener- ate labels for the two participants in the relation using the same strategy as the DMM (based on the similarity with the question content). Similar to the DMM, these features take real values ob- tained by averaging the cosine similarity of the ar- guments with the question content. 9 <ref type="figure" target="#fig_1">Fig. 2</ref> shows several such features, created around two RST Elaboration relations, indicating that the latter sentences expand on the information at the begin- ning of the answer. Other common relations in- clude Attribution, Contrast, Background, and Evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Lexical Semantics Model</head><p>Inspired by the work of <ref type="bibr" target="#b21">Yih et al. (2013)</ref>, we in- clude lexical semantics in our reranking model. Several of their proposed models rely on propri- etary data; here we focus on LS models that rely on open-source data and frameworks. In particu- lar, we use the recurrent neural network language model (RNNLM) of <ref type="bibr" target="#b10">Mikolov et al. (2013;</ref>. Like any language model, a RNNLM estimates the probability of observing a word given the preced- ing context, but, in this process, it learns word embeddings into a latent, conceptual space with a fixed number of dimensions. Consequently, re- lated words tend to have vectors that are close to each other in this space.</p><p>We derive two LS measures from these vec- tors, which are then are included as features in the reranker. The first is a measure of the over- all LS similarity of the question and answer can-didate, which is computed as the cosine similarity between the two composite vectors of the ques- tion and the answer candidate. These composite vectors are assembled by summing the vectors for individual question (or answer candidate) words, and re-normalizing this composite vector to unit length. Both this overall similarity score, as well as the average pairwise cosine similarity between each word in the question and answer candidate, serve as features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data</head><p>To test the utility of our approach, we experi- mented with the two QA scenarios introduced in §3 using the following two datasets: The entire biology text (at para- graph granularity) serves as the possible set of an- swers. Note that while our system retrieves an- swers at paragraph granularity, the expert was not constrained in any way during the annotation pro- cess, so gold answers might be smaller than a para- graph or span multiple paragraphs. This compli- cates evaluation metrics on this dataset (see §5.3).</p><formula xml:id="formula_0">Yahoo</formula><p>10 http://answers.yahoo.com 11 Note that our experimental setup, i.e., reranking all the answers provided for each question, is different from that of Surdeanu et al. For each question, they retrieved candidate answers from all answers voted as best for some question in the collection. The setup in this paper, commonly used in the CQA community ( <ref type="bibr" target="#b20">Wang et al., 2009)</ref>, is more relevant here because it includes both high and low quality answers.</p><p>For the YA CQA corpora, 50% of QA pairs were used for training, 25% for development, and 25% for test. Because of the small size of the Bio corpus, it was evaluated using 5-fold cross- validation, with three folds for training, one for development, and one for test.</p><p>The following additional resources were used:</p><p>Discourse Markers: A set of 75 high-frequency 12 single-word discourse markers were extracted from Marcu's (1997) list of cue phrases, and used for feature generation in DMM. These discourse markers are extremely common in the answer cor- pora -for example, the YA corpus contains an av- erage of 7 markers per answer.  <ref type="bibr">13</ref> For the Bio experiments, we trained a domain specific RNNLM over a concatenation of the textbook and a subset of Wikipedia spe- cific to biology. The latter was created by ex- tracting: (a) pages matching a word/phrase in a glossary of biology (derived from the textbook); plus (b) pages hyperlinked from (a) that are also tagged as being in a small set of (hand-selected) biology-related categories. The combined dataset contains 7.7M words. For all RNNLMs we used 200-dimensional vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Hyper Parameter Tuning</head><p>The following hyper parameters were tuned using grid search to maximize P@1 on each develop- ment partition: (a) the segment matching thresh- olds that determine the minimum cosine simi- larity between an answer segment and a ques- tion for the segment to be labeled QSEG; and (b) <ref type="bibr">12</ref> We selected all cue phrases with more than 100 occur- rences in the Brown corpus. <ref type="bibr">13</ref>   in a given column. * indicates that a score is significantly bet- ter (p &lt; 0.05) than the score of the corresponding baseline.</p><p>All significance tests were implemented using one-tailed non- parametric bootstrap resampling using 10,000 iterations. SVM rank 's regularization parameter C. For all ex- periments, the sentence range parameter (SRx) for DMM ranged from 0 (within sentence) to ±3 sen- tences. <ref type="bibr">14</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation Metrics</head><p>For YA, we used the standard implementations for P@1 and mean reciprocal rank (MRR) ( <ref type="bibr" target="#b7">Manning et al., 2008</ref>). In the Bio corpus, because answer candidates are not guaranteed to match gold anno- tations exactly, these metrics do not immediately apply. We adapted them to this dataset by weigh- ing each answer by its overlap with gold answers, where overlap is measured as the highest F1 score between the candidate and a gold answer. Thus, P@1 reduces to this F1 score for the top answer. For MRR, we used the rank of the candidate with the highest overlap score, weighed by the inverse of the rank. For example, if the best answer for a question appears at rank 2 with an F1 score of 0.3, the corresponding MRR score is 0.3/2. <ref type="table" target="#tab_2">Table 1</ref> analyzes the performance of the proposed reranking model on the three datasets and against two baselines. The first baseline sorts the can- didate answers in descending order of the scores produced by the candidate retrieval (CR) module. The second baseline (CR + LS) trains a rerank- ing model without discourse, using just the CR and LS features. For YA, we include an addi- tional baseline that selects an answer randomly. We list multiple versions of the proposed rerank- ing model, broken down by the features used. For Bio, we retrieved the top 20 answer candidates in CR. At this setting, the oracle performance (i.e., the performance with perfect reranking of the 20 candidates) was 69.6% P@1 for Bio HOW, and 72.3% P@1 for Bio WHY. These relatively low oracle scores, which serve as a performance ceil- ing for our approach, highlight the difficulty of the task. For YA, we used all answers provided for each given question. For all experiments we used a linear SVM kernel. <ref type="bibr">15</ref> Examining <ref type="table" target="#tab_2">Table 1</ref>, several trends are clear. Both discourse models significantly increase both P@1 and MRR performance over all baselines broadly across genre, domain, and question types. More specifically, DMM and DPM show similar performance benefits when used individually, but their combination generally outperforms the indi- vidual models, illustrating the fact that the two models capture related but different discourse in- formation. This is a motivating result for discourse analysis, especially considering that the discourse parser was trained on a domain different from the corpora used here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Overall Results</head><p>Lexical semantic features increase performance for all settings, but demonstrate far more utility to the open-domain YA corpus. This disparity is likely due to the difficulty in assembling LS training data at an appropriate level for the bi- ology corpus, contrasted with the relative abun- dance of large scale open-domain lexical seman- tic resources. For the YA corpus, where lexical semantics showed the most benefit, simply adding <ref type="bibr">15</ref> The performance of all models can ultimately be in- creased by using more sophisticated learning frameworks, and considering more answer candidates in CR (for Bio). For example, SVMs with polynomial kernels of degree two showed approximately half a percent (absolute) performance gain over the linear kernel. However, this came at the ex- pense of an experiment runtime about an order of magni- tude larger. Experiments with more answer candidates in Bio showed similar trends to the results reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q How does myelination affect action potentials? A baseline</head><p>The major selective advantage of myelination is its space ef- ficiency. A myelinated axon 20 microns in diameter has a conduction speed faster than that of a squid giant axon <ref type="bibr">[. . . ]</ref>. Furthermore, more than 2,000 of those myelinated axons can be packed into the space occupied by just one giant axon. A rerank A nerve impulse travels [. . . ] to the synaptic terminals by propagation of a series action potentials along the axon. The speed of conduction increases <ref type="bibr">[. . . ]</ref> with myelination. Action potentials in myelinated axons jump between the nodes of Ranvier, a process called saltatory conduction. where the correct answer is elevated to the top position by the discourse model. Abaseline is the top answer proposed by the CR + LS baseline, which is incorrect, whereas Arerank is the correct answer boosted to the top after reranking. <ref type="bibr">[. . . ]</ref> indicates non-essential text that was removed for space.</p><p>LS features to the CR baseline increases baseline P@1 performance from 19.57 to 26.57, a +36% relative improvement. Most importantly, compar- ing lines 5 and 9 with their respective baselines (lines 2 and 6, respectively) indicates that LS is largely orthogonal to discourse. Line 5, the top- performing model with discourse but without LS outperforms the CR baseline by +5.24 absolute P@1 improvement. Similarly, line 9, the top- performing model that combines discourse with LS has a +5.69 absolute P@1 improvement over the CR + LS baseline. That this absolute perfor- mance increase is nearly identical indicates that LS features are complementary to and additive with the full discourse model. Indeed, an analy- sis of the questions improved by discourse vs. LS (line 5 vs. 6) showed that the intersection of the two sets is low (approximately a third of each set). Finally, while the discourse models perform well for HOW or manner questions, performance on Bio WHY corpus suggests that reason ques- tions are particularly amenable to discourse anal- ysis. Relative improvements on WHY questions reach +38% (without LS) and +24% (with LS), with absolute performance on these non-factoid questions jumping from 28% to nearly 40% P@1. <ref type="table" target="#tab_3">Table 2</ref> shows one example where discourse helps boost the correct answer to the top posi- tion. In this example, the correct answer con- tains multiple Elaboration relations that are both cross sentence (e.g., between the first two sen- tences) and intra-sentence (e.g., between the first part of the second sentence and the phrase "with myelination"). Model features associated with Elaboration relations are ranked highly by the learned model. In contrast, the answer preferred by the baseline contains mostly Joint relations,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Range</head><p>Bio HOW Bio WHY YA CR + LS + DMM + DPM within-sentence +0.8% +8.4% +13.1% full model +21.0% * +23.9% * +14.8% which "represent the lack of a rhetorical relation between the two nuclei" ( <ref type="bibr" target="#b6">Mann and Thompson, 1988)</ref> and have very small weights in the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Intra vs. Inter-sentence Features</head><p>To tease apart the relative contribution of dis- course features that occur only within a single sentence versus features that span multiple sen- tences, we examined the performance of the full model when using only intra-sentence features, i.e., SR0 features for DMM, and features based on discourse relations where both EDUs appear in the same sentence for DPM, versus the full intersen- tence models. The results are shown in <ref type="table" target="#tab_4">Table 3</ref>.</p><p>For the Bio corpus where answer candidates consist of entire paragraphs of a biology text, over- all performance is dominated by inter-sentence discourse features. Conversely, for YA, a large proportion of performance comes from features that span only a single sentence. This is caused by the fact that YA answers are far shorter and of variable grammatical quality, with 39% of an- swer candidates consisting of only a single sen- tence, and 57% containing two or fewer sentences. All in all, this experiment emphasizes that model- ing both intra-and inter-sentence discourse (where available) is beneficial for non-factoid QA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Domain Transfer</head><p>Because these discourse models appear to cap- ture high-level information about answer struc- tures, we hypothesize that the models should make use of many of the same discourse features, even when training on data from different domains. <ref type="table" target="#tab_6">Ta- ble 4</ref> shows that of the highest-weighted SVM features learned when training models for HOW questions on YA and Bio, many are shared (e.g., 56.5% of the features in the top half of both DPMs are shared), suggesting that a core set of discourse features may be of utility across domains.</p><p>To test the generality of these features, we per- formed a transfer study where the full model was trained and tuned on the open-domain YA cor- pus, then evaluated as is on Bio HOW. This is  . These experiments were performed in sev- eral groups: both with and without LS features, as well as using either a single SVM or an ensem- ble model that linearly interpolates the predictions of two SVM classifiers (one each for DMM and DPM). <ref type="bibr">16</ref> The results are summarized in <ref type="table" target="#tab_8">Table 5</ref>. The transferred models always outperform the baselines, but only the ensemble model's improve- ment is statistically significant. This confirms ex- isting evidence that ensemble models perform bet- ter cross-domain because they overfit less <ref type="bibr" target="#b1">(Domingos, 2012;</ref><ref type="bibr" target="#b3">Hastie et al., 2009</ref>). The ensemble model without LS (third line) has a nearly identi- cal P@1 score as the equivalent in-domain model (line 13 in <ref type="table" target="#tab_2">Table 1</ref>), while slightly surpassing in- domain MRR performance. To the best of our knowledge, this is one of the most striking demon- strations of domain transfer in answer ranking for non-factoid QA, and highlights the generality of these discourse features in identifying answer structures across domains and genres.</p><p>The results of the transferred models that in- clude LS features are slightly lower, but still ap- proach statistical significance for P@1 and are sig- nificant for MRR. We hypothesize that the limited transfer observed for models with LS compared to their counterparts without LS is due to the dispar- ity in the size and utility of the biology LS training data compared to the open-domain LS resources. The open-domain YA model learns to place more weight on LS features, which are unable to provide the same utility in the biology domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Integrating Discourse and LS</head><p>So far, we have treated LS and discourse as dis- tinct features in the reranking model, However, given that LS features greatly improve the CR baseline, we hypothesize that a natural extension  to the discourse models would be to make use of LS similarity (in addition to the traditional infor- mation retrieval similarity) to label discourse seg- ments. For example, for the question "How do cells replicate?", answer discourse segments con- taining LS associates of cell and replicate, e.g., nu- cleus, membrane, genetic, and duplicate, should be considered as related to the question (i.e., be labeled QSEG). We implemented two such mod- els, denoted DMM LS and DPM LS , by replacing the component that assigns argument labels with one that relies on LS. Specifically, as in §4.3, we compute the cosine similarity between the com- posite LS vectors of the question text and each marker argument (in DMM) or EDU (in DPM), and label the corresponding answer segment QSEG if this score is higher than a threshold, or OTHER otherwise. This way, the DMM and DPM features jointly capture discourse structures and semantic similarity between answer segments and question.</p><p>To test this, we use the YA corpus, which has the best-performing LS model. Because we are adding two new discourse models, we now tune four segment matching thresholds, one for each of the DMM, DPM, DMM LS , and DPM LS mod- els. <ref type="bibr">17</ref> The results are shown in <ref type="table" target="#tab_10">Table 6</ref>. These re- sults demonstrate that incorporating LS in the dis- course models further increases performance for all configurations, nearly doubling the relative per- formance benefits over models that do not inte- grate LS and discourse (compare with lines 6-9 of <ref type="table" target="#tab_2">Table 1</ref>). For example, the last model in the table, which combines four discourse representa- tions, improves P@1 by 24%, whereas the equiv- alent model without this integration (line 9 in Ta- ble 1) outperforms the baseline by only 15%.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8">Error Analysis</head><p>We performed an error analysis of the full QA model (CR + LS + DMM + DPM) across the en- tire Bio corpus (lines 17 and 25 from <ref type="table" target="#tab_2">Table 1</ref>). We chose the Bio setup for this analysis because it is more complex than the CQA one: here gold an- swers may have a granularity completely differ- ent from what the system choses as best answers (in our particular case, the QA system is currently limited to answers consisting of single paragraphs, whereas gold answers may be of any size).</p><p>Here, 94 of the 378 Bio HOW and WHY ques- tions have improved answer scores, while 36 have reduced performance relative to the CR baseline. Of these 36 questions where answer scores de- creased, nearly two thirds were directly related to the paragraph granularity of the candidate answer retrieval (see §5.1):</p><p>Same Subsection (50%): In these cases, the model selected an on-topic answer paragraph in the same subsection of the textbook as a gold an- swer. Often times this paragraph directly preceded or followed the gold answer.</p><p>Answer Window Size (14%): Here, both the CR and full model chose a paragraph containing a dif- ferent gold answer. However, as discussed, gold answers may unevenly straddle paragraph bound- aries, and the paragraph chosen by the model hap- pened to have a somewhat lower overlap with its gold answer than the one chosen by the baseline.</p><p>Similar Topic (25%): The model chose a para- graph that had a similar topic to the question, but doesn't answer the question. These are challeng- ing errors, often associated with short questions (e.g. "How does HIV work?") that provide few keywords. In these cases, discourse features tend to dominate, and shift the focus towards answers that have many discourse structures deemed rel- evant. For example, for the above question, the model chose a paragraph containing many dis- course structures positively correlated with high- quality answers, but which describes the origins of HIV instead of how the virus enters a cell. Similar Words, Different Topic (8%): The model chose a paragraph that had many of the same words as the question, but is on a different topic. For example, for the question "How are fos- sil fuels formed, and why do they contain so much energy?", the model selected an answer that men- tions fossil fuels in a larger discussion of human ecological footprints. Here, the matching of both keywords and discourse structures shifted the an- swer towards a different, incorrect topic.</p><p>Finally, in one case (3%), the model identified an answer paragraph that contained a gold answer, but was missed by the domain expert annotator.</p><p>In summary, this analysis suggests that, for the majority of errors, the QA system selects an an- swer that is both topical and adjacent to a gold an- swer selected by the domain expert. This suggests that most errors are minor and are driven by cur- rent limitations of our answer boundary selection mechanism, rather than the inherent limitations of the discourse model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>This work focuses on two important aspects of an- swer reranking for non-factoid QA: similarity be- tween question and answer content, and answer structure. While the former has been addressed with a variety of lexical-semantic models, the lat- ter has received little attention. Here we show how to model answer structures using discourse and how to integrate the two aspects into a holis- tic framework. Empirically we show that model- ing answer discourse structures is complementary to modeling lexical semantic similarity and that the best performance is obtained when they are tightly integrated. We evaluate the proposed ap- proach on multiple genres and question types and obtain benefits of up to 24% relative improvement over a strong baseline that combines information retrieval and lexical semantics. We further demon- strate that answer discourse structures are largely independent of domain and transfer well, even be- tween radically different datasets.</p><p>This work is open source and available at:</p><p>http://nlp.sista.arizona.edu/releases/ acl2014.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture of the reranking framework for QA.</figDesc><graphic url="image-1.png" coords="2,308.75,62.81,212.59,158.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Top: Example feature generation for the discourse marker model, for one question (Q) and one answer candidate (AC). Answer candidates are searched for discourse markers (italic) and question word matches (bold), which are used to generate features both within-sentence (SR0), and ±1 sentence (SR1). The actual DMM exhaustively generates features for all markers and all sentence ranges. Here we show just a few for brevity. Bottom: Example feature generation for the discourse parser model using the output of an actual discourse parser. The DPM creates one feature for each individual discourse relation.</figDesc><graphic url="image-2.png" coords="4,72.00,62.81,453.54,109.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>! Answers Corpus (YA): Yahoo! An- swers 10 is an open domain community-generated QA site, with questions and answers that span for- mal and precise to informal and ambiguous lan- guage. Due to the speed limitations of the dis- course parser, we randomly drew 10,000 QA pairs from the corpus of how questions described by Surdeanu et al. (2011) using their filtering crite- ria, with the additional criterion that answers had to contain at least four community-generated an- swers, one of which was voted as the top answer. The number of answers to each question ranged from 4 to over 50, with the average 9. 11 Biology Textbook Corpus (Bio): This corpus fo- cuses on the domain of cellular biology, and con- sists of 185 how and 193 why questions hand- crafted by a domain expert. Each question has one or more gold answers identified in Campbell's Biology (Reece et al., 2011), a popular under- graduate text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>P@1</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Overall results across three datasets. The improve- ments in each section are computed relative to their respective baseline (CR or CR + LS). Bold font indicates the best score</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>An example question from the Biology corpus 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 : Relative P@1 performance increase over the CR + LS baseline for a model containing only intra-sentence fea-</head><label>3</label><figDesc></figDesc><table>tures, compared to the full model. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><head>Table 4 : Percentage of top features with the highest SVM weights that are shared between Bio HOW and YA models.</head><label>4</label><figDesc></figDesc><table>a somewhat radical setup, where the target cor-
pus has both a different genre (formal text vs. 
CQA) and different domain (biology vs. open do-
main)</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Transfer performance from YA to Bio HOW for 

single classifiers and ensembles (denoted with a ∪).  † indi-

cates approaching statistical significance with p = 0.07 or 

0.06. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 6 : YA results with integrated discourse and LS.</head><label>6</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> http://trec.nist.gov 2 http://www.clef-initiative.eu</note>

			<note place="foot" n="3"> http://answers.yahoo.com</note>

			<note place="foot" n="4"> Although most of these works use shallow textual features and focus mostly on meta data, e.g., number of votes for a particular answer. Here we use no meta data and rely solely on linguistic features.</note>

			<note place="foot" n="9"> We investigated more complex features, e.g., by exploring depths of two and three in the discourse tree, and also models that relied on tree kernels over these trees, but none improved upon this simple representation. This suggests that, in the domains explored here, there is a degree of noise introduced by the discourse parser, and the simple features proposed here are the best strategy to avoid overfitting on it.</note>

			<note place="foot" n="14"> This was only limited to reduce the combinatorial expansion of feature generation, and in principle could be set much broader.</note>

			<note place="foot" n="16"> The interpolation parameter was tuned on the YA development corpus. The in-domain performance of the ensemble model is similar to that of the single classifier in both YA and Bio HOW so we omit these results here for simplicity.</note>

			<note place="foot" n="17"> These hyperparameters were tuned on the development corpus, and were found to be stable over broad ranges.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank the Allen Institute for Artificial Intelli-gence for funding this work. We would also like to thank the three anonymous reviewers for their helpful comments and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Building a Discourse-Tagged Corpus in the Framework of Rhetorical Structure Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynn</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ellen</forename><surname>Okurowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Current Directions in Discourse and Dialogue</title>
		<editor>Jan van Kuppevelt and Ronnie Smith</editor>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="85" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A few useful things to know about machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Text-level discourse parsing with rich linguistic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Vanessa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename></persName>
		</author>
		<title level="m">The Elements of Statistical Learning: Data Mining, Inference, and Prediction</title>
		<meeting><address><addrLine>Sec</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Corpus-based question answering for whyquestions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryuichiro</forename><surname>Higashinaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Isozaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Proceedings of the Third International Joint Conference on Natural Language Processing (IJCNLP)</title>
		<meeting>the the Third International Joint Conference on Natural Language Processing (IJCNLP)<address><addrLine>Hyderabad, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="418" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">H</forename><surname>Martin</surname></persName>
		</author>
		<title level="m">Speech and Language Processing</title>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Second Edition</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rhetorical structure theory: Toward a functional theory of text organization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><forename type="middle">A</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Text</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="243" to="281" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Introduction to Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prabhakar</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The Rhetorical Parsing, Summarization, and Generation of Natural Language Texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Cernocky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Annual Conference of the International Speech Communication Association</title>
		<meeting>the 11th Annual Conference of the International Speech Communication Association</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>TERSPEECH 2010)</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Why-question answering using intra-and inter-sentential causal relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong-Hoon</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Torisawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chikara</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Motoki</forename><surname>Sano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stijn De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyonori</forename><surname>Saeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ohtake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2013-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1733" to="1743" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">High-Performance, OpenDomain Question Answering from Large Text Collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Pasca</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
		<respStmt>
			<orgName>Southern Methodist University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Question-answering by predictive annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anni</forename><surname>Coden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR &apos;00</title>
		<meeting>the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR &apos;00<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="184" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Reece</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Urry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Cain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Wasserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Minorsky</surname></persName>
		</author>
		<title level="m">Campbell Biology. Pearson</title>
		<meeting><address><addrLine>Benjamin Cummings</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Statistical machine translation for query expansion in answer retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Riezler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Vasserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhu</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 45th Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="464" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to rank answers to nonfactoid questions from web collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Ciaramita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="351" to="383" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Discourse-based answering of why-questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><surname>Verberne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lou</forename><surname>Boves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nelleke</forename><surname>Oostdijk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peterarno</forename><surname>Coppen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Traitement Automatique des Langues, Discours et document: traitements automatiques</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="21" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">What is not in the bag of words for why-qa? Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suzan</forename><surname>Verberne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lou</forename><surname>Boves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nelleke</forename><surname>Oostdijk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter-Arno</forename><surname>Coppen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="229" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to rank for why-question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suzan</forename><surname>Verberne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Halteren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Theijssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Raaijmakers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lou</forename><surname>Boves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="107" to="132" />
			<date type="published" when="2011-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ranking community answers by modeling question-answer relationships via analogical reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin-Jing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual ACM SIGIR Conference</title>
		<meeting>the Annual ACM SIGIR Conference</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Question answering using enhanced lexical semantic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrzej</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pastusiak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
