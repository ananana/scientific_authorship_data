<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:59+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SuperNMT: Neural Machine Translation with Semantic Supersenses and Syntactic Supertags</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Vanmassenhove</surname></persName>
							<email>eva.vanmassenhove@adaptcentre.ie</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Adapt Centre Dublin City University</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Way</surname></persName>
							<email>andy.way@adaptcentre.ie</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Adapt Centre Dublin City University</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SuperNMT: Neural Machine Translation with Semantic Supersenses and Syntactic Supertags</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of ACL 2018, Student Research Workshop</title>
						<meeting>ACL 2018, Student Research Workshop <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="67" to="73"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>67</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper we incorporate semantic su-persensetags and syntactic supertag features into EN-FR and EN-DE factored NMT systems. In experiments on various test sets, we observe that such features (and particularly when combined) help the NMT model training to converge faster and improve the model quality according to the BLEU scores.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural Machine Translation (NMT) models have recently become the state-of-the art in the field of Machine Translation ( <ref type="bibr">Bahdanau et al., 2014;</ref><ref type="bibr">Cho et al., 2014;</ref><ref type="bibr">Kalchbrenner et al., 2014;</ref><ref type="bibr" target="#b14">Sutskever et al., 2014</ref>). Compared to Statistical Machine Translation (SMT), the previous state-of-the-art, NMT performs particularly well when it comes to word-reorderings and translations involving mor- phologically rich languages ( <ref type="bibr">Bentivogli et al., 2016)</ref>. Although NMT seems to partially 'learn' or generalize some patterns related to syntax from the raw, sentence-aligned parallel data, more com- plex phenomena (e.g. prepositional-phrase at- tachment) remain problematic ( <ref type="bibr">Bentivogli et al., 2016</ref>). More recent work showed that explic- itly ( <ref type="bibr" target="#b3">Nadejde et al., 2017;</ref><ref type="bibr">Bastings et al., 2017;</ref><ref type="bibr">Aharoni and Goldberg, 2017</ref>) or implicitly ( <ref type="bibr">Eriguchi et al., 2017</ref>) model- ing extra syntactic information into an NMT sys- tem on the source (and/or target) side could lead to improvements in translation quality.</p><p>When integrating linguistic information into an MT system, following the central role assigned to syntax by many linguists, the focus has been mainly on the integration of syntactic features. Al- though there has been some work on semantic fea- tures for SMT ( <ref type="bibr">Banchs and Costa-Jussà, 2011</ref>), so far, no work has been done on enriching NMT sys- tems with more general semantic features at the word-level. This might be explained by the fact that NMT models already have means of learning semantic similarities through word-embeddings, where words are represented in a common vector space ( <ref type="bibr">Mikolov et al., 2013</ref>). However, making some level of semantics more explicitly available at the word level can provide the translation sys- tem with a higher level of abstraction beneficial to learn more complex constructions. Furthermore, a combination of both syntactic and semantic fea- tures would provide the NMT system with a way of learning semantico-syntactic patterns.</p><p>To apply semantic abstractions at the word-level that enable a characterisation beyond that what can be superficially derived, coarse-grained semantic classes can be used. Inspired by Named Entity Recognition which provides such abstractions for a limited set of words, supersense-tagging uses an inventory of more general semantic classes for domain-independent settings <ref type="bibr" target="#b7">(Schneider and Smith, 2015)</ref>. We investigate the effect of inte- grating supersense features (26 for nouns, 15 for verbs) into an NMT system. To obtain these fea- tures, we used the AMALGrAM 2.0 tool ( <ref type="bibr" target="#b6">Schneider et al., 2014;</ref><ref type="bibr" target="#b7">Schneider and Smith, 2015</ref>) which analyses the input sentence for Multi-Word Ex- pressions as well as noun and verb supersenses. The features are integrated using the framework of , replicating the tags for every subword unit obtained by byte-pair encod- ing (BPE). We further experiment with a combi- nation of semantic supersenses and syntactic su- pertag features (CCG syntactic categories <ref type="bibr" target="#b13">(Steedman, 2000</ref>) using EasySRL ( <ref type="bibr">Lewis et al., 2015)</ref>) and less complex features such as POS-tags, as- suming that supersense-tags have the potential to be useful especially in combination with syntactic information.</p><p>The remainder of this paper is structured as fol- lows: First, in Section 2, the related work is dis- cussed. Next, Section 3 presents the semantic and syntactic features used. The experimental set-up is described in Section 4 followed by the results in Section 5. Finally, We conclude and present some of the ideas for future work in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In SMT, various linguistic features such as stems ( <ref type="bibr" target="#b16">Toutanova et al., 2008</ref><ref type="bibr">) lemmas (Mareček et al., 2011</ref><ref type="bibr">Fraser et al., 2012</ref>), POS- tags ( <ref type="bibr">Avramidis and Koehn, 2008)</ref>, dependency labels ( <ref type="bibr">Avramidis and Koehn, 2008)</ref> and su- pertags ( <ref type="bibr">Hassan et al., 2007;</ref><ref type="bibr">Haque et al., 2009)</ref> are integrated using pre-or post-processing techniques often involving factored phrase-based models <ref type="bibr">(Koehn and Hoang, 2007</ref>). Compared to factored NMT models, factored SMT models have some disadvantages: (a) adding factors increases the sparsity of the models, (b) the n-grams limit the size of context that is taken into account, and (c) features are assumed to be independent of each other. However, adding syntactic features to SMT systems led to improvements with respect to word order and morphological agreement <ref type="bibr" target="#b18">(Williams and Koehn, 2012;</ref><ref type="bibr" target="#b8">Sennrich, 2015)</ref>.</p><p>One of the main strengths of NMT is its strong ability to generalize. The integration of linguis- tic features can be handled in a flexible way with- out creating sparsity issues or limiting context in- formation (within the same sentence). Further- more, the encoder and attention layers can be shared between features. By representing the en- coder input as a combination of features <ref type="bibr">(Alexandrescu and Kirchhoff, 2006</ref>), Sennrich and Had- dow (2016) generalized the embedding layer in such a way that an arbitrary number of linguistic features can be explicitly integrated. They then in- vestigated whether features such as lemmas, sub- word tags, morphological features, POS tags and dependency labels could be useful for NMT sys- tems or whether their inclusion is redundant.</p><p>Similarly, on the syntax level, <ref type="bibr" target="#b12">Shi et al. (2016)</ref> show that although NMT systems are able to par- tially learn syntactic information, more complex patterns remain problematic. Furthermore, some- times information is present in the encoding vec- tors but is lost during the decoding phase ( <ref type="bibr" target="#b17">Vanmassenhove et al., 2017)</ref>.  show that the inclusion of linguistic fea- tures leads to improvements over the NMT base- line for EN-DE (0.6 BLEU), DE-EN (1.5 BLEU) and EN-RO (1.0 BLEU). When evaluating the gains from the features individually, it results that the gain from different features is not fully cumu- lative. <ref type="bibr" target="#b3">Nadejde et al. (2017)</ref> extend their work by including CCG supertags as explicit features in a factored NMT systems. Moreover, they experi- ment with serializing and multitasking and show that tightly coupling the words with their syntac- tic features leads to improvements in translation quality (measured by BLEU) while a multitask ap- proach (where the NMT predicts CCG supertags and words independently) does not perform bet- ter than the baseline system. A similar observa- tion was made by <ref type="bibr">Li et al (2017)</ref>, who incorporate the linearized parse trees of the source sentences into ZH-EN NMT systems. They propose three different sorts of encoders: (a) a parallel RNN, (b) a hierarchical RNN, and (c) a mixed RNN. Like <ref type="bibr" target="#b3">Nadejde et al. (2017)</ref>, <ref type="bibr">Li et al (2017)</ref> observe that the mixed RNN (the simplest RNN encoder), where words and label annotation vectors are sim- ply stitched together in the input sequences, yields the best performance with a significant improve- ment (1.4 BLEU). Similarly, <ref type="bibr">Eriguchi et al. (2016)</ref> integrated syntactic information in the form of lin- earized parse trees by using an encoder that com- putes vector representations for each phrase in the source tree. They focus on source-side syn- tactic information based on Head-Driven Phrase Structure Grammar ( <ref type="bibr" target="#b5">Sag et al., 1999</ref>) where target words are aligned not only with the correspond- ing source words but with the entire source phrase. <ref type="bibr" target="#b19">Wu et al. (2017)</ref> focus on incorporating source- side long distance dependencies by enriching each source state with global dependency structure.</p><p>To the best of our knowledge, there has not been any work on explicitly integrating semantic information in NMT. Similarly to syntactic fea- tures, we hypothesize that semantic features in the form of semantic 'classes' can be beneficial for NMT providing it with an extra ability to general- ize and thus better learn more complex semantico- syntactic patters.</p><p>3 Semantics and Syntax in NMT</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Supersense Tags</head><p>The novelty of our work is the integration of ex- plicit semantic features supersenses into an NMT system. Supersenses are a term which refers to the top-level hypernyms in the WordNet <ref type="bibr" target="#b2">(Miller, 1995)</ref> taxonomy, sometimes also referred to as se- mantic fields ( <ref type="bibr" target="#b7">Schneider and Smith, 2015)</ref>. The supersenses cover all nouns and verbs with a total of 41 supersense categories, 26 for nouns and 15 for verbs. To obtain the supersense tags we used the AMALGrAM (A Machine Analyzer of Lexical Groupings and Meanings) 2.0 tool 1 which in ad- dition to the noun and verb supersenses analyzes English input sentences for MWEs. An exam- ple of a sentence annotated with the AMALGrAM tool is given in (1): <ref type="bibr">2</ref> (1) (a) "He seemed to have little faith in our democratic structures, suggesting that various articles could be misused by governments." (b) "He seemed|cognition to have|stative lit- tle faith|COGNITION in our democratic structures|ARTIFACT , suggesting|communication that various articles|COMMUNICATION could be|'a misused|social by governments|GROUP ."</p><p>As can be noted in (1), some supersenses, such as cognition exist for both nouns and verbs. How- ever, the supersense tags for verbs are always low- ercased while the ones for nouns are capitalized. This way, the supersenses also provide syntactic information useful for disambiguation as in <ref type="formula">(2)</ref>, where the word work is correctly tagged as a noun (with its capitalized supersense tag ACT) in the first part of the sentence and as a verb (with the lowercased supersense tag social). Furthermore, there is a separate tag to distinguish auxiliary verbs from main verbs. Since MWEs and supersenses naturally comple- ment each other, <ref type="bibr" target="#b7">Schneider and Smith (2015)</ref> in- tegrated the MWE identification task ( <ref type="bibr" target="#b6">Schneider et al., 2014</ref>) with the supersense tagging task of <ref type="bibr">Ciaramita and Altun (2006</ref> </p><note type="other">MWEs in fact, a number of and EU citizens are retrieved by the tagger.</note><p>We add this semantico-syntactic information in the source as an extra feature in the embedding layer following the approach of , who extended the model of <ref type="bibr">Bahdanau et al. (2014)</ref></p><note type="other">. A separate embedding is learned for every source-side feature provided (the word itself, POS-tag, supersense tag etc.). These em- bedding vectors are then concatenated into one embedding vector and used in the model instead of the simple word embedding one (Sennrich and Haddow, 2016). To reduce the number of out-of-vocabulary (OOV) words, we follow the approach of Sennrich et al. (2016) using a variant of BPE for word seg- mentation capable of encoding open vocabularies with a compact symbol vocabulary of variable- length subword units. For each word that is split into subword units, we copy the features of the word in question to its subword units. In (3), we</note><p>give an example with the word 'stormtroopers' that is tagged with the supersense tag 'GROUP'. It is split into 5 subword units so the supersense tag feature is copied to all its five subword units. Furthermore, we add a none tag to all words that did not receive a supersense tag.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(3)</head><p>Input:</p><p>"the stormtroopers" SST:</p><p>"the stormtroopers|GROUP" BPE:</p><p>"the stor@@ m@@ tro@@ op@@ ers" Output: "the|none stor@@|GROUP ... op@@|GROUP ers|GROUP"</p><p>For the MWEs we decided to copy the super- sense tag to all the words of the MWE (if provided by the tagger), as in (4). If the MWE did not re- ceive a particular tag, we added the tag mwe to all its components, as in example <ref type="formula" target="#formula_1">(5)</ref> </p><p>Input:</p><p>"EU citizens" SST:</p><p>"EU citizens|GROUP" Output: "EU|GROUP citizens|GROUP"</p><p>Input:</p><p>"a number of" SST:</p><p>"a number of" Output: "a|mwe number|mwe of|mwe "</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Supertags and POS-tags</head><p>We hypothesize that more general semantic infor- mation can be particularly useful for NMT in com- bination with more detailed syntactic information. To support our hypothesis we also experimented with syntactic features (separately and in com- bination with the semantic ones): POS tags and CCG supertags.</p><p>The POS tags are generated by the Stanford POS-tagger ( <ref type="bibr" target="#b15">Toutanova et al., 2003)</ref>; for the su- pertags we used the EasySRL tool ( <ref type="bibr">Lewis et al., 2015</ref>) which annotates words with CCG tags. CCG tags provide global syntactic information on the lexical level. This kind of information can help resolve ambiguity in terms of prepositional attachment, among others. An example of a CCG- tagged sentence is given in (6): </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Set-Up</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data sets</head><p>Our NMT systems are trained on 1M parallel sen- tences of the Europarl corpus for EN-FR and EN- DE ( <ref type="bibr">Koehn, 2005</ref>). We test the systems on 5K sen- tences (different from the training data) extracted from Europarl and the newstest2013. Two differ- ent test sets are used in order to show how ad- ditional semantic and syntactic features can help the NMT system translate different types of test sets and thus evaluate the general effect of our im- provement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Description of the NMT system</head><p>We used the nematus toolkit ( ) to train encoder-decoder NMT models with the following parameters: vocabulary size: 35000, maximum sentence length: 60, vector dimension: 1024, word embedding layer: 700, learning op- timizer: adadelta. We keep the embedding layer fixed to 700 for all models in order to en- sure that the improvements are not simply due to an increase of the parameters in the embedding layer. In order to by-pass the OOV problem and reduce the number of dictionary entries we use word-segmentation with BPE <ref type="bibr" target="#b8">(Sennrich, 2015)</ref>. We ran the BPE algorithm with 89, 500 operations. We trained all our BPE-ed NMT systems with CCG tag features, supersensetags (SST), POS tags and the combination of syntactic features (POS or CCG) with the semantic ones (SST). All systems are trained for 150,000 iterations and evaluated af- ter every 10,000 iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">English-French</head><p>For both test sets, the NMT system with super- senses (SST) converges faster than the baseline (BPE) NMT system. As we hypothesized, the ben- efits of the features added, was more clear on the newstest2013 than on the Europarl test set. <ref type="figure" target="#fig_2">Fig- ure 1</ref> compares the BPE-ed baseline system (BPE) with the supertag-supersensetag system (CCG- SST) automatically evaluated on the newstest2013 (in terms of BLEU ( <ref type="bibr" target="#b4">Papineni et al., 2002)</ref>) over all 150,000 iterations. From the graph, it can also be observed that the system has a more robust, con- sistent learning curve. To see in more detail how our semantically en- riched SST system compares to an NMT system with syntactic CCG supertags and how a system that integrates both semantic features and syntac- tic features (SST-CCG) performs, a more detailed graph is provided in <ref type="figure" target="#fig_3">Figure 2</ref> where we zoom in on later stages of the learning process. Although  observe that features are not necessarily cumulative (possibly since the information from the syntactic features partially overlapped), the system enriched with both se- mantic and syntactic features outperforms the two separate systems as well as the baseline system. The best CCG-SST model (23.21 BLEU) out- performs the best BPE-ed baseline model (22.54 BLEU) with 0.67 BLEU (see <ref type="table">Table 1</ref>). More- over, the benefit of syntactic and semantic fea- tures seems to be more than cumulative at some points, confirming the idea that providing both in- formation sources can help the NMT system learn semantico-syntactic patterns. This supports our hypothesis that semantic and syntactic features are particularly useful when combined.  <ref type="table">Table 1</ref>: Best BLEU scores for Baseline (BPE), Syntac- tic (CCG), Semantic (SST) and Combined (SST-CCG) NMT systems for EN-FR evaluated on the newstest2013</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">English-German</head><p>The results for the EN-DE system are very similar to the EN-FR system: the model converges faster and we observe the same trends with respect to the BLEU scores of the different systems. <ref type="figure" target="#fig_4">Figure 3</ref> compares the BPE-ed baseline system (BPE) with the NMT system enriched with SST and CCG tags (SST-CCG). In the last iterations, see <ref type="figure" target="#fig_5">Figure 4</ref>, we see how the two systems enriched with super- sense tags and CCG tags lead to small improve- ments over the baseline. However, their combi- nation (SST-CCG) leads to a more robust NMT system with a higher BLEU (see <ref type="table" target="#tab_1">Table 2</ref>).   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>In this work we experimented with EN-FR and EN-DE data augmented with semantic and syn- tactic features. For both language pairs we observe that adding extra semantic and/or syntactic fea- tures leads to faster convergence. Furthermore, the benefit of the additional features is more clear on a dissimilar test set which is in accordance with our original hypothesis stating that semantic and syn- tactic features (and their combination) can be ben- eficial for generalization. In the future, we would like to perform manual evaluations on the outputs of our systems to see whether they correlate with the BLEU scores. In the next step, we will let the models converge, create the ensemble models for the different systems and compute whether the in- crease in BLEU score is significant. Furthermore, we would like to experiment with larger datasets to verify whether the positive effect of the linguistic features remains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgements</head><p>I would like to thank Natalie Schluter for her in- sightful comments and feedback and Roam Ana- lytics for sponsoring the ACL SRW travel grant. This work has been supported by the Dublin City University Faculty of Engineering &amp; Com- puting under the Daniel O'Hare Research Schol-arship scheme and by the ADAPT Centre for Digital Content Technology which is funded un- der the SFI Research Centres Programme (Grant 13/RC/2106) and is co-funded under the European Regional Development Fund. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) "In the course of my work on the opinion, I in fact became aware of quite a number of problems and difficulties for EU citizens who live and work in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>It|NP is|(S[dcl]\NP)/NP a|NP/N modern|N/N form|N/PP of|PP/NP colonialism|N .|.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Baseline (BPE) vs Combined (SST-CCG) NMT Systems for EN-FR, evaluated on the newstest2013.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Baseline (BPE) vs Syntactic (CCG) vs Semantic (SST) and Combined (SST-CCG) NMT Systems for EN-FR, evaluated on the newstest2013.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Baseline (BPE) vs Combined (CCG-SST) NMT Systems for English-German, evaluated on the Europarl test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Baseline (BPE) vs Syntactic (CCG) vs Semantic (SST) and Combined (CCG-SST) NMT Systems for ENDE, evaluated on the Europarl test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Best BLEU scores for Baseline (BPE), Syntac-

tic (CCG), Semantic (SST) and Combined (SST-CCG) NMT 
systems for EN-DE evaluated on the Europarl test set. 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Linguistic Regularities in Continuous Space Word Representations</title>
	</analytic>
	<monogr>
		<title level="m">hlt-Naacl</title>
		<meeting><address><addrLine>Atlanta, USA</addrLine></address></meeting>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Predicting Target Language CCG Supertags Improves Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Nadejde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Dwojak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="68" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Syntactic theory: A formal introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Sag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><forename type="middle">M</forename><surname>Wasow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan A</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Center for the Study of Language and Information</title>
		<meeting><address><addrLine>Stanford, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">92</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Discriminative Lexical Semantic Segmentation with Gaps: Running the MWE Gamut</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Danchik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="193" to="206" />
			<pubPlace>Baltimore, Maryland, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Corpus and Model Integrating Multiword Expressions and Supersenses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>Denver, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1537" to="1547" />
		</imprint>
	</monogr>
	<note>NAACL HLT 2015</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Modelling and Optimizing on Syntactic N-grams for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="169" to="182" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Nematus: a Toolkit for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Hitschler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Valerio Miceli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="65" to="68" />
		</imprint>
	</monogr>
	<note>Jozef Mokry, and Maria Nadejde</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Linguistic Input Features Improve Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation, WMT, ACL</title>
		<meeting>the First Conference on Machine Translation, WMT, ACL<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="83" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural Machine Translation of Rare Words with Subword Units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Does String-Based Neural MT Learn Source Syntax?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inkit</forename><surname>Padhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods on Natural Language Processing</title>
		<meeting>Empirical Methods on Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
		<title level="m">The Syntactic Process</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sequence to Sequence Learning with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Feature-rich Partof-Speech Tagging with a Cyclic Dependency Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter</title>
		<meeting>the 2003 Conference of the North American Chapter</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Applying Morphology Generation Models to Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisami</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Achim</forename><surname>Ruopp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="514" to="522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Investigating &apos;aspect&apos; in nmt and smt: Translating the english simple past and present perfect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Vanmassenhove</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhua</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Way</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics in the Netherlands Journal</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="109" to="128" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ghkm Rule Extraction and Scope-3 Parsing in Moses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Workshop on Statistical Machine Translation</title>
		<meeting>the Seventh Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="388" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improved Neural Machine Translation with Source Syntax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangzhi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4179" to="4185" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
