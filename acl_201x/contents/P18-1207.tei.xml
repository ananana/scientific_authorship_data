<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multimodal Affective Analysis Using Hierarchical Attention Strategy with Word-Level Alignment</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Gu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Multimedia Image Processing Lab Electrical and Computer Engineering Department</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<settlement>Piscataway</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kangning</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Multimedia Image Processing Lab Electrical and Computer Engineering Department</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<settlement>Piscataway</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Multimedia Image Processing Lab Electrical and Computer Engineering Department</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<settlement>Piscataway</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhong</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Multimedia Image Processing Lab Electrical and Computer Engineering Department</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<settlement>Piscataway</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Multimedia Image Processing Lab Electrical and Computer Engineering Department</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<settlement>Piscataway</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Marsic</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Multimedia Image Processing Lab Electrical and Computer Engineering Department</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<settlement>Piscataway</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multimodal Affective Analysis Using Hierarchical Attention Strategy with Word-Level Alignment</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2225" to="2235"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2225</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Multimodal affective computing, learning to recognize and interpret human affect and subjective information from multiple data sources, is still challenging because: (i) it is hard to extract informative features to represent human affects from heterogeneous inputs; (ii) current fusion strategies only fuse different modalities at abstract levels, ignoring time-dependent interactions between modalities. Addressing such issues, we introduce a hierarchical multimodal architecture with attention and word-level fusion to classify utterance-level sentiment and emotion from text and audio data. Our introduced model outper-forms state-of-the-art approaches on published datasets, and we demonstrate that our model&apos;s synchronized attention over modalities offers visual interpretability.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the recent rapid advancements in social me- dia technology, affective computing is now a pop- ular task in human-computer interaction. Senti- ment analysis and emotion recognition, both of which require applying subjective human concepts for detection, can be treated as two affective com- puting subtasks on different levels ( <ref type="bibr" target="#b19">Poria et al., 2017a)</ref>. A variety of data sources, including voice, facial expression, gesture, and linguistic content have been employed in sentiment analysis and emotion recognition. In this paper, we focus on a multimodal structure to leverage the advantages of each data source. Specifically, given an utter- ance, we consider the linguistic content and acous- tic characteristics together to recognize the opin- ion or emotion. Our work is important and useful * Equally Contribution because speech is the most basic and commonly used form of human expression.</p><p>A basic challenge in sentiment analysis and emotion recognition is filling the gap between extracted features and the actual affective states ( . The lack of high-level fea- ture associations is a limitation of traditional ap- proaches using low-level handcrafted features as representations ( <ref type="bibr">Seppi et al., 2008;</ref><ref type="bibr" target="#b24">Rozgic et al., 2012)</ref>. Recently, deep learning structures such as CNNs and LSTMs have been used to extract high-level features from text and audio <ref type="bibr" target="#b5">(Eyben et al., 2010a;</ref><ref type="bibr" target="#b20">Poria et al., 2015</ref>). However, not all parts of the text and vocal signals contribute equally to the predictions. A specific word may change the entire sentimental state of text; a differ- ent vocal delivery may indicate inverse emotions despite having the same linguistic content. Re- cent approaches introduce attention mechanisms to focus the models on informative words <ref type="bibr" target="#b30">(Yang et al., 2016</ref>) and attentive audio frames <ref type="bibr" target="#b16">(Mirsamadi et al., 2017</ref>) for each individual modality. However, to our knowledge, there is no common multimodal structure with attention for utterance- level sentiment and emotion classification. To ad- dress such issue, we design a deep hierarchical multimodal architecture with an attention mech- anism to classify utterance-level sentiments and emotions. It extracts high-level informative tex- tual and acoustic features through individual bidi- rectional gated recurrent units (GRU) and uses a multi-level attention mechanism to select the in- formative features in both the text and audio mod- ule.</p><p>Another challenge is the fusion of cues from heterogeneous data. Most previous works fo- cused on combining multimodal information at a holistic level, such as integrating independent predictions of each modality via algebraic rules <ref type="bibr">(WÃ¶llmer et al., 2013)</ref> or fusing the extracted modality-specific features from entire utterances <ref type="bibr" target="#b22">(Poria et al., 2016</ref>). They extract word-level fea- tures in a text branch, but process audio at the frame-level or utterance-level. These methods fail to properly learn the time-dependent interac- tions across modalities and restrict feature integra- tion at timestamps due to the different time scales and formats of features of diverse modalities <ref type="bibr" target="#b19">(Poria et al., 2017a</ref>). However, to determine human meaning, it is critical to consider both the linguis- tic content of the word and how it is uttered. A loud pitch on different words may convey inverse emotions, such as the emphasis on "hell" for anger but indicating happy on "great". Synchronized at- tentive information across text and audio would then intuitively help recognize the sentiments and emotions. Therefore, we compute a forced align- ment between text and audio for each word and propose three fusion approaches (horizontal, ver- tical, and fine-tuning attention fusion) to integrate both the feature representations and attention at the word-level.</p><p>We evaluated our model on four published sen- timent and emotion datasets. Experimental results show that the proposed architecture outperforms state-of-the-art approaches. Our methods also al- low for attention visualization, which can be used for interpreting the internal attention distribution for both single-and multi-modal systems. The contributions of this paper are: (i) a hierarchical multimodal structure with attention mechanism to learn informative features and high-level associ- ations from both text and audio; (ii) three word- level fusion strategies to combine features and learn correlations in a common time scale across different modalities; (iii) word-level attention vi- sualization to help human interpretation.</p><p>The paper is organized as follows: We list re- lated work in section 2. Section 3 describes the proposed structure in detail. We present the exper- iments in section 4 and provide the result analysis in section 5. We discuss the limitations in section 6 and conclude with section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Despite the large body of research on audio-visual affective analysis, there is relatively little work on combining text data. Early work combined human transcribed lexical features and low-level hand- crafted acoustic features using feature-level fu- sion (Forbes- ). Others used SVMs fed bag of words (BoW) and part of speech (POS) features in addition to low-level acoustic features ( <ref type="bibr">Seppi et al., 2008;</ref><ref type="bibr" target="#b24">Rozgic et al., 2012;</ref><ref type="bibr" target="#b26">Savran et al., 2012;</ref><ref type="bibr" target="#b23">Rosas et al., 2013;</ref><ref type="bibr" target="#b12">Jin et al., 2015</ref>). All of the above extracted low-level features from each modality separately. More recently, deep learning was used to extract higher-level multimodal fea- tures. Bidirectional LSTMs were used to learn long-range dependencies from low-level acoustic descriptors and derivations (LLDs) and visual fea- tures ( <ref type="bibr" target="#b5">Eyben et al., 2010a;</ref><ref type="bibr" target="#b29">WÃ¶llmer et al., 2013)</ref>. CNNs can extract both textual ( <ref type="bibr" target="#b20">Poria et al., 2015)</ref> and visual features ( <ref type="bibr" target="#b22">Poria et al., 2016</ref>) for multi- ple kernel learning of feature-fusion. Later, hier- archical LSTMs were used ( <ref type="bibr" target="#b21">Poria et al., 2017b)</ref>. A deep neural network was used for feature-level fusion in ( <ref type="bibr" target="#b8">Gu et al., 2018)</ref> and ) introduced a tensor fusion network to fur- ther improve the performance. A very recent work using word-level fusion was provided by . The key differences between this work and the proposed architecture are: (i) we de- sign a fine-tunable hierarchical attention structure to extract word-level features for each individual modality, rather than simply using the initialized textual embedding and extracted LLDs from CO- VAREP ( <ref type="bibr" target="#b4">Degottex et al., 2014</ref>); (ii) we propose di- verse representation fusion strategies to combine both the word-level representations and attention weights, instead of using only word-level fusion; (iii) our model allows visualizing the attention dis- tribution at both the individual modality and at fu- sion to help model interpretability.</p><p>Our architecture is inspired by the document classification hierarchical attention structure that works at both the sentence and word level <ref type="bibr" target="#b30">(Yang et al., 2016)</ref>. For audio, an attention-based BLSTM and CNN were applied to discovering emotion from frames ( <ref type="bibr" target="#b10">Huang and Narayanan, 2016;</ref><ref type="bibr" target="#b18">Neumann and Vu, 2017)</ref>. Frame-level weighted-pooling with local attention was shown to outperform frame-wise, final-frame, and frame- level mean-pooling for speech emotion recogni- tion ( <ref type="bibr" target="#b16">Mirsamadi et al., 2017</ref>). level fusion module. We first make a forced align- ment between the text and audio during prepro- cessing. Then, the text attention module and audio attention module extract the features from the cor- responding inputs (shown in Algorithm 1). The word-level fusion module fuses the extracted fea- ture vectors and makes the final prediction via a shared representation (shown in Algorithm 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Forced Alignment and Preprocessing</head><p>The forced alignment between the audio and text on the word-level prepares the different data for feature extraction. We align the data at the word- level because words are the basic unit in English for human speech comprehension. We used ae- neas 1 to determine the time interval for each word in the audio file based on the Sakoe-Chiba Band Dynamic Time Warping (DTW) algorithm <ref type="bibr" target="#b25">(Sakoe and Chiba, 1978)</ref>.</p><p>For the text input, we first embedded the words into 300-dimensional vectors by <ref type="bibr">word2vec (Mikolov et al., 2013)</ref>, which gives us the best re- sult compared to GloVe and LexVec. Unknown words were randomly initialized. Given a sentence S with N words, let w i represent the ith word. We embed the words through the word2vec em- bedding matrix W e by:</p><formula xml:id="formula_0">T i = W e w i , i â [1, N ] (1)</formula><p>where T i is the embedded word vector.</p><p>For the audio input, we extracted Mel- frequency spectral coefficients (MFSCs) from raw audio signals as acoustic inputs for two reasons. Firstly, MFSCs maintain the locality of the data by preventing new bases of spectral energies resulting from discrete cosine transform in MFCCs extrac- tion ( <ref type="bibr" target="#b0">Abdel-Hamid et al., 2014</ref>). Secondly, it has more dimensions in the frequency domain that aid learning in deep models ( <ref type="bibr" target="#b9">Gu et al., 2017)</ref>. We used 64 filter banks to extract the MFSCs for each audio frame to form the MFSCs map. To facilitate train- ing, we only used static coefficients. Each word's MFSCs can be represented as a matrix with 64 Ã n dimensions, where n is the interval for the given word in frames. We zero-pad all intervals to the same length L, the maximum frame numbers of the word in the dataset. We did extract LLD fea- tures using OpenSmile ( <ref type="bibr" target="#b6">Eyben et al., 2010b</ref>) soft- ware and combined them with the MFSCs during our training stage. However, we did not find an </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Text Attention Module</head><p>To extract features from embedded text input at the word level, we first used bidirectional GRUs, which are able to capture the contextual informa- tion between words. It can be represented as:</p><formula xml:id="formula_1">t h â i , t h â i = bi GRU (T i ), i â [1, N ] (2)</formula><p>where bi GRU is the bidirectional GRU, t h â i and t h â i denote respectively the forward and backward contextual state of the input text. We combined t h â i and t h â i as t h i to represent the feature vector for the ith word. We choose GRUs instead of LSTMs because our experiments show that LSTMs lead to similar performance (0.07% higher accuracy) with around 25% more trainable parameters.</p><p>To create an informative word representation, we adopted a word-level attention strategy that generates a one-dimensional vector denoting the importance for each word in a sequence ( <ref type="bibr" target="#b30">Yang et al., 2016)</ref>. As defined by (Bahdanau et al.,</p><formula xml:id="formula_2">Algorithm 1 FEATURE EXTRACTION 1: procedure FORCED ALIGNMENT 2:</formula><p>Determine time interval of each word 3:  </p><formula xml:id="formula_3">find w i â â [A ij ], j â [1, L], i â [1, N ] 4</formula><formula xml:id="formula_4">for i â [1, N ] do 8: T i â getEmbedded(w i ) 9: t h i â bi GRU (T i ) 10: t e i â getEnergies(t h i ) 11: t Î± i â getDistribution(t e i ) 12</formula><formula xml:id="formula_5">for i â [1, N ] do 17:</formula><p>Frame-Level Attention Module 18:</p><formula xml:id="formula_6">for j â [1, L] do 19: f h ij â bi GRU (A ij ) 20: f e ij â getEnergies(f h ij ) 21: f Î± ij â getDistribution(f e ij ) 22:</formula><p>end for 23:</p><formula xml:id="formula_7">f V i â weightedSum(f Î± ij , f h ij ) 24:</formula><p>Word-Level Attention Module <ref type="bibr">25</ref>:</p><formula xml:id="formula_8">w h i â bi GRU (f V i ) 26:</formula><p>w e i â getEnergies(w h i )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>27:</head><p>w Î± i â getDistribution(w e i ) <ref type="bibr">28:</ref> end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>29:</head><p>return w h i , w Î± i 30: end procedure 2014), we compute the textual attentive energies t e i and textual attention distribution t Î± i by:</p><formula xml:id="formula_9">t e i = tanh(W t t h i + b t ), i â [1, N ] (3) t Î± i = exp(t e i v t ) N k=1 exp(t e k v t )<label>(4)</label></formula><p>where W t and b t are the trainable parameters and v t is a randomly-initialized word-level weight vec- tor in the text branch. To learn the word-level in- teractions across modalities, we directly use the textual attention distribution t Î± i and textual bidi- rectional contextual state t h i as the output to aid word-level fusion, which allows further computa- tions between text and audio branch on both the contextual states and attention distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Audio Attention Module</head><p>We designed a hierarchical attention model with frame-level acoustic attention and word-level at- tention for acoustic feature extraction.</p><p>Frame-level Attention captures the important MFSC frames from the given word to generate the word-level acoustic vector. Similar to the text at- tention module, we used a bidirectional GRU:</p><formula xml:id="formula_10">f h â ij , f h â ij = bi GRU (A ij ), j â [1, L] (5)</formula><p>where f h â ij and f h â ij denote the forward and backward contextual states of acoustic frames. A ij denotes the MFSCs of the jth frame from the ith word, i â <ref type="bibr">[1, N ]</ref>. f h ij represents the hidden state of the jth frame of the ith word, which consists of f h â ij and f h â ij . We apply the same atten- tion mechanism used for textual attention mod- ule to extract the informative frames using equa- tion 3 and 4. As shown in <ref type="figure">Figure 1</ref>, the input of equation 3 is f h ij and the output is the frame- level acoustic attentive energies f e ij . We cal- culate the frame-level attention distribution f Î± ij by using f e ij as the input for equation 4. We form the word-level acoustic vector f V i by taking a weighted sum of bidirectional contextual state f h ij of the frame and the corresponding frame- level attention distribution f Î± ij Specifically,</p><formula xml:id="formula_11">f V i = j f Î± ij f h ij (6)</formula><p>Word-level Attention aims to capture the word-level acoustic attention distribution w Î± i based on formed word vector f V i . We first used equation 2 to generate the word-level acoustic contextual states w h i , where the input is f V i and w h i = (w h â i , w h â i ). Then, we compute the word-level acoustic attentive energies w e i via equation 3 as the input for equation 4. The final output is an acoustic attention distribution w Î± i from equation 4 and acoustic bidirectional contex- tual state w h i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Word-level Fusion Module</head><p>Fusion is critical to leveraging multimodal fea- tures for decision-making. Simple feature con- catenation without considering the time scales ig- nores the associations across modalities. We in- troduce word-level fusion capable of associating the text and audio at each word. We propose three fusion strategies <ref type="figure" target="#fig_1">(Figure 2</ref> and Algorithm 2): hori- zontal fusion, vertical fusion, and fine-tuning at- tention fusion. These methods allow easy syn- chronization between modalities, taking advan- tage of the attentive associations across text and audio, creating a shared high-level representation. Horizontal Fusion (HF)</p><p>3:</p><formula xml:id="formula_12">for i â [1, N ] do 4: t V i â weighted(t Î± i , t h i ) 5:</formula><p>w V i â weighted(w Î± i , w h i )</p><p>6:</p><formula xml:id="formula_13">V i â dense([t V i , w V i ]) 7:</formula><p>end for </p><formula xml:id="formula_14">E â convN et(V 1 , V 2 , ..., V N ) 22:</formula><p>return E 23: end procedure Horizontal Fusion (HF) provides the shared representation that contains both the textual and acoustic information for a given word <ref type="figure" target="#fig_1">(Figure 2  (a)</ref>). The HF has two steps: (i) combining the bidi- rectional contextual states (t h i and w h i in <ref type="figure">Fig- ure 1)</ref> and attention distributions for each branch (t Î± i and w Î± i in <ref type="figure">Figure 1)</ref> independently to form the word-level textual and acoustic representa- tions. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, given the input (t Î± i , t h i ) and (w Î± i , w h i ), we first weighed each in- put branch by:</p><formula xml:id="formula_15">t V i = t Î± i t h i<label>(7)</label></formula><formula xml:id="formula_16">w V i = w Î± i w h i<label>(8)</label></formula><p>where t V i and w V i are word-level representa- tions for text and audio branches, respectively; (ii) concatenating them into a single space and further applying a dense layer to create the shared context vector V i , and</p><formula xml:id="formula_17">V i = (t V i , w V i ).</formula><p>The HF com- bines the unimodal contextual states and attention weights; there is no attention interaction between the text modality and audio modality. The shared vectors retain the most significant characteristics from respective branches and encourages the deci- sion making to focus on local informative features. Vertical Fusion (VF) combines textual atten- tions and acoustic attentions at the word-level, using a shared attention distribution over both modalities instead of focusing on local informa- tive representations <ref type="figure" target="#fig_1">(Figure 2 (b)</ref>). The VF is com- puted in three steps: (i) using a dense layer after the concatenation of the word-level textual (t h i ) and acoustic (w h i ) bidirectional contextual states to form the shared contextual state h i ; (ii) averag- ing the textual (t Î± i ) and acoustic (w Î± i ) atten- tions for each word as the shared attention dis- tribution s Î± i ; (iii) computing the weight of h i and s Î± i as final shared context vectors V i , where V i = h i s Î± i . Because the shared attention dis- tribution (s Î± i ) is based on averages of unimodal attentions, it is a joint attention of both textual and acoustic attentive information.</p><p>Fine-tuning Attention Fusion (FAF) preserves the original unimodal attentions and provides a fine-tuning attention for the final prediction <ref type="figure" target="#fig_1">(Figure2 (c)</ref>). The averaging of attention weights in vertical fusion potentially limits the representa- tional power. Addressing such issue, we propose a trainable attention layer to tune the shared atten- tion in three steps: (i) computing the shared at- tention distribution s Î± i and shared bidirectional contextual states h i separately using the same ap- proach as in vertical fusion; (ii) applying attention fine-tuning:</p><formula xml:id="formula_18">u e i = tanh(W u h i + b u )<label>(9)</label></formula><formula xml:id="formula_19">u Î± i = exp(u e i v u ) N k=1 exp(u e k v u ) + s Î± i<label>(10)</label></formula><p>where W u , b u , and v u are additional trainable pa- rameters. The u Î± i can be understood as the sum of the fine-tuning score and the original shared attention distribution s Î± i ; (iii) calculating the weight of u Î± i and h i to form the final shared con- text vector V i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Decision Making</head><p>The output of the fusion layer V i is the ith shared word-level vectors. To further make use of the combined features for classification, we applied a CNN structure with one convolutional layer and one max-pooling layer to extract the final repre- sentation from shared word-level vectors (Poria et al., 2016; <ref type="bibr" target="#b28">Wang et al., 2016</ref>). We set up various widths for the convolutional filters <ref type="bibr" target="#b13">(Kim, 2014)</ref> and generated a feature map c k by:</p><formula xml:id="formula_20">f i = tanh(W c V i:i+kâ1 + b c )<label>(11)</label></formula><formula xml:id="formula_21">c k = max{f 1 , f 2 , ..., f N }<label>(12)</label></formula><p>where k is the width of the convolutional filters, f i represents the features from window i to i + k â 1. W c and b c are the trainable weights and biases. We get the final representation c by concatenating all the feature maps. A softmax function is used for the final classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluated our model on four published datasets: two multimodal sentiment datasets (MOSI and YouTube) and two multimodal emotion recognition datasets (IEMOCAP and EmotiW).</p><p>MOSI dataset is a multimodal sentiment inten- sity and subjectivity dataset consisting of 93 re- views with 2199 utterance segments ( <ref type="bibr" target="#b32">Zadeh et al., 2016)</ref>. Each segment was labeled by five individ- ual annotators between -3 (strong negative) to +3 (strong positive). We used binary labels based on the sign of the annotations' average.</p><p>YouTube dataset is an English multimodal dataset that contains 262 positive, 212 negative, and 133 neutral utterance-level clips provided by <ref type="bibr" target="#b17">(Morency et al., 2011</ref>). We only consider the pos- itive and negative labels during our experiments.</p><p>IEMOCAP is a multimodal emotion dataset in- cluding visual, audio, and text data ( <ref type="bibr" target="#b2">Busso et al., 2008)</ref>. For each sentence, we used the label agreed on by the majority (at least two of the three an- notators). In this study, we evaluate both the 4- catgeory (happy+excited, sad, anger, and neutral) and 5-catgeory(happy+excited, sad, anger, neu- tral, and frustration) emotion classification prob- lems. The final dataset consists of 586 happy, 1005 excited, 1054 sad, 1076 anger, 1677 neutral, and 1806 frustration.</p><p>EmotiW 2 is an audio-visual multimodal utterance-level emotion recognition dataset con- sist of video clips. To keep the consistency with the IEMOCAP dataset, we used four emotion categories as the final dataset including 150 happy, 117 sad, 133 anger, and 144 neutral. We used IBM Watson 3 speech to text software to transcribe the audio data into text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>We compared the proposed architecture to pub- lished models. Because our model focuses on extracting sentiment and emotions from human speech, we only considered the audio and text branch applied in the previous studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Sentiment Analysis Baselines</head><p>BL-SVM extracts a bag-of-words as textual fea- tures and low-level descriptors as acoustic fea- tures. An SVM structure is used to classify the sentiments ( <ref type="bibr" target="#b23">Rosas et al., 2013)</ref>.</p><p>LSTM-SVM uses LLDs as acoustic features and bag-of-n-grams (BoNGs) as textual features. The final estimate is based on decision-level fu- sion of text and audio predictions <ref type="bibr">(WÃ¶llmer et al., 2013</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentiment Analysis (MOSI) Emotion Recognition (IEMOCAP) Approach</head><p>Category WA(%) UA(%) Weighted-F1 Approach Category WA(%) UA(%) Weighted-F1 BL-SVM* 2-class 70.  <ref type="table">Table 1</ref>: Comparison of models. WA = weighted accuracy. UA = unweighted accuracy. * denotes that we duplicated the method from cited research with the corresponding dataset in our experiment.</p><note type="other">4-class 71.8 71.8 0.713 Ours-VF 2-class 75.3 75.3 0.755 Ours-FAF 4-class 72.7 72.7 0.726 Ours-FAF 2-class 76.4 76.5 0.768 Ours-FAF 5-class 64.6 63.4 0.644</note><p>C-MKL 1 uses a CNN structure to capture the textual features and fuses them via multiple kernel learning for sentiment analysis ( <ref type="bibr" target="#b20">Poria et al., 2015)</ref>.</p><p>TFN uses a tensor fusion network to extract in- teractions between different modality-specific fea- tures ( .</p><p>LSTM(A) introduces a word-level LSTM with temporal attention structure to predict sentiments on MOSI dataset ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Emotion Recognition Baselines</head><p>SVM Trees extracts LLDs and handcrafted bag- of-words as features. The model automatically generates an ensemble of SVM trees for emotion classification ( <ref type="bibr" target="#b24">Rozgic et al., 2012)</ref>.</p><p>GSV-eVector generates new acoustic represen- tations from selected LLDs using Gaussian Super- vectors and extracts a set of weighed handcrafted textual features as an eVector. A linear kernel SVM is used as the final classifier ( <ref type="bibr" target="#b12">Jin et al., 2015)</ref>.</p><p>C-MKL 2 extracts textual features using a CNN and uses openSMILE to extract 6373 acoustic fea- tures. Multiple kernel learning is used as the final classifier ( <ref type="bibr" target="#b22">Poria et al., 2016)</ref>.</p><p>H-DMS uses a hybrid deep multimodal struc- ture to extract both the text and audio emotional features. A deep neural network is used for feature-level fusion (Gu et al., 2018).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Fusion Baselines</head><p>Utterance-level Fusion (UL-Fusion) focuses on fusing text and audio features from an entire ut- terance ( <ref type="bibr" target="#b9">Gu et al., 2017)</ref>. We simply concatenate the textual and acoustic representations into a joint feature representation. A softmax function is used for sentiment and emotion classification.</p><p>Decision-level Fusion (DL-Fusion) Inspired by <ref type="bibr">(WÃ¶llmer et al., 2013)</ref>, we extract textual and acoustic sentence representations individually and infer the results via two softmax classifiers, re- spectively. As suggested by WÃ¶llmer, we calculate a weighted sum of the text (1.2) result and audio (0.8) result as the final prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model Training</head><p>We implemented the model in Keras with Tensor- flow as the backend. We set 100 as the dimension for each GRU, meaning the bidirectional GRU di- mension is 200. For the decision making, we se- lected 2, 3, 4, and 5 as the filter width and apply 300 filters for each width. We used the rectified linear unit (ReLU) activation function and set 0.5 as the dropout rate. We also applied batch nor- malization functions between each layer to over- come internal covariate shift <ref type="bibr" target="#b11">(Ioffe and Szegedy, 2015)</ref>. We first trained the text attention module and audio attention module individually. Then, we tuned the fusion network based on the word-level representation outputs from each fine-tuning mod- ule. For all training procedures, we set the learn- ing rate to 0.001 and used Adam optimization and categorical cross-entropy loss. For all datasets, we considered the speakers independent and used an 80-20 training-testing split. We further separated 20% from the training dataset for validation. We trained the model with 5-fold cross validation and used 8 as the mini batch size. We set the same amount of samples from each class to balance the training dataset during each iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Result Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Comparison with Baselines</head><p>The experimental results of different datasets show that our proposed architecture achieves state-of-the-art performance in both sentiment analysis and emotion recognition <ref type="table">(Table 1)</ref>. We re-implemented some published methods ( <ref type="bibr" target="#b23">Rosas et al., 2013;</ref><ref type="bibr" target="#b29">WÃ¶llmer et al., 2013</ref>) on MOSI to get baselines.</p><p>For sentiment analysis, the proposed architec- ture with FAF strategy achieves 76.4% weighted accuracy, which outperforms all the five base- lines <ref type="table">(Table 1</ref>). The result demonstrates that the proposed hierarchical attention architecture and word-level fusion strategies indeed help im- prove the performance. There are several find- ings worth mentioning: (i) our model outper- forms the baselines without using the low-level handcrafted acoustic features, indicating the suf- ficiency of MFSCs; (ii) the proposed approach achieves performance comparable to the model us- ing text, audio, and visual data together . This demonstrates that the visual fea- tures do not contribute as much during the fusion and prediction on MOSI; (iii) we notice that <ref type="bibr" target="#b21">(Poria et al., 2017b</ref>) reports better accuracy (79.3%) on MOSI, but their model uses a set of utterances instead of a single utterance as input.</p><p>For emotion recognition, our model with FAF achieves 72.7% accuracy, outperforming all the baselines. The result shows the proposed model brings a significant accuracy gain to emotion recognition, demonstrating the pros of the fine- tuning attention structure. It also shows that word- level attention indeed helps extract emotional fea- tures. Compared to C-MKL 2 and SVM Trees that require feature selection before fusion and predic- tion, our model does not need an additional ar- chitecture to select features. We further evalu- ated our models on 5 emotion categories, includ- ing frustration. Our model shows 4.2% perfor- mance improvement over H-DMS and achieves 0.644 weighted-F1. As H-DMS only achieves 0.594 F1 and also uses low-level handcrafted fea- tures, our model is more robust and efficient.</p><p>From <ref type="table">Table 1</ref>, all the three proposed fusion strategies outperform UL-Fusion and DL-Fusion on both MOSI and IEMOCAP. Unlike utterance- level fusion that ignores the time-scale-sensitive associations across modalities, word-level fusion combines the modality-specific features for each word by aligning text and audio, allowing asso- ciative learning between the two modalities, sim- ilar to what humans do in natural conversation. The result indicates that the proposed methods im- prove the model performance by around 6% accu- <ref type="table">Modality  MOSI  IEMOCAP  WA  F1  WA  F1  T</ref> 75.0 0.748 61.8 0.620 A 60.2 0.604 62.5 0.614 T+A 76.4 0.768 72.7 0.726 <ref type="table">Table 2</ref>: Accuracy (%) and F1 score on text only (T), audio only (A), and multi-modality using FAF (T+A).  <ref type="table">Table 3</ref>: Accuracy (%) and F1 score for general- ization testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approach</head><p>racy. We also notice that the structure with FAF outperforms the HF and VF on both MOSI and IEMOCAP dataset, which demonstrates the effec- tiveness and importance of the FAF strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Modality and Generalization Analysis</head><p>From <ref type="table">Table 2</ref>, we see that textual information dominates the sentiment prediction on MOSI and there is an only 1.4% accuracy improvement from fusing text and audio. However, on IEMOCAP, audio-only outperforms text-only, but as expected, there is a significant performance improvement by combining textual and audio. The difference in modality performance might because of the more significant role vocal delivery plays in emotional expression than in sentimental expression.</p><p>We further tested the generalizability of the pro- posed model. For sentiment generalization test- ing, we trained the model on MOSI and tested on the YouTube dataset <ref type="table">(Table 3)</ref>, which achieves 66.2% accuracy and 0.665 F1 scores. For emo- tion recognition generalization testing, we tested the model (trained on IEMOCAP) on EmotiW and achieves 61.4% accuracy. The potential rea- sons that may influence the generalization are: (i) the biased labeling for different datasets (five an- notators of MOSI vs one annotator of Youtube); (ii) incomplete utterance in YouTube dataset (such as "about", "he", etc.); (iii) without enough speech information (EmotiW is a wild audio- visual dataset that focuses on facial expression). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Visualize Attentions</head><p>Our model allows us to easily visualize the atten- tion weights of text, audio, and fusion to better understand how the attention mechanism works. We introduce the emotional distribution visual- izations for word-level acoustic attention (w Î± i ), word-level textual attention (t Î± i ), shared atten- tion (s Î± i ), and fine-tuning attention based on the FAF structure (u Î± i ) for two example sentences ( <ref type="figure" target="#fig_2">Figure 3</ref>). The color gradation represents the im- portance of the corresponding source data at the word-level.</p><p>Based on our visualization, the textual attention distribution (t Î± i ) denotes the words that carry the most emotional significance, such as "hell" for anger <ref type="figure" target="#fig_2">(Figure 3 a)</ref>. The textual attention shows that "don't", "like", and "west-sider" have simi- lar weights in the happy example <ref type="figure" target="#fig_2">(Figure 3 b)</ref>. It is hard to assign this sentence happy given only the text attention. However, the acoustic atten- tion focuses on "you're" and "west-sider", remov- ing emphasis from "don't" and "like". The shared attention (s Î± i ) and fine-tuning attention (u Î± i ) successfully combine both textual and acoustic attentions and assign joint attention to the cor- rect words, which demonstrates that the proposed method can capture emphasis from both modali- ties at the word-level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>There are several limitations and potential solu- tions worth mentioning: (i) the proposed architec- ture uses both the audio and text data to analyze the sentiments and emotions. However, not all the data sources contain or provide textual informa- tion. Many audio-visual emotion clips only have acoustic and visual information. The proposed ar- chitecture is more related to spoken language anal- ysis than predicting the sentiments or emotions based on human speech. Automatic speech recog- nition provides a potential solution for generating the textual information from vocal signals. (ii)</p><p>The word alignment can be easily applied to hu- man speech. However, it is difficult to align the visual information with text, especially if the text only describes the video or audio. Incorporating visual information into an aligning model like ours would be an interesting research topic. (iii) The limited amount of multimodal sentiment analysis and emotion recognition data is a key issue for cur- rent research, especially for deep models that re- quire a large number of samples. Compared large unimodal sentiment analysis and emotion recog- nition datasets, the MOSI dataset only consists of 2199 sentence-level samples. In our experiments, the EmotiW and MOUD datasets could only be used for generalization analysis due to their small size. Larger and more general datasets are neces- sary for multimodal sentiment analysis and emo- tion recognition in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we proposed a deep multimodal ar- chitecture with hierarchical attention for sentiment and emotion classification. Our model aligned the text and audio at the word-level and applied atten- tion distributions on textual word vectors, acoustic frame vectors, and acoustic word vectors. We in- troduced three fusion strategies with a CNN struc- ture to combine word-level features to classify emotions. Our model outperforms the state-of- the-art methods and provides effective visualiza- tion of modality-specific features and fusion fea- ture interpretation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1</head><label></label><figDesc>Figure 1: Overall Architecture obvious performance improvement, especially for the sentiment analysis. Considering the training cost of the proposed hierarchical acoustic architecture, we decided the extra features were not worth the tradeoff. The output is a 3D MFSCs map with dimensions [N, 64, L].</figDesc><graphic url="image-1.png" coords="3,307.28,62.81,218.27,295.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Fusion strategies. t h i : word-level textual bidirectional state. t Î± i : word-level textual attention distribution. w h i : word-level acoustic bidirectional state. w Î± i : word-level acoustic attention distribution. s Î± i : shared attention distribution. u Î± i : fine-tuning attention distribution. V i : shared word-level representation. Algorithm 2 FUSION 1: procedure FUSION BRANCH 2:</figDesc><graphic url="image-2.png" coords="5,72.00,62.81,453.54,155.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Attention visualization.</figDesc><graphic url="image-3.png" coords="9,72.00,62.81,453.55,97.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>:</head><label></label><figDesc></figDesc><table>end for 

13: 

return t h i , t Î± i 
14: end procedure 
15: procedure AUDIO BRANCH 

16: 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>8 :</head><label>8</label><figDesc></figDesc><table>Vertical Fusion (VF) 

9: 

for i â [1, N ] do 

10: 

h i â dense([t h i , w h i ]) 

11: 

s Î± i â average([t Î± i , w Î± i ]) 

12: 

V i â weighted(h i , s Î± i ) 

13: 

end for 

14: 

Fine-tuning Attention Fusion (FAF) 

15: 

for i â [1, N ] do 

16: 

u e i â getEnergies(h i ) 

17: 

u Î± i â getDistribution(u e i , s Î± i ) 

18: 

V i â weighted(h i , u Î± i ) 

19: 

end for 

20: 

Decision Making 

21: 

</table></figure>

			<note place="foot" n="3"> Method We introduce a multimodal hierarchical attention structure with word-level alignment for sentiment analysis and emotion recognition (Figure 1). The model consists of three major parts: text attention module, audio attention module, and word</note>

			<note place="foot" n="2"> https://cs.anu.edu.au/few/ChallengeDetails.html 3 https://www.ibm.com/watson/developercloud/speechto-text/api/v1/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous review-ers for their valuable comments and feedback. We thank the useful suggestions from Kaixiang Huang. This research was funded by the Na-tional Institutes of Health under Award Number R01LM011834.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ossama</forename><surname>Abdel-Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdel-Rahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Penn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on audio</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1533" to="1545" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>and language processing</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murtaza</forename><surname>Bulut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Chun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Mower</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeannette</forename><forename type="middle">N</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungbok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shrikanth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Narayanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page">335</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multimodal sentiment analysis with wordlevel fusion and reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadas</forename><surname>BaltruÅ¡aitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM International Conference on Multimodal Interaction</title>
		<meeting>the 19th ACM International Conference on Multimodal Interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="163" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Covarepa collaborative voice analysis repository for speech technologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Degottex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Drugman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuomo</forename><surname>Raitio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="960" to="964" />
		</imprint>
	</monogr>
	<note>2014 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On-line emotion recognition in a 3-d activation-valence-time continuum using acoustic and linguistic cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>WÃ¶llmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">BjÃ¶rn</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Douglas-Cowie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roddy</forename><surname>Cowie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal on Multimodal User Interfaces</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="7" to="19" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Opensmile: the munich versatile and fast open-source audio feature extractor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>WÃ¶llmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">BjÃ¶rn</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM international conference on Multimedia</title>
		<meeting>the 18th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1459" to="1462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Predicting emotion in spoken dialogue from multiple knowledge sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Riley</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Litman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL</title>
		<meeting>the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deep multimodal learning for emotion recognition in spoken language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Marsic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08332</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Speech intention classification with multimodal deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Marsic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Canadian Conference on Artificial Intelligence</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="260" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attention assisted discovery of sub-utterance structure in speech emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Che-Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shrikanth S Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1387" to="1391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Speech emotion recognition with acoustic and lexical features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4749" to="4753" />
		</imprint>
	</monogr>
	<note>IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Predicting student emotions in computer-human tutoring dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Diane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Litman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Forbes-Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 351. Association for Computational Linguistics</title>
		<meeting>the 42nd Annual Meeting on Association for Computational Linguistics, page 351. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic speech emotion recognition using recurrent neural networks with local attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyedmahdad</forename><surname>Mirsamadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emad</forename><surname>Barsoum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cha</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2227" to="2231" />
		</imprint>
	</monogr>
	<note>2017 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Towards multimodal sentiment analysis: Harvesting opinions from the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Payal</forename><surname>Doshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th international conference on multimodal interfaces</title>
		<meeting>the 13th international conference on multimodal interfaces</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Attentive convolutional neural network based speech emotion recognition: A study on the impact of input features, signal length, and acted speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><forename type="middle">Thang</forename><surname>Vu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.00612</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A review of affective computing: From unimodal analysis to multimodal fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajiv</forename><surname>Bajpai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">formation Fusion</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="98" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep convolutional neural network textual features and multiple kernel learning for utterance-level multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 conference on empirical methods in natural language processing</title>
		<meeting>the 2015 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2539" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Context-dependent sentiment analysis in user-generated videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="873" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Convolutional mkl based multimodal emotion recognition and sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iti</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining (ICDM), 2016 IEEE 16th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="439" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multimodal sentiment analysis of spanish online videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>VerÃ³nica PÃ©rez Rosas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louisphilippe</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="38" to="45" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ensemble of svm trees for multimodal emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sankaranarayanan</forename><surname>Viktor Rozgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirin</forename><surname>Ananthakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Saleem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Prasad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal &amp; Information Processing Association Annual Summit and Conference (APSIPA ASC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dynamic programming algorithm optimization for spoken word recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroaki</forename><surname>Sakoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seibi</forename><surname>Chiba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on acoustics, speech, and signal processing</title>
		<imprint>
			<date type="published" when="1978" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="43" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Combining video, audio and lexical indicators of affect in spontaneous conversation via particle filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Savran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miraj</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ragini</forename><surname>Verma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM international conference on Multimodal interaction</title>
		<meeting>the 14th ACM international conference on Multimodal interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="485" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Laurence Devillers, Laurence Vidrascu, Noam Amir, and Vered Aharonson. 2008. Patterns, prototypes, performance: classifying emotional user states</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dino</forename><surname>Seppi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Batliner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">BjÃ¶rn</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Steidl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thurid</forename><surname>Vogt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ninth Annual Conference of the International Speech Communication Association</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Select-additive learning: Improving cross-individual generalization in multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaksha</forename><surname>Meghawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.05244</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Youtube movie reviews: Sentiment analysis in an audio-visual context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>WÃ¶llmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Knaup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">BjÃ¶rn</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congkai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louisphilippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="46" to="53" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Tensor fusion network for multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07250</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Pincus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louisphilippe</forename><surname>Morency</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.06259</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning affective features with a hybrid deep model for audio-visual emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
